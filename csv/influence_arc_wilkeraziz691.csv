2010.eamt-1.31,N06-1003,0,0.193211,"aspects of the process that generated them, such as the appropriateness of the replacement in the context of the specific source sentence, allowing for example reach to be preferred to strike or attack in replacing hit in “We hit the city at lunch time”. Dynamic and static biphrases compete during the search for an optimal translation. At training time, standard techniques such as MERT (Minimum Error Rate Training) (Och, 2003), which attempt to maximize automatic metrics like BLEU (Papineni et al., 2002) based on a bilingual corpus, are directly applicable. However, as has been discussed in (Callison-Burch et al., 2006; Mirkin et al., 2009), such automatic measures are poor indicators of improvements in translation quality in presence of semantic modifications of the kind we are considering here. Therefore, we perform the training and evaluation on the basis of human annotations. We use a form of active learning to focus the annotation effort on a small set of candidates which are useful for the training. Sentences containing OOV words represent a fairly small fraction of the sentences to be translated2 . Thus, to avoid human annotation of a large sample with relatively few cases of OOV words, for the purpo"
2010.eamt-1.31,W08-0309,0,0.0194641,"Missing"
2010.eamt-1.31,2005.mtsummit-papers.30,0,0.0118202,"sters”, where the rank between clusters is clear, but the elements inside each cluster are considered indistinguishable. The annotators are also asked to concentrate their judgment on the portions of the sentences which are affected by the different replacements. To cover potential cases of cognates, annotators can choose the actual OOV as the best “replacement”. Active sampling In order to keep the sample of candidate translations to be annotated for a given OOV source sentence small, but still informative for training, we adopt an active learning scheme (Settles, 2010; Haffari et al., 2009; Eck et al., 2005). We do not extract a priori a sample of translation candidates for each sentence in the OOV training set and ask the annotators to work on these samples — which would mean that they might have to compare candidates that have little chance of being selected by the end-model after training. Instead, This is an iterative process, with a slice of the OOV training set selected for each iteration. When sampling candidate translations (out of a given slice of the OOV training set) to be annotated in the next iteration, we use the translations produced by the model Λ ⊕ M obtained so far, after traini"
2010.eamt-1.31,N09-1047,0,0.0131136,"in a few distinct “clusters”, where the rank between clusters is clear, but the elements inside each cluster are considered indistinguishable. The annotators are also asked to concentrate their judgment on the portions of the sentences which are affected by the different replacements. To cover potential cases of cognates, annotators can choose the actual OOV as the best “replacement”. Active sampling In order to keep the sample of candidate translations to be annotated for a given OOV source sentence small, but still informative for training, we adopt an active learning scheme (Settles, 2010; Haffari et al., 2009; Eck et al., 2005). We do not extract a priori a sample of translation candidates for each sentence in the OOV training set and ask the annotators to work on these samples — which would mean that they might have to compare candidates that have little chance of being selected by the end-model after training. Instead, This is an iterative process, with a slice of the OOV training set selected for each iteration. When sampling candidate translations (out of a given slice of the OOV training set) to be annotated in the next iteration, we use the translations produced by the model Λ ⊕ M obtained s"
2010.eamt-1.31,J10-4005,0,0.0120582,"us the annotation task on the specific problem of sentences containing OOV words, and (ii) even for these sentences, we should only hand the annotators a small, well-chosen, sample of translation candidates to assess, not an exhaustive list. Finally, we need to be careful not to bias training towards the human annotated sample in such a way that the integrated decoder becomes better on the OOV sentences, but is degraded on the “normal” sentences. We address these requirements as follows. 2 Integrated decoding The integrated decoder consists of a standard phrase-based SMT decoder (Lopez, 2008; Koehn, 2010) enhanced with the ability to add dynamic biphrases at runtime and attempting to maximize a variant of the standard “log-linear” objective function. The standard SMT decoder tries to find argmax(a,t) Λ · G(s, t, a), where Λ is a vector of weights, and G(s, t, a) a vector of features depending on the source sentence s, the target sentence t and the phrase-level alignment a. The integrated decoder tries to find argmax(a,t) Λ · G(s, t, a) + M · H(s, t, a) where M is an additional vector of weights and H(s, t, a) an additional vector of “dynamic” features associated with the dynamic biphrases and"
2010.eamt-1.31,D09-1040,0,0.228261,"al data are scarce or the text to be translated is not from the same domain as the data used to train the system. One approach consists in replacing the OOV word by a paraphrase, i.e. a word that is equivalent and known to the phrase-table. For instance, in the sentence “The police hit the protester”, if the source word “hit” is OOV, it could be replaced by its paraphrase “struck”. In previous work such paraphrases are learnt by “pivoting” through parallel texts involving multiple languages (CallisonBurch et al., 2006) or on the basis of monolingual data and distributional similarity metrics (Marton et al., 2009). Mirkin et al. (2009) go beyond the use of paraphrase to incorporate the notion of an entailed phrase, that is, a word which is implied by the OOV word, but is not necessarily equivalent to it — for example, this could result in “hit” being replaced by the entailed phrase “attacked”. Both paraphrases and entailed phrases are obtained using monolingual resources such as WordNet (Fellbaum, 1998). This approach results in higher coverage and human acceptability of the translations produced relative to approaches based only on paraphrases. In (Mirkin et al., 2009) a replacement for the OOV word i"
2010.eamt-1.31,P04-1036,0,0.029228,"he vectors of all the content words in it. 3 Technically, this ratio is only defined for π (M ) (X) 6= 0, i.e. for cases where the pair of translations differ in their M projections; in the rare instances where this might not be true, we can simply ignore the pair in the learning process. Domain similarity Score representing how well rep can replace oov in general in texts of a given domain. It is computed as the cosine similarity between the LSA vectors of the two words and is intended to give preference to replacements which correspond to more frequent senses of the OOV word in that domain (McCarthy et al., 2004). Information loss Measures the distance in WordNet’s hierarchy, denoted d, between oov and rep: 1 S(unk, sub) = 1 − ( d+1 ), where the distance between synonyms is 0, and the further the hypernym is up the hierarchy, the smaller the score. This can be considered a simple approximation to the notion of information loss, that is, the further the rep is from the oov in a hierarchy, the fewer semantic traits exist between the two, and therefore the more information is lost if we use rep. Identity Binary feature to mark the cases where the OOV is kept in the sentence, what we call an “identity” re"
2010.eamt-1.31,P09-1089,1,0.237426,"the text to be translated is not from the same domain as the data used to train the system. One approach consists in replacing the OOV word by a paraphrase, i.e. a word that is equivalent and known to the phrase-table. For instance, in the sentence “The police hit the protester”, if the source word “hit” is OOV, it could be replaced by its paraphrase “struck”. In previous work such paraphrases are learnt by “pivoting” through parallel texts involving multiple languages (CallisonBurch et al., 2006) or on the basis of monolingual data and distributional similarity metrics (Marton et al., 2009). Mirkin et al. (2009) go beyond the use of paraphrase to incorporate the notion of an entailed phrase, that is, a word which is implied by the OOV word, but is not necessarily equivalent to it — for example, this could result in “hit” being replaced by the entailed phrase “attacked”. Both paraphrases and entailed phrases are obtained using monolingual resources such as WordNet (Fellbaum, 1998). This approach results in higher coverage and human acceptability of the translations produced relative to approaches based only on paraphrases. In (Mirkin et al., 2009) a replacement for the OOV word is chosen based on a sc"
2010.eamt-1.31,P03-1021,0,0.00474717,"rapp´e) and (hit, a attaqu´e) from the static ones (struck, a frapp´e) and (attacked, a attaqu´e). Such dynamic biphrases are assigned several features that characterize different aspects of the process that generated them, such as the appropriateness of the replacement in the context of the specific source sentence, allowing for example reach to be preferred to strike or attack in replacing hit in “We hit the city at lunch time”. Dynamic and static biphrases compete during the search for an optimal translation. At training time, standard techniques such as MERT (Minimum Error Rate Training) (Och, 2003), which attempt to maximize automatic metrics like BLEU (Papineni et al., 2002) based on a bilingual corpus, are directly applicable. However, as has been discussed in (Callison-Burch et al., 2006; Mirkin et al., 2009), such automatic measures are poor indicators of improvements in translation quality in presence of semantic modifications of the kind we are considering here. Therefore, we perform the training and evaluation on the basis of human annotations. We use a form of active learning to focus the annotation effort on a small set of candidates which are useful for the training. Sentences"
2010.eamt-1.31,P02-1040,0,0.0818727,"´e) and (attacked, a attaqu´e). Such dynamic biphrases are assigned several features that characterize different aspects of the process that generated them, such as the appropriateness of the replacement in the context of the specific source sentence, allowing for example reach to be preferred to strike or attack in replacing hit in “We hit the city at lunch time”. Dynamic and static biphrases compete during the search for an optimal translation. At training time, standard techniques such as MERT (Minimum Error Rate Training) (Och, 2003), which attempt to maximize automatic metrics like BLEU (Papineni et al., 2002) based on a bilingual corpus, are directly applicable. However, as has been discussed in (Callison-Burch et al., 2006; Mirkin et al., 2009), such automatic measures are poor indicators of improvements in translation quality in presence of semantic modifications of the kind we are considering here. Therefore, we perform the training and evaluation on the basis of human annotations. We use a form of active learning to focus the annotation effort on a small set of candidates which are useful for the training. Sentences containing OOV words represent a fairly small fraction of the sentences to be"
2010.eamt-1.31,H05-1095,1,0.886826,"Missing"
2011.mtsummit-papers.58,P11-1022,0,0.105777,"ions (HTER). The estimated scores showed to correlate very well with human post-editing effort. Subsequently, Specia (2011) focuses on a more objective type of annotation: post-editing time. This has shown to be the most useful to allow ranking translations according to the post-editing effort they require. A recent direction in QE is the addition of linguistic information as features. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) use POS tags of neighbor words and a link grammar parser to indicate words that are not connected to the rest of the sentence. Bach et al. (2011) check whether the dependency relations in the source sentence are preserved in the translation. Both approaches have shown the potential of linguistic features, but only Bach et al. (2011) use features contrasting the source and translation texts. However, these papers either focus on word-level quality estimation or on the estimation of automatic evaluation metrics. Moreover, they do not distinguish the types of errors in terms of ﬂuency and adequacy: a substitution error referring to a simple morphological variation (with no effect on adequacy) is considered in the same way as a content wor"
2011.mtsummit-papers.58,N10-1099,0,0.0282637,"ease refer to (Blatz et al., 2003) for a complete list of source complexity and ﬂuency features. SF - Source complexity features: • • • • source sentence length source sentence type/token ratio average source word length source sentence 3-gram language model probability obtained based on the source side of the parallel corpus used to build the translation model of the SMT system TF - Target ﬂuency features: • target sentence 3-gram language model probability obtained based on a large indomain corpus of the target language • translation sentence length • coherence of the target sentence as in (Burstein et al., 2010) AF - Adequacy features: • ratio of number of tokens in source and target and vice-versa • absolute difference between number of tokens and source and target normalized by source length • ratio of percentages of numbers, content/ non-content words in the source & target • ratio of percentage of nouns/verbs/etc in the source and target • absolute difference between number of superﬁcial constructions in the source and target: brackets, numbers, punctuation symbols • proportion of dependency relations with constituents aligned between source and target • absolute difference between the depth of t"
2011.mtsummit-papers.58,W08-1301,0,0.0446022,"Missing"
2011.mtsummit-papers.58,W10-1751,0,0.0437188,"Missing"
2011.mtsummit-papers.58,P10-1074,0,0.151173,"tem For feature extraction, all the datasets were preprocessed as follows: Arabic (source): word transliteration, segmentation and morphological analysis using MADA (Habash and Rambow, 2005); POS tagging and chunking using AMIRA (Diab, 2009), constituent and dependency parsing using the Stanford parser (Green and Manning, 2010), and NER using a model learned from projections of English named entities (Section 3.2.1). English (target): chunking using OpenNLP2 , constituent and dependency parsing using the Stanford parser (de Marneffe and Manning, 2008), NER using a combination of the Stanford (Finkel and Manning, 2010) and OpenNLP NER systems. 3.2 Features The feature set used in this paper includes features from all categories shown in Figure 2. In total, 122 MT system-independent features were extracted for both S1 ans S2 datasets. In what follows we describe the adequacy features proposed in this paper, as well 2 of translations scored using the scheme MT S1 S2 S1 S2 S1 S2 http://incubator.apache.org/opennlp/ as provide some examples of the non-adequacy related features - please refer to (Blatz et al., 2003) for a complete list of source complexity and ﬂuency features. SF - Source complexity features: •"
2011.mtsummit-papers.58,C10-1045,0,0.0525677,"Missing"
2011.mtsummit-papers.58,P05-1071,0,0.104287,"Missing"
2011.mtsummit-papers.58,P10-1064,0,0.140521,"Missing"
2011.mtsummit-papers.58,J03-1002,0,0.00412171,"and type of entities in the source and target sentences. Since no freely available wide-coverage Named Entity Recognizer (NER) for Arabic exists, we implemented a simple model based on the projection of English NE obtained using a large Arabic-English in-domain parallel corpus. The English side of the parallel corpus is ﬁrst annotated for NEs (Person, Location and Organization). We use both the Stanford (Finkel and Manning, 2010) and the OpenNLP NER systems3 . The English annotations are projected to the Arabic side using word-alignment information. We align the parallel corpus using GIZA++ (Och and Ney, 2003) in the both directions (ar-en and en-ar) to produce the symmetrized alignment using tools provided by the Moses toolkit4 . We then collect entities and their types to compute the context-independent probability distribution p(ar|tag). More speciﬁcally, the word alignment and the source annotation is used to extract synchronous productions in a similar way to the rule extraction in tree-based SMT. The collection of annotated phrases is stored in a rule table with some relevant scores as exempliﬁed by Figure 3. We use the resulting rule table to estimate the probability of assigning a given ent"
2011.mtsummit-papers.58,P02-1040,0,0.088385,"Experiments with Arabic-English translations show that the proposed prediction models can yield more reliable adequacy estimators for new translations. In Section 2 we present related work in the ﬁeld of quality estimation for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translat"
2011.mtsummit-papers.58,quirk-2004-training,0,0.233792,"ble adequacy estimators for new translations. In Section 2 we present related work in the ﬁeld of quality estimation for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics"
2011.mtsummit-papers.58,2006.amta-papers.25,0,0.095262,"anslations. Specia et al. (2009) use similar features to train 514 a regression algorithm on larger datasets annotated by humans for post-editing effort. Satisfactory results were achieved when using the estimated scores for practical applications such as the selection of the best translation among alternatives from different MT systems (Specia et al., 2010). He et al. (2010) propose using QE to recommend a translation from either an MT or a Translation Memory (TM) system for each source segment for post-editing. The QE model is trained on automatic annotation for Translation Edit Rate (TER) (Snover et al., 2006) and the goal is to predict the translation that would yield the minimum edit distance to a reference translation. Specia and Farzindar (2010) use TER to estimate the distance between machine translations and their post-edited versions (HTER). The estimated scores showed to correlate very well with human post-editing effort. Subsequently, Specia (2011) focuses on a more objective type of annotation: post-editing time. This has shown to be the most useful to allow ranking translations according to the post-editing effort they require. A recent direction in QE is the addition of linguistic infor"
2011.mtsummit-papers.58,2010.jec-1.5,1,0.937272,"ion for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics used do not correlate well with human judgments at the sentence-level. In fact, Quirk (2004) showed that using a small set of t"
2011.mtsummit-papers.58,2009.eamt-1.5,1,0.841078,"estimators for new translations. In Section 2 we present related work in the ﬁeld of quality estimation for MT. In Section 3 we describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics used do not correlat"
2011.mtsummit-papers.58,2011.eamt-1.12,1,0.939109,"describe the proposed approach, the datasets, features, resources and algorithms used. In Section 4 we present our experiments and results. 2 Related Work Most work on sentence-level quality estimation (QE) – also called conﬁdence estimation – proposed so far has focused on (i) estimating general quality scores - such as automatic metrics like BLEU (Papineni et al., 2002) - for tasks like n-best list reordering or MT system selection (Blatz et al., 2003; Quirk, 2004; Specia et al., 2009; Specia et al., 2010) and (ii) estimating post-editing effort (He et al., 2010; Specia and Farzindar, 2010; Specia, 2011). The ﬁrst signiﬁcant effort towards sentence level quality estimation is presented in (Blatz et al., 2003). A large number of source, target and MT system features are used to train machine learning algorithms to estimate automatic metrics such as NIST (Doddington, 2002), which are then thresholded into binary scores to distinguish “good” from “bad” translations. The results were not very encouraging, possibly due to the fact that the automatic metrics used do not correlate well with human judgments at the sentence-level. In fact, Quirk (2004) showed that using a small set of translations man"
2011.mtsummit-papers.58,P10-1062,0,0.0340104,"ce to a reference translation. Specia and Farzindar (2010) use TER to estimate the distance between machine translations and their post-edited versions (HTER). The estimated scores showed to correlate very well with human post-editing effort. Subsequently, Specia (2011) focuses on a more objective type of annotation: post-editing time. This has shown to be the most useful to allow ranking translations according to the post-editing effort they require. A recent direction in QE is the addition of linguistic information as features. Focusing on word-error detection through the estimation of WER, Xiong et al. (2010) use POS tags of neighbor words and a link grammar parser to indicate words that are not connected to the rest of the sentence. Bach et al. (2011) check whether the dependency relations in the source sentence are preserved in the translation. Both approaches have shown the potential of linguistic features, but only Bach et al. (2011) use features contrasting the source and translation texts. However, these papers either focus on word-level quality estimation or on the estimation of automatic evaluation metrics. Moreover, they do not distinguish the types of errors in terms of ﬂuency and adequa"
2012.amta-wptp.2,aziz-etal-2012-pet,1,0.787719,"et (common) was edited by all the eight post-editors, that is, all of them post-edited the same 20 machine translations. The machine translations in the second dataset (main) were randomly distributed amongst the post-editors so that each of them only edited one translation for a given source sentence, and all of them edited a similar number of translations from each MT system (on average 23 per system). In sum, each post-editor edited 203 sentences (20 in common and 183 in main). For each translation, post-editing effort indicators were logged using PET,1 a freely available postediting tool (Aziz et al., 2012). Among these indicators, of particular interest to our study are: 1 http://pers-www.wlv.ac.uk/˜in1676/pet/ • TIME the post-editing time of a sentence; • SPW seconds per word, that is, the TIME the translator spent to post-edit the sentence divided by the length (in tokens) of the postedited translation; • KEYS the number of keystrokes pressed during the post-editing of the sentence (PKEYS is the subset of printable keys, that is, those that imply a visible change in the text); and • HTER the standard edit distance between the original machine translation and its post-edited version (Snover et"
2012.amta-wptp.2,2011.mtsummit-papers.17,0,0.134559,"ion and its post-edited version have a crucial limitation: they cannot fully capture the effort resulting from post-editing such a translation. Certain operations can be more difficult than others, based not only on the type of edit (deletion, insertion, substitution), but also on the words being edited. Edits due to incorrect morphological variants or function words are generally treated the same way as more complex edits such as fixing an untranslated content word. While variants of such metric assigning weights for specific edits or classes of words can be implemented (Snover et al., 2010; Blain et al., 2011), defining classes of complex words to postedit requires a lexicalised, linguistically- motivated and thus language-dependent approach. In addition, the complexity of a correction cannot always be characterized based only on a local edit, as it may depend on the neighbourhood of that edit. Recently, Koponen (2012) conducted an error analysis on post-edited translations with HTER and 1-5 scores assigned by humans for post-editing effort. A number of cases were found where postedited translations with low HTER (few edits) were assigned low quality scores (high post-editing effort), and vice-vers"
2012.amta-wptp.2,2009.mtsummit-posters.5,0,0.479427,"Missing"
2012.amta-wptp.2,W12-3123,1,0.7105,"ncorrect morphological variants or function words are generally treated the same way as more complex edits such as fixing an untranslated content word. While variants of such metric assigning weights for specific edits or classes of words can be implemented (Snover et al., 2010; Blain et al., 2011), defining classes of complex words to postedit requires a lexicalised, linguistically- motivated and thus language-dependent approach. In addition, the complexity of a correction cannot always be characterized based only on a local edit, as it may depend on the neighbourhood of that edit. Recently, Koponen (2012) conducted an error analysis on post-edited translations with HTER and 1-5 scores assigned by humans for post-editing effort. A number of cases were found where postedited translations with low HTER (few edits) were assigned low quality scores (high post-editing effort), and vice-versa. This seems to indicate that certain edits require more cognitive effort than others, which is not captured by HTER. Post-editing effort consists of different aspects: temporal, technical and cognitive (Krings, 2001). However, these aspects are highly interconnected. The temporal effort (time spent on post-editi"
2012.amta-wptp.2,padro-etal-2010-freeling,0,0.0395098,"Missing"
2012.amta-wptp.2,2006.amta-papers.25,0,0.565698,"as a way of evaluating the quality of machine translations, particularly for the purpose of comparing various MT systems. Effective ways of measuring post-editing effort – and thus MT quality – in both scenarios is a very relevant but open problem. Standard MT evaluation metrics have proved to correlate significantly better with human assessments of quality when computed having a postedited version of the automatic translation as reference as opposed to translations created independently from automatic translations. One of these metrics is HTER – the “Human-targeted Translation Edit Rate” – (Snover et al., 2006), which was used as the official metric in the DARPA GALE program (Olive et al., 2011). HTER is an edit distance metric that computes the minimum number of edits between the system output and its (often minimally) post-edited version. It is a simple metric which has nevertheless shown to be very effective. However, this and other metrics that estimate the similarity or distance between a system translation and its post-edited version have a crucial limitation: they cannot fully capture the effort resulting from post-editing such a translation. Certain operations can be more difficult than othe"
2012.amta-wptp.2,R11-1014,1,0.590295,"ice in the translation industry. Tatsumi (2009) examines the correlation between post-editing time and certain automatic metrics measuring textual differences. They find that the relationship between these two measures is not always linear, and offer some variables such as source sentence length and structure as well as specific types of dependency errors as possible explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on post-editing translations from different MT systems and on translating from scratch. The study has shown that sentences requiring less time to post-edit are more often tagged by humans as demanding low effort. It has also shown that post-editing time has good correlation with HTER for ranking both systems and segments. Specia (2011) uses post-editing time as a way of evaluating quality estimation systems. A comparison is made between the post-editing of sentences predicted to be good and average quality sentences, showing that sentences in the first batch can be post"
2012.amta-wptp.2,2011.eamt-1.12,1,0.737598,"le explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on post-editing translations from different MT systems and on translating from scratch. The study has shown that sentences requiring less time to post-edit are more often tagged by humans as demanding low effort. It has also shown that post-editing time has good correlation with HTER for ranking both systems and segments. Specia (2011) uses post-editing time as a way of evaluating quality estimation systems. A comparison is made between the post-editing of sentences predicted to be good and average quality sentences, showing that sentences in the first batch can be postedited much faster. Focusing on sub-segments, Doherty and O’Brien (2009) use an eye-tracker tool to log the fixation and gaze counts and time of translators while reading the output of an MT system. Overall translation quality was quantified on the basis of the number and the duration of fixations. Results show that fixation counts correlate well with human j"
2012.amta-wptp.2,2009.mtsummit-posters.20,0,0.392199,"h edited word as a separate action, PEAs incorporate several interrelated edit operations. For example, changing a noun propagates changes to its attributes (number, gender) which are then treated as one action. This approach has the disadvantages that it is hardly generalizable across languages, and it requires annotated corpus to train a model to classify PEAs for new texts. A practical alternative, measuring time as a way of assessing post-editing effort, has only recently started to be used by researchers, although we believe this may be a more common practice in the translation industry. Tatsumi (2009) examines the correlation between post-editing time and certain automatic metrics measuring textual differences. They find that the relationship between these two measures is not always linear, and offer some variables such as source sentence length and structure as well as specific types of dependency errors as possible explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on p"
2012.amta-wptp.2,temnikova-2010-cognitive,0,0.334492,"A practical alternative, measuring time as a way of assessing post-editing effort, has only recently started to be used by researchers, although we believe this may be a more common practice in the translation industry. Tatsumi (2009) examines the correlation between post-editing time and certain automatic metrics measuring textual differences. They find that the relationship between these two measures is not always linear, and offer some variables such as source sentence length and structure as well as specific types of dependency errors as possible explanations. (Temnikova and Orasan, 2009; Temnikova, 2010) contrast the time translators spent fixing translations for texts produced according to a controlled language, versus translations produced using noncontrolled language. Sousa et al. (2011) compare the time spent on post-editing translations from different MT systems and on translating from scratch. The study has shown that sentences requiring less time to post-edit are more often tagged by humans as demanding low effort. It has also shown that post-editing time has good correlation with HTER for ranking both systems and segments. Specia (2011) uses post-editing time as a way of evaluating qu"
2012.amta-wptp.2,vilar-etal-2006-error,0,0.0478576,"Missing"
2012.amta-wptp.2,2012.eamt-1.31,0,\N,Missing
2012.amta-wptp.2,2012.tc-1.5,1,\N,Missing
2012.eamt-1.33,aziz-etal-2012-pet,1,0.850289,"Missing"
2012.eamt-1.33,P05-1074,0,0.0416939,"vies and other audiovisual materials. The English-Brazilian Portuguese portion of the corpus amounts to 28 million subtitle pairs. We selected the top 14 million pairs to build a translation model, which we judged to be enough for a PB-SMT system. The data is already automatically pre-processed: tokenized, truecased and word-aligned. To generate the tuning and test sets we took the most recent episodes of three TV series from the same source of fan-made subtitles, which were not included in the Opus release: Dexter (D), How I 4 Experiments with popular methods to generate paraphrases such as (Bannard and Callison-Burch, 2005) resulted in very poor paraphrases for this domain, most likely due to the highly non-literal nature of translations. 5 http://www.opensubtitles.org 107 Models and baselines In all cases, the tuning of the systems was performed individually for each TV series. 4.3 Evaluation In order to objectively evaluate our approach for both translation and compression, we have human translators post-editing the machine translations and collect various information from this process. Meta-information from post-editing has been successfully used in previous work to avoid the subjective nature of explicit sco"
2012.eamt-1.33,P11-2031,0,0.016716,"lators and adapted after a pilot experiment with 150 subtitles post-edited per translators. In a nutshell, translators should minimally correct translations to make them fluent and adequate (style and consistency should be disregarded) and compress them when necessary. The following instructions summarise the guidelines: 5 In this section we discuss the performance of the systems in terms of automatic metrics computed using the human post-edited translations for the 3 test sets (i.e. D, H and T). Note that translation quality and compression are jointly evaluated. We use the multeval toolkit (Clark et al., 2011) to score the systems and test them for statistical significance.7 We report BLEU, TER and the hypothesis length over the reference length in percentage (LENGTH).8 To make the reference set we put together all post-edited translations that were length compliant. In addition, references longer than the ideal length were kept only if no compliant paraphrase was produced by any of the annotators (we observed only 5 of those cases). For all test sets (Tables 1 to 3), systems trained using subtitles data outperform B1 (Google) by a large margin, which shows that parallel subtitles provide phrase pa"
2012.eamt-1.33,D11-1108,0,0.0808727,"Missing"
2012.eamt-1.33,W06-2907,0,0.0607647,"Missing"
2012.eamt-1.33,P07-2045,0,0.00493119,"ntrol of translation quality and compression rate in a single step. Additional paraphrases generated by any means could also be added to the phrase table, for example, following the method in (Ganitkevitch et al., 2011). Compression may incur some loss of information. To prevent unnecessary and excessive compression, we treat compression as a less deterministic process by dynamically modeling the need for compression as a function of the time/space constraints of each specific source segment. Our approach models time/space constraints by (i) adding model components to the Moses PB-SMT system (Koehn et al., 2007) to control the need of compression, and (ii) guiding the tuning process to prefer shorter translations. Each of these strategies is described in what follows. 3.3 Dynamic length penalty Time and space constraints can be represented as a function of the time available for the source subtitle, as described in Section 3.1. In practice, these constraints will affect the length of the target subtitle, and therefore hereafter we refer to them as a length constraint. To incorporate this constraint into the Moses decoder, we define a characterbased length penalty to adjust translations so that they m"
2012.eamt-1.33,W07-0734,0,0.0294797,"ss a dataset for which gold translations are known is used to incrementally tune the model parameters towards improving a measure of quality, traditionally BLEU (Papineni et al., 2002). In order to guide the model to select translation candidates that are likely to be good while complying with the length constraint, at tuning time, when compression is necessary the model must reward phrases that are shorter. This can be done by i) biasing the evaluation metric towards shorter translations (Ganitkevitch et al., 2011); ii) using evaluation metrics that go beyond string matching, such as METEOR (Lavie and Agarwal, 2007), which also matches synonyms and paraphrases; iii) adding multiple reference translations that vary in length; or (iv) filtering the tuning set so that it contains only pairs of segments that comply with the length constraint. These strategies do not necessarily exclude each other, and can rather complement each other. An evaluation metric that rewards compression in general does not suit our application to subtitle translation, where segments should only be compressed when necessary. As for tuning with metrics like METEOR, the lack of quality indomain Portuguese paraphrases for the subtitle"
2012.eamt-1.33,P02-1040,0,0.0855942,"ength. If instead the source subtitle is longer, the model targets the ideal length, aiming at producing a translation that observes the time and space constraints even though the original text is too lengthy. 3.4 Tuning process Adding a new component to the model requires learning its contribution and its interaction with the other components. These model parameters are adjusted in a process often referred to as tuning. In this process a dataset for which gold translations are known is used to incrementally tune the model parameters towards improving a measure of quality, traditionally BLEU (Papineni et al., 2002). In order to guide the model to select translation candidates that are likely to be good while complying with the length constraint, at tuning time, when compression is necessary the model must reward phrases that are shorter. This can be done by i) biasing the evaluation metric towards shorter translations (Ganitkevitch et al., 2011); ii) using evaluation metrics that go beyond string matching, such as METEOR (Lavie and Agarwal, 2007), which also matches synonyms and paraphrases; iii) adding multiple reference translations that vary in length; or (iv) filtering the tuning set so that it cont"
2012.eamt-1.33,piperidis-etal-2004-multimodal,0,0.0719133,"Missing"
2012.eamt-1.33,W00-0506,0,0.113837,"Missing"
2012.eamt-1.33,2006.amta-papers.25,0,0.0604581,"anslation observes time/space constraints. It uses colors to facilitate the visualization of the compression needs and indicates the number of characters that need to be compressed or remain to be used in the translation. Each test set was given to human translators along with the post-editing tool and guidelines for translation correction and compression. Eight Brazilian Portuguese native speakers and fluent speakers of English with significant experience in English-Portuguese translation post-edited the MT outputs. We base our evaluation on the computation of automatic metrics such as HTER (Snover et al., 2006) between the machine translation and its post-edited version (Section 5). 4.3.1 Post-editing guidelines and task design Guidelines and examples of translations were given to the translators and adapted after a pilot experiment with 150 subtitles post-edited per translators. In a nutshell, translators should minimally correct translations to make them fluent and adequate (style and consistency should be disregarded) and compress them when necessary. The following instructions summarise the guidelines: 5 In this section we discuss the performance of the systems in terms of automatic metrics comp"
2012.eamt-1.33,R11-1014,1,0.864466,"hrases for this domain, most likely due to the highly non-literal nature of translations. 5 http://www.opensubtitles.org 107 Models and baselines In all cases, the tuning of the systems was performed individually for each TV series. 4.3 Evaluation In order to objectively evaluate our approach for both translation and compression, we have human translators post-editing the machine translations and collect various information from this process. Meta-information from post-editing has been successfully used in previous work to avoid the subjective nature of explicit scoring schemes (Specia, 2011; Sousa et al., 2011). We use a post-editing tool6 that gathers postediting effort indicators on a per-subtitle basis, including keystrokes, time spent by translators to post-edit the subtitle and the actual post-edited 6 http://pers-www.wlv.ac.uk/˜in1676/pet/ subtitle (Aziz et al., 2012). The tool allows the specification of the length constraints and renders the tasks differently according to how well the translation observes time/space constraints. It uses colors to facilitate the visualization of the compression needs and indicates the number of characters that need to be compressed or remain to be used in the"
2012.eamt-1.33,2011.eamt-1.12,1,0.837329,"ery poor paraphrases for this domain, most likely due to the highly non-literal nature of translations. 5 http://www.opensubtitles.org 107 Models and baselines In all cases, the tuning of the systems was performed individually for each TV series. 4.3 Evaluation In order to objectively evaluate our approach for both translation and compression, we have human translators post-editing the machine translations and collect various information from this process. Meta-information from post-editing has been successfully used in previous work to avoid the subjective nature of explicit scoring schemes (Specia, 2011; Sousa et al., 2011). We use a post-editing tool6 that gathers postediting effort indicators on a per-subtitle basis, including keystrokes, time spent by translators to post-edit the subtitle and the actual post-edited 6 http://pers-www.wlv.ac.uk/˜in1676/pet/ subtitle (Aziz et al., 2012). The tool allows the specification of the length constraints and renders the tasks differently according to how well the translation observes time/space constraints. It uses colors to facilitate the visualization of the compression needs and indicates the number of characters that need to be compressed or rem"
2012.eamt-1.33,W04-1015,0,0.0128661,"Missing"
2012.tc-1.5,aziz-etal-2012-pet,1,0.207823,"Missing"
2012.tc-1.5,2012.eamt-1.33,1,0.674991,"measuring translation quality and diagnosing MT systems. 1 http://www.trados.com/en/ http://www.wordfast.net/ 3 http://translate.google.com/ 4 http://www.systran.co.uk/ 2 1 Figure 1: Annotation window These limitations mostly constrain developers of translation technologies and researchers in machine (or computer-aided) translation. For a detailed study on translation tools that allow post-editing (e.g. Caitra,5 Lingotek,6 D´ej`a Vu X2,7 and OmegaT8 ) and requirements from the human translator’s perspective, we refer the reader to Vieira and Specia (2011). We present PET (Post-Editing Tool) (Aziz et al., 2012a), a simple, freely available opensource standalone tool that allows the PE of any MT system and records various segment-level information. While PET is not yet a full-fledge tool for post-editing, offering limited built-in functionalities (dictionaries, etc.), it offers the flexibility that other tools lack (i) to enable easy design of post-editing tasks with specific requirements (such as constraints on the revisions produced in terms of length, word use, etc.), and (ii) to collect a number of (customisable) effort indicators and statistics on post-editing tasks. 2 Basic functioning PET was"
2012.tc-1.5,2012.amta-wptp.2,1,0.862933,"Missing"
2012.tc-1.5,R11-1014,1,0.843693,"his case, only the post-edited segment and time indicators are logged and the unit was edited only once. Figure 4: Extract of output file 5 Conclusions We have presented a simple tool for post-editing and assessing translations that is MT systemindependent and allows customisation at various levels, including the types of assessments that can be collected and restrictions on the post-editing process. The tool has already been used in different experiments, including (i) comparing different translation systems (Sankaran et al., 2012), (ii) contrasting post-editing and translation from scratch (Sousa et al., 2011), (iii) collecting information to build and compare quality estimation models (Specia, 2011), and (iv) measuring translation quality through post-editing for subtitles, where the tool dynamically restricts the length of each post-edited translation based on the length of the source segment and general time and space constraints for units Aziz et al. (2012b). A more detailed analysis on the use of information collected by the tool for measuring post-editing effort is presented in (Koponen et al., 2012). PET is freely available for download at http://pers-www.wlv.ac.uk/~in1676/pet. Documentation"
2012.tc-1.5,2011.eamt-1.12,1,0.431545,"out the post-editing process that could be used for measuring translation quality and diagnosing MT systems. 1 http://www.trados.com/en/ http://www.wordfast.net/ 3 http://translate.google.com/ 4 http://www.systran.co.uk/ 2 1 Figure 1: Annotation window These limitations mostly constrain developers of translation technologies and researchers in machine (or computer-aided) translation. For a detailed study on translation tools that allow post-editing (e.g. Caitra,5 Lingotek,6 D´ej`a Vu X2,7 and OmegaT8 ) and requirements from the human translator’s perspective, we refer the reader to Vieira and Specia (2011). We present PET (Post-Editing Tool) (Aziz et al., 2012a), a simple, freely available opensource standalone tool that allows the PE of any MT system and records various segment-level information. While PET is not yet a full-fledge tool for post-editing, offering limited built-in functionalities (dictionaries, etc.), it offers the flexibility that other tools lack (i) to enable easy design of post-editing tasks with specific requirements (such as constraints on the revisions produced in terms of length, word use, etc.), and (ii) to collect a number of (customisable) effort indicators and statis"
2012.tc-1.5,P02-1040,0,\N,Missing
2012.tc-1.5,P07-2045,0,\N,Missing
2012.tc-1.5,W10-1703,0,\N,Missing
2014.iwslt-evaluation.11,2012.eamt-1.60,0,0.0264834,"Missing"
2014.iwslt-evaluation.11,W14-3300,0,0.191444,"Missing"
2014.iwslt-evaluation.11,P10-2041,0,0.0751895,"Missing"
2014.iwslt-evaluation.11,2012.iwslt-papers.15,0,0.0392782,"Missing"
2014.iwslt-evaluation.11,P07-2045,0,0.00308277,"source language to form pseudo ASR outputs, which contained no case and punctuation information. Numbers, symbols and acronyms were also converted to their verbal forms with lookup tables. We then used this synthesised corpus of pseudo ASR as the source, and the original corpus as the target of our monolingual MT. The monolingual translation system was trained on 37.6M words (Table 2). It performed monotonic translation with phrases of as long as 7 words. 5. Decoding The evaluation systems for ASR and MT are multi-pass systems with resource optimisation and environment management capabilities [11, 18]. The ASR is a two-stream multipass system. It is illustrated in Figure 1. The two streams ASR1 and ASR2 differ by the acoustic model training data (detailed in Table 1) and also the tandem configurations (detailed in §3). Both streams follow the same routine along the multi-pass decoding system. In pass 1, a unified decoding result was generated using a non-VTLN DNN and GMM-HMM tandem system with cepstral mean and variance (CMVN) normalisation trained on ASR2 data. These 88 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Tab"
2014.iwslt-evaluation.11,N13-1073,0,0.0612584,"Missing"
2014.iwslt-evaluation.11,P13-2121,0,0.0286297,"Missing"
2014.iwslt-evaluation.11,P07-1019,0,0.0553151,"Missing"
2014.iwslt-evaluation.11,P13-4014,1,0.794414,"MT system. System integration experiments were tried in the En→Fr SLT task and the results were submitted as contrastive systems. Figure 2 depicts the integrated system and its comparison with the pipeline system. In the integrated system, ASR system hypotheses are expanded in the form of lattices, confusion networks or N -best lists. A quality estimation (QE) module evaluated and rescored the ASR outputs before they were fed to the MT system. In our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25]. Table 7: Co"
2014.iwslt-evaluation.11,2013.mtsummit-papers.21,1,0.725335,"are expanded in the form of lattices, confusion networks or N -best lists. A quality estimation (QE) module evaluated and rescored the ASR outputs before they were fed to the MT system. In our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25]. Table 7: Contrastive SLT system performance (En→Fr) Setting Contrastive 1 (baseline) Contrastive 2 Tst12 31.33 Tst14 23.18 (+ 10-best list rescoring) 31.51 23.27 31.87 23.44 7. Summary In this paper, the USFD SLT system for IWSLT 2014 was described. Automatic speech r"
2014.iwslt-evaluation.11,W14-3348,0,0.0121121,"MT system. In our implementation, 10-best outputs from the ASR system on the IWSLT 2011 evaluation data were used for QE training. The QE module derived 117 QuEst [21, 22] features from each sentence to describe its linguistic, statistical properties as well as the statistics from the ASR and MT models. Out of the 117 features, top 58 features were selected using the Gaussian Process (GP) with RBF kernel as described in [23]. Further, GP was used to learn the relationship between the selected features and the translation performance of the sentence (in this case, sentence-based METEOR score) [24]. During testing, the estimated translation performance was used to rescore the 10-best ASR output. Details of the integrated system were described in [25]. Table 7: Contrastive SLT system performance (En→Fr) Setting Contrastive 1 (baseline) Contrastive 2 Tst12 31.33 Tst14 23.18 (+ 10-best list rescoring) 31.51 23.27 31.87 23.44 7. Summary In this paper, the USFD SLT system for IWSLT 2014 was described. Automatic speech recognition (ASR) is achieved by two multi-pass deep neural network systems with slightly different tandem configurations and different training data. Machine translation (MT)"
2014.iwslt-evaluation.11,N12-1047,0,0.0726943,"Missing"
2014.iwslt-evaluation.11,2011.iwslt-papers.7,0,\N,Missing
2014.iwslt-evaluation.11,2013.iwslt-evaluation.3,0,\N,Missing
2020.acl-main.646,N16-1024,0,0.0264078,"(1.05), ε (1), ω (0.01) KL weight β (0.66) β (0.7), λ (31.62) α (−21.7), target MMD (0.0017) target -ELBO (100.8) Table 1: Techniques and their hyperparameters. with our trained approximate posterior as importance distribution (we use S = 1000 samples). We first report on experiments using the English Penn Treebank (PTB; Marcus et al., 1993).6 R NN LM The baseline R NN LM generator is a building block for all of our S EN VAEs, thus we validate its performance as a strong standalone generator. We highlight that it outperforms an external baseline that employs a comparable number of parameters (Dyer et al., 2016) and that this performance boost is mostly due to tying embeddings with the output layer.7 Appendix A.1 presents the complete architecture and a comparison. Bayesian optimisation The techniques we compare are sensitive to one or more hyperparameters (see Table 1), which we tune using Bayesian optimisation (BO) towards minimising estimated NLL of the validation data. For each technique, we ran 25 iterations of BO, each iteration encompassing training a model to full convergence. This was sufficient for the hyperparameters of each technique to converge. See Appendix A.2 for details. On optimisat"
2020.acl-main.646,W19-4315,1,0.753957,"computer vision and natural language processing (NLP). In NLP, VAEs have been developed for word representation (Rios et al., 2018), morphological analysis (Zhou and Neubig, 2017), syntactic and Work done while the first author was at the University of Amsterdam. Code is available at https://github.com/ tom-pelsmaeker/deep-generative-lm Wilker Aziz ILLC University of Amsterdam w.aziz@uva.nl semantic parsing (Corro and Titov, 2018; Lyu and Titov, 2018), document modelling (Miao et al., 2016), summarisation (Miao and Blunsom, 2016), machine translation (Zhang et al., 2016; Schulz et al., 2018; Eikema and Aziz, 2019), language and vision (Pu et al., 2016; Wang et al., 2017), dialogue modelling (Wen et al., 2017; Serban et al., 2017), speech modelling (Fraccaro et al., 2016), and, of course, language modelling (Bowman et al., 2016; Goyal et al., 2017). One problem remains common to the majority of these models, VAEs often learn to ignore the latent variables. We investigate this problem, dubbed posterior collapse, in the context of language models (LMs). In a deep generative LM (Bowman et al., 2016), sentences are generated conditioned on samples from a continuous latent space, an idea with various practic"
2020.acl-main.646,K16-1002,0,0.116065,"t author was at the University of Amsterdam. Code is available at https://github.com/ tom-pelsmaeker/deep-generative-lm Wilker Aziz ILLC University of Amsterdam w.aziz@uva.nl semantic parsing (Corro and Titov, 2018; Lyu and Titov, 2018), document modelling (Miao et al., 2016), summarisation (Miao and Blunsom, 2016), machine translation (Zhang et al., 2016; Schulz et al., 2018; Eikema and Aziz, 2019), language and vision (Pu et al., 2016; Wang et al., 2017), dialogue modelling (Wen et al., 2017; Serban et al., 2017), speech modelling (Fraccaro et al., 2016), and, of course, language modelling (Bowman et al., 2016; Goyal et al., 2017). One problem remains common to the majority of these models, VAEs often learn to ignore the latent variables. We investigate this problem, dubbed posterior collapse, in the context of language models (LMs). In a deep generative LM (Bowman et al., 2016), sentences are generated conditioned on samples from a continuous latent space, an idea with various practical applications. For example, one can constrain this latent space to promote generalisations that are in line with linguistic knowledge and intuition (Xu and Durrett, 2018). This also allows for greater flexibility in"
2020.acl-main.646,N19-1021,0,0.196217,"e. 7227 the ELBO itself becomes the culprit (Chen et al., 2017; Alemi et al., 2018), as it lacks a preference regarding MI. Posterior collapse has also been ascribed to approximate inference (Kim et al., 2018; Dieng and Paisley, 2019). Beyond the techniques compared and developed in this work, other solutions have been proposed, including modifications to the generator (Semeniuta et al., 2017; Yang et al., 2017; Park et al., 2018; Dieng et al., 2019), side losses based on weak generators (Zhao et al., 2017), factorised likelihoods (Ziegler and Rush, 2019; Ma et al., 2019), cyclical annealing (Liu et al., 2019) and changes to the ELBO (Tolstikhin et al., 2018; Goyal et al., 2017). Exploiting a mismatch in correlation between the prior and the approximate posterior, and thus forcing a lower-bound on the rate, is the principle behind δ-VAEs (Razavi et al., 2019) and hyperspherical VAEs (Xu and Durrett, 2018). The generative model of δ-VAEs has one latent variable per step of the sequence, i.e. z = hz1 , . . . , z|x |i, making it quite different from that of the S EN VAEs considered here. Their mean-field inference model is a product of independent Gaussians, one per step, but they construct a correlat"
2020.acl-main.646,P18-1037,0,0.0165527,"timates (Kingma and Welling, 2014; Rezende et al., 2014), better known as variational auto-encoders (VAEs), have spurred much interest in various domains, including computer vision and natural language processing (NLP). In NLP, VAEs have been developed for word representation (Rios et al., 2018), morphological analysis (Zhou and Neubig, 2017), syntactic and Work done while the first author was at the University of Amsterdam. Code is available at https://github.com/ tom-pelsmaeker/deep-generative-lm Wilker Aziz ILLC University of Amsterdam w.aziz@uva.nl semantic parsing (Corro and Titov, 2018; Lyu and Titov, 2018), document modelling (Miao et al., 2016), summarisation (Miao and Blunsom, 2016), machine translation (Zhang et al., 2016; Schulz et al., 2018; Eikema and Aziz, 2019), language and vision (Pu et al., 2016; Wang et al., 2017), dialogue modelling (Wen et al., 2017; Serban et al., 2017), speech modelling (Fraccaro et al., 2016), and, of course, language modelling (Bowman et al., 2016; Goyal et al., 2017). One problem remains common to the majority of these models, VAEs often learn to ignore the latent variables. We investigate this problem, dubbed posterior collapse, in the context of language mo"
2020.acl-main.646,D19-1437,0,0.040979,"estingourISestimateisclosetoconvergence. 7227 the ELBO itself becomes the culprit (Chen et al., 2017; Alemi et al., 2018), as it lacks a preference regarding MI. Posterior collapse has also been ascribed to approximate inference (Kim et al., 2018; Dieng and Paisley, 2019). Beyond the techniques compared and developed in this work, other solutions have been proposed, including modifications to the generator (Semeniuta et al., 2017; Yang et al., 2017; Park et al., 2018; Dieng et al., 2019), side losses based on weak generators (Zhao et al., 2017), factorised likelihoods (Ziegler and Rush, 2019; Ma et al., 2019), cyclical annealing (Liu et al., 2019) and changes to the ELBO (Tolstikhin et al., 2018; Goyal et al., 2017). Exploiting a mismatch in correlation between the prior and the approximate posterior, and thus forcing a lower-bound on the rate, is the principle behind δ-VAEs (Razavi et al., 2019) and hyperspherical VAEs (Xu and Durrett, 2018). The generative model of δ-VAEs has one latent variable per step of the sequence, i.e. z = hz1 , . . . , z|x |i, making it quite different from that of the S EN VAEs considered here. Their mean-field inference model is a product of independent Gaussians, one"
2020.acl-main.646,J93-2004,0,0.0694281,"respect to x is larger than 0.01. 7224 Technique KL annealing Word dropout (WD) FB and MDR SFB β-VAE InfoVAE LagVAE Hyperparameters Mode −5 R NN LM Vanilla Annealing WD FB SFB MDR β-VAE InfoVAE LagVAE increment γ (2 × 10 ) decrement γ (2 × 10−5 ) target rate r (5) r (6.46), γ (1.05), ε (1), ω (0.01) KL weight β (0.66) β (0.7), λ (31.62) α (−21.7), target MMD (0.0017) target -ELBO (100.8) Table 1: Techniques and their hyperparameters. with our trained approximate posterior as importance distribution (we use S = 1000 samples). We first report on experiments using the English Penn Treebank (PTB; Marcus et al., 1993).6 R NN LM The baseline R NN LM generator is a building block for all of our S EN VAEs, thus we validate its performance as a strong standalone generator. We highlight that it outperforms an external baseline that employs a comparable number of parameters (Dyer et al., 2016) and that this performance boost is mostly due to tying embeddings with the output layer.7 Appendix A.1 presents the complete architecture and a comparison. Bayesian optimisation The techniques we compare are sensitive to one or more hyperparameters (see Table 1), which we tune using Bayesian optimisation (BO) towards minim"
2020.acl-main.646,D16-1031,0,0.0473452,"iational auto-encoders (VAEs), have spurred much interest in various domains, including computer vision and natural language processing (NLP). In NLP, VAEs have been developed for word representation (Rios et al., 2018), morphological analysis (Zhou and Neubig, 2017), syntactic and Work done while the first author was at the University of Amsterdam. Code is available at https://github.com/ tom-pelsmaeker/deep-generative-lm Wilker Aziz ILLC University of Amsterdam w.aziz@uva.nl semantic parsing (Corro and Titov, 2018; Lyu and Titov, 2018), document modelling (Miao et al., 2016), summarisation (Miao and Blunsom, 2016), machine translation (Zhang et al., 2016; Schulz et al., 2018; Eikema and Aziz, 2019), language and vision (Pu et al., 2016; Wang et al., 2017), dialogue modelling (Wen et al., 2017; Serban et al., 2017), speech modelling (Fraccaro et al., 2016), and, of course, language modelling (Bowman et al., 2016; Goyal et al., 2017). One problem remains common to the majority of these models, VAEs often learn to ignore the latent variables. We investigate this problem, dubbed posterior collapse, in the context of language models (LMs). In a deep generative LM (Bowman et al., 2016), sentences are generat"
2020.acl-main.646,N18-1162,0,0.0390873,"Missing"
2020.acl-main.646,N18-1092,1,0.885126,"Missing"
2020.acl-main.646,D17-1066,0,0.114991,"Missing"
2020.acl-main.646,D18-1480,0,0.0536818,"l., 2016), and, of course, language modelling (Bowman et al., 2016; Goyal et al., 2017). One problem remains common to the majority of these models, VAEs often learn to ignore the latent variables. We investigate this problem, dubbed posterior collapse, in the context of language models (LMs). In a deep generative LM (Bowman et al., 2016), sentences are generated conditioned on samples from a continuous latent space, an idea with various practical applications. For example, one can constrain this latent space to promote generalisations that are in line with linguistic knowledge and intuition (Xu and Durrett, 2018). This also allows for greater flexibility in how the model is used, for example, to generate sentences that live—in latent space—in a neighbourhood of a given observation (Bowman et al., 2016). Despite this potential, VAEs that employ strong generators (e.g. recurrent NNs) tend to ignore the latent variable. Figure 1 illustrates this point: neighbourhood in latent space does not correlate to patterns in data space, and the model behaves just like a standard LM. Recently, many techniques have been proposed to address this problem (§3 and §7) and they range from modifications to the objective t"
2020.acl-main.646,D16-1050,0,0.0314226,"ch interest in various domains, including computer vision and natural language processing (NLP). In NLP, VAEs have been developed for word representation (Rios et al., 2018), morphological analysis (Zhou and Neubig, 2017), syntactic and Work done while the first author was at the University of Amsterdam. Code is available at https://github.com/ tom-pelsmaeker/deep-generative-lm Wilker Aziz ILLC University of Amsterdam w.aziz@uva.nl semantic parsing (Corro and Titov, 2018; Lyu and Titov, 2018), document modelling (Miao et al., 2016), summarisation (Miao and Blunsom, 2016), machine translation (Zhang et al., 2016; Schulz et al., 2018; Eikema and Aziz, 2019), language and vision (Pu et al., 2016; Wang et al., 2017), dialogue modelling (Wen et al., 2017; Serban et al., 2017), speech modelling (Fraccaro et al., 2016), and, of course, language modelling (Bowman et al., 2016; Goyal et al., 2017). One problem remains common to the majority of these models, VAEs often learn to ignore the latent variables. We investigate this problem, dubbed posterior collapse, in the context of language models (LMs). In a deep generative LM (Bowman et al., 2016), sentences are generated conditioned on samples from a continuo"
2020.acl-main.646,2006.amta-papers.25,0,0.0998489,"ever, show a further increase in active units (VampPrior) and accuracy gap (both priors). Again, Appendix B contains plots supporting these claims. Generated samples Figure 3 shows samples from a well-trained S EN VAE, where we decode greedily from a prior sample—this way, all variability is due to the generator’s reliance on the latent sample. Recall that a vanilla VAE ignores z and thus greedy generation from a prior sample is essentially deterministic in that case (see Figure 1a). Next to the samples we show the closest training instance, which we measure in terms of an edit distance (TER; Snover et al., 2006).11 This “nearest neighbour” helps us assess whether the generator is producing novel text or simply reproducing something it memorised from training. In Figure 4 we show a homotopy: here we decode greedily from points lying between a posterior sample conditioned on the first sentence and a posterior sample conditioned on the last sentence. In contrast to the vanilla VAE (Figure 1b), neighbourhood in latent space is now used to capture some regularities in data space. These samples add support to the quantitative evidence that our DGMs have been trained not to neglect the latent space. In Appe"
2020.coling-main.398,P16-1231,0,0.0466989,"t comparing partial translations more fairly (Huang et al., 2017; Shu and Nakayama, 2018). This problem has also been studied in parsing literature, where it’s known as imbalanced probability search bias (Stanojevi´c and Steedman, 2020). Label bias. Where a conditional model makes independence assumptions about its inputs (i.e., variables the model does not generate), local normalisation prevents the model from revising its decisions, a problem known as label bias (Bottou, 1991; Lafferty et al., 2001). This is a model specification problem which limits the distributions a model can represent (Andor et al., 2016). While this is the case in incremental parsing (Stern et al., 2017) and simultaneous translation (Gu et al., 2017), where inputs are incrementally available for conditioning, this is not the case in standard NMT (Sountsov and Sarawagi, 2016, Section 5), where inputs are available for conditioning in all generation steps. It is plausible that local normalisation might affect the kind of local optima we find in NMT, but that is orthogonal to label bias. 1 Though fully generative accounts do exist (Shah and Barber, 2018; Eikema and Aziz, 2019). The term MAP decoding was coined in the context of"
2020.coling-main.398,2020.ngt-1.11,0,0.0137921,"ts multiple solutions. 9 Even though one can alter BLEU such that it is defined at the sentence level (for example, by adding a small positive constant to n-gram counts), this “smoothing” in effect biases BLEU’s sufficient statistics. Unbiased statistics are the key to MBR, thus we opt for a metric that is already defined at the sentence level. 4514 deficiency of the model scoring function. We argue this is another piece of evidence for the inadequacy of the mode: by using beam search, they emphasise statistics of high-scoring translations, potentially rare and inadequate ones. Very recently, Borgeaud and Emerson (2020) present a voting-theory perspective on decoding for image captioning and machine translation. Their proposal is closely-related to MBR, but motivated differently. Their decision rule too is guided by beam search, which may emphasise pathologies of highest-likelihood paths, but they also propose and investigate stronger utility functions which lead to improvements w.r.t. length, diversity, and human judgements. The only instance that we are aware of where unbiased samples from an NMT model support a decision rule is the concurrent work by Naskar et al. (2020). The authors make the same observa"
2020.coling-main.398,W14-4012,0,0.180203,"Missing"
2020.coling-main.398,D14-1179,0,0.0157196,"Missing"
2020.coling-main.398,W11-2107,0,0.0375893,"pled at least once in 1, 000 samples. When an empty sequence is sampled it only occurs on average 1.2 ± 0.5 times. Even though it could well be, as the evidence that Stahlberg and Byrne (2019) provide is strong, that often the true mode under our models is the empty sequence, the empty string remains a rather unlikely outcome under the models. 7.3 Sample Quality The number of translations that an NMT model assigns non-negligible mass to can be very large as we have seen in Section 7.1. We now investigate what the average quality of these samples is. For quality assessments, we compute METEOR (Denkowski and Lavie, 2011) using the mteval-v13a tokeniser.7 We translate the test sets using a single ancestral sample per input sentence and repeat the experiment 30 times to report the average in Table 1 (sample). We also report beam search scores (beam). We see that, on average, samples of the model always perform worse than beam search translations. 7 For our analysis, it is convenient to use a metric defined both at the corpus and at the segment level. We use METEOR, rather than BLEU (Papineni et al., 2002), for it outperforms (smoothed) BLEU at the segment-level (Ma et al., 2018). 4512 METEOR METEOR 22.8 21.9 21"
2020.coling-main.398,W19-4315,1,0.848473,"m which limits the distributions a model can represent (Andor et al., 2016). While this is the case in incremental parsing (Stern et al., 2017) and simultaneous translation (Gu et al., 2017), where inputs are incrementally available for conditioning, this is not the case in standard NMT (Sountsov and Sarawagi, 2016, Section 5), where inputs are available for conditioning in all generation steps. It is plausible that local normalisation might affect the kind of local optima we find in NMT, but that is orthogonal to label bias. 1 Though fully generative accounts do exist (Shah and Barber, 2018; Eikema and Aziz, 2019). The term MAP decoding was coined in the context of generative classifiers and their structured counterparts, where the posterior probability p(y|x, θ) ∝ p(y|θ)p(x|y, θ) updates our prior beliefs about y in light of x. This is not the case in NMT, where we do not express a prior over target sentences, and p(y|x, θ) is a direct parameterisation of the likelihood, rather than a posterior probability inferred via Bayes rule. Nonetheless, we stick to the conventions used in the MT literature. 2 4508 4 Biased Statistics and the Inadequacy of the Mode In most NMT research, criticisms of the model a"
2020.coling-main.398,E17-1099,0,0.0245437,"studied in parsing literature, where it’s known as imbalanced probability search bias (Stanojevi´c and Steedman, 2020). Label bias. Where a conditional model makes independence assumptions about its inputs (i.e., variables the model does not generate), local normalisation prevents the model from revising its decisions, a problem known as label bias (Bottou, 1991; Lafferty et al., 2001). This is a model specification problem which limits the distributions a model can represent (Andor et al., 2016). While this is the case in incremental parsing (Stern et al., 2017) and simultaneous translation (Gu et al., 2017), where inputs are incrementally available for conditioning, this is not the case in standard NMT (Sountsov and Sarawagi, 2016, Section 5), where inputs are available for conditioning in all generation steps. It is plausible that local normalisation might affect the kind of local optima we find in NMT, but that is orthogonal to label bias. 1 Though fully generative accounts do exist (Shah and Barber, 2018; Eikema and Aziz, 2019). The term MAP decoding was coined in the context of generative classifiers and their structured counterparts, where the posterior probability p(y|x, θ) ∝ p(y|θ)p(x|y,"
2020.coling-main.398,D19-1632,0,0.0319589,"Missing"
2020.coling-main.398,D17-1227,0,0.0252351,"partial translations are ranked in terms of loglikelihood without regards to (or with crude approximations of) their future score, which may lead to good translations being pruned too early. This corresponds to searching with a non-admissible heuristic (Hart et al., 1968), that is, a heuristic that may underestimate the likelihood of completing a translation. This biased search affects statistics of beam search outputs in unknown ways and may well account for some of the pathologies of Section 2, and has motivated variants of the algorithm aimed at comparing partial translations more fairly (Huang et al., 2017; Shu and Nakayama, 2018). This problem has also been studied in parsing literature, where it’s known as imbalanced probability search bias (Stanojevi´c and Steedman, 2020). Label bias. Where a conditional model makes independence assumptions about its inputs (i.e., variables the model does not generate), local normalisation prevents the model from revising its decisions, a problem known as label bias (Bottou, 1991; Lafferty et al., 2001). This is a model specification problem which limits the distributions a model can represent (Andor et al., 2016). While this is the case in incremental parsi"
2020.coling-main.398,W17-3204,0,0.328202,"entially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation. 1 Introduction Recent findings in neural machine translation (NMT) suggest that modern translation systems have some serious flaws. This is based on observations such as: i) translations produced via beam search typically under-estimate sequence length (Sountsov and Sarawagi, 2016; Koehn and Knowles, 2017), the length bias; ii) translation quality generally deteriorates with better approximate search (Koehn and Knowles, 2017; Murray and Chiang, 2018; Ott et al., 2018; Kumar and Sarawagi, 2019), the beam search curse; iii) the true most likely translation under the model (i.e., the mode of the distribution) is empty in many cases (Stahlberg and Byrne, 2019) and a general negative correlation exists between likelihood and quality beyond a certain likelihood value (Ott et al., 2018), we call this the inadequacy of the mode problem. A number of hypotheses have been formulated to explain these obser"
2020.coling-main.398,N04-1022,0,0.122681,"hat some length and lexical biases are introduced by approximate MAP decoding. We demonstrate that beam search outputs are rare events, particularly so when test data stray from the training domain. The empty string, shown to often be the true mode (Stahlberg and Byrne, 2019), too is an infrequent event. Finally, we show that samples obtained by following the model’s own generative story are of reasonable quality, which suggests we should base decisions on statistics gathered from the distribution holistically. One such decision rule is minimum Bayes risk (MBR) decoding (Goel and Byrne, 2000; Kumar and Byrne, 2004). We show that an approximation to MBR performs rather well, especially so when models are more uncertain. To summarise: we argue that i) MAP decoding is not well-suited as a decision rule for MLE-trained NMT; we also show that ii) pathologies and biases observed in NMT are not necessarily inherent to NMT as a model or its training objective, rather, MAP decoding is at least partially responsible for many of these pathologies and biases; finally, we demonstrate that iii) a straight-forward approximation to a sampling-based decision rule known as minimum Bayes risk decoding gives good results,"
2020.coling-main.398,W18-6450,0,0.0141793,"ts, we compute METEOR (Denkowski and Lavie, 2011) using the mteval-v13a tokeniser.7 We translate the test sets using a single ancestral sample per input sentence and repeat the experiment 30 times to report the average in Table 1 (sample). We also report beam search scores (beam). We see that, on average, samples of the model always perform worse than beam search translations. 7 For our analysis, it is convenient to use a metric defined both at the corpus and at the segment level. We use METEOR, rather than BLEU (Papineni et al., 2002), for it outperforms (smoothed) BLEU at the segment-level (Ma et al., 2018). 4512 METEOR METEOR 22.8 21.9 21.1 20.2 19.3 36.0 34.4 32.8 31.2 29.6 en-de 5 10 15 20 25 30 29.6 28.8 28.0 27.2 26.4 41.4 40.0 38.6 37.2 35.8 de-en 5 10 15 20 25 30 43.7 42.0 40.3 38.7 37.0 37.3 35.7 34.1 32.5 31.0 en-ne 5 10 15 20 25 30 35.8 34.2 32.7 31.1 29.5 20.3 19.4 18.6 17.7 16.8 ne-en 5 10 15 20 25 30 41.9 39.9 37.9 35.9 33.9 37.2 35.4 33.6 31.8 30.0 en-si 5 10 15 20 25 30 36.6 34.6 32.6 30.6 28.6 21.8 20.8 19.9 18.9 18.0 si-en 5 10 15 20 25 30 Figure 3: METEOR scores for oracle-selected samples as a function of sample size on the held-out data (top) and newstest2018 / F LORES (botto"
2020.coling-main.398,W18-6322,0,0.577286,"hat an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation. 1 Introduction Recent findings in neural machine translation (NMT) suggest that modern translation systems have some serious flaws. This is based on observations such as: i) translations produced via beam search typically under-estimate sequence length (Sountsov and Sarawagi, 2016; Koehn and Knowles, 2017), the length bias; ii) translation quality generally deteriorates with better approximate search (Koehn and Knowles, 2017; Murray and Chiang, 2018; Ott et al., 2018; Kumar and Sarawagi, 2019), the beam search curse; iii) the true most likely translation under the model (i.e., the mode of the distribution) is empty in many cases (Stahlberg and Byrne, 2019) and a general negative correlation exists between likelihood and quality beyond a certain likelihood value (Ott et al., 2018), we call this the inadequacy of the mode problem. A number of hypotheses have been formulated to explain these observations. They mostly suggest there is something fundamentally wrong with NMT as a model (i.e., its factorisation as a product of locally normalise"
2020.coling-main.398,P02-1040,0,0.10857,"w investigate what the average quality of these samples is. For quality assessments, we compute METEOR (Denkowski and Lavie, 2011) using the mteval-v13a tokeniser.7 We translate the test sets using a single ancestral sample per input sentence and repeat the experiment 30 times to report the average in Table 1 (sample). We also report beam search scores (beam). We see that, on average, samples of the model always perform worse than beam search translations. 7 For our analysis, it is convenient to use a metric defined both at the corpus and at the segment level. We use METEOR, rather than BLEU (Papineni et al., 2002), for it outperforms (smoothed) BLEU at the segment-level (Ma et al., 2018). 4512 METEOR METEOR 22.8 21.9 21.1 20.2 19.3 36.0 34.4 32.8 31.2 29.6 en-de 5 10 15 20 25 30 29.6 28.8 28.0 27.2 26.4 41.4 40.0 38.6 37.2 35.8 de-en 5 10 15 20 25 30 43.7 42.0 40.3 38.7 37.0 37.3 35.7 34.1 32.5 31.0 en-ne 5 10 15 20 25 30 35.8 34.2 32.7 31.1 29.5 20.3 19.4 18.6 17.7 16.8 ne-en 5 10 15 20 25 30 41.9 39.9 37.9 35.9 33.9 37.2 35.4 33.6 31.8 30.0 en-si 5 10 15 20 25 30 36.6 34.6 32.6 30.6 28.6 21.8 20.8 19.9 18.9 18.0 si-en 5 10 15 20 25 30 Figure 3: METEOR scores for oracle-selected samples as a function"
2020.coling-main.398,P16-1159,0,0.0440478,"we argue, underlies many biases in NMT and explains some of the pathologies discussed in Section 2. Exposure bias. MLE parameters are estimated conditioned on observations sampled from the training data. Clearly, those are not available at test time, when we search through the learnt distribution. This mismatch between training and test, known as exposure bias (Ranzato et al., 2016), has been linked to many of the pathologies of NMT and motivated modifications or alternatives to MLE aimed at exposing the model to its own predictions during training (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019). While exposure bias has been a point of critique mostly against MLE, it has only been studied in the context of approximate MAP decoding. The use of MAP decoding and its approximations shifts the distribution of the generated translations away from data statistics (something we provide evidence for in later sections), thereby exacerbating exposure bias. Non-admissible heuristic search bias. In beam search, partial translations are ranked in terms of loglikelihood without regards to (or with crude approximations of) their future score, which may le"
2020.coling-main.398,P18-2054,0,0.025534,"s are ranked in terms of loglikelihood without regards to (or with crude approximations of) their future score, which may lead to good translations being pruned too early. This corresponds to searching with a non-admissible heuristic (Hart et al., 1968), that is, a heuristic that may underestimate the likelihood of completing a translation. This biased search affects statistics of beam search outputs in unknown ways and may well account for some of the pathologies of Section 2, and has motivated variants of the algorithm aimed at comparing partial translations more fairly (Huang et al., 2017; Shu and Nakayama, 2018). This problem has also been studied in parsing literature, where it’s known as imbalanced probability search bias (Stanojevi´c and Steedman, 2020). Label bias. Where a conditional model makes independence assumptions about its inputs (i.e., variables the model does not generate), local normalisation prevents the model from revising its decisions, a problem known as label bias (Bottou, 1991; Lafferty et al., 2001). This is a model specification problem which limits the distributions a model can represent (Andor et al., 2016). While this is the case in incremental parsing (Stern et al., 2017) a"
2020.coling-main.398,D16-1158,0,0.644398,"he mode can be considered essentially arbitrary. We therefore advocate for the use of decision rules that take into account the translation distribution holistically. We show that an approximation to minimum Bayes risk decoding gives competitive results confirming that NMT models do capture important aspects of translation well in expectation. 1 Introduction Recent findings in neural machine translation (NMT) suggest that modern translation systems have some serious flaws. This is based on observations such as: i) translations produced via beam search typically under-estimate sequence length (Sountsov and Sarawagi, 2016; Koehn and Knowles, 2017), the length bias; ii) translation quality generally deteriorates with better approximate search (Koehn and Knowles, 2017; Murray and Chiang, 2018; Ott et al., 2018; Kumar and Sarawagi, 2019), the beam search curse; iii) the true most likely translation under the model (i.e., the mode of the distribution) is empty in many cases (Stahlberg and Byrne, 2019) and a general negative correlation exists between likelihood and quality beyond a certain likelihood value (Ott et al., 2018), we call this the inadequacy of the mode problem. A number of hypotheses have been formula"
2020.coling-main.398,D19-1331,0,0.309203,"al machine translation (NMT) suggest that modern translation systems have some serious flaws. This is based on observations such as: i) translations produced via beam search typically under-estimate sequence length (Sountsov and Sarawagi, 2016; Koehn and Knowles, 2017), the length bias; ii) translation quality generally deteriorates with better approximate search (Koehn and Knowles, 2017; Murray and Chiang, 2018; Ott et al., 2018; Kumar and Sarawagi, 2019), the beam search curse; iii) the true most likely translation under the model (i.e., the mode of the distribution) is empty in many cases (Stahlberg and Byrne, 2019) and a general negative correlation exists between likelihood and quality beyond a certain likelihood value (Ott et al., 2018), we call this the inadequacy of the mode problem. A number of hypotheses have been formulated to explain these observations. They mostly suggest there is something fundamentally wrong with NMT as a model (i.e., its factorisation as a product of locally normalised distributions) or its most popular training algorithm (i.e., regularised maximum likelihood estimation, MLE for short). These explanations make an unspoken assumption, namely, that identifying the mode of the"
2020.coling-main.398,E17-2058,0,0.0546756,"Missing"
2020.coling-main.398,2020.acl-main.378,0,0.050307,"Missing"
2020.coling-main.398,D17-1178,0,0.0303871,"u and Nakayama, 2018). This problem has also been studied in parsing literature, where it’s known as imbalanced probability search bias (Stanojevi´c and Steedman, 2020). Label bias. Where a conditional model makes independence assumptions about its inputs (i.e., variables the model does not generate), local normalisation prevents the model from revising its decisions, a problem known as label bias (Bottou, 1991; Lafferty et al., 2001). This is a model specification problem which limits the distributions a model can represent (Andor et al., 2016). While this is the case in incremental parsing (Stern et al., 2017) and simultaneous translation (Gu et al., 2017), where inputs are incrementally available for conditioning, this is not the case in standard NMT (Sountsov and Sarawagi, 2016, Section 5), where inputs are available for conditioning in all generation steps. It is plausible that local normalisation might affect the kind of local optima we find in NMT, but that is orthogonal to label bias. 1 Though fully generative accounts do exist (Shah and Barber, 2018; Eikema and Aziz, 2019). The term MAP decoding was coined in the context of generative classifiers and their structured counterparts, where the"
2020.coling-main.398,D08-1065,0,0.13864,"Missing"
2020.coling-main.398,D16-1137,0,0.0208269,"s many biases in NMT and explains some of the pathologies discussed in Section 2. Exposure bias. MLE parameters are estimated conditioned on observations sampled from the training data. Clearly, those are not available at test time, when we search through the learnt distribution. This mismatch between training and test, known as exposure bias (Ranzato et al., 2016), has been linked to many of the pathologies of NMT and motivated modifications or alternatives to MLE aimed at exposing the model to its own predictions during training (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019). While exposure bias has been a point of critique mostly against MLE, it has only been studied in the context of approximate MAP decoding. The use of MAP decoding and its approximations shifts the distribution of the generated translations away from data statistics (something we provide evidence for in later sections), thereby exacerbating exposure bias. Non-admissible heuristic search bias. In beam search, partial translations are ranked in terms of loglikelihood without regards to (or with crude approximations of) their future score, which may lead to good translations"
2020.coling-main.398,D18-1342,0,0.18495,"t and incorrect sequences due to local normalisation. Later studies have also confirmed the existence of this bias in NMT (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019; Kumar and Sarawagi, 2019). Notably, all these studies employ beam search decoding. In fact, some studies link the length bias to the beam search curse: the observation that large beam sizes hurt performance in NMT (Koehn and Knowles, 2017). Sountsov and Sarawagi (2016) already note that larger beam sizes exacerbate the length bias. Later studies have confirmed this connection (Blain et al., 2017; Murray and Chiang, 2018; Yang et al., 2018; Kumar and Sarawagi, 2019). Murray and Chiang (2018) attribute both problems to local normalisation which they claim introduces label bias (Lafferty et al., 2001) to NMT. Yang et al. (2018) show that likelihood negatively correlates with translation length. These findings suggest that the mode might suffer from length bias, likely thereby failing to sufficiently account for adequacy. In fact, Stahlberg and Byrne (2019) show that oftentimes the true mode is the empty sequence. The connection with the length bias is not the only reason for the beam search curse. Ott et al. (2018) find that the"
2020.coling-main.398,P19-1426,0,0.0479818,"Missing"
2020.coling-main.398,P09-1064,0,\N,Missing
2020.coling-main.398,L18-1275,0,\N,Missing
2020.coling-main.398,tiedemann-2012-parallel,0,\N,Missing
2020.emnlp-main.262,P19-1284,1,0.901642,"Missing"
2020.emnlp-main.262,Q19-1004,0,0.159133,"in et al., 2019). Introduction Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learning; see Belinkov and Glass (2019) and Jacovi and Goldberg (2020) for reviews. In this work, we study post hoc interpretability where the goal is to explain the prediction of a trained model and to reveal how the model arrives at the decision. This goal is usually approached with attribution methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017), which explain the behavior of a model by assigning relevance to inputs. One way to perform attribution is to use erasure where a subset of features (e.g., input tokens) is considered irrelevant if it can be removed without affecting the model prediction (Li et"
2020.emnlp-main.262,D14-1179,0,0.0240881,"Missing"
2020.emnlp-main.262,N19-1240,1,0.898731,"Missing"
2020.emnlp-main.262,N19-1423,0,0.20494,") and sometimes evaluated using erasure as ground-truth (Serrano and Smith, 2019; Jain and Wallace, 2019). Despite its conceptual simplicity, subset erasure is not commonly used in practice. First, it is generally intractable, and beam search (Feng et al., 2018) or leave-one-out estimates (Zintgraf et al., 2017) are typically used instead. These approximations may be inaccurate. For example, leaveone-out can underestimate the contribution of features due to saturation (Shrikumar et al., 2017). More importantly, even these approximations remain very expensive with modern deep (e.g., BERTbased; Devlin et al., 2019) models, as they require multiple computation passes through the model. Second, the method is susceptible to the hindsight bias: the fact that a feature can be dropped does not mean that the model ‘knows’ that it can be dropped and that the feature is not used by the model when processing the example. This results in over-aggressive pruning that does not reflect what information the model uses to arrive at the decision. The issue is pronounced in NLP tasks (see Figure 2d and Feng et al., 2018), though it is easier to see on an artificial example (Figure 3a). A model is asked to predict if ther"
2020.emnlp-main.262,D18-1407,0,0.0381253,"Missing"
2020.emnlp-main.262,N18-2017,0,0.0644358,"Missing"
2020.emnlp-main.262,D19-1424,0,0.0258942,"n but use the Gumbel softmax trick (Maddison et al., 2017; Jang et al., 2017) to approximate minimal subset selection. They assume that the subset contains exactly k elements where k is a hyperparameter. Moreover, their explainer is a separate model predicting input subsets, rather than a ‘probe’ on top of the model’s hidden layers, and hence cannot be used to reveal how decisions are formed across layers. A large body of literature analyzed BERT and Transformed-based models. For example, Tenney et al. (2019) and van Aken et al. (2019) probed BERT layers for a range of linguistic tasks, while Hao et al. (2019) analyzed the optimization surface. Rogers et al. (2020) provides a comprehensive overview of recent BERT analysis papers. There is a stream of work on learning interpretable models by means of extracting latent rationales (Lei et al., 2016; Bastings et al., 2019). Some of the techniques underlying D IFF M ASK are related to that line of work. They employ stochastic masks to learn an interpretable model, which they train by minimizing a downstream loss subject to constraints on L0 , whereas we employ stochastic masks to interpret an existing model, and for that, we minimize L0 subject to const"
2020.emnlp-main.262,2020.acl-main.386,0,0.0691378,"Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learning; see Belinkov and Glass (2019) and Jacovi and Goldberg (2020) for reviews. In this work, we study post hoc interpretability where the goal is to explain the prediction of a trained model and to reveal how the model arrives at the decision. This goal is usually approached with attribution methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017), which explain the behavior of a model by assigning relevance to inputs. One way to perform attribution is to use erasure where a subset of features (e.g., input tokens) is considered irrelevant if it can be removed without affecting the model prediction (Li et al., 2016; Feng et al., 2018)."
2020.emnlp-main.262,N19-1357,0,0.0199656,"mostly to the answer span itself (underlined). Our method (d) reveals that the model pays attention to other named entities and the predicate ‘practice’ in both sentences. Predictions of the path-based methods (a) are more spread-out. Exact search (e) as well as approximate search (f) leads to pathological attributions. bution (Rockt¨aschel et al., 2016; Serrano and Smith, 2019; Vashishth et al., 2019) or back-propagation methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017). These approaches received much scrutiny in recent years (Nie et al., 2018; Sixt et al., 2020; Jain and Wallace, 2019), as they cannot guarantee that the network is ignoring low-scored features. They are often motivated as approximations of erasure (Baehrens et al., 2010; Simonyan et al., 2014; Feng et al., 2018) and sometimes evaluated using erasure as ground-truth (Serrano and Smith, 2019; Jain and Wallace, 2019). Despite its conceptual simplicity, subset erasure is not commonly used in practice. First, it is generally intractable, and beam search (Feng et al., 2018) or leave-one-out estimates (Zintgraf et al., 2017) are typically used instead. These approximations may be inaccurate. For example, leaveone-o"
2020.emnlp-main.262,D18-1546,0,0.0525125,"Missing"
2020.emnlp-main.262,D19-1445,0,0.0416496,"Missing"
2020.emnlp-main.262,D16-1011,0,0.0708883,"Missing"
2020.emnlp-main.262,D17-1159,1,0.837947,"lying D IFF M ASK are related to that line of work. They employ stochastic masks to learn an interpretable model, which they train by minimizing a downstream loss subject to constraints on L0 , whereas we employ stochastic masks to interpret an existing model, and for that, we minimize L0 subject to constraints on that model’s output distribution. In our very recent work Schlichtkrull et al. (2020), we also employ stochastic masks and L0 regularization for analyzing graph neural networks. We learn which edges are relevant in multi-hop question answering and graph-based semantic role labeling (Marcheggiani and Titov, 2017; De Cao et al., 2019). 5 Conclusion We have introduced a new post hoc interpretation method which learns to completely remove subsets of inputs or hidden states through masking. We circumvent an intractable search by learning an end-to-end differentiable prediction model. To overcome the hindsight bias problem, we probe the model’s hidden states at different depths and amortize predictions over the training set. Faithfulness is validated in a controlled experiment pointing more clearly to some flaws of other attribution methods. We used our method to study BERT-based models on sentiment class"
2020.emnlp-main.262,D16-1264,0,0.0188232,"ectively) receive more attention than the rest.6 Attributions by Schulz et al. (2020) and Guan et al. (2019) assign slightly higher importance to hidden states corresponding to ‘highly’ and ‘enjoyable’, whereas it is hard to see any informative patterns provided by integrated gradient. Notice that for D IFF M ASK, a near-zero attribution has a very clear interpretation: such a state is not used for prediction since in expectation it is dropped (not gated). 3.3 Question Answering We turn now to QA where we analyse a fine-tuned BERTLARGE model on the Stanford Question Answering Dataset (SQ UAD; Rajpurkar et al., 2016). Analysis We start by asking D IFF M ASK which tokens does the model keep? We do a similar analysis as for sentiment classification of POS tags over the entire validation set. We summarize the 6 Voita et al. (2019b) and Michel et al. (2019) pointed out that many Transformer heads play no or minor role. [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (b) Sundararajan et al. (2017). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (d) Guan et al. (2019). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (f) D IFF M ASK on hidden. Figure 6: Sentime"
2020.emnlp-main.262,N16-3020,0,0.12846,"o mask the input and re-compute the forward pass (bottom). The classifier g is trained to mask the input as much as possible without changing the output (minimizing a divergence D? ). Lipton, 2018) or verify that a model is fair and does not exhibit harmful biases (Sun et al., 2019; Holstein et al., 2019). Introduction Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learning; see Belinkov and Glass (2019) and Jacovi and Goldberg (2020) for reviews. In this work, we study post hoc interpretability where the goal is to explain the prediction of a trained model and to reveal how the model arrives at the decision. This goal is usually approached with attribution methods (Bach et al., 2015; Shrikumar et al., 2"
2020.emnlp-main.262,2020.tacl-1.54,0,0.0159067,"2017; Jang et al., 2017) to approximate minimal subset selection. They assume that the subset contains exactly k elements where k is a hyperparameter. Moreover, their explainer is a separate model predicting input subsets, rather than a ‘probe’ on top of the model’s hidden layers, and hence cannot be used to reveal how decisions are formed across layers. A large body of literature analyzed BERT and Transformed-based models. For example, Tenney et al. (2019) and van Aken et al. (2019) probed BERT layers for a range of linguistic tasks, while Hao et al. (2019) analyzed the optimization surface. Rogers et al. (2020) provides a comprehensive overview of recent BERT analysis papers. There is a stream of work on learning interpretable models by means of extracting latent rationales (Lei et al., 2016; Bastings et al., 2019). Some of the techniques underlying D IFF M ASK are related to that line of work. They employ stochastic masks to learn an interpretable model, which they train by minimizing a downstream loss subject to constraints on L0 , whereas we employ stochastic masks to interpret an existing model, and for that, we minimize L0 subject to constraints on that model’s output distribution. In our very"
2020.emnlp-main.262,P19-1282,0,0.0226684,"ford University and stayed at the Santa Clara Marriott . (e) Our D IFF M ASK. (f) Our D IFF M ASK non-amortized. Figure 2: Question Answering token attribution: (b) and (c), are misleading (i.e., not faithful) as they attribute the prediction mostly to the answer span itself (underlined). Our method (d) reveals that the model pays attention to other named entities and the predicate ‘practice’ in both sentences. Predictions of the path-based methods (a) are more spread-out. Exact search (e) as well as approximate search (f) leads to pathological attributions. bution (Rockt¨aschel et al., 2016; Serrano and Smith, 2019; Vashishth et al., 2019) or back-propagation methods (Bach et al., 2015; Shrikumar et al., 2017; Sundararajan et al., 2017). These approaches received much scrutiny in recent years (Nie et al., 2018; Sixt et al., 2020; Jain and Wallace, 2019), as they cannot guarantee that the network is ignoring low-scored features. They are often motivated as approximations of erasure (Baehrens et al., 2010; Simonyan et al., 2014; Feng et al., 2018) and sometimes evaluated using erasure as ground-truth (Serrano and Smith, 2019; Jain and Wallace, 2019). Despite its conceptual simplicity, subset erasure is no"
2020.emnlp-main.262,D13-1170,0,0.00302756,"thods do not allow for this type of inspection. These observations are consistent across the entire test set. For attribution to hidden states (i.e., the output of the feed-forward layer) we can compare methods in terms of how much their attributions resemble the ground-truth across the test set. Table 1 shows how the different approaches deviate from the gold-truth in terms of Kullback-Leibler (DKL ) and Jensen–Shannon (DJS ) divergences.5 3.2 Sentiment Classification We turn now to a real task and analyze models finetuned for sentiment classification on the Stanford Sentiment Treebank (SST; Socher et al., 2013). Erasure search as learning masks Before diving into an analysis of a BERT sentiment model, we would like to demonstrate that we can approximate the result of erasure well through our differentiable relaxations. For that, we train a singlelayer GRU sentiment classifier and compare the analyses by D IFF M ASK to solutions provided by 4 To enable comparison across methods, the attributions in this Section are normalized between 0 and 1. 5 We use DKL [pkq] and DJS [pkq] where p is the groundtruth distribution and q is the predicted attribution distribution. 3247 Metric Precision Recall F1 Optima"
2020.emnlp-main.262,P19-1159,0,0.0298372,"tmaps but also analyze how decisions are formed across network layers. We use D IFF M ASK to study BERT models on sentiment classification and question answering.1 1 Model with gated input Gated input Figure 1: D IFF M ASK: hidden states up to layer ` from a model (top) are fed to a classifier g that predicts a mask z. We use this to mask the input and re-compute the forward pass (bottom). The classifier g is trained to mask the input as much as possible without changing the output (minimizing a divergence D? ). Lipton, 2018) or verify that a model is fair and does not exhibit harmful biases (Sun et al., 2019; Holstein et al., 2019). Introduction Deep neural networks have become standard tools in NLP demonstrating impressive improvements over traditional approaches on many tasks (Goldberg, 2017). Their power typically comes at the expense of interpretability, which may prevent users from trusting predictions (Kim, 2015; Ribeiro et al., 2016), makes it hard to detect model or data deficiencies (Gururangan et al., 2018; Kaushik and 1 Source code available at https://github.com/ nicola-decao/diffmask These challenges have motivated work on interpretability, both in NLP and generally in machine learni"
2020.emnlp-main.262,D19-1448,1,0.785224,"rd to see any informative patterns provided by integrated gradient. Notice that for D IFF M ASK, a near-zero attribution has a very clear interpretation: such a state is not used for prediction since in expectation it is dropped (not gated). 3.3 Question Answering We turn now to QA where we analyse a fine-tuned BERTLARGE model on the Stanford Question Answering Dataset (SQ UAD; Rajpurkar et al., 2016). Analysis We start by asking D IFF M ASK which tokens does the model keep? We do a similar analysis as for sentiment classification of POS tags over the entire validation set. We summarize the 6 Voita et al. (2019b) and Michel et al. (2019) pointed out that many Transformer heads play no or minor role. [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (b) Sundararajan et al. (2017). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (d) Guan et al. (2019). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (f) D IFF M ASK on hidden. Figure 6: Sentiment classification: comparison between attribution method for hidden layers w.r.t. the predicted label. All plots are normalized per-layer by the largest attribution. Attention heatmap is obtained max pooling over h"
2020.emnlp-main.262,P19-1580,1,0.761029,"rd to see any informative patterns provided by integrated gradient. Notice that for D IFF M ASK, a near-zero attribution has a very clear interpretation: such a state is not used for prediction since in expectation it is dropped (not gated). 3.3 Question Answering We turn now to QA where we analyse a fine-tuned BERTLARGE model on the Stanford Question Answering Dataset (SQ UAD; Rajpurkar et al., 2016). Analysis We start by asking D IFF M ASK which tokens does the model keep? We do a similar analysis as for sentiment classification of POS tags over the entire validation set. We summarize the 6 Voita et al. (2019b) and Michel et al. (2019) pointed out that many Transformer heads play no or minor role. [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (b) Sundararajan et al. (2017). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (d) Guan et al. (2019). [CLS] A fast , funny , highly enjoyable movie . [SEP] E 3 6 9 12 (f) D IFF M ASK on hidden. Figure 6: Sentiment classification: comparison between attribution method for hidden layers w.r.t. the predicted label. All plots are normalized per-layer by the largest attribution. Attention heatmap is obtained max pooling over h"
2021.emnlp-main.522,2020.emnlp-main.262,1,0.874608,"Missing"
2021.emnlp-main.522,N19-1423,0,0.051768,"Missing"
2021.emnlp-main.522,2020.emnlp-main.400,0,0.0149476,"ilar to Elastic Weight Consolidation (Kirkpatrick et al., 2017), a technique developed for preventing catastrophic forgetting in neural network models. reach the prediction quality of alternatives that retrieve and use context. Approaches that incentivize memorization of factual knowledge show to be beneficial for many downstream tasks suggesting that research on methods that effectively edit the memory of a model is indeed important (Zhang et al., 2019; Sun et al., 2019, 2020). Some recent hybrid approaches that use both implicit and explicit memory show some benefits for question answering (Févry et al., 2020; Verga et al., 2020). Notably, language models that only rely on internal implicit memory are state-of-the-art for (multilingual-) Entity Linking (De Cao et al., 2021a,b). An effective mechanism for editing LM’s implicit memory may be applicable in all these settings. Causal Interventions Identification of minimal changes to neural networks needed to achieve a certain behaviour has been studied in the context of research in interpreting neural networks (Lakretz et al., 2019; Vig et al., 2020; Elazar et al., 2021; Csordás et al., 2021). The components which need to be updated can be interprete"
2021.emnlp-main.522,2020.tacl-1.28,0,0.0368313,"et al., 2017)—i.e., a neural network that predicts the parameters of another network. Since the task requires every other prediction to stay the same—except the one we desire to change—we cast the learning task as a constrained optimization problem. Knowledge in Language Models Petroni et al. (2019) show that pre-trained language models recall factual knowledge without fine-tuning, which they do by feeding specific prompts to LMs. Handcrafted prompts have been found not to be the best option to extract knowledge from LMs, and various solutions have been proposed to understand what LMs ‘know’ (Jiang et al., 2020; Shin et al., 2020; Liu et al., 2021). Additionally, Roberts et al. (2020) show that large models can be fine-tuned to access their internal memories to answer questions in natural language without any additional context and with surprisingly high accuracy—a setting they Optimization For an input x, changing the prereferred to as closed-book question answering. Al- diction of a model f (·; θ) to a corresponds to minthough performing quite well, these models cannot imizing the loss L(θ; x, a) incurred when a is the 6494 target. Preserving the rest of the knowledge corresponds to constraining t"
2021.emnlp-main.522,P17-1147,0,0.0305987,"Missing"
2021.emnlp-main.522,P18-4020,0,0.0239798,"Missing"
2021.emnlp-main.522,Q19-1026,0,0.0230255,"Missing"
2021.emnlp-main.522,N19-1002,0,0.0210426,"2020). Some recent hybrid approaches that use both implicit and explicit memory show some benefits for question answering (Févry et al., 2020; Verga et al., 2020). Notably, language models that only rely on internal implicit memory are state-of-the-art for (multilingual-) Entity Linking (De Cao et al., 2021a,b). An effective mechanism for editing LM’s implicit memory may be applicable in all these settings. Causal Interventions Identification of minimal changes to neural networks needed to achieve a certain behaviour has been studied in the context of research in interpreting neural networks (Lakretz et al., 2019; Vig et al., 2020; Elazar et al., 2021; Csordás et al., 2021). The components which need to be updated can be interpreted as controlling or encoding the corresponding phenomena (e.g., subject-verb agreement). Much of this research focused on modifying neuron activations rather than weights and on sparse interventions (e.g., modifying one or a handful of neurons). While far from our goals, there are interesting connections with our work. For example, our analysis of updates in Section 6.4, though very limited, may shed some light on how factual knowledge is encoded in the parameters of a model"
2021.emnlp-main.522,2020.fever-1.5,0,0.0782322,"Missing"
2021.emnlp-main.522,K17-1034,0,0.0182328,"e report these baselines fine-tuning all parameters or just a subset of them. We limit the search to selecting entire layers and base our decision on performance on a subset of the validation set. Note that selecting a subset of parameters for update requires an extensive search, which K NOWLEDGE E DITOR dispenses with by automatically learning it. 5.2 Models and data on a task with a more complex output space: closedbook question answering (QA). For that we finetune a BART base model (Lewis et al., 2020) with a standard seq2seq objective on the Zero-Shot Relation Extraction (zsRE) dataset by Levy et al. (2017). We evaluate on this dataset because it is annotated with human-generated question paraphrases that we can use to measure our model’s robustness to semantically equivalent inputs. We create alternative predictions for FC simply flipping the labels, whereas for QA we pick all hypotheses enumerated via beam search except the top-1. The latter ensures high-probability outcomes under the model distribution. We generate semantically equivalent inputs with back-translation. See Appendix B for technical details on models and data collection. 6 Results Table 1 reports the main results for fact-checki"
2021.emnlp-main.522,2020.acl-main.703,0,0.128836,"tual knowledge, neural models implicitly memorize facts in their parameters. One cannot easily access and interpret their computation and memories (Ribeiro et al., 2016; Belinkov and Glass, 2019; Voita et al., 2019; De Cao et al., 2020), thus, modifying their knowledge is a challenging problem. Motivated by practical con1 Introduction siderations, we formulate the following desiderata Using pre-trained transformer-based Language for a method aimed at tackling this problem (see Models (LMs; Vaswani et al., 2017; Devlin et al., Section 2 for a more formal treatment): 2019; Radford et al., 2019; Lewis et al., 2020; Raf• Generality: be able to modify a model that fel et al., 2020; Brown et al., 2020) has recently was not specifically trained to be editable (i.e., 1 no need for special pre-training of LMs, such Source code available at https://github.com/ nicola-decao/KnowledgeEditor as using meta-learning); 6491 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6491–6506 c November 7–11, 2021. 2021 Association for Computational Linguistics Semantically equivalent What is the capital of Namibia? Answers Scores Namibia -0.43 Nigeria -0.69 Nibia -0.89 Namibia -1."
2021.emnlp-main.522,D19-1448,1,0.833456,"ver time (e.g., not reflecting changes of heads of states or country populations). Developing reliable and computationally efficient methods for bug-fixing models without the need for expensive re-training would be beneficial. See Figure 2 for an example of revising the memory of a model that initially misremembered Namibia’s capital. Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowledge, neural models implicitly memorize facts in their parameters. One cannot easily access and interpret their computation and memories (Ribeiro et al., 2016; Belinkov and Glass, 2019; Voita et al., 2019; De Cao et al., 2020), thus, modifying their knowledge is a challenging problem. Motivated by practical con1 Introduction siderations, we formulate the following desiderata Using pre-trained transformer-based Language for a method aimed at tackling this problem (see Models (LMs; Vaswani et al., 2017; Devlin et al., Section 2 for a more formal treatment): 2019; Radford et al., 2019; Lewis et al., 2020; Raf• Generality: be able to modify a model that fel et al., 2020; Brown et al., 2020) has recently was not specifically trained to be editable (i.e., 1 no need for special pre-training of LMs, s"
2021.emnlp-main.522,P18-1042,0,0.0415729,"Missing"
2021.emnlp-main.604,2020.emnlp-main.630,0,0.0241356,"of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framing EL as matching in vector space, and instead frames it as a sequence-to-sequence problem. GENRE tackles MD and ED for all mention-entity pairs jointly by autoregressively generating a version of the input markup-annotated with the entities’ unique identifiers expressed in natural language. Although we focus on EL, GENRE was also applied to ED alone as well as to page-level document retrieval for fact-checking, open-domain question Related work EL is typically decomposed in ans"
2021.emnlp-main.604,C10-1032,0,0.0424516,"ctive entities. This necestimes faster and more accurate than the presitates using an autoregressive decoder, precluding vious generative method, outperforming stateparallelism across mentions. Generation also has a of-the-art approaches on the standard English 1 high computational cost due to relying on a comdataset AIDA-CoNLL. plex and deep Transformer (Vaswani et al., 2017) decoder. Transformers are state-less and their mem1 Introduction ory footprint scales with sequence length, making Entity Linking (EL; Bunescu and Pa¸sca, 2006; them memory-consuming when generating long Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL"
2021.emnlp-main.604,2020.emnlp-main.400,0,0.0175661,"relying on a comdataset AIDA-CoNLL. plex and deep Transformer (Vaswani et al., 2017) decoder. Transformers are state-less and their mem1 Introduction ory footprint scales with sequence length, making Entity Linking (EL; Bunescu and Pa¸sca, 2006; them memory-consuming when generating long Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL and generate mention-entity pairs conditionformation extraction (Sarawagi, 2008; Martinez- ally independently given the input. This allows Rodriguez et al., 2020), to name a few. Popular ear- for parallelism across mentions, which we exploit lier methods address the Mention Detection (MD) by"
2021.emnlp-main.604,D17-1277,0,0.0487857,"Missing"
2021.emnlp-main.604,D11-1072,0,0.0619097,"hastic gradient descent (SGD; Robbins and Monro, 1951; Kiefer and Wolfowitz, 1952; Bottou, 2012). For the language model component, we employ length normalization (Sutskever et al., 2011, 2014) and label smoothing (Szegedy et al., 2016). All components are further regularized with dropout (Srivastava et al., 2014). The classification loss is the negative logarithm of Equation 4, and we approximate the normalization constant via negative sampling, with samples drawn from a candidate set specific to each training instance. 4 4.1 Experiments Setting We use the standard English AIDA-CoNLL splits (Hoffart et al., 2011) for training, validation (i.e., for doing model selection), and test. See Table 1 for statistics of this dataset. AIDA provides full supervision for both MD and ED. We only link mentions that have a valid gold KB entity, a setting 3 We limit the maximum number of tokens per span to referred to as InKB evaluation (Röder et al., 2018). 15 to avoid memory overhead (in the training set there is no mention with more than 12 tokens). This is in line with many previous models (Luo 7664 Split Documents Mentions 942 216 230 18,540 4,791 4,485 Training Validation Test Table 1: Statistics of the AIDA-Co"
2021.emnlp-main.604,P11-1055,0,0.0605715,"are Schmidhuber, 1997) as an encoder and then local full cross-encoders of context and entity since deand global scoring functions to link mentions. They coders can use attention to context. Bi-encoders exploit pre-computed entity embeddings by Ganea solutions may be sub-optimal and memory ineffiand Hofmann (2017) and match the embeddings cient although memory-efficient dense retrieval has 2 https://www.wikidata.org/wiki/Q60 recently received attention (Izacard et al., 2020; 7663 Task Entity Linking (EL) is the task of predicting a set Y of mention-entity pairs contained in some input text x (Hoffmann et al., 2011). Each mention m is a pair of start and end positions hms , me i indicating a span in x. Each mention m refers to an entity e in a fixed Knowledge Base (KB)—note that entities can be referred to with multiple ambiguous surface forms (e.g. in Wikidata “NYC"" and “New York"" both refers to the entity “New York City""2 ). Min et al., 2021; Lewis et al., 2021). A caveat of joint modeling all mention-entity pairs with an autoregressive model (i.e., without any independence assumptions) is the lack of parallelism, which makes GENRE extremely slow for the complete task of EL. In addition, generation of"
2021.emnlp-main.604,2021.eacl-main.40,0,0.0567059,"Missing"
2021.emnlp-main.604,K18-1050,0,0.0601994,"t et al., 2011) dataset. et al., 2015; Ganea and Hofmann, 2017; Yamada et al., 2016) and all systems we compare to. As in several previous approaches, for linking we assume the availability of a pre-computed set of candidates instead of considering the whole KB. For that, we use the candidates by Pershina et al. (2015). We also use these candidates to provide negative samples for the discriminative loss during training (see Equation 4). 4.2 Architecture details Method Micro-F1 Hoffart et al. (2011) Steinmetz and Sack (2013) Daiber et al. (2013) Moro et al. (2014) Piccinno and Ferragina (2014) Kolitsas et al. (2018) Peters et al. (2019) Broscheit (2019) Martins et al. (2019) van Hulst et al. (2020)† Févry et al. (2020b) De Cao et al. (2021a) Kannan Ravi et al. (2021) 72.8 42.3 57.8 48.5 73.0 82.4 73.7 79.3 81.9 80.5 76.7 83.7 83.1 Ours 85.5 Ablations (ours) LM score only Classifier score only 81.5 81.7 As the document encoder, we use a LongBeam Search w/ candidates 84.9 former (Beltagy et al., 2020). A Longformer is Beam Search w/o candidates 49.4* a RoBERTa (Liu et al., 2019) model with a limited attention window (we use 128 tokens). It Table 2: Results (InKB) on the AIDA test set and some has 12 layers"
2021.emnlp-main.604,P18-1148,1,0.848643,"sive decoder, precluding vious generative method, outperforming stateparallelism across mentions. Generation also has a of-the-art approaches on the standard English 1 high computational cost due to relying on a comdataset AIDA-CoNLL. plex and deep Transformer (Vaswani et al., 2017) decoder. Transformers are state-less and their mem1 Introduction ory footprint scales with sequence length, making Entity Linking (EL; Bunescu and Pa¸sca, 2006; them memory-consuming when generating long Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL and generate mention-entity pairs conditionformation extraction (Sarawagi, 2008; Martine"
2021.emnlp-main.604,2020.emnlp-main.522,0,0.0120542,"oregressive entity linking that retains the advantages of being generative while being &gt;70 times faster than a previous generative formulation and as fast as non-generative models. We optimize for the correctness of the decoder’s ranking with a discriminative loss to improve autoregressive EL further. The model outperforms state-of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framing EL as matching in vector space, and instead frames it as a sequence-to-sequence problem. GENRE tackles MD and ED for all mention-entity pairs join"
2021.emnlp-main.604,2021.ccl-1.108,0,0.0540938,"Missing"
2021.emnlp-main.604,D15-1104,0,0.0739129,"Missing"
2021.emnlp-main.604,P19-2026,0,0.0534211,"encoder (Beltagy et al., 2020) designed to support long sequences. Figure 1 outlines our model. Contributions We propose a highly parallel model for autoregressive entity linking that retains the advantages of being generative while being &gt;70 times faster than a previous generative formulation and as fast as non-generative models. We optimize for the correctness of the decoder’s ranking with a discriminative loss to improve autoregressive EL further. The model outperforms state-of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framin"
2021.emnlp-main.604,I11-1029,0,0.0213794,"optimize for the correctness of the decoder’s ranking with a discriminative loss to improve autoregressive EL further. The model outperforms state-of-the-art approaches on the standard English AIDA dataset. 2 Background to contextualized mention representations. Martins et al. (2019) also explore joint learning of Named Entity Recognition (NER) and EL showing that the two tasks benefit from joint training, while Li et al. (2020) approach EL specifically for questions. In this work, we focus on monolingual EL in English while there is a line of work that explores cross-lingual entity linking (McNamee et al., 2011; Ji et al., 2015), that is linking from any source language to a standard one (e.g. English), and multilingual entity linking (Botha et al., 2020) that is a generalization of both. Autoregressive Linking The GENRE model by De Cao et al. (2021a) departs from framing EL as matching in vector space, and instead frames it as a sequence-to-sequence problem. GENRE tackles MD and ED for all mention-entity pairs jointly by autoregressively generating a version of the input markup-annotated with the entities’ unique identifiers expressed in natural language. Although we focus on EL, GENRE was also app"
2021.emnlp-main.604,Q14-1019,0,0.0524117,"Missing"
2021.emnlp-main.604,D19-1258,0,0.0290497,"Missing"
2021.emnlp-main.604,N15-1026,0,0.0605548,"Missing"
2021.emnlp-main.604,D19-1005,0,0.0441375,"Missing"
2021.emnlp-main.604,2021.naacl-main.200,1,0.772611,"Missing"
2021.emnlp-main.604,2021.acl-long.194,0,0.0167169,"ng Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., sequences. Additionally, Transformers-based de2011; Le and Titov, 2018) is a fundamental task in coders are notably data-hungry, and their effective NLP employed as a building block for text under- training requires large amounts of data. For examstanding (Févry et al., 2020a; Verga et al., 2020), ple, De Cao et al. (2021a) had to pre-train their question answering (Nie et al., 2019; Asai et al., model on Wikipedia abstracts. 2020; De Cao et al., 2019), dialog modeling (DiIn this work, we revisit the generative approach nan et al., 2019; Sevegnani et al., 2021), and in- to EL and generate mention-entity pairs conditionformation extraction (Sarawagi, 2008; Martinez- ally independently given the input. This allows Rodriguez et al., 2020), to name a few. Popular ear- for parallelism across mentions, which we exploit lier methods address the Mention Detection (MD) by employing a shallow LSTM-based decoder. To and Entity Disambiguation (ED) stages of EL sep- optimize more explicitly the generator’s ranking, arately (Ceccarelli et al., 2013; Daiber et al., 2013; we use a discriminative correction term that pushes Steinmetz and Sack, 2013; Piccinno and Fer"
2021.emnlp-main.604,2020.emnlp-main.519,0,0.0231975,"nerating a version of the input markup-annotated with the entities’ unique identifiers expressed in natural language. Although we focus on EL, GENRE was also applied to ED alone as well as to page-level document retrieval for fact-checking, open-domain question Related work EL is typically decomposed in answering, slot filling, and dialog (Petroni et al., Mention Detection (MD, i.e., the task of finding 2021). mGENRE (De Cao et al., 2021b) is the mention spans in text) and Entity Disambiguation multilingual extension of GENRE. (ED, i.e., the task of disambiguating a mention Modern techniques (Wu et al., 2020; Botha et al., to its respective entity). Many methods (Hoffart 2020) are based on a dense retriever module that et al., 2011; Piccinno and Ferragina, 2014; Steinuses maximum inner-product search (MIPS) to metz and Sack, 2013) treat these sub-tasks sepamatch mention vectors to entity embeddings. In rately, training different modules. More modern apcontrast with MIPS for linking, generative models proaches – known as end-to-end EL – instead use a i) exploit knowledge learned during pre-training, ii) shared (typically neural) architecture. Kolitsas et al. are memory-efficient as they do not nee"
2021.emnlp-main.604,K16-1025,0,0.0178117,"istics of this dataset. AIDA provides full supervision for both MD and ED. We only link mentions that have a valid gold KB entity, a setting 3 We limit the maximum number of tokens per span to referred to as InKB evaluation (Röder et al., 2018). 15 to avoid memory overhead (in the training set there is no mention with more than 12 tokens). This is in line with many previous models (Luo 7664 Split Documents Mentions 942 216 230 18,540 4,791 4,485 Training Validation Test Table 1: Statistics of the AIDA-CoNLL standard splits (Hoffart et al., 2011) dataset. et al., 2015; Ganea and Hofmann, 2017; Yamada et al., 2016) and all systems we compare to. As in several previous approaches, for linking we assume the availability of a pre-computed set of candidates instead of considering the whole KB. For that, we use the candidates by Pershina et al. (2015). We also use these candidates to provide negative samples for the discriminative loss during training (see Equation 4). 4.2 Architecture details Method Micro-F1 Hoffart et al. (2011) Steinmetz and Sack (2013) Daiber et al. (2013) Moro et al. (2014) Piccinno and Ferragina (2014) Kolitsas et al. (2018) Peters et al. (2019) Broscheit (2019) Martins et al. (2019) v"
2021.mtsummit-research.8,2020.wmt-1.5,1,0.795255,"en prepended to the source sentence to specify the output language. We use WMT data (see Section 4.1) for training and early stopping. Training of the from-scratch system. Training consists of fine-tuning a pretrained model with Pashto–English parallel data, using it to generate initial backtranslations which are combined with the parallel data and used to train another round of the model, starting again from a pretrained model. At this point, we include the first 220,000 sentence pairs of “Bytedance” filtered parallel data, sorted by filtering rank. Following similar work with English–Tamil (Bawden et al., 2020), we start with our mBART-like model and we fine-tune it in the Pashto→English direction with our parallel data. Then we use this model to backtranslate the Pashto monolingual data, generating a pseudo-parallel corpus which we combine with our true parallel corpus and use to train a English→Pashto model again starting from mBART. We use this model to backtranslate the first 5,000,000 monolingual English sentences (we also experimented with the full corpus, but found minimal difference), and we train another round of Pashto→English followed by another round of English→Pashto, both initialized f"
2021.mtsummit-research.8,W18-2716,0,0.065695,"Missing"
2021.mtsummit-research.8,D18-2012,0,0.0212368,"f these could be Pashto speakers. Pashto (also spelled Pukhto and Pakhto is an Iranian language of the Indo-European family and is grouped with other Iranian languages such as Persian, Dari, Tajiki, in spite of major linguistic diferences among them. Pashto is written with a unique enriched Perso-Arabic script with 45 letters and four diacritics. Translating between English and Pashto poses interesting challenges. Pashto has a richer morphology than that of English; the induced data sparseness may partly be remedied with segmentation in subword units tokenization models such as SentencePiece (Kudo and Richardson, 2018), as used in mBART50. There are Pashto categories in Pashto that do not overtly exist in English (such as verb aspect or the oblique case in general nouns) and categories in English that do not overtly exist in Pashto (such as definite and indefinite articles), which may pose a certain challenge when having to generate correct text in machine translation output. Due to the chronic political and social instability and conflict that Afghanistan has experienced in its recent history, the country features prominently in global news coverage. Closely following the developments there remains a key p"
2021.mtsummit-research.8,2020.tacl-1.47,0,0.318683,"he BBC and DW for a short period of time. Given the impact of the COVID-19 pandemic, a twomonth period was considered realistic. On 1 February 2021, BBC and DW revealed the chosen language to be Pashto. By completing and documenting how this challenge was addressed, we prove we are able to bootstrap a new high quality neural machine translation task within a very limited window of time. There has also been a considerable amount of recent interest in using pretrained language models for improving performance on downstream natural language processing tasks, especially in a low resource setting (Liu et al., 2020; Brown et al., 2020; Qiu et al., 2020), but how best to do this is still an open question. A key question in this work is how best to use training data which is not English (en) to Pashto (ps) translations. We experimented, on the one hand, with pretraining models on a high-resource language pair (German–English, one of the most studied high-resource language pairs) and, on the other hand, with fine-tuning an existing large pretrained translation model (mBART50) trained on parallel data involving English and 49 languages including Pashto (Tang et al., 2020). We show that both approaches perfo"
2021.mtsummit-research.8,W17-4770,0,0.0257416,"Missing"
2021.mtsummit-research.8,W18-6319,0,0.0127791,"tively small decrease in memory consumption: for example, the GPU memory requirements of mBART n–to–n at inference time (setting the maximum number of tokens per mini-batch to 100) moved from around 4 GB to around 3 GB. 5 Results and Discussion Tables 2 and 3 show BLEU and chrF2 scores, respectively, for the English to Pashto systems with different test sets. The evaluation metrics for the Google MT system are also included for reference purposes. Similarly, tables 4 and 5 show BLEU and chrF2 scores, respectively, for the Pashto to English systems. All the scores were computed with sacrebleu (Post, 2018). The test sets considered are the two in-house parallel sets created by BBC and DW (see Section 3) and the devtest set provided in the FLORES19 benchmark (2,698 sentences). 19 https://github.com/facebookresearch/flores 8 Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 99 Google from-scratch mBART50 + small + small, large + small, large, synthetic BBC test DW test FLORES devtest 35.03 20.00 19.42 22.55 25.27 25.38 24.65 15.06 15.30 17.50 19.13 17.88 21.54 14.90 14.59 14.77 17.71 17.08 Table 4: BLEU scores of the Pa"
aziz-etal-2012-pet,P02-1040,0,\N,Missing
aziz-etal-2012-pet,P07-2045,0,\N,Missing
aziz-etal-2012-pet,W10-1703,0,\N,Missing
aziz-etal-2012-pet,2011.eamt-1.12,1,\N,Missing
aziz-etal-2012-pet,R11-1014,1,\N,Missing
C16-1296,J93-2003,0,0.144905,"Missing"
C16-1296,N13-1073,0,0.11035,"d evaluation. Symmetrised alignments were obtained with the grow-diag-final-and heuristic. Data We used the WMT 2014 news commentary data9 to train our models and the corresponding dev (newstest2013) and test (newstest2014) sets for tuning and evaluation. We use all available monolingual data to train 5-gram language models with KenLM (Heafield, 2011). Models We report results for the Bayesian HMM described in Section 2 (BHMM) and our collocationbased model described in Section 3 (BHMM-Z). To enable comparison with standard alignment toolkits, we also report results with Giza++ and fastAlign (Dyer et al., 2013). Finally, we make a comparison with the collocation-based IBM2 model of Schulz et al. (2016) (BIBM2-Z). Hyperparameters The hyperparameters of our model were set to β = γ = 0.0001 to obtain sparse lexical distributions. For the jump prior we chose α = 1, not giving preference to any particular distribution. The same choices apply for the BIBM2-Z. However, that model does not employ hyperparameter inference and we therefore set s = 1, r = 0.01. Finally, the BHMM uses the same parameters as the previous two models, except those associated with the collocation variable. All samplers were run for"
C16-1296,N13-1117,0,0.0136153,"her with a Gibbs sampler for word alignment was first presented for IBM model 1 by Mermer and Sarac¸lar (2011). They also gave a more detailed analysis of their method and extended it to IBM model 2 in Mermer et al. (2013). The model presented here is also similar in spirit to Schulz et al. (2016) who proposed to use a language model component together with an alignment model. However, they used IBM models 1 and 2 as alignment components. Apart from the present work, the only other work on Bayesian word alignment that we know of that performed as well as or better than Giza++ was presented in Gal and Blunsom (2013). These authors reformulated IBM models 1-4 as hierarchical Pitman-Yor processes. While their models are highly expressive, the Gibbs sampler based on Chinese Restaurant processes that they used is very slow and thus their models are unfortunately not useful in practice. 7 Discussion and Future Work We envision several useful extensions of our model for the future. Firstly, we plan to turn the language model distribution into a hierarchical distribution. We plan to use either a hierarchical Dirichlet process or Pitman-Yor process for this. The advantage of this technique is that information ab"
C16-1296,W11-2123,0,0.0202706,"urce position unaligned. 5 Experiments and Results All experiments were run using the Moses phrase-based system (Koehn et al., 2007) with lexicalized reordering. In order to speed up our experiments we used cube pruning with a pop limit of 1000 in both tuning and evaluation. Symmetrised alignments were obtained with the grow-diag-final-and heuristic. Data We used the WMT 2014 news commentary data9 to train our models and the corresponding dev (newstest2013) and test (newstest2014) sets for tuning and evaluation. We use all available monolingual data to train 5-gram language models with KenLM (Heafield, 2011). Models We report results for the Bayesian HMM described in Section 2 (BHMM) and our collocationbased model described in Section 3 (BHMM-Z). To enable comparison with standard alignment toolkits, we also report results with Giza++ and fastAlign (Dyer et al., 2013). Finally, we make a comparison with the collocation-based IBM2 model of Schulz et al. (2016) (BIBM2-Z). Hyperparameters The hyperparameters of our model were set to β = γ = 0.0001 to obtain sparse lexical distributions. For the jump prior we chose α = 1, not giving preference to any particular distribution. The same choices apply fo"
C16-1296,N09-1036,0,0.0307775,"(s)) p(u|s) = p(s|u) ∝ (9) p(s) 0 otherwise The posterior that we slice sample from is simply proportional to the product of likelihood and prior of the Beta distribution: p(s|f1m , z1m , r) ∝ P (z1m |f1m , s, r) × p(s). Notice that the conditions in Equation (9) guarantee that at least the current point will be in the slice. We will therefore always be able to obtain a new sample. Intuitively, slice sampling works because the marginal distribution p(s) stays unchanged. 4.5 Decoding After we have taken a number of samples, we are ready to decode. We use a version of maximum marginal decoding (Johnson and Goldwater, 2009) in which we assign to each source position j the target position that was most often sampled as a value for Aj . If most of the time the collocation variable Zj was active, however, we leave that source position unaligned. 5 Experiments and Results All experiments were run using the Moses phrase-based system (Koehn et al., 2007) with lexicalized reordering. In order to speed up our experiments we used cube pruning with a pop limit of 1000 in both tuning and evaluation. Symmetrised alignments were obtained with the grow-diag-final-and heuristic. Data We used the WMT 2014 news commentary data9"
C16-1296,N03-1017,0,0.0177683,"useful in practice, we devise an auxiliary variable Gibbs sampler that allows us to resample alignment links in constant time independently of the target sentence length. This leads to considerable speed improvements. Experimental results show that our model performs as well as existing word alignment toolkits in terms of resulting BLEU score. 1 Introduction Word alignment is one of the basic problems in statistical machine translation (SMT). The IBM models were originally devised for translation by Brown et al. (1993). Later, when SMT started to employ entire phrases instead of single words (Koehn et al., 2003), the IBM models were repurposed as word alignment models. The alignments they produce guide the phrase extraction heuristics that are used in many modern SMT systems. There are several extensions of the classical IBM models that try to weaken their independence assumptions. Notably, Vogel et al. (1996) introduced Markovian dependencies between individual alignment links. Those links were treated as independent events in IBM models 1, 2 and 3. The model of Vogel et al. (1996) can be viewed as a Hidden Markov Model (HMM) in which the hidden Markov Chain induces a probability distribution over l"
C16-1296,N06-1014,0,0.0597548,"re all intractable. In practice, hill-climbing heuristics are employed to approximate expectations in these more complex models. Unfortunately, all convergence guarantees of the learning algorithm are lost this way. A major problem for the HMM aligner is the handling of NULL words.1 The NULL word is a special This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 The original work of Vogel et al. (1996) did not use NULL words and instead aligned all source words. Later work by Och and Ney (2003) and Liang et al. (2006) has shown that using NULL words to allow for unaligned words improves performance. 3146 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3146–3155, Osaka, Japan, December 11-17 2016. lexical item that is hypothesised to stand at the beginning of every target (English) sentence. Since in word alignment the source (French) side is generated given the target side, the NULL word is used to generate source words that do not have lexical translations on the target side. Such untranslatable words are often idiosyncratic to the source"
C16-1296,P11-2032,0,0.0350439,"Missing"
C16-1296,J03-1002,0,0.0187068,"M models 3 to 5 which are all intractable. In practice, hill-climbing heuristics are employed to approximate expectations in these more complex models. Unfortunately, all convergence guarantees of the learning algorithm are lost this way. A major problem for the HMM aligner is the handling of NULL words.1 The NULL word is a special This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/ 1 The original work of Vogel et al. (1996) did not use NULL words and instead aligned all source words. Later work by Och and Ney (2003) and Liang et al. (2006) has shown that using NULL words to allow for unaligned words improves performance. 3146 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3146–3155, Osaka, Japan, December 11-17 2016. lexical item that is hypothesised to stand at the beginning of every target (English) sentence. Since in word alignment the source (French) side is generated given the target side, the NULL word is used to generate source words that do not have lexical translations on the target side. Such untranslatable words are often idi"
C16-1296,P16-2028,1,0.55377,"Data We used the WMT 2014 news commentary data9 to train our models and the corresponding dev (newstest2013) and test (newstest2014) sets for tuning and evaluation. We use all available monolingual data to train 5-gram language models with KenLM (Heafield, 2011). Models We report results for the Bayesian HMM described in Section 2 (BHMM) and our collocationbased model described in Section 3 (BHMM-Z). To enable comparison with standard alignment toolkits, we also report results with Giza++ and fastAlign (Dyer et al., 2013). Finally, we make a comparison with the collocation-based IBM2 model of Schulz et al. (2016) (BIBM2-Z). Hyperparameters The hyperparameters of our model were set to β = γ = 0.0001 to obtain sparse lexical distributions. For the jump prior we chose α = 1, not giving preference to any particular distribution. The same choices apply for the BIBM2-Z. However, that model does not employ hyperparameter inference and we therefore set s = 1, r = 0.01. Finally, the BHMM uses the same parameters as the previous two models, except those associated with the collocation variable. All samplers were run for 1000 iterations without burn-in and samples were taken after each 25th iteration. The initia"
C16-1296,C96-2141,0,0.696474,"oolkits in terms of resulting BLEU score. 1 Introduction Word alignment is one of the basic problems in statistical machine translation (SMT). The IBM models were originally devised for translation by Brown et al. (1993). Later, when SMT started to employ entire phrases instead of single words (Koehn et al., 2003), the IBM models were repurposed as word alignment models. The alignments they produce guide the phrase extraction heuristics that are used in many modern SMT systems. There are several extensions of the classical IBM models that try to weaken their independence assumptions. Notably, Vogel et al. (1996) introduced Markovian dependencies between individual alignment links. Those links were treated as independent events in IBM models 1, 2 and 3. The model of Vogel et al. (1996) can be viewed as a Hidden Markov Model (HMM) in which the hidden Markov Chain induces a probability distribution over latent alignment links. Besides weakening the independence assumptions of the simpler IBM models, the HMM alignment model has the additional benefit of being tractable. This means that expectations under the HMM aligner can be computed exactly using the forward-backward algorithm. These expectations are"
C16-1296,D10-1058,0,0.0185711,"r alignment links, they bias 10 The run times for our models include the computation of the initial state of the sampler with IBM model 1. The run time of the sampler itself is thus slightly lower than the reported times. Also notice that due to its highly optimised posterior computations, fastAlign finishes in under 10 minutes on average. 3153 their model to preferably align positions which are close to each other. Using standard results for series, they manage to make the posterior computations in their model extremely fast. Another interesting extension of the HMM alignment is presented in Zhao and Gildea (2010) who added a fertility distribution to the HMM. This made posterior computations in their model intractable, however, they avoided the use of heuristics and instead approximated the posterior using MCMC. The idea of using Bayesian inference together with a Gibbs sampler for word alignment was first presented for IBM model 1 by Mermer and Sarac¸lar (2011). They also gave a more detailed analysis of their method and extended it to IBM model 2 in Mermer et al. (2013). The model presented here is also similar in spirit to Schulz et al. (2016) who proposed to use a language model component together"
C16-1296,P07-2045,0,\N,Missing
D14-1131,W09-0437,0,0.0142068,"the leftmost uncovered source position (Lopez, 2009). This is a widely used strategy and it is in use in the Moses toolkit (Koehn et al., 2007).8 Nevertheless, the problem of finding the best 7 If d is a maximum from g and g(d) = f (d), then it is easy to show by contradiction that d is the actual maximum from f : if there existed d0 such that f (d0 ) &gt; f (d), then it follows that g(d0 ) ≥ f (d0 ) &gt; f (d) = g(d), and hence d would not be a maximum for g. 8 A distortion limit characterises a form of pruning that acts directly in the generative capacity of the model leading to induction errors (Auli et al., 2009). Limiting reordering like that lowers complexity to a polynomial function of I and an exponential function of the distortion limit. 1240 derivation under the model remains impracticable due to nonlocal parameterisation (namely, the n-gram LM component). The weighted set hD(x), f (d)i, which represents the objective, is a complex hypergraph which we cannot afford to construct. We propose to construct instead a simpler hypergraph for which optimisation by dynamic programming is feasible. This proxy rep resents the weighted set D(x), g (0) (d) , where g (0) (d) ≥ f (d) for every d ∈ D(x). Note t"
D14-1131,W13-2260,1,0.88383,"1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of"
D14-1131,J93-2003,0,0.06433,"siting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the queue. More recently, Heafield et al. (2014) applied their beam filling algorithm to phrase-based decoding. 3.2 Exact optimisation Exact optimisation for monotone translation has been done using A∗ search (Tillmann et al., 1997) and finite-state operations (Kumar et al., 2006). Och et al. (2001) design near-admissible heuristics for A∗ and decode very short sentences (614 words) for a word-based model (Brown et al., 1993) with a maximum distortion strategy (d = 3). Zaslavskiy et al. (2009) frame phrase-based decoding as an instance of a generalised Travelling Salesman Problem (TSP) and rely on robust solvers to perform decoding. In this view, a salesman graph encodes the translation options, with each node representing a biphrase. Nonoverlapping constraints are imposed by the TSP solver, rather than encoded directly in the salesman graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large.4 Chang and Collins (2011) relax phrase-based models w.r.t."
D14-1131,D12-1103,1,0.792042,"esents a left-shift. The rule N ON - ADJACENT handles the remaining cases i &gt; l provided that the expansion skips at most d input words |r − i + 1 |≤ d. In the consequent, the window C is simply updated to record the translation of the input span i..i0 . In the nonadjacent case, a gap constraint imposes that the resulting item will require skipping no more than d positions before the leftmost uncovered word is translated |i0 − l + 1 |≤ d.10 Finally, note that deductions incorporate the weighted upperbound ω(·), rather than the true LM component ψ(·).11 4.3 LM upperbound and Max-ARPA Following Carter et al. (2012) we compute an upperbound on n-gram conditional probabilities by precomputing max-backoff weights stored in a “Max-ARPA” table, an extension of the ARPA format (Jurafsky and Martin, 2000). A standard ARPA table T stores entries 10 This constraint prevents items from becoming dead-ends where incomplete derivations require a reordering step larger than d. This is known to prevent many search errors in beam search (Chang and Collins, 2011). 11 Unlike Aziz et al. (2013), rather than unigrams only, we score all n-grams within a translation rule (including incomplete ones). 1241 hZ, Z.p, Z.bi, where"
D14-1131,J07-2003,0,0.49343,"d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and"
D14-1131,W13-2212,0,0.0177391,"t. the upperbound distribution) due to an extended context. LM 5 LM Experiments We used the dataset made available by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013) to train a German-English phrase-based system using the Moses toolkit (Koehn et al., 2007) in a standard setup. For phrase extraction, we used both Europarl (Koehn, 2005) and News Commentaries (NC) totalling about 2.2M sentences.15 For language modelling, in addition to the monolingual parts of Europarl 15 Pre-processing: tokenisation, truecasing and automatic compound-splitting (German only). Following Durrani et al. (2013), we set the maximum phrase length to 5. where w0 = λψ qLM (yγ0 ) qLM (γ0 ) Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = hV, Ei, a node v0 ∈ V singly identified by its carry [l0 , C0 , r0 , γ0 ] and a left-extension y for its LM context γE0 . The program copies most of the edges D w → v ∈ E. If a derivation goes through v0 uσ − and the string under v0 ends in yγ0 , the program refines and reweights it. and NC, we added News-2013 totalling about 25M sentences. We performed language model interpolation and batch-mira tuning (Cherry and Foster, 2012) u"
D14-1131,W12-6106,1,0.916844,"exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics lected via maximisation. A refinement increases the complexity of the model only slightly, hence dynamic programming remains feasible throughout the search until convergence. We test our decoding strategy with realistic models using stand"
D14-1131,D11-1003,0,0.642929,"ang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249, c October 25-29, 2014, Doha, Qatar. 2014 Associa"
D14-1131,P01-1030,0,0.385025,"ues are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refin"
D14-1131,N12-1047,0,0.0208928,"wing Durrani et al. (2013), we set the maximum phrase length to 5. where w0 = λψ qLM (yγ0 ) qLM (γ0 ) Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = hV, Ei, a node v0 ∈ V singly identified by its carry [l0 , C0 , r0 , γ0 ] and a left-extension y for its LM context γE0 . The program copies most of the edges D w → v ∈ E. If a derivation goes through v0 uσ − and the string under v0 ends in yγ0 , the program refines and reweights it. and NC, we added News-2013 totalling about 25M sentences. We performed language model interpolation and batch-mira tuning (Cherry and Foster, 2012) using newstest2010 (2,849 sentence pairs). For tuning we used cube pruning with a large beam size (k = 5000) and a distortion limit d = 4. Unpruned language models were trained using lmplz (Heafield et al., 2013b) which employs modified Kneser-Ney smoothing (Kneser and Ney, 1995). We report results on newstest2012. Our exact decoder produces optimal translation derivations for all the 3,003 sentences in the test set. Table 1 summarises the performance of our novel decoder for language models of order n = 3 to n = 5. For 3-gram LMs we also varied the distortion limit d (from 4 to 6). We report"
D14-1131,J99-4004,0,0.12781,"Missing"
D14-1131,P05-1033,0,0.0283693,"noverlapping constraints are imposed by the TSP solver, rather than encoded directly in the salesman graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large.4 Chang and Collins (2011) relax phrase-based models w.r.t. the non-overlapping constraints, which are replaced by soft penalties through Lagrangian multipliers, and intersect the LM component exhaustively. They do employ a maximum distortion limit (d = 4), thus the problem they tackle is no longer NP-complete. Rush and Collins (2011) relax a hierarchical phrase-based model (Chiang, 2005)5 w.r.t. the LM component. The translation forest and the language model trade their weights (through Lagrangian multipliers) so as to ensure agreement on what each component believes to be the maximum. In both approaches, when the dual converges to a compliant solution, the solution is guaranteed to be optimal. Other4 Exact decoding had been similarly addressed with Integer Linear Programming (ILP) in the context of word-based models for very short sentences using a 2-gram LM (Germann et al., 2001). Riedel and Clarke (2009) revisit that formulation and employ a cutting-plane algorithm (Dantzi"
D14-1131,N13-1116,0,0.108849,"rding to common traits and a fast-to-compute heuristic view of outside weights (cheapest way to complete a hypothesis) puts them to compete at a fairer level. Beam search exhausts a node’s possible expansions, scores them, and discards all but the k highest-scoring ones. This process is wasteful in that k is typically much smaller than the number of possible expansions. Cube pruning employs a priority queue at beam filling and computes k highscoring expansions directly in near best-first order. The parameter k is known as beam size and it controls the time-accuracy trade-off of the algorithm. Heafield et al. (2013a) move away from using the language model as a black-box and build a more involved beam filling algorithm. Even though they target approximate search, some of their ideas have interesting connections to ours (see §4). They group hypotheses that share partial language model state (Li and Khudanpur, 2008) reasoning over multiple hypotheses at once. They fill a beam in best-first order by iteratively visiting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the qu"
D14-1131,P13-2121,0,0.075138,"Missing"
D14-1131,P14-2022,0,0.0232288,"Missing"
D14-1131,P07-1019,0,0.0131256,"erised and contains all translation derivations (a translation lattice or forest), and one that re-ranks the first as a function of the interactions between translation steps. The model of translational equivalences parameterised only with φ is an instance of the former. An n-gram LM component is an instance of the latter. 2.1 Hypergraphs A backward-hypergraph, or simply hypergraph, is a generalisation of a graph where edges have multiple origins and one destination (Gallo et al., 1993). They can represent both finite-state and context-free weighted sets and they have been widely used in SMT (Huang and Chiang, 2007). A hypergraph is defined by a set of nodes (or ver3 Figure 1 can be seen as a specification for a weighted acyclic finite-state automaton whose states are indexed by [l, C, r] and transitions are labelled with biphrases. However, for generality of representation, we opt for using acyclic hypergraphs instead of automata (see §2.1). 1238 tices) V and a weighted set of edges hE, wi. An edge e connects a sequence of nodes in its tail t[e] ∈ V ∗ under a head node h[e] ∈ V and has weight w(e). A node v is a terminal node if it has no incoming edges, otherwise it is a nonterminal node. The node that"
D14-1131,E09-1044,0,0.0282357,"Missing"
D14-1131,J00-4006,0,0.0193412,"w C is simply updated to record the translation of the input span i..i0 . In the nonadjacent case, a gap constraint imposes that the resulting item will require skipping no more than d positions before the leftmost uncovered word is translated |i0 − l + 1 |≤ d.10 Finally, note that deductions incorporate the weighted upperbound ω(·), rather than the true LM component ψ(·).11 4.3 LM upperbound and Max-ARPA Following Carter et al. (2012) we compute an upperbound on n-gram conditional probabilities by precomputing max-backoff weights stored in a “Max-ARPA” table, an extension of the ARPA format (Jurafsky and Martin, 2000). A standard ARPA table T stores entries 10 This constraint prevents items from becoming dead-ends where incomplete derivations require a reordering step larger than d. This is known to prevent many search errors in beam search (Chang and Collins, 2011). 11 Unlike Aziz et al. (2013), rather than unigrams only, we score all n-grams within a translation rule (including incomplete ones). 1241 hZ, Z.p, Z.bi, where Z is an n-gram equal to the concatenation Pz of a prefix P with a word z, Z.p is the conditional probability p(z|P), and Z.b is a so-called “backoff” weight associated with Z. The condit"
D14-1131,J99-4005,0,0.273875,"m starts from its axioms and follows exhaustively deducing new items by combination of existing ones and no deduction happens twice. In Figure 1, a nonteminal item summarises partial derivation (or hypotheses). It is denoted by [C, r, γ] (also known as carry), where: C is a coverage vector, necessary to impose the non-overlapping constraint; r is the rightmost position most recently covered, necessary for the computation of δ; and γ is the last n − 1 words 1 Preventing phrases from overlapping requires an exponential number of constraints (the powerset of x) rendering the problem NP-complete (Knight, 1999). 2 Weighted logics have been extensively used to describe weighted sets (Lopez, 2009), operations over weighted sets (Chiang, 2007; Dyer and Resnik, 2010), and a variety of dynamic programming algorithms (Cohen et al., 2008).   I TEM {0, 1}I , [0, I + 1], ∆n−1 G OAL 1I , I + 1, EOS A XIOM hBOS → BOSi [0I , 0, BOS] : ψ(BOS) E XPAND   D i0 φr j 0 E j−1 − → yj xi − C, r, yj−n+1 Li0 ¯ h i k=i ck = 0 0 j C 0 , i0 , yj 0 −n+2 : w where c0k = ck if k &lt; i or k &gt; i0 else ¯ 1 0 j−1 w = φr ⊗ δ(r, i) ⊗ ψ(yjj |yj−n+1 ) ACCEPT  I  1 , r, γ r≤I [1I , I + 1, EOS] : δ(r, I + 1) ⊗ ψ(EOS|γ) Figure 1: Sp"
D14-1131,N03-1017,0,0.307503,"s independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2"
D14-1131,P07-2045,0,0.0134732,"d), for some finite t. At which point dt is the optimum derivation d∗ from f and the sequence of upperbounds provides a proof of optimality.7 4.2 Model We work with phrase-based models in a standard parameterisation (Equation 2). However, to avoid having to deal with NP-completeness, we constrain reordering to happen only within a limited window given by a notion of distortion limit. We require that the last source word covered by any biphrase must be within d words from the leftmost uncovered source position (Lopez, 2009). This is a widely used strategy and it is in use in the Moses toolkit (Koehn et al., 2007).8 Nevertheless, the problem of finding the best 7 If d is a maximum from g and g(d) = f (d), then it is easy to show by contradiction that d is the actual maximum from f : if there existed d0 such that f (d0 ) &gt; f (d), then it follows that g(d0 ) ≥ f (d0 ) &gt; f (d) = g(d), and hence d would not be a maximum for g. 8 A distortion limit characterises a form of pruning that acts directly in the generative capacity of the model leading to induction errors (Auli et al., 2009). Limiting reordering like that lowers complexity to a polynomial function of I and an exponential function of the distortion"
D14-1131,2005.mtsummit-papers.11,0,0.0460421,"NE, which instead of copying them, creates new ones headed by a refined version of v0 . Finally, R EWEIGHT continues from the refined node with reweighted copies of the edges leaving v0 . The weight update represents a change in LM probability (w.r.t. the upperbound distribution) due to an extended context. LM 5 LM Experiments We used the dataset made available by the Workshop on Statistical Machine Translation (WMT) (Bojar et al., 2013) to train a German-English phrase-based system using the Moses toolkit (Koehn et al., 2007) in a standard setup. For phrase extraction, we used both Europarl (Koehn, 2005) and News Commentaries (NC) totalling about 2.2M sentences.15 For language modelling, in addition to the monolingual parts of Europarl 15 Pre-processing: tokenisation, truecasing and automatic compound-splitting (German only). Following Durrani et al. (2013), we set the maximum phrase length to 5. where w0 = λψ qLM (yγ0 ) qLM (γ0 ) Figure 4: Local intersection via LM right state refinement. The input is a hypergraph G = hV, Ei, a node v0 ∈ V singly identified by its carry [l0 , C0 , r0 , γ0 ] and a left-extension y for its LM context γE0 . The program copies most of the edges D w → v ∈ E. If a"
D14-1131,W08-0402,0,0.0190682,"t k is typically much smaller than the number of possible expansions. Cube pruning employs a priority queue at beam filling and computes k highscoring expansions directly in near best-first order. The parameter k is known as beam size and it controls the time-accuracy trade-off of the algorithm. Heafield et al. (2013a) move away from using the language model as a black-box and build a more involved beam filling algorithm. Even though they target approximate search, some of their ideas have interesting connections to ours (see §4). They group hypotheses that share partial language model state (Li and Khudanpur, 2008) reasoning over multiple hypotheses at once. They fill a beam in best-first order by iteratively visiting groups using a priority queue: if the top group contains a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the queue. More recently, Heafield et al. (2014) applied their beam filling algorithm to phrase-based decoding. 3.2 Exact optimisation Exact optimisation for monotone translation has been done using A∗ search (Tillmann et al., 1997) and finite-state operations (Kumar et al., 2006). Och et al. (2001) design nea"
D14-1131,E09-1061,0,0.624957,"existing ones and no deduction happens twice. In Figure 1, a nonteminal item summarises partial derivation (or hypotheses). It is denoted by [C, r, γ] (also known as carry), where: C is a coverage vector, necessary to impose the non-overlapping constraint; r is the rightmost position most recently covered, necessary for the computation of δ; and γ is the last n − 1 words 1 Preventing phrases from overlapping requires an exponential number of constraints (the powerset of x) rendering the problem NP-complete (Knight, 1999). 2 Weighted logics have been extensively used to describe weighted sets (Lopez, 2009), operations over weighted sets (Chiang, 2007; Dyer and Resnik, 2010), and a variety of dynamic programming algorithms (Cohen et al., 2008).   I TEM {0, 1}I , [0, I + 1], ∆n−1 G OAL 1I , I + 1, EOS A XIOM hBOS → BOSi [0I , 0, BOS] : ψ(BOS) E XPAND   D i0 φr j 0 E j−1 − → yj xi − C, r, yj−n+1 Li0 ¯ h i k=i ck = 0 0 j C 0 , i0 , yj 0 −n+2 : w where c0k = ck if k &lt; i or k &gt; i0 else ¯ 1 0 j−1 w = φr ⊗ δ(r, i) ⊗ ψ(yjj |yj−n+1 ) ACCEPT  I  1 , r, γ r≤I [1I , I + 1, EOS] : δ(r, I + 1) ⊗ ψ(EOS|γ) Figure 1: Specification for the weighted set of translation derivations in phrase-based SMT with u"
D14-1131,W01-1408,0,0.200059,"e steps in a derivation. That is, Hk (d) = e∈d hk (e), where hk is a (local) feature function that assesses steps independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Othe"
D14-1131,P03-1021,0,0.00758593,"sk of producing a translation for an input string x = hx1 , x2 , . . . , xI i is typically associated with finding the best derivation d∗ compatible with the input under a linear model. In this view, a derivation is a structured output that represents a sequence of steps that covers the input producing a translation. Equation 1 illustrates this decoding process. d∗ = argmax f (d) d∈D(x) (1) The set D(x) is the space of all derivations compatible with x and supported by a model of translational equivalences (Lopez, 2008). The function f (d) = Λ · H(d) is a linear parameterisation of the model (Och, 2003). It assigns a real-valued score (or weight) to every derivation d ∈ D(x), where Λ ∈ Rm assigns a relative importance to different aspects of the derivation independently captured by m feature functions H(d) = hH1 (d), . . . , Hm (d)i ∈ Rm . The fully parameterised model can be seen as a discrete weighted set such that feature functions factorise P over the steps in a derivation. That is, Hk (d) = e∈d hk (e), where hk is a (local) feature function that assesses steps independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight"
D14-1131,D08-1012,0,0.0205634,"s incrementally refined to be closer to the goal until the maximum is found (or until the sampling performance exceeds a certain level). Figure 2 illustrates exact optimisation with OS∗ . Suppose f is a complex target goal distribution, such that we cannot optimise f , but we can assess f (d) for a given d. Let g (0) be an upperbound to f , i.e., g (0) (d) ≥ f (d) for all d ∈ D(x). Moreover, suppose that g (0) is simple enough to be optimised efficiently. The algorithm proceeds by solving d0 = argmaxd g (0) (d) and comput6 The intuition that a full intersection is wasteful is also present in (Petrov et al., 2008) in the context of approximate search. They start from a coarse distribution based on automatic word clustering which is refined in multiple passes. At each pass, hypotheses are pruned a posteriori on the basis of their marginal probabilities, and word clusters are further split. We work with upperbounds, rather than word clusters, with unpruned distributions, and perform exact optimisation. g(0) f* f1 g(1) f f0 d0 d1 d* D(x) Figure 2: Sequence of incrementally refined upperbound proposals. ing the quantity r0 = f (d0 )/g(0) (d0 ). If r0 were sufficiently close to 1, then g (0) (d0 ) would be"
D14-1131,N09-2002,0,0.0230745,"er NP-complete. Rush and Collins (2011) relax a hierarchical phrase-based model (Chiang, 2005)5 w.r.t. the LM component. The translation forest and the language model trade their weights (through Lagrangian multipliers) so as to ensure agreement on what each component believes to be the maximum. In both approaches, when the dual converges to a compliant solution, the solution is guaranteed to be optimal. Other4 Exact decoding had been similarly addressed with Integer Linear Programming (ILP) in the context of word-based models for very short sentences using a 2-gram LM (Germann et al., 2001). Riedel and Clarke (2009) revisit that formulation and employ a cutting-plane algorithm (Dantzig et al., 1954) reaching 30 words. 5 In hierarchical translation, reordering is governed by a synchronous context-free grammar and the underlying problem is no longer NP-complete. Exact decoding remains infeasible because the intersection between the translation forest and the target LM is prohibitively slow. 1239 wise, a subset of the constraints is explicitly added and the dual optimisation is repeated. They handle sentences above average length, however, resorting to compact rulesets (10 translation options per input segm"
D14-1131,P11-1008,0,0.0834511,"ercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz et al., 2013). Other work has employed less common approximations to the model reducing its search space complexity (Kumar et al., 2006; Chang and Collins, 2011; Rush and Collins, 2011). These do not answer whether or not current decoding algorithms perform well at real translation tasks with state-of-the-art models. We propose an exact decoder for phrase-based SMT based on a coarse-to-fine search strategy (Dymetman et al., 2012). In a nutshell, we relax the decoding problem with respect to the Language Model (LM) component. This coarse view is incrementally refined based on evidence col1237 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1237–1249, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Li"
D14-1131,P97-1037,0,0.461893,"ons factorise P over the steps in a derivation. That is, Hk (d) = e∈d hk (e), where hk is a (local) feature function that assesses steps independently and d = he1 , e2 , . . . , el i is a sequence of l steps. Under this assumption, each step is assigned the weight w(e) = Λ·hh1 (e), h2 (e), . . . , hm (e)i. The set D is typically finite, however, it contains a very large number of structures — exponential (or even factorial, see §2) with the size of x — making exhaustive enumeration prohibitively slow. Only in very restricted cases combinatorial optimisation techniques are directly applicable (Tillmann et al., 1997; Och et al., 2001), thus it is common to resort to heuristic techniques in order to find an approximation to d∗ (Koehn et al., 2003; Chiang, 2007). Evaluation exercises indicate that approximate search algorithms work well in practice (Bojar et al., 2013). The most popular algorithms provide solutions with unbounded error, thus precisely quantifying their performance requires the development of a tractable exact decoder. To date, most attempts were limited to short sentences and/or somewhat toy models trained with artificially small datasets (Germann et al., 2001; Iglesias et al., 2009; Aziz"
D14-1131,P09-1038,1,0.891528,"a single hypothesis, the hypothesis is added to the beam, otherwise the group is partitioned and the parts are pushed back to the queue. More recently, Heafield et al. (2014) applied their beam filling algorithm to phrase-based decoding. 3.2 Exact optimisation Exact optimisation for monotone translation has been done using A∗ search (Tillmann et al., 1997) and finite-state operations (Kumar et al., 2006). Och et al. (2001) design near-admissible heuristics for A∗ and decode very short sentences (614 words) for a word-based model (Brown et al., 1993) with a maximum distortion strategy (d = 3). Zaslavskiy et al. (2009) frame phrase-based decoding as an instance of a generalised Travelling Salesman Problem (TSP) and rely on robust solvers to perform decoding. In this view, a salesman graph encodes the translation options, with each node representing a biphrase. Nonoverlapping constraints are imposed by the TSP solver, rather than encoded directly in the salesman graph. They decode only short sentences (17 words on average) using a 2-gram LM due to salesman graphs growing too large.4 Chang and Collins (2011) relax phrase-based models w.r.t. the non-overlapping constraints, which are replaced by soft penalties"
D14-1131,N10-1128,0,\N,Missing
D14-1131,W13-2201,1,\N,Missing
D17-1209,N16-1024,0,0.0410979,"hanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags. Latent structure. Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer. They encode source words as a learned average of potential parents emulating a relaxed dependency tree. While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improv"
D17-1209,P17-2021,0,0.19234,"ures and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side. Nadejde et al. (2017) multi-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG s"
D17-1209,P10-1146,0,0.0329964,"riguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dependency syntax."
D17-1209,W14-4012,0,0.196706,"Missing"
D17-1209,D14-1179,0,0.140705,"Missing"
D17-1209,P16-1078,0,0.267892,"y we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent"
D17-1209,P17-2012,0,0.225287,"ystems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and,"
D17-1209,D17-1012,0,0.137824,"Missing"
D17-1209,D14-1080,0,0.0742857,"Missing"
D17-1209,D13-1176,0,0.0955223,", unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work. Our main contributions can be summarized as follows: • we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German. 2 Background Notation. We use x for vectors, x1:t for a sequence of t vectors, and X for matrices. The i-th value of vector x is denoted by xi . We use ◦ for vector concatenation. 2.1 Neural Machine Translation In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx ) of translating a source sentence x1:Tx (a sequence of Tx words) into a target sentence y1:Ty . NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism. We will now briefly describe the components that we use in this paper. 2.1.1 Encoders An encoder is a function that takes as input the source sentence and p"
D17-1209,Q16-1037,0,0.200605,"pturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCN"
D17-1209,W16-2209,0,0.0464331,"N <=10 11-20 21-30 31-40 Sentence length 41+ Figure 4: Validation BLEU per sentence length. Discussion. Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by BLEU4 (as well as TER and BEER). Consistent gains in terms of Kendall tau and BLEU1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax. 5 Related Work We review various accounts to syntax in NMT as well as other convolutional encoders. Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-t"
D17-1209,W16-2323,0,0.105372,"as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups. 1 Introduction Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015"
D17-1209,P16-1162,0,0.532548,"as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups. 1 Introduction Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015"
D17-1209,D16-1159,0,0.0446397,"perties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require th"
D17-1209,W06-3104,0,0.0297664,"Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is natural for us to use dep"
D17-1209,D15-1166,0,0.52241,"nrich et al., 2016a). State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language. One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders, including RNNs. Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006"
D17-1209,2006.amta-papers.25,0,0.0452057,"Missing"
D17-1209,D17-1159,1,0.838954,"tional Linguistics det The dobj nsubj monkey • we introduce a method for incorporating structure into NMT using syntactic GCNs; det eats a banana • we show that GCNs can be used along with RNN and CNN encoders; Figure 1: A dependency tree for the example sentence: “The monkey eats a banana.” th order neighborhood (i.e. nodes at most k hops aways from the node) (Gilmer et al., 2017). They are generally simple and computationally inexpensive. We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017). Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework. As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task. Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information. A comparison to RNNs is the most challe"
D17-1209,P02-1040,0,0.0986164,"using the syntaxnet/demo.sh shell script. 9 https://github.com/moses-smt/mosesdecoder 1962 8 English-German English-German (full) English-Czech Source Target 37824 50000 33786 8099 (BPE) 16000 (BPE) 8116 (BPE) Table 2: Vocabulary sizes. lent to the dimensionality of their input. We report results for 2-layer GCNs, as we find them most effective (see ablation studies below). Baselines. We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN. See §2.1.1 for details. Evaluation. We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10 4.2.1 Results English-German. Table 3 shows test results on English-German. Unsurprisingly, the bag-ofwords baseline performs the worst. We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens. The CNN baseline reaches a higher BLEU4 score than the BoW models, but interestingly its BLEU1 score is lower than the BoW+GCN model. The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for BLEU1 and BLEU4 , respectively. The BiRNN, the strongest baseline, reaches a BLEU4 of 14.9. Interes"
D17-1209,D13-1170,0,0.00748132,"sion (Linzen et al., 2016; Shi et al., 2016). Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively. In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains). For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees. However, in this work we solely focus on dependency syntax and leave more general investigation for future work. Our main contributions can be summarized as follows: • we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German. 2 Background Notation. We use x for vectors, x1:t for a sequence of t vectors, and X for matrices. The i-th value of vector x is denoted by xi . We use ◦ for vector concatenation. 2.1 Neural Machine Translation In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014;"
D17-1209,P16-2049,0,0.03803,"better word order and lexical/BPE selection, two phenomena that depend crucially on syntax. 5 Related Work We review various accounts to syntax in NMT as well as other convolutional encoders. Syntactic features and/or constraints. Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings. Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree. In the decoder, word and node representations compete under the same attention mechanism. Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT. Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments. Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees. Multi-task Learning. Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations. Luong et al. (2015a) predict linearized constituency parses as an additional task. Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve"
D17-1209,D14-1025,1,0.89451,"Missing"
D17-1209,W06-3119,0,0.0765112,"learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)). Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task. This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT. Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation. Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output. Since vectors correspond to words, it is n"
D17-1209,W08-2121,0,\N,Missing
D17-1209,P11-2031,0,\N,Missing
D17-1209,P17-1012,0,\N,Missing
L16-1649,P06-2103,0,0.138628,", v1 ) represents adjacent sentences, and c(·) is a function that counts how often a pattern (or a pair of patterns) was observed in the training data. m n Y Y 1 X c(ui , vj ) + α (3) p(D) = m i=1 c(ui ) + α|V | m n j=1 Y m n X Y p(vj |ui ) (4) n j=1 i=0 (um 1 ,v1 )∈D We resort to Expectation Maximisation (EM) to estimate the parameters in Equation 4 (Brown et al., 1993). Due to the convexity of IBM Model 1, EM is guaranteed to converge to a global optimum. Moreover, as we observe more data, this model converges to better parameters. A similar solution was proposed in a different context by (Soricut and Marcu, 2006) in their work on word co-occurrences. To avoid assigning 0 probability to documents containing unseen patterns, we modify the original training procedure to treat all the singletons as belonging to an unknown category (U NK), thus reserving probability mass for future unseen items. 3. Running the Toolkit Input The input data for the toolkit is raw text, with markup for document breaks. The syntax models take ptb marked up files, which can be derived from code included in the toolkit. Models available (u1 ,v1 )∈D To account for unseen syntactic patterns at test time, the model is smoothed by a"
L16-1649,J08-1001,0,0.223135,"Missing"
L16-1649,J93-2003,0,0.174796,"Section 2.2. we describe our entity graph implementation, which captures different aspects of lexical coherence from an entity grid, tracking connections between non-adjacent entities in the text. This model covers aspects of coherence that reflect the centrality and topic of the discourse. Following that we present our implementation of the syntax-based coherence model of Louis and Nenkova (2012), which captures syntactic patterns between adjacent sentences (Section 2.3.). Finally, in Section 2.4. we present our own extension of this model, a fully generative model incorporating IBM Model 1 (Brown et al., 1993) to model alignments over syntactic items in adjacent sentences. 2.1. Entity Grid Model As detailed in (Barzilay and Lapata, 2008; Elsner et al., 2007), the entity-based approach derives from the assumption that entities in a coherent text are distributed in a certain manner, as posed by various discourse theories, in par1 http://cs.brown.edu/melsner/manual.html ticular Centering Theory (Grosz et al., 1995). This theory holds that coherent texts are characterised by salient entities in strong grammatical roles, such as subject or object. Entity grids are constructed by identifying the discours"
L16-1649,P10-1020,0,0.154027,"ith a given syntactic role, namely, subject (S), object (O), or other (X). Transitions are observed by examining the grid vertically for each entity. We replicate the generative model of document coherence based on entity transitions introduced by Lapata (2005). Equation 1 shows this formulation, where m is the number of entities, n is the number of sentences in a document D and rs,e is the role taken by entity e in sentence s. m n 1 YY p(D) = p(rs,e |r(s−h),e . . . r(s−1),e ) (1) m · n e=1 s=1 This is because the syntactic patterns which hold for English, do not hold for German, for example (Cheung and Penn, 2010). 2.2. Entity Graph Model Guinaudeau and Strube (2013) converted a standard entity grid into a bipartite graph which tracks the occurrence of entities throughout the document, including between nonadjacent sentences. A local coherence score is calculated directly as the average outdegree of a projection, summing the shared edges of entities between two sentences. The general form of the coherence score assigned to a document D in this approach is shown in Equation 2. This is a centrality measure based on the average outdegree across the N sentences represented in a directed document graph. The"
L16-1649,N07-1055,0,0.0275142,"ons between non-adjacent entities in the text. This model covers aspects of coherence that reflect the centrality and topic of the discourse. Following that we present our implementation of the syntax-based coherence model of Louis and Nenkova (2012), which captures syntactic patterns between adjacent sentences (Section 2.3.). Finally, in Section 2.4. we present our own extension of this model, a fully generative model incorporating IBM Model 1 (Brown et al., 1993) to model alignments over syntactic items in adjacent sentences. 2.1. Entity Grid Model As detailed in (Barzilay and Lapata, 2008; Elsner et al., 2007), the entity-based approach derives from the assumption that entities in a coherent text are distributed in a certain manner, as posed by various discourse theories, in par1 http://cs.brown.edu/melsner/manual.html ticular Centering Theory (Grosz et al., 1995). This theory holds that coherent texts are characterised by salient entities in strong grammatical roles, such as subject or object. Entity grids are constructed by identifying the discourse entities in the documents under consideration, and constructing a 2D grid for each document, whereby each column corresponds to the entity, i.e. noun"
L16-1649,J95-2003,0,0.813958,"ptures syntactic patterns between adjacent sentences (Section 2.3.). Finally, in Section 2.4. we present our own extension of this model, a fully generative model incorporating IBM Model 1 (Brown et al., 1993) to model alignments over syntactic items in adjacent sentences. 2.1. Entity Grid Model As detailed in (Barzilay and Lapata, 2008; Elsner et al., 2007), the entity-based approach derives from the assumption that entities in a coherent text are distributed in a certain manner, as posed by various discourse theories, in par1 http://cs.brown.edu/melsner/manual.html ticular Centering Theory (Grosz et al., 1995). This theory holds that coherent texts are characterised by salient entities in strong grammatical roles, such as subject or object. Entity grids are constructed by identifying the discourse entities in the documents under consideration, and constructing a 2D grid for each document, whereby each column corresponds to the entity, i.e. noun, being tracked, and each row represents a particular sentence in the document. An entity transition is defined as a consecutive occurrence of an entity with a given syntactic role, namely, subject (S), object (O), or other (X). Transitions are observed by ex"
L16-1649,P13-1010,0,0.433436,"object (O), or other (X). Transitions are observed by examining the grid vertically for each entity. We replicate the generative model of document coherence based on entity transitions introduced by Lapata (2005). Equation 1 shows this formulation, where m is the number of entities, n is the number of sentences in a document D and rs,e is the role taken by entity e in sentence s. m n 1 YY p(D) = p(rs,e |r(s−h),e . . . r(s−1),e ) (1) m · n e=1 s=1 This is because the syntactic patterns which hold for English, do not hold for German, for example (Cheung and Penn, 2010). 2.2. Entity Graph Model Guinaudeau and Strube (2013) converted a standard entity grid into a bipartite graph which tracks the occurrence of entities throughout the document, including between nonadjacent sentences. A local coherence score is calculated directly as the average outdegree of a projection, summing the shared edges of entities between two sentences. The general form of the coherence score assigned to a document D in this approach is shown in Equation 2. This is a centrality measure based on the average outdegree across the N sentences represented in a directed document graph. The outdegree of a sentence si , denoted o(si ), is the t"
L16-1649,D12-1106,0,\N,Missing
N18-1092,P05-1074,0,0.0930066,"R and E N -D E . Note that these two systems perform sometimes better sometimes worse depending on the benchmark. There is no clear pattern, but differences may well come from some qualitative difference in the induced latent space. It is a known fact that different languages realise lexical ambiguities differently, thus representations induced towards different languages are likely to capture different generalisations.8 As C OMBO results show, the representations induced from different corpora are somewhat complementary. That same observation has guided paraphrasing models based on pivoting (Bannard and Callison-Burch, 2005). Once more we report a monolingual variant of E MBEDA LIGN (indicated by E N) in an attempt to illustrate how crucial the 6 http://scikit-learn.org/stable/ In Appendix A we provide bar plots marked with error bars (2 standard deviations). 8 We also acknowledge that our treatment of German is likely suboptimal due to the lack of subword features, as it can also be seen in AER results. 1017 7 translation signal is. Dataset 3.4 MTurk-771 SIMLEX-999 WS-353-ALL YP-130 VERB-143 MEN-TR-3k SimVerb-3500 RG-65 WS-353-SIM RW-STANFORD WS-353-REL MC-30 MTurk-287 Word similarity Word similarity benchmarks"
N18-1092,W14-3302,0,0.0315809,"Missing"
N18-1092,D15-1075,0,0.0323215,"compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model’s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity. 1 Introduction Natural language processing applications often count on the availability of word representations trained on large textual data as a means to alleviate problems such as data sparsity and lack of linguistic resources (Collobert et al., 2011; Socher et al., 2011; Tu et al., 2017; Bowman et al., 2015). Traditional approaches to inducing word representations circumvent the need for explicit semantic annotation by capitalising on some form of indirect semantic supervision. A typical example is to fit a binary classifier to detect whether or not a target word is likely to co-occur with neighbouring words (Mikolov et al., 2013). If the binary classifier represents a word as a continuous vector, that vector will be trained to be discriminative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https://github.com/ uva-slpl"
N18-1092,K16-1002,0,0.149238,"Missing"
N18-1092,J93-2003,0,0.221747,"|m, n) = p(z1m ) Pθ (xi |zi ) i=1 × n Y m X j=1 aj =1 (2) P (aj |m)Pθ (yj |zaj )dz1m Due to the conditional independences of our model, it is trivial to marginalise lexical alignments for any given latent embeddings z1m , but marginalising the embeddings themselves is intractable. Thus, we employ amortised mean field variational inference using the inference model qφ (z1m |xm 1 ) , m Y i=1 N (zi |ui , diag(si si )) (3) where each factor is a diagonal Gaussian. We map m from xm 1 to a sequence u1 of independent posterior 2 We pad L1 sentences with N ULL to account for untranslatable L2 words (Brown et al., 1993). Instead, Schulz et al. (2016) generate untranslatable words from L2 context—an alternative we leave for future work. 1012 mean (or location) vectors, where ui , µ(hi ; φ), as well as a sequence sm 1 of independent standard deviation (or scale) vectors, where si , σ(hi ; φ), m and hm 1 = enc(x1 ; φ) is a deterministic encoding of the L1 sequence (we discuss concrete architectures in §2.3). All mappings are realised by neural networks whose parameters are collectively denoted by φ (the variational parameters). Note that we choose to approximate the posterior without conditioning on y1n . This"
N18-1092,D17-1070,0,0.191218,"W 2 VEC , employ bidirectional encoders. leave it as a rather speculative point. One additional point worth highlighting: the middle section of Table 7. E NBoW and E NBiRNN show what happens when we do not give E MBEDA LIGN L2 supervision at training. That is, imagine the model of Figure 1 without the bottom plate. In that case, the model representations overfit for L1 word-byword prediction. Without the need to predict any notion of context (monolingual or otherwise), the representations drift away from semantic-driven generalisations and fail at lexical substitution. 3.3 Sentence Evaluation Conneau et al. (2017) developed a framework to evaluate unsupervised sentence level representations trained on large amounts of data on a range of supervised NLP tasks. We assess our induced representations using their framework on the following benchmarks evaluated on classification ↑accuracy (MRPC is further evaluated on ↑F1) MR classification of positive or negative movie reviews; SST fined-grained labelling of movie reviews from the Stanford sentiment treebank (SST); TREC classification of questions into k-classes; CR classification of positive or negative product reviews; SUBJ classification of a sentence int"
N18-1092,P02-1033,0,0.066345,"istributional hypothesis hinges on the definition of context and different models are based on different definitions. Importantly, the nature of the context determines the range of linguistic properties the representations may capture (Levy and Goldberg, 2014b). For example, Levy and Goldberg (2014a) propose to use syntactic context derived from dependency parses. They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (Mikolov et al., 2013). In this work, we take lexical translation as indirect semantic supervision (Diab and Resnik, 2002). Effectively we make two assumptions. First, that every word has a foreign equivalent that stands for its meaning. Second, that we can find this equivalent in translation data through lexical alignments.1 For that we induce both a latent mapping between words in a bilingual sentence pair and distributions over latent word representations. To summarise our contributions: • we model a joint distribution over sentence pairs that generates data from latent word representations and latent lexical alignments; • we embed words in context mining positive correlations from translation data; • we find"
N18-1092,P14-5004,0,0.0809295,"with English-French only— recall that models based on that pair performed better in terms of AER. Results in this section are based on E MBEDA LIGN (with bidirectional variational encoder) trained on the Giga web corpus (see Table 1 for statistics). Due to the scale of the experiment, we report on a single run. We trained on Giga with the same hyperparameters that we trained on Europarl, however, for 3 epochs instead of 30 (with this dataset an epoch amounts to 183, 000 updates). Again, we performed model selection on AER. Table 9 shows the results for several datasets using the framework of Faruqui and Dyer (2014a). Note that E MBE DA LIGN was designed to make use of context information, thus this evaluation setup is a bit unnatural for our model. Still, it outperforms S KIP G RAM on 5 out of 13 benchmarks, in particular, on SIMLEX-999, whose relevance has been argued by Upadhyay et al. (2016). We also remark that this model achieves 0.25 test AER and 45.16 test GAP on lexical substitution—a considerable improvement compared to models trained on Europarl and reported in Tables 4 (AER) and 7 (GAP). 4 Related work Our model is inspired by lexical alignment models such as IBM1 (Brown et al., 1993), howev"
N18-1092,E14-1049,0,0.236194,"with English-French only— recall that models based on that pair performed better in terms of AER. Results in this section are based on E MBEDA LIGN (with bidirectional variational encoder) trained on the Giga web corpus (see Table 1 for statistics). Due to the scale of the experiment, we report on a single run. We trained on Giga with the same hyperparameters that we trained on Europarl, however, for 3 epochs instead of 30 (with this dataset an epoch amounts to 183, 000 updates). Again, we performed model selection on AER. Table 9 shows the results for several datasets using the framework of Faruqui and Dyer (2014a). Note that E MBE DA LIGN was designed to make use of context information, thus this evaluation setup is a bit unnatural for our model. Still, it outperforms S KIP G RAM on 5 out of 13 benchmarks, in particular, on SIMLEX-999, whose relevance has been argued by Upadhyay et al. (2016). We also remark that this model achieves 0.25 test AER and 45.16 test GAP on lexical substitution—a considerable improvement compared to models trained on Europarl and reported in Tables 4 (AER) and 7 (GAP). 4 Related work Our model is inspired by lexical alignment models such as IBM1 (Brown et al., 1993), howev"
N18-1092,P14-1006,0,0.0229007,"elation coefficient (↑). The first column is from (Faruqui and Dyer, 2014a). There is a vast literature on exploiting multilingual context to strengthen the notion of synonymy captured by monolingual models. Roughly, the literature splits into two groups, namely, approaches that derive additional features and/or training objectives based on pre-trained alignments (Klementiev et al., 2012; Faruqui and Dyer, 2014b; Luong et al., ˇ 2015; Suster et al., 2016), and approaches that promote a joint embedding space by working with sentence level representations that dispense with explicit alignments (Hermann and Blunsom, 2014; AP et al., 2014; Gouws et al., 2015; Hill et al., 2014). The work of Koˇcisk´y et al. (2014) is closer to ours in that they also learn embeddings by marginalising alignments, however, their model is conditional—much like IBM models—and their embeddings are not part of the probabilistic model, but rather part of the architecture design. The joint formulation allows our latent embeddings to harvest learning signal from L2 while still being driven by the learning signal from L1—in a conditional model the representations can become specific to alignment deviating from the purpose of well represe"
N18-1092,C12-1089,0,0.027609,".2172 0.5384 0.6962 0.3878 0.6094 0.6258 0.6698 0.5229 0.3887 0.3968 0.4784 0.4593 0.4191 0.3539 0.6389 0.4509 0.3278 0.3494 0.5559 0.3965 Table 9: Evaluation of English word embeddings out of context in terms of Spearman’s rank correlation coefficient (↑). The first column is from (Faruqui and Dyer, 2014a). There is a vast literature on exploiting multilingual context to strengthen the notion of synonymy captured by monolingual models. Roughly, the literature splits into two groups, namely, approaches that derive additional features and/or training objectives based on pre-trained alignments (Klementiev et al., 2012; Faruqui and Dyer, 2014b; Luong et al., ˇ 2015; Suster et al., 2016), and approaches that promote a joint embedding space by working with sentence level representations that dispense with explicit alignments (Hermann and Blunsom, 2014; AP et al., 2014; Gouws et al., 2015; Hill et al., 2014). The work of Koˇcisk´y et al. (2014) is closer to ours in that they also learn embeddings by marginalising alignments, however, their model is conditional—much like IBM models—and their embeddings are not part of the probabilistic model, but rather part of the architecture design. The joint formulation all"
N18-1092,2005.mtsummit-papers.11,0,0.0605266,"max (W1 zi + b1 ) g(zaj ; θ) = softmax W2 zaj + b2  (7a) LSTM hidden states (ith step) that process the sequence in opposite directions. We use 128 units for deterministic embeddings, and 100 units for LSTMs (Hochreiter and Schmidhuber, 1997) and latent representations (i.e. d = 100). 3 We start the section describing the data used to estimate our model’s parameters as well as details about the optimiser. The remainder of the section presents results on various benchmarks. Training data We train our model on bilingual parallel data. In particular, we use parliament proceedings (Europarl-v7) (Koehn, 2005) from two language pairs: English-French and EnglishGerman.4 We employed very minimal preprocessing, namely, tokenisation and lowercasing using scripts from M OSES (Koehn et al., 2007), and have discarded sentences longer than 50 tokens. Table 1 lists more information about the training data, including the English-French Giga web corpus (Bojar et al., 2014) which we use in §3.4.5 Corpus (7b) where W1 ∈ Rvx ×d , b1 ∈ Rvx , W2 ∈ Rvy ×d , b2 ∈ Rvy , and vx (vy ) is the size of the vocabulary of L1 (L2). With the approximation of §2.2, we replace  the L1 softmax layer (7a) by exp zi> cx + bx norm"
N18-1092,P07-2045,0,0.00462417,"ddings, and 100 units for LSTMs (Hochreiter and Schmidhuber, 1997) and latent representations (i.e. d = 100). 3 We start the section describing the data used to estimate our model’s parameters as well as details about the optimiser. The remainder of the section presents results on various benchmarks. Training data We train our model on bilingual parallel data. In particular, we use parliament proceedings (Europarl-v7) (Koehn, 2005) from two language pairs: English-French and EnglishGerman.4 We employed very minimal preprocessing, namely, tokenisation and lowercasing using scripts from M OSES (Koehn et al., 2007), and have discarded sentences longer than 50 tokens. Table 1 lists more information about the training data, including the English-French Giga web corpus (Bojar et al., 2014) which we use in §3.4.5 Corpus (7b) where W1 ∈ Rvx ×d , b1 ∈ Rvx , W2 ∈ Rvy ×d , b2 ∈ Rvy , and vx (vy ) is the size of the vocabulary of L1 (L2). With the approximation of §2.2, we replace  the L1 softmax layer (7a) by exp zi> cx + bx normalised by the CSS estimate (6) at training, and similarly for the L2 softmax layer (7b). In that case, we have parameters for cx , cy ∈ Rd —deterministic embeddings for x and y, respec"
N18-1092,P14-2037,0,0.0397872,"Missing"
N18-1092,P14-2050,0,0.0335999,"inative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https://github.com/ uva-slpl/embedalign MR and WA contributed equally. The underlying assumption is that context (e.g. neighbouring words) stands for the meaning of the target word (Harris, 1954; Firth, 1957). The success of this distributional hypothesis hinges on the definition of context and different models are based on different definitions. Importantly, the nature of the context determines the range of linguistic properties the representations may capture (Levy and Goldberg, 2014b). For example, Levy and Goldberg (2014a) propose to use syntactic context derived from dependency parses. They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (Mikolov et al., 2013). In this work, we take lexical translation as indirect semantic supervision (Diab and Resnik, 2002). Effectively we make two assumptions. First, that every word has a foreign equivalent that stands for its meaning. Second, that we can find this equivalent in translation data through lexical alignments.1 For that we induce both a laten"
N18-1092,W14-1618,0,0.0342058,"inative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https://github.com/ uva-slpl/embedalign MR and WA contributed equally. The underlying assumption is that context (e.g. neighbouring words) stands for the meaning of the target word (Harris, 1954; Firth, 1957). The success of this distributional hypothesis hinges on the definition of context and different models are based on different definitions. Importantly, the nature of the context determines the range of linguistic properties the representations may capture (Levy and Goldberg, 2014b). For example, Levy and Goldberg (2014a) propose to use syntactic context derived from dependency parses. They show that their representations are much more discriminative of syntactic function than models based on immediate neighbourhood (Mikolov et al., 2013). In this work, we take lexical translation as indirect semantic supervision (Diab and Resnik, 2002). Effectively we make two assumptions. First, that every word has a foreign equivalent that stands for its meaning. Second, that we can find this equivalent in translation data through lexical alignments.1 For that we induce both a laten"
N18-1092,W15-1521,0,0.0464016,"Missing"
N18-1092,W15-1501,0,0.063635,"task dispenses with inferences about L2. Each candidate is compared to the target word in context through a measure of overlap between their inferred densities—we take KL divergence. We then rank candidates using this measure. Table 7 lists GAP scores for variants of E M BEDA LIN (bottom section) as well as some baselines and other established methods (top section). For comparison, we also compute GAP by sorting candidates in terms of cosine similarity, in which case we take the Gaussian mean as a summary of the density. The top section of the table contains systems reported by Melamud et al. (2015) (R ANDOM and S KIP G RAM) and by Brazinskas et al. (2017) (BSG). Note that both S KIP G RAM (Mikolov et al., 2013) and BSG were trained on the very large ukWaC English corpus (Ferraresi et al., 2008). S KIP G RAM is known to perform remarkably well regardless of its apparent insensitivity to context (in terms of design). BSG is a close relative of our model which gives S KIP G RAM a Bayesian treatment (also by means of amortised variational inference) and is by design sensitive to context in a manner similar to E MBEDA LIGN, that is, through its inferred posteriors. Our first observation is t"
N18-1092,W03-0301,0,0.177076,"lso important to highlight that we do not employ regularisation techniques (such as batch normalisation, dropout, or L2 penalty) for they did not seem to yield consistent results. 3.1 Word alignment Since our model leverages learning signal from parallel data by marginalising latent lexical alignments, we use alignment error rate to double check whether the model learns sensible word correspondences. Intrinsic assessment of word alignment quality requires manual annotation. For EnglishFrench, we use the NAACL English-French handaligned data (37 sentence pairs for validation and 447 for test) (Mihalcea and Pedersen, 2003). For English-German, we use the data by Pad´o and Lapata (2006) (98 sentence pairs for validation and 987 for test). Alignment quality is then measured in terms of alignment error rate (AER) (Och and Ney, 2000)—an F-measure over predicted alignment links. For prediction we condition on the posterior means E[Z1m ] which is just the predicted variational means um 1 and select the L1 position for which P (yj , aj |um 1 ) is maximum (a form of approximate Viterbi alignment). Model L1 accuracy L2 accuracy B OW B OW α B I RNN B I RNNα ↓AER 95.59 ± 2.22 99.87 ± 0.22 95.72 ± 1.28 99.97 ± 0.09 5.69 ±"
N18-1092,P00-1056,0,0.411317,"ges learning signal from parallel data by marginalising latent lexical alignments, we use alignment error rate to double check whether the model learns sensible word correspondences. Intrinsic assessment of word alignment quality requires manual annotation. For EnglishFrench, we use the NAACL English-French handaligned data (37 sentence pairs for validation and 447 for test) (Mihalcea and Pedersen, 2003). For English-German, we use the data by Pad´o and Lapata (2006) (98 sentence pairs for validation and 987 for test). Alignment quality is then measured in terms of alignment error rate (AER) (Och and Ney, 2000)—an F-measure over predicted alignment links. For prediction we condition on the posterior means E[Z1m ] which is just the predicted variational means um 1 and select the L1 position for which P (yj , aj |um 1 ) is maximum (a form of approximate Viterbi alignment). Model L1 accuracy L2 accuracy B OW B OW α B I RNN B I RNNα ↓AER 95.59 ± 2.22 99.87 ± 0.22 95.72 ± 1.28 99.97 ± 0.09 5.69 ± 2.07 6.16 ± 0.39 7.31 ± 0.64 7.25 ± 0.62 35.41 ± 1.16 30.94 ± 2.49 34.32 ± 1.08 29.18 ± 1.91 bidirectional encoder in the variational approximation. Table 2 (3) lists ↓AER for E N -F R (E N -D E) as well as accu"
N18-1092,P06-1146,0,0.0602891,"Missing"
N18-1092,P16-2028,1,0.892072,"Missing"
N18-1092,D11-1014,0,0.127503,"Missing"
N18-1092,W17-2632,0,0.027447,"ich allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model’s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity. 1 Introduction Natural language processing applications often count on the availability of word representations trained on large textual data as a means to alleviate problems such as data sparsity and lack of linguistic resources (Collobert et al., 2011; Socher et al., 2011; Tu et al., 2017; Bowman et al., 2015). Traditional approaches to inducing word representations circumvent the need for explicit semantic annotation by capitalising on some form of indirect semantic supervision. A typical example is to fit a binary classifier to detect whether or not a target word is likely to co-occur with neighbouring words (Mikolov et al., 2013). If the binary classifier represents a word as a continuous vector, that vector will be trained to be discriminative of the contexts it co-occurs with, and thus words in similar contexts will have similar representations. Code available from https:"
N18-1092,P16-1157,0,0.0181223,"we report on a single run. We trained on Giga with the same hyperparameters that we trained on Europarl, however, for 3 epochs instead of 30 (with this dataset an epoch amounts to 183, 000 updates). Again, we performed model selection on AER. Table 9 shows the results for several datasets using the framework of Faruqui and Dyer (2014a). Note that E MBE DA LIGN was designed to make use of context information, thus this evaluation setup is a bit unnatural for our model. Still, it outperforms S KIP G RAM on 5 out of 13 benchmarks, in particular, on SIMLEX-999, whose relevance has been argued by Upadhyay et al. (2016). We also remark that this model achieves 0.25 test AER and 45.16 test GAP on lexical substitution—a considerable improvement compared to models trained on Europarl and reported in Tables 4 (AER) and 7 (GAP). 4 Related work Our model is inspired by lexical alignment models such as IBM1 (Brown et al., 1993), however, we generate words y1n from a latent vector representation z1m of xm 1 , rather than directly from the observation xm . IBM1 takes L1 sequences as con1 ditioning context and does not model their distribution. Instead, we propose a joint model, where L1 sentences are generated from l"
N18-1092,N16-1160,0,0.0389102,"Missing"
N19-1240,D17-1209,1,0.857368,"tch at test time. CorefGRU, similarly to us, encodes relations between entity mentions in the document. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recurrent network and an RN"
N19-1240,D18-1454,0,0.122338,"Missing"
N19-1240,N18-2007,0,0.53592,"ttention. The methods reported by Welbl et al. (2018) approach the task by merely concatenating all documents into a single long text and training a standard RNN-based reading comprehension model, namely, BiDAF (Seo et al., 2016) and FastQA (Weissenborn et al., 2017). Document concatenation in this setting is also used in Weaver (Raison et al., 2018) and MHPGM (Bauer et al., 2018). The only published paper which 2306 Proceedings of NAACL-HLT 2019, pages 2306–2317 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics goes beyond concatenation is due to Dhingra et al. (2018), where they augment RNNs with jump-links corresponding to co-reference edges. Though these edges provide a structural bias, the RNN states are still tasked with passing the information across the document and performing multihop reasoning. Instead, we frame question answering as an inference problem on a graph representing the document collection. Nodes in this graph correspond to named entities in a document whereas edges encode relations between them (e.g., crossand within-document coreference links or simply co-occurrence in a document). We assume that reasoning chains can be captured by p"
N19-1240,P17-1147,0,0.0654028,"2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in W IKI H OP consists of a collection of documents, a qu"
N19-1240,Q18-1023,0,0.0288008,"such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in W IKI H OP consists of"
N19-1240,D17-1082,0,0.0373422,"ann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly created to facilitate the development of systems dealing with these scenarios. Each example in W IKI H OP consists of a collection of documents, a query and a set of candidate an"
N19-1240,D17-1018,0,0.0235344,"n and across supporting documents. For a given query q = hs, r, ?i, we identify mentions in Sq of the entities in Cq ∪ {s} and create one node per mention. This process is based on the following heuristic: 1. we consider mentions spans in Sq exactly matching an element of Cq ∪ {s}. Admittedly, this is a rather simple strategy which may suffer from low recall. 2. we use predictions from a coreference resolution system to add mentions of elements in Cq ∪ {s} beyond exact matching (including both noun phrases and anaphoric pronouns). In particular, we use the end-to-end coreference resolution by Lee et al. (2017). 3. we discard mentions which are ambiguously resolved to multiple coreference chains; this may sacrifice recall, but avoids propagating ambiguity. Figure 2: Supporting documents (dashed ellipses) organized as a graph where nodes are mentions of either candidate entities or query entities. Nodes with the same color indicates they refer to the same entity (exact match, coreference or both). Nodes are connected by three simple relations: one indicating co-occurrence in the same document (solid edges), another connecting mentions that exactly match (dashed edges), and a third one indicating a co"
N19-1240,D17-1159,1,0.839662,"cument and evaluating exact match at test time. CorefGRU, similarly to us, encodes relations between entity mentions in the document. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recu"
N19-1240,Q17-1008,0,0.0355064,"t. Instead of using graph neural network layers, as we do, they augment RNNs with jump links corresponding to pairs of corefereed mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recurrent network and an RNN encoder. We do not train any RNN document encoders in this work. 6 Conclusion We designed a grap"
N19-1240,D14-1162,0,0.0823299,"icted probability since a candidate answer is realized in multiple locations via different nodes. 2.3 Node Annotations Keeping in mind we want an efficient model, we encode words in supporting documents and in the query using only a pre-trained model for contextualized word representations rather than training our own encoder. Specifically, we use ELMo2 (Peters et al., 2018), a pre-trained bi-directional language model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014)), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). We choose not to fine tune nor propagate gradients through the ELMo architecture, as it would have defied the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model behaves using non-contextualized word representations (we use GloVe). Documents pre-processing ELMo encodings are used to produce a set of representations D {xi }N i=1 , where xi ∈ R denotes the ith candidate mention in context. Note that these repre"
N19-1240,N18-1202,0,0.0625823,"fo ([q, hi ]) , i∈Mc (1) where fo is a parameterized affine transformation, and Mc is the set of node indices such that i ∈ Mc only if node vi is a mention of c. The max operator in Equation 1 is necessary to select the node with highest predicted probability since a candidate answer is realized in multiple locations via different nodes. 2.3 Node Annotations Keeping in mind we want an efficient model, we encode words in supporting documents and in the query using only a pre-trained model for contextualized word representations rather than training our own encoder. Specifically, we use ELMo2 (Peters et al., 2018), a pre-trained bi-directional language model that relies on character-based input representation. ELMo representations, differently from other pre-trained word-based models (e.g., word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014)), are contextualized since each token representation depends on the entire text excerpt (i.e., the whole sentence). We choose not to fine tune nor propagate gradients through the ELMo architecture, as it would have defied the goal of not having specialized RNN encoders. In the experiments, we will also ablate the use of ELMo showing how our model beh"
N19-1240,D16-1264,0,0.139682,".ed.ac.uk Stockholm is the capital of Sweden and the most populous city in [..] query: country Thorildsplan candidates: {Denmark, Finland, Sweden, Italy, ...} answer: Sweden Figure 1: A sample from W IKI H OP where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. Introduction The long-standing goal of natural language understanding is the development of systems which can acquire knowledge from text collections. Fresh interest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi"
N19-1240,K17-1028,0,0.496915,"t collections. Fresh interest in reading comprehension tasks was sparked by the availability of large-scale datasets, such as SQuAD (Rajpurkar et al., 2016) and CNN/Daily Mail (Hermann et al., 2015), enabling end-to-end training of neural models (Seo et al., 2016; Xiong et al., 2016; Shen et al., 2017). These systems, given a text and a question, need to answer the query relying on the given document. Recently, it has been observed that most questions in these datasets do not require reasoning across the document, but they can be answered relying on information contained in a single sentence (Weissenborn et al., 2017). The last generation of large-scale reading comprehension datasets, such as a NarrativeQA (Kocisky et al., 2018), TriviaQA (Joshi et al., 2017), and RACE (Lai et al., 2017), have been created in such a way as to address this shortcoming and to ensure that systems relying only on local information cannot achieve competitive performance. Even though these new datasets are challenging and require reasoning within documents, many question answering and search applications require aggregation of information across multiple documents. The W IKI H OP dataset (Welbl et al., 2018) was explicitly creat"
N19-1240,Q18-1021,0,0.164649,"paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and crossdocument coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, W IKI H OP (Welbl et al., 2018). 1 Ivan Titov University of Edinburgh University of Amsterdam ititov@inf.ed.ac.uk Stockholm is the capital of Sweden and the most populous city in [..] query: country Thorildsplan candidates: {Denmark, Finland, Sweden, Italy, ...} answer: Sweden Figure 1: A sample from W IKI H OP where multi-step reasoning and information combination from different documents is necessary to infer the correct answer. Introduction The long-standing goal of natural language understanding is the development of systems which can acquire knowledge from text collections. Fresh interest in reading comprehension tasks"
N19-1240,D18-1244,0,0.0500509,"mentions. MHPGM uses a multi-attention mechanism in combination with external commonsense relations to perform multiple hops of reasoning. Weaver is a deep coencoding model that uses several alternating biLSTMs to process the concatenated documents and the query. Graph neural networks have been shown successful on a number of NLP tasks (Marcheggiani and Titov, 2017; Bastings et al., 2017; Zhang et al., 2313 2018a), including those involving document level modeling (Peng et al., 2017). They have also been applied in the context of asking questions about knowledge contained in a knowledge base (Zhang et al., 2018b). In Schlichtkrull et al. (2018), GCNs are used to capture reasoning chains in a knowledge base. Our work and unpublished concurrent work by Song et al. (2018) are the first to study graph neural networks in the context of multidocument QA. Besides differences in the architecture, Song et al. (2018) propose to train a combination of a graph recurrent network and an RNN encoder. We do not train any RNN document encoders in this work. 6 Conclusion We designed a graph neural network that operates over a compact graph representation of a set of documents where nodes are mentions to entities and"
P16-2028,J93-2003,0,0.212355,"Missing"
P16-2028,N13-1073,0,0.105968,"Missing"
P16-2028,P13-2121,0,0.0310779,"Missing"
P16-2028,N09-1036,0,0.0804932,"an auxiliary variable (Tanner and Wong, 1987) that uniformly chooses only one possible new assignment per sampled link. The sampling complexity, which would normally be linear in the size of the target sentence, thus becomes constant. In practice this speed up the sampler by several orders of magnitude, making our aligner as fast as Giza++. Unfortunately, this strategy also slightly impairs the mobility of our sampler. 3.4 Decoding Our samples contain assignments of the A and Z variables. If for a word fj we have zj = 1, we treat the word as not aligned. We then use maximum marginal decoding (Johnson and Goldwater, 2009) over alignment links to generate final word alignments. This means that we align each source word to the target word it has been aligned to most often in the samples. If the word was unaligned in most samples, we leave it unaligned in the output alignment. 4  P Zj = 0|C−Zj , H ∝ Experiments and results (6) c(fj |eaj , z = 0) + α (c(z = 0|fj−1 ) + s) P (aj ) c(eaj |z = 0) + αVf  P Zj = 1|C−Zj , H ∝ (7) c(fj |fj−1 , z = 1) + β (c(z = 1|fj−1 ) + r) c(z = 1|fj−1 ) + βVf We present translation experiments on English paired with German, French, Czech and Japanese, thereby covering four language f"
P16-2028,N06-1014,0,0.115658,"nglish and the source side is usually taken to be French. 169 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 169–174, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in the jth position (fj ) is aligned to under am 1 . In IBM model 1 P (am ) is uniform. In IBM 1 model 2, all alignment links aj are assumed to be independent and follow a categorical distribution. Here, we choose to parametrise this categorical based on the distance between the two words to be aligned, as has been done by Vogel et al. (1996) and Liang et al. (2006). Thus, in our IBM model 2    m m Y Y jl m P (a1 ) = P (aj ) = P i− (2) m j=1 q θa γ D s, r fprv θf f Sm S j=1 β Vf θe eS1 l α Ve Figure 1: A graphical representation of our model for S sentence pairs. We use Vf /e to denote the source/target vocabulary sizes and D to denote the number of possible alignment link configurations. Furthermore, Sm/l is the number of source/target words in the current sentence and fprv the source word preceding the one that we currently generate. Removing the NULL word 3.1 a Vf where i is the position of the English word that aj links to and the values l and m"
P16-2028,P11-2032,0,0.218989,"Missing"
P16-2028,J03-1002,0,0.0809498,"Missing"
P16-2028,P03-1021,0,0.2316,"Missing"
P16-2028,P02-1040,0,0.0948133,"Missing"
P16-2028,C96-2141,0,0.373139,"often identified with English and the source side is usually taken to be French. 169 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 169–174, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics in the jth position (fj ) is aligned to under am 1 . In IBM model 1 P (am ) is uniform. In IBM 1 model 2, all alignment links aj are assumed to be independent and follow a categorical distribution. Here, we choose to parametrise this categorical based on the distance between the two words to be aligned, as has been done by Vogel et al. (1996) and Liang et al. (2006). Thus, in our IBM model 2    m m Y Y jl m P (a1 ) = P (aj ) = P i− (2) m j=1 q θa γ D s, r fprv θf f Sm S j=1 β Vf θe eS1 l α Ve Figure 1: A graphical representation of our model for S sentence pairs. We use Vf /e to denote the source/target vocabulary sizes and D to denote the number of possible alignment link configurations. Furthermore, Sm/l is the number of source/target words in the current sentence and fprv the source word preceding the one that we currently generate. Removing the NULL word 3.1 a Vf where i is the position of the English word that aj links to"
P16-2028,P07-2045,0,\N,Missing
P18-1115,E17-3017,0,0.0292378,"of TED talks and their respective translations. We trained models to Data Arabic Czech French German Train Dev Test 224,125 6,746 2,762 114,389 5,326 2,762 220,399 5,937 2,762 196,883 6,996 2,762 Table 1: Number of parallel sentence pairs for each language paired with English for IWSLT data. translate from English into Arabic, Czech, French and German. The number of sentences for each language after preprocessing is shown in Table 1. The vocabulary was split into 50,000 subword units using Google’s sentence piece3 software in its standard settings. As our baseline NMT systems we use Sockeye (Hieber et al., 2017)4 . Sockeye implements several different NMT models but here we use the standard recurrent attentional model described in Section 2. We report baselines with and without dropout (Srivastava et al., 2014). For dropout a retention probability of 0.5 was used. As a second baseline we use our own implementation of the model of Zhang et al. (2016) which contains a single sentence-level Gaussian latent variable (SENT). Our implementation differs from theirs in three aspects. First, we feed the last hidden state of the bidirectional encoding into encoding of the source and target sentence into the in"
P18-1115,K16-1002,0,0.185215,"s are available at https://github.com/philschulz/ stochastic-decoder. † Work done prior to joining Amazon. Our proposal is to augment this model with latent sources of variation that are able to represent more of the variation present in the training data. The noise sources are modelled as Gaussian random variables. The contributions of this work are: • The introduction of an NMT system that is capable of capturing word-level variation in translation data. • A thorough discussions of issues encountered when training this model. In particular, we motivate the use of KL scaling as introduced by Bowman et al. (2016) theoretically. 1 Notice that from a statistical perspective the output of an NMT system is a distribution over target sentences and not any particular sentence. The mapping from the output distribution to a sentence is performed by a decision rule (e.g. argmax decoding) which can be chosen independently of the NMT system. 1243 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1243–1252 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics • An empirical demonstration of the improvements achievable"
P18-1115,D16-1031,0,0.0310306,"fect introduces a long range dependency between the main and auxiliary verb (underlined) that the model handles well. The second example shows variation in the lexical realisation of the verb. The second variant uses a particle verb and we again observe a long range dependency between the main verb and its particle (underlined). els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016). 7 on several language pairs. As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future. Here, we list the three which we believe to be most promising. Conclusion and Future Work We have presented a recurrent decoder for machine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translation corpora. Our experiments confirm our intuition that modelling variation is crucial to the success of machine translation. The proposed model consistently outperforms st"
P18-1115,P02-1040,0,0.103839,"ility vector returned by the softmax. 3 In practice, of course, human translators’ performance varies according to their level of education, their experience on the job, their familiarity with the textual domain and myriads of other factors. Even within a single translator variation may occur due to level of stress, tiredness or status of health. That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing one machinegenerated translation against several human translations (see e.g. Papineni et al., 2002). Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable. Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations. A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions. Multimodal modelling of these variations is th"
P18-1115,D17-1066,0,0.0152791,"ur model (SDEC) and the sentent-level latent variable model (SENT). The first SDEC example shows alternation between the German simple past and past perfect. The past perfect introduces a long range dependency between the main and auxiliary verb (underlined) that the model handles well. The second example shows variation in the lexical realisation of the verb. The second variant uses a particle verb and we again observe a long range dependency between the main verb and its particle (underlined). els have been applied mostly in monolingual settings such as text generation (Bowman et al., 2016; Semeniuta et al., 2017), morphological analysis (Zhou and Neubig, 2017), dialogue modelling (Wen et al., 2017), question selection (Miao et al., 2016) and summarisation (Miao and Blunsom, 2016). 7 on several language pairs. As this is the first work that systematically considers word-level variation in NMT, there are lots of research ideas to explore in the future. Here, we list the three which we believe to be most promising. Conclusion and Future Work We have presented a recurrent decoder for machine translation that uses word-level Gaussian variables to model underlying sources of variation observed in translatio"
P18-1115,D16-1050,0,0.45503,"experience on the job, their familiarity with the textual domain and myriads of other factors. Even within a single translator variation may occur due to level of stress, tiredness or status of health. That translation corpora contain variation is acknowledged by the machine translation community in the design of their evaluation metrics which are geared towards comparing one machinegenerated translation against several human translations (see e.g. Papineni et al., 2002). Prior to our work, the only attempt at modelling the latent variation underlying these different translations was made by Zhang et al. (2016) who introduced a sentence level Gaussian variable. Intuitively, however, there is more to latent variation than a unimodal density can capture, for example, there may be several highly likely clusters of plausible variations. A cluster may e.g. consist of identical syntactic structures that differ in word choice, another may consist of different syntactic constructs such as active or passive constructions. Multimodal modelling of these variations is thus called for—and our results confirm this intuition. An example of variation comes from free word order and agreement phenomena in morphologic"
P18-1115,P17-1029,0,0.0516846,"RNN (ti−1 , yi−1 , zi ) q(z0n ) = q(z0 ) n ∏ q(zi |z<i ) . (9) i=1 (8) The remaining computations stay unchanged. Notice that the latent values are used directly in updating the decoder state. This makes the decoder state a function of a random variable and thus the decoder state is itself random. Applying this argument recursively shows that also the attention mechanism is random, making the decoder entirely stochastic. 4 that use several variables usually employ a mean field approximation under which all latent variables are independent. This turns the ELBO into a sum of expectations (e.g. Zhou and Neubig, 2017). For our stochastic decoder we design a more flexible approximation posterior family which respects the dependencies between the latent variables, Our stochastic decoder can be viewed as a stack of conditional DGMs (Sohn et al., 2015) in which the latent variables depend on one another. The ELBO thus consists of nested positional ELBOs, ELBO0 + Eq(z0 ) [ELBO1 +Eq(z1 ) [ELBO2 + . . .]] , (10) where for a given target position i the ELBO is Inference and Training ELBOi = Eq(zi ) [log p(yi |xm 1 , y<i , z<i , zi )] We use variational inference (see e.g. Blei et al., 2017) to train the model. In"
P19-1284,D17-1042,0,0.100006,"Missing"
P19-1284,D15-1003,0,0.0234473,"those areas. Interpretability. Machine learning research has been focusing more and more on interpretability (Gilpin et al., 2018). However, there are many nuances to interpretability (Lipton, 2016), and amongst them we focus on model transparency. One strategy is to extract a simpler, interpretable model from a neural network, though this comes at the cost of performance. For example, Thrun (1995) extract if-then rules, while Craven and Shavlik (1996) extract decision trees. There is also work on making word vectors more interpretable. Faruqui et al. (2015) make word vectors more sparse, and Herbelot and Vecchi (2015) learn to map distributional word vectors to model-theoretic semantic vectors. Similarly to Lei et al. (2016), Titov and McDonald (2008) extract informative fragments of text by jointly training a classifier and a model predicting a stochastic mask, while relying on Gibbs sampling to do so. Their focus is on using the sentiment labels as a weak supervision signal for opinion summarization rather than on rationalizing classifier predictions. There are also related approaches that aim to interpret an already-trained model, in contrast to Lei et al. (2016) and our approach where the rationale is"
P19-1284,D15-1075,0,0.0339015,"Missing"
P19-1284,P16-1139,0,0.0232646,"Missing"
P19-1284,P17-1152,0,0.016633,"0 0 man 0 0 77 21 0 0 0 0 walking 0 0 0 0 88 0 0 0 dog 0 0 0 0 0 0 86 0 Natural Language Inference In Natural language inference (NLI), given a premise sentence x(p) and a hypothesis sentence x(h) , the goal is to predict their relation y which can be contradiction, entailment, or neutral. As our dataset we use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015). Baseline. We use the Decomposable Attention model (DA) of Parikh et al. (2016).9 DA does not make use of LSTMs, but rather uses attention to find connections between the premise and the hy9 Better results e.g. Chen et al. (2017) and data sets for NLI exist, but are not the focus of this paper. Figure 6: Example of HardKuma attention between a premise (rows) and hypothesis (columns) in SNLI (cell values shown in multiples of 10−2 ). 2969 7 Related Work This work has connections with work on interpretability, learning from rationales, sparse structures, and rectified distributions. We discuss each of those areas. Interpretability. Machine learning research has been focusing more and more on interpretability (Gilpin et al., 2018). However, there are many nuances to interpretability (Lipton, 2016), and amongst them we fo"
P19-1284,P18-1031,0,0.0163499,"outhfeel is quite strong in the sense that you can get a good taste of it before you even swallow . Rationale Extractor pours a dark amber color with decent head that does not recede much . it ’s a tad too dark to see the carbonation , but fairs well . smells of roasted malts and mouthfeel is quite strong in the sense that you can get a good taste of it before you even swallow . Figure 1: Rationale extraction for a beer review. know why a prediction was made, we do not know if we can trust it. Introduction Neural networks are bringing incredible performance gains on text classification tasks (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). However, this power comes hand in hand with a desire for more interpretability, even though its definition may differ (Lipton, 2016). While it is useful to obtain high classification accuracy, with more data available than ever before it also becomes increasingly important to justify predictions. Imagine having to classify a large collection of documents, while verifying that the classifications make sense. It would be extremely time-consuming to read each document to evaluate the results. Moreover, if we do not What if the model could provide us th"
P19-1284,D18-1108,0,0.0624407,"ftmax itself, but in particular, they target relaxations such as sparsemax (Martins and Astudillo, 2016) which, unlike softmax, are sparse (i.e. produce vectors of probability values with components that evaluate to exactly 0). Their activation functions are themselves solutions to convex optimization problems, to which they provide efficient forward and backward passes. The technique can be seen as a deterministic sparsely activated layer which they use as a drop-in replacement to standard attention mechanisms. In contrast, in this paper we focus on binary outcomes rather than K-valued ones. Niculae et al. (2018) extend the framework to structured discrete spaces where they learn sparse parameterizations of discrete latent models. In this context, parameter estimation requires exact marginalization of discrete variables or gradient estimation via REINFORCE. They show that oftentimes distributions are sparse enough to enable exact marginal inference. Peng et al. (2018) propose SPIGOT, a proxy gradient to the non-differentiable arg max operator. This proxy requires an arg max solver (e.g. Viterbi for structured prediction) and, like the straight-through estimator (Bengio et al., 2013), is a biased estim"
P19-1284,D16-1244,0,0.101556,"Missing"
P19-1284,P18-1173,0,0.0743277,"Missing"
P19-1284,N18-1202,0,0.0135614,"in the sense that you can get a good taste of it before you even swallow . Rationale Extractor pours a dark amber color with decent head that does not recede much . it ’s a tad too dark to see the carbonation , but fairs well . smells of roasted malts and mouthfeel is quite strong in the sense that you can get a good taste of it before you even swallow . Figure 1: Rationale extraction for a beer review. know why a prediction was made, we do not know if we can trust it. Introduction Neural networks are bringing incredible performance gains on text classification tasks (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). However, this power comes hand in hand with a desire for more interpretability, even though its definition may differ (Lipton, 2016). While it is useful to obtain high classification accuracy, with more data available than ever before it also becomes increasingly important to justify predictions. Imagine having to classify a large collection of documents, while verifying that the classifications make sense. It would be extremely time-consuming to read each document to evaluate the results. Moreover, if we do not What if the model could provide us the most important part"
P19-1284,N16-3020,0,0.706564,"tributional word vectors to model-theoretic semantic vectors. Similarly to Lei et al. (2016), Titov and McDonald (2008) extract informative fragments of text by jointly training a classifier and a model predicting a stochastic mask, while relying on Gibbs sampling to do so. Their focus is on using the sentiment labels as a weak supervision signal for opinion summarization rather than on rationalizing classifier predictions. There are also related approaches that aim to interpret an already-trained model, in contrast to Lei et al. (2016) and our approach where the rationale is jointly modeled. Ribeiro et al. (2016) make any classifier interpretable by approximating it locally with a linear proxy model in an approach called LIME, and Alvarez-Melis and Jaakkola (2017) propose a framework that returns input-output pairs that are causally related. Learning from rationales. Our work is different from approaches that aim to improve classification using rationales as an additional input (Zaidan et al., 2007; Zaidan and Eisner, 2008; Zhang et al., 2016). Instead, our rationales are latent and we are interested in uncovering them. We only use annotated rationales for evaluation. Sparse layers. Also arguing for e"
P19-1284,D08-1004,0,\N,Missing
P19-1284,P15-1144,0,\N,Missing
P19-1284,N07-1033,0,\N,Missing
P19-1284,P08-1036,1,\N,Missing
P19-1284,D13-1170,0,\N,Missing
P19-1284,N19-1423,0,\N,Missing
P19-1642,W18-6402,0,0.265179,"ng image features in addition to only conditioning on them, (ii) imposing a constraint on the KL term to promote models with nonnegligible mutual information between inputs and latent variable, and (iii) by training on additional target-language image descriptions (i.e. synthetic data). 1 Introduction Multi-modal machine translation (MMT) is an exciting novel take on machine translation (MT) where we are interested in learning to translate sentences in the presence of visual input (mostly images). In the last three years there have been shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) where many research groups proposed different techniques to integrate images into MT, e.g. Caglayan et al. (2017); Libovick´y and Helcl (2017). Most MMT models expand neural machine translation (NMT) architectures (Sutskever et al., 2014; Bahdanau et al., 2015) to additionally condition on an image in order to compute the likelihood of a translation in context. This gives the model a chance to exploit correlations in visual and language data, but also means that images must be available at test time. An exception to this rule is the work of Toyama et al. (2016) who exploit the framework of co"
P19-1642,W17-4755,0,0.0170981,"a bilingual BPE model with 10k merge operations (Sennrich et al., 2016b). We quantitatively evaluate translation quality using case-insensitive and tokenized outputs in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF3 (Popovi´c, 2015), and BEER (Stanojevi´c and Sima’an, 2014). By using these, we hope to include word-level metrics which are traditionally used by the MT community (i.e. BLEU and METEOR), as well as more recent metrics which operate at the character level and that better correlate with human judgements of translation quality (i.e. chrF3 and BEER) (Bojar et al., 2017). 3.1 Datasets The Flickr30k dataset (Young et al., 2014) consists of images from Flickr and their English descriptions. We use the translated Multi30k (M30kT ) dataset (Elliott et al., 2016), i.e. an extension of Flickr30k where for each image one of its English descriptions was translated into German by a professional translator. Training, validation and test sets contain 29k, 1014 and 1k images respectively, each accompanied by the original English sentence and its translation into German. In addition to the test set released for the first run of the multimodal translation shared task (Elli"
P19-1642,W17-4746,0,0.415657,"Missing"
P19-1642,W16-3210,0,0.36975,"Missing"
P19-1642,I17-1014,0,0.266888,"Missing"
P19-1642,P16-1227,0,0.0232106,"act that multi-modal machine translation was only recently addressed by the MT community by means of a shared task (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Nevertheless, we now discuss relevant variational and deterministic multi-modal MT models in the literature. Fully supervised MMT models. All submissions to the three runs of the multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) model conditional probabilities directly without latent variables. Perhaps the first MMT model proposed prior to these shared tasks is that of Hitschler et al. (2016), who used image features to re-rank translations of image descriptions generated by a phrase-based statistical MT model (PBSMT) and reported significant improvements. Shah et al. (2016) propose a similar model where image logits are used to rerank the output of PBSMT. Global image features, i.e. features computed over an entire image (such as pool5 ResNet-50 features used in this work), have been directly used as “tokens” in the source sentence, to initialise encoder RNN hidden states, or as additional information used to initialise the decoder RNN states (Huang et al., 2016; Libovick´y et al"
P19-1642,P82-1020,0,0.800234,"Missing"
P19-1642,W16-2360,0,0.0972461,"s is that of Hitschler et al. (2016), who used image features to re-rank translations of image descriptions generated by a phrase-based statistical MT model (PBSMT) and reported significant improvements. Shah et al. (2016) propose a similar model where image logits are used to rerank the output of PBSMT. Global image features, i.e. features computed over an entire image (such as pool5 ResNet-50 features used in this work), have been directly used as “tokens” in the source sentence, to initialise encoder RNN hidden states, or as additional information used to initialise the decoder RNN states (Huang et al., 2016; Libovick´y et al., 2016; Calixto and Liu, 2017). On the other hand, spatial visual features, i.e. local features that encode different parts of the image separately in different vectors, have been used in doubly-attentive models where there is one attention mechanism over the source RNN hidden states and another one over the image features (Caglayan et al., 2016; Calixto et al., 2017). Finally, Caglayan et al. (2017) proposed to interact image features with target word embeddings, more specifically to perform an element-wise multiplication of the (projected) global image features and the tar"
P19-1642,P17-4012,0,0.180426,"(Kingma and Welling, 2014; Titsias and L´azaro-Gredilla, 2014). In addition, for two Gaussians the KL term in the ELBO (6) can be computed in closed form (Kingma and Welling, 2014, Appendix B). Altogether, we can obtain a reparameterised gradient estimate of the ELBO, we use a single sample estimate of the first term, and count on stochastic gradient descent to attain a local optimum of (6). Architecture All of our parametric functions are neural network architectures. In particular, fπ is a standard sequence-to-sequence architecture with attention and a softmax output. We build upon OpenNMT (Klein et al., 2017), which we modify 6394 slightly by providing z as additional input to the target-language decoder at each time step. Location layers fµ , fν and gu , and scale layers fσ and gs , are feed-forward networks with a single ReLU hidden layer. Furthermore, location layers have a linear output while scale layers have a softplus output. For the generative model, fµ and fσ transform the average source-language encoder hidden state. We let the inference model condition on sourcelanguage encodings without updating them, and we use a target-language bidirectional LSTM encoder in order to also condition on"
P19-1642,P17-2031,0,0.21491,"Missing"
P19-1642,W16-2361,0,0.0391454,"Missing"
P19-1642,D15-1166,0,0.206467,"Missing"
P19-1642,P02-1040,0,0.106727,"not improve BLEU4 scores on the validation set for 10 epochs or more. We report mean and standard deviation over 4 independent runs for all models we trained ourselves (NMT, VMMTF , VMMTC ), and other baseline results are the ones reported in the authors’ publications (Toyama et al., 2016; Elliott and K´ad´ar, 2017). We preprocess our data by tokenizing, lowercasing, and converting words to subword tokens using a bilingual BPE model with 10k merge operations (Sennrich et al., 2016b). We quantitatively evaluate translation quality using case-insensitive and tokenized outputs in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF3 (Popovi´c, 2015), and BEER (Stanojevi´c and Sima’an, 2014). By using these, we hope to include word-level metrics which are traditionally used by the MT community (i.e. BLEU and METEOR), as well as more recent metrics which operate at the character level and that better correlate with human judgements of translation quality (i.e. chrF3 and BEER) (Bojar et al., 2017). 3.1 Datasets The Flickr30k dataset (Young et al., 2014) consists of images from Flickr and their English descriptions. We use the translated Multi30k (M30kT ) dataset (Elliott et al., 20"
P19-1642,W15-3049,0,0.0265868,"Missing"
P19-1642,P18-1115,1,0.899788,"Missing"
P19-1642,P16-1009,0,0.263043,"on BLEU4, and use the best performing model on the validation set to translate test data. Moreover, we halt training if the model does not improve BLEU4 scores on the validation set for 10 epochs or more. We report mean and standard deviation over 4 independent runs for all models we trained ourselves (NMT, VMMTF , VMMTC ), and other baseline results are the ones reported in the authors’ publications (Toyama et al., 2016; Elliott and K´ad´ar, 2017). We preprocess our data by tokenizing, lowercasing, and converting words to subword tokens using a bilingual BPE model with 10k merge operations (Sennrich et al., 2016b). We quantitatively evaluate translation quality using case-insensitive and tokenized outputs in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF3 (Popovi´c, 2015), and BEER (Stanojevi´c and Sima’an, 2014). By using these, we hope to include word-level metrics which are traditionally used by the MT community (i.e. BLEU and METEOR), as well as more recent metrics which operate at the character level and that better correlate with human judgements of translation quality (i.e. chrF3 and BEER) (Bojar et al., 2017). 3.1 Datasets The Flickr30k dataset (Young et al.,"
P19-1642,P16-1162,0,0.424596,"on BLEU4, and use the best performing model on the validation set to translate test data. Moreover, we halt training if the model does not improve BLEU4 scores on the validation set for 10 epochs or more. We report mean and standard deviation over 4 independent runs for all models we trained ourselves (NMT, VMMTF , VMMTC ), and other baseline results are the ones reported in the authors’ publications (Toyama et al., 2016; Elliott and K´ad´ar, 2017). We preprocess our data by tokenizing, lowercasing, and converting words to subword tokens using a bilingual BPE model with 10k merge operations (Sennrich et al., 2016b). We quantitatively evaluate translation quality using case-insensitive and tokenized outputs in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF3 (Popovi´c, 2015), and BEER (Stanojevi´c and Sima’an, 2014). By using these, we hope to include word-level metrics which are traditionally used by the MT community (i.e. BLEU and METEOR), as well as more recent metrics which operate at the character level and that better correlate with human judgements of translation quality (i.e. chrF3 and BEER) (Bojar et al., 2017). 3.1 Datasets The Flickr30k dataset (Young et al.,"
P19-1642,W16-2363,0,0.0195872,"less, we now discuss relevant variational and deterministic multi-modal MT models in the literature. Fully supervised MMT models. All submissions to the three runs of the multi-modal MT shared tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) model conditional probabilities directly without latent variables. Perhaps the first MMT model proposed prior to these shared tasks is that of Hitschler et al. (2016), who used image features to re-rank translations of image descriptions generated by a phrase-based statistical MT model (PBSMT) and reported significant improvements. Shah et al. (2016) propose a similar model where image logits are used to rerank the output of PBSMT. Global image features, i.e. features computed over an entire image (such as pool5 ResNet-50 features used in this work), have been directly used as “tokens” in the source sentence, to initialise encoder RNN hidden states, or as additional information used to initialise the decoder RNN states (Huang et al., 2016; Libovick´y et al., 2016; Calixto and Liu, 2017). On the other hand, spatial visual features, i.e. local features that encode different parts of the image separately in different vectors, have been used"
P19-1642,W16-2346,0,0.448909,"Missing"
P19-1642,D14-1025,0,0.0580001,"Missing"
P19-1642,Q14-1006,0,0.0874328,"et al., 2016b). We quantitatively evaluate translation quality using case-insensitive and tokenized outputs in terms of BLEU4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), chrF3 (Popovi´c, 2015), and BEER (Stanojevi´c and Sima’an, 2014). By using these, we hope to include word-level metrics which are traditionally used by the MT community (i.e. BLEU and METEOR), as well as more recent metrics which operate at the character level and that better correlate with human judgements of translation quality (i.e. chrF3 and BEER) (Bojar et al., 2017). 3.1 Datasets The Flickr30k dataset (Young et al., 2014) consists of images from Flickr and their English descriptions. We use the translated Multi30k (M30kT ) dataset (Elliott et al., 2016), i.e. an extension of Flickr30k where for each image one of its English descriptions was translated into German by a professional translator. Training, validation and test sets contain 29k, 1014 and 1k images respectively, each accompanied by the original English sentence and its translation into German. In addition to the test set released for the first run of the multimodal translation shared task (Elliott et al., 2016), henceforth test2016, we also use test2"
P19-1642,D16-1050,0,0.116006,"late archway correctly; the NMT baseline translates it as “scheibe” (disk) and “bogen” (bow), and VMMTC also incorrectly translates it as “bogen” (bow). However, VMMTC translates without errors when trained on additional back-translated data, i.e. “torbogen” (archway). In the second example, the NMT baseline translates bay as “luft” (air) or “meer” (sea), whereas VMMTF translates it as “bucht” (bay) or “wellen” (waves) and VMMTC always as “bucht” (bay). additional back-translated data. 4 Related work Even though there has been growing interest in variational approaches to machine translation (Zhang et al., 2016; Schulz et al., 2018; Shah and Barber, 2018; Eikema and Aziz, 2019) and to tasks that integrate vision and language, e.g. image description generation (Pu et al., 2016; Wang et al., 2017), relatively little attention has been dedicated to variational models for multi-modal translation. This is partly due to the fact that multi-modal machine translation was only recently addressed by the MT community by means of a shared task (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Nevertheless, we now discuss relevant variational and deterministic multi-modal MT models in the liter"
R11-1014,2005.mtsummit-papers.11,0,0.00530899,"revised to guarantee the largest possible set of 1-1 correspondences and also to correct mistakes that resulted from the particularities of aligning subtitles. After the correction of the sentence alignment, four episodes were randomly chosen and kept aside as our test data. Statistics about the resulting sentence-aligned parallel corpus are reported in Table 1. Armstrong et al. (2006) train an EBMT system in two scenarios: i) using a homogenous corpus compiled exclusively with DVD subtitles, and ii) using a heterogenous corpus compiled with a mix of subtitles and sentences from the Europarl (Koehn, 2005). The results show that a homogenous setting leads to better translations. Flanagan (2009) extends the work of Armstrong et al. (2006) by using larger parallel corpora of subtitles from multiple genres. A subjective evaluation querying users who watched movies containing the translated subtitles in terms of intelligibility and acceptability was performed. Results show an average performance (∼ 3 on a 1-6 scale). Melero et al. (2006) combine a black-box MT system and a TM using a corpus of newspaper articles and United Nation texts to translate subtitles. They find that MT+TM performs significa"
R11-1014,2006.tc-1.10,0,0.559222,"and iv) XML-like tags. Subsequently, the corpus was automatically aligned at the sentence level using heuristics aimed at maximizing the time overlap between the source and target subtitles. The sentence alignment was revised to guarantee the largest possible set of 1-1 correspondences and also to correct mistakes that resulted from the particularities of aligning subtitles. After the correction of the sentence alignment, four episodes were randomly chosen and kept aside as our test data. Statistics about the resulting sentence-aligned parallel corpus are reported in Table 1. Armstrong et al. (2006) train an EBMT system in two scenarios: i) using a homogenous corpus compiled exclusively with DVD subtitles, and ii) using a heterogenous corpus compiled with a mix of subtitles and sentences from the Europarl (Koehn, 2005). The results show that a homogenous setting leads to better translations. Flanagan (2009) extends the work of Armstrong et al. (2006) by using larger parallel corpora of subtitles from multiple genres. A subjective evaluation querying users who watched movies containing the translated subtitles in terms of intelligibility and acceptability was performed. Results show an av"
R11-1014,P02-1040,0,0.100466,"etting leads to better translations. Flanagan (2009) extends the work of Armstrong et al. (2006) by using larger parallel corpora of subtitles from multiple genres. A subjective evaluation querying users who watched movies containing the translated subtitles in terms of intelligibility and acceptability was performed. Results show an average performance (∼ 3 on a 1-6 scale). Melero et al. (2006) combine a black-box MT system and a TM using a corpus of newspaper articles and United Nation texts to translate subtitles. They find that MT+TM performs significantly better than MT in terms of BLEU (Papineni et al., 2002) in an English-Spanish task. For EnglishCzech they compare HT against PE in terms of time. The comparison is somewhat inconclusive as the HT and PE were compared using different texts and a single human translator. Volk (2008) uses a large proprietary corpus of subtitles (5 million sentences) to train an SMT system. The author reports BLEU: i) using a single reference, and ii) using the translations produced by six post-editors. The author finds that SMT outputs can still be acceptable translations even though they do not exactly match the HT as long as they lie within 5 keystrokes, distance f"
R11-1014,W00-0506,0,0.317615,"Missing"
R11-1014,2011.eamt-1.12,1,0.262575,"produced by six post-editors. The author finds that SMT outputs can still be acceptable translations even though they do not exactly match the HT as long as they lie within 5 keystrokes, distance from it. Similarly to prior work we compile a corpus of DVD subtitles in order to perform in-domain subtitle translations. We train our own SMT model and compare it against other MT approaches and a TM. Our main goal is to demonstrate that, regardless of the MT/TM strategy, PE is faster than HT without a loss in quality. For that, we design a comprehensive evaluation: i) objectively in terms of time (Specia, 2011), ii) subjectively using well specified scoring guidelines (Specia, 2011), and iii) automatically in terms of BLEU using single and multiple references. As a by-product, a comparison between different translation approaches is performed. 3 Corpus en tokens pt tokens Sentence pairs Training 720,845 613,201 76,295 Test 17,796 14,000 2,379 Table 1: Token and sentence numbers in the parallel corpus 4 Experiments This section describes how the effort to translate subtitles from scratch was compared to the effort to post-edit translations automatically obtained through different tools. 4.1 Systems W"
R11-1014,P07-2045,0,\N,Missing
R11-1031,P98-2127,0,0.0253836,"preferences. act 2 attribute 1 communication 1 food 1,2 motive 1 living thing plant 2 quantity shape 1,2 substance 1 tool 1 solid abstraction animal body part event 3 group natural object person 1,2 possession relation 1 state 2 time device 1 liquid thing Aiming at producing an SRL system with features that can be easily extracted for different languages and also to provide additional lexical information, we expanded chunks’ heads with similar words. For every head word on its base form, regardless its part-of-speech, we selected the 10-most similar words from Lin’s distributional thesaurus (Lin, 1998). Lin’s thesaurus is an automatically constructed resource that maps words to similar concepts in terms of a distributional lexical similarity metric. The last column in Figure 2 exemplifies similar words retrieved for some chunks. 3.3 artifact cognition feeling location physical object phenomenon process 6 relation 2,3,6 state 6 vehicle 1 garment 1 physical entity Motivated by VerbNet’s (Kipper et al., ) selectional restrictions, we manually selected the 38 categories listed in Figure 1 and mapped them into the WordNet lexicon. We chose general hypernyms in order to avoid fine-grained sense d"
R11-1031,W05-0629,0,0.0202164,"n, or semantic role labels, to the syntactic annotation of the Penn Treebank. The test set used was CoNLL-2005 (Carreras and M`arquez, 2005), which has predicate-argument information for approximately 2.5K sentences from the Wall Street Journal (WSJ) (in-domain evaluation) and 450 sentences from Brown corpus (out-of-domain evaluation). Table 1 presents the overall results for the SRL taskon the in-domain test set (WSJ), and Table 2 presents the same analysis on the out-of-domain test set (Brown). They also show CoNLL 2005’s baseline (Carreras and M`arquez, 2005) and a similar chunk-based SRL (Mitsumori et al., 2005). The figures refer to the weighted average of the performance in correctly classifying target predicates (V), their core arguments (A0 to A5) and their modifiers. Tables 1 and 2 show that the proposed lexicalized features yielded an important gain in Templates The CRF++ toolkit allows the definition of templates over the basic feature space, that is, rules that combine multiple features. Templates are expanded token-by-token, that is, for every CRF token the original feature set is used to create additional features. Templates can be based on features only, referred to as unigram templates, o"
R11-1031,J05-1004,0,0.0478113,"10-most Similar Words: as described in 3.2, henceforth referred to as 10sim 3.4 Results We experimented with different configurations of features in order to understand the impact of their contribution. The baseline model (B) contains all features apart from the selectional preferences and the 10-most similar words, the main contributions of this paper. We added the selectional preferences (B+sp) and the most similar words (B+10sim) separately, and built a final model containing all the features (B+10sim+sp), as described in Section 3. Training was performed using the whole Proposition Bank (Palmer et al., 2005) (except Section 23, which is part of the test set). The Proposition Bank adds a layer of predicate-argument information, or semantic role labels, to the syntactic annotation of the Penn Treebank. The test set used was CoNLL-2005 (Carreras and M`arquez, 2005), which has predicate-argument information for approximately 2.5K sentences from the Wall Street Journal (WSJ) (in-domain evaluation) and 450 sentences from Brown corpus (out-of-domain evaluation). Table 1 presents the overall results for the SRL taskon the in-domain test set (WSJ), and Table 2 presents the same analysis on the out-of-doma"
R11-1031,N04-1030,0,0.240475,"s available. 1 Introduction Identifying the relations that words or groups of words have with verbs in a sentence constitutes an important step for many applications in Natural Language Processing (NLP). This is addressed by the field of Semantic Role Labeling (SRL). SRL has been shown to contribute to many NLP applications, such as Information Extraction, Question Answering and Machine Translation. Most of the SRL approaches operate via two consecutive steps: i) the identification of the arguments of a target predicate and ii) the classification of those arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004). Alternatively, graph models can rely on the sequential nature of the shallow 226 Proceedings of Recent Advances in Natural Language Processing, pages 226–232, Hissar, Bulgaria, 12-14 September 2011. extracted from other sources of structured information such as DBpedia1 . The paper is structured as follows: in Section 2 we give an overview of the related work; in Section 3 we describe the proposed system; in Section 4 we present the results of our experiments. Finally, in Section 5 we present our conclusions and some directions for future work. 2 lemmas and named entities. Wordforms and lemm"
R11-1031,J08-2006,0,0.12042,"While parse trees allow a set of very informative path-based, structural features, chunks can provide more reliable annotations. Hacioglu et al. (2004) propose the use of base phrases as data representation using Support Vector Machines in order to perform a single argument classification step. Roth and tau Yih (2005) use the same sort of representation with Conditional Random Fields (CRF) as learning algorithm, motivated by the sequential nature of the task. Cohn and Blunsom (2005) use CRF to perform SRL in a single identification/classification step based on features from constituent trees. Pradhan et al. (2008) point out the lack of semantic features as the bottleneck in argument role classification, a task closely-related to that of word sense disambiguation. Shallow lexical features such as word forms and word lemmas are very sparse. Although named-entity categories have been proposed to alleviate this sparsity problem, they only apply to a fraction of the arguments’ words. In this paper we propose the addition of other forms of lexical knowledge in order to address this problem. The proposed SRL system tags data in a joint identification/classification step using CRF as the learning algorithm. Th"
R11-1031,W97-0209,0,0.123426,"ns and some directions for future work. 2 lemmas and named entities. Wordforms and lemmas make very sparse features; while more general features such as named-entities generalize just a fraction of all the nouns that verbs might take as arguments. To improve argument classification, Zapirain et al. (2010) propose to merge selectional preferences into a state-of-the-art SRL system. They define selectional preference as a similarity score between the predicate, the argument role and the constituent head word. The similarity is computed using different strategies: i) Resnik’s similarity measure (Resnik, 1997) based on WordNet (Miller et al., 1990), and ii) different corpus-based distributional similarity metrics, considering both first and second order similarities. They report consistent gains on argument classification by combining models based on different similarity metrics. In this work we propose to add lexical information in a different fashion. Instead of measuring the similarity between the argument head word and the predicate we: i) understand selectional preferences as categories, such as the usual named-entities, however covering any sort of noun; ii) provide additional evidence of lex"
R11-1031,W05-0620,0,0.0941713,"Missing"
R11-1031,W05-0622,0,0.136825,"eously (Roth and tau Yih, 2005; Cohn and Blunsom, 2005). Features for SRL are usually extracted from chunks or constituent parse trees. While parse trees allow a set of very informative path-based, structural features, chunks can provide more reliable annotations. Hacioglu et al. (2004) propose the use of base phrases as data representation using Support Vector Machines in order to perform a single argument classification step. Roth and tau Yih (2005) use the same sort of representation with Conditional Random Fields (CRF) as learning algorithm, motivated by the sequential nature of the task. Cohn and Blunsom (2005) use CRF to perform SRL in a single identification/classification step based on features from constituent trees. Pradhan et al. (2008) point out the lack of semantic features as the bottleneck in argument role classification, a task closely-related to that of word sense disambiguation. Shallow lexical features such as word forms and word lemmas are very sparse. Although named-entity categories have been proposed to alleviate this sparsity problem, they only apply to a fraction of the arguments’ words. In this paper we propose the addition of other forms of lexical knowledge in order to address"
R11-1031,W04-3212,0,0.0311835,", however covering any sort of noun; ii) provide additional evidence of lexical similarity by expanding the head of any base phrase to its 10-most similar concepts retrieved from a distributional thesaurus. Related Work In most previous work, improvements in SRL come from new features used either in the argument identification or in the argument classification step. It is common to train different binary classifiers to perform each of the two steps separately (Gildea and Jurafsky, 2002; Pradhan et al., 2004). In the first step chunks are identified as potential arguments of a given predicate. Xue and Palmer (2004) apply syntax-driven heuristics in order to prune unlikely candidates. In the second step, the selected arguments are individually labeled with semantic roles. Pradhan et al. (2004) use features such as the role of the preceding argument in order to create a dependency between the classification of different arguments. Hacioglu et al. (2004) propose a single identification/classification step using SVM by labeling chunks within a window centered in the predicated from left to right. The authors propose to label base phrases instead of constituents in a full parse tree. They also change the dat"
R11-1031,J02-3001,0,0.432526,"ittle syntactic knowledge is available. 1 Introduction Identifying the relations that words or groups of words have with verbs in a sentence constitutes an important step for many applications in Natural Language Processing (NLP). This is addressed by the field of Semantic Role Labeling (SRL). SRL has been shown to contribute to many NLP applications, such as Information Extraction, Question Answering and Machine Translation. Most of the SRL approaches operate via two consecutive steps: i) the identification of the arguments of a target predicate and ii) the classification of those arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004). Alternatively, graph models can rely on the sequential nature of the shallow 226 Proceedings of Recent Advances in Natural Language Processing, pages 226–232, Hissar, Bulgaria, 12-14 September 2011. extracted from other sources of structured information such as DBpedia1 . The paper is structured as follows: in Section 2 we give an overview of the related work; in Section 3 we describe the proposed system; in Section 4 we present the results of our experiments. Finally, in Section 5 we present our conclusions and some directions for future work. 2 lemmas and named entit"
R11-1031,N10-1058,0,0.0524722,"Missing"
R11-1031,W04-2416,0,0.0258518,"ument classification step. It is common to train different binary classifiers to perform each of the two steps separately (Gildea and Jurafsky, 2002; Pradhan et al., 2004). In the first step chunks are identified as potential arguments of a given predicate. Xue and Palmer (2004) apply syntax-driven heuristics in order to prune unlikely candidates. In the second step, the selected arguments are individually labeled with semantic roles. Pradhan et al. (2004) use features such as the role of the preceding argument in order to create a dependency between the classification of different arguments. Hacioglu et al. (2004) propose a single identification/classification step using SVM by labeling chunks within a window centered in the predicated from left to right. The authors propose to label base phrases instead of constituents in a full parse tree. They also change the data representation of the roles to IOB2 notation which is more adequate to shallow parsing. In the proposed representation, the features of base phrases include those that can be extracted from their head words as well as some chunk oriented features (e.g the distance of the chunk to the predicate). Cohn and Blunsom (2005) approach induces an"
R11-1031,N07-1070,0,\N,Missing
R11-1031,C98-2122,0,\N,Missing
S10-1024,2005.mtsummit-papers.11,0,0.034221,"recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, 117 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117–122, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 2 Resources 2.1 The process to generate the corpus-based dictionary for USPwlv is described in Section 4. Parallel corpus The English-Spanish part of Europarl (Koehn, 2005), a parallel corpus from the European Parliament proceedings, was used as a source of sentence level aligned data. The nearly 1.7M sentence pairs of English-Spanish translations, as provided by the Fourth Workshop on Machine Translation (WMT091 ), sum up to approximately 48M tokens in each language. Europarl was used both to train the SMT system and to generate dictionaries based on inter-language mutual information. 2.2 2.3 Pre-processing techniques The Europarl parallel corpus was tokenized and lowercased using standard tools provided by the WMT09 competition. Additionally, the sentences tha"
S10-1024,2009.eamt-1.15,0,0.0784297,"ovided by a dictionary. USPwlv System For each source word occurring in the context of a specific sentence, this system uses a linear combination of features to rank the options from an automatically built English-Spanish dictionary. For the best subtask, the translation ranked first is chosen, while for the oot subtask, the 10 best ranked translations are used without repetition. The building of the dictionary, the features used and the learning scheme are described in what follows. Dictionary Building The dictionary building is based on the concept of inter-language Mutual Information (MI) (Raybaud et al., 2009). It consists in detecting which words in a source-language sentence trigger the appearance of other words in its target-language translation. The inter-language MI in Equation 3 can be defined for pairs of source (s) and target (t) words by observing their occurrences at the sentence level in a parallel, sentence aligned corpus. Both simple (Equation 1) and joint distributions (Equation 2) were built based on the English-Spanish Europarl corpus using its Lemma.pos version (Section 2.3). pl (x) = countl (x) T otal (1) fen,es (s, t) (2) T otal   pen,es (s, t) (3) M I(s, t) = pen,es (s, t)log"
S10-1024,P07-1006,1,0.845329,"Substitution task in Semeval-2010 (Mihalcea et al., 2010) is to find the best (best subtask) Spanish translation or the 10-best (oot subtask) translations for 100 different English source words depending on their context of occurrence. Source words include nouns, adjectives, adverbs and verbs. 1, 000 occurrences of such words are given along with a short context (a sentence). This task resembles that of Word Sense Disambiguation (WSD) within Machine Translation (MT). A few approaches have recently been proposed using standard WSD features to learn models using translations instead of senses (Specia et al., 2007; Carpuat and Wu, 2007; Chan and Ng, 2007). In such approaches, the global WSD score is added as a feature to statistical MT systems, 117 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117–122, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 2 Resources 2.1 The process to generate the corpus-based dictionary for USPwlv is described in Section 4. Parallel corpus The English-Spanish part of Europarl (Koehn, 2005), a parallel corpus from the European Parliament proceedings, was used as a source of sentence level aligned dat"
S10-1024,D07-1007,0,\N,Missing
S10-1024,S10-1002,0,\N,Missing
S12-1100,W10-1703,0,0.0694003,"Missing"
S12-1100,W04-3205,0,0.0753091,"Missing"
S12-1100,W10-1751,0,0.027057,"d Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment. The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric. Finally, we use METEOR (Denkowski and Lavie, 2010), also a common metric for machine translation evaluation, that also computes inexact word overlap as at way of measuring the impact of our semantic metrics. The lexical and syntactic metrics complement the semantic metrics in dealing with the phenomena observed in the task’s dataset. For instance, from the MSRvid dataset: S1 Two men are playing football. S2 Two men are practicing football. In this case, as typical of paraphrasing, the situation and participants are the same while the surface realization differs, but playing can be considered similar to practicing. From the SMT-eur dataset: S3"
S12-1100,P98-2127,0,0.0491073,"Missing"
S12-1100,P02-1040,0,0.085034,"clusive. 1 Introduction We describe the UOW submissions to the Semantic Textual Similarity (STS) task at SemEval-2012. Our systems are based on combining similarity scores as features using a regression algorithm to predict the degree of semantic equivalence between a pair of sentences. We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic. The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words. The syntactic metric computes BLEU (Papineni et al., 2002), a machine translation evaluation metric, over a labels of basephrases (chunks). Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact"
S12-1100,W11-2112,1,0.725819,"sion algorithm to predict the degree of semantic equivalence between a pair of sentences. We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic. The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words. The syntactic metric computes BLEU (Papineni et al., 2002), a machine translation evaluation metric, over a labels of basephrases (chunks). Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment. The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric. Finally, we use METEOR (Denkowski and Lavie, 2010), also a comm"
S12-1100,C98-2122,0,\N,Missing
S12-1100,W11-2100,0,\N,Missing
W11-2112,W05-0909,0,0.381717,"urus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering bo"
W11-2112,W10-1703,0,0.157528,"(vh = order,vr = book): Ah = {A0, A1, AM-LOC} Ar = {A0, A1, AM-LOC} 7. different word forms: expand the representation: HA1 = {ski, stays} and RA1 = {ski, holidays} expand to: HA1 = {{ski},{stays, remain... journey...}} RA1 = {{ski},{holidays, vacations, trips... journey...}} argument score = 0.5 8. similarly to HAM −LOC and RAM −LOC argument score = 0.72 10. A(H, R) = 0.74 2 Results We set the weights α and β by experimental testing to α = 1 and β = 0.25. The lexical component weight is prioritized because it has shown a good average Kendall’s tau correlation (0.23) on a development dataset (Callison-Burch et al., 2010). Table 1 shows the correlation of the lexical component with human judgments for a number of language pairs. Metric Lexical 6. exact matches: HA0 = {people} and RA0 = {people} argument score = 1 1+0.5+0.72 3 4 Table 1: Kendall’s tau segment-level correlation of the lexical component with human judgments 5. Ah ∩ Ar = {A0, A1, AM-LOC} 9. verb score (order, book) = for and reserve (something for someone else) in advance, etc.). Thus, a WordNet-based similarity measure would require disambiguating segments, an additional step and a possible source of errors. Second, a thresholds would need to be"
W11-2112,W05-0620,0,0.0644847,"Missing"
W11-2112,W04-3205,0,0.020109,"will penalize less the matching than POW. That is specially interesting when core arguments 118 get merged with modifiers due to bad semantic role labeling (e.g. [A0 I] [T bought] [A1 something to eat yesterday] instead of [A0 I] [T bought] [A1 something to eat] [AM-TMP yesterday]). P A(H, R) = v∈V verb score(Hv , Rv ) |Vr | (3) In the adequacy component, V is the set of verbs aligned between H and R, and |Vr |is the number of verbs in R. Hereafter the indexes h and r stand for hypothesis and reference translations, respectively. Verbs are aligned using VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004). A verb in the hypothesis vh is aligned to a verb in the reference vr if they are related according to the following heuristics: (i) the pair of verbs share at least one class in VerbNet; or (ii) the pair of verbs holds a relation in VerbOcean. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear. P verb score(Hv , Rv ) = a∈Ar ∩At arg score(Ha , Ra ) |Ar | (4) The similarity between the arguments of a verb pair (vh , vr ) in V is measured as defined in Equation (4), where Ah and At are the sets of labe"
W11-2112,W10-1751,0,0.419678,"s (core arguments and modifiers), and iii) matching predicate arguments using distributional semantics. TINE’s performance is comparable to that of previous metrics at segment level for several language pairs, with average Kendall’s tau correlation from 0.26 to 0.29. We show that the addition of the shallow-semantic component improves the performance of simple lexical matching strategies and metrics such as BLEU. 1 A number of other metrics have been proposed to address these limitations, for example, by allowing for the matching of synonyms or paraphrases of content words, such as in METEOR (Denkowski and Lavie, 2010). Other attempts have been made to capture whether the reference translation and hypothesis translations share the same meaning using shallow semantics, i.e., Semantic Role Labeling (Gim´enez and M´arquez, 2007). However, these are limited to the exact matching of semantic roles and their fillers. Introduction The automatic evaluation of Machine Translation (MT) is a long-standing problem. A number of metrics have been proposed in the last two decades, mostly measuring some form of matching between the MT output (hypothesis) and one or more human (reference) translations. However, most of thes"
W11-2112,W07-0738,0,0.211147,"Missing"
W11-2112,W10-1750,0,0.0470798,"Missing"
W11-2112,W10-1753,0,0.0330133,"Missing"
W11-2112,P98-2127,0,0.163445,"the semantic structure of the sentence and the content of the semantic elements. The metric uses SRLs such as in (Gim´enez and M´arquez, 2007). However, it analyses the content of predicates and arguments seeking for either exact or “similar” matches. The 116 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 116–122, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics inexact matching is based on the use of ontologies such as VerbNet (Schuler, 2006) and distributional semantics similarity metrics, such as Dekang Lin’s thesaurus (Lin, 1998) . In the remainder of this paper we describe some related work (Section 2), present our metric - TINE - (Section 3) and its performance compared to previous work (Section 4) as well as some further improvements. We then provide an analysis of these results and discuss the limitations of the metric (Section 5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lav"
W11-2112,W10-1754,0,0.0159783,"the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives and adverbs can be considered BE-Unigrams, while a BE-Bigram could be formed from a syntactic relation (e.g. subject+verb, verb+object). BEs can be lexically different, but semantically similar. Pad´o et al. (200"
W11-2112,N06-1006,0,0.0829905,"Missing"
W11-2112,P02-1040,0,0.0817216,"Missing"
W11-2112,W10-1755,0,0.0150449,"5) and present conclusions and future work (Section 6). 2 Related Work A few metrics have been proposed in recent years to address the problem of measuring whether a hypothesis and a reference translation share the same meaning. The most well-know metric is probably METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010). METEOR is based on a generalized concept of unigram matching between the hypothesis and the reference translation. Alignments are based on exact, stem, synonym, and paraphrase matches between words and phrases. However, the structure of the sentences is not considered. Wong and Kit (2010) measure word choice and word order by the matching of words based on surface forms, stems, senses and semantic similarity. The informativeness of matched and unmatched words is also weighted. Liu et al. (2010) propose to match bags of unigrams, bigrams and trigrams considering both recall and precision and F-measure giving more importance to recall, but also using WordNet synonyms. Tratz and Hovy (2008) use transformations in order to match short syntactic units defined as Basic Elements (BE). The BE are minimal-length syntactically well defined units. For example, nouns, verbs, adjectives an"
W11-2112,C98-2122,0,\N,Missing
W11-2136,P02-1040,0,0.0802485,"Missing"
W11-2136,W05-0908,0,0.073685,"Missing"
W11-2136,E09-3008,0,0.015054,"ent corpus. The parallel corpus was then tokenized and truecased. Additionally, for en-de, compound splitting of the German side of the corpus was performed using a frequency based method described in (Koehn and Knight, 2003). This method helps alleviate sparsity, reducing the size of the vocabulary by decomposing compounds into their base words. Recasing and detokenization, along with compound merging of the translations into German, were handled at post-processing stage. Compound merging was performed by finding the most likely sequences of words to be merged into previously seen compounds (Stymne, 2009). 3.1 Source Language Annotation For rule extraction, training and test, the English side of the corpus was annotated with Semantic Role Labels (SRL) using the toolkit SENNA2 , which also outputs POS and base-phrase (without prepositional attachment) tags. The resulting source language annotation was used to produce trees in order to build a tree-to-string model in Moses. 1 http://www.speech.sri.com/projects/ srilm/ 2 http://ml.nec-labs.com/senna/ S NP PRP he VBZ intends VP TO to NP VB donate DT this NN money PP TO to NP NN charity O PUNC , O CC but NP PRP he VBZ has VP RB VBD not decided NP W"
W11-2136,W06-3119,0,0.488725,"cialized vocabulary also leads to spurious ambiguity (Chiang, 2005). Syntax-based models are hierarchical models whose rules are constrained by syntactic information.The syntactic constraints have an impact in the rule extraction process, reducing drastically the number of rules available to the system. While this may be helpful to reduce ambiguity, it can lead to poorer performance (Ambati and Lavie, 2008). Motivated by the fact that syntactically constraining a hierarchical model can decrease translation quality, some attempts to overcome the problems at rule extraction time have been made. Venugopal and Zollmann (2006) propose a heuristic method to relax parse trees known as Syntax Augmented Machine Translation (SAMT). Significant gains are obtained by grouping nonterminals under categories when they do not span across syntactic constituents. Hoang and Koehn (2010) propose a soft syntaxbased model which combines the precision of a syntax-constrained model with the coverage of an 316 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 316–322, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics unconstrained hierarchical model. Instead of having h"
W11-2136,N09-2004,0,0.0563643,"nd, UK, July 30–31, 2011. 2011 Association for Computational Linguistics unconstrained hierarchical model. Instead of having heuristic strategies to combine nonterminals in a parse tree, whenever a rule cannot be retrieved because it does not span a constituent, the extraction procedure falls back to the hierarchical approach, retrieving a rule with unlabeled nonterminals. Performance gains are reported over standard hierarchical models using both full parse trees and shallow syntax. Moving beyond syntactic information, some attempts have recently been made to add semantic annotations to SMT. Wu and Fung (2009) present a two-pass model to incorporate semantic information to the phrase-based SMT pipeline. The method performs conventional translation in a first step, followed by a constituent reordering step seeking to maximize the cross-lingual match of the semantic role labels of the translation and source sentences. Liu and Gildea (2010) add features extracted from the source sentences annotated with semantic role labels in a tree-to-string SMT model. They modify a syntax-based SMT system in order to penalize/reward role reordering and role deletion. The input sentence is parsed for semantic roles"
W11-2136,C10-1081,0,\N,Missing
W11-2136,W10-1715,0,\N,Missing
W11-2136,2010.amta-papers.7,0,\N,Missing
W11-2136,W10-1761,0,\N,Missing
W11-2136,E03-1076,0,\N,Missing
W11-2136,P07-2045,0,\N,Missing
W11-2136,W07-0733,0,\N,Missing
W11-2136,P05-1033,0,\N,Missing
W11-2136,2009.iwslt-papers.4,0,\N,Missing
W11-4533,J93-2003,0,0.0169584,"wercase and tokenize them; 2. For every pt document we replace the original words by their n-best translations according to our bilingual dictionary; 3. We represent documents as distributional profiles (DP), that is, a vector that contains a word w and its frequency f in the document: profile : w 7→ f ; 4. For every possible document pair we compute the cosine similarity of their DPs. 5. We align each en document to its best pt candidate if the similarity score is above a given threshold. We compiled bilingual dictionaries using the small corpus Fapesp-v1 running 5 iterations of IBM Model 1 [Brown et al. 1993]. IBM Model 1 estimates a word-based translation probability distribution from a sentence aligned corpus. We set the number of best translations from the dictionary to 3 and the similarity metric threshold to 0.15 after a short round of evaluation described in Section 3. The same procedure was used to align pt-es articles. We obtained 2, 675 pt-en and 2, 668 pt-es parallel texts. 2.3. Sentence Alignment We use an implementation of TCA specially written for aligning Brazilian Portuguese to a foreign text [Caseli and Nunes 2003]. While the most desired alignment type is the substitution (1-1),"
W11-4533,P05-1033,0,0.042189,"n automatically aligned at document- and sentence-level. The resulting corpora contain about 2,700 parallel documents totaling over 150,000 aligned sentences each. The quality of the corpora and their usefulness are tested in an experiment with machine translation. 1. Introduction Parallel corpora are collections of texts that are mutual translations. Machine Translation (MT) systems have recently become very popular due to the latest advances in the field of Statistical Machine Translation (SMT). The most advanced SMT approaches, such as phrase-based [Koehn et al. 2003] and hierarchical SMT [Chiang 2005], learn how to perform translation from parallel corpora. A popular way of building a parallel corpus is collecting large amounts of data from the web. The alignment between documents, sentences and words allows the development of resources such as dictionaries and SMT systems. [Espl`a-Gomis and Forcada 2010] present Bitextor, a language-independent tool to harvest parallel documents from multilingual web sites. It uses features from the HTML structure of the documents and their URLs to perform document alignment. [Aziz et al. 2008] describe the creation of a Brazilian Portuguese-Spanish (pt-"
W11-4533,W08-0318,0,0.0169663,"language, we split the corpus in one training, two developments and two test sets as shown in Table 1 (the last column points to the HTML documents in the website). 236 Table 1. Splitting of the Fapesp-v2 for training and test purposes Sentences Issues HTML pointer Set pt-en pt-es training 150,968 146,755 April, 2005 - March, 2011 19 - 945 dev 1,375 1,302 June, 2003 118 1,608 1,601 June, 2010 934 dev-test test-a 1,314 1,201 July, 2003 119 1,447 1,379 July, 2010 935 test-b We trained a PBSMT model using the Moses toolkit [Koehn et al. 2007] following its “baseline” settings and truecased data [Koehn et al. 2008]. For comparison purposes, we trained a model using the old version of the training set (18, 232 sentence pairs): pt-en Fapesp-v1 (UoW-fapesp-v1), and another using the new version: pt-en Fapesp-v2 (UoW-fapesp-v2). In both experiments we used the Fapesp-v2 dev for tuning the features of the PBSMT model. The systems were tested using 3 test sets: i) Fapesp-v1 test (667 sentences), ii) Fapesp-v2 test-a, and iii) Fapesp-v2 test-b. We assessed the system’s performances in terms of BLEU [Papineni et al. 2002] and its case-sensitive version BLEU-c. We also compared the translations against those pr"
W11-4533,P07-2045,0,0.00951855,"T) systems and evaluated their quality using automatic metrics. For each language, we split the corpus in one training, two developments and two test sets as shown in Table 1 (the last column points to the HTML documents in the website). 236 Table 1. Splitting of the Fapesp-v2 for training and test purposes Sentences Issues HTML pointer Set pt-en pt-es training 150,968 146,755 April, 2005 - March, 2011 19 - 945 dev 1,375 1,302 June, 2003 118 1,608 1,601 June, 2010 934 dev-test test-a 1,314 1,201 July, 2003 119 1,447 1,379 July, 2010 935 test-b We trained a PBSMT model using the Moses toolkit [Koehn et al. 2007] following its “baseline” settings and truecased data [Koehn et al. 2008]. For comparison purposes, we trained a model using the old version of the training set (18, 232 sentence pairs): pt-en Fapesp-v1 (UoW-fapesp-v1), and another using the new version: pt-en Fapesp-v2 (UoW-fapesp-v2). In both experiments we used the Fapesp-v2 dev for tuning the features of the PBSMT model. The systems were tested using 3 test sets: i) Fapesp-v1 test (667 sentences), ii) Fapesp-v2 test-a, and iii) Fapesp-v2 test-b. We assessed the system’s performances in terms of BLEU [Papineni et al. 2002] and its case-sen"
W11-4533,N03-1017,0,0.00949287,"ual Brazilian magazine. The texts are then automatically aligned at document- and sentence-level. The resulting corpora contain about 2,700 parallel documents totaling over 150,000 aligned sentences each. The quality of the corpora and their usefulness are tested in an experiment with machine translation. 1. Introduction Parallel corpora are collections of texts that are mutual translations. Machine Translation (MT) systems have recently become very popular due to the latest advances in the field of Statistical Machine Translation (SMT). The most advanced SMT approaches, such as phrase-based [Koehn et al. 2003] and hierarchical SMT [Chiang 2005], learn how to perform translation from parallel corpora. A popular way of building a parallel corpus is collecting large amounts of data from the web. The alignment between documents, sentences and words allows the development of resources such as dictionaries and SMT systems. [Espl`a-Gomis and Forcada 2010] present Bitextor, a language-independent tool to harvest parallel documents from multilingual web sites. It uses features from the HTML structure of the documents and their URLs to perform document alignment. [Aziz et al. 2008] describe the creation of"
W11-4533,P02-1040,0,0.0880041,"g the Moses toolkit [Koehn et al. 2007] following its “baseline” settings and truecased data [Koehn et al. 2008]. For comparison purposes, we trained a model using the old version of the training set (18, 232 sentence pairs): pt-en Fapesp-v1 (UoW-fapesp-v1), and another using the new version: pt-en Fapesp-v2 (UoW-fapesp-v2). In both experiments we used the Fapesp-v2 dev for tuning the features of the PBSMT model. The systems were tested using 3 test sets: i) Fapesp-v1 test (667 sentences), ii) Fapesp-v2 test-a, and iii) Fapesp-v2 test-b. We assessed the system’s performances in terms of BLEU [Papineni et al. 2002] and its case-sensitive version BLEU-c. We also compared the translations against those produced by Google Translate, an off-the-shelf, out-of-domain PBSMT system. Table 2 shows that the model trained using our automatically compiled corpus significantly outperforms both Google Translate and a model trained on the previous, smaller version of the corpus, UoW-fapesp-v1, in all test sets. PBSMT system Test set fapesp-v1 fapesp-v2-a fapesp-v2-b Table 2. Performance on the test sets UoW-fapesp-v1 UoW-fapesp-v2 Google BLEU BLEU-c BLEU BLEU-c BLEU BLEU-c 39.99 37.94 57.09 54.25 37.62 36.97 43.24 40"
W13-2260,W09-1114,0,0.0804647,"case for minimum error rate training (Och, 2003; Watanabe et al., 2007), minimum risk training (Smith and Eisner, 2006) and minimum risk decoding (Kumar and Byrne, 2004). Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as approximation to true probabilistic samples. A known issue with n-best lists is that they tend to be clustered around only one mode of the distribution. A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations (Arun et al., 2009; Blunsom and Osborne, 2008). We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. We also present a first implementation of that method along with experimental results shedding light on some fundamental issues. In hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model. We replace this intractable distribution by a sequence"
W13-2260,P89-1018,0,0.248527,"list of promising candidates. This is an approximation technique, related to beam-search, which performs well in practice, but is not guaranteed to find the actual optimum. In the approach presented here — described in detail in §3 — we do not prune the search space. While we do construct the full initial grammar G(f ), we proceed by incrementally intersecting it with simple automata associated with upperbounds of A, for which the intersection is tractable. 2.3 Earley Intersection In their classical paper Bar-Hillel et al. (1961) showed that the intersection of a CFG with a FSA is a CFG, and Billot and Lang (1989) were possibly the first to notice the connection of this construct with chart-parsing. In general, parsing with a CFG can be seen as a special case of intersection, with the input sequence represented as a “flat” (linear chain) automaton, and this insight allows to generalise various parsing algorithms to corresponding intersection algorithms. One such algorithm, for weighted context-free grammars and automata, inspired by the CKY parsing algorithm, is presented in Nederhof and Satta (2008). The algorithm that we are using is different; it is inspired by Earley parsing, and was introduced in"
W13-2260,D08-1023,0,0.114699,"ror rate training (Och, 2003; Watanabe et al., 2007), minimum risk training (Smith and Eisner, 2006) and minimum risk decoding (Kumar and Byrne, 2004). Due to the additional computational challenges posed by sampling, n-best lists, a by-product of optimisation, are typically used as approximation to true probabilistic samples. A known issue with n-best lists is that they tend to be clustered around only one mode of the distribution. A more direct procedure is to attempt to directly draw samples from the underlying distribution rather than rely on n-best list approximations (Arun et al., 2009; Blunsom and Osborne, 2008). We present a method for inference in hierarchical phrase-based translation, where both optimisation and sampling are performed in a common exact inference framework related to adaptive rejection sampling. We also present a first implementation of that method along with experimental results shedding light on some fundamental issues. In hierarchical translation, inference needs to be performed over a high-complexity distribution defined by the intersection of a translation hypergraph and a target language model. We replace this intractable distribution by a sequence of tractable upper-bounds f"
W13-2260,W12-3102,0,0.0503581,"Missing"
W13-2260,D12-1103,1,0.855206,"mples from p. In the case of optimisation, one finds the maximum x relative to q, and again computes the ratio r = p(x)/q(x). If this ratio equals 1, then it is easy to show that x is the actual maximum from p.1 Otherwise we refine the proposal in a similar way to the sampling case, continuing until we find a ratio equal to 1 (or very close to 1 if we are willing to accept an approximation to the maximum). For finite spaces X, this optimisation technique is argued to be a generalisation of A∗ . An application of the OS ∗ technique to sampling/optimisation with High-Order HMM’s is described in Carter et al. (2012) and provides background for this paper. In that work, while the highorder HMM corresponds to an intractable goal distribution, it can be upper-bounded by a sequence of tractable distributions for which optimisers and samplers can be obtained through standard dynamic programming techniques. 2.2 Hierarchical Translation An abstract formulation of the decoding process for hierarchical translation models such as that of Chiang (2007) can be expressed as a sequence of three steps. In a first step, a translation model G, represented as a weighted synchronous contextfree grammar (SCFG) (Chiang, 2005"
W13-2260,P05-1033,0,0.686456,"t al. (2012) and provides background for this paper. In that work, while the highorder HMM corresponds to an intractable goal distribution, it can be upper-bounded by a sequence of tractable distributions for which optimisers and samplers can be obtained through standard dynamic programming techniques. 2.2 Hierarchical Translation An abstract formulation of the decoding process for hierarchical translation models such as that of Chiang (2007) can be expressed as a sequence of three steps. In a first step, a translation model G, represented as a weighted synchronous contextfree grammar (SCFG) (Chiang, 2005), is applied to (in other words, intersected with) the source sentence f to produce a weighted context-free grammar G(f ) over the target language.2 In a second step, G(f ) is intersected with a weighted finitestate automaton A representing the target language model, resulting in a weighted context-free grammar G0 (f ) = G(f ) ∩ A. In a final step, a dynamic programming procedure (see §2.4) is applied to find the maximum derivation x in G0 (f ), and the sequence of leaves of yield(x) is the result translation. While this formulation gives the general principle, already mentioned in Chiang (200"
W13-2260,P10-4002,0,0.0380892,"hted finitestate automaton A representing the target language model, resulting in a weighted context-free grammar G0 (f ) = G(f ) ∩ A. In a final step, a dynamic programming procedure (see §2.4) is applied to find the maximum derivation x in G0 (f ), and the sequence of leaves of yield(x) is the result translation. While this formulation gives the general principle, already mentioned in Chiang (2007), most implementations do not exactly follow these steps or use this terminology. In practice, the closest approach to this abstract formulation is that of Dyer (2010) and the related system cdec (Dyer et al., 2010); we follow a similar approach here. 1 This is because if x0 was such that p(x0 ) &gt; p(x), then q(x0 ) ≥ p(x0 ) &gt; p(x) = q(x), and hence x would not be a maximum for q, a contradiction. 2 G(f ) is thus a compact representation of a forest over target sequences, and is equivalent to a hypergraph, using different terminology. Whatever the actual implementation chosen, all approaches face a common problem: the complexity of the intersection G0 (f ) = G(f ) ∩ A increases rapidly with the order of the language model, and can become unwieldy for moderate-length input sentences even with a bigram mode"
W13-2260,W12-6106,1,0.794456,"model. We replace this intractable distribution by a sequence of tractable upper-bounds for which exact optimisers and samplers are easy to obtain. Our experiments show that exact inference is then feasible using only a fraction of the time and space that would be required by the full intersection, without recourse to pruning techniques that only provide approximate solutions. While the current implementation is limited in the size of inputs it can handle in reasonable time, our experiments provide insights towards obtaining future speedups, while staying in the same general framework. 1 OS∗ (Dymetman et al., 2012a) is a recent approach that stresses a unified view between the two types of inference, optimisation and sampling. In this view, rather than resorting to pruning in order to cope with the tractability issues, one upperbounds the complex goal distribution with a simpler “proposal” distribution for which dynamic programming is feasible. This proposal is incrementally refined to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level. Introduction In statistical machine translation (SMT), optimisation — the task of searching for an optimum tran"
W13-2260,P13-2121,0,0.0963369,"Missing"
W13-2260,P07-1019,0,0.0667205,", and hence x would not be a maximum for q, a contradiction. 2 G(f ) is thus a compact representation of a forest over target sequences, and is equivalent to a hypergraph, using different terminology. Whatever the actual implementation chosen, all approaches face a common problem: the complexity of the intersection G0 (f ) = G(f ) ∩ A increases rapidly with the order of the language model, and can become unwieldy for moderate-length input sentences even with a bigram model. In order to address this problem, most implementations employ variants of a technique called cube-pruning (Chiang, 2007; Huang and Chiang, 2007), where the cells constructed during the intersection process retain only a k-best list of promising candidates. This is an approximation technique, related to beam-search, which performs well in practice, but is not guaranteed to find the actual optimum. In the approach presented here — described in detail in §3 — we do not prune the search space. While we do construct the full initial grammar G(f ), we proceed by incrementally intersecting it with simple automata associated with upperbounds of A, for which the intersection is tractable. 2.3 Earley Intersection In their classical paper Bar-Hi"
W13-2260,N07-1018,0,0.037525,"ax-times semiring. Here, instead of maximising over the weights of the competing derivations rooted in the same nonterminal, one sums over these weights. By proceeding in the same bottom-up way, one ends with an accumulation of all the weights on the initial nonterminal (this can also be seen as the partition function associated with the grammar). An efficient exact sampler is then obtained by starting at the root nonterminal, randomly selecting an expansion proportionally to the weight of this expansion, and iterating in a topdown way. This process is described in more detail in section 4 of Johnson et al. (2007), for instance. 3 Approach The complexity of building the full intersection G(f ) ∩ A, when A represents a language model of order n, is related to the fact that the number of states of A grows exponentially with n, and that each nonterminal N in G(f ) tends to generate in the grammar G0 (f ) many indexed nonterminals of the form (i, N, j), where i, j are states of A and the nonterminal (i, N, j) can be interpreted as an N connecting an i state to a j state. In our approach, instead of explicitly constructing the full intersection G(f ) ∩ A, which, using the notation of §2.1, is identified wit"
W13-2260,N03-1017,0,0.0792796,"bution for which dynamic programming is feasible. This proposal is incrementally refined to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level. Introduction In statistical machine translation (SMT), optimisation — the task of searching for an optimum translation — is performed over a high-complexity distribution defined by the intersection between a translation hypergraph and a target language model (LM). This distribution is too complex to be represented exactly and one typically resorts to approximation techniques such as beam-search (Koehn et al., 2003) and cube-pruning (Chiang, 2007), where maximisation is performed over a pruned representation of the full distribution. This paper applies the OS∗ approach to the problem of inference in hierarchical SMT (Chiang, 2007). In a nutshell, the idea is to replace the intractable problem of intersecting a contextfree grammar with a full language model by the tractable problem of intersecting it with a simplified, optimistic version of this LM which “forgets” parts of n-gram contexts, and to incrementally add more context based on evidence of the need to do so. Evidence is gathered by optimising or s"
W13-2260,P07-2045,0,0.0063725,"g in the increased number of exact samples and better acceptance rate. 10 Note that, starting from iteration one, all refinements here correspond to 2-grams (i.e. one-word contexts). This can be explained by the fact that, in sampling, lower-order refinements are those that mostly increase acceptance rate (rationale: highorder n-grams are compatible with fewer grammar rules). L1 ● ● ● ● 9 ● ● ● ● ● ● ● ● ● ● 1.0 1.5 2.0 0.1 0.2 0.3 0 1000 exact ● ● ● ● ● ● ● ● ● ● ● ● ● ● accrate ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● refinement ● ● ● ● ● ● ● ● ● 0 5 10 4 Experiments We used the Moses toolkit (Koehn et al., 2007) to extract a SCFG following Chiang (2005) from the 6th version of the Europarl collection (Koehn, 2005) (German-English portion). We trained language models using lmplz (Heafield et al., 2013) and interpolated the models trained on the English monolingual data made available by the WMT (Callison-Burch et al., 2012) (i.e. Europarl, newscommentaries, news-2012 and commoncrawl). Tuning was performed via MERT using newstest2010 as development set; test sentences were extracted from newstest2011. Finally, we restricted our SCFGs to having at most 10 target productions for a given source production"
W13-2260,2005.mtsummit-papers.11,0,0.0260972,"e, all refinements here correspond to 2-grams (i.e. one-word contexts). This can be explained by the fact that, in sampling, lower-order refinements are those that mostly increase acceptance rate (rationale: highorder n-grams are compatible with fewer grammar rules). L1 ● ● ● ● 9 ● ● ● ● ● ● ● ● ● ● 1.0 1.5 2.0 0.1 0.2 0.3 0 1000 exact ● ● ● ● ● ● ● ● ● ● ● ● ● ● accrate ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● refinement ● ● ● ● ● ● ● ● ● 0 5 10 4 Experiments We used the Moses toolkit (Koehn et al., 2007) to extract a SCFG following Chiang (2005) from the 6th version of the Europarl collection (Koehn, 2005) (German-English portion). We trained language models using lmplz (Heafield et al., 2013) and interpolated the models trained on the English monolingual data made available by the WMT (Callison-Burch et al., 2012) (i.e. Europarl, newscommentaries, news-2012 and commoncrawl). Tuning was performed via MERT using newstest2010 as development set; test sentences were extracted from newstest2011. Finally, we restricted our SCFGs to having at most 10 target productions for a given source production. Figure 3 shows some properties of the initial grammar G(f ) as a function of the input sentence length"
W13-2260,N04-1022,0,0.0704041,"Missing"
W13-2260,P03-1021,0,0.0682464,"Missing"
W13-2260,P11-1008,0,0.343513,"Missing"
W13-2260,P06-2101,0,0.0494604,"Missing"
W13-2260,D07-1080,0,0.031726,"Missing"
W13-2260,J00-4006,0,\N,Missing
W13-2260,J07-2003,0,\N,Missing
W13-3522,P07-2045,0,0.00736798,"-best paraphrases of each method and distributed them amongst the evaluators. We considered two evaluation scenarios: multi: the paraphrasing model with multilingual constraints introduced in this paper. Gold-standard translations: the English translation as found in Europarl was taken as sense tag, using automatic word-alignments to identify the English phrase that constrains the sense of the Spanish phrase. CCB: the model in (Bannard and CallisonBurch, 2005) which does not explicitly perform any sense disambiguation. SMT translations: a phrase-based SMT system built using the Moses toolkit (Koehn et al., 2007) and the whole Spanish-English dataset (except the sentences in the test set) was used to translated the Spanish sentences. Instead of gold-standard translations as a quasiperfect sense annotation (quasi because the word-alignment is still automatic and thus prone to errors), the phrase-based SMT system plays the role of a sense annotation module predicting the “sense” tags. CCB-wsd: an extended model in (Bannard and Callison-Burch, 2005) using English phrases as sense tags for pivoting. Using each of these three models, we paraphrased the 258 samples in our test set, retrieving the 3-best par"
W13-3522,P05-1074,0,0.442645,"13. 2013 Association for Computational Linguistics the meaning of the source in the triangulated translation, as different languages are likely to realise ambiguities differently. Although their findings apply to generating translation candidates, the input phrases are not constrained to specific senses, and as a consequence multiple translations, which are valid in different contexts but not generally interchangeable, are mixed together in the same distribution. In SMT the target Language Model (LM) helps selecting the adequate translation candidate in context. Callison-Burch (2008) extends (Bannard and Callison-Burch, 2005) by adding syntactic constraints to the model. Paraphrase extraction is done by pivoting using word-alignment information, as before, but sentences are syntactically annotated and paraphrases are restricted to those with the same syntactic category. This addresses categorial ambiguity by preventing that words with a given category (e.g. a noun) are paraphrased by words with other categories (e.g., a verb). However, the approach does not solve the more complex issue of polysemous paraphrases: words with the same category but different meanings, such as the noun bank as financial institution and"
W13-3522,N06-1003,0,0.0480386,"Missing"
W13-3522,2005.mtsummit-papers.11,0,0.0750779,"e paraphrased. Ambiguity is determined on the basis of the number of synsets in the Spanish WordNet. We note that this information was only used to bias the selection of the phrases, i.e., WordNet is not used in the proposed approach. Experimental settings 4.1 Resources La idea de conceder a la Uni´on Europea su propia competencia fiscal - la palabra clave es el “impuesto por Europa” - est´a siendo debatida. The idea of granting the EU its own tax competence - the keyword is the “Europe tax” - is being discussed. The source of bilingual data used in the experiments is the Europarl collection (Koehn, 2005). We paraphrase Spanish (es) phrases using their corresponding English (en) phrases as sense tags and nine European languages as pivots: German (de), Dutch (nl), Danish (da), Swedish (sv), Finnish (fi), French (fr), Italian (it), Portuguese (pt) and Greek (el). The tools provided along with the corpus were used to extract the sentence aligned parallel data as shown in Table 1. The sentence aligned parallel data is first wordaligned using GIZA++ in both source-target and target-source directions, followed by the application of traditional symmetrisation heuristics (Och and Ney, 2003). These ali"
W13-3522,W10-1754,0,0.0141708,"mables 0.16 ∗ kind 0.12 especie 0.09 ∗ amable 0.08 tipo 0.07 ∗ Table 5: Top paraphrases of forma annotated by the English words way, form, means and kind. Starred phrases denote inadequate candidates. 5.3 Potential applications tilingual constraints offer more control over ambiguities, therefore potentially leading to more accurate phrase pairs added to the phrase-table. In what follows we discuss two applications which we believe could directly benefit from the paraphrase extraction approach proposed in this paper. 6 MT evaluation metrics such as METEOR (Denkowski and Lavie, 2010) and TESLA (Liu et al., 2010) already use paraphrases of n-grams in the machine translated sentence in an attempt to match more of the reference translation’s ngrams. TESLA, in particular, uses paraphrases constrained by a single pivot language as sense tag as originally proposed in (Bannard and CallisonBurch, 2005). Metrics like METEOR, which use paraphrases simply as a repository with extra options for the n-gram matching, could be extended to use the word-alignment between the source sentence and the translation to constrain the translated phrases while paraphrasing them with multilingual constraints. In this case the"
W13-3522,D08-1021,0,0.103433,"LM component in getting the senses of the paraphrases right. Results The evaluation was performed by seven native speakers of Spanish who judged a total of 5, 110 sentences containing one paraphrased input phrase each. We used 40 overlapping judgements across annotators to measure inter-annotator agreement. The average inter-annotator agreement in terms of Cohen’s Kappa (Cohen, 1960) is 0.54 ± 0.15 for meaning judgements, 0.63 ± 0.16 for grammar judgements and 0.62 ± 0.20 for correctness judgements. These figures are similar or superior to those reported in (Bannard and Callison-Burch, 2005; Callison-Burch, 2008), which we consider particularly encouraging as in our case we have seven instead of only two annotators. In Tables 2, 3 and 4 we report the performance of the three models in terms of precision, recall and F1 , with p-values &lt; 0.01 based on the t-test for statistical significance. 5.1 G 23 9 43 Table 3: Performance (F1 ) in correctly retrieving the best paraphrase in context using gold-standard translations without the 5-gram LM component. Table 2: Performance in retrieving paraphrases in context using gold-standard translations for sense tags and a 5-gram LM component. 5 M 33 19 64 To show t"
W13-3522,J10-3003,0,0.0214049,"g preserving in context. This paper is structured as follows: Section 2 describes additional previous work on paraphrase extraction and pivoting. Section 3 presents the proposed model. Section 4 introduces our experimental settings, while Section 5 shows the results of a series of experiments. 2 Related work 3 In addition to the well-known approach by (Bannard and Callison-Burch, 2005), the following previous approaches using pivot languages for paraphrasing can be mentioned. For a recent and comprehensive survey on a number of datadriven paraphrase generation methods, we refer the reader to (Madnani and Dorr, 2010). Cohn and Lapata (2007) make use of multiple parallel corpora to improve Statistical Machine Translation (SMT) by triangulation for languages with little or no source-target parallel data available. Translation tables are learnt by pivoting through languages for which source-pivot and pivot-target bilingual corpora can be found. Multiple pivot languages were found useful to preserve Paraphrasing through multilingual constraints Our approach to paraphrasing can be applied to both individual words or sequences of words of any length, conditioned only on sufficient evidence of these segments in"
W13-3522,D07-1007,0,0.0250728,"is, a valid translation of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases (Resnik and Yarowsky, 1999; Specia et al., 2006). These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques (Carpuat and Wu, 2007; Chan et al., 2007). Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f ∈ F align back to the input language, producing the paraphrase e2 , and to the target language, producing the sense tag q. To avoid computing the distribution p(e2 , q|f ) – which would require a trilingual parallel corpus – we assume that e2 and q are conditionally independent on f : p(e2 , q|f ) e2 ⊥ ⊥q|f = itory, in addition to bilingual parallel corpora between the input language and the pivot languages, our model re"
W13-3522,W07-0716,0,0.0434365,"Missing"
W13-3522,P07-1005,0,0.0311003,"on of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases (Resnik and Yarowsky, 1999; Specia et al., 2006). These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques (Carpuat and Wu, 2007; Chan et al., 2007). Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f ∈ F align back to the input language, producing the paraphrase e2 , and to the target language, producing the sense tag q. To avoid computing the distribution p(e2 , q|f ) – which would require a trilingual parallel corpus – we assume that e2 and q are conditionally independent on f : p(e2 , q|f ) e2 ⊥ ⊥q|f = itory, in addition to bilingual parallel corpora between the input language and the pivot languages, our model requires bilingual par"
W13-3522,D09-1040,0,0.0448874,"Missing"
W13-3522,P09-1089,1,0.859292,"ond morphological variants of the original text is a challenging problem and has been shown to be useful in many natural language applications. These include i) expanding the set of reference translations for Machine Translation (MT) evaluation (Denkowski and Lavie, 2010; Liu et al., 2010) and parameter optimisation (Madnani et al., 2007), where multiple reference translations are important to accommodate for valid variations of system translations; ii) addressing the problem of out-of-vocabulary words or phrases in MT, either by replacing these by paraphrases that are known to the MT system (Mirkin et al., 2009) or by exX f ∈F p(f |e1 )p(e2 |f ) (1) Equation 1 allows paraphrases to be extracted by using multiple pivot languages such that these languages help discard inadequate paraphrases resulting from ambiguous pivot phrases. However in this formulation all senses of the input phrase are mixed together in a single distribution. For example, for the Spanish input phrase acabar con, both paraphrases superar (overcome) and eliminar (eliminate) may be adequate depending on the context, however they are not generally interchangeable. In (Bannard and Callison-Burch, 1 The distributions p(f |e) and p(e|f"
W13-3522,P07-1092,0,0.0160503,"This paper is structured as follows: Section 2 describes additional previous work on paraphrase extraction and pivoting. Section 3 presents the proposed model. Section 4 introduces our experimental settings, while Section 5 shows the results of a series of experiments. 2 Related work 3 In addition to the well-known approach by (Bannard and Callison-Burch, 2005), the following previous approaches using pivot languages for paraphrasing can be mentioned. For a recent and comprehensive survey on a number of datadriven paraphrase generation methods, we refer the reader to (Madnani and Dorr, 2010). Cohn and Lapata (2007) make use of multiple parallel corpora to improve Statistical Machine Translation (SMT) by triangulation for languages with little or no source-target parallel data available. Translation tables are learnt by pivoting through languages for which source-pivot and pivot-target bilingual corpora can be found. Multiple pivot languages were found useful to preserve Paraphrasing through multilingual constraints Our approach to paraphrasing can be applied to both individual words or sequences of words of any length, conditioned only on sufficient evidence of these segments in a parallel corpus. We us"
W13-3522,J03-1002,0,0.00593946,"parl collection (Koehn, 2005). We paraphrase Spanish (es) phrases using their corresponding English (en) phrases as sense tags and nine European languages as pivots: German (de), Dutch (nl), Danish (da), Swedish (sv), Finnish (fi), French (fr), Italian (it), Portuguese (pt) and Greek (el). The tools provided along with the corpus were used to extract the sentence aligned parallel data as shown in Table 1. The sentence aligned parallel data is first wordaligned using GIZA++ in both source-target and target-source directions, followed by the application of traditional symmetrisation heuristics (Och and Ney, 2003). These aligned corpora are used for paraphrase extraction, except for a subset of them used in the creation of a test set (Section 4.2). 4.2 Figure 3: Example of context selected for the phrase clave. tag to avoid selecting simpler, categorial ambiguities). Figure 2 lists the selected words and phrases in their base forms. The bilingual corpus was queried for sentences containing at least one of the 50 phrases listed in Figure 2, or any of their morphological variants. The resulting sentences were then grouped on the basis of whether or not they shared the same English translation. To find th"
W13-3522,W10-1751,0,0.0509239,"Missing"
W13-3522,P02-1040,0,0.0970258,"Missing"
W13-3522,N03-1017,0,0.00925389,"Missing"
W13-3522,P07-1059,0,0.0741067,"Missing"
W13-3522,W06-2505,1,0.73993,"millions of sentence pairs the pivot phrases that will lead to adequate paraphrases. In our approach a sense tag consists in a phrase in a foreign language, that is, a valid translation of the input phrase in a language of interest, here referred to as target language. Treating the target language vocabulary as a sense repository is a good strategy from both theoretical and practical perspectives: it has been shown that monolingual sense distinctions can be effectively captured by translations into second languages, especially as language family distance increases (Resnik and Yarowsky, 1999; Specia et al., 2006). These translations can be easily captured given the availability of bilingual parallel data and robust automatic word-alignment techniques (Carpuat and Wu, 2007; Chan et al., 2007). Figure 1 illustrates the proposed model to produce sense tagged paraphrases. We start the process at e1 and we need to make sure that the pivot phrases f ∈ F align back to the input language, producing the paraphrase e2 , and to the target language, producing the sense tag q. To avoid computing the distribution p(e2 , q|f ) – which would require a trilingual parallel corpus – we assume that e2 and q are condition"
W15-2507,W12-3156,0,0.226705,"data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference dep"
W15-2507,2010.iwslt-papers.10,0,0.364938,"n the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison"
W15-2507,P13-4033,0,0.418241,"ta artificially manipulated to contain errors of coherence common in MT output. Such a corpus could then be used as a benchmark for coherence models in MT, and potentially as training data for coherence models in supervised settings. 1 Introduction Discourse information has only recently started to attract attention in MT, particularly in Statistical Machine Translation (SMT), the focus of this paper. Most decoders work on a sentence by sentence basis, isolated from context, due to both modelling and computational complexity. An exception are approaches to multi-pass decoding, such as Docent (Hardmeier et al., 2013a). Our work focuses on an issue which has not yet been much explored in MT, that of coherence. Coherence is undeniably a cognitive process, and we will limit our remit to the extent that this process is guided by linguistic elements discernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceeding"
W15-2507,P12-2023,0,0.0308778,"pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to improve evaluation of MT output. Drawing from research on topic modelling (Eidelman et al., 2012), where lexical probabilities conditioned on topics are computed, Xiong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the appropriate words. They observed slight improvements in terms of general standard metrics, indicating perhaps that these metrics fail to account for discourse improvements. As far as we aware, no attempts have been made to create a corpus exhibiting incoherence, other than by shuffling ordered sentences. There has been work"
W15-2507,D13-1037,0,0.413157,"ta artificially manipulated to contain errors of coherence common in MT output. Such a corpus could then be used as a benchmark for coherence models in MT, and potentially as training data for coherence models in supervised settings. 1 Introduction Discourse information has only recently started to attract attention in MT, particularly in Statistical Machine Translation (SMT), the focus of this paper. Most decoders work on a sentence by sentence basis, isolated from context, due to both modelling and computational complexity. An exception are approaches to multi-pass decoding, such as Docent (Hardmeier et al., 2013a). Our work focuses on an issue which has not yet been much explored in MT, that of coherence. Coherence is undeniably a cognitive process, and we will limit our remit to the extent that this process is guided by linguistic elements discernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceeding"
W15-2507,P11-2022,0,0.144231,"n Karin Sim Smith§ , Wilker Aziz† and Lucia Specia§ §Department of Computer Science, University of Sheffield, UK {kmsimsmith1,l.specia}@sheffield.ac.uk †Institute for Logic, Language and Computation University of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our"
W15-2507,E14-1063,0,0.126945,"in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to improve evaluation of MT output. Drawing from research on topic modelling (Eidelman et al., 2012), where lexical probabilities conditioned on topics are computed, Xiong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the appropriate words. They obse"
W15-2507,W05-0820,0,0.0193956,"13) The reference translation has a clausal pattern which is more cohesive to the English reader. coherent and incoherent texts. This will be done in a flexible manner, such that the incoherent documents can be created for a variety of (coherent) input texts. Moreover they can be created for specific types of errors. The quality of MT output varies greatly from one language pair and MT system to another. For example, the output from a French-English MT system trained in very large collections is superior to that of, for example, an English-Finnish system trained on smaller quantities of data (Koehn and Monz, 2005; Bojar et al., 2015).The errors encountered also vary, depending on the language pair, in particular for aspects such as discourse markers and syntax. Some of these error patterns are more relevant for particular language pairs, e.g. negation for French-English, which is otherwise a wellperforming language pair. We propose to inject errors programmatically in a systematic manner, as detailed below. Negation MT systems often miss the focus of the negation. This results in incorrectly transferred negations that affect coherence (Wetzel and Bond, 2012; Fancellu and Webber, 2014). src: ‘Aucun dir"
W15-2507,W15-1301,0,0.0138909,"ersions of the corpus with different proportions of errors will be created. We will inject errors systematically and incrementally to vary the degree and location of the errors. The errors will be introduced systematically via pattern-matching, and as highlighted by Brockett et al. (2006), may not be distributed in a natural way. Artificially generating coherence errors Significant work has already been done in the areas of coreference resolution (Michal, 2011; Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2013b; Guillou, 2012) and negation (Wetzel and Bond, 2012; Fancellu and Webber, 2015; Fancellu and Webber, 2014) in MT. In our corpus we will focus on less studied issues and limit ourselves to targeting coherence more specifically than cohesion. The proposed framework will take as input well-formed documents that are determined ‘coherent’ (i.e. grammatically correct and coherent) and then artificially distort them in ways (detailed below) that directly affect coherence in the manner that an MT system would. The resulting texts will make a corpus of ‘incoherent’ texts for assessing the ability of models to discriminate between 4.2 Error Injection We will inject errors of the"
W15-2507,W10-1737,0,0.0627839,"Missing"
W15-2507,D14-1218,0,0.0970977,"rsity of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suffers from other aspects of incoherence. Moreover, while the MT output can potentially be considered ‘incoherent’, it contains a multitude of prob"
W15-2507,E14-3013,0,0.0178387,"iong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the appropriate words. They observed slight improvements in terms of general standard metrics, indicating perhaps that these metrics fail to account for discourse improvements. As far as we aware, no attempts have been made to create a corpus exhibiting incoherence, other than by shuffling ordered sentences. There has been work in other areas to introduce errors in correct texts. For example, Felice and Yuan (2014) and Brockett et al. (2006) inject grammatical errors common to non-native speakers of English in good quality texts. Felice and Yuan (2014) use existing corrected corpora to derive the error distribution, while Brockett et al. (2006) adopt a de3 Issues of incoherence in MT systems Current MT approaches suffer from a lack of linguistic information at various stages (modelling, decoding, pruning) causing the lack of coherence in the output. Below we describe a number of issues that are generally viewed as coherence issues which MT approaches deal poorly with and which have also been the subject"
W15-2507,P11-1100,0,0.271648,"Missing"
W15-2507,J86-3001,0,0.333161,"onal complexity. An exception are approaches to multi-pass decoding, such as Docent (Hardmeier et al., 2013a). Our work focuses on an issue which has not yet been much explored in MT, that of coherence. Coherence is undeniably a cognitive process, and we will limit our remit to the extent that this process is guided by linguistic elements discernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 52–58, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 2 Existing work terministic approach based on hand-crafted rules. Logacheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing dat"
W15-2507,W15-4907,1,0.809712,"ernible in the discourse. While it does include cohesion, it is wider in terms of describing how a text becomes semantically meaningful overall, and additionally spans the entire document. We are interested in capturing aspects of coherence as defined by Grosz and Sidner (1986), based on the attentional state, intentional structure and linguistic 52 Proceedings of the Second Workshop on Discourse in Machine Translation (DiscoMT), pages 52–58, c Lisbon, Portugal, 17 September 2015. 2015 Association for Computational Linguistics. 2 Existing work terministic approach based on hand-crafted rules. Logacheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xio"
W15-2507,E12-3001,0,0.265322,"2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to"
W15-2507,P13-1010,0,0.0930757,"§ §Department of Computer Science, University of Sheffield, UK {kmsimsmith1,l.specia}@sheffield.ac.uk †Institute for Logic, Language and Computation University of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suff"
W15-2507,D12-1106,0,0.472344,"heffield, UK {kmsimsmith1,l.specia}@sheffield.ac.uk †Institute for Logic, Language and Computation University of Amsterdam, The Netherlands w.aziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suffers from other aspects of incoherence. Moreove"
W15-2507,P14-1065,0,0.0355369,"Missing"
W15-2507,W12-4203,0,0.488354,"resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which highlights the differences of coreference depending on the language pair. Since then Hardmeier et al. (2013b) have used a new approach for anaphora resolution via neural networks which achieves comparable results to a standard anaphora resolutions system, but without annotated data. Recently work has begun on negation in MT, particularly by Wetzel and Bond (2012; Fancellu and Webber (2014). There is also work focusing on evaluation against reference translations (Guzm´an et al., 2014) based on the comparison between discourse trees in MT versus reference. This information was found to improve evaluation of MT output. Drawing from research on topic modelling (Eidelman et al., 2012), where lexical probabilities conditioned on topics are computed, Xiong and Zhang (2013) attempt to improve coherence based using topic information. They determine the topic of the source sentence and project it onto the target as a feature to ensure the decoder selects the"
W15-2507,W12-0117,0,0.338111,"p¨ater an den Folgen der schweren Verletzungen gestorben.’ mt: ‘The victim was later at the consequences of the serious injuries died.’ ref: ‘The victim later died as a result of the serious injuries.’ (Bojar et al., 2014). This can affect the understanding of the sentence, the overall logic of it in the context of the surrounding sentences, or simply require a reread which itself is indicative of impaired coherence. Discourse connectives Discourse connectives are vital for the correct understanding of discourse. Yet in MT systems these can be incorrect or missing (Meyer and Pol´akov´a, 2013; Meyer and Popescu-Belis, 2012; Meyer et al., 2011; Steele, 2015). In particular, where discourse connectives are ambiguous, e.g. those which can be temporal 54 src: ‘Bereits im Jahr 1925 wurde in Polen eine Eisenbahn-Draisine gebaut, f¨ur die ein Raketenantrieb geplant war. Der Autor des Entwurfs und die Details dieses Vorhabens blieben leider unbekannt.’ mt: ‘Already in 1925 a railway trolley was built in Poland, for which a rocket was planned. The author of the design and the details of the project remained unfortunately unknown.’ ref: In 1925, Poland had already built a handcar which was supposed to be fitted with a ro"
W15-2507,D12-1097,0,0.442224,"d-crafted rules. Logacheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010)"
W15-2507,W11-2022,0,0.129008,"en Verletzungen gestorben.’ mt: ‘The victim was later at the consequences of the serious injuries died.’ ref: ‘The victim later died as a result of the serious injuries.’ (Bojar et al., 2014). This can affect the understanding of the sentence, the overall logic of it in the context of the surrounding sentences, or simply require a reread which itself is indicative of impaired coherence. Discourse connectives Discourse connectives are vital for the correct understanding of discourse. Yet in MT systems these can be incorrect or missing (Meyer and Pol´akov´a, 2013; Meyer and Popescu-Belis, 2012; Meyer et al., 2011; Steele, 2015). In particular, where discourse connectives are ambiguous, e.g. those which can be temporal 54 src: ‘Bereits im Jahr 1925 wurde in Polen eine Eisenbahn-Draisine gebaut, f¨ur die ein Raketenantrieb geplant war. Der Autor des Entwurfs und die Details dieses Vorhabens blieben leider unbekannt.’ mt: ‘Already in 1925 a railway trolley was built in Poland, for which a rocket was planned. The author of the design and the details of the project remained unfortunately unknown.’ ref: In 1925, Poland had already built a handcar which was supposed to be fitted with a rocket engine. Unfortu"
W15-2507,D13-1163,0,0.212259,"acheva and Specia (2015) inject various types of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work i"
W15-2507,potet-etal-2012-collection,0,0.0699691,"Missing"
W15-2507,C14-1090,0,0.0496657,"ziz@uva.nl Abstract structure of discourse. As a result, we believe that a coherent discourse should have a context and a focus, be characterised by appropriate coherence relations, and structured in a logical manner. Previous computational models for assessing coherence in a monolingual context have covered entity transitions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Burstein et al., 2010; Guinaudeau and Strube, 2013), syntactic patterns (Louis and Nenkova, 2012), discourse relations (Lin et al., 2011), distributed sentence representations (Li and Hovy, 2014) and lexical chains (Somasundaran et al., 2014). For evaluation, these studies in coherence have typically used automatically summarized texts, or texts with sentences artificially shuffled as their ‘incoherent’ data. The latter is an example of artificially created labelled data, distorting the ordered logic of the text and thus affecting some aspects of coherence. However, it is inadequate for our task, as MT preserves the sentence ordering, but suffers from other aspects of incoherence. Moreover, while the MT output can potentially be considered ‘incoherent’, it contains a multitude of problems, which are not all due to lack of coherenc"
W15-2507,P98-2202,0,0.0661022,"e elements, in terms of cue words, and their organisation. A comparison of the discourse connectives in the MT and the Human Translation (HT) will be established, and where these differ, a syntactic check is made automatically (Pitler and Nenkova, ) to establish if the connective is a synonym or incorrect. We can also refer to the discourse connectives in the original source text, and automatically check, for example, if the correct sense of the connective has been transferred. These can be identified from a list compiled from appropriate resources (e.g. DiMLex for German, LexConn for French)(Stede and Umbach, 1998; Roze and Danlos, 2012) and a list of problematic ones derived e.g. from work by (Meyer and Popescu-Belis, 2012; Meyer and Pol´akov´a, 2013) for French. 5 Conclusion We can parse the discourse tree structure and extract grammatical information using the Stanford parser2 and POS tagger3 , before distorting the parse tree by swapping nodes at the relevant level. References We have introduced our initiative for artificially generating a corpus with coherence errors from well-formed data that specifically simulate coherence issues in MT. Other possible direction could be to use an nbest list, tak"
W15-2507,N15-2015,0,0.11992,"orben.’ mt: ‘The victim was later at the consequences of the serious injuries died.’ ref: ‘The victim later died as a result of the serious injuries.’ (Bojar et al., 2014). This can affect the understanding of the sentence, the overall logic of it in the context of the surrounding sentences, or simply require a reread which itself is indicative of impaired coherence. Discourse connectives Discourse connectives are vital for the correct understanding of discourse. Yet in MT systems these can be incorrect or missing (Meyer and Pol´akov´a, 2013; Meyer and Popescu-Belis, 2012; Meyer et al., 2011; Steele, 2015). In particular, where discourse connectives are ambiguous, e.g. those which can be temporal 54 src: ‘Bereits im Jahr 1925 wurde in Polen eine Eisenbahn-Draisine gebaut, f¨ur die ein Raketenantrieb geplant war. Der Autor des Entwurfs und die Details dieses Vorhabens blieben leider unbekannt.’ mt: ‘Already in 1925 a railway trolley was built in Poland, for which a rocket was planned. The author of the design and the details of the project remained unfortunately unknown.’ ref: In 1925, Poland had already built a handcar which was supposed to be fitted with a rocket engine. Unfortunately, both th"
W15-2507,W10-2602,0,0.0432997,"pes of errors to generate negative data for quality estimation purposes, but these are at the word level, and the process was guided by post-editing data. They derived an error distribution of MT output by inspecting post editing data. We do not have a similar way of inducing a distribution of errors for coherence. A large amount of post editings of entire documents would be needed, and it still be difficult to isolate which of the edits relate to coherence errors. There has been previous work in the area of lexical cohesion in MT (Wong and Kit, 2012; Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012). Lexical cohesion is part of coherence, as it looks at the linguistic elements which hold a text together. However, there has been very little work in the wider area of coherence as a whole. Besides lexical cohesion, another discourse related phenomenon that has been addressed in MT is reference resolution. As detailed in greater depth by Hardmeier (2012), the results for earlier attempts to address this issue were not very successful (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010). More recent work includes that of Guillou (2012), which h"
W15-2507,N10-1099,0,\N,Missing
W15-2507,P06-1032,0,\N,Missing
W15-2507,J08-1001,0,\N,Missing
W15-2507,C98-2197,0,\N,Missing
W15-2507,W15-3001,1,\N,Missing
W15-2507,2015.eamt-1.8,1,\N,Missing
W15-2507,W13-2201,1,\N,Missing
W15-2507,W13-3306,0,\N,Missing
W16-2213,N12-1047,0,0.02632,". Let (s, t) be a training instance consisting of a source sentence s and a target sentence t and let s′ be the target-order source sentence obtained via the word alignments. For each training instance, we select the preordered source ˆs′ as follows: In our translation experiments, we use the following experimental setup, datasets and parameters. Translation system Translation experiments are performed with a phrase-based machine translation system, a version of Moses (Koehn et al., 2007) with extended lattice support.10 We use the basic Moses features and perform 15 iterations of batch MIRA (Cherry and Foster, 2012). English–Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task. Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9. All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the paralle"
W16-2213,P05-1066,0,0.186788,"ereas lattices enable further improvement for preordering English into the strict word order language Japanese, lattices in conjunction with our proposed lattice silver training scheme turn out to be crucial to reach satisfactory empirical performance for English–German. This result highlights that when predicting word order of free word order languages given source clues only, it is important to ensure that the word order predictions and the backend system are suitably fitted together. Many approaches to preordering have made use of syntactic representations of the source sentence, including Collins et al. (2005) who restructure the source phrase structure parse tree by applying a sequence of transformation rules. More recently, Jehl et al. (2014) learn to order sibling nodes in the source-side dependency parse tree. The space of possible permutations is explored via depth-first branch-and-bound search (Balas and Toth, 1983). In later work, the authors further improve this model by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015), which results in improved empirical results and eliminates the need for feature engineering. Lerner and Petrov (2013"
W16-2213,D08-1089,0,0.0903304,"2007) with extended lattice support.10 We use the basic Moses features and perform 15 iterations of batch MIRA (Cherry and Foster, 2012). English–Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task. Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9. All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Commentary corpus. We removed all sentences longer than 80 words and tokenization and truecasing is performed using the standard Moses tokenizer and truecaser. We use a 5-gram Kneser-Ne"
W16-2213,graca-etal-2008-building,0,0.380872,"Missing"
W16-2213,W06-1609,0,0.053661,"Missing"
W16-2213,P13-2121,0,0.0729712,"Missing"
W16-2213,N15-1105,0,0.173681,"Missing"
W16-2213,2013.iwslt-papers.11,0,0.213256,"a-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of latent reordering grammars (Stanojevi´c and Sima’an, 2015) based on permutation trees (Zhang and Gildea, 2007). 2 Related Work Preordering has been explored from the perspective of the upper-bound achievable translation quality in several studies, including Khalilov and Sima’an (2012) and Herrmann et al. (2013), which compare various systems and provide oracle scores for syntax-based preordering models. Target-order source sentences, in which the word order is determined via automatic alignments, enable translation systems great jumps in translation quality and provide improvements in compactness and efficiency of downstream phrase-based translation models. Approaches have largely followed two directions: (1) predicting word order based on some form of source-syntactic representation and (2) approaches which do not depend on source syntax. 2.3 Lattice Translation A lattice is an acyclic finite-state"
W16-2213,E14-1026,0,0.201465,"Missing"
W16-2213,D11-1018,0,0.0161892,"ring without Source Syntax Tromble and Eisner (2009) learn to predict the orientation of any two words (straight or inverted order) using a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of latent reordering grammars (Stanojevi´c and Sima’an, 2015) based on permutation trees (Zhang and Gildea, 2007). 2 Related Work Preordering has been explored from the perspective of the upper-bound achievable translation quality in several studies, including Khalilov and Sima’an (2012) and Herrmann et al. (2013), which compare various systems and provide oracle scores for syntax-based preordering models. Target-order source sentences, in which the word order is determined via automatic alignments, enable translation systems great jumps in translation quality and provide i"
W16-2213,P09-1064,0,0.0320201,"18:18 664 18:18 608 627 646 665 (a) Linear form. (b) Minimized lattice. Figure 3: Example permutation lattice. 5 Machine Translation with Permutation Lattices 5.1 Permutation Lattices ∏ where P (d) = r∈d P (r) is the probability of a derivation, and Chart(s) is the space of all possible derivations of all possible permutation trees for source sentence s. Two main modifications to this formula are made in order to make inference fast: First, Kendall τ is used as a cost function because it decomposes well,8 which allows usage of efficient dynamic programming minimum Bayesrisk (MBR) computation (DeNero et al., 2009). Second, instead of computing the MBR derivation over the full chart, computation is done over 10,000 unbiased samples from the chart. To build the permutation lattice with this model we use the top n permutations which have the lowest expected Kendall τ cost. We call a permutation lattice for sentence s = ⟨s1 , . . . , sn ⟩ an acyclic finite-state automaton where every path from the initial state reaches an accepting state in exactly n uniquely labeled transitions. Transitions are labeled with pairs in {(i, si )ni=1 } and each path represents an arbitrary permutation of the source’s n tokens"
W16-2213,W09-2310,0,0.0203734,", 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d gr"
W16-2213,P08-1115,0,0.108666,"y due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d grows with O(stack size × n × 2d ) where n represents the inp"
W16-2213,W01-1812,0,0.108479,"3). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d grows with O(stack size × n × 2d ) where n represents the input length, and the number of translation options is capped due to beam search (Koehn et al., 2003). With lattice input, the dependency on n is replaced by |Q |where Q is the set of states of the lattice. The stack size makes the number of translation options explored by the decoder independent of the number of transitions in the lattice. As in standard decoding, the states of a lattice can also"
W16-2213,W07-0729,0,0.0350462,"ctions: (1) predicting word order based on some form of source-syntactic representation and (2) approaches which do not depend on source syntax. 2.3 Lattice Translation A lattice is an acyclic finite-state automaton defining a finite language. A more restricted class of lattices, namely, confusion networks (Bertoldi et al., 2007), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation bo"
W16-2213,knight-al-onaizan-1998-translation,0,0.243919,"tworks (Bertoldi et al., 2007), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a general"
W16-2213,W14-0313,0,0.173881,"ge they consider. Our estimation only involves word-aligned bilingual sentence pairs with a source dependency tree. Manual alignments are available for a limited number of language pairs and often only for a diminishingly small number of sentences. Consequently the question arises, whether automatic word alignments are sufficient for this task. To answer this question, we apply our measure to a set of manually aligned as well as a larger set of automatically aligned sentence pairs. In addition to the German and Japanese alignments mentioned above, we use manual alignments for English–Italian (Farajian et al., 2014), English–French (Och and Ney, 2003), English–Spanish (Grac¸a et al., 2008) and English– Portuguese (Grac¸a et al., 2008). 3.2 Bilingual Head Direction Entropy While such a qualitative comparison provides insight into the order differences of selected language pairs, it is not straight-forward to compare across many language pairs. From a linguistic perspective, Futrell et al. (2015) use entropy to compare word order freedom in dependency corpora across various languages. While the authors observed that artifacts of the data such as treebank annotation style can hamper comparability, they foun"
W16-2213,N03-1017,0,0.0335767,"sed MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Klein and Manning, 2001). In this work, we focus on phrase-based models. The space of translation options in standard phrase-based decoding with a distortion limit d grows with O(stack size × n × 2d ) where n represents the input length, and the number of translation options is capped due to beam search (Koehn et al., 2003). With lattice input, the dependency on n is replaced by |Q |where Q is the set of states of the lattice. The stack size makes the number of translation options explored by the decoder independent of the number of transitions in the lattice. As in standard decoding, the states of a lattice can also be visited non-monotonically. However, two states in a lattice are not always connected by a path, and, in general, paths connecting two nodes might differ in length. Dyer et al. (2008) proposed to pick the shortest path between two nodes to be representative of the distance between them.2 Just like"
W16-2213,W15-2112,0,0.377773,", we can reach far more complex word orders. Crucially, our models are better predictors of word order than standard distortion-based reordering, thus we manage to decode with relatively small permutation lattices. While varying degrees of word order freedom are a well-studied topic in linguistics, word order freedom has only recently been studied from a quantitative perspective. This has been enabled partly by the increasing availability of syntactic treebanks. Kuboˇn and Lopatkov´a (2015) propose a measure of word order freedom based on a set of six common word order types (SVO, SOV, etc.). Futrell et al. (2015) define various entropy measures based on the prediction of word order given unordered dependency trees. Both approaches require a dependency treebank for each language. In practical applications such as machine translation, it is difficult to quantify the influence of word order freedom. For an arbitrary language pair, our goal is to quantify a notion of the target language’s word order freedom based only on parallel sentences and source syntax. In their head direction entropy measure, Futrell et al. (2015) approach the problem of quantifying word order freedom by measuring the difficulty of"
W16-2213,P11-2093,0,0.0267283,"ntal setup, datasets and parameters. Translation system Translation experiments are performed with a phrase-based machine translation system, a version of Moses (Koehn et al., 2007) with extended lattice support.10 We use the basic Moses features and perform 15 iterations of batch MIRA (Cherry and Foster, 2012). English–Japanese Our experiments are performed on the NTCIR-8 Patent Translation (PATMT) Task. Tuning is performed on the NTCIR-7 dev sets, and translation is evaluated on the test set from NTCIR-9. All data is tokenized (using the Moses tokenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Comme"
W16-2213,W04-3250,0,0.126483,"he training data is extracted. Table 3 shows that for English– German, lattice silver training is successful in bridging the gap between the preordering model and the alignment-based target word order, both for monotonic translation and when allowing the decoder to additionally reorder translations. 6.2 Translation Experiments Distortion limit We report lowercased BLEU (Papineni et al., 2002) and Kendall τ calculated from the forcealigned hypothesis and reference. Statistical significance tests are performed for the translation scores using the bootstrap resampling method with p-value < 0.05 (Koehn, 2004). The standard preordering systems (“first-best” in Table 2 and 4) use an additional lexicalized reordering model (MSD), while the lattice systems use only lattice distortion. For training preordered translation models, we recreate word alignments from the original MGIZA alignments and the permutation for En– De and re-align preordered and target sentences for En–Ja using MGIZA.13 Gold training Lattice silver training 0 3 21.44 21.88 21.60 21.88 Table 3: Lattice silver training (BLEU, En–De). English–Japanese Results for translation into Japanese are shown in Table 4. Discussion Although preor"
W16-2213,2005.mtsummit-papers.11,0,0.0474772,"kenizer for English and KyTea 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Commentary corpus. We removed all sentences longer than 80 words and tokenization and truecasing is performed using the standard Moses tokenizer and truecaser. We use a 5-gram Kneser-Ney language model, estimated using lmplz (Heafield ˆs′ = arg max overlap(ˆs′L , s′ ) ˆ s′L ∈ πk (s) where πk (s) is the set of k-best permutations predicted by the preordering model. Each ˆs′L ∈ πk (s) represents a single path through the lattice. As 10 Made available at https://github.com/ wilkeraziz/mosesdecoder. 11 http://statmt.org/wmt16/ 125 et al., 2013). The"
W16-2213,J03-1002,0,0.0184192,"volves word-aligned bilingual sentence pairs with a source dependency tree. Manual alignments are available for a limited number of language pairs and often only for a diminishingly small number of sentences. Consequently the question arises, whether automatic word alignments are sufficient for this task. To answer this question, we apply our measure to a set of manually aligned as well as a larger set of automatically aligned sentence pairs. In addition to the German and Japanese alignments mentioned above, we use manual alignments for English–Italian (Farajian et al., 2014), English–French (Och and Ney, 2003), English–Spanish (Grac¸a et al., 2008) and English– Portuguese (Grac¸a et al., 2008). 3.2 Bilingual Head Direction Entropy While such a qualitative comparison provides insight into the order differences of selected language pairs, it is not straight-forward to compare across many language pairs. From a linguistic perspective, Futrell et al. (2015) use entropy to compare word order freedom in dependency corpora across various languages. While the authors observed that artifacts of the data such as treebank annotation style can hamper comparability, they found that a simple entropy measure for"
W16-2213,P06-1146,0,0.314577,"Missing"
W16-2213,P02-1040,0,0.0985882,"alignments. performs better even when translating monotonically with a distortion limit of 0. Lattice silver training To examine the utility of the lattice silver training scheme, we train systems which differ only in the way the training data is extracted. Table 3 shows that for English– German, lattice silver training is successful in bridging the gap between the preordering model and the alignment-based target word order, both for monotonic translation and when allowing the decoder to additionally reorder translations. 6.2 Translation Experiments Distortion limit We report lowercased BLEU (Papineni et al., 2002) and Kendall τ calculated from the forcealigned hypothesis and reference. Statistical significance tests are performed for the translation scores using the bootstrap resampling method with p-value < 0.05 (Koehn, 2004). The standard preordering systems (“first-best” in Table 2 and 4) use an additional lexicalized reordering model (MSD), while the lattice systems use only lattice distortion. For training preordered translation models, we recreate word alignments from the original MGIZA alignments and the permutation for En– De and re-align preordered and target sentences for En–Ja using MGIZA.13"
W16-2213,N03-1019,0,0.0274531,"), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice translation both for phrase-based and hierarchical phrasebased MT. The former requires a modification of the standard phrase-based decoding algorithm as to maintain a coverage vector over states, rather than input word positions. The latter requires intersecting a lattice and a context-free grammar, which can be seen as a generalized form of parsing (Kl"
W16-2213,P06-1055,0,0.0221854,"ial permutation π ′ of length k ′ by 7 Our implementation is based on http://nlg.isi. edu/software/nplm/. 123 sentence and its permutation are observed during training. The exact PET that generated this permutation is not observed and there could be (exponentially) many PETs that could have generated the observed permutation. Hence, the bracketings of potential PETs are treated as latent variables. The second source of latent variables is state splitting of non-terminals (labels that indicate how to reorder the children) in a similar way as done in monolingual parsing (Matsuzaki et al., 2005; Petrov et al., 2006; Prescher, 2005). Each latent permutation tree has many latent derivations and the generative probabilistic model needs to account for them. The probability of the observed permutation π is defined in the following way: 11:11 16:16 15:15 1 0:0 20 0:0 39 0:0 58 0:0 77 0:0 96 0:0 115 0:0 134 0:0 153 0:0 172 0:0 191 0:0 210 0:0 229 0:0 248 0:0 267 0:0 286 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 2 21 40 59 78 97 116 135 154 173 192 211 230 249 268 287 2:2 2:2 2:2 2:2 2:2 2:2 79 98 2:2 117 2:2 136 2:2 155 2:2 174 2:2 193 2:2 2:2 2:2 2:2 2:2 3:3 4 3:3 23 7:7 6:6 6 7:7 4:4 7"
W16-2213,D13-1049,0,0.419988,"g Collins et al. (2005) who restructure the source phrase structure parse tree by applying a sequence of transformation rules. More recently, Jehl et al. (2014) learn to order sibling nodes in the source-side dependency parse tree. The space of possible permutations is explored via depth-first branch-and-bound search (Balas and Toth, 1983). In later work, the authors further improve this model by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015), which results in improved empirical results and eliminates the need for feature engineering. Lerner and Petrov (2013) train classifiers to predict the permutations of up to 6 tree nodes in the source dependency tree. The authors found that by only predicting the best 20 permutations of n nodes, they could cover a large majority of the reorderings in their data. 2.2 Preordering without Source Syntax Tromble and Eisner (2009) learn to predict the orientation of any two words (straight or inverted order) using a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-"
W16-2213,P13-1135,0,0.0167523,"a 5 for Japanese (Neubig et al., 2011)) and filtered for sentences between 4 and 50 words. As a baseline we use a translation system with distortion limit 6 and a lexicalized reordering model (Galley and Manning, 2008). We use a 5-gram language model estimated using lmplz (Heafield et al., 2013) on the target side of the parallel corpus. English–German For translation into German, we built a machine translation system based on the WMT 2016 news translation data.11 The system is trained on all available parallel data, consisting of 4.5m sentence pairs from Europarl (Koehn, 2005), Common Crawl (Smith et al., 2013) and the News Commentary corpus. We removed all sentences longer than 80 words and tokenization and truecasing is performed using the standard Moses tokenizer and truecaser. We use a 5-gram Kneser-Ney language model, estimated using lmplz (Heafield ˆs′ = arg max overlap(ˆs′L , s′ ) ˆ s′L ∈ πk (s) where πk (s) is the set of k-best permutations predicted by the preordering model. Each ˆs′L ∈ πk (s) represents a single path through the lattice. As 10 Made available at https://github.com/ wilkeraziz/mosesdecoder. 11 http://statmt.org/wmt16/ 125 et al., 2013). The language model is trained on 189m"
W16-2213,P05-1010,0,0.0104461,"ined by extending a partial permutation π ′ of length k ′ by 7 Our implementation is based on http://nlg.isi. edu/software/nplm/. 123 sentence and its permutation are observed during training. The exact PET that generated this permutation is not observed and there could be (exponentially) many PETs that could have generated the observed permutation. Hence, the bracketings of potential PETs are treated as latent variables. The second source of latent variables is state splitting of non-terminals (labels that indicate how to reorder the children) in a similar way as done in monolingual parsing (Matsuzaki et al., 2005; Petrov et al., 2006; Prescher, 2005). Each latent permutation tree has many latent derivations and the generative probabilistic model needs to account for them. The probability of the observed permutation π is defined in the following way: 11:11 16:16 15:15 1 0:0 20 0:0 39 0:0 58 0:0 77 0:0 96 0:0 115 0:0 134 0:0 153 0:0 172 0:0 191 0:0 210 0:0 229 0:0 248 0:0 267 0:0 286 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 1:1 2 21 40 59 78 97 116 135 154 173 192 211 230 249 268 287 2:2 2:2 2:2 2:2 2:2 2:2 79 98 2:2 117 2:2 136 2:2 155 2:2 174 2:2 193 2:2 2:2 2:2 2:2 2:2 3:3 4 3:3 23"
W16-2213,D15-1005,1,0.825977,"Missing"
W16-2213,tiedemann-2012-parallel,0,0.0169245,"entences. Finally, while dependency treebanks rarely cover the same corpora or even domains, our method can utilize sentences from the same or similar corpora for each language, thus minimizing potential corpus biases. Berber Turkish Hebrew German Russian French Spanish Italian Portuguese Esperanto Japanese Mandarin 0 0.2 0.4 0.6 Translation from English Figure 2 plots bilingual head direction entropy for an English source side and a set of typologically diverse languages on the target side. For each language pair, we use 18,000 sentence pairs and automatic alignments from the Tatoeba corpus (Tiedemann, 2012).6 Languages at the top of the plot in Figure 2 show a greater degree of word order freedom with respect to the English source syntax. Thus, predicting their word order from English source clues alone is likely to be difficult. We argue that in such cases it is crucial to pass on the ambiguity over the space of predictions to the translation model. By doing so, word order decisions can be influenced by translation decisions, while still shaping the space of reachable translations. Figure 2: Bilingual head direction entropy with English source side. Since a limited number of manually aligned se"
W16-2213,D09-1105,0,0.0233312,"nd search (Balas and Toth, 1983). In later work, the authors further improve this model by replacing the logistic regression classifier with a feed-forward neural network (de Gispert et al., 2015), which results in improved empirical results and eliminates the need for feature engineering. Lerner and Petrov (2013) train classifiers to predict the permutations of up to 6 tree nodes in the source dependency tree. The authors found that by only predicting the best 20 permutations of n nodes, they could cover a large majority of the reorderings in their data. 2.2 Preordering without Source Syntax Tromble and Eisner (2009) learn to predict the orientation of any two words (straight or inverted order) using a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of la"
W16-2213,J97-3002,0,0.221249,"he input layer, 2 on the output layer, and 50 and 100 on the two hidden layers. We use a learning rate of 0.01, batch size of 1000 and perform 20 training epochs. 4.2 Reordering Grammar Induction Reordering Grammar (RG) (Stanojevi´c and Sima’an, 2015) is a recent approach for preordering that is hierarchical and fully unsupervised. It is based on inducing a probabilistic context-free grammar from aligned parallel data. This grammar can predict permutation trees (PETs) (Zhang and Gildea, 2007) — projective constituency trees that can fully describe any permutation. PETs are reminiscent of ITG (Wu, 1997) with the important distinction that PETs can handle any permutation, unlike ITG which can only handle binarizable ones. As in ITG, constituents in PETs are labeled with the permutation of their children. Induction of RGs is performed by specifying a generative probabilistic model and then estimating its parameters using the EM algorithm. The reasoning behind using EM is that many latent variables are present in the model. Only the source Search Search in this model consists of finding the sequence of swaps leading to the best overall score according to the model. Let a partial permutation of"
W16-2213,2005.iwslt-1.18,0,0.0371916,"followed two directions: (1) predicting word order based on some form of source-syntactic representation and (2) approaches which do not depend on source syntax. 2.3 Lattice Translation A lattice is an acyclic finite-state automaton defining a finite language. A more restricted class of lattices, namely, confusion networks (Bertoldi et al., 2007), has been extensively used to pack alternative input sequences for decoding.1 However, applications mostly focused on speech translation (Ney, 1999; Bertoldi et al., 2007), or to account for lexical and/or segmentation ambiguity due to preprocessing (Xu et al., 2005; Dyer, 2007). In very 1 A confusion network is a special case of a lattice where every path from start to final state goes through every node. 119 3 Quantifying Word Order Freedom few occasions, lattice input has been used to determine the space of permutations of the input considered by the decoder (Knight and Al-Onaizan, 1998; Kumar and Byrne, 2003). The effectiveness of lattices of permutations was demonstrated by Zhang et al. (2007). However, except in the cases of n-gram based decoders (Khalilov et al., 2009) this approach is not a common practice. Dyer et al. (2008) formalized lattice t"
W16-2213,zeman-etal-2012-hamledt,0,0.031049,"Missing"
W16-2213,W07-0404,0,0.181631,"a perceptron. The search for the best reordering is performed with a O(n3 ) chart parsing algorithm. More basic approaches to syntax-less preordering include the application of multiple MT systems (Costa-juss`a and Fonollosa, 2006), where a first system learns preordering and a second learns to translate the preordered sentence into the target sentence. Finally, there have been successful attempts at the automatic induction of parse trees from aligned data (DeNero and Uszkoreit, 2011) and the estimation of latent reordering grammars (Stanojevi´c and Sima’an, 2015) based on permutation trees (Zhang and Gildea, 2007). 2 Related Work Preordering has been explored from the perspective of the upper-bound achievable translation quality in several studies, including Khalilov and Sima’an (2012) and Herrmann et al. (2013), which compare various systems and provide oracle scores for syntax-based preordering models. Target-order source sentences, in which the word order is determined via automatic alignments, enable translation systems great jumps in translation quality and provide improvements in compactness and efficiency of downstream phrase-based translation models. Approaches have largely followed two directi"
W16-2213,W07-0401,0,0.0776075,"Missing"
W16-2213,P07-2045,0,\N,Missing
W16-3407,J93-2003,0,0.0438733,"e representation. Further experiments could use grammatical productions as an alternative. 3.4 Syntax-based model with IBM 1 The syntax model by Louis and Nenkova (2012) does not model latent alignments. This is possible under the assumption that all available alignment configurations have been directly observed in the training data. It is worth highlighting that in reality the training data is incomplete in the sense that it lacks alignment information. We introduce alignments between syntactic patterns in adjacent sentences as a latent variable. Our model does that based on the IBM model 1 (Brown et al., 1993), where the current sentence is generated by the preceding one, one pattern at a time, with a uniform prior over alignment configurations. The latent alignment variable allows us to model the fact that some patterns are more likely to trigger certain subsequent patterns. In IBM model 1, a latent alignment function a : j 7→ i maps patterns in v1n (current sentence) to patterns in um 0 (preceding sentence), where u0 is a special N ULL symbol which models insertion. The score Y of a document is given by Equation 4. P (D) = p(v1 . . . vn , a1 . . . an |u0 . . . um ) (4) n (um 1 ,v1 )∈D Here n is t"
W16-3407,N10-1099,0,0.0277711,"is however guided by elements of discourse that we believe can be modelled automatically to some extent. Most previous computational models for assessing coherence have focused on entity transitions, syntactic patterns and discourse relations. The most popular models are detailed in Section 3. In what follows we describe these models, and our work to apply these models to MT. Lin et al. (2011) evaluate the coherence of texts from discourse role transitions in a grid-based model, on the basis that there is a preferential, canonical, ordering of discourse relations that leads to coherent texts. Burstein et al. (2010) use the entity-grid for student essay evaluation, which is a scenario closer to ours. They used a range of additional features specifically targeting grammar and style. These proved 180 Sim Smith et al. useful for discriminating good from bad quality essays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are not publicly available for us to assess this. Somasundaran et al. (2014) consider how lexical chains affect discourse coherence. They use lexical chaining features such as length, density, and link strength to detect textual c"
W16-3407,W12-3156,0,0.0217953,"strate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and coherence as continuity in meaning and context”. While lexical cohesion can be detected and addressed to so"
W16-3407,W11-1211,0,0.0724736,"Missing"
W16-3407,P11-2022,0,0.0686515,"parser.shtml 182 3.2 Sim Smith et al. Entity graph approach Guinaudeau and Strube (2013) adapted the entity-grid into a graph format using a bipartite graph which they claim avoids the data sparsity issues encountered by Barzilay and Lapata (2008) and achieves equal performance, without training. Additionally, their representation can track any cross-sentential references, as opposed to only those present in adjacent sentences. The graph tracks the presence of all entities and connections to the sentences they occur in, taking all nouns in the document as discourse entities, as recommended by Elsner and Charniak (2011). The coherence of a text in this model is measured by calculating the average outdegree of a projection, summing the shared edges. The general form of the coherence score assigned to a document in this approach is shown in Equation 2. This is a centrality measure based on the average outdegree across the N sentences represented in the document graph. The outdegree of a sentence si , denoted o(si ), is the total weight of edges leaving that sentence, a notion of how connected (or how central) it is. This weight is the sum of the contributions of all edges connecting si to any sj ∈ D. s(D) = N"
W16-3407,W15-2504,0,0.0187447,"They assess their model on an MT reranking task, progressively reranking consecutive sentences. In the MT domain, Xiong et al. (2013) attempt to improve lexical coherence with a topic-based model. They extract a coherence chain for the source sentence, and project it onto the target sentence to try and make lexical choices taken during decoding more coherent. They report very marginal improvement with respect to a baseline system in terms of automatic evaluation. This could indicate that current evaluation metrics are limited in their ability to account for improvements related to discourse. Gong et al. (2015) attempt to integrate their lexical chain and topic-based metrics into traditional BLEU and METEOR scores, showing greater correlation with human judgements on MT output. While the task of automatically evaluating text coherence has been addressed previously within applications such as multi-document text summarisation or in terms of optimal ordering within shuffled texts, our aim is to further investigate these components in an MT context without the use of a reference translation. We ultimately expect to be able to bias the translation process to ensure coherence in MT. 3 Coherence Models He"
W16-3407,P13-1010,0,0.466941,"generated by standard MT systems, on a sentence-by-sentence basis, several phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses (Sim Smith et al., 2015). We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (Barzilay and Lapata, 2008), an entity graph similarity metric (Guinaudeau and Strube, 2013), and a model based on syntactic patterns (Louis and Nenkova, 2012). In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Wo"
W16-3407,J12-4004,0,0.0789939,"Missing"
W16-3407,D14-1218,0,0.0391281,"ays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are not publicly available for us to assess this. Somasundaran et al. (2014) consider how lexical chains affect discourse coherence. They use lexical chaining features such as length, density, and link strength to detect textual continuity, elaboration, lexical variety and organisation, all vital aspects of coherent texts. They claim that the interaction between lexical chains and discourse cues can also show whether cohesive devices are organised in a coherent fashion. Recently, Li and Hovy (2014) developed a coherence model based on distributed sentence representation. They used recurrent and recursive neural networks to perform ordering and readability tasks. They leverage semantic representations to establish coherent orderings, using original texts as positive examples and shuffled versions as negative ones for optimising the neural networks. Li et al. (2015) train a hierarchical Long-Short Term Memory (LSTM) to explore neural Natural Language Generation, and assess whether local semantic and syntactic coherence can be represented at a higher level, namely paragraphs. In their mode"
W16-3407,P15-1107,0,0.0190308,"l variety and organisation, all vital aspects of coherent texts. They claim that the interaction between lexical chains and discourse cues can also show whether cohesive devices are organised in a coherent fashion. Recently, Li and Hovy (2014) developed a coherence model based on distributed sentence representation. They used recurrent and recursive neural networks to perform ordering and readability tasks. They leverage semantic representations to establish coherent orderings, using original texts as positive examples and shuffled versions as negative ones for optimising the neural networks. Li et al. (2015) train a hierarchical Long-Short Term Memory (LSTM) to explore neural Natural Language Generation, and assess whether local semantic and syntactic coherence can be represented at a higher level, namely paragraphs. In their model, different LSTM layers represents word embeddings, sentences, and paragraphs. They are then able to regenerate the text to a degree that indicates neural networks are able to capture certain elements of coherence. Lin and Li (2015) use a hierarchical recurrent neural network language model (RNNLM) to combine a word-level model with a sentence-level model for document m"
W16-3407,D15-1106,0,0.105287,"Missing"
W16-3407,P11-1100,0,0.0612579,"xtual indicators necessary for coherence assessment are much more difficult to capture, even though judging coherence is an intuitive process for a human reader. Coherence is undeniably a complex cognitive process, which is however guided by elements of discourse that we believe can be modelled automatically to some extent. Most previous computational models for assessing coherence have focused on entity transitions, syntactic patterns and discourse relations. The most popular models are detailed in Section 3. In what follows we describe these models, and our work to apply these models to MT. Lin et al. (2011) evaluate the coherence of texts from discourse role transitions in a grid-based model, on the basis that there is a preferential, canonical, ordering of discourse relations that leads to coherent texts. Burstein et al. (2010) use the entity-grid for student essay evaluation, which is a scenario closer to ours. They used a range of additional features specifically targeting grammar and style. These proved 180 Sim Smith et al. useful for discriminating good from bad quality essays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are"
W16-3407,D12-1106,0,0.533596,"eral phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses (Sim Smith et al., 2015). We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (Barzilay and Lapata, 2008), an entity graph similarity metric (Guinaudeau and Strube, 2013), and a model based on syntactic patterns (Louis and Nenkova, 2012). In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT"
W16-3407,W15-2507,1,0.437252,"79 can affect the application of such models in various ways, making it harder to pinpoint coherence-related problems. Finally, judgements on the coherence of the translations may be dependent on the source text. Nevertheless, measuring coherence in MT is important: given the way translations are generated by standard MT systems, on a sentence-by-sentence basis, several phenomena spanning sentence boundaries can lead to incoherent document translations, such as incorrect co-referencing, inadequate discourse markers, and lack of lexical cohesion, as established by previous corpus analyses (Sim Smith et al., 2015). We apply three existing coherence models to original, shuffled and machine translated texts in an attempt to evaluate their ability to discriminate between coherent and incoherent documents: an entity-grid model (Barzilay and Lapata, 2008), an entity graph similarity metric (Guinaudeau and Strube, 2013), and a model based on syntactic patterns (Louis and Nenkova, 2012). In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly con"
W16-3407,C14-1090,0,0.0191759,"e role transitions in a grid-based model, on the basis that there is a preferential, canonical, ordering of discourse relations that leads to coherent texts. Burstein et al. (2010) use the entity-grid for student essay evaluation, which is a scenario closer to ours. They used a range of additional features specifically targeting grammar and style. These proved 180 Sim Smith et al. useful for discriminating good from bad quality essays, but it is unclear how much of the problem with low quality essays was due to coherence issues. Their features are not publicly available for us to assess this. Somasundaran et al. (2014) consider how lexical chains affect discourse coherence. They use lexical chaining features such as length, density, and link strength to detect textual continuity, elaboration, lexical variety and organisation, all vital aspects of coherent texts. They claim that the interaction between lexical chains and discourse cues can also show whether cohesive devices are organised in a coherent fashion. Recently, Li and Hovy (2014) developed a coherence model based on distributed sentence representation. They used recurrent and recursive neural networks to perform ordering and readability tasks. They"
W16-3407,P06-2103,0,0.107905,"nment is hidden, we marginalise over all possible configurations, which is tractable due to an independence assumption (that items align independently of each other). Equation 5 shows this tractable marginalisation. n X m Y Y p(D) = p(vj |ui ) (5) n j=1 i=0 (um 1 ,v1 )∈D We resort to Expectation Maximisation (EM) to estimate the parameters in Equation 5 (Brown et al., 1993): due to the convexity of IBM model 1, EM is guaranteed to converge to a global optimum. Moreover, as we observe more data this model converges to better parameters. A similar solution was proposed in a different context by Soricut and Marcu, (2006) in their work on word co-occurrences. 184 Sim Smith et al. Table 1: Number of documents and sentences in the training (Gigaword) and test (WMT14) sets. Corpus Gigaword WMT14 WMT14 WMT14 Portion Documents Sentences 12/2010 41,564 774,965 de-en 164 3,003 fr-en 176 3,003 ru-en 175 3,003 To avoid assigning 0 probability to documents containing unseen patterns, we modify the training procedure to treat all the singletons as pertaining to an unknown category (U NK), thus reserving probability mass for future unseen items.3 In addition to this special U NK item, we also include N ULL alignments, whi"
W16-3407,W10-2602,0,0.0300047,"tax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and coherence as continuity in meaning and context”. While lexical"
W16-3407,D12-1097,0,0.145699,"In addition, we propose a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and cohe"
W16-3407,D13-1163,0,0.359832,"se a fully generative extension of the syntax-based coherence model. We illustrate the difference between assessing the output from MT systems and assessing the coherence of shuffled texts in a highly consistent, structured corpus. The remainder of this paper, is organised as follows: in Section 2 we review related work on coherence and cohesion in the context of MT. Section 3 covers the background of the coherence models used in this paper. Experiments and results are discussed in Section 4. 2 Related Work There has been recent work in the area of lexical cohesion in MT (Wong and Kit, 2012); Xiong et al., 2013a; Xiong et al., 2013b; Tiedemann, 2010; Hardmeier, 2012; Carpuat and Simard, 2012), as a sub-category of coherence, looking at the linguistic elements which hold a text together. However, there seems to be little work in the wider area of coherence as a whole. Coherence is indeed a more complex discourse element to define in the first place. While it does include cohesion, it also describes how a text becomes semantically meaningful overall, and how easy it is for the reader to follow. Louwerse (2005) defines “cohesion as continuity in word and sentence structure, and coherence as continuity"
W16-3407,J08-1001,0,\N,Missing
W16-3407,D11-1034,0,\N,Missing
W16-3407,W14-3302,1,\N,Missing
W17-4734,W05-0909,0,0.158301,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W17-4734,P09-1064,0,0.0328843,"translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER"
W17-4734,D17-1209,1,0.891192,"Missing"
W17-4734,E14-2008,1,0.856971,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W16-2302,1,0.832947,". Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER (Stanojevic and Simaan, 2014) or C HR F (Popovic, 2015). The entire list of MT candidates is then entirely re-ranked according to the averaged score of each candidate. Different from most re-ranking approaches which make use of additional information usually treated as new model components and combined with the existing ones, we here focus only on the MT candidates. The difference between the consensus-based n-best list selection and an oracle translation is the absence Since only one development set was provided we split the given development set into two parts: newsdev2017/1 a"
W17-4734,W14-3310,1,0.870459,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4703,1,0.884283,"Missing"
W17-4734,2014.iwslt-evaluation.7,1,0.873477,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4737,1,0.831634,"Missing"
W17-4734,W11-2123,0,0.0435093,"o this end, k-best hypothesis from the dictionary were generated, as well as the n-best hypothesis 3.4 Tilde The Tilde system is a Moses phrase-based SMT system that was trained on the Tilde MT platform (Vasil¸jevs et al., 2012). The system was trained using all available parallel data - 1.74 million unique sentence pairs after filtering, and 3 million unique sentence pairs that were acquired by re-translating a random selection of indomain monolingual sentences with a neural machine translation system (Pinnis et al., 2017). The system has a 5-gram language model that was trained using KenLM (Heafield, 2011) on all available monolingual data (27.83 million unique sentences). 3.5 UEDIN The University of Edinburgh’s system is an attentional encoder-decoder (Bahdanau et al., 2015), trained using the Nematus toolkit (Sennrich et al., 2017c). As training data, we used all parallel and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden la"
W17-4734,W15-3049,0,0.0536506,"Missing"
W17-4734,E17-2025,0,0.0291776,"l and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden layers of size 1024, with the size of the source and target network vocabularies fixed to the size of the respective BPE vocabularies. In order to reduce the size of the models, the target-side embedding weights were tied with the transpose of 350 the output weight matrix (Press and Wolf, 2017). We used a deep transition architecture inspired by the one proposed by Zilly et al. (2016) for language modelling. In experiments conducted during feature development, we found that this gave consistent improvements across multiple language pairs. We also applied layer normalisation (Ba et al., 2016) to all recurrent and feed-forward layers, except for layers that are followed by a softmax. In preliminary experiments, we found that using layer normalisation led to faster convergence and resulted in slightly better performance. We trained the models with adam (Kingma and Ba, 2015), using a le"
W17-4734,E17-3017,0,0.0486901,"Missing"
W17-4734,P17-4012,0,0.0301124,"stem for the WMT 2017 shared task for machine translation of news 1 are seven individual 1 http://www.statmt.org/wmt17/ translation-task.html 348 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 348–357 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tool. The number of sentences being removed is approximately 50000. in Neural Monkey. Instead, the translations were generated using greedy search. 3 3.2 Translation Systems The neural machine translation models from KIT are built with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for p"
W17-4734,D17-1159,0,0.0716203,"Missing"
W17-4734,D16-1096,0,0.0289091,"rom backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about the effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of"
W17-4734,P03-1021,0,0.0379532,"re case-sensitive. of reference translation: each translation hypothesis is scored against all the other hypotheses used as references while in an oracle translation each translation hypothesis is scored against a single reference. This results in obtaining as best translation hypothesis the candidate that is most similar to the most likely translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varyi"
W17-4734,P16-1162,0,0.11892,"he effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of the system was built using Neural Monkey2 (Helcl and Libovick´y, 2017), a flexible sequence-to-sequence toolkit implementing primarily the Bahdanau et al. (2015) model but useful also in multi-modal translation and multi-task training. We used essentially the baseline setup of the system as released for the WMT17 NMT Training Task3 (Bojar et al., 2017) for an 8GB GPU card. This involves BPE (Sennrich et al., 2016) with 30k merges, maximum sentence length for both source and target limited to 50 (BPE) tokens, no dropout and embeddings (both source and target) of 600, vocabulary shared between encoder and decoder, attention and conditional GRU (Firat and Cho, 2016). We experimented with the RNN size of the encoder and decoder and increased them to 800 instead of 600, at the expense of reducing batch size to 10. The batch size of 30 with this enlarged model would still fit into our GPU card but this run was prematurely interrupted due to a hardware failure and we noticed that it converges slower in terms"
W17-4734,W14-3354,0,0.0640118,"Missing"
W17-4734,P16-5005,0,0.0206781,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P16-1008,0,0.0235141,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P12-3008,0,0.0243602,"Missing"
W19-4315,P02-1040,0,0.10354,"Missing"
W19-4315,C14-1182,0,0.0398222,"Missing"
W19-4315,W18-6319,0,0.0167275,"ther the generative model we propose is robust to unknown and diverse sources of variation mixed together in one training set (e.g. NC + IWSLT or gold-standard + synthetic data). However, note that a point we are certainly not trying to make is that the model has been designed to perform domain adaptation. Nonetheless, in Appendix C we try to shed light on what happens when we use the model to translate genres we additionally report METEOR (Denkowski and Lavie, 2011) and TER (Snover et al., 2006). We de-truecase and de-tokenize our system’s predictions and compute BLEU scores using SacreBLEU (Post, 2018).6 For BEER, METEOR and TER, we tokenize the results and test sets using the same tokenizer as used by SacreBLEU. We make use of BEER 2.0, and for METEOR and TER use M ULT E VAL (Clark et al., 2011). In Appendix D we report validation results, in this case in terms of BLEU alone as that is what we used for model selection. Finally, to give an indication of the degree to which results are sensitive to initial conditions (e.g. random initialisation of parameters), and to avoid possibly misleading signifiance testing, we report the average and standard deviation of 5 independently trained models."
W19-4315,E17-1101,0,0.0173862,"translate target monolingual data to the source language (Sennrich et al., 2016a). Translation direction, Contributions We introduce a deep generative model for NMT (§3) and discuss theoretical advantages of joint modelling over conditional modelling (§3.1). We also derive an efficient approximation to MAP decoding that requires only a single forward pass through the network for prediction (§3.3). Finally, we show in §4 that our proposed model improves translation performance in at least three practical scenarios: i) in-domain 1 Also note that this list is by no means exhaustive. For example, Rabinovich et al. (2017) show influence of factors such as personal traits and demographics in translation. Another clear case is presented by Johnson et al. (2017), who combine parallel resources for multiple languages to train a single encoder-decoder architecture. 124 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 124–141 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics training on little data, where test data are expected to follow the training data distribution closely; ii) mixed-domain training, where we train a single model but test in"
W19-4315,Q17-1024,0,0.051563,"Missing"
W19-4315,P16-1009,0,0.511177,"these systems are a byproduct of mixing different sources of data. For example, labelled data are sometimes obtained by putting together corpora from different domains (Sennrich et al., 2017). Even for a single domain, parallel data often result from the combination of documents independently translated from different languages by different people or agencies, possibly following different guidelines. When resources are scarce, it is not uncommon to mix in some synthetic data, e.g. bilingual data artificially obtained by having a model translate target monolingual data to the source language (Sennrich et al., 2016a). Translation direction, Contributions We introduce a deep generative model for NMT (§3) and discuss theoretical advantages of joint modelling over conditional modelling (§3.1). We also derive an efficient approximation to MAP decoding that requires only a single forward pass through the network for prediction (§3.3). Finally, we show in §4 that our proposed model improves translation performance in at least three practical scenarios: i) in-domain 1 Also note that this list is by no means exhaustive. For example, Rabinovich et al. (2017) show influence of factors such as personal traits and"
W19-4315,P16-1162,0,0.778695,"these systems are a byproduct of mixing different sources of data. For example, labelled data are sometimes obtained by putting together corpora from different domains (Sennrich et al., 2017). Even for a single domain, parallel data often result from the combination of documents independently translated from different languages by different people or agencies, possibly following different guidelines. When resources are scarce, it is not uncommon to mix in some synthetic data, e.g. bilingual data artificially obtained by having a model translate target monolingual data to the source language (Sennrich et al., 2016a). Translation direction, Contributions We introduce a deep generative model for NMT (§3) and discuss theoretical advantages of joint modelling over conditional modelling (§3.1). We also derive an efficient approximation to MAP decoding that requires only a single forward pass through the network for prediction (§3.3). Finally, we show in §4 that our proposed model improves translation performance in at least three practical scenarios: i) in-domain 1 Also note that this list is by no means exhaustive. For example, Rabinovich et al. (2017) show influence of factors such as personal traits and"
W19-4315,D16-1050,0,0.0271503,"a¨ıve to expect we can model them well with a simple Gaussian prior. If that turns out to be the case, we will investigate mixing Gaussian components (Miao et al., 2016; Srivastava and Sutton, 2017) and/or employing a hierarchical prior (Goyal et al., 2017). Variational LMs and NMT Bowman et al. (2016) first proposed to augment a neural language model with a prior over latent space. Our source component is an instance of their model. More recently, Xu and Durrett (2018) proposed to use a hyperspherical uniform prior rather than a Gaussian and showed the former leads to better representations. Zhang et al. (2016) proposed the first VAE for NMT. They augment the conditional with a Gaussian sentence embedding and model observations as draws from the marginal R P (y|x, θ) = p(z|x, θ)P (y|x, z, θ)dz. Their formulation is a conditional deep generative model (Sohn et al., 2015) that does not model the source side of the data, where, rather than a fixed standard Gaussian, the latent model is itself parameterised and depends on the data. Schulz et al. (2018) extend the model of Zhang et al. (2016) with a 131 Acknowledgements from a continuous space. In Proceedings of CoNLL, 2016, pages 10–21, Berlin, Germany."
W19-4315,D16-1160,0,0.0233266,", 2017), as well as for NMT (Chu and Wang, 2018), but a full characterisation of this exciting field is beyond the scope of this paper. Multi-task learning An alternative to joint learning is to turn to multi-task learning and explore parameter sharing across models trained on different, though related, data with different objectives. For example, Cheng et al. (2016) incorporate both source and target monolingual data by multi-tasking with a non-differentiable autoencoding objective. They jointly train a source-totarget and target-to-source system that act as encoder and decoder respectively. Zhang and Zong (2016) combine a source language model objective with a source-to-target conditional NMT objective and shared the source encoder in a multitask learning fashion. 6 Discussion and Future Work We have presented a joint generative model of translation data that generates both observations conditioned on a shared latent representation. Our formulation leads to questions such as why joint learning? and why latent variable modelling? to which we give an answer based on statistical facts about conditional modelling and marginalisation as well as empirical evidence of improved performance. Our model shows m"
