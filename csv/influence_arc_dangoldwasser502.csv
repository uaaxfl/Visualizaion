2020.acl-main.476,C18-1013,0,0.139071,"well as the complexity of the processes that differ across states, often leads to public disengagement from local politics. This results in decisions being made with little understanding of the processes that shape them and how they are likely to influence different demographics. Similarly, most effort directed at understanding political processes using data was directed at the Federal level. In the NLP community, several works looked at analyzing political texts (Iyyer et al., 2014) and the resulting behaviors of legislators (Gerrish and Blei, 2011, 2012). The only exception is recent work (Eidelman et al., 2018), predicting whether a bill would pass the preliminary stage, legislative committee, to a full-body vote. State-level demographic cleavages: Our goal in this paper is to take a first step towards understanding the processes and interests that underlie how decisions are passed using data-driven methods. Our main intuition is that the impact of bills on different demographics will be reflected in the behavior and voting patterns of their representatives. Thus, providing the ability to automatically identify bills, before they are put to a vote, that will have a positive or negative influence on"
2020.acl-main.476,P18-2081,0,0.464515,"Missing"
2020.acl-main.476,D16-1221,0,0.350007,"Missing"
2020.acl-main.476,D17-1159,0,0.0265073,"Missing"
2020.acl-main.476,K19-1053,0,0.394861,"Missing"
2020.acl-main.476,P14-1105,0,0.0327046,"lls than their Federal counterparts, adding up to over 120,000 bills per year (King, 2019). Also, the lack of general interest, as well as the complexity of the processes that differ across states, often leads to public disengagement from local politics. This results in decisions being made with little understanding of the processes that shape them and how they are likely to influence different demographics. Similarly, most effort directed at understanding political processes using data was directed at the Federal level. In the NLP community, several works looked at analyzing political texts (Iyyer et al., 2014) and the resulting behaviors of legislators (Gerrish and Blei, 2011, 2012). The only exception is recent work (Eidelman et al., 2018), predicting whether a bill would pass the preliminary stage, legislative committee, to a full-body vote. State-level demographic cleavages: Our goal in this paper is to take a first step towards understanding the processes and interests that underlie how decisions are passed using data-driven methods. Our main intuition is that the impact of bills on different demographics will be reflected in the behavior and voting patterns of their representatives. Thus, prov"
2020.acl-main.476,D14-1162,0,0.105646,"Missing"
2020.acl-main.476,N12-1097,0,0.0724879,"Missing"
2020.coling-main.323,W13-3520,0,0.0372364,"tion, including Precision, NDCG, MAP and MRR. For each query, the corresponding documents are sorted by their relevance score, and the metrics are averaged over all queries. Pmr @1 represents the precision for MR document at top 1 position. Pr @5 represents the precision for MR and SR documents combined at top 5 positions. NDCG@5 is Normalized Discounted Cumulative Gain for top 5 documents. MAP is calculated as the mean of the average precision scores for each query. M RRmr is the Mean reciprocal rank of MR document. 3621 Experimental Setup In our experiments, we use the pre-trained Polyglot [Al-Rfou et al., 2013] embeddings of dimension 64 as the initialization for the corresponding languages. These embeddings are fine-tuned during the training. We also observe improved performance by shuffling the training set. We select the thresholds θ = (0.2, 0.7) for SOSL, via cross-validation using grid searching. We use Adam optimizer in all experiments for optimization, with fixed learning rate 0.01. We set batch sizes as 128 and we stop after 30 epochs for all languages, resulting in about 310,000 training steps on French and 240,000 on Tagalog, while the other two languages’ training steps are in-between. W"
2020.coling-main.323,D14-1162,0,0.0854648,"ly relevant, and irrelevant) in document search with huge amount of real commercial data. Cross-Lingual Information Retrieval Traditionally, Cross-Lingual Information Retrieval (CLIR) is conducted in two steps in a pipeline: machine translation followed with monolingual information retrieval [Nie, 2010]. However, this approach requires a well-trained translation model and usually suffers from translation ambiguity [Zhou et al., 2012]. The error propagation from machine translation may even deteriorate the retrieval results. As an alternative, pre-trained word embeddings [Mikolov et al., 2013; Pennington et al., 2014] have led to a surge of improved performance on many language tasks, which learns word representations of different languages on large scale text corpora. Nevertheless, the training objective of these embeddings are different from IR tasks, thus their direct application may be limited. Generalization Error There has been previous works on the generalization error of learn-to-rank models. Lan et al. [2008] analyzed the stability of pairwise models and gave query-level generalization error bounds. Lan et al. [2009] provided a theoretical framework for ranking algorithms and proved generalizatio"
2020.coling-main.323,N18-2073,0,0.109159,"o indicate the relevance score r, which avoids gradient explosion and therefore stabilizes the training process. Finally, we introduce Smooth Ordinal Search Loss for optimizing r. 3.1 Text Representation We use embeddings as low dimensional dense vectors to represent both queries and documents. Differing from mono-lingual tasks, in cross-lingual document retrieval one can rarely observe common tokens between queries and documents. Therefore, cross-lingual document retrieval usually requires embeddings built from different vocabularies as queries and documents are from two different languages [Sasaki et al., 2018]. This requirement naturally expands the size of parameters of the model if we regard embeddings as part of model parameters and fine-tune them during training, thus the retrieval model itself demands a higher stability. Queries and documents are represented numerically. A query q is tokenized into a list of words q = q1 q2 ...qlq of length lq . For example, the tokenization of “Apple is a fruit” is [“Apple”, “is”, “a”, “fruit”] of length 4. Similarly, d = d1 d2 ...dld is an expression of a document of length ld from document language. For simplicity, we use A and B to represent query languag"
2020.coling-main.323,C12-1164,0,0.0116762,". Differing from previous works that used click-through data with only two classes, Nigam et al. [2019] proposed a loss function that differentiates three classes (relevant, partially relevant, and irrelevant) in document search with huge amount of real commercial data. Cross-Lingual Information Retrieval Traditionally, Cross-Lingual Information Retrieval (CLIR) is conducted in two steps in a pipeline: machine translation followed with monolingual information retrieval [Nie, 2010]. However, this approach requires a well-trained translation model and usually suffers from translation ambiguity [Zhou et al., 2012]. The error propagation from machine translation may even deteriorate the retrieval results. As an alternative, pre-trained word embeddings [Mikolov et al., 2013; Pennington et al., 2014] have led to a surge of improved performance on many language tasks, which learns word representations of different languages on large scale text corpora. Nevertheless, the training objective of these embeddings are different from IR tasks, thus their direct application may be limited. Generalization Error There has been previous works on the generalization error of learn-to-rank models. Lan et al. [2008] ana"
2020.coling-main.344,P16-1231,0,0.0176241,"blem to find the most probable tree, while transition-based parsers [Nivre, 2004, 2008] treat parsing as a sequence of actions at different stages leading to a complete dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures that were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transitionbased parsing [Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016]. The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing [Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2018]. It also leads to advances in using unlabeled data for constituent grammar [Shen et al., 2018b,a] Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires margina"
2020.coling-main.344,D17-1209,0,0.0243796,"ducted experiments on WSJ and UD dependency parsing data sets, showing that our models can exploit the unlabeled data to improve the performance given a limited amount of labeled data, and outperform a previously proposed semi-supervised model. 1 Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing [Reddy et al., 2016; Marcheggiani and Titov, 2017], machine translation [Bastings et al., 2017; Ding and Palmer, 2007], information extraction [Culotta and Sorensen, 2004; Liu et al., 2015] and question answering [Cui et al., 2005]. As a result, efficient parsers [Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018] have been developed using various neural architectures. root pu nsubj poss advmod xcomp dobj PRP$ NN RB VBZ VBG NN PUNC My dog also likes eating sausage . Figure 1: A dependency tree: directional arcs represent head-modifier relation between words. Despite the success of supervised approaches, they require large amounts of labeled da"
2020.coling-main.344,D17-1171,0,0.0502225,"Missing"
2020.coling-main.344,D18-1020,0,0.0232324,"2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2018]. It also leads to advances in using unlabeled data for constituent grammar [Shen et al., 2018b,a] Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches have been widely applied to alleviating this problem, as they try to improve the lower bound of the original objective, and were employed in several recent NLP works [Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a]. Variational Autoencoder (VAE) [Kingma and Welling, 2014] is particularly useful for latent representation learning, and is studied in semi-supervised context as the Conditional VAE (CVAE) [Sohn et al., 2015]. Note our work differs from VAE as VAE is designed for tabular data but not for structured prediction, as in our circumstance, the input are the sentential tokens and the output is the dependency tree. The work mostly related to ours is Corro and Titov [2018]’s as they consider the dependency tree as the latent variable, but their work takes a second approximation t"
2020.coling-main.344,P04-1054,0,0.154174,"that our models can exploit the unlabeled data to improve the performance given a limited amount of labeled data, and outperform a previously proposed semi-supervised model. 1 Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing [Reddy et al., 2016; Marcheggiani and Titov, 2017], machine translation [Bastings et al., 2017; Ding and Palmer, 2007], information extraction [Culotta and Sorensen, 2004; Liu et al., 2015] and question answering [Cui et al., 2005]. As a result, efficient parsers [Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018] have been developed using various neural architectures. root pu nsubj poss advmod xcomp dobj PRP$ NN RB VBZ VBG NN PUNC My dog also likes eating sausage . Figure 1: A dependency tree: directional arcs represent head-modifier relation between words. Despite the success of supervised approaches, they require large amounts of labeled data, particularly when neural architectures are used. Syntactic annotation is"
2020.coling-main.344,P08-1068,0,0.0823343,"hand-crafted features were replaced by embeddings and deep neural network architectures that were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transitionbased parsing [Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016]. The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing [Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2018]. It also leads to advances in using unlabeled data for constituent grammar [Shen et al., 2018b,a] Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches have been widely applied to alleviating this problem, as they try to improve the lower bound of the original objective, and were employed in several recent NLP works [Stratos, 2019; Chen et al., 2018; Kim"
2020.coling-main.344,P14-1043,0,0.0157984,"ures were replaced by embeddings and deep neural network architectures that were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transitionbased parsing [Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016]. The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing [Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2018]. It also leads to advances in using unlabeled data for constituent grammar [Shen et al., 2018b,a] Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches have been widely applied to alleviating this problem, as they try to improve the lower bound of the original objective, and were employed in several recent NLP works [Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a]"
2020.coling-main.344,P15-2047,0,0.0220103,"the unlabeled data to improve the performance given a limited amount of labeled data, and outperform a previously proposed semi-supervised model. 1 Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing [Reddy et al., 2016; Marcheggiani and Titov, 2017], machine translation [Bastings et al., 2017; Ding and Palmer, 2007], information extraction [Culotta and Sorensen, 2004; Liu et al., 2015] and question answering [Cui et al., 2005]. As a result, efficient parsers [Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018] have been developed using various neural architectures. root pu nsubj poss advmod xcomp dobj PRP$ NN RB VBZ VBG NN PUNC My dog also likes eating sausage . Figure 1: A dependency tree: directional arcs represent head-modifier relation between words. Despite the success of supervised approaches, they require large amounts of labeled data, particularly when neural architectures are used. Syntactic annotation is notoriously diffi"
2020.coling-main.344,P18-1130,0,0.0199,"y parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing [Reddy et al., 2016; Marcheggiani and Titov, 2017], machine translation [Bastings et al., 2017; Ding and Palmer, 2007], information extraction [Culotta and Sorensen, 2004; Liu et al., 2015] and question answering [Cui et al., 2005]. As a result, efficient parsers [Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018] have been developed using various neural architectures. root pu nsubj poss advmod xcomp dobj PRP$ NN RB VBZ VBG NN PUNC My dog also likes eating sausage . Figure 1: A dependency tree: directional arcs represent head-modifier relation between words. Despite the success of supervised approaches, they require large amounts of labeled data, particularly when neural architectures are used. Syntactic annotation is notoriously difficult and requires specialized linguistic expertise, posing a serious challenge for low-resource languages. Semi-supervised parsing aims to alleviate this problem by comb"
2020.coling-main.344,D17-1159,0,0.0290959,"ed and unlabeled data with shared parameters. We conducted experiments on WSJ and UD dependency parsing data sets, showing that our models can exploit the unlabeled data to improve the performance given a limited amount of labeled data, and outperform a previously proposed semi-supervised model. 1 Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing [Reddy et al., 2016; Marcheggiani and Titov, 2017], machine translation [Bastings et al., 2017; Ding and Palmer, 2007], information extraction [Culotta and Sorensen, 2004; Liu et al., 2015] and question answering [Cui et al., 2005]. As a result, efficient parsers [Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018] have been developed using various neural architectures. root pu nsubj poss advmod xcomp dobj PRP$ NN RB VBZ VBG NN PUNC My dog also likes eating sausage . Figure 1: A dependency tree: directional arcs represent head-modifier relation between words. Despite the success of supervised approac"
2020.coling-main.344,J93-2004,0,0.0698726,"Missing"
2020.coling-main.344,P05-1012,0,0.177607,"t approximation by sampling, which tightens the lower bound. 3869 3 Graph-based Dependency Parsing A dependency graph of a sentence can be regarded as a directed tree spanning all the words of the sentence, including a special “word”–the ROOT–to originate out. Assuming a sentence of length l, a dependency tree can be denoted as T = (&lt; h0 , m0 >, &lt; h1 , m1 >, . . . , &lt; hl−1 , ml−1 >), where ht is the index in the sequence of the head word of the dependency connecting the tth word mt as a modifier. Our graph-based parsers are constructed by following the standard structured prediction paradigm [McDonald et al., 2005; Taskar et al., 2005]. In inference, based on the parameterized scoring function SΛ with parameter Λ, the parsing problem is formulated as finding the most probable directed spanning tree for a given sentence x: T ∗ = arg max SΛ (x, T˜ ), T˜ ∈T where T ∗ is the highest scoring parse tree and T is the set of all valid trees for the sentence x. It is common to factorize the score of the entire graph into the summation of its substructures: the individual arc scores [McDonald et al., 2005]: SΛ (x, T˜ ) = X sΛ (h, m) = l−1 X sΛ (ht , mt ), t=0 (h,m)∈T˜ where T˜ represents the candidate parse tree"
2020.coling-main.344,P13-2017,0,0.0708831,"Missing"
2020.coling-main.344,D16-1031,0,0.028985,"oken-representations that capture properties of the full sentence. Typically, each token in the sentence is represented by its latent variable zt , which is a high-dimensional Gaussian variable. This configuration ensures the continuous latent variable retains the contextual information from lower-level neural models to assist finding its head or its modifier; as well as forcing the representation of similar tokens closer. We adjust the original VAE setup in our semi-supervised task by considering examples with labels, similar to recent conditional variational formulations [Sohn et al., 2015; Miao and Blunsom, 2016; Zhou 3871 and Neubig, 2017]. We propose a full probabilistic model for a given sentence x, with the unified objective to maximize for both supervised and unsupervised parsing as follows: ( 1, if T exists,  J = log Pθ (x)Pω (T |x),  = 0, otherwise. This objective can be interpreted as follows: if the training example has a golden tree T with it, then the objective is the log joint probability Pθ,ω (T , x); if the golden tree is missing, then the objective is the log marginal probability Pθ (x). The probability of a certain tree is modeled by a tree-CRF in Eq. 1 with parameters ω as Pω (T |x"
2020.coling-main.344,W04-0308,0,0.0850906,"of the latent dependency tree analytically for GAP, which is naturally extendable to other tree-structured graphical models; 3. We show improved performance of both LAP and GAP with unlabeled data on WSJ and UD data sets over using labeled data alone, and over a recent semi-supervised parser [Corro and Titov, 2018]. 2 Related Work Most dependency parsing studies fall into two major groups: graph-based and transition-based [Kubler et al., 2009]. Graph-based parsers [McDonald, 2006] regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers [Nivre, 2004, 2008] treat parsing as a sequence of actions at different stages leading to a complete dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures that were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transitionbased parsing [Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016]. The"
2020.coling-main.344,J08-4003,0,0.0781265,"Missing"
2020.coling-main.344,D14-1081,0,0.0227112,"al., 2009]. Graph-based parsers [McDonald, 2006] regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers [Nivre, 2004, 2008] treat parsing as a sequence of actions at different stages leading to a complete dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures that were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transitionbased parsing [Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016]. The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing [Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2018]. It also leads to advances in using unlabeled data for constituent grammar [Shen et al., 2018b,a] Similar to other structured prediction tasks, di"
2020.coling-main.344,P15-1031,0,0.0213781,"raph-based parsers [McDonald, 2006] regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers [Nivre, 2004, 2008] treat parsing as a sequence of actions at different stages leading to a complete dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures that were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transitionbased parsing [Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016]. The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing [Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2018]. It also leads to advances in using unlabeled data for constituent grammar [Shen et al., 2018b,a] Similar to other structured prediction tasks, directly optimizing"
2020.coling-main.35,N18-1094,0,0.205227,"19). While the quality of arguments can potentially be judged in isolation based on their merits, their effectiveness, when it comes to convincing readers, has to be considered in conjunction with the subjective perspectives and biases of these users. For example, social psychology studies have shown that ideological stances are highly correlated with different moral arguments preferences (Haidt and Graham, 2007; Graham et al., 2009; Graham et al., 2012; Johnson and Goldwasser, 2018), while Lukin et al., (2017) studied the relation between personality type and factual vs. emotional arguments. Durmus and Cardie (2018) studied linguistic properties of convincing arguments, conditioned on users’ political and religious convictions. This work aims to study the effectiveness of debate arguments, with respect to the biases of users reading them. Our goal is to mimic a realistic setting, in which only partial information about these users and their behaviors is available. To accommodate this setting, we use data from the website debate.org, used in several previous studies (Durmus and Cardie, 2018; Durmus and Cardie, 2019; Luu et al., 2019; Pacheco and Goldwasser, 2020), and formulate a new classification task,"
2020.coling-main.35,K18-1044,0,0.0433486,"Missing"
2020.coling-main.35,P19-1093,0,0.0521281,"h archives significantly better results compared to the end-to-end approach using BERT over the same inputs. 1 Introduction One of the main drivers of social interaction is convincing people to adopt new ideas and perspectives. Understanding how to make compelling arguments that would achieve this goal has been studied extensively in psychology and the social sciences (Fogg and B.J., 2003; Popkin, 1991), and more recently by the NLP community. Most of these works focus on analyzing argumentative text in isolation (Habernal and Gurevych, 2016a; Potash and Rumshisky, 2017; Persing and Ng, 2017; Gleize et al., 2019). While the quality of arguments can potentially be judged in isolation based on their merits, their effectiveness, when it comes to convincing readers, has to be considered in conjunction with the subjective perspectives and biases of these users. For example, social psychology studies have shown that ideological stances are highly correlated with different moral arguments preferences (Haidt and Graham, 2007; Graham et al., 2009; Graham et al., 2012; Johnson and Goldwasser, 2018), while Lukin et al., (2017) studied the relation between personality type and factual vs. emotional arguments. Dur"
2020.coling-main.35,D16-1129,0,0.343705,"the arguments they are exposed to. Our experiments show that our modular approach archives significantly better results compared to the end-to-end approach using BERT over the same inputs. 1 Introduction One of the main drivers of social interaction is convincing people to adopt new ideas and perspectives. Understanding how to make compelling arguments that would achieve this goal has been studied extensively in psychology and the social sciences (Fogg and B.J., 2003; Popkin, 1991), and more recently by the NLP community. Most of these works focus on analyzing argumentative text in isolation (Habernal and Gurevych, 2016a; Potash and Rumshisky, 2017; Persing and Ng, 2017; Gleize et al., 2019). While the quality of arguments can potentially be judged in isolation based on their merits, their effectiveness, when it comes to convincing readers, has to be considered in conjunction with the subjective perspectives and biases of these users. For example, social psychology studies have shown that ideological stances are highly correlated with different moral arguments preferences (Haidt and Graham, 2007; Graham et al., 2009; Graham et al., 2012; Johnson and Goldwasser, 2018), while Lukin et al., (2017) studied the r"
2020.coling-main.35,W17-5102,0,0.0170147,"their original perspective more appealing, they are more likely to be persuaded. We design several hierarchical modular networks, and compare them to a strong end-to-end baseline based on BERT (Devlin et al., 2018). Our results demonstrate the importance of characterizing the user bias using the modules leading to significant improvements. 2 Related Work Understanding the properties of persuasive arguments and identifying them in text have been widely studied in the NLP community (Tan et al., 2016; Habernal and Gurevych, 2016b; Wei et al., 2016; Persing and Ng, 2017; Stab and Gurevych, 2017; Hidey et al., 2017; Lukin et al., 2017; El Baff et al., 2018; Durmus and Cardie, 2018; Gleize et al., 2019; Yang et al., 2019; Xiao and Khazaei, 2019). Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes. We suggest a modular approach, adapting the modular architecture"
2020.coling-main.35,N18-1010,0,0.022147,"Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes. We suggest a modular approach, adapting the modular architecture suggested by (Zhang and Goldwasser, 2019) for sequence tagging to detecting user-specific persuasive text. Closest to our settings is Jo et al. (2018), which follows the CMV line of research and proposed an end to end neural network with a modular attention mechanism. It consists on two modules, one that models the malleable parts of the original poster’s arguments, and another that focuses the difference in content between both authors. 3 The Dataset and Preprocessing The ChangeMyStance task is defined over two texts with opposing perspectives on an issue, and a user indicating if their stance on the issue changed after reading the arguments in the texts. The debates, collected from www.debate.org, are between two contenders: the initiator"
2020.coling-main.35,P18-1067,1,0.886441,"Missing"
2020.coling-main.35,E17-1070,0,0.115248,"pective more appealing, they are more likely to be persuaded. We design several hierarchical modular networks, and compare them to a strong end-to-end baseline based on BERT (Devlin et al., 2018). Our results demonstrate the importance of characterizing the user bias using the modules leading to significant improvements. 2 Related Work Understanding the properties of persuasive arguments and identifying them in text have been widely studied in the NLP community (Tan et al., 2016; Habernal and Gurevych, 2016b; Wei et al., 2016; Persing and Ng, 2017; Stab and Gurevych, 2017; Hidey et al., 2017; Lukin et al., 2017; El Baff et al., 2018; Durmus and Cardie, 2018; Gleize et al., 2019; Yang et al., 2019; Xiao and Khazaei, 2019). Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes. We suggest a modular approach, adapting the modular architecture suggested by (Zhang"
2020.coling-main.35,Q19-1031,0,0.0210274,"Missing"
2020.coling-main.35,D17-1261,0,0.0321438,"Missing"
2020.coling-main.35,J17-3005,0,0.0203346,"side that conflicts with their original perspective more appealing, they are more likely to be persuaded. We design several hierarchical modular networks, and compare them to a strong end-to-end baseline based on BERT (Devlin et al., 2018). Our results demonstrate the importance of characterizing the user bias using the modules leading to significant improvements. 2 Related Work Understanding the properties of persuasive arguments and identifying them in text have been widely studied in the NLP community (Tan et al., 2016; Habernal and Gurevych, 2016b; Wei et al., 2016; Persing and Ng, 2017; Stab and Gurevych, 2017; Hidey et al., 2017; Lukin et al., 2017; El Baff et al., 2018; Durmus and Cardie, 2018; Gleize et al., 2019; Yang et al., 2019; Xiao and Khazaei, 2019). Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes. We suggest a modular approach, adapting the m"
2020.coling-main.35,P16-2032,0,0.0161893,". If the user finds the arguments of the side that conflicts with their original perspective more appealing, they are more likely to be persuaded. We design several hierarchical modular networks, and compare them to a strong end-to-end baseline based on BERT (Devlin et al., 2018). Our results demonstrate the importance of characterizing the user bias using the modules leading to significant improvements. 2 Related Work Understanding the properties of persuasive arguments and identifying them in text have been widely studied in the NLP community (Tan et al., 2016; Habernal and Gurevych, 2016b; Wei et al., 2016; Persing and Ng, 2017; Stab and Gurevych, 2017; Hidey et al., 2017; Lukin et al., 2017; El Baff et al., 2018; Durmus and Cardie, 2018; Gleize et al., 2019; Yang et al., 2019; Xiao and Khazaei, 2019). Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes"
2020.coling-main.35,N19-1364,0,0.0112286,"ical modular networks, and compare them to a strong end-to-end baseline based on BERT (Devlin et al., 2018). Our results demonstrate the importance of characterizing the user bias using the modules leading to significant improvements. 2 Related Work Understanding the properties of persuasive arguments and identifying them in text have been widely studied in the NLP community (Tan et al., 2016; Habernal and Gurevych, 2016b; Wei et al., 2016; Persing and Ng, 2017; Stab and Gurevych, 2017; Hidey et al., 2017; Lukin et al., 2017; El Baff et al., 2018; Durmus and Cardie, 2018; Gleize et al., 2019; Yang et al., 2019; Xiao and Khazaei, 2019). Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes. We suggest a modular approach, adapting the modular architecture suggested by (Zhang and Goldwasser, 2019) for sequence tagging to detecting user-specific persuasive text."
2020.coling-main.35,P19-1055,1,0.827109,", 2017; El Baff et al., 2018; Durmus and Cardie, 2018; Gleize et al., 2019; Yang et al., 2019; Xiao and Khazaei, 2019). Most relevant to our work is the ChangeMyView (CMV) task, introduced by Tan et al. (2016), predicting stance changes in Reddit forums. Similar to our work, Durmus and Cardie (2018) highlight the importance of modeling the user’s belief when analyzing persuasive language. In their work, Xiao et al. (2019) approach the CMV problem using features that characterize the users’ psychological attributes. We suggest a modular approach, adapting the modular architecture suggested by (Zhang and Goldwasser, 2019) for sequence tagging to detecting user-specific persuasive text. Closest to our settings is Jo et al. (2018), which follows the CMV line of research and proposed an end to end neural network with a modular attention mechanism. It consists on two modules, one that models the malleable parts of the original poster’s arguments, and another that focuses the difference in content between both authors. 3 The Dataset and Preprocessing The ChangeMyStance task is defined over two texts with opposing perspectives on an issue, and a user indicating if their stance on the issue changed after reading the"
2020.emnlp-main.620,P15-2072,0,0.500007,"ce to train an embedding model, which represents in the same space the subframes labels, the lexicon containing subframes indicator expressions and paragraphs extracted from news articles containing these expressions. The embedding model captures the context in which subframe appear, and as a result can generalize and capture subframe usage in new texts. Our approach can be viewed as a middle ground between event-specific frames, emerging from the data and capturing properties unique to the given topic (Tsur et al., 2015; Demszky et al., 2019), and general issue frames (Boydstun et al., 2014; Card et al., 2015b; Johnson et al., 2017a; Field et al., 2018; Hartmann et al., 2019) that use the same set of framing dimensions for all topics. On the one hand, it can capture nuanced, topic-specific subframes, while on the other, it maps these subframes into general framing dimensions. In the above example, it allows us to identify that the economy frame is important for both the liberal and conservative perspectives on immigration, despite the fact that it is instantiated using a different subframe. We evaluate the quality of the learned model in several ways, by applying it politically-motivated news arti"
2020.emnlp-main.620,D19-1664,0,0.0586646,"d sub-frames can effectively separate between ideological standpoints expressed in the articles. Second, we evaluate the quality of the learned model, showing that subframe labels assigned to new paragraphs correlate well with human judgements. Finally, we use the model to analyze the different perspectives in left and right leaning news coverage, and their change over time. 2 Related Work Understanding and analyzing political perspectives in news coverage has gathered significant interest in recent years (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014; Li and Goldwasser, 2019; Fan et al., 2019; Jiang et al., 2019; Hanawa et al., 2019), broadly related to analysis of bias or partisanship and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018; Gentzkow et al., 2016; Monroe et al., 2008; An et al., 2019; Menini et al., 2017). In addition to predicting the underlying perspective, our work focuses on explaining the perspectives underlying the ideological coverage of news events. We build specifically on issue-frames (Boydstun et al., 2014), however our work is related to framing and agenda setting analysis work more broadly (Tsur et al., 20"
2020.emnlp-main.620,D18-1393,0,0.543878,"sents in the same space the subframes labels, the lexicon containing subframes indicator expressions and paragraphs extracted from news articles containing these expressions. The embedding model captures the context in which subframe appear, and as a result can generalize and capture subframe usage in new texts. Our approach can be viewed as a middle ground between event-specific frames, emerging from the data and capturing properties unique to the given topic (Tsur et al., 2015; Demszky et al., 2019), and general issue frames (Boydstun et al., 2014; Card et al., 2015b; Johnson et al., 2017a; Field et al., 2018; Hartmann et al., 2019) that use the same set of framing dimensions for all topics. On the one hand, it can capture nuanced, topic-specific subframes, while on the other, it maps these subframes into general framing dimensions. In the above example, it allows us to identify that the economy frame is important for both the liberal and conservative perspectives on immigration, despite the fact that it is instantiated using a different subframe. We evaluate the quality of the learned model in several ways, by applying it politically-motivated news article coverage of divisive topics. First, we s"
2020.emnlp-main.620,L16-1591,0,0.0372448,"Missing"
2020.emnlp-main.620,N19-1304,0,0.205999,"eating expressions with these subframes. Finally, we exploit this resource to train an embedding model, which represents in the same space the subframes labels, the lexicon containing subframes indicator expressions and paragraphs extracted from news articles containing these expressions. The embedding model captures the context in which subframe appear, and as a result can generalize and capture subframe usage in new texts. Our approach can be viewed as a middle ground between event-specific frames, emerging from the data and capturing properties unique to the given topic (Tsur et al., 2015; Demszky et al., 2019), and general issue frames (Boydstun et al., 2014; Card et al., 2015b; Johnson et al., 2017a; Field et al., 2018; Hartmann et al., 2019) that use the same set of framing dimensions for all topics. On the one hand, it can capture nuanced, topic-specific subframes, while on the other, it maps these subframes into general framing dimensions. In the above example, it allows us to identify that the economy frame is important for both the liberal and conservative perspectives on immigration, despite the fact that it is instantiated using a different subframe. We evaluate the quality of the learned m"
2020.emnlp-main.620,P14-1105,0,0.125577,"that the lexicon we developed and the induced sub-frames can effectively separate between ideological standpoints expressed in the articles. Second, we evaluate the quality of the learned model, showing that subframe labels assigned to new paragraphs correlate well with human judgements. Finally, we use the model to analyze the different perspectives in left and right leaning news coverage, and their change over time. 2 Related Work Understanding and analyzing political perspectives in news coverage has gathered significant interest in recent years (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014; Li and Goldwasser, 2019; Fan et al., 2019; Jiang et al., 2019; Hanawa et al., 2019), broadly related to analysis of bias or partisanship and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018; Gentzkow et al., 2016; Monroe et al., 2008; An et al., 2019; Menini et al., 2017). In addition to predicting the underlying perspective, our work focuses on explaining the perspectives underlying the ideological coverage of news events. We build specifically on issue-frames (Boydstun et al., 2014), however our work is related to framing and agenda setting"
2020.emnlp-main.620,E12-1021,0,0.079145,"Missing"
2020.findings-emnlp.446,P13-2013,0,0.0249148,"2017) were proposed for mitigating the impact of data noise. GCN were also used for NLP applications, to represent structure (Marcheggiani and Titov, 2017) and social information (Li and Goldwasser, 2019). In this paper, we adopt R-GCN for NG, as modeling different types of relationships is crucial for event commonsense inference, as attested by Lee and Goldwasser (2019). Discourse relations are crucial aspects for completing language understanding. Early works focused on identifying explicit and implicit discourse relations under supervised settings (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Xue et al., 2016), while recent works mined discourse connectives to refine sentence representations unsupervisedly (Malmi et al., 2017; Nie et al., 2019; Sileo et al., 2019). Our work learns discourse relations between events by leveraging the fact that some explicit connectives and their categories are relatively easy to identify. We build a simplified discourse annotator that can be used to extract discourse relations between events without suffering from high noise. 3 3.1 Model Overview We propose a learning framework for constructing event embeddings, contextualized by a relational even"
2020.findings-emnlp.446,P08-1090,0,0.468938,"ove language understanding is one of the longest standing AI goals. Structured knowledge representations such as scripts (Schank and Abelson, 1977) capture temporal relations between events to describe human-level representations of common scenarios. For example, the Restaurant Script captures the fact that food is first ordered and only then paid for. Initial works relied on manual script construction, a labor-intensive task that is hard to scale to the number of possible scenarios. More recent works focus on extracting this knowledge directly from text, using symbolic event representations (Chambers and Jurafsky, 2008) or more recently, exploiting representation learning advances and representing events using dense vectors, learned from data (Pichotta and Mooney, 2016a; Granroth-Wilding and Clark, 2016; Wang et al., 2017; Lee and Goldwasser, 2018; Li et al., 2018). While these works differ in the way the internal structure of the event is represented, broadly speaking, the resulting models resemble word-embedding approaches (Mikolov Example 1: Jenny gets coffee Jenny woke up very early and had some time to kill. She went outside and noticed that it was raining, so she went inside her favorite coffee-shop. S"
2020.findings-emnlp.446,D14-1165,0,0.118647,"d-Level Contextualization Pre-trained BERT Input Tokens p1 p2 p3 p4 Event Predicates Figure 2: Neural architecture for the Narrative Graph model. Objective There are two common objectives researchers have been using for optimizing graphical networks: node classifications and link predictions (Schlichtkrull et al., 2018). We select the latter one, as our goal is to capture structural transitions between events. However, it is possible to train for both objectives jointly within our framework, and we leave it for future work. We score a target link (triplet) with a modified version of DistMult (Chang et al., 2014), an effective scoring function designed for knowledge base completion. The function is defined as follows: f (h, r, t) = eTh Wr et , (3) where eh and et are the representations for head and tail events of the triplet, and Wr ∈ Rd×d are relation-specific parameters. The original DistMult restricts Wr to a diagonal matrix to account for the huge amount of relation types existing in knowledge bases. We relax this as we need to address more fine-grained differences between relations, such as directionality3 . The final loss function is the Cross-Entropy Loss with weighted classes: X 1 L =− y log("
2020.findings-emnlp.446,Q16-1038,1,0.848437,"ervised Modeling of Contextualized Event Embedding for Discourse Relations I-Ta Lee, Maria Leonor Pacheco, Dan Goldwasser Department of Computer Science Purdue University West Lafayette, IN, USA {lee2226, pachecog, dgoldwas}@purdue.edu Abstract et al., 2013), representing event co-occurrence in a low-dimensional vector space, and as a result use vector similarity over their embedding to measure their relationship. In this paper, we follow the observation that many natural language understanding tasks require a more expressive representation that can capture the context in which events appear (Goldwasser and Zhang, 2016) and consider multiple relations between events (Lee and Goldwasser, 2019), and going beyond simple event similarity to represent relations. To help explain the intuition behind it, consider the following example, consisting of a short story and a Multiple-Choice Narrative Cloze (MCNC) question (Granroth-Wilding and Clark, 2016), the standard evaluation for such models. Representing, and reasoning over, long narratives requires models that can deal with complex event structures connected through multiple relationship types. This paper suggests to represent this type of information as a narrati"
2020.findings-emnlp.446,D19-1041,0,0.0679019,"s of a specific entity to model sequences of events. PredicateGR (Granroth-Wilding and Clark, 2016) was a widely adopted event definition here, consisting of a pair of dependency type, such as subject or object, and predicate token, such as verbs. Recent works (Pichotta and Mooney, 2014; Lee and Goldwasser, 2018, 2019) moved to the predicate-centric events (also called multi-argument events). Each event was anchored at a predicate and considered related entity mentions and modifiers as context to the event. Other works focusing on event extraction (Walker et al., 2006) or relation extraction (Han et al., 2019) also adopted this definition, as it tends to capture more comprehensive view of events’ semantics, aggregating information from multiple entities. In this paper, we also choose this definition, since our goal is to utilize events’ context to model event relationships. Graph neural models are often applied in Knowledge Base Completion. Early works used randomwalk-based methods, aiming to build scalable neural models, such as DeepWalk (Perozzi et al., 2014), node2vec (Grover and Leskovec, 2016), LINE (Tang et al., 2015) and GraphSAGE (Hamilton et al., 2017). Graph Convolution Networks (GCN) int"
2020.findings-emnlp.446,P19-1413,1,0.873247,"-Ta Lee, Maria Leonor Pacheco, Dan Goldwasser Department of Computer Science Purdue University West Lafayette, IN, USA {lee2226, pachecog, dgoldwas}@purdue.edu Abstract et al., 2013), representing event co-occurrence in a low-dimensional vector space, and as a result use vector similarity over their embedding to measure their relationship. In this paper, we follow the observation that many natural language understanding tasks require a more expressive representation that can capture the context in which events appear (Goldwasser and Zhang, 2016) and consider multiple relations between events (Lee and Goldwasser, 2019), and going beyond simple event similarity to represent relations. To help explain the intuition behind it, consider the following example, consisting of a short story and a Multiple-Choice Narrative Cloze (MCNC) question (Granroth-Wilding and Clark, 2016), the standard evaluation for such models. Representing, and reasoning over, long narratives requires models that can deal with complex event structures connected through multiple relationship types. This paper suggests to represent this type of information as a narrative graph and learn contextualized event representations over it using a re"
2020.findings-emnlp.446,P19-1247,1,0.817271,"gregating features from neighboring nodes. Several GCN variants followed, Relational Graph Convolution Networks (R-GCN) (Schlichtkrull et al., 2018) added relation type information to address the multi-relational knowledge bases. Graph Attention Networks (GAT) (Veliˇckovi´c et al., 2017) manipulated attention layers for aggregating neighboring messages. Gated mechanisms (Marcheggiani and Titov, 2017; Dauphin et al., 2017) were proposed for mitigating the impact of data noise. GCN were also used for NLP applications, to represent structure (Marcheggiani and Titov, 2017) and social information (Li and Goldwasser, 2019). In this paper, we adopt R-GCN for NG, as modeling different types of relationships is crucial for event commonsense inference, as attested by Lee and Goldwasser (2019). Discourse relations are crucial aspects for completing language understanding. Early works focused on identifying explicit and implicit discourse relations under supervised settings (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Xue et al., 2016), while recent works mined discourse connectives to refine sentence representations unsupervisedly (Malmi et al., 2017; Nie et al., 2019; Sileo et al., 2019). Our"
2020.findings-emnlp.446,P14-5010,0,0.00499275,"stics of the relations we extracted from the corpus English Gigaword (Parker et al., 2011). We explain each relation type as follows: (1) The CNext relation stands for Coreferent Next relation, inspired by (Chambers and Jurafsky, 2008), capturing narrative relationships between events with shared entities on coreference chains1 . Based on the procedure proposed by (Lee and Goldwasser, 2019), we first identify all possible events and connect pairs of the events with a CNext relation if they have entity mentions appearing in the same coreference chain. For example, “Jim shot 1 Stanford CoreNLP (Manning et al., 2014) pipeline is used for extracting dependency trees and coreference resolutions. John. John died.” shot and died have the CNext relation (shot, CNext, died) because the entity John is the participant to both events in a sequential order. (2) The Next relation is defined between events appearing in the neighboring sentences. It aims to capture the event relationship where two events are relevant but do not have shared participants. For example, “The weather turned bad. The rain started falling.” has the relation (turned, Next, falling). These two events have no shared participant but are clearly"
2020.findings-emnlp.446,D17-1159,0,0.0732784,"Missing"
2020.findings-emnlp.446,P19-1442,0,0.0222872,"cial information (Li and Goldwasser, 2019). In this paper, we adopt R-GCN for NG, as modeling different types of relationships is crucial for event commonsense inference, as attested by Lee and Goldwasser (2019). Discourse relations are crucial aspects for completing language understanding. Early works focused on identifying explicit and implicit discourse relations under supervised settings (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Xue et al., 2016), while recent works mined discourse connectives to refine sentence representations unsupervisedly (Malmi et al., 2017; Nie et al., 2019; Sileo et al., 2019). Our work learns discourse relations between events by leveraging the fact that some explicit connectives and their categories are relatively easy to identify. We build a simplified discourse annotator that can be used to extract discourse relations between events without suffering from high noise. 3 3.1 Model Overview We propose a learning framework for constructing event embeddings, contextualized by a relational event graph. The proposed approach can be used for many discourse and narrative analysis tasks, that go beyond the sentence level. The framework consists of tw"
2020.findings-emnlp.446,K16-2019,1,0.686985,"Missing"
2020.findings-emnlp.446,W12-1614,0,0.0221579,"2017; Dauphin et al., 2017) were proposed for mitigating the impact of data noise. GCN were also used for NLP applications, to represent structure (Marcheggiani and Titov, 2017) and social information (Li and Goldwasser, 2019). In this paper, we adopt R-GCN for NG, as modeling different types of relationships is crucial for event commonsense inference, as attested by Lee and Goldwasser (2019). Discourse relations are crucial aspects for completing language understanding. Early works focused on identifying explicit and implicit discourse relations under supervised settings (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Xue et al., 2016), while recent works mined discourse connectives to refine sentence representations unsupervisedly (Malmi et al., 2017; Nie et al., 2019; Sileo et al., 2019). Our work learns discourse relations between events by leveraging the fact that some explicit connectives and their categories are relatively easy to identify. We build a simplified discourse annotator that can be used to extract discourse relations between events without suffering from high noise. 3 3.1 Model Overview We propose a learning framework for constructing event embeddings, contextual"
2020.findings-emnlp.446,N18-1202,0,0.0581215,"associated with it (i.e., ordering coffee). To meet this challenge we suggest a multirelational contextualized representation of events, 4962 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4962–4972 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Narrative Graph raining greet CNext Next Reason Next CNext which one? CNext CNext go inside coﬀee shop order a latte ask for a job wake up have time Figure 1: Narrative Graph extracted for Example 1. Some edges are omitted for clarity. generalizing ideas from contextualized word representations (Peters et al., 2018; Devlin et al., 2018) to multi-relational narrative representation. Similar to contextualized word representations, we suggest learning an event representations which captures the narrative it is a part of. For example, the event “she went inside the coffee-shop” would be represented differently given different context, such as different weather conditions (“it was sunny and warm”), different time of the day (“it was almost noon when she woke up”) or if the protagonist needed employment. In each one of these cases, the relationship between the contextualized event representation and the answe"
2020.findings-emnlp.446,E14-1024,0,0.0660973,"a pre-trained language model–BERT, coupled with multi-relational graph structures, to learn event representations. In the literature, the definitions of events can be categorized in two ways: entity-centric or predicate-centric. Early works (Chambers and Jurafsky, 2008) operated on the entity-centric events, following coreference chains of a specific entity to model sequences of events. PredicateGR (Granroth-Wilding and Clark, 2016) was a widely adopted event definition here, consisting of a pair of dependency type, such as subject or object, and predicate token, such as verbs. Recent works (Pichotta and Mooney, 2014; Lee and Goldwasser, 2018, 2019) moved to the predicate-centric events (also called multi-argument events). Each event was anchored at a predicate and considered related entity mentions and modifiers as context to the event. Other works focusing on event extraction (Walker et al., 2006) or relation extraction (Han et al., 2019) also adopted this definition, as it tends to capture more comprehensive view of events’ semantics, aggregating information from multiple entities. In this paper, we also choose this definition, since our goal is to utilize events’ context to model event relationships."
2020.findings-emnlp.446,P16-1027,0,0.265727,"temporal relations between events to describe human-level representations of common scenarios. For example, the Restaurant Script captures the fact that food is first ordered and only then paid for. Initial works relied on manual script construction, a labor-intensive task that is hard to scale to the number of possible scenarios. More recent works focus on extracting this knowledge directly from text, using symbolic event representations (Chambers and Jurafsky, 2008) or more recently, exploiting representation learning advances and representing events using dense vectors, learned from data (Pichotta and Mooney, 2016a; Granroth-Wilding and Clark, 2016; Wang et al., 2017; Lee and Goldwasser, 2018; Li et al., 2018). While these works differ in the way the internal structure of the event is represented, broadly speaking, the resulting models resemble word-embedding approaches (Mikolov Example 1: Jenny gets coffee Jenny woke up very early and had some time to kill. She went outside and noticed that it was raining, so she went inside her favorite coffee-shop. She greeted the waiter ... What happened next? (a) she bought a new car. (b) she ordered a steamy latte. (c) she ordered a large breakfast (d) she asked"
2020.findings-emnlp.446,K16-2007,0,0.0276032,"Missing"
2020.findings-emnlp.446,K16-2004,0,0.0313904,"Missing"
2020.findings-emnlp.446,D17-1006,0,0.400495,"resentations of common scenarios. For example, the Restaurant Script captures the fact that food is first ordered and only then paid for. Initial works relied on manual script construction, a labor-intensive task that is hard to scale to the number of possible scenarios. More recent works focus on extracting this knowledge directly from text, using symbolic event representations (Chambers and Jurafsky, 2008) or more recently, exploiting representation learning advances and representing events using dense vectors, learned from data (Pichotta and Mooney, 2016a; Granroth-Wilding and Clark, 2016; Wang et al., 2017; Lee and Goldwasser, 2018; Li et al., 2018). While these works differ in the way the internal structure of the event is represented, broadly speaking, the resulting models resemble word-embedding approaches (Mikolov Example 1: Jenny gets coffee Jenny woke up very early and had some time to kill. She went outside and noticed that it was raining, so she went inside her favorite coffee-shop. She greeted the waiter ... What happened next? (a) she bought a new car. (b) she ordered a steamy latte. (c) she ordered a large breakfast (d) she asked about open positions. Events typically correspond to p"
2020.findings-emnlp.446,K16-2001,0,0.114336,"extualized representation of the individual event. We define an unsupervised learning process, learning to recover removed edges from a given narrative graph and capture incorrect associations between event nodes and edges. This process allows the model to learn the association between the missing information and the observed context in the narrative graph. We use the New York Times section of English Gigaword (Parker et al., 2011) for training the model. We evaluate the model on MCNC and its relational variants, as well as the popular, and challenging, implicit discourse classification task (Xue et al., 2016). 2 Related Work Statistical script learning is an unsupervised learning problem addressing the probabilities of event co-occurrence. Chambers and Jurafsky (2008) started the early work, using Pairwise Mutual Information (PMI) -based models to calculate the conditional probability distribution. In recent years, neural-based learning frameworks emerged, leading to a wave of model evolution. Granroth-Wilding and Clark (2016) combine SkipGram (Mikolov et al., 2013), a word embedding model, with neural networks for learning event representations. Pichotta and Mooney (2016b) built a Long Short Term"
2020.findings-emnlp.446,C10-2172,0,0.0446545,"heggiani and Titov, 2017; Dauphin et al., 2017) were proposed for mitigating the impact of data noise. GCN were also used for NLP applications, to represent structure (Marcheggiani and Titov, 2017) and social information (Li and Goldwasser, 2019). In this paper, we adopt R-GCN for NG, as modeling different types of relationships is crucial for event commonsense inference, as attested by Lee and Goldwasser (2019). Discourse relations are crucial aspects for completing language understanding. Early works focused on identifying explicit and implicit discourse relations under supervised settings (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Xue et al., 2016), while recent works mined discourse connectives to refine sentence representations unsupervisedly (Malmi et al., 2017; Nie et al., 2019; Sileo et al., 2019). Our work learns discourse relations between events by leveraging the fact that some explicit connectives and their categories are relatively easy to identify. We build a simplified discourse annotator that can be used to extract discourse relations between events without suffering from high noise. 3 3.1 Model Overview We propose a learning framework for constructing event"
2020.findings-emnlp.446,N19-1351,0,0.022745,"Li and Goldwasser, 2019). In this paper, we adopt R-GCN for NG, as modeling different types of relationships is crucial for event commonsense inference, as attested by Lee and Goldwasser (2019). Discourse relations are crucial aspects for completing language understanding. Early works focused on identifying explicit and implicit discourse relations under supervised settings (Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Xue et al., 2016), while recent works mined discourse connectives to refine sentence representations unsupervisedly (Malmi et al., 2017; Nie et al., 2019; Sileo et al., 2019). Our work learns discourse relations between events by leveraging the fact that some explicit connectives and their categories are relatively easy to identify. We build a simplified discourse annotator that can be used to extract discourse relations between events without suffering from high noise. 3 3.1 Model Overview We propose a learning framework for constructing event embeddings, contextualized by a relational event graph. The proposed approach can be used for many discourse and narrative analysis tasks, that go beyond the sentence level. The framework consists of two levels of hierarchi"
2020.iwpt-1.5,P16-1231,0,0.0195831,"rediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires margina"
2020.iwpt-1.5,K17-3002,0,0.173779,"Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing (Reddy et al., 2016; Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017; Ding and Palmer, 2007), information extraction (Culotta and Sorensen, 2004; Liu et al., 2015) and question answering (Cui et al., 2005). Recently, efficient parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018) have been developed using various neural architectures. While supervised approaches have been very successful, they require large amounts of labeled data, particularly when neural architectures are used, which usually are over-parameterized. Syntactic annotation is notoriously difficult and requires specialized linguistic expertise, posing a serious challenge for low-resource languages. Semi-supervised 2 Related Work Most dependency parsing studies fall into two major groups: graph-based and transition-based 40 Proceedings of the 16th International Conference on Parsing Tech"
2020.iwpt-1.5,D17-1209,0,0.0249167,"ed and unlabeled data. 3. We show improved performance of the proposed model with unlabeled data on the WSJ data sets, and the performance is on a par with a recently proposed semi-supervised parser (Corro and Titov, 2019), with faster inference. Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing (Reddy et al., 2016; Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017; Ding and Palmer, 2007), information extraction (Culotta and Sorensen, 2004; Liu et al., 2015) and question answering (Cui et al., 2005). Recently, efficient parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018) have been developed using various neural architectures. While supervised approaches have been very successful, they require large amounts of labeled data, particularly when neural architectures are used, which usually are over-parameterized. Syntactic annotation is notoriously difficult and requires specialized linguistic expertise, pos"
2020.iwpt-1.5,P15-1033,0,0.0133511,"2006) regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underly"
2020.iwpt-1.5,D16-1073,0,0.0180715,"ing, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen e"
2020.iwpt-1.5,K16-1002,0,0.0787435,"Missing"
2020.iwpt-1.5,P19-1228,0,0.0160121,"2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a). Variational Autoencoder (VAE) (Kingma and Welling, 2014) is particularly useful for latent representation learning, and has been studied in semi-supervised context as the Conditional VAE (CVAE) (Sohn et al., 2015). Note our work differs from VAE as VAE is designed for tabular data but not for structured prediction, as the input towards VAP is the sequence of sentential tokens and the output is the dependency tree. 3 Our graph-based VAP parser is constructed based on the following standard structured prediction paradigm (McDonald et al., 2005; Taskar et al., 2005). In inference, based on"
2020.iwpt-1.5,D17-1171,0,0.103396,"ral network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a). Variational Autoencoder (VAE) (Kingma and Welling,"
2020.iwpt-1.5,N19-1114,0,0.0200656,"2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a). Variational Autoencoder (VAE) (Kingma and Welling, 2014) is particularly useful for latent representation learning, and has been studied in semi-supervised context as the Conditional VAE (CVAE) (Sohn et al., 2015). Note our work differs from VAE as VAE is designed for tabular data but not for structured prediction, as the input towards VAP is the sequence of sentential tokens and the output is the dependency tree. 3 Our graph-based VAP parser is constructed based on the following standard structured prediction paradigm (McDonald et al., 2005; Taskar et al., 2005). In inference, based on"
2020.iwpt-1.5,D14-1082,0,0.410843,"based parsers (McDonald, 2006) regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficu"
2020.iwpt-1.5,D18-1020,0,0.0250555,", 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a). Variational Autoencoder (VAE) (Kingma and Welling, 2014) is particularly useful for latent representation learning, and has been studied in semi-supervised context as the Conditional VAE (CVAE) (Sohn et al., 2015). Note our work differs from VAE as VAE is designed for tabular data but not for structured prediction, as the input towards VAP is the sequence of sentential tokens and the output is the dependency tree. 3 Our graph-based VAP parser is constructed based on the following standard structured prediction paradigm (McDonald et al., 2005; Taskar et al., 2005). In in"
2020.iwpt-1.5,P15-1031,0,0.0189384,"al., 2009). Graph-based parsers (McDonald, 2006) regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing"
2020.iwpt-1.5,P14-1043,0,0.0238033,"features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a)."
2020.iwpt-1.5,P15-2047,0,0.0289784,"the WSJ data sets, and the performance is on a par with a recently proposed semi-supervised parser (Corro and Titov, 2019), with faster inference. Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing (Reddy et al., 2016; Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017; Ding and Palmer, 2007), information extraction (Culotta and Sorensen, 2004; Liu et al., 2015) and question answering (Cui et al., 2005). Recently, efficient parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018) have been developed using various neural architectures. While supervised approaches have been very successful, they require large amounts of labeled data, particularly when neural architectures are used, which usually are over-parameterized. Syntactic annotation is notoriously difficult and requires specialized linguistic expertise, posing a serious challenge for low-resource languages. Semi-supervised 2 Related Work Most depende"
2020.iwpt-1.5,P18-1130,0,0.016272,"ency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing (Reddy et al., 2016; Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017; Ding and Palmer, 2007), information extraction (Culotta and Sorensen, 2004; Liu et al., 2015) and question answering (Cui et al., 2005). Recently, efficient parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018) have been developed using various neural architectures. While supervised approaches have been very successful, they require large amounts of labeled data, particularly when neural architectures are used, which usually are over-parameterized. Syntactic annotation is notoriously difficult and requires specialized linguistic expertise, posing a serious challenge for low-resource languages. Semi-supervised 2 Related Work Most dependency parsing studies fall into two major groups: graph-based and transition-based 40 Proceedings of the 16th International Conference on Parsing Technologies and the I"
2020.iwpt-1.5,P10-2038,0,0.027526,"upervised parser with faster inference. 1 poss advmod xcomp dobj PRP$ NN RB VBZ VBG NN PUNC My dog also likes eating sausage . Figure 1: A dependency tree: directional arcs represent head-modifier relation between words. parsing aims to alleviate this problem by combining a small amount of labeled data and a large amount of unlabeled data, to improve parsing performance on using labeled data alone. Traditional semi-supervised parsers use unlabeled data to generate additional features in order to assist the learning process (Koo et al., 2008), together with different variants of self-training (Søgaard, 2010). However, these approaches are usually pipe-lined and error-propagation may occur. In this paper, we propose Variational Autoencoding Parser, or VAP, extends the idea of VAE, illustrated in Figure 3. The VAP model uses unlabeled examples to learn continuous latent variables of the sentence, which can be used to support tree inference by providing an enriched representation. We summarize our contributions as follows: 1. We proposed a Variational Autoencoding Parser (VAP) for semi-supervised dependency parsing; 2. We designed a unified loss function for the proposed parser to deal with both lab"
2020.iwpt-1.5,D17-1159,0,0.0261147,"tion for the proposed parser to deal with both labeled and unlabeled data. 3. We show improved performance of the proposed model with unlabeled data on the WSJ data sets, and the performance is on a par with a recently proposed semi-supervised parser (Corro and Titov, 2019), with faster inference. Introduction Dependency parsing captures bi-lexical relationships by constructing directional arcs between words, defining a head-modifier syntactic structure for sentences, as shown in Figure 1. Dependency trees are fundamental for many downstream tasks such as semantic parsing (Reddy et al., 2016; Marcheggiani and Titov, 2017), machine translation (Bastings et al., 2017; Ding and Palmer, 2007), information extraction (Culotta and Sorensen, 2004; Liu et al., 2015) and question answering (Cui et al., 2005). Recently, efficient parsers (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018) have been developed using various neural architectures. While supervised approaches have been very successful, they require large amounts of labeled data, particularly when neural architectures are used, which usually are over-parameterized. Syntactic annotation is notoriously difficult and re"
2020.iwpt-1.5,J93-2004,0,0.0711217,"Missing"
2020.iwpt-1.5,N19-1113,0,0.0274292,"2; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a). Variational Autoencoder (VAE) (Kingma and Welling, 2014) is particularly useful for latent representation learning, and has been studied in semi-supervised context as the Conditional VAE (CVAE) (Sohn et al., 2015). Note our work differs from VAE as VAE is designed for tabular data but not for structured prediction, as the input towards VAP is the sequence of sentential tokens and the output is the dependency tree. 3 Our graph-based VAP parser is constructed based on the following standard structured prediction paradigm (McDonald et al., 2005; Taskar e"
2020.iwpt-1.5,P05-1012,0,0.235821,"recent NLP works (Stratos, 2019; Chen et al., 2018; Kim et al., 2019b,a). Variational Autoencoder (VAE) (Kingma and Welling, 2014) is particularly useful for latent representation learning, and has been studied in semi-supervised context as the Conditional VAE (CVAE) (Sohn et al., 2015). Note our work differs from VAE as VAE is designed for tabular data but not for structured prediction, as the input towards VAP is the sequence of sentential tokens and the output is the dependency tree. 3 Our graph-based VAP parser is constructed based on the following standard structured prediction paradigm (McDonald et al., 2005; Taskar et al., 2005). In inference, based on the scoring function SΛ with parameter Λ, the parsing problem is formulated as finding the most probable directed spanning tree for a given sentence x: T ∗ = arg max SΛ (x, T˜ ), T˜ ∈T where T ∗ is the highest scoring parse tree and T is the set of all valid trees for the sentence x. It is common to factorize the score of the entire graph into the summation of its substructures–the individual arc scores (McDonald et al., 2005): SΛ (x, T˜ ) = X (h,m)∈T˜ sΛ (h, m) = l X sΛ (ht , mt ), t=1 where T˜ represents the candidate parse tree, and sΛ is a fun"
2020.iwpt-1.5,D16-1031,0,0.0289961,"to an arc score. 41 Root Root X1 0 … Xt … XL-1 latent variable retains the contextual information from lower-level neural models to assist finding its head or its modifier; as well as forcing the representation of similar tokens to be closer. The latent variable group z is modeled via P (z|x). In addition, we model the process of reconstructing the input sentence from the latent variable through a generative story P (x|z). We adjust the original VAE setup in our semisupervised task by considering examples with labels, similar to recent conditional variational formulations (Sohn et al., 2015; Miao and Blunsom, 2016; Zhou and Neubig, 2017). We propose a full probabilistic model for a given sentence x, with the unified objective to maximize for both supervised and unsupervised parsing as follows: ( 1, if T exists,  J = log Pθ (x)Pω (T |x),  = 0, otherwise. The score of the (t, t-1) right arc X1 0 … 0 0 S(t, t-1) S(t-1, t) 0 … Xt 0 The score of the (t-1, t) left arc XL-1 0 Figure 2: In this illustration of the arc scoring matrix, each entry represents the (h(head) → m(modif ier)) score. ˆ X x ˆt x 1 x ˆt x ˆt+1 Y yt z yt yt+1 x x Decoder 1 This objective can be interpreted as follows: if the training exa"
2020.iwpt-1.5,W04-0308,0,0.141068,"and requires specialized linguistic expertise, posing a serious challenge for low-resource languages. Semi-supervised 2 Related Work Most dependency parsing studies fall into two major groups: graph-based and transition-based 40 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 40–47 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics (Kubler et al., 2009). Graph-based parsers (McDonald, 2006) regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation d"
2020.iwpt-1.5,D12-1121,0,0.0319327,"anual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches are a natural way to alleviate this difficulty, as they try to improve the lower bound of the original objective, and have been applied in several recent NLP works (S"
2020.iwpt-1.5,J08-4003,0,0.179099,"Missing"
2020.iwpt-1.5,P15-1032,0,0.0188019,"ng as a structured prediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic mo"
2020.iwpt-1.5,D14-1081,0,0.027685,"s (Kubler et al., 2009). Graph-based parsers (McDonald, 2006) regard parsing as a structured prediction problem to find the most probable tree, while transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, di"
2020.iwpt-1.5,D16-1137,0,0.0244278,"ile transition-based parsers (Nivre, 2004, 2008) treat parsing as a sequence of actions at different stages leading to a dependency tree. While earlier works relied on manual feature engineering, in recent years the hand-crafted features were replaced by embeddings and deep neural network architectures were used to learn representation for scoring structural decisions, leading to improved performance in both graph-based and transition-based parsing (Nivre, 2014; Pei et al., 2015; Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Wiseman and Rush, 2016). The annotation difficulty for this task, has also motivated work on unsupervised (grammar induction) and semi-supervised approaches to parsing (Tu and Honavar, 2012; Jiang et al., 2016; Koo et al., 2008; Li et al., 2014; Kiperwasser and Goldberg, 2015; Cai et al., 2017; Corro and Titov, 2019). It also leads to advances in using unlabeled data for constituent grammar (Shen et al., 2018b,a) Similar to other structured prediction tasks, directly optimizing the objective is difficult when the underlying probabilistic model requires marginalizing over the dependency trees. Variational approaches"
2020.iwpt-1.5,D17-1173,0,0.0181308,"rameters ω as Pω (T |x). Given the assumed generative process Pθ (x|z), directly optimizing this objective is intractable, thus instead we optimize its Evidence Lower BOund (ELBO): Encoder X (a) VAE (b) VAP Figure 3: Illustration of variational autoencoder (VAE)(left) and variational autoencoding parser (VAP)(right). For a single sentence, we can form a scoring matrix as shown in Figure 2, by filling each entry in the matrix using the score we obtained. Therefore, the scoring matrix is used to represent the head-modifier arc score for all the possible arcs connecting two tokens in a sentence (Zheng, 2017). Using this scoring arc matrix, we build our graphbased parser. 4 Jlap = E z∼Qφ (z|x) [log Pθ (x|z)] − KL(Qφ (z|x)||Pθ (z)) + E [log Pω (T |z)] . z∼Qφ (z|x) We show Jlap is the ELBO of J in the appendix A.1. In practice, similar as VAE-style models, [log Pθ (x|z)] is apE z∼Qφ (z|x) 1 PN proximated by j=1 log Pθ (x|zj ) N and [log P (T |z)] by E ω z∼Qφ (z|x) 1 PN j=1 log Pω (T |zj ), where zj is the j-th N sample of N samples sampled from Qφ (z|x). At prediction stage, we simply use µz rather than sampling z. Variational Autoencoding Parser VAP (illustrated in Figure 3b) is a semi-supervised"
2020.iwpt-1.5,P17-1029,0,0.0222381,"t Root X1 0 … Xt … XL-1 latent variable retains the contextual information from lower-level neural models to assist finding its head or its modifier; as well as forcing the representation of similar tokens to be closer. The latent variable group z is modeled via P (z|x). In addition, we model the process of reconstructing the input sentence from the latent variable through a generative story P (x|z). We adjust the original VAE setup in our semisupervised task by considering examples with labels, similar to recent conditional variational formulations (Sohn et al., 2015; Miao and Blunsom, 2016; Zhou and Neubig, 2017). We propose a full probabilistic model for a given sentence x, with the unified objective to maximize for both supervised and unsupervised parsing as follows: ( 1, if T exists,  J = log Pθ (x)Pω (T |x),  = 0, otherwise. The score of the (t, t-1) right arc X1 0 … 0 0 S(t, t-1) S(t-1, t) 0 … Xt 0 The score of the (t-1, t) left arc XL-1 0 Figure 2: In this illustration of the arc scoring matrix, each entry represents the (h(head) → m(modif ier)) score. ˆ X x ˆt x 1 x ˆt x ˆt+1 Y yt z yt yt+1 x x Decoder 1 This objective can be interpreted as follows: if the training example has a golden tree T"
2020.sigdial-1.10,W17-0802,0,0.0565645,"Missing"
2020.sigdial-1.10,N16-1070,0,0.0212925,"his intuition. We design a hybrid relational model that combines neural networks and declarative inference Introduction Online conversations are rampant on social media channels, news forums, course websites and various other discussion websites consisting of diverse groups of participants. While most efforts have been directed towards identifying and filtering negative and abusive content (Wang and Cardie, 2014; Wulczyn et al., 2017; Zhang et al., 2018), in this paper we focus on characterizing and automatically identifying the positive aspects of online conversations (Jurafsky et al., 2009; Niculae and Danescu-Niculescu-Mizil, 2016; Napoles et al., 2017a). We specifically focus on collaborative conversations, which help achieve a shared goal such as gaining new insights about the discussion topic like response informativeness, engagement etc. Rather than looking at the outcomes of such conversations (e.g., task completion (Niculae and Danescu-Niculescu-Mizil, 2016)), we analyze conversational behaviors, specifically looking at indications of collaborative behavior that is conducive to 74 Proceedings of the SIGdial 2020 Conference, pages 74–78 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Lin"
2020.sigdial-1.10,P14-2113,0,0.0307335,"intuition in this paper is that reasoning and enforcing consistency over these behaviors can help capture the conversational dynamics and lead to more accurate predictions. Our technical approach follows this intuition. We design a hybrid relational model that combines neural networks and declarative inference Introduction Online conversations are rampant on social media channels, news forums, course websites and various other discussion websites consisting of diverse groups of participants. While most efforts have been directed towards identifying and filtering negative and abusive content (Wang and Cardie, 2014; Wulczyn et al., 2017; Zhang et al., 2018), in this paper we focus on characterizing and automatically identifying the positive aspects of online conversations (Jurafsky et al., 2009; Niculae and Danescu-Niculescu-Mizil, 2016; Napoles et al., 2017a). We specifically focus on collaborative conversations, which help achieve a shared goal such as gaining new insights about the discussion topic like response informativeness, engagement etc. Rather than looking at the outcomes of such conversations (e.g., task completion (Niculae and Danescu-Niculescu-Mizil, 2016)), we analyze conversational behav"
2020.sigdial-1.10,W97-0703,0,0.0400113,"function. After scoring factors, values are assigned to the output variables by running an inference procedure. DRaiL uses Integer Linear Programming (ILP) to solve the inference problem. In our setup, we compare two models, with and without inference, corresponding to the global and local models. i subject to C, ∀i; ri ∈ {0, 1} Features Degree of sentiment and intensity Sentences per post, words per posts, post depth Upvote/downvote ratio, u − d, u + d, u/(u + d) 2 per. pronouns, quotes of prev. posts, @username tags (Dis)agreement markers, content indicators, post references Lexical chains (Barzilay and Elhadad, 1997) Profanity, bad words, short posts indicators Number of posts, number of threads Question marks, question forms, question types Global Learning When multiple rules are defined in DRaiL, each has its own neural architecture and parameters. Since these rules are interconnected, DRaiL learns a globally normalized model which uses inference to ensure that the scoring functions for all rules result in a globally consistent decision. We adapted the structured hinge loss used in DRaiL to handle latent predicates. The loss function is defined over all neural parameters w, and the error is back-propaga"
2020.sigdial-1.10,P18-1125,0,0.0158945,"d enforcing consistency over these behaviors can help capture the conversational dynamics and lead to more accurate predictions. Our technical approach follows this intuition. We design a hybrid relational model that combines neural networks and declarative inference Introduction Online conversations are rampant on social media channels, news forums, course websites and various other discussion websites consisting of diverse groups of participants. While most efforts have been directed towards identifying and filtering negative and abusive content (Wang and Cardie, 2014; Wulczyn et al., 2017; Zhang et al., 2018), in this paper we focus on characterizing and automatically identifying the positive aspects of online conversations (Jurafsky et al., 2009; Niculae and Danescu-Niculescu-Mizil, 2016; Napoles et al., 2017a). We specifically focus on collaborative conversations, which help achieve a shared goal such as gaining new insights about the discussion topic like response informativeness, engagement etc. Rather than looking at the outcomes of such conversations (e.g., task completion (Niculae and Danescu-Niculescu-Mizil, 2016)), we analyze conversational behaviors, specifically looking at indications o"
2020.sigdial-1.10,W16-0504,0,0.0675388,"Missing"
2020.sigdial-1.10,W16-5906,1,0.834586,"tional features, we treat them as discrete latent variables which are assigned together-with, and consistent-with, the final classification task. Each behavior is captured by a binary latent variable, denoted as h = hh1 , ..., hk i, indicating if it’s active or not in the given thread. These decisions are then connected with the final prediction, denoted y, a binary output value. This results in a factor graph (Figure 1). Each individual decision is scored by a neural net, and uses a set of features capturing relevant properties in the input conversation. To learn this model, we extend DRaiL (Zhang et al., 2016), a recently introduced framework for combining declarative inference with neural networks, described briefly in the following section. Our extension allows for the introduction of discrete latent predicates into the model. 2.1 Non-Collaborative Discourse Behaviors (A) Low Idea Development users who: (1) deviate from the thread topic and change the topic, (2) ignore previously raised ideas and give preference to their own, (3) repeat or reinforce previous viewpoints. (B) Low User Engagement users who: (1) show little interest, (2) add shallow contributions, such as jokes or links. (C) Negative"
2020.sigdial-1.10,N09-1072,0,0.0419197,"ical approach follows this intuition. We design a hybrid relational model that combines neural networks and declarative inference Introduction Online conversations are rampant on social media channels, news forums, course websites and various other discussion websites consisting of diverse groups of participants. While most efforts have been directed towards identifying and filtering negative and abusive content (Wang and Cardie, 2014; Wulczyn et al., 2017; Zhang et al., 2018), in this paper we focus on characterizing and automatically identifying the positive aspects of online conversations (Jurafsky et al., 2009; Niculae and Danescu-Niculescu-Mizil, 2016; Napoles et al., 2017a). We specifically focus on collaborative conversations, which help achieve a shared goal such as gaining new insights about the discussion topic like response informativeness, engagement etc. Rather than looking at the outcomes of such conversations (e.g., task completion (Niculae and Danescu-Niculescu-Mizil, 2016)), we analyze conversational behaviors, specifically looking at indications of collaborative behavior that is conducive to 74 Proceedings of the SIGdial 2020 Conference, pages 74–78 c 1st virtual meeting, 01-03 July 2"
2020.starsem-1.18,P08-1090,0,0.0651691,"ediction, with meaningful confounding candidate choices that force systems not to depend only on textual information. (3) we establish baseline performances using language models and justify the importance of looking beyond textual information in understanding human relationships. 2 Related Work Manually constructed scripts are used to represent structured knowledge in 1970-80s (Schank and Abelson, 1977). Subsequently, narrative event chains, unsupervised generation of such representations are introduced. Narrative event chains are sequences of events revolving around one central protagonist (Chambers and Jurafsky, 2008). Stemming from narrative event chains is the narrative cloze task where one event chain is removed and systems are required to fill in the blanks (Chambers and Jurafsky, 2008). The task is further refined to multiple choice form - Multiple Choice Narrative Cloze (MCNC) where different choices are randomly sampled from events that do not belong to the chain (Granroth-Wilding and Clark, 2016). Swaf Af (Zellers et al., 2018) and Hella Swag (Zellers et al., 2019) are datasets for next event prediction, containing multiple choice questions covering a wide range of grounded situations that are cons"
2020.starsem-1.18,N19-1423,0,0.0342871,"Missing"
2020.starsem-1.18,D10-1008,0,0.0321248,"Missing"
2020.starsem-1.18,D16-1057,0,0.0692492,"ATOMIC as the foundation, SocialIQA, a dataset of 38,000 multiple-choice questions about social situations is constructed (Sap et al., 2019). Event2Mind is a corpus of phrasal verbs that is constructed to support the examination of the intents and reactions of common situations (Rashkin et al., 2018b). (Mohammad, 2018) presents NRC VAD Lexicon, a corpus of 20,000 English words with human ratings of valence, arousal and dominance while NRC Affect Intensity Lexicon (AIL) provides emotional categories and associated real values for approximately 6,000 English words (Mohammad, 2017). SocialSent (Hamilton et al., 2016) assigns tokens sentiment scores with contexts taken into consideration. One set of lexicons is constructed from the subreddit r/relationships which we use in our error analyses. Other works model relationships, between literary characters (Iyyer et al., 2016) or countries (Han et al., 2019), described in text using an unsupervised neural model, by mapping them to a latent space. Elements of psychology in ROCstories are analyzed in (Rashkin et al., 2018a) where ROCStories are annotated with motivations and emotional reactions of the characters involved. We create Social Narrative Tree with var"
2020.starsem-1.18,N19-1167,0,0.0228835,"2018b). (Mohammad, 2018) presents NRC VAD Lexicon, a corpus of 20,000 English words with human ratings of valence, arousal and dominance while NRC Affect Intensity Lexicon (AIL) provides emotional categories and associated real values for approximately 6,000 English words (Mohammad, 2017). SocialSent (Hamilton et al., 2016) assigns tokens sentiment scores with contexts taken into consideration. One set of lexicons is constructed from the subreddit r/relationships which we use in our error analyses. Other works model relationships, between literary characters (Iyyer et al., 2016) or countries (Han et al., 2019), described in text using an unsupervised neural model, by mapping them to a latent space. Elements of psychology in ROCstories are analyzed in (Rashkin et al., 2018a) where ROCStories are annotated with motivations and emotional reactions of the characters involved. We create Social Narrative Tree with various social and psychological elements embedded in each story and these elements together contribute to the rise and fall of a relationship. 3 Collaborative Construction of Social Narratives Our main contribution in this paper is the construction of a social narrative corpus consisting of sh"
2020.starsem-1.18,N16-1180,0,0.0566185,"Missing"
2020.starsem-1.18,P18-1017,0,0.0941295,"rks. (Sap et al., 2018) present ATOMIC, a knowledge graph of commonsense knowledge with 877k descriptions in free text form and focuses on ’if-then’ relationships of causes, effects and attributes. Automatic construction of such knowledge base is explored in Comet (Bosselut et al., 2019). With ATOMIC as the foundation, SocialIQA, a dataset of 38,000 multiple-choice questions about social situations is constructed (Sap et al., 2019). Event2Mind is a corpus of phrasal verbs that is constructed to support the examination of the intents and reactions of common situations (Rashkin et al., 2018b). (Mohammad, 2018) presents NRC VAD Lexicon, a corpus of 20,000 English words with human ratings of valence, arousal and dominance while NRC Affect Intensity Lexicon (AIL) provides emotional categories and associated real values for approximately 6,000 English words (Mohammad, 2017). SocialSent (Hamilton et al., 2016) assigns tokens sentiment scores with contexts taken into consideration. One set of lexicons is constructed from the subreddit r/relationships which we use in our error analyses. Other works model relationships, between literary characters (Iyyer et al., 2016) or countries (Han et al., 2019), descr"
2020.starsem-1.18,N16-1098,0,0.356134,"d methods discussed in current work do not directly address these challenges. A fixed set of mappings from events to relationship impact, in a style similar to Event2Mind (Rashkin et al., 2018b), is not sufficient to capture the uniqueness of each social situation. Similarly, SocialIQA (Sap et al., 2019) inspects various elements individually but does not unify them into a single force that influences a relationship. Other current datasets 168 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 168–178 Barcelona, Spain (Online), December 12–13, 2020 (Mostafazadeh et al., 2016; Zellers et al., 2018, 2019) cover a broad spectrum of commonsense knowledge, but do not focus on the social aspect. Although it can be annotated for such inspection such as in the investigation of intents and reactions (Rashkin et al., 2018a), this method is not pertaining to relationship analyses because a meaningful change may not always be present. For example, here is a description of a social scenario sampled from the ROCStories corpus: Tina decided on going hiking with her friend Tony. They both decided on a difficult path. Upon ascension, Tina fell and cut her leg. They both decided i"
2020.starsem-1.18,W17-5543,0,0.0273696,"ng conflicts with other people. This skill is formally termed as social competence, which humans easily acquire (Rubin et al., 1995). In contrast to humans’ innate ability to perceive social situations, machines struggle in developing social understanding. For example, machines’ performance in reasoning about motivation and emotional reactions given a short context is significantly lower compared to human performance (Sap et al., 2019). Moreover, identifying the intents and reactions of characters in narratives poses another challenge for machines (Goyal et al., 2010; Chaturvedi et al., 2016; Rahimtoroghi et al., 2017; Rashkin et al., 2018a). Reasoning about relationship trajectories adds yet another layer of difficulty as in addition to the challenge of identifying the implicit factors that guide characters’ behavior, such as intents, reactions and mental states, systems also need to understand how these elements collectively contribute to the evolution of relationships. This additional layer of understanding is challenging primarily because the impact of a particular event on a relationship is unique to each social scenario. For instance, an argument can help resolve differences or deepen them, depending"
2020.starsem-1.18,P18-1213,0,0.280263,"ple. This skill is formally termed as social competence, which humans easily acquire (Rubin et al., 1995). In contrast to humans’ innate ability to perceive social situations, machines struggle in developing social understanding. For example, machines’ performance in reasoning about motivation and emotional reactions given a short context is significantly lower compared to human performance (Sap et al., 2019). Moreover, identifying the intents and reactions of characters in narratives poses another challenge for machines (Goyal et al., 2010; Chaturvedi et al., 2016; Rahimtoroghi et al., 2017; Rashkin et al., 2018a). Reasoning about relationship trajectories adds yet another layer of difficulty as in addition to the challenge of identifying the implicit factors that guide characters’ behavior, such as intents, reactions and mental states, systems also need to understand how these elements collectively contribute to the evolution of relationships. This additional layer of understanding is challenging primarily because the impact of a particular event on a relationship is unique to each social scenario. For instance, an argument can help resolve differences or deepen them, depending on the personalities,"
2020.starsem-1.18,P18-1043,0,0.338468,"ple. This skill is formally termed as social competence, which humans easily acquire (Rubin et al., 1995). In contrast to humans’ innate ability to perceive social situations, machines struggle in developing social understanding. For example, machines’ performance in reasoning about motivation and emotional reactions given a short context is significantly lower compared to human performance (Sap et al., 2019). Moreover, identifying the intents and reactions of characters in narratives poses another challenge for machines (Goyal et al., 2010; Chaturvedi et al., 2016; Rahimtoroghi et al., 2017; Rashkin et al., 2018a). Reasoning about relationship trajectories adds yet another layer of difficulty as in addition to the challenge of identifying the implicit factors that guide characters’ behavior, such as intents, reactions and mental states, systems also need to understand how these elements collectively contribute to the evolution of relationships. This additional layer of understanding is challenging primarily because the impact of a particular event on a relationship is unique to each social scenario. For instance, an argument can help resolve differences or deepen them, depending on the personalities,"
2020.starsem-1.18,D19-1454,0,0.0636496,"other hand, they may insist on their own views and part ways. For humans, being able to reason about relationship trajectories is crucial in achieving personal goals while avoiding conflicts with other people. This skill is formally termed as social competence, which humans easily acquire (Rubin et al., 1995). In contrast to humans’ innate ability to perceive social situations, machines struggle in developing social understanding. For example, machines’ performance in reasoning about motivation and emotional reactions given a short context is significantly lower compared to human performance (Sap et al., 2019). Moreover, identifying the intents and reactions of characters in narratives poses another challenge for machines (Goyal et al., 2010; Chaturvedi et al., 2016; Rahimtoroghi et al., 2017; Rashkin et al., 2018a). Reasoning about relationship trajectories adds yet another layer of difficulty as in addition to the challenge of identifying the implicit factors that guide characters’ behavior, such as intents, reactions and mental states, systems also need to understand how these elements collectively contribute to the evolution of relationships. This additional layer of understanding is challengin"
2020.starsem-1.18,D18-1009,0,0.132879,"ent work do not directly address these challenges. A fixed set of mappings from events to relationship impact, in a style similar to Event2Mind (Rashkin et al., 2018b), is not sufficient to capture the uniqueness of each social situation. Similarly, SocialIQA (Sap et al., 2019) inspects various elements individually but does not unify them into a single force that influences a relationship. Other current datasets 168 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 168–178 Barcelona, Spain (Online), December 12–13, 2020 (Mostafazadeh et al., 2016; Zellers et al., 2018, 2019) cover a broad spectrum of commonsense knowledge, but do not focus on the social aspect. Although it can be annotated for such inspection such as in the investigation of intents and reactions (Rashkin et al., 2018a), this method is not pertaining to relationship analyses because a meaningful change may not always be present. For example, here is a description of a social scenario sampled from the ROCStories corpus: Tina decided on going hiking with her friend Tony. They both decided on a difficult path. Upon ascension, Tina fell and cut her leg. They both decided it was too dangerous fo"
2020.starsem-1.18,P19-1472,0,0.0131232,"of such representations are introduced. Narrative event chains are sequences of events revolving around one central protagonist (Chambers and Jurafsky, 2008). Stemming from narrative event chains is the narrative cloze task where one event chain is removed and systems are required to fill in the blanks (Chambers and Jurafsky, 2008). The task is further refined to multiple choice form - Multiple Choice Narrative Cloze (MCNC) where different choices are randomly sampled from events that do not belong to the chain (Granroth-Wilding and Clark, 2016). Swaf Af (Zellers et al., 2018) and Hella Swag (Zellers et al., 2019) are datasets for next event prediction, containing multiple choice questions covering a wide range of grounded situations that are constructed from video captions. Furthermore, (Mostafazadeh et al., 2016) create ROCStories, a corpus of 50k commonsense stories each of fivesentence long and propose Story Cloze Test where systems are required to select the most plausible 169 ending for an incomplete narrative. In our work, Relationship Outlook Prediction and Resolution Prediction are multiple-choice tasks of similar motivation with a focus on social implications. Various social elements are exte"
2021.eacl-main.100,P16-1231,0,0.0294726,"nference based on the popular AD3 algorithm (Martins et al., 2015) and the two randomized inference algorithms. Our experiments show that in all cases, deep structured prediction outperforms traditional shallow approaches, structured learning outperforms inference over locally trained models, and generic randomized inference performs competitively to exact inference. 2 Related Work Using deep structured prediction for NLP has been studied in previous work, typically on traditional sentence-level tasks such as dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016), named entity recognition (Lample et al., 2016) and sequence labeling systems (Ma and Hovy, 2016). In most of these cases, inference is tractable. More recently, some efforts have started to look at incorporating deep structured prediction to discourse tasks such as argument mining (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off-the-shelf optimizers or approximation algorithms like AD3 (Ma"
2021.eacl-main.100,D14-1082,0,0.0471064,"nd computational cost. We compare exact ILP models, approximate inference based on the popular AD3 algorithm (Martins et al., 2015) and the two randomized inference algorithms. Our experiments show that in all cases, deep structured prediction outperforms traditional shallow approaches, structured learning outperforms inference over locally trained models, and generic randomized inference performs competitively to exact inference. 2 Related Work Using deep structured prediction for NLP has been studied in previous work, typically on traditional sentence-level tasks such as dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016), named entity recognition (Lample et al., 2016) and sequence labeling systems (Ma and Hovy, 2016). In most of these cases, inference is tractable. More recently, some efforts have started to look at incorporating deep structured prediction to discourse tasks such as argument mining (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off"
2021.eacl-main.100,N19-1423,0,0.0570898,"Missing"
2021.eacl-main.100,D19-1041,0,0.424762,"m and solved either using off-the-shelf optimizers or approximation algorithms like AD3 (Martins et al., 2015). Randomized approximation has been introduced as an alternative to exact inference. Zhang et al. (2014) suggest a simple randomized greedy inference algorithm and empirically demonstrate its effectiveness for dependency parsing and other traditional NLP tasks (Zhang et al., 2015). The theoretical results in (Honorio and Jaakkola, 2016), based on the probably approximately correct Bayes framework, characterize these findings by providing generalization bounds. More recently, Ma et al. (2019) extended the work of (Zhang et al., 2014, 2015) to structured prediction tasks with large structured outputs by leveraging local classifiers to find good starting solutions and improve the accuracy of search. All of these methods were evaluated on linear structured models. In this paper, we focus on two tasks: mining argumentative structures in essays and stance prediction in online debates. Stab and Gurevych (2017) approach argumentative essays using an exhaustive set of hand-crafted features, linear local classifiers and ILP at test time. Niculae et al. (2017) jointly learn to score multipl"
2021.eacl-main.100,I13-1191,0,0.111421,"utput scores of independently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures. 1 Introduction Many discourse-level NLP tasks require modeling complex interactions between multiple sentences, paragraphs or even documents. For example, analyzing opinions in online conversations (Hasan and Ng, 2013; Sridhar et al., 2015) requires modeling the dependencies between the opinions in individual posts, the disagreement between posts in long conversational threads and the overall view of users, given all their posts. Learning in these settings is extremely challenging. It requires highly expressive models that can capture the claims made in each document, either by using a rich, manually crafted feature set, or by ∗ Contributed equally to this work as first authors using neural architectures to learn an expressive representation (Ji and Eisenstein, 2014; Niculae et al., 2017). In addition, rea"
2021.eacl-main.100,P14-1002,0,0.0336163,", analyzing opinions in online conversations (Hasan and Ng, 2013; Sridhar et al., 2015) requires modeling the dependencies between the opinions in individual posts, the disagreement between posts in long conversational threads and the overall view of users, given all their posts. Learning in these settings is extremely challenging. It requires highly expressive models that can capture the claims made in each document, either by using a rich, manually crafted feature set, or by ∗ Contributed equally to this work as first authors using neural architectures to learn an expressive representation (Ji and Eisenstein, 2014; Niculae et al., 2017). In addition, reasoning about the interaction between these decisions is often computationally challenging, as it requires incorporating domain-specific constraints into the search procedure, making exact inference intractable. As a result, most current work relies on highly engineered solutions, which are difficult to adapt. Instead of training structured predictors that model the interaction between decisions during training, they combine locally trained classifiers at test time (Stab and Gurevych, 2017). Our goal in this paper is to study realistic settings, in which"
2021.eacl-main.100,P19-1464,0,0.0241033,"Missing"
2021.eacl-main.100,2021.tacl-1.7,1,0.83535,"Missing"
2021.eacl-main.100,N16-1030,0,0.375838,"engineered solutions, which are difficult to adapt. Instead of training structured predictors that model the interaction between decisions during training, they combine locally trained classifiers at test time (Stab and Gurevych, 2017). Our goal in this paper is to study realistic settings, in which discourse-level problems can be learned efficiently when leveraging deep structured prediction, a framework for combining rich neural representation with an inference-layer, forcing consistency between them (Zheng et al., 2015). These models were applied successfully to simpler NLP tagging tasks (Lample et al., 2016), in which inference is tractable. However, as shown in a recent argumentation mining work (Niculae et al., 2017), their applicability to more complex learning tasks is not guaranteed. Randomized inference algorithms have been proposed for structured NLP tasks, such as tagging and dependency parsing, in the context of linear models (Zhang et al., 2014, 2015; Ma et al., 2019). This approach offers an efficient alternative to exact inference. Instead of finding the optimal output state, the algorithm makes greedy updates to a randomly initialized (or locally initialized) output assignment state."
2021.eacl-main.100,N16-1000,0,0.22761,"Missing"
2021.eacl-main.100,D14-1162,0,0.0855341,"ational cost. We compare exact ILP models, approximate inference based on the popular AD3 algorithm (Martins et al., 2015) and the two randomized inference algorithms. Our experiments show that in all cases, deep structured prediction outperforms traditional shallow approaches, structured learning outperforms inference over locally trained models, and generic randomized inference performs competitively to exact inference. 2 Related Work Using deep structured prediction for NLP has been studied in previous work, typically on traditional sentence-level tasks such as dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016), named entity recognition (Lample et al., 2016) and sequence labeling systems (Ma and Hovy, 2016). In most of these cases, inference is tractable. More recently, some efforts have started to look at incorporating deep structured prediction to discourse tasks such as argument mining (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off"
2021.eacl-main.100,C18-1316,1,0.840562,"models to previous work on this dataset. Sridhar et al. (2015) use probabilistic soft logic (PSL) to learn a global assignment for the post labels. They use local classifiers to obtain the input scores to PSL. The main difference between their approach and ours is that we are able to backpropagate the global error back into the classifiers, and we find that it improves performance considerably. Even though we use BERT encoders in our structured procedure, we can see that BERT alone is not able to solve the task. Lastly, we compare to the structured representation learning method of Li et al. (2018) and find that we are able to improve on abortion and gay marriage only. Note that these two are the issues with more data available (8,000 and 7,000 posts respectively). The main difference with their approach and ours is that they push author profile information into the learned representation. We hypothesize that this is key to obtain good performance for gun control, which contains only 4,000 posts. Inference Analysis In our experiments, randomized inference always outperforms ILP and AD3 in terms of runtime. Figure 3 shows the speedup factor per epoch against ILP and AD3 . In argument min"
2021.eacl-main.100,P19-1629,0,0.118366,"LP has been studied in previous work, typically on traditional sentence-level tasks such as dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016), named entity recognition (Lample et al., 2016) and sequence labeling systems (Ma and Hovy, 2016). In most of these cases, inference is tractable. More recently, some efforts have started to look at incorporating deep structured prediction to discourse tasks such as argument mining (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off-the-shelf optimizers or approximation algorithms like AD3 (Martins et al., 2015). Randomized approximation has been introduced as an alternative to exact inference. Zhang et al. (2014) suggest a simple randomized greedy inference algorithm and empirically demonstrate its effectiveness for dependency parsing and other traditional NLP tasks (Zhang et al., 2015). The theoretical results in (Honorio and Jaakkola, 2016), based on the probably approximately correct Bayes framework, c"
2021.eacl-main.100,P16-1101,0,0.0143588,"algorithms. Our experiments show that in all cases, deep structured prediction outperforms traditional shallow approaches, structured learning outperforms inference over locally trained models, and generic randomized inference performs competitively to exact inference. 2 Related Work Using deep structured prediction for NLP has been studied in previous work, typically on traditional sentence-level tasks such as dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016), named entity recognition (Lample et al., 2016) and sequence labeling systems (Ma and Hovy, 2016). In most of these cases, inference is tractable. More recently, some efforts have started to look at incorporating deep structured prediction to discourse tasks such as argument mining (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off-the-shelf optimizers or approximation algorithms like AD3 (Martins et al., 2015). Randomized approximation has been introduced as an alternative to exact infer"
2021.eacl-main.100,S14-2082,0,0.028307,"t and they can be labeled either as claims, major claims or premises. Edges correspond to stances (i.e., support/attack relations between nodes). The spans of 1175 texts are given, and we need to label nodes, predict which pairs of nodes are connected by an edge and label the edges. Domain knowledge can be exploited as there are only valid edges between pairs of premises, a premise and a claim, or a claim and a major claim. At the same time, all trees are rooted at major claims. Similarly to previous work, we model second order relationships: grandparent (a → b → c) and co-parent (a → b ← c) (Martins and Almeida, 2014; Niculae et al., 2017). Figure 1 has a visual representation of the structure. In this problem, each forest defines a factor graph Ψ and G is the collection of all documents. We define a set of five neural architectures corresponding to the five types of decisions that we need to make: N N = {ρnode , ρlink , ρstance , ρgrandparent , ρcoparent }, each with its own set of parameters θ = {θnode , θlink , θstance , θgrandparent , θcoparent }. Note that in principle, we can substitute each (ρi , θi ) with any neural architecture. We include details about the architectures in the experimental secti"
2021.eacl-main.100,P17-1091,0,0.329187,"Missing"
2021.eacl-main.100,D17-1143,0,0.033749,"Missing"
2021.eacl-main.100,W10-0214,0,0.0247292,"on two tasks: mining argumentative structures in essays and stance prediction in online debates. Stab and Gurevych (2017) approach argumentative essays using an exhaustive set of hand-crafted features, linear local classifiers and ILP at test time. Niculae et al. (2017) jointly learn to score multiple decisions while enforcing domain constraints. They explore structured SVMs and RNNs, using the AD3 inference algorithm (Martins et al., 2015). On the other hand, there are several works on predicting user stances in online debates. Some approaches model the problem as a text classification task (Somasundaran and Wiebe, 2010; Sun et al., 2018), while other approaches take a collective approach to model user behavior and interactions (Walker et al., 2012; Hasan and Ng, 2013; Sridhar et al., 2015; Li et al., 2018). In the latter case, inference procedures include MaxCut, ILP and probabilistic soft logic (Bach et al., 2017). 3 Modeling We look at two challenging structured prediction problems that deal with long texts where dependencies span across different paragraphs, documents and authors. To deal with these setups, we define neural factor graphs G = {Ψ} where each decision ψi ∈ Ψ is associated with a neural arch"
2021.eacl-main.100,P15-1012,0,0.350383,"pendently trained classifiers. One of the main reasons for this is that constrained inference comes at a high computational cost. In this paper, we explore the use of randomized inference to alleviate this concern and show that we can efficiently leverage deep structured prediction and expressive neural encoders for a set of tasks involving complicated argumentative structures. 1 Introduction Many discourse-level NLP tasks require modeling complex interactions between multiple sentences, paragraphs or even documents. For example, analyzing opinions in online conversations (Hasan and Ng, 2013; Sridhar et al., 2015) requires modeling the dependencies between the opinions in individual posts, the disagreement between posts in long conversational threads and the overall view of users, given all their posts. Learning in these settings is extremely challenging. It requires highly expressive models that can capture the claims made in each document, either by using a rich, manually crafted feature set, or by ∗ Contributed equally to this work as first authors using neural architectures to learn an expressive representation (Ji and Eisenstein, 2014; Niculae et al., 2017). In addition, reasoning about the intera"
2021.eacl-main.100,J17-3005,0,0.256747,"using neural architectures to learn an expressive representation (Ji and Eisenstein, 2014; Niculae et al., 2017). In addition, reasoning about the interaction between these decisions is often computationally challenging, as it requires incorporating domain-specific constraints into the search procedure, making exact inference intractable. As a result, most current work relies on highly engineered solutions, which are difficult to adapt. Instead of training structured predictors that model the interaction between decisions during training, they combine locally trained classifiers at test time (Stab and Gurevych, 2017). Our goal in this paper is to study realistic settings, in which discourse-level problems can be learned efficiently when leveraging deep structured prediction, a framework for combining rich neural representation with an inference-layer, forcing consistency between them (Zheng et al., 2015). These models were applied successfully to simpler NLP tagging tasks (Lample et al., 2016), in which inference is tractable. However, as shown in a recent argumentation mining work (Niculae et al., 2017), their applicability to more complex learning tasks is not guaranteed. Randomized inference algorithms"
2021.eacl-main.100,C18-1203,0,0.0128982,"tive structures in essays and stance prediction in online debates. Stab and Gurevych (2017) approach argumentative essays using an exhaustive set of hand-crafted features, linear local classifiers and ILP at test time. Niculae et al. (2017) jointly learn to score multiple decisions while enforcing domain constraints. They explore structured SVMs and RNNs, using the AD3 inference algorithm (Martins et al., 2015). On the other hand, there are several works on predicting user stances in online debates. Some approaches model the problem as a text classification task (Somasundaran and Wiebe, 2010; Sun et al., 2018), while other approaches take a collective approach to model user behavior and interactions (Walker et al., 2012; Hasan and Ng, 2013; Sridhar et al., 2015; Li et al., 2018). In the latter case, inference procedures include MaxCut, ILP and probabilistic soft logic (Bach et al., 2017). 3 Modeling We look at two challenging structured prediction problems that deal with long texts where dependencies span across different paragraphs, documents and authors. To deal with these setups, we define neural factor graphs G = {Ψ} where each decision ψi ∈ Ψ is associated with a neural architecture ρi and a s"
2021.eacl-main.100,N12-1072,0,0.246396,"tive essays using an exhaustive set of hand-crafted features, linear local classifiers and ILP at test time. Niculae et al. (2017) jointly learn to score multiple decisions while enforcing domain constraints. They explore structured SVMs and RNNs, using the AD3 inference algorithm (Martins et al., 2015). On the other hand, there are several works on predicting user stances in online debates. Some approaches model the problem as a text classification task (Somasundaran and Wiebe, 2010; Sun et al., 2018), while other approaches take a collective approach to model user behavior and interactions (Walker et al., 2012; Hasan and Ng, 2013; Sridhar et al., 2015; Li et al., 2018). In the latter case, inference procedures include MaxCut, ILP and probabilistic soft logic (Bach et al., 2017). 3 Modeling We look at two challenging structured prediction problems that deal with long texts where dependencies span across different paragraphs, documents and authors. To deal with these setups, we define neural factor graphs G = {Ψ} where each decision ψi ∈ Ψ is associated with a neural architecture ρi and a set of parameters θi . In this section, we introduce the tasks in detail. 3.1 Argument Mining This task aims to i"
2021.eacl-main.100,P15-1032,0,0.0270766,"e compare exact ILP models, approximate inference based on the popular AD3 algorithm (Martins et al., 2015) and the two randomized inference algorithms. Our experiments show that in all cases, deep structured prediction outperforms traditional shallow approaches, structured learning outperforms inference over locally trained models, and generic randomized inference performs competitively to exact inference. 2 Related Work Using deep structured prediction for NLP has been studied in previous work, typically on traditional sentence-level tasks such as dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016), named entity recognition (Lample et al., 2016) and sequence labeling systems (Ma and Hovy, 2016). In most of these cases, inference is tractable. More recently, some efforts have started to look at incorporating deep structured prediction to discourse tasks such as argument mining (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off-the-shelf optimizers"
2021.eacl-main.100,D14-1109,0,0.336674,"hen leveraging deep structured prediction, a framework for combining rich neural representation with an inference-layer, forcing consistency between them (Zheng et al., 2015). These models were applied successfully to simpler NLP tagging tasks (Lample et al., 2016), in which inference is tractable. However, as shown in a recent argumentation mining work (Niculae et al., 2017), their applicability to more complex learning tasks is not guaranteed. Randomized inference algorithms have been proposed for structured NLP tasks, such as tagging and dependency parsing, in the context of linear models (Zhang et al., 2014, 2015; Ma et al., 2019). This approach offers an efficient alternative to exact inference. Instead of finding the optimal output state, the algorithm makes greedy updates to a randomly initialized (or locally initialized) output assignment state. Our main contribution is to explore these ideas in the context of deep structured models composed of expressive text encoders, where theoretical guarantees are weak or nonexistent. Moreover, we do this for discourse-level tasks involving a rich set of domain constraints. To do this, we consider two variations of this approach. In the first, the 1174"
2021.eacl-main.100,N15-1005,0,0.0199355,"ng (Niculae et al., 2017), event and temporal relation extraction (Han et al., 2019) and discourse representation parsing (Liu et al., 2019). In all of these cases, constrained inference is formulated as an integer linear program and solved either using off-the-shelf optimizers or approximation algorithms like AD3 (Martins et al., 2015). Randomized approximation has been introduced as an alternative to exact inference. Zhang et al. (2014) suggest a simple randomized greedy inference algorithm and empirically demonstrate its effectiveness for dependency parsing and other traditional NLP tasks (Zhang et al., 2015). The theoretical results in (Honorio and Jaakkola, 2016), based on the probably approximately correct Bayes framework, characterize these findings by providing generalization bounds. More recently, Ma et al. (2019) extended the work of (Zhang et al., 2014, 2015) to structured prediction tasks with large structured outputs by leveraging local classifiers to find good starting solutions and improve the accuracy of search. All of these methods were evaluated on linear structured models. In this paper, we focus on two tasks: mining argumentative structures in essays and stance prediction in onlin"
2021.emnlp-main.102,N19-1053,0,0.0265544,"tasks. Although contextual information Our architecture generates a distributed represen- is captured by these models, they are not explicitly tation for each item in the graph that is contextu- designed to capture entity/event-centric informaalized by the representations of others. It can dy- tion. Hence, tasks that require such information 1 namically respond to queries, focusing the induced Repository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (M"
2021.emnlp-main.102,2020.acl-main.476,1,0.773335,"2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (20"
2021.emnlp-main.102,N19-1304,0,0.0222449,"s of others. It can dy- tion. Hence, tasks that require such information 1 namically respond to queries, focusing the induced Repository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are us"
2021.emnlp-main.102,D19-6603,0,0.0200392,"ore focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (2017) and Müller et al. (2019), aims to address those limitations via"
2021.emnlp-main.102,N19-1167,0,0.046782,"Missing"
2021.emnlp-main.102,N16-1180,0,0.173545,"that is contextu- designed to capture entity/event-centric informaalized by the representations of others. It can dy- tion. Hence, tasks that require such information 1 namically respond to queries, focusing the induced Repository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of t"
2021.emnlp-main.102,W16-5609,1,0.827189,"Missing"
2021.emnlp-main.102,P18-1067,1,0.839295,"fies all the information in the graph in one-shot. many NLP tasks. Although contextual information Our architecture generates a distributed represen- is captured by these models, they are not explicitly tation for each item in the graph that is contextu- designed to capture entity/event-centric informaalized by the representations of others. It can dy- tion. Hence, tasks that require such information 1 namically respond to queries, focusing the induced Repository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a"
2021.emnlp-main.102,P18-2081,0,0.0770567,"ph in one-shot. many NLP tasks. Although contextual information Our architecture generates a distributed represen- is captured by these models, they are not explicitly tation for each item in the graph that is contextu- designed to capture entity/event-centric informaalized by the representations of others. It can dy- tion. Hence, tasks that require such information 1 namically respond to queries, focusing the induced Repository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entit"
2021.emnlp-main.102,D16-1221,0,0.020429,"n Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (2017) and Müller et al. (2019), aims to address those limitations via a generic entity-issue-event-document graph, which is used to learn highly effective representations. Representing legislative preferences is typically done by modeling the ideal point of legislators represented in a Euclidean space from roll-call records (Poole et al., 1997). Recent approaches incorporate bill text information into this representation (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016; Kornilova et al., 2018c). Most relevant to our work is (Spell et al., 2020b) which uses social media information. We significantly extend these approaches by contextualizing the social media content using a novel architecture. 3 Data Data News Events Author Entities Ref. Entities Wikipedia Total Docs Count 367 455 10, 506 455 187, 811 Data Tweets Press Releases Perspectives News Articles Count 86, 409 62, 257 30, 446 8, 244 Table 1: Summary statistics of data topics: guns, LGBTQ rights, abortion, immigration, economic policy, taxes, middle east & environment. The data focused on 455 members"
2021.emnlp-main.102,D18-1388,0,0.0224761,"epository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Comp"
2021.emnlp-main.102,P19-1247,1,0.834871,"mpted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (2017) and Müller et al. (2019), aims to address those limitations via a generic entity-issue-event-document graph, which is used to"
2021.emnlp-main.102,2021.ccl-1.108,0,0.0448572,"Missing"
2021.emnlp-main.102,P14-5010,0,0.00420136,"do overlap. These events last for 7 − 10 days on average and hence the non-overlapping assumption within an issue is a reasonable relaxation of reality. To illustrate our point: coronavirus and civil-rights are separate issues and hence have overlapping events. An example event related to coronavirus could be “First case of COVID-19 outside of China”. Similarly an event about civil-rights could be “Officer part of George Floyd killing suspended”. We inspected the events manually by random sampling. More example events are in the appendix. 3.2 Data Pre-processing We use Stanford CoreNLP tool (Manning et al., 2014), Wikifier (Brank et al., 2017) and BERTbase-uncased implementation by Wolf et al. (2019) to preprocess data for our experiments. We tokenize the documents, apply coreference resolution and extract referenced entities from each document. The referenced entities are then wikified using Wikifier tool (Brank et al., 2017). The documents are then categorized by issues and events. News articles from allsides.com and perspectives from ontheissues.org are already classified by issues. We 2 https://projects.propublica.org/ We collected US political text related to 8 broad api-docs/congress-api/ 1355 4"
2021.emnlp-main.102,L16-1623,0,0.0274552,"), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (2017) and Müller et al. (2019), aims to address th"
2021.emnlp-main.102,D19-1603,0,0.0260312,"stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (2017) and Müller et al. (2019), aims to address those limitations via a generic entity-issue-event-document graph, which is used to learn highly effective representations. Representing legislative preferences is typically done by modeling the ideal point of legislators represented in a Euclidean space from roll-call records (Poole et al., 1997). Recent approaches incorporate bill text information into this representation (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016; Kornilova et al., 2018c). Most relevant to our work is (Spell et al., 2020b) which uses social media information. We significantly extend t"
2021.emnlp-main.102,P15-1139,0,0.0302733,"odel, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from Veliˇckovi´c et al. (2017) and Müller et al. (2019), aims to address those limitations via a generic entity-issue-event-document graph, which is used to learn highly effective representations. Representing legislative preferences is typically done by modeling the ideal point of legislators represented in a Euclidean space from roll-call records (Poole et al., 1997). Recent approaches incorporate bill text information into this representation (Gerrish and Blei, 2011; Nguyen et al., 2015; Kraft et al., 2016; Kornilova et al., 2018c). Most relevant to our work is (Spell et al., 2020b) which uses social media information. We significantly extend these approaches by contextualizing the social media content using a novel architecture. 3 Data Data News Events Author Entities Ref. Entities Wikipedia Total Docs Count 367 455 10, 506 455 187, 811 Data Tweets Press Releases Perspectives News Articles Count 86, 409 62, 257 30, 446 8, 244 Table 1: Summary statistics of data topics: guns, LGBTQ rights, abortion, immigration, economic policy, taxes, middle east & environment. The data foc"
2021.emnlp-main.102,S19-2172,0,0.0321529,"Missing"
2021.emnlp-main.102,K19-1053,0,0.0571689,"ann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention N"
2021.emnlp-main.102,N18-1202,0,0.0526721,"rengths of the different inputs. We address the first challenge by introducing a graph structure that ties together first-person informal (tweets) and formal discourse (press releases and perspectives), third-person current (news) and consolidated (Wikipedia) discourse. These documents are connected via their authors, the issues/events they discuss and the entities mentioned in them. As a clarifying example consider the tweet 2 Related Work by former-President Trump “The NRA is under Due to recent advances in text representations catalsiege by Cuomo”. This tweet will be represented in ysed by Peters et al. (2018), Vaswani et al. (2017) our graph by connecting the text node to the author and followed by Devlin et al. (2019), Liu et al. node (Trump) and the referenced entity node (NY (2019) and Yang et al. (2019), we are now able Gov. Cuomo). These settings are shown in Fig. 2. We propose a novel neural architecture that uni- to create rich textual representations, effective for fies all the information in the graph in one-shot. many NLP tasks. Although contextual information Our architecture generates a distributed represen- is captured by these models, they are not explicitly tation for each item in t"
2021.emnlp-main.102,P17-1068,0,0.0560116,"Missing"
2021.emnlp-main.102,2020.emnlp-main.620,1,0.722028,"- tion. Hence, tasks that require such information 1 namically respond to queries, focusing the induced Repository: https://github.com/pujarirepresentation on a specific context. In our exam- rajkumar/compositional_learner 1354 (Biessmann, 2016; Johnson and Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to s"
2021.emnlp-main.102,2020.emnlp-main.46,0,0.131981,"nd Goldwasser, 2018, 2016; Kornilova et al., 2018a; Chen et al., 2019), would benefit from more focused representations. Of late, several works attempted to solve such tasks, such as analyzing relationships and their evolution (Iyyer et al., 2016; Han et al., 2019), analyzing political discourse on news and social media (Demszky et al., 2019; Roy and Goldwasser, 2020) and political ideology (Diermeier et al., 2012; Preo¸tiuc-Pietro et al., 2017; Kulkarni et al., 2018). Various political tasks such as roll call vote prediction (Clinton et al., 2003; Kornilova et al., 2018b; Patil et al., 2019; Spell et al., 2020a; Davoodi et al., 2020), entity stance detection (Mohammad et al., 2016; Fang et al., 2019), hyper-partisan/fake news detection (Li and Goldwasser, 2019; Pali´c et al., 2019; Baly et al., 2020) require a rich understanding of the context around the entities that are present in the text. But, the representations used are usually limited in scope to specific tasks and not rich enough to capture information that is useful across several tasks. The Compositional Reader model, that builds upon Devlin et al. (2019) embeddings and consists of a transformer-based Graph Attention Network inspired from"
2021.emnlp-main.783,W19-1203,0,0.0276379,"Missing"
2021.emnlp-main.783,P16-1101,0,0.0176818,"ce, our work is broadly related to the works on entity-centric affect analysis (Deng and Wiebe, 2015; Field and Tsvetkov, 2019; Park et al., 2020). Combining neural networks and structured inference was explored for traditional NLP tasks such Usage of sociological theories like the Moral Foun- as dependency parsing (Chen and Manning, 2014; dation Theory (MFT) (Haidt and Joseph, 2004; Weiss et al., 2015; Andor et al., 2016), named enHaidt and Graham, 2007) and Framing (Entman, tity recognition (Lample et al., 2016) and sequence 1993; Chong and Druckman, 2007; Boydstun et al., labeling systems (Ma and Hovy, 2016; Zhang et al., 2014) in Natural Language Processing tasks has 2017). Recently, these efforts have expanded to gained significant interest. The Moral Foundation discourse-level tasks such as argumentation minTheory (MFT) has been widely used to study the ing (Niculae et al., 2017; Widmoser et al., 2021), effect of moral values on human behavioral pat- event/temporal relation extraction (Han et al., 2019) terns, such as charitable donations (Hoover et al., and discourse representation parsing (Liu et al., 2018), violent protests (Mooijman et al., 2018) and 2019). Following this trend, Pacheco a"
2021.findings-acl.401,D16-1148,0,0.0322219,"Missing"
2021.findings-acl.401,N19-1423,0,0.441881,"g. However the story is framed in very different ways - while the bottom article frames the story directly as a discussion of Antifa involvement, the top discusses it in the context of political messaging and journalism. Furthermore, we notice that the difference is focused on a specific entity - John Sullivan. Despite the fact that these distinctions are easily detectable by a human reader familiar with the political divisions in the U.S., they are very difficult to detect automatically. Recent success stories using large-scale pre-training for constructing highly expressive language models (Devlin et al., 2019) are designed to capture co-occurrence patterns, likely to miss these subtle differences. In this paper we suggest that bias detection requires a different set of self-supervised pre-training objectives that can help provide a better starting point for training downstream biased detection tasks. Specifically, we design three learning objectives. The first, captures political knowledge, focusing on the embedding of political entities discussed in the text. The second one captures external social context. Following the intuition that different social groups would engage with documents expressing"
2021.findings-acl.401,S15-1015,0,0.0606228,"Missing"
2021.findings-acl.401,D19-1664,0,0.0616571,"references to known ideological talking points (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” vs. “prochoice”). However, bias in news media is often nuanced and very difficult to detect. Journalists often strive to appear impartial and use language that does not reveal their opinions directly. Also, by their nature, news articles describing the same realworld event will share many similar details of the event, regardless of their political perspectives. Instead, bias is often expressed through informational choices (Fan et al., 2019), which highlight different aspects of the news story and frame facts shared by all articles in different ways. For example, the following articles capture different perspectives (Top left, Bottom right), while discussing the same news event– the 2021 storming of the U.S. Capitol 1 . Understanding the political perspective shaping the way events are discussed in the media is increasingly important due to the dramatic change in news distribution. With the advance in text classification models, the performance of political perspective detection is also improving rapidly. However, current deep le"
2021.findings-acl.401,D18-1393,0,0.101199,"contexts. Our results demonstrate the importance of all aspects, each contributing to the model’s performance. 2 https://en.wikipedia.org/wiki/Antifa_ (United_States) 3 https://pan.webis.de/semeval19/ semeval19-web/ 2 Related Work The problem of perspective identification is originally studied as a text classification task (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014), in which a classifier is trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by (Fan et al., 2019) aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested3 . Many works attempt to solve this problem with deep learning models (Jiang et al., 2019; Hanawa et al., 2019). We build on these works to help shape our text representation approach. Several recent works also started to make use of concepts or entities appearing in the text to get a better re"
2021.findings-acl.401,N15-1171,0,0.0212549,"owledge from various contexts. Our results demonstrate the importance of all aspects, each contributing to the model’s performance. 2 https://en.wikipedia.org/wiki/Antifa_ (United_States) 3 https://pan.webis.de/semeval19/ semeval19-web/ 2 Related Work The problem of perspective identification is originally studied as a text classification task (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014), in which a classifier is trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by (Fan et al., 2019) aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested3 . Many works attempt to solve this problem with deep learning models (Jiang et al., 2019; Hanawa et al., 2019). We build on these works to help shape our text representation approach. Several recent works also started to make use of concepts or entities appearing in the te"
2021.findings-acl.401,N09-1057,0,0.0184797,"artisan news detection (Kiesel et al., 2019). We compared our approach to several competitive text classification models and conducted a careful ablation study designed to evaluate the individual contribution of pre-training through knowledge from various contexts. Our results demonstrate the importance of all aspects, each contributing to the model’s performance. 2 https://en.wikipedia.org/wiki/Antifa_ (United_States) 3 https://pan.webis.de/semeval19/ semeval19-web/ 2 Related Work The problem of perspective identification is originally studied as a text classification task (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014), in which a classifier is trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by (Fan et al., 2019) aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested3 . Many works attempt to solve this problem"
2021.findings-acl.401,S19-2185,0,0.0163448,"trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by (Fan et al., 2019) aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested3 . Many works attempt to solve this problem with deep learning models (Jiang et al., 2019; Hanawa et al., 2019). We build on these works to help shape our text representation approach. Several recent works also started to make use of concepts or entities appearing in the text to get a better representation. Wang et al. (2017) treats the extracted concepts as pseudo words and appends them to the original word sequence which is then fed to a CNN. The KCNN model by Wang et al. (2018), used for news recommendation, concatenates entity embeddings with the respective word embeddings at each word position to enhance the input. We take a different approach and instead try to inject knowledge of entities into t"
2021.findings-acl.401,P14-1105,0,0.0258484,"iesel et al., 2019). We compared our approach to several competitive text classification models and conducted a careful ablation study designed to evaluate the individual contribution of pre-training through knowledge from various contexts. Our results demonstrate the importance of all aspects, each contributing to the model’s performance. 2 https://en.wikipedia.org/wiki/Antifa_ (United_States) 3 https://pan.webis.de/semeval19/ semeval19-web/ 2 Related Work The problem of perspective identification is originally studied as a text classification task (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014), in which a classifier is trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by (Fan et al., 2019) aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested3 . Many works attempt to solve this problem with deep learning m"
2021.findings-acl.401,W16-5609,1,0.754471,"ollowing the intuition that different social groups would engage with documents expressing a different bias (e.g., left-leaning users are more likely to read the NYTimes article compared to the Fox News article), we collect social information contextualizing news articles and learn to predict the social context of each article, based on its content, thus aligning the two representations. Finally, the third is based on linguistic knowledge, focusing on the issue framing decisions made by the authors. Framing decisions have been repeatedly shown to capture political bias (Recasens et al., 2013; Johnson and Goldwasser, 2016; Roy and Goldwasser, 2020; Mendelsohn et al., 2021), and we argue that infusing a language model with this information can help capture relevant information. Note that this information is only used for pre-training. Other works using social information to analyze political bias (Li and Goldwasser, 2019; Nguyen et al., 2020; Pacheco and Goldwasser, 2021) augment the text with social information, however since this information can be difficult to obtain in real-time, we decided to investigate if it can be used as a distant supervision source for pre-training a language model. These pre-training"
2021.findings-acl.401,S19-2145,0,0.0199769,"Missing"
2021.findings-acl.401,2021.naacl-main.179,0,0.0219549,"d engage with documents expressing a different bias (e.g., left-leaning users are more likely to read the NYTimes article compared to the Fox News article), we collect social information contextualizing news articles and learn to predict the social context of each article, based on its content, thus aligning the two representations. Finally, the third is based on linguistic knowledge, focusing on the issue framing decisions made by the authors. Framing decisions have been repeatedly shown to capture political bias (Recasens et al., 2013; Johnson and Goldwasser, 2016; Roy and Goldwasser, 2020; Mendelsohn et al., 2021), and we argue that infusing a language model with this information can help capture relevant information. Note that this information is only used for pre-training. Other works using social information to analyze political bias (Li and Goldwasser, 2019; Nguyen et al., 2020; Pacheco and Goldwasser, 2021) augment the text with social information, however since this information can be difficult to obtain in real-time, we decided to investigate if it can be used as a distant supervision source for pre-training a language model. These pre-training tasks are then used for training a Multi-head Atten"
2021.findings-acl.401,2021.tacl-1.7,1,0.701836,"s aligning the two representations. Finally, the third is based on linguistic knowledge, focusing on the issue framing decisions made by the authors. Framing decisions have been repeatedly shown to capture political bias (Recasens et al., 2013; Johnson and Goldwasser, 2016; Roy and Goldwasser, 2020; Mendelsohn et al., 2021), and we argue that infusing a language model with this information can help capture relevant information. Note that this information is only used for pre-training. Other works using social information to analyze political bias (Li and Goldwasser, 2019; Nguyen et al., 2020; Pacheco and Goldwasser, 2021) augment the text with social information, however since this information can be difficult to obtain in real-time, we decided to investigate if it can be used as a distant supervision source for pre-training a language model. These pre-training tasks are then used for training a Multi-head Attention Network (MAN) which creates a bias-aware representation of the text. We conducted our experiments over two datasets, Allsides (Li and Goldwasser, 2019) and SemEval Hyperpartisan news detection (Kiesel et al., 2019). We compared our approach to several competitive text classification models and cond"
2021.findings-acl.401,D14-1162,0,0.0840082,"Missing"
2021.findings-acl.401,N18-1202,0,0.053014,"training. However, they predict the tokens for the masked 4570 entity instead of relying on meaningful representations for entities as ours. Political framing, due to its relation with ideology and perspective, is studied in the NLP communities (Johnson et al., 2017; Field et al., 2018; Shurafa et al., 2020). There is also growing interest in utilizing framing differences to identify bias in news articles (Roy and Goldwasser, 2020). Pre-trained models are widely used in numerous NLP tasks, from the early word2vec representation (Mikolov et al., 2013) to the generic language models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, people also started to work on task specific pre-training that try to bring task and domain related knowledge into the model. Xu et al. (2019) is similar to our work as it proposes to enhance the BERT model through training on review data and sentiment classification tasks so that it can obtain better performance across multiple review-based tasks. 3 ?? … ?? ??? ??? ??? sharing user Sentence Attention … ?? ?? ?? Sentence Encoder … ?? ?? ?? entity frame ?? ?? Word Attention … ??? ??? ??? Word Encoder … Political Perspective Identification Task The prob"
2021.findings-acl.401,P13-1162,0,0.22547,"ernal social context. Following the intuition that different social groups would engage with documents expressing a different bias (e.g., left-leaning users are more likely to read the NYTimes article compared to the Fox News article), we collect social information contextualizing news articles and learn to predict the social context of each article, based on its content, thus aligning the two representations. Finally, the third is based on linguistic knowledge, focusing on the issue framing decisions made by the authors. Framing decisions have been repeatedly shown to capture political bias (Recasens et al., 2013; Johnson and Goldwasser, 2016; Roy and Goldwasser, 2020; Mendelsohn et al., 2021), and we argue that infusing a language model with this information can help capture relevant information. Note that this information is only used for pre-training. Other works using social information to analyze political bias (Li and Goldwasser, 2019; Nguyen et al., 2020; Pacheco and Goldwasser, 2021) augment the text with social information, however since this information can be difficult to obtain in real-time, we decided to investigate if it can be used as a distant supervision source for pre-training a lang"
2021.findings-acl.401,2020.emnlp-main.620,1,0.876521,"fferent social groups would engage with documents expressing a different bias (e.g., left-leaning users are more likely to read the NYTimes article compared to the Fox News article), we collect social information contextualizing news articles and learn to predict the social context of each article, based on its content, thus aligning the two representations. Finally, the third is based on linguistic knowledge, focusing on the issue framing decisions made by the authors. Framing decisions have been repeatedly shown to capture political bias (Recasens et al., 2013; Johnson and Goldwasser, 2016; Roy and Goldwasser, 2020; Mendelsohn et al., 2021), and we argue that infusing a language model with this information can help capture relevant information. Note that this information is only used for pre-training. Other works using social information to analyze political bias (Li and Goldwasser, 2019; Nguyen et al., 2020; Pacheco and Goldwasser, 2021) augment the text with social information, however since this information can be difficult to obtain in real-time, we decided to investigate if it can be used as a distant supervision source for pre-training a language model. These pre-training tasks are then used for t"
2021.findings-acl.401,N19-1242,0,0.0134732,"ology and perspective, is studied in the NLP communities (Johnson et al., 2017; Field et al., 2018; Shurafa et al., 2020). There is also growing interest in utilizing framing differences to identify bias in news articles (Roy and Goldwasser, 2020). Pre-trained models are widely used in numerous NLP tasks, from the early word2vec representation (Mikolov et al., 2013) to the generic language models like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019). Recently, people also started to work on task specific pre-training that try to bring task and domain related knowledge into the model. Xu et al. (2019) is similar to our work as it proposes to enhance the BERT model through training on review data and sentiment classification tasks so that it can obtain better performance across multiple review-based tasks. 3 ?? … ?? ??? ??? ??? sharing user Sentence Attention … ?? ?? ?? Sentence Encoder … ?? ?? ?? entity frame ?? ?? Word Attention … ??? ??? ??? Word Encoder … Political Perspective Identification Task The problem of political perspective identification in news media can be formalised as follows. Given a news article d, where d consists of sentences si , i ∈ [1, L], and each sentence si consi"
2021.findings-acl.401,N16-1174,0,0.0218108,"ot only the context within the text but also the knowledge about the entities (e.g. their political affiliation, or stance on controversial issues), sharing users, and frame indicators. We explain the structure of our model and the rich social and linguistic context we consider in detail below. Note that our pre-training strategies are not tied with any specific model structure and can be easily applied to other text models. Document Vector ??? ??? ??? Figure 1: Overall Architecture of MAN Model. 3.1 Multi-Head Attention Network The basic component of our model is the Hierarchical LSTM model (Yang et al., 2016). The goal of our model is to learn document representation vd for political perspective prediction. It consists of several parts: a word sequence encoder, a wordlevel attention layer, a sentence sequence encoder, and a sentence-level attention layer. To capture the context in both directions, we use bidirectional LSTM in this work. For each element in the input sequence, the hidden state h is a concatenation of → − the forward hidden state h and backward hidden ← − state h computed by the respective LSTM cells. Given a sentence with words wit , t ∈ [1, T ], each word is first converted to its"
2021.findings-acl.401,P19-1139,0,0.0116747,"epresentation approach. Several recent works also started to make use of concepts or entities appearing in the text to get a better representation. Wang et al. (2017) treats the extracted concepts as pseudo words and appends them to the original word sequence which is then fed to a CNN. The KCNN model by Wang et al. (2018), used for news recommendation, concatenates entity embeddings with the respective word embeddings at each word position to enhance the input. We take a different approach and instead try to inject knowledge of entities into the text model through the masked entity training. Zhang et al. (2019) also uses entity-level masking for training. However, they predict the tokens for the masked 4570 entity instead of relying on meaningful representations for entities as ours. Political framing, due to its relation with ideology and perspective, is studied in the NLP communities (Johnson et al., 2017; Field et al., 2018; Shurafa et al., 2020). There is also growing interest in utilizing framing differences to identify bias in news articles (Roy and Goldwasser, 2020). Pre-trained models are widely used in numerous NLP tasks, from the early word2vec representation (Mikolov et al., 2013) to the"
2021.internlp-1.7,D18-1389,0,0.015008,"gate this content in their social circles. Social networks provide easy means to distribute news and commentary, resulting in a sharp increase in the number of media outlets (Ribeiro et al., 2018), and a rapid spread of content. In particular, false news stories tend to spread at lightning speeds, and due to the volume, cannot be checked manually. An alternative to fact-checking claims, which is arguably easier to scale, is to focus on their source, and ask who can you trust? Prior works have formulated this as a traditional classification problem using techniques such as feature-based SVM’s (Baly et al., 2018, 2020), and more recently Graph Neural Networks (GNNs) 46 Proceedings of the First Workshop on Interactive Learning for Natural Language Processing , pages 46–53 August 5, 2021. ©2021 Association for Computational Linguistics 2 2.1 Model Graph Creation and Training We start with defining our social context information graph. It consists of sources (S), articles they publish (A), and Twitter users that interact with sources/articles (U ). Our goal is fake news source factuality classification. Each node in the graph is represented by a high dimensional feature vector, (similar to prior work (B"
2021.internlp-1.7,P19-1247,1,0.879348,"Missing"
2021.internlp-1.7,2021.ccl-1.108,0,0.0479819,"Missing"
2021.internlp-1.7,N19-1195,1,0.819787,"stronger representation of the complex information landscape on social media that enables fake news to spread, allowing it to be better detected. For this reason, we adopt graphs as our automated framework (1). Despite the success of these works, fake news detection is still a challenging research problem and human performance is significantly higher than fully automated systems (Shaar et al., 2020). Clearly, having humans fact check every information source is not scalable. Thus, our goal in this paper is to explore a different form of interaction with humans, where they can provide advice (Mehta and Goldwasser, 2019) to the automated system. Advice corresponds to localized judgements (provided through natural language) that help characterize the content and social interactions associated with sources. These judgements, associated with article and social media user nodes, are then propagated through the information graph using the GNN, allowing the system to take advantage of it to improve it’s representation. As advice is not providing source labels directly, which is a timeconsuming process requiring a global view of the source’s interactions, it is scalable. For example, one challenging aspect of the pr"
2021.naacl-main.391,P17-1067,0,0.0168938,"Providing the ap- et al., 2018; Devlin et al., 2019), we suggest learnpropriate narrative representation for making such ing an entity-based representation which captures inferences is therefore a key component. In this the narrative it is a part of. For example, in “She depaper, we suggest a novel narrative representation cided to try to make baked apples for the first time” model and evaluate it on two narrative understand- the mental state of “she” would be represented difing tasks, analyzing the characters’ mental states ferently given a different context, such as a differand motivations (Abdul-Mageed and Ungar, 2017; ent motivation for the action (“Her mother asked Rashkin et al., 2018; Chen et al., 2020), and desire her to make an apple dish for a dinner party”). In fulfillment (Chaturvedi et al., 2016; Rahimtoroghi this case, the contextualized representation would et al., 2017). capture the different emotion associated with it We follow the observation that narrative under- (e.g., F EAR of disappointing her mother). Unlike standing requires an expressive representation cap- contextualized word embeddings, entity-based conturing the context in which events appear and the textualization needs to conside"
2021.naacl-main.391,D14-1165,0,0.060945,"Missing"
2021.naacl-main.391,2020.conll-1.43,0,0.0185166,"ion for making such ing an entity-based representation which captures inferences is therefore a key component. In this the narrative it is a part of. For example, in “She depaper, we suggest a novel narrative representation cided to try to make baked apples for the first time” model and evaluate it on two narrative understand- the mental state of “she” would be represented difing tasks, analyzing the characters’ mental states ferently given a different context, such as a differand motivations (Abdul-Mageed and Ungar, 2017; ent motivation for the action (“Her mother asked Rashkin et al., 2018; Chen et al., 2020), and desire her to make an apple dish for a dinner party”). In fulfillment (Chaturvedi et al., 2016; Rahimtoroghi this case, the contextualized representation would et al., 2017). capture the different emotion associated with it We follow the observation that narrative under- (e.g., F EAR of disappointing her mother). Unlike standing requires an expressive representation cap- contextualized word embeddings, entity-based conturing the context in which events appear and the textualization needs to consider, at least, two levels interactions between characters’ states. To clarify, of context: lo"
2021.naacl-main.391,N19-1423,0,0.60933,"ed in it. These elements can help explain intentional behavior and capture causal con- be invoked by the motivation can help ensure the consistency of this analysis and improve its quality. nections between the characters’ actions and their goals. While this is straightforward for humans, To meet this challenge, we suggest a graphmachine readers often struggle as a correct anal- contextualized representation for entity states. Simysis relies on making long range common-sense ilar to contextualized word representations (Peters inferences over the narrative text. Providing the ap- et al., 2018; Devlin et al., 2019), we suggest learnpropriate narrative representation for making such ing an entity-based representation which captures inferences is therefore a key component. In this the narrative it is a part of. For example, in “She depaper, we suggest a novel narrative representation cided to try to make baked apples for the first time” model and evaluate it on two narrative understand- the mental state of “she” would be represented difing tasks, analyzing the characters’ mental states ferently given a different context, such as a differand motivations (Abdul-Mageed and Ungar, 2017; ent motivation for the"
2021.naacl-main.391,elson-2012-dramabank,0,0.0324518,"roceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4916–4926 June 6–11, 2021. ©2021 Association for Computational Linguistics the context of event relationships can spread over a long narrative, exceeding maximum sequence length limitation in modern contextualized word embedding models such as BERT (Devlin et al., 2019). In this paper, we propose an Entity-based Narrative Graph (ENG) representation of the text. Unlike other graph-based narrative representations (Lehnert, 1981; Goyal et al., 2010; Elson, 2012) which require intensive human annotation, we design our models around low-cost supervision sources and shift the focus from symbolic graph representations of nuanced information to their learned embedding. In ENG, each node is associated with an entityevent pair, representing an entity mention that is involved in an event. Edges represent observed relations between entities or events. We adapt the definition of event relationships introduced in Lee et al. (2020) to our entity-event scenario. For entity relationships, the CNext relationship connects two coreferent entity nodes. For event relat"
2021.naacl-main.391,2020.acl-main.426,0,0.244651,"heir dataset, each character mention is annotated with three types of mental state descriptors: Maslow’s “hierarchy of needs” (Maslow, 1943), Reiss’ “basic motives” (Reiss, 2004), that provide a more informative range of motivations, and Plutchik’s “wheel of emotions” (Plutchik, 1980), comprised of eight basic emotional dimensions (e.g. joy, sadness, etc). In their paper, they showed that neural models with explicit or latent entity representations achieve promising results on this task. Paul and Frank (2019) approached this task by extracting multi-hop relational paths from ConceptNet, while Gaonkar et al. (2020) leveraged semantics of the emotional states by embedding their textual description and modeling the co-relation between different entity states. Rahimtoroghi et al. (2017) introduced a dataset for the task of desire fulfillment. They identified desire expressions in firstperson narratives and annotated their fulfillment status. They showed that models that capture the flow of the narrative perform well on this task. Representing the narrative flow of stories using graph structures and multi-relational embeddings has been studied in the context of script learning (Li et al., 2018; Lee and Gold"
2021.naacl-main.391,D10-1008,0,0.102259,"Missing"
2021.naacl-main.391,2020.acl-main.740,0,0.0481354,"Missing"
2021.naacl-main.391,D17-1195,0,0.0189216,"evaluated downstream tasks include two challenging narrative analysis tasks, predicting characters’ psychological states (Rashkin et al., 2018) and desire fulfilment (Rahimtoroghi et al., 2017). Results show that our model can outperform competitive transformer-based representations of the narrative text, suggesting that explicitly modeling the relational structure of entities and events is beneficial. Our code and trained models are publicly available1 . 2 Related Work Tracking entities and modeling their properties has proven successful in a wide range of tasks, including language modeling (Ji et al., 2017), question answering (Henaff et al., 2017) and text generation (Bosselut et al., 2018). In an effort to model complex story dynamics in text, Rashkin et al. (2018) released a dataset for tracking the emotional reactions of characters in stories. In their dataset, each character mention is annotated with three types of mental state descriptors: Maslow’s “hierarchy of needs” (Maslow, 1943), Reiss’ “basic motives” (Reiss, 2004), that provide a more informative range of motivations, and Plutchik’s “wheel of emotions” (Plutchik, 1980), comprised of eight basic emotional dimensions (e.g. joy, sadnes"
2021.naacl-main.391,D14-1181,0,0.00877966,"Missing"
2021.naacl-main.391,P19-1413,1,0.849212,"t al. (2020) leveraged semantics of the emotional states by embedding their textual description and modeling the co-relation between different entity states. Rahimtoroghi et al. (2017) introduced a dataset for the task of desire fulfillment. They identified desire expressions in firstperson narratives and annotated their fulfillment status. They showed that models that capture the flow of the narrative perform well on this task. Representing the narrative flow of stories using graph structures and multi-relational embeddings has been studied in the context of script learning (Li et al., 2018; Lee and Goldwasser, 2019; Lee et al., To further enhance our model, we investigate three possible pre-training paradigms: whole-wordmasking, node prediction, and link prediction. All of them are constructed by automatically extracting noisy supervision and pre-training on a largescale corpus. We show that choosing the right pre-training strategy can lead to significant performance enhancements in downstream tasks. For example, automatically extracting sentiment for en1 tities can impact downstream emotion predictions. https://github.com/doug919/entity_ Finally, we explore the use of a symbolic inference based_narrati"
2021.naacl-main.391,2020.findings-emnlp.446,1,0.895231,"Narrative Graph (ENG) representation of the text. Unlike other graph-based narrative representations (Lehnert, 1981; Goyal et al., 2010; Elson, 2012) which require intensive human annotation, we design our models around low-cost supervision sources and shift the focus from symbolic graph representations of nuanced information to their learned embedding. In ENG, each node is associated with an entityevent pair, representing an entity mention that is involved in an event. Edges represent observed relations between entities or events. We adapt the definition of event relationships introduced in Lee et al. (2020) to our entity-event scenario. For entity relationships, the CNext relationship connects two coreferent entity nodes. For event relationships, the Next relationship captures the sequential order of events as they appear in the text, and six discourse relation types from the Penn Discourse Tree Bank (PDTB) (Prasad et al., 2007) are used. These include Before, After, Sync., Contrast, Reason and Result. Note that these are extracted in a weakly supervised manner, without expensive human annotations. To contextualize the entity embeddings over ENG, we apply a Relational Graph Convolution Network ("
2021.naacl-main.391,P18-1213,0,0.0358357,"Missing"
2021.naacl-main.391,P14-5010,0,0.012776,"ght relation types (|R |= 8) that have been shown to be useful for modeling narratives. N EXT denotes if two nodes appear in neighboring sentences. CN EXT expresses the next occurrence of a specific entity following its co-reference chain. Six discourse relation types, used by Lee et al. (2020) and defined in Penn Discourse Tree Bank (PDTB) (Prasad et al., 2007), are also used in this work, including B E FORE, A FTER , S YNC ., C ONTRAST, R EASON , R E SULT . Their corresponding definition in PDTB and can be found in Table 1. Following Lee et al. (2020), we use the Stanford CoreNLP pipeline3 (Manning et al., 2014) to obtain co-reference links and dependency trees. We use them as heuristics to extract the above relations and identify entities for TAPT4 . Details of this procedure can be found in (Lee et al., 2020). Note that although we share the same relation definitions, our nodes are defined over entities, instead of events. For encoding the graph, we use a Relational Graph Convolution Network (RGCN) (Schlichtkrull et al., 2018), which is designed for Knowledge Base Completion. This 2 Note that all candidate labels are appended to every example, without denoting which one is the right answer. Our pre"
2021.naacl-main.391,N16-1098,0,0.0251801,"= P ; k zk hd = X i αi hi (5) 3.5 Task-Adaptive Pre-training Recent studies demonstrate that downstream tasks performance can be improved by performing selfsupervised pre-training on the text of the target domain (Gururangan et al., 2020), called Task-Adaptive Pre-Training (TAPT). To investigate whether different TAPT objectives can provide different insights for downstream tasks, we apply three possible pre-training paradigms and compare them on StoryCommonsense. We focus on StoryCommonsense given that the dataset was created by annotating characters’ mental states on a subset of RocStories (Mostafazadeh et al., 2016), a corpus with 90K short common-sense stories. This provides us with a large unlabeled resource for investigating different pre-training methods. We run TAPT on all the RocStories text6 . We use the learning parameters suggested by Gururangan et al. (2020) and explore the following strategies: Whole-Word Masking: Randomly masks a subset of words and asks the model to recover them from their context (Radford et al., 2019; Liu et al., 2019). We perform this task over RoBERTa, initialized with roberta-base. ENG Link Prediction: Weakly-supervised TAPT over the ENG. The setup follows Sec. 3.4 (Lin"
2021.naacl-main.391,N19-1368,0,0.0804227,"ashkin et al. (2018) released a dataset for tracking the emotional reactions of characters in stories. In their dataset, each character mention is annotated with three types of mental state descriptors: Maslow’s “hierarchy of needs” (Maslow, 1943), Reiss’ “basic motives” (Reiss, 2004), that provide a more informative range of motivations, and Plutchik’s “wheel of emotions” (Plutchik, 1980), comprised of eight basic emotional dimensions (e.g. joy, sadness, etc). In their paper, they showed that neural models with explicit or latent entity representations achieve promising results on this task. Paul and Frank (2019) approached this task by extracting multi-hop relational paths from ConceptNet, while Gaonkar et al. (2020) leveraged semantics of the emotional states by embedding their textual description and modeling the co-relation between different entity states. Rahimtoroghi et al. (2017) introduced a dataset for the task of desire fulfillment. They identified desire expressions in firstperson narratives and annotated their fulfillment status. They showed that models that capture the flow of the narrative perform well on this task. Representing the narrative flow of stories using graph structures and mu"
2021.naacl-main.391,N18-1202,0,0.042975,"Missing"
2021.naacl-main.391,W17-5543,0,0.340243,"e representations by considering the graph structure through graph convolutions and learn a composition function. This architecture allows us to take into account the narrative structure and the different discourse relations connecting the entity-event nodes. layer to model relationships in the output space, and show that we can obtain additional gains in the downstream tasks that have strong correlation in the output space. The evaluated downstream tasks include two challenging narrative analysis tasks, predicting characters’ psychological states (Rashkin et al., 2018) and desire fulfilment (Rahimtoroghi et al., 2017). Results show that our model can outperform competitive transformer-based representations of the narrative text, suggesting that explicitly modeling the relational structure of entities and events is beneficial. Our code and trained models are publicly available1 . 2 Related Work Tracking entities and modeling their properties has proven successful in a wide range of tasks, including language modeling (Ji et al., 2017), question answering (Henaff et al., 2017) and text generation (Bosselut et al., 2018). In an effort to model complex story dynamics in text, Rashkin et al. (2018) released a da"
2021.nlp4if-1.10,D18-1393,0,0.0892983,"these perspectives can help ensure that all point of view are represented by news aggregation services, and help avoid “information echo-chambers” in which only a single view point is represented. Past work studying expression of bias in text has focused on lexical and syntactic representations of bias (Greene and Resnik, 2009; Recasens et al., 2013; Elfardy et al., 2015). Expressions of bias can include the use of the passive voice (e.g., “mistakes were made”), or references to known ideological talking points and framing decisions (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” 66 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 66–75 June 6, 2021. ©2021 Association for Computational Linguistics text representation approach. Several recent works also started to make use of concepts or entities appearing in text to get a better representation. Wang et al., 2017 treats the extracted concepts as pseudo words and appending them to the original word sequence which is then fed to a CNN. The KCNN (Wang et al., 2018) model, used for news recommendation, concatenates entity embeddings with the respective wo"
2021.nlp4if-1.10,N15-1171,0,0.136412,"n the highly polarized coverage of news events, recognizing these perspectives can help ensure that all point of view are represented by news aggregation services, and help avoid “information echo-chambers” in which only a single view point is represented. Past work studying expression of bias in text has focused on lexical and syntactic representations of bias (Greene and Resnik, 2009; Recasens et al., 2013; Elfardy et al., 2015). Expressions of bias can include the use of the passive voice (e.g., “mistakes were made”), or references to known ideological talking points and framing decisions (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” 66 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 66–75 June 6, 2021. ©2021 Association for Computational Linguistics text representation approach. Several recent works also started to make use of concepts or entities appearing in text to get a better representation. Wang et al., 2017 treats the extracted concepts as pseudo words and appending them to the original word sequence which is then fed to a CNN. The KCNN (Wang et al., 2018) model, used for news recommenda"
2021.nlp4if-1.10,D16-1148,0,0.0180634,"vents, recognizing these perspectives can help ensure that all point of view are represented by news aggregation services, and help avoid “information echo-chambers” in which only a single view point is represented. Past work studying expression of bias in text has focused on lexical and syntactic representations of bias (Greene and Resnik, 2009; Recasens et al., 2013; Elfardy et al., 2015). Expressions of bias can include the use of the passive voice (e.g., “mistakes were made”), or references to known ideological talking points and framing decisions (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” 66 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 66–75 June 6, 2021. ©2021 Association for Computational Linguistics text representation approach. Several recent works also started to make use of concepts or entities appearing in text to get a better representation. Wang et al., 2017 treats the extracted concepts as pseudo words and appending them to the original word sequence which is then fed to a CNN. The KCNN (Wang et al., 2018) model, used for news recommendation, concatenates entity embeddings wi"
2021.nlp4if-1.10,D16-1171,0,0.0121501,"ve word embeddings at each word position to enhance the input. We take a different approach, and instead learn a document representation with respect to each entity in the article. Using auxiliary information to improve text model was studied recently. Tang et al. proposes user-word composition vector model that modifies word embeddings given author representations in order to capture user-specific modification to word meanings. Other works incorporate user and product information to compute attentions over different semantic levels in the context of sentiment classification of online review (Chen et al., 2016; Wu et al., 2018). In this work, we learn the entity embedding based on external knowledge source (i.e. Wikipedia) or text, instead of including them in the training of bias prediction task. Therefore, we are able to capture rich knowledge about entities from various sources. Another series of work that is closely related to ours is aspect based sentiment analysis. It aims at determining the sentiment polarity of a text span in a specific aspect or toward a target in the text. Many neural network based approaches have been proposed (Wang et al., 2016; Chen et al., 2017; Fan et al., 2018) to i"
2021.nlp4if-1.10,N09-1057,0,0.0860036,"identifying the right leaning perspective of the article. The perspectives underlying the way information is conveyed to readers can prime them to take similar stances and shape their world view (Gentzkow and Shapiro, 2010, 2011). Given the highly polarized coverage of news events, recognizing these perspectives can help ensure that all point of view are represented by news aggregation services, and help avoid “information echo-chambers” in which only a single view point is represented. Past work studying expression of bias in text has focused on lexical and syntactic representations of bias (Greene and Resnik, 2009; Recasens et al., 2013; Elfardy et al., 2015). Expressions of bias can include the use of the passive voice (e.g., “mistakes were made”), or references to known ideological talking points and framing decisions (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” 66 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 66–75 June 6, 2021. ©2021 Association for Computational Linguistics text representation approach. Several recent works also started to make use of concepts or entities appearing in text to"
2021.nlp4if-1.10,S19-2185,0,0.011511,"s trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by Fan et al., 2019 aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested1 . Many works attempt to solve this problem with deep learning models (Jiang et al., 2019; Hanawa et al., 2019). We build on these works to help shape our 1 3 Model The problem of political perspective detection in news media can be formalised as follows. Given a news article d, where d consists of sentences si , i ∈ [1, L], and each sentence si consists of words wit , t ∈ [1, T ]. L and T are the number of sentences in d and number of words in si respectively. The goal of this task is to predict the political perspective https://pan.webis.de/semeval19/semeval19-web/ 67 y of the document. Given different datasets, this can either be a binary classification task, where y ∈ {0, 1} (hyperpartisan or not),"
2021.nlp4if-1.10,D17-1047,0,0.0218768,"ation of online review (Chen et al., 2016; Wu et al., 2018). In this work, we learn the entity embedding based on external knowledge source (i.e. Wikipedia) or text, instead of including them in the training of bias prediction task. Therefore, we are able to capture rich knowledge about entities from various sources. Another series of work that is closely related to ours is aspect based sentiment analysis. It aims at determining the sentiment polarity of a text span in a specific aspect or toward a target in the text. Many neural network based approaches have been proposed (Wang et al., 2016; Chen et al., 2017; Fan et al., 2018) to incorporate the aspect term into the text model. Recently, several works (Zeng et al., 2019; Song et al., 2019) designed their model based on BERT (Devlin et al., 2019). Unlike these works, we are not trying to determine the sentiment toward each entity mentioned in text. Instead, we are interested in identifying the underlying political perspective through the angles of these entities. In this paper, we tackle this challenge and suggest an entity-centric approach to bias detection in news media. We follow the observation that expressions of bias often revolve around the"
2021.nlp4if-1.10,P14-1105,0,0.0270609,"partisan news detection (Kiesel et al., 2019). We compared our approach to several competitive text classification models, and conducted a careful ablation study designed to evaluate the individual contribution of representing world knowledge using entity embedding, and creating the entity-aware text representation using multi-head attention. Our results demonstrate the importance of both aspects, each contributing to the model’s performance. 2 Related Work The problem of perspective identification is originally studied as a text classification task (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014), in which a classifier is trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by Fan et al., 2019 aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested1 . Many works attempt to solve this problem with deep learning mod"
2021.nlp4if-1.10,N19-1423,0,0.054017,"hem in the training of bias prediction task. Therefore, we are able to capture rich knowledge about entities from various sources. Another series of work that is closely related to ours is aspect based sentiment analysis. It aims at determining the sentiment polarity of a text span in a specific aspect or toward a target in the text. Many neural network based approaches have been proposed (Wang et al., 2016; Chen et al., 2017; Fan et al., 2018) to incorporate the aspect term into the text model. Recently, several works (Zeng et al., 2019; Song et al., 2019) designed their model based on BERT (Devlin et al., 2019). Unlike these works, we are not trying to determine the sentiment toward each entity mentioned in text. Instead, we are interested in identifying the underlying political perspective through the angles of these entities. In this paper, we tackle this challenge and suggest an entity-centric approach to bias detection in news media. We follow the observation that expressions of bias often revolve around the main characters described in news stories, by associating them with different properties, highlighting their contribution to some events, while diminishing it, in others. To help account for"
2021.nlp4if-1.10,S15-1015,0,0.0270065,"article. The perspectives underlying the way information is conveyed to readers can prime them to take similar stances and shape their world view (Gentzkow and Shapiro, 2010, 2011). Given the highly polarized coverage of news events, recognizing these perspectives can help ensure that all point of view are represented by news aggregation services, and help avoid “information echo-chambers” in which only a single view point is represented. Past work studying expression of bias in text has focused on lexical and syntactic representations of bias (Greene and Resnik, 2009; Recasens et al., 2013; Elfardy et al., 2015). Expressions of bias can include the use of the passive voice (e.g., “mistakes were made”), or references to known ideological talking points and framing decisions (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” 66 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 66–75 June 6, 2021. ©2021 Association for Computational Linguistics text representation approach. Several recent works also started to make use of concepts or entities appearing in text to get a better representation. Wang et al., 201"
2021.nlp4if-1.10,S19-2145,0,0.0316523,"Missing"
2021.nlp4if-1.10,D18-1380,0,0.0255616,"Missing"
2021.nlp4if-1.10,D19-1664,0,0.017199,"and creating the entity-aware text representation using multi-head attention. Our results demonstrate the importance of both aspects, each contributing to the model’s performance. 2 Related Work The problem of perspective identification is originally studied as a text classification task (Lin et al., 2006; Greene and Resnik, 2009; Iyyer et al., 2014), in which a classifier is trained to differentiate between specific perspectives. Other works use linguistic indicators of bias and expressions of implicit sentiment (Recasens et al., 2013; Baumer et al., 2015; Field et al., 2018). Recent work by Fan et al., 2019 aims to characterize content relevant for bias detection. Unlike their work which relies on annotated spans of text, we aim to characterize this content without explicit supervision. In the recent SemEval-2019, a hyperpartisan news article detection task was suggested1 . Many works attempt to solve this problem with deep learning models (Jiang et al., 2019; Hanawa et al., 2019). We build on these works to help shape our 1 3 Model The problem of political perspective detection in news media can be formalised as follows. Given a news article d, where d consists of sentences si , i ∈ [1, L], and"
2021.nlp4if-1.10,N16-1174,0,0.047689,"egative entity candidates from all possible entities uniformly. The learned entity representations can directly capture the context in the news articles they appear in. 3.1.3 trained Glove (Pennington et al., 2014) word embeddings or deep contextualized word representation ELMo (Gardner et al., 2017) for this step. The word vectors are then fed into a word level bidirectional LSTM network to incorporate contextual information within the sentence. The hidden states hit from the bidirectional LSTM network, are passed to the next layer. Text Based Relation Representation Word Level Attention In (Yang et al., 2016), a self attention mechanism is introduced to identify words that are important to the meaning of the sentence, and therefore higher weights are given to them when forming the aggregated sentence vector. Actually the same words can also convey different meanings on distinct entities or relations. Following this intuition, we extend the idea by taking the aspect knowledge into account. Similarly, we learn representation for an entity pair to encode the relationship between them. Given a sentence with two entity mentions masked, our model tries to predict the pair of entities. Again, a bidirecti"
2021.nlp4if-1.10,D14-1162,0,0.0838968,"Missing"
2021.nlp4if-1.10,P13-1162,0,0.192364,"ning perspective of the article. The perspectives underlying the way information is conveyed to readers can prime them to take similar stances and shape their world view (Gentzkow and Shapiro, 2010, 2011). Given the highly polarized coverage of news events, recognizing these perspectives can help ensure that all point of view are represented by news aggregation services, and help avoid “information echo-chambers” in which only a single view point is represented. Past work studying expression of bias in text has focused on lexical and syntactic representations of bias (Greene and Resnik, 2009; Recasens et al., 2013; Elfardy et al., 2015). Expressions of bias can include the use of the passive voice (e.g., “mistakes were made”), or references to known ideological talking points and framing decisions (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018) (e.g., “pro-life” 66 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 66–75 June 6, 2021. ©2021 Association for Computational Linguistics text representation approach. Several recent works also started to make use of concepts or entities appearing in text to get a better represent"
2021.nlp4if-1.10,D16-1058,0,0.0124378,"sentiment classification of online review (Chen et al., 2016; Wu et al., 2018). In this work, we learn the entity embedding based on external knowledge source (i.e. Wikipedia) or text, instead of including them in the training of bias prediction task. Therefore, we are able to capture rich knowledge about entities from various sources. Another series of work that is closely related to ours is aspect based sentiment analysis. It aims at determining the sentiment polarity of a text span in a specific aspect or toward a target in the text. Many neural network based approaches have been proposed (Wang et al., 2016; Chen et al., 2017; Fan et al., 2018) to incorporate the aspect term into the text model. Recently, several works (Zeng et al., 2019; Song et al., 2019) designed their model based on BERT (Devlin et al., 2019). Unlike these works, we are not trying to determine the sentiment toward each entity mentioned in text. Instead, we are interested in identifying the underlying political perspective through the angles of these entities. In this paper, we tackle this challenge and suggest an entity-centric approach to bias detection in news media. We follow the observation that expressions of bias often"
2021.socialnlp-1.1,J90-1003,0,0.293092,"ation between entity mentions and moral foundation usage by different groups, which helps pave the way to analyze partisan sentiment towards entities using MFT. In that sense, our work is broadly related to entity-centric affective analysis (Deng and Wiebe, 2015; Field and Tsvetkov, 2019; Park et al., 2020). 3.2 Candidate Datasets Building Topic Indicator Lexicon To build a topic indicator lexicon, we take the dataset proposed by Johnson and Goldwasser (2018). We build topic indicator lexicons for each of the 6 topics comprised of n-grams (n≤5) using Pointwise Mutual Information (PMI) scores (Church and Hanks, 1990). For an n-gram, w we 2 calculate the pointwise mutual information (PMI) with topic t, I(w, t) using the following formula. I(w, t) = log a deep relational learning framework. In this section, we first describe the model we use for the supervised classification. Then, we describe our training procedure and analyze the performance of our model on a held out set. Finally, we describe the procedure to infer moral foundations in the collected dataset using our model. P (w|t) P (w) Where P (w|t) is computed by taking all tweets count(w) with topic t and computing count(allngrams) and similarly, P ("
2021.socialnlp-1.1,D19-1041,0,0.0332821,"Missing"
2021.socialnlp-1.1,P17-1091,0,0.0141971,"tive) to F (most liberal), while for the topic of Gun Control (Immigration), the proportion of Harm (Loyalty) tweets increases. Second, even when using the same moral foundation, their targets differ. For example, when discussing Gun Control, using the Loyalty moral foundation, liberal mostly mention march life, Gabby Gifford, while conservatives mention gun owner, Texas. We use a deep structured prediction approach (Pacheco and Goldwasser, 2021) to identify moral foundations in tweets by being motivated from the works that combine structured prediction with deep neural networks in NLP tasks (Niculae et al., 2017; Han et al., 2019; Liu et al., 2019; Widmoser et al., 2021). 2 To study the nuanced stances and sentiment towards entities of politicians using MFT on the text they use, ideally, we need a text dataset annotated for moral foundations from US politicians with known political bias. To the best of our knowledge there are two existing Twitter datasets that are annotated for moral foundations - (1) The Moral Foundations Twitter Corpus (MFTC) by Hoover et al. (2020), and (2) The tweets by US politicians by Johnson and Goldwasser (2018). In MFTC, the moral foundation annotation is done in 35k Tweets"
2021.socialnlp-1.1,D14-1162,0,0.092721,"a globally consistent decision (globally) using the structured hinge loss: max(∆(ˆy, y) + ˆ y∈Y X ψr ∈Ψ Φt (xr , y ˆr ; θt )) − X Experimental Evaluation (1) Lexicon matching with Moral Foundations Dictionary (MFD) This approach does not have a training phase. Rather we use the Moral Foundation Dictionary (Graham et al., 2009) and identify moral foundation in a tweet using unigram matching from the MFD. A tweet having no dictionary matching is labeled as ‘Non-moral’. Φt (xr , yr ; θt ) ψr ∈Ψ (2) Bidirectional-LSTM We run a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) over the Glove (Pennington et al., 2014) word embeddings of the words of the tweets. We concatenate the hidden states of the two opposite directional LSTMs to get representation over one timestamp and average the representations of all time stamps to get the final representation of a tweet. We map each tweet to a 128-d space using Bi-LSTM and use this representation for moral foundation classification using a fully connected output layer. We use the same folds as the DRaiL experiments. Here, t is rule template, Φt is the associated neural network, and θt is the parameter set. y and ˆy are gold assignments and predictions resulting f"
2021.socialnlp-1.1,W16-5609,1,0.757776,"t al., 2017; Hoover et al., 2020). Recent works have shown that political discourse can also be explained using MFT (Dehghani et al., 2014; Johnson and Goldwasser, 2018, 2019). Existing works explain the political discourse mostly at issue and sentence level (Fulgoni et al., 2016; Garten et al., 2016; Lin et al., 2018; Xie et al., 2019) and at left-right polar domains of politics. Several works have looked at analyzing political ideologies, beyond the left and right divide, using text (Sim et al., 2013; Preo¸tiuc-Pietro et al., 2017), and specifically using Twitter data (Conover et al., 2011; Johnson and Goldwasser, 2016; Mohammad et al., 2016; Demszky et al., 2019). To the best of our knowledge, this is the first work that studies whether MFT can be used to explain nuanced political standpoints of the US politicians, breaking the left/right political spectrum to nuanced standpoints. We also study the correlation between entity mentions and moral foundation usage by different groups, which helps pave the way to analyze partisan sentiment towards entities using MFT. In that sense, our work is broadly related to entity-centric affective analysis (Deng and Wiebe, 2015; Field and Tsvetkov, 2019; Park et al., 2020"
2021.socialnlp-1.1,P17-1068,0,0.0489422,"Missing"
2021.socialnlp-1.1,P18-1067,1,0.821644,"oning over the pros and cons of different alternatives, gathering support for these policies often relies on appealing to peoples’ “gut feeling” and invoking an emotional response (Haidt, 2001). Moral Foundation Theory (MFT) provides a theoretical framework for analyzing the use of moral sentiment in text. The theory (Haidt and Joseph, 2004; Haidt and Graham, 2007) suggests that there In this paper, we study the relationship between moral foundation usage by politicians on social media and the stances they take on two policy issues, Gun Control and Immigration. We use the dataset provided by (Johnson and Goldwasser, 2018) for training a model for automatically identifying moral foundations in tweets. We then apply the model to a collection of 74k and 87k congressional tweets discussing the two issues - Gun Control and Immigration, respectively. Our analysis goes beyond binary liberal-conservative ideological labels (Preo¸tiuc-Pietro et al., 2017). We use a scale of 5 letter grades assigned to politicians by 1 Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, pages 1–13 Online Workshop, June 10, 2021. ©2021 Association for Computational Linguistics relevant policy"
2021.socialnlp-1.1,2020.emnlp-main.620,1,0.816726,"Missing"
2021.socialnlp-1.1,W19-2112,1,0.914006,"hest PMI topic only. Then for each topic we manually go through the n-gram lexicon and omit any n-gram that is not related to the topic. In this manner, we found an indicator lexicon for each topic. The lexicons for the topics Gun Control and Immigration can be found in Appendix A. Note that, as a pre-processing step, n-grams were stemmed and singularized. 3.3 4.1 For the identification of moral foundation (MF) in tweets, Johnson and Goldwasser (2018) rely on linguistic cues such as - political slogans, Policy Frames, Annotator’s Rationale; along with party affiliation, topic and so on, while Johnson and Goldwasser (2019) models the behavioural aspects of the politicians in MF identification. In both of the works they use Probabilistic Soft Logic for modeling. Some of the features used by Johnson and Goldwasser (2018) and Johnson and Goldwasser (2019) are hard to get for a large corpus and some require human annotation. Note that, in this section, our goal is not to outperform the state-of-the-art MF classification results, rather we want to identify MFs in the large corpus where only limited information is available. So, to identify MFs in our corpus we mostly rely on text and the information available with t"
2021.socialnlp-1.1,D13-1010,0,0.0222837,"aining social behaviour of humans (Mooijman et al., 2018; Hoover et al., 2018; Dehghani et al., 2016; Brady et al., 2017; Hoover et al., 2020). Recent works have shown that political discourse can also be explained using MFT (Dehghani et al., 2014; Johnson and Goldwasser, 2018, 2019). Existing works explain the political discourse mostly at issue and sentence level (Fulgoni et al., 2016; Garten et al., 2016; Lin et al., 2018; Xie et al., 2019) and at left-right polar domains of politics. Several works have looked at analyzing political ideologies, beyond the left and right divide, using text (Sim et al., 2013; Preo¸tiuc-Pietro et al., 2017), and specifically using Twitter data (Conover et al., 2011; Johnson and Goldwasser, 2016; Mohammad et al., 2016; Demszky et al., 2019). To the best of our knowledge, this is the first work that studies whether MFT can be used to explain nuanced political standpoints of the US politicians, breaking the left/right political spectrum to nuanced standpoints. We also study the correlation between entity mentions and moral foundation usage by different groups, which helps pave the way to analyze partisan sentiment towards entities using MFT. In that sense, our work i"
2021.socialnlp-1.1,2021.eacl-main.100,1,0.698178,"rol (Immigration), the proportion of Harm (Loyalty) tweets increases. Second, even when using the same moral foundation, their targets differ. For example, when discussing Gun Control, using the Loyalty moral foundation, liberal mostly mention march life, Gabby Gifford, while conservatives mention gun owner, Texas. We use a deep structured prediction approach (Pacheco and Goldwasser, 2021) to identify moral foundations in tweets by being motivated from the works that combine structured prediction with deep neural networks in NLP tasks (Niculae et al., 2017; Han et al., 2019; Liu et al., 2019; Widmoser et al., 2021). 2 To study the nuanced stances and sentiment towards entities of politicians using MFT on the text they use, ideally, we need a text dataset annotated for moral foundations from US politicians with known political bias. To the best of our knowledge there are two existing Twitter datasets that are annotated for moral foundations - (1) The Moral Foundations Twitter Corpus (MFTC) by Hoover et al. (2020), and (2) The tweets by US politicians by Johnson and Goldwasser (2018). In MFTC, the moral foundation annotation is done in 35k Tweets on 7 distinct domains, some of which are not related to pol"
2021.socialnlp-1.1,D19-1472,0,0.0185666,"ing a lexicon matching approach. 3.1 Related Works The Moral Foundation Theory (MFT) (Haidt and Joseph, 2004; Haidt and Graham, 2007) has been proven to be useful in explaining social behaviour of humans (Mooijman et al., 2018; Hoover et al., 2018; Dehghani et al., 2016; Brady et al., 2017; Hoover et al., 2020). Recent works have shown that political discourse can also be explained using MFT (Dehghani et al., 2014; Johnson and Goldwasser, 2018, 2019). Existing works explain the political discourse mostly at issue and sentence level (Fulgoni et al., 2016; Garten et al., 2016; Lin et al., 2018; Xie et al., 2019) and at left-right polar domains of politics. Several works have looked at analyzing political ideologies, beyond the left and right divide, using text (Sim et al., 2013; Preo¸tiuc-Pietro et al., 2017), and specifically using Twitter data (Conover et al., 2011; Johnson and Goldwasser, 2016; Mohammad et al., 2016; Demszky et al., 2019). To the best of our knowledge, this is the first work that studies whether MFT can be used to explain nuanced political standpoints of the US politicians, breaking the left/right political spectrum to nuanced standpoints. We also study the correlation between ent"
2021.socialnlp-1.1,P19-1629,0,0.0196798,"topic of Gun Control (Immigration), the proportion of Harm (Loyalty) tweets increases. Second, even when using the same moral foundation, their targets differ. For example, when discussing Gun Control, using the Loyalty moral foundation, liberal mostly mention march life, Gabby Gifford, while conservatives mention gun owner, Texas. We use a deep structured prediction approach (Pacheco and Goldwasser, 2021) to identify moral foundations in tweets by being motivated from the works that combine structured prediction with deep neural networks in NLP tasks (Niculae et al., 2017; Han et al., 2019; Liu et al., 2019; Widmoser et al., 2021). 2 To study the nuanced stances and sentiment towards entities of politicians using MFT on the text they use, ideally, we need a text dataset annotated for moral foundations from US politicians with known political bias. To the best of our knowledge there are two existing Twitter datasets that are annotated for moral foundations - (1) The Moral Foundations Twitter Corpus (MFTC) by Hoover et al. (2020), and (2) The tweets by US politicians by Johnson and Goldwasser (2018). In MFTC, the moral foundation annotation is done in 35k Tweets on 7 distinct domains, some of whic"
2021.tacl-1.7,D15-1075,0,0.106576,"Missing"
2021.tacl-1.7,N19-1423,0,0.277482,"er, ideology). User data is considerably sparse. We create two evaluation scenarios, random and hard. In the random split, debates are randomly divided into ten folds of equal size. In the hard split, debates are separated by political issue. This results in a harder prediction problem, as the test data will not share topically related debates with the training data. We perform 10-fold cross validation and report accuracy. Entity and Relation Encoders: We represent posts and titles using a pre-trained BERT-small2 encoder (Turc et al., 2019), a compact version of the language model proposed by Devlin et al. 2019. For users, we use feed-forward computations with ReLU activations over the profile features and a pre-trained node embedding (Grover and Leskovec, 2016) over the friendship graph. All relation and rule encoders are represented as feed-forward networks with one hidden layer, ReLU activations and a softmax on top. Note that all of these modules are updated during learning. Table 2 (Left) shows results for all the models described in Section 5.1. In E2E models, post and user information is collapsed into a single module (rule), whereas in INDNETS, JOINTINF, GLOBAL and RELNETS they are modeled s"
2021.tacl-1.7,Q16-1023,0,0.0303062,"dy) is a conjunction of observed and predicted relations, and tRH (head) is the output relation to be learned. Consider the debate prediction task in Figure 1, it consists of several sub-tasks, involving textual inference (Agree(t1 , t2 )), social relations (VoteFor(u, v)) and their combination (Agree(u, t)). We illustrate how Deep structured models. More generally, deep structured prediction approaches have been successfully applied to various NLP tasks such as named entity recognition and dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Kiperwasser and Goldberg, 2016; Malaviya et al., 2018). When the need arises to go beyond sentencelevel, some works combine the output scores of independently trained classifiers using inference (Beltagy et al., 2014; ?; Liu et al., 2016; Subramanian et al., 2017; Ning et al., 2018), whereas others implement joint learning for their specific domains (Niculae et al., 2017; Han et al., 2019). Our main differentiating factor is that we provide a general interface that leverages first order logic clauses to specify factor graphs and express constraints. To summarize these differences, we outline a feature matrix in Table 1. Gi"
2021.tacl-1.7,P16-1101,0,0.0386109,"Horn clauses: tLH ⇒ tRH , where tLH (body) is a conjunction of observed and predicted relations, and tRH (head) is the output relation to be learned. Consider the debate prediction task in Figure 1, it consists of several sub-tasks, involving textual inference (Agree(t1 , t2 )), social relations (VoteFor(u, v)) and their combination (Agree(u, t)). We illustrate how Deep structured models. More generally, deep structured prediction approaches have been successfully applied to various NLP tasks such as named entity recognition and dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Kiperwasser and Goldberg, 2016; Malaviya et al., 2018). When the need arises to go beyond sentencelevel, some works combine the output scores of independently trained classifiers using inference (Beltagy et al., 2014; ?; Liu et al., 2016; Subramanian et al., 2017; Ning et al., 2018), whereas others implement joint learning for their specific domains (Niculae et al., 2017; Han et al., 2019). Our main differentiating factor is that we provide a general interface that leverages first order logic clauses to specify factor graphs and express constraints. To summarize these di"
2021.tacl-1.7,P19-1464,0,0.183876,"BiLSTMs are considerably faster than BERT, and inference is more expensive for this task. Table 7: Previous work on argument mining. trying to predict links correctly. For this task, we did not apply TensorLog, given that we couldn’t find a way to express tree constraints using their syntax. Once again, we see the advantage of using global learning, as well as sharing information between rules using RELNETS. Table 7 shows the performance of our model against previously published results. While we are able to outperform models that use the same underlying encoders and features, recent work by Kuribayashi et al. (2019) further improved performance by exploiting contextualized word embeddings that look at the whole document, and making a distinction between argumentative markers and argumentative components. We did not find a significant improvement by incorporating their ELMo-LSTM encoders into our framework,3 nor by replacing our BiLSTM encoders with BERT. We leave the exploration of an effective way to leverage contextualized embeddings for this task for future work. 5.7 Analysis of Loss Functions In this section we perform an evaluation of the CRF loss for issue-specific stance prediction. Note that one"
2021.tacl-1.7,P19-1413,1,0.80869,"is represented using an independent neural network, and E2E, where the features for the different components are concatenated at the input and fed to a single neural network. Relational Embedding Methods: Introduced in Section 2.2, these methods embed nodes and edge types for relational data. They are typically designed to represent symbolic entities and relations. However, because our entities can be defined by raw textual content and other features, we define the relational objectives over our encoders. This adaptation has proven successful for domains dealing with rich textual information (Lee and Goldwasser, 2019). We test three When inference is intractable, approximate ˆ. inference (e.g., AD3 ) can be used to obtain y To approximate the global normalization term Z (x) in the general CRF case, we follow Zhou et al. (2015); Andor et al. (2016) and keep a pool βk of k of high-quality feasible solutions during inference. This way, we can sum over the solutions in the partition function  P the pool Q to approximate t) . ′ ; θ exp Φ ( x , y t r y’∈βk ψr ∈Ψ r In this paper, we use the structured hinge loss for most experiments, and include a discussion on the approximated CRF loss in Section 5.7. Joint Inf"
2021.tacl-1.7,C18-1316,1,0.776087,"pond to each other’s posts. We treat the task as a collective classification problem, and model the agreement between posts and their replies, as well as the consistency between posts written by the 110 Figure 4: Statements made by politicians classified using our model trained on debate.org. same author. The DRAIL program for this task can be observed in Appendix A. Dataset: We use the 4Forums dataset from the Internet Argument Corpus (Walker et al., 2012), consisting of a total of 1,230 debates and 24,658 posts on abortion, evolution, gay marriage, and gun control. We use the same splits as Li et al. (2018) and perform 5-fold cross validation. Entity and Relation Encoders: We represented posts using pre-trained BERT encoders (Devlin et al., 2019) and do not generate features for authors. As in the previous task, we model all relations and rules using feed-forward networks with one hidden layer and ReLU activations. Note that we fine-tune all parameters during training. In Table 4 we can observe the general results for this task. We report macro F1 for post stance and agreement between posts for all issues. As in the previous task, we find that ComplEx and RotatE relational embeddings outperform"
2021.tacl-1.7,D14-1162,0,0.0901805,"Missing"
2021.tacl-1.7,P17-1091,0,0.372527,"More generally, deep structured prediction approaches have been successfully applied to various NLP tasks such as named entity recognition and dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Kiperwasser and Goldberg, 2016; Malaviya et al., 2018). When the need arises to go beyond sentencelevel, some works combine the output scores of independently trained classifiers using inference (Beltagy et al., 2014; ?; Liu et al., 2016; Subramanian et al., 2017; Ning et al., 2018), whereas others implement joint learning for their specific domains (Niculae et al., 2017; Han et al., 2019). Our main differentiating factor is that we provide a general interface that leverages first order logic clauses to specify factor graphs and express constraints. To summarize these differences, we outline a feature matrix in Table 1. Given our focus in NLP tasks, we require a neural-symbolic system that (1) allows us to integrate state-of-the-art text encoders and NLP tools, (2) supports structured prediction across long texts, (3) lets us combine several modalities and their representations (e.g., social and textual information), and (4) results in 103 we describe the neu"
2021.tacl-1.7,P15-1012,0,0.130736,"Missing"
2021.tacl-1.7,U17-1003,0,0.169046,"c to introduce distant supervision and labeling functions. Each rule is regarded as a latent variable, and the logic defines a joint probability distribution over all labeling decisions. Then, the rule weights and the network parameters are learned jointly using variational EM. In contrast, DRAIL focuses on learning multiple interdependent decisions from data, handling and requiring supervision for all unknown atoms in a given example. Lastly, Deep Logic Models (DLMs) (Marra et al., 2019) learn a set of parameters to encode atoms in a probabilistic logic program. Similarly to Donadello et al. (2017) Symbolic Features Neural Features Symbolic Raw Decla- Prob/Logic Rule Embed. End-to-end Backprop. to Architecture Multi-Task Open Inputs Inputs rative Inference Induction Symbols Neural Encoders Agnostic Learning Source MLN ✓ ✓ ✓ ✓ ✓ FACTORIE ✓ ✓ CCM ✓ ✓ ✓ ✓ ✓ ✓ PSL ✓ ✓ ✓ LRNNs ✓ ✓ ✓ RelNNs ✓ ✓ ✓ ✓ LTNs ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ TensorLog ✓ ✓ ✓ NTPs ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Neural LP ✓ ✓ ✓ ✓ ✓ DRUM ✓ ✓ NLMs ✓ ✓ ✓ ✓ ✓ DeepProbLog ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ DPL ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ DLMs ✓ ✓ ✓ ✓ DRAIL ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ System Table 1: Comparing systems. and Cohen et al. (2020), they use differentiable inference, a"
2021.tacl-1.7,P19-1055,1,0.848133,"ough local learning is faster, the learned scoring functions might not be consistent with the correct global prediction. Following (Han et al., 2019), we initialize the parameters using local models. RELNETS: We will show the advantage of having relational representations that are shared across different decisions, in contrast to having independent parameters for each rule. Note that in all cases, we will use the global learning objective to train RELNETS. Modularity: Decomposing decisions into relevant modules has been shown to simplify the learning process and lead to better generalization (Zhang and Goldwasser, 2019). We will contrast the performance of modular and end-to-end models to represent text and user information when predicting stances. Representation Learning and Interpretability: We will do a qualitative analysis to show how we are able to embed symbols and explain data by moving between symbolic and sub-symbolic representations, as outlined in Section 3.3. model from scratch. In this task, we would like to leverage the fact that stances in different domains are correlated. Instead of using a pre-defined set of debate topics (i.e., symbolic entities) we define the prediction task over claims, e"
2021.tacl-1.7,P15-1032,0,0.0724661,"plates, formatted as Horn clauses: tLH ⇒ tRH , where tLH (body) is a conjunction of observed and predicted relations, and tRH (head) is the output relation to be learned. Consider the debate prediction task in Figure 1, it consists of several sub-tasks, involving textual inference (Agree(t1 , t2 )), social relations (VoteFor(u, v)) and their combination (Agree(u, t)). We illustrate how Deep structured models. More generally, deep structured prediction approaches have been successfully applied to various NLP tasks such as named entity recognition and dependency parsing (Chen and Manning, 2014; Weiss et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Kiperwasser and Goldberg, 2016; Malaviya et al., 2018). When the need arises to go beyond sentencelevel, some works combine the output scores of independently trained classifiers using inference (Beltagy et al., 2014; ?; Liu et al., 2016; Subramanian et al., 2017; Ning et al., 2018), whereas others implement joint learning for their specific domains (Niculae et al., 2017; Han et al., 2019). Our main differentiating factor is that we provide a general interface that leverages first order logic clauses to specify factor graphs and express constraints. To"
C16-1279,W11-0702,0,0.117535,"our global modeling approach, which outperforms both the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and PSL collective classification (Bach et a"
C16-1279,P13-2144,0,0.0693432,"ferent frames for the issue. effectiveness of our global modeling approach, which outperforms both the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), an"
C16-1279,W13-1106,0,0.267746,"Missing"
C16-1279,D15-1008,0,0.268024,"ier inference time computational cost. In recent years there has been a growing interest in analyzing political discourse in both traditional 1 In our data set, there are few “@” mentions or retweet examples forming a conversation, thus we do not have access to argument or conversation structures for analysis. 2967 and social media. Several previous works have explored topic framing (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models"
C16-1279,N15-1171,0,0.100168,"pervision, to assign soft values (0 to 1 inclusive) to output variables, rather than Markov Logic Networks, which assign hard (0 or 1) values to model variables and incur heavier inference time computational cost. In recent years there has been a growing interest in analyzing political discourse in both traditional 1 In our data set, there are few “@” mentions or retweet examples forming a conversation, thus we do not have access to argument or conversation structures for analysis. 2967 and social media. Several previous works have explored topic framing (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivi"
C16-1279,W11-3702,0,0.436419,"cators, is indicative of a stance (e.g., also capturing when politicians fail to tweet about a topic). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015; Volkova et al., 2014; Conover et al., 2011). Other works have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), analyzing types of tweets and Twitter network effects around political events (Maireder and Ausserhofer, 2013), automatic polls based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as applications of distant supervision (Marchetti-Bowick and Chambers, 2012). 3 3.1 Data and Problem Setting Data Collection We collected tweets for 32 politicians, the 16 Republicans (all 2016 presidential candidates) and 16 Democrats (5 of which were candidates) listed in Table 1. Our initial goal was to compare politicians participating in the 2016 U.S. presidential election. To increase representation of Democrats, we collected tweets of Democrats who hold leadership roles within their party, because more well known politicians tend"
C16-1279,P15-2072,0,0.279051,"of gun control shown in Figure 1. To identify the stance taken by each politician, our model combines both content and behavioral features, accumulated from all of a politician’s tweets on that issue. First, the tweet’s relevance to an issue can be identified using issue indicators (highlighted in green). Second, the similarity between the stances taken by two of the politicians (agreement) can be identified by observing differences in how the issue is framed (shown in yellow), a tool often used by politicians to create bias toward a stance and contextualize the discussion (Tsur et al., 2015; Card et al., 2015). Tweets (1) and (3) frame the issue as a matter of safety, while tweet (2) frames it as related to personal freedom, thus revealing the agreement and disagreement patterns between the politicians. Third, we can consider the timing of these tweets, i.e. whether these tweets are posted continually or just around events concerning gun violence. Finally, we can also use sentiment indicators (e.g., the negative sentiment of tweet (1)). Notice that each feature individually might not contain sufficient information for correct classification, but combining all aspects, This work is licensed under a"
C16-1279,N13-1037,0,0.0186651,"itter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in microblogging platforms. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. In contrast to this task, as well as most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we investigate how a politician’s overall Twitter behavior, as represented by combined content and temporal indicators, is indicative of a stance (e.g., also capturing when politicians fail to tweet about a topic). Predicting political affiliation and"
C16-1279,N09-1057,0,0.0759102,"ng (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in microblogging platforms. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the st"
C16-1279,D14-1083,0,0.0524342,"the DOM refer to different frames for the issue. effectiveness of our global modeling approach, which outperforms both the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling"
C16-1279,P14-1105,0,0.151969,"ables and incur heavier inference time computational cost. In recent years there has been a growing interest in analyzing political discourse in both traditional 1 In our data set, there are few “@” mentions or retweet examples forming a conversation, thus we do not have access to argument or conversation structures for analysis. 2967 and social media. Several previous works have explored topic framing (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and w"
C16-1279,D14-1214,0,0.0475801,"2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in microblogging platforms. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. In contrast to this task, as well as most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we investigate how a politician’s overall Twitter behavior, as represented by combined content and temporal indicators, is indicati"
C16-1279,P14-1016,0,0.0788582,"2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in microblogging platforms. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. In contrast to this task, as well as most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we investigate how a politician’s overall Twitter behavior, as represented by combined content and temporal indicators, is indicati"
C16-1279,E12-1062,0,0.0207782,"ffiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015; Volkova et al., 2014; Conover et al., 2011). Other works have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), analyzing types of tweets and Twitter network effects around political events (Maireder and Ausserhofer, 2013), automatic polls based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as applications of distant supervision (Marchetti-Bowick and Chambers, 2012). 3 3.1 Data and Problem Setting Data Collection We collected tweets for 32 politicians, the 16 Republicans (all 2016 presidential candidates) and 16 Democrats (5 of which were candidates) listed in Table 1. Our initial goal was to compare politicians participating in the 2016 U.S. presidential election. To increase representation of Democrats, we collected tweets of Democrats who hold leadership roles within their party, because more well known politicians tend to focus their tweets on national rather than local (district/state) events. For all 32 politicians we have a total of 99,161 tweets:"
C16-1279,P15-1139,0,0.0471097,"litical discourse in both traditional 1 In our data set, there are few “@” mentions or retweet examples forming a conversation, thus we do not have access to argument or conversation structures for analysis. 2967 and social media. Several previous works have explored topic framing (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et"
C16-1279,C14-1019,0,0.11026,"eets. In contrast to this task, as well as most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we investigate how a politician’s overall Twitter behavior, as represented by combined content and temporal indicators, is indicative of a stance (e.g., also capturing when politicians fail to tweet about a topic). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015; Volkova et al., 2014; Conover et al., 2011). Other works have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), analyzing types of tweets and Twitter network effects around political events (Maireder and Ausserhofer, 2013), automatic polls based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as applications of distant supervision (Marchetti-Bowick and Chambers, 2012). 3 3.1 Data and Problem Setting Data Collection We collected tweets for 32 politicians, the 16 Republicans (all 2016 presidential candidates) and 16 Democrats (5 of which w"
C16-1279,P13-1162,0,0.0586217,"rd et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in microblogging platforms. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweet"
C16-1279,N10-1020,0,0.0617658,"to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in microblogging platforms. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. In contrast to this task, as well as most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we investigate how a politician’s overall Twitter behavior, as represented by combined content and temporal indicators, is indicative of a stance (e.g., also capturing when politicians fail to tweet about a topic). Predictin"
C16-1279,D13-1010,0,0.135103,"tational cost. In recent years there has been a growing interest in analyzing political discourse in both traditional 1 In our data set, there are few “@” mentions or retweet examples forming a conversation, thus we do not have access to argument or conversation structures for analysis. 2967 and social media. Several previous works have explored topic framing (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) of public statements, congressional speeches, and news articles. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we use the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Concerning Twitter specifically, analysis of users and political tweets has attracted considerable attention. Unsupervised and weakly supervised models of Twitter data fo"
C16-1279,P09-1026,0,0.094641,"th the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and PSL collective classification (Bach et al., 2015) are closest to our work, but these typically operat"
C16-1279,W10-0214,0,0.0597845,"approach, which outperforms both the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and PSL collective classification (Bach et al., 2015) are closest to our w"
C16-1279,P15-1012,0,0.0459786,"periments demonstrate the DOM refer to different frames for the issue. effectiveness of our global modeling approach, which outperforms both the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), so"
C16-1279,P15-1157,0,0.217407,"weets on the issue of gun control shown in Figure 1. To identify the stance taken by each politician, our model combines both content and behavioral features, accumulated from all of a politician’s tweets on that issue. First, the tweet’s relevance to an issue can be identified using issue indicators (highlighted in green). Second, the similarity between the stances taken by two of the politicians (agreement) can be identified by observing differences in how the issue is framed (shown in yellow), a tool often used by politicians to create bias toward a stance and contextualize the discussion (Tsur et al., 2015; Card et al., 2015). Tweets (1) and (3) frame the issue as a matter of safety, while tweet (2) frames it as related to personal freedom, thus revealing the agreement and disagreement patterns between the politicians. Third, we can consider the timing of these tweets, i.e. whether these tweets are posted continually or just around events concerning gun violence. Finally, we can also use sentiment indicators (e.g., the negative sentiment of tweet (1)). Notice that each feature individually might not contain sufficient information for correct classification, but combining all aspects, This work"
C16-1279,P14-1018,0,0.0700908,"Missing"
C16-1279,N12-1072,0,0.065492,"sue. effectiveness of our global modeling approach, which outperforms both the weak learners that provide the initial supervision and a supervised text based baseline. Our results show that understanding political discourse on Twitter requires modeling not only the word content of tweets but the social behavior associated with those tweets as well. 2 Related Work To the best of our knowledge this is the first work predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and PSL collective clas"
C16-1279,Q14-1024,0,0.10449,"ork predicting politicians’ stances using Twitter data, based on content, frames, and temporal activity. Several works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data by exploiting argument and threaded conversation structures, both of which are not always present in short Twitter data1 . Social interaction and group structure has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and PSL collective classification (Bach et al., 2015) are closest to our work, but these typically operate in supervised settings. Conversely, we use PSL without direct supervision, to assign soft values (0 to 1 inclusive) to output variables, rather than Markov Logic Networks, which assign hard (0 or 1) values to model variables and incur heavier inference time computational cost. In recent years there has been a growing interest i"
C16-1279,H05-2018,0,0.0662975,"e issues at once (e.g., religion may indicate the tweet refers to ISIS, Religion, or Marriage). Tweets are classified as relating to a certain issue based on the majority of matching keywords, with rare cases of ties manually resolved. The output of this classifier is all of the issue-related tweets of a politician, which are used as input for the PSL predicate T WEETS (P1, I SSUE ), a binary predicate which indicates if that politician has tweeted about the issue or not. 4.2 Sentiment of Tweets The sentiment of a tweet can indicate a politician’s stance on a certain issue. OpinionFinder 2.0 (Wilson et al., 2005) is used to label each politician’s issue-related tweets as positive, negative, or neutral. We observed, however, that for all politicians, a majority of tweets will be labeled as neutral. This may be caused by the difficulty of labeling sentiment for Twitter data. When this results with a politician having no positive or negative tweets, they are assigned their party’s majority sentiment for that issue. The majority sentiment of a party is calculated by running all politicians’ tweets through OpinionFinder and using whichever sentiment (positive or negative) is assigned the most per party. Th"
C16-1279,J04-3002,0,\N,Missing
C18-1316,L16-1704,0,0.0903183,"eir ability to share information between the represented objects. We exploit this property, and show that by adding additional information to the embedding space, the overall performance of the model improves, even if this information is not directly relevant to the classification task. We demonstrate this fact by comparing stance prediction performance, when trained over the multiple topic separately or jointly (thus allowing the model to share information between the representations of multiple debate topics). We evaluate our approach over the Internet Argument Corpus (Walker et al., 2012a; Abbott et al., 2016), collected from two debate websites,C REATE D EBATE and 4F ORUMS. We conduct several experiments, both using in-domain data and out-of-domain data (when we train and test on different debate topics). Our experiments show that formulating the problem as structured representation learning indeed allows debate entities to share information and generalize better, resulting in even larger improvements when multiple stances (corresponding to different output labels) are trained jointly. Furthermore, we show that by using inference over the relationships between the learned representations we can ou"
C18-1316,W11-1701,0,0.442498,"ous competitive models. 1 Introduction In recent years, social media platforms play an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year”"
C18-1316,P16-1231,0,0.194752,"he closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also broadly related to deep learning methods that capture the structural dependencies between decisions. This can be done either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), or by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016). Unlike these work, we formulate our problem as a structured representation learning problem, which to our knowledge is the first work to identify the ties between the two problems. 3 Model Overview In this paper we suggest casting stance classification as a structured representation learning task. Our approach revolves around two key ideas. First, stance classification can be done by embedding both the input objects (i.e., posts) and the output labels in the same space. The actual classification is performed by comparing the similarity between the embedded representations of an input object"
C18-1316,D16-1084,0,0.0130886,"roach (Walker et al., 2012b; 1 Please refer to https://github.com/BillMcGrady/StancePrediction for data and source code. 3729 Global Representation Learning Global MAP Inference Joint Representation Learning Debate Data con pro pro con Relational Embedding Output Predictions Figure 2: Overall Learning and Inference Processes Hasan and Ng, 2013; Sridhar et al., 2015). Stance prediction is not limited to online debates, as was also studied in the context of congressional speeches (Bansal et al., 2008; Burfoot et al., 2011) and social media outlets, such as Twitter (Johnson and Goldwasser, 2016; Augenstein et al., 2016; Ebrahimi et al., 2016), including a recent SemEval-16 task (Mohammad et al., 2016). While most work view the task as supervised classification tasks, several work suggest exploiting the interactions between users as a form of distant supervision (Johnson and Goldwasser, 2016; Dong et al., 2017). This task is broadly related to argumentation mining (Ghosh et al., 2014) and stance reason classification (Hasan and Ng, 2014). Our technical work relies on exploiting distributed representations (i.e., embedding), building on highly influential work on embedding words (Mikolov et al., 2013b; Pennin"
C18-1316,P11-1151,0,0.0245817,"ions (Somasundaran and Wiebe, 2010; Anand et al., 2011), while later work took a collective approach (Walker et al., 2012b; 1 Please refer to https://github.com/BillMcGrady/StancePrediction for data and source code. 3729 Global Representation Learning Global MAP Inference Joint Representation Learning Debate Data con pro pro con Relational Embedding Output Predictions Figure 2: Overall Learning and Inference Processes Hasan and Ng, 2013; Sridhar et al., 2015). Stance prediction is not limited to online debates, as was also studied in the context of congressional speeches (Bansal et al., 2008; Burfoot et al., 2011) and social media outlets, such as Twitter (Johnson and Goldwasser, 2016; Augenstein et al., 2016; Ebrahimi et al., 2016), including a recent SemEval-16 task (Mohammad et al., 2016). While most work view the task as supervised classification tasks, several work suggest exploiting the interactions between users as a form of distant supervision (Johnson and Goldwasser, 2016; Dong et al., 2017). This task is broadly related to argumentation mining (Ghosh et al., 2014) and stance reason classification (Hasan and Ng, 2014). Our technical work relies on exploiting distributed representations (i.e.,"
C18-1316,P15-1030,0,0.0135946,"create a common representation for the them. The closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also broadly related to deep learning methods that capture the structural dependencies between decisions. This can be done either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), or by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016). Unlike these work, we formulate our problem as a structured representation learning problem, which to our knowledge is the first work to identify the ties between the two problems. 3 Model Overview In this paper we suggest casting stance classification as a structured representation learning task. Our approach revolves around two key ideas. First, stance classification can be done by embedding both the input objects (i.e., posts) and the output labels in the same space. The actual classification is performed by comparing the similarity between the em"
C18-1316,D16-1105,0,0.0150585,"2b; 1 Please refer to https://github.com/BillMcGrady/StancePrediction for data and source code. 3729 Global Representation Learning Global MAP Inference Joint Representation Learning Debate Data con pro pro con Relational Embedding Output Predictions Figure 2: Overall Learning and Inference Processes Hasan and Ng, 2013; Sridhar et al., 2015). Stance prediction is not limited to online debates, as was also studied in the context of congressional speeches (Bansal et al., 2008; Burfoot et al., 2011) and social media outlets, such as Twitter (Johnson and Goldwasser, 2016; Augenstein et al., 2016; Ebrahimi et al., 2016), including a recent SemEval-16 task (Mohammad et al., 2016). While most work view the task as supervised classification tasks, several work suggest exploiting the interactions between users as a form of distant supervision (Johnson and Goldwasser, 2016; Dong et al., 2017). This task is broadly related to argumentation mining (Ghosh et al., 2014) and stance reason classification (Hasan and Ng, 2014). Our technical work relies on exploiting distributed representations (i.e., embedding), building on highly influential work on embedding words (Mikolov et al., 2013b; Pennington et al., 2014), sent"
C18-1316,W14-2106,0,0.124941,"ls. 1 Introduction In recent years, social media platforms play an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year” Pro a2 “Whether it k"
C18-1316,I13-1191,0,0.151689,"media platforms play an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year” Pro a2 “Whether it kills people or not, it still is harmful to"
C18-1316,D14-1083,0,0.0204601,"lso studied in the context of congressional speeches (Bansal et al., 2008; Burfoot et al., 2011) and social media outlets, such as Twitter (Johnson and Goldwasser, 2016; Augenstein et al., 2016; Ebrahimi et al., 2016), including a recent SemEval-16 task (Mohammad et al., 2016). While most work view the task as supervised classification tasks, several work suggest exploiting the interactions between users as a form of distant supervision (Johnson and Goldwasser, 2016; Dong et al., 2017). This task is broadly related to argumentation mining (Ghosh et al., 2014) and stance reason classification (Hasan and Ng, 2014). Our technical work relies on exploiting distributed representations (i.e., embedding), building on highly influential work on embedding words (Mikolov et al., 2013b; Pennington et al., 2014), sentences (Kiros et al., 2015) and even full documents (Le and Mikolov, 2014). Our work explores the connections between text, users and attributes, attempting to create a common representation for the them. The closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also br"
C18-1316,C16-1279,1,0.860875,"ter work took a collective approach (Walker et al., 2012b; 1 Please refer to https://github.com/BillMcGrady/StancePrediction for data and source code. 3729 Global Representation Learning Global MAP Inference Joint Representation Learning Debate Data con pro pro con Relational Embedding Output Predictions Figure 2: Overall Learning and Inference Processes Hasan and Ng, 2013; Sridhar et al., 2015). Stance prediction is not limited to online debates, as was also studied in the context of congressional speeches (Bansal et al., 2008; Burfoot et al., 2011) and social media outlets, such as Twitter (Johnson and Goldwasser, 2016; Augenstein et al., 2016; Ebrahimi et al., 2016), including a recent SemEval-16 task (Mohammad et al., 2016). While most work view the task as supervised classification tasks, several work suggest exploiting the interactions between users as a form of distant supervision (Johnson and Goldwasser, 2016; Dong et al., 2017). This task is broadly related to argumentation mining (Ghosh et al., 2014) and stance reason classification (Hasan and Ng, 2014). Our technical work relies on exploiting distributed representations (i.e., embedding), building on highly influential work on embedding words (Miko"
C18-1316,P16-1087,0,0.0314888,"(Le and Mikolov, 2014). Our work explores the connections between text, users and attributes, attempting to create a common representation for the them. The closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also broadly related to deep learning methods that capture the structural dependencies between decisions. This can be done either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), or by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016). Unlike these work, we formulate our problem as a structured representation learning problem, which to our knowledge is the first work to identify the ties between the two problems. 3 Model Overview In this paper we suggest casting stance classification as a structured representation learning task. Our approach revolves around two key ideas. First, stance classification can be done by embedding both the input objects (i.e., posts) and the output"
C18-1316,N16-1030,0,0.0273779,"ation for the them. The closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also broadly related to deep learning methods that capture the structural dependencies between decisions. This can be done either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), or by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016). Unlike these work, we formulate our problem as a structured representation learning problem, which to our knowledge is the first work to identify the ties between the two problems. 3 Model Overview In this paper we suggest casting stance classification as a structured representation learning task. Our approach revolves around two key ideas. First, stance classification can be done by embedding both the input objects (i.e., posts) and the output labels in the same space. The actual classification is performed by comparing the similarity between the embedded representation"
C18-1316,S16-1003,0,0.236052,"rtant role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year” Pro a2 “Whether it kills people or not, it still is harmful to your body.” Con a3 “There are many things th"
C18-1316,D14-1162,0,0.0972896,", 2016; Ebrahimi et al., 2016), including a recent SemEval-16 task (Mohammad et al., 2016). While most work view the task as supervised classification tasks, several work suggest exploiting the interactions between users as a form of distant supervision (Johnson and Goldwasser, 2016; Dong et al., 2017). This task is broadly related to argumentation mining (Ghosh et al., 2014) and stance reason classification (Hasan and Ng, 2014). Our technical work relies on exploiting distributed representations (i.e., embedding), building on highly influential work on embedding words (Mikolov et al., 2013b; Pennington et al., 2014), sentences (Kiros et al., 2015) and even full documents (Le and Mikolov, 2014). Our work explores the connections between text, users and attributes, attempting to create a common representation for the them. The closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also broadly related to deep learning methods that capture the structural dependencies between decisions. This can be done either by modeling the dependencies between the hidden representations of co"
C18-1316,W10-0214,0,0.793216,"tter results compared to previous competitive models. 1 Introduction In recent years, social media platforms play an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for mariju"
C18-1316,P15-1012,0,0.61502,"y an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year” Pro a2 “Whether it kills people or not, it still is harmful to your body.” Con a3 “T"
C18-1316,N16-1027,0,0.0318153,"nd even full documents (Le and Mikolov, 2014). Our work explores the connections between text, users and attributes, attempting to create a common representation for the them. The closest to our work is (Li et al., 2015), that jointly integrates different kinds of cues (text, attribute, graph) into a single latent representation to get user embeddings. Our work is also broadly related to deep learning methods that capture the structural dependencies between decisions. This can be done either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), or by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016). Unlike these work, we formulate our problem as a structured representation learning problem, which to our knowledge is the first work to identify the ties between the two problems. 3 Model Overview In this paper we suggest casting stance classification as a structured representation learning task. Our approach revolves around two key ideas. First, stance classification can be done by embedding both the input objects (i"
C18-1316,walker-etal-2012-corpus,0,0.603249,"n recent years, social media platforms play an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year” Pro a2 “Whether it kills people or not, i"
C18-1316,N12-1072,0,0.51945,"n recent years, social media platforms play an increasingly important role in shaping political discourse. Online debate forums allow users to voice their opinions and engage with other users holding different views. Understanding the interactions between the users on these platforms can help provide insight into current political discourse, argumentation strategies and can help gauge public sentiment on policy issues on a large scale. The importance of understanding debate dialog has motivated significant research efforts (Somasundaran and Wiebe, 2010; Anand et al., 2011; Ghosh et al., 2014; Walker et al., 2012b; Hasan and Ng, 2013; Sridhar et al., 2015; Mohammad et al., 2016; Dong et al., 2017). In this paper, we focus on stance prediction, automatically identifying the stance expressed in debate posts on various issues. For example, Figure 1 describes a short debate dialog about marijuana legalization between three users (denoted a1 , a2 , a3 ). The content associated with each user is classified as supportive of legalization (P RO), or not (C ON). Author Discussion Text Stance a1 “There are no deaths related to the actual use for marijuana this past year” Pro a2 “Whether it kills people or not, i"
D08-1037,N06-1046,0,0.0152546,"problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter354 mine initial weights. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our key contribution is that we use a c"
D08-1037,P07-1083,0,0.333194,"address this problem. The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al., 2006) and train a discriminative classifier. That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other. Several papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (Klementiev and Roth, 2006b; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008). While it has been clear that the relevancy of pairwise features is context sensitive and that there are contextual constraints among them, the hope was that a discriminative approach will be sufficient to account for those by weighing features appropriately. This has been shown to be difficult for language pairs which are very different, such as English and Hebrew (Goldwasser and Roth, 2008). In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem. We formalize it as an o"
D08-1037,P08-2014,1,0.734752,"common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al., 2006) and train a discriminative classifier. That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other. Several papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (Klementiev and Roth, 2006b; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008). While it has been clear that the relevancy of pairwise features is context sensitive and that there are contextual constraints among them, the hope was that a discriminative approach will be sufficient to account for those by weighing features appropriately. This has been shown to be difficult for language pairs which are very different, such as English and Hebrew (Goldwasser and Roth, 2008). In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem. We formalize it as an optimization problem over a s"
D08-1037,P08-1045,0,0.204128,"Missing"
D08-1037,N06-1011,1,0.0874405,"ng English NEs to Hebrew. 1 Figure 1: Named entities transliteration pairs in English and Hebrew and the character level mapping between the two names. The Hebrew names can be romanized as eeta-l-ya and a-ya Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language based on phonetic similarity between the entities. Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al., 2008). It may appear at first glance that identifying the phonetic correlation between names based on an orthographic analysis is a simple, straight-forward task; however in many cases a consistent deterministic mapping between characters does not exist; rather, the mapping depends on the context the characters appear in and on transliteration conventions which may change across domains. Figure 1 exhibits two examples of NE transliterations in English and Hebrew, with the correct mapping across the two scripts. Although the two Hebrew names share a common prefix1 , this pr"
D08-1037,P06-1103,1,0.0915542,"ng English NEs to Hebrew. 1 Figure 1: Named entities transliteration pairs in English and Hebrew and the character level mapping between the two names. The Hebrew names can be romanized as eeta-l-ya and a-ya Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language based on phonetic similarity between the entities. Identifying transliteration pairs is an important component in many linguistic applications which require identifying out-of-vocabulary words, such as machine translation and multilingual information retrieval (Klementiev and Roth, 2006b; Hermjakob et al., 2008). It may appear at first glance that identifying the phonetic correlation between names based on an orthographic analysis is a simple, straight-forward task; however in many cases a consistent deterministic mapping between characters does not exist; rather, the mapping depends on the context the characters appear in and on transliteration conventions which may change across domains. Figure 1 exhibits two examples of NE transliterations in English and Hebrew, with the correct mapping across the two scripts. Although the two Hebrew names share a common prefix1 , this pr"
D08-1037,W05-0618,0,0.0331978,"character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter354 mine initial weights. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our key contribution is that we use a constrained optimization approach also to determine"
D08-1037,W05-0639,0,0.134871,"ze it as an optimization problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter354 mine initial weights. We then use a method similar to (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008) in order to discriminatively train a better weight vector for the objective function. Our key cont"
D08-1037,W04-2401,1,0.930743,"lish and Hebrew (Goldwasser and Roth, 2008). In this paper, we address these difficulties by proposing to view the transliteration decision as a globally phrased constrained optimization problem. We formalize it as an optimization problem over a set of local pairwise features – character n-gram matches across the two string – and subject to legitimacy constraints. We use a discriminatively trained classifier as a way to learn the objective function for the global constrained optimization problem. Our technical approach follows a large body of work developed over the last few years, following (Roth and Yih, 2004) that has formalized global decisions problems in NLP as constrained optimization problems and solved these optimization problems using Integer Linear Programming (ILP) or other methods (Punyakanok et al., 2005; Barzilay and Lapata, 2006; Clarke and Lapata, ; Marciniak and Strube, 2005). We investigate several ways to train our objective function, which is represented as a dot product between a set of features chosen to represent a pair (ws , wt ), and a vector of initial weights. Our first baseline makes use of all features extracted from a pair, along with a simple counting method to deter35"
D08-1037,W06-1630,0,0.358201,"all our example the Hebrew script is shown left-to-right to simplify the visualization of the transliteration mapping. 353 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 353–362, c Honolulu, October 2008. 2008 Association for Computational Linguistics In recent years, as it became clear that solutions that are based on linguistics rules are not satisfactory, machine learning approaches have been developed to address this problem. The common approach adopted is therefore to view this problem as a classification problem (Klementiev and Roth, 2006a; Tao et al., 2006) and train a discriminative classifier. That is, given two strings, one in the source and the other in the target language, extract pairwise features, and train a classifier that determines if one is a transliteration of the other. Several papers have followed up on this basic approach and focused on semi-supervised approaches to this problem or on extracting better features for the discriminative classifier (Klementiev and Roth, 2006b; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008). While it has been clear that the relevancy of pairwise features is context sensitive and that there are"
D08-1037,D07-1001,0,\N,Missing
D09-1100,D08-1082,0,0.023721,"work also considered the supervision signal obtained by interpreting natural language in the context of a formal domain. Branavan et al. (2009) use feedback from a world model as a supervision signal. Chen and Mooney (2008) use temporal alignment of text and grounded descriptions of the world state. In these approaches, concrete domain entities are grounded in language interpretation, and therefore require only a propositional semantic representation. Previous approaches for interpreting generalized natural language statements are trained from labeled examples (Zettlemoyer and Collins, 2005; Lu et al., 2008). 8 Conclusion This paper demonstrates a new setting for semantic analysis, which we term reading to learn. We handle text which describes the world in general terms rather than refereing to concrete entities in the domain. We obtain a semantic abstract of multiple documents, using a novel, minimallysupervised generative model that accounts for both syntax and lexical choice. The semantic abstract is represented as a set of predicate logic formulae, which are applied as higher-order features for learning. We demonstrate that these features improve learning performance, and that both the lexica"
D09-1100,P09-1010,0,0.123404,"tic structure. The results are stronger than the S ENSORS ONLY and R ELATIONAL - RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be evaluated given a specific state. This contrasts to previous work which interprets “directions” and thus assumes a direct correspondence between text and world state (Branavan et al., 2009; Chen and M"
D09-1100,C92-2106,0,0.223445,"Missing"
D09-1100,W05-0602,0,0.495566,"tation is not due merely to the added expressivity of our features. The third row compares against N O - SYNTAX, a crippled version of our model that incorporates lexical features but not the syntactic structure. The results are stronger than the S ENSORS ONLY and R ELATIONAL - RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. 7 Related Work This paper draws on recent literature on extracting logical forms from surface text (Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Downey et al., 2005; Liang et al., 2009), interpreting language in the context of a domain (Chen and Mooney, 2008), and using an actionable domain to guide text interpretation (Branavan et al., 2009). We differentiate our research in several dimensions: 965 Language Interpretation Instructional text describes generalized statements about entities in the domain and the way they interact, thus the text does not correspond directly to concrete sensory inputs in the world (i.e., a specific world state). Our interpretation captures these generalizations as first-order logic statements that can be"
D09-1100,W00-1308,0,0.0113873,"owest, highest higher, sequence, sequential black, red, color suit, club, diamond, spade, heart onto bottom, available, top empty obtained from the Internet. Due to the popularity of the Microsoft implementation of Freecell, instructions often contain information specific to playing Freecell on a computer. We manually removed sentences which did not focus on the card aspects of Freecell (e.g., how to set up the board and information regarding where to click to move cards). In order to use our semantic abstraction model, the instructions were part-of-speech tagged with the Stanford POS Tagger (Toutanova and Manning, 2000) and dependency parses were obtained using Malt (Nivre, 2006). Table 1: Predicates in the Freecell world model, with natural language glosses obtained from the development set text. Glosses Our reading to learn setting requires a small set of glosses, which are surface forms commonly used to represent predicates from the world model. We envision an application scenario in which a designer manually specifies a few glosses for each predicate. However, for the purposes of evaluation, it would be unprincipled for the experimenters to handcraft the ideal set of glosses. Instead, we gathered a devel"
D09-1100,P07-1121,0,0.146856,"Missing"
D09-1100,P06-1085,0,0.0178021,"forward. For word slots to which no literals are aligned, the lexical item is drawn from a language model θ, estimated from the entire document collection. For slots to which at least one literal is aligned, we construct a language model φ in which the probability mass is divided equally among all glosses of aligned predicates. The language model θ is used as a backoff, so that there is a strong bias in favor of generating glosses, but some probability mass is reserved for the other lexical items. 1 There are many recent applications of Dirichlet processes in natural language processing, e.g. Goldwater et al. (2006). 961 4 Inference To compute the probability of a parsed sentence given a formula, we sum over alignments, This section describes a sampling-based inference procedure for obtaining a set of formulae f that explain the observed text s and dependency structures t. We perform Gibbs sampling over the formulae assigned to each sentence. Using the Chinese Restaurant Process interpretation of the Dirichlet Process (Aldous, 1985), we marginalize π, the infinite multinomial over all possible formulae: at each sampling step we select either an existing formula, or stochastically generate a new formula."
D09-1100,P09-1011,0,0.199814,"erely wish to acquire a semantic abstract of a document or document collection, and use the discovered relations to facilitate datadriven learning. This will allow us to directly evaluate the contribution of the extracted relations for learning. We develop an approach to recover semantic abstracts that uses minimal supervision: we assume only a very small set of lexical glosses, which map from words to sensors. This marks a substantial departure from previous work on semantic parsing, which requires either annotations of the meanings of each individual sentence (Zettlemoyer and Collins, 2005; Liang et al., 2009), or alignments of sentences to grounded representations of the Machine learning offers a range of tools for training systems from data, but these methods are only as good as the underlying representation. This paper proposes to acquire representations for machine learning by reading text written to accommodate human learning. We propose a novel form of semantic analysis called reading to learn, where the goal is to obtain a high-level semantic abstract of multiple documents in a representation that facilitates learning. We obtain this abstract through a generative model that requires no label"
D09-1100,W05-0600,0,\N,Missing
D17-1179,P16-1231,0,0.124857,"their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lam"
D17-1179,N16-1181,0,0.0256035,"r constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm. All of these tasks can benefit from semisupervised learning algorithms.1 1 Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni e"
D17-1179,D14-1082,0,0.0605891,"der and the decoder simultaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (A"
D17-1179,P15-1030,0,0.117236,"ltaneously by decoupling their parameters. Our experimental results over the Part-of-Speech (POS) tagging task on eight different languages, show that the NCRF-AE model can outperform competitive systems in both supervised and semi-supervised scenarios. 1 Introduction The recent renaissance of deep learning has led to significant strides forward in several AI fields. In Natural Language Processing (NLP), characterized by highly structured tasks, promising results were obtained by models that combine deep learning methods with traditional structured learning algorithms (Chen and Manning, 2014; Durrett and Klein, 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng a"
D17-1179,P82-1020,0,0.71631,"Missing"
D17-1179,P16-1087,0,0.0298882,"g/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient s"
D17-1179,D14-1181,0,0.00281176,"tasks such as dependency parsing or constituent parsing by replacing the forwardbackward algorithm with the inside-outside algorithm. All of these tasks can benefit from semisupervised learning algorithms.1 1 Our code and experimental set up will be available at https://github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the H"
D17-1179,D16-1116,0,0.114307,"Missing"
D17-1179,N16-1030,0,0.366426,"016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-"
D17-1179,N15-1144,0,0.36216,"as ˆ ), where Y a generative model, describing P (X|Y is the label. In our model, illustrated in Figure 1b, the encoder is a CRF model with neural networks 1701 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1701–1711 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics as its potential extractors, while the decoder is a generative model, trying to reconstruct the input. Our model carries the merit of autoencoders, which can exploit valuable information from unlabeled data. Recent works (Ammar et al., 2014; Lin et al., 2015) suggested using an autoencoder with a CRF model as an encoder in an unsupervised setting. We significantly expand on these works and suggest the following contributions: 1. We propose a unified model seamlessly accommodating both unlabeled and labeled data. While past work focused on unsupervised structured prediction, neglecting the discriminative power of such models, our model easily supports learning in both fully supervised and semisupervised settings. We developed a variation of the Expectation-Maximization (EM) algorithm, used for optimizing the encoder and the decoder of our model sim"
D17-1179,P16-1101,0,0.255363,", 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCRF-AE) model for semi-supervised learning"
D17-1179,D16-1028,0,0.0307705,"013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstru"
D17-1179,N06-1020,0,0.0692806,"2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unif"
D17-1179,D16-1031,0,0.0417379,"(Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstruct the input X ˆ aiming to maximize the log probability P (X|X) without knowing the latent variable Y explicitly. Since we focus"
D17-1179,P16-2025,0,0.0234859,", 2015; Andor et al., 2016; Wiseman and Rush, 2016). These models combine the strengths of neural models, that can score local decisions using a rich non-linear representation, with efficient inference procedures used to combine the local decisions into a coherent global decision. Among these models, neural variants of the Conditional Random Fields (CRF) model (Lafferty et al., 2001) are especially popular. By replacing the linear potentials with non-linear potential using neural networks these models were able to improve performance in several structured prediction tasks (Andor et al., 2016; Peng and Dredze, 2016; Lample et al., 2016; Ma and Hovy, 2016; Durrett and Klein, 2015). Despite their promise, wider adoption of these algorithms for new structured prediction tasks can be difficult. Neural networks are notoriously susceptible to over-fitting unless large amounts of training data are available. This problem is exacerbated in the structured settings, as accounting for the dependencies between decisions requires even more data. Providing it through manual annotation is often a difficult labor-intensive task. In this paper we tackle this problem, and propose an end-to-end neural CRF autoencoder (NCR"
D17-1179,D16-1137,0,0.0510587,"Missing"
D17-1179,P13-1045,0,0.0184407,"eration (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Mar"
D17-1179,N10-1116,0,0.0132529,"16; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalize"
D17-1179,W15-1511,0,0.0137207,"pproaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the origi"
D17-1179,D10-1017,0,0.0806109,"or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a)"
D17-1179,W16-5907,0,0.0258514,"2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficulty of providing sufficient supervision has motivated work on semi-supervised and unsupervised learning for many of these tasks (McClosky et al., 2006; Spitkovsky et al., 2010; Subramanya et al., 2010; Stratos and Collins, 2015; Marinho et al., 2016; Tran et al., 2016), including several that also used autoencoders (Ammar et al., 2014; Lin et al., 2015; Miao and Blunsom, 2016; Kocisk´y et al., 2016; Cheng et al., 2017). In this paper we expand on these works, and suggest a neural CRF autoencoder, that can leverage both labeled and unlabeled data. 3 Neural CRF Autoencoder In semi-supervised learning the algorithm needs to utilize both labeled and unlabeled data. Autoencoders offer a convenient way of dealing with both types of data in a unified fashion. A generalized autoencoder (Figure 1a) tries to ˆ given the original input X, reconstruct the input X ˆ aim"
D17-1179,N16-1027,0,0.0315798,"//github.com/cosmozhang/NCRF-AE 2 Related Work Neural networks were successfully applied to many NLP tasks, including tagging (Ma and Hovy, 2016; Mesnil et al., 2015; Lample et al., 2016), parsing (Chen and Manning, 2014), text generation (Sutskever et al., 2011), machine translation (Bahdanau et al., 2015), sentiment analysis (Kim, 2014) and question answering (Andreas et al., 2016). Most relevant to this work are structured prediction models capturing dependencies between decisions, either by modeling the dependencies between the hidden representations of connected decisions using RNN/LSTM (Vaswani et al., 2016; Katiyar and Cardie, 2016), by explicitly modeling the structural dependencies between output predictions (Durrett and Klein, 2015; Lample et al., 2016; Andor et al., 2016), or by combining the two approaches (Socher et al., 2013; Wiseman and Rush, 2016). In contrast to supervised latent variable models, such as the Hidden Conditional Random Fields in (Quattoni et al., 2007), which utilize additional latent variables to infer for supervised structure prediction, we do not presume any additional latent variables in our NCRF-AE model in both supervised and semi-supervised setting. The difficult"
E14-1069,P13-1162,0,0.017982,"gue and are likely to attract opposition. We use several resources to capture this information. We use a lexicon of subjective and positive/negative sentiment expressions (Riloff and Wiebe, 2003). This resource can help identify subjective statements attempting to bias the discussion (e.g., “So he was driving negligently?”) We use a list of hedges and boosters (Hyland, 2005). This resource can potentially allow the model to identify evasive (“I might have seen him”) and (overly) confident responses (“I am absolutely sure that I have seen him”). We use a lexicon of biased language provided by (Recasens et al., 2013), this lexicon extracted from Wikipedia edits consists of words indicative of bias, for example in an attempt to frame the facts raised in the discussion according to one of the viewpoints (“The death of Nicolle Simposon” vs. “The murder of Nicolle Simposon”). Finally we use a Patient Polarity Verbs lexicon (Goyal et al., 2010). This lexicon consists 2.1 Mining Courtroom Proceedings The first step in forming our dataset consists of collecting a large set of relevant courtroom dialogue snippets. First, we look for textual occurrences of objections in the trial transcript by looking for sustain"
E14-1069,W03-1014,0,0.105716,"r relevant utterances, such as ones making claims associating individuals with locations. We use the Named Entity Recognizer (NER) described in (Finkel et al., 2005) to identify this information. (2) Subjective and Biased Language Equally important to understanding the topics of conversation is the way they are discussed. Expressions of subjectivity and sentiment are useful linguistic tools for changing the tone of the dialogue and are likely to attract opposition. We use several resources to capture this information. We use a lexicon of subjective and positive/negative sentiment expressions (Riloff and Wiebe, 2003). This resource can help identify subjective statements attempting to bias the discussion (e.g., “So he was driving negligently?”) We use a list of hedges and boosters (Hyland, 2005). This resource can potentially allow the model to identify evasive (“I might have seen him”) and (overly) confident responses (“I am absolutely sure that I have seen him”). We use a lexicon of biased language provided by (Recasens et al., 2013), this lexicon extracted from Wikipedia edits consists of words indicative of bias, for example in an attempt to frame the facts raised in the discussion according to one of"
E14-1069,D11-1039,0,0.0322247,"ant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Discriminative latent variables models have seen a surge of interest in recent years, both in the machine learning community (Yu and Joachims, 2009; Quattoni et al., 2007) as well as various application domains such as NLP (T¨ackstr¨om and McDonald, 2011) and computer vision (Felzenszwalb et al., 2010). In NLP, one of the most wellknown applications of discriminative latent strucInstead of focusing on actions, like the above work, we focus on dialogue content and relationships between utterances. Furthermore, unlike m"
E14-1069,W05-0613,0,0.0268158,"he structure prediction inference problem (Roth and Yih, 2007). Our prediction task, identifying the actionable result of a dialogue, requires capturing the dialogue and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Discriminative latent variables models have seen a surge of interest in recent years, both in the machine learning community (Yu and Joachims, 2009; Quattoni et al., 2007) as well"
E14-1069,J00-3003,0,0.365358,"Missing"
E14-1069,P08-1090,0,0.0404005,"ted sentences as irrelevant, while marking topically related sentences and identifying the connection between the question-answer pair (decisions marked in solid blue lines). When trained without situated information, the latent output structure marks topically unrelated sentences as relevant for objection classification. Note that in this case all the edge variables are turned off (marked with dashed red lines). A related area of work with different motivations and different technical approaches has focused on attempting to understand narrative structure. For instance, Chambers and Jurafsky (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) model narrative flow in the style of Schankian scripts (Schank and Abelson, 1977). Their focus is on common sequences of actions, not specifically related to dialogue. Somewhat more related is recent work (Goyal et al., 2010) that aimed to build a computational model of Lehnert’s Plot Units (Lehnert, 1981) model. That work focused primarily on actions and not on dialogue: in fact, their results showed that the lack of dialogue understanding was a significant detriment to their ability to model plot structure. room dialogues. We adopted the structured latent varia"
E14-1069,P09-1068,0,0.033846,"while marking topically related sentences and identifying the connection between the question-answer pair (decisions marked in solid blue lines). When trained without situated information, the latent output structure marks topically unrelated sentences as relevant for objection classification. Note that in this case all the edge variables are turned off (marked with dashed red lines). A related area of work with different motivations and different technical approaches has focused on attempting to understand narrative structure. For instance, Chambers and Jurafsky (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) model narrative flow in the style of Schankian scripts (Schank and Abelson, 1977). Their focus is on common sequences of actions, not specifically related to dialogue. Somewhat more related is recent work (Goyal et al., 2010) that aimed to build a computational model of Lehnert’s Plot Units (Lehnert, 1981) model. That work focused primarily on actions and not on dialogue: in fact, their results showed that the lack of dialogue understanding was a significant detriment to their ability to model plot structure. room dialogues. We adopted the structured latent variable model defined in (Chang et"
E14-1069,N10-1066,1,0.929036,"ng margin-based optimization problem, where λ is a regularization parameter, and ` is the squared-hinge loss function: In our experiments, we formalize Eq. (4) as an ILP instance, which we solve using the highly optimized Gurobi toolkit4 . 4 Learning and Inference 6 refers to all linguistic resources used. We also included a +/-1 word window around words appearing in these resources http://www.gurobi.com/ “*” denotes all properties 659 4.2 This formulation is not a convex optimization problem and care must be taken to find a good optimum. In our experiments, we use the algorithm presented in (Chang et al., 2010) to solve this problem. The algorithm solves this non-convex optimization function iteratively, decreasing the value of the objective in each iteration until convergence. In each iteration, the algorithm determines the values of the latent variables of positive examples, and optimizes the modified objective function using a cutting plane algorithm. This algorithmic approach is conceptually (and algorithmically) related to the algorithm suggested by (Yu and Joachims, 2009). As standard, we classify x as positive iff fw (x) ≥ 0. In Eq. (4), wT φs (x) is the score associated with the substructure"
E14-1069,N13-1100,0,0.0210085,"ki/Foundation_ (evidence) 661 … MR. DARDEN THE COURT MS. KESTLER tured classification is to the Textual Entailment (TE) task (Chang et al., 2010; Wang and Manning, 2010). The TE task bears some resemblances ours, as both tasks require making a binary decision on the basis of a complex input object (i.e., the history of dialogue, pairs of paragraphs), creating the need for a learning framework that is flexible enough to model the complex latent structure that exists in the input. Another popular application domain is sentiment analysis (Yessenalina et al., 2010; T¨ackstr¨om and McDonald, 2011; Trivedi and Eisenstein, 2013). The latent variable model allows the learner to identify finer grained sentiment expression than annotated in the data. Your Honor, this is hearsay Overruled I don&apos;t recall if there was that day or not. I know at some point, we had a meeting as to what evidence we had and what was going to be tested and who was going to test it. MR. NEUFELD On the very next day, June 16th, did you participate in another meeting about this case? MS. KESTLER I don&apos;t recall. MR. NEUFELD Do you recall being at a meeting with Erin Reilly, Collin Yamauchi and Dennis Fung and … Greg Matheson about this case on June"
E14-1069,P05-1045,0,0.00631614,"dication of structure, topics of controversy, and the sentiment and tone of language used in the dialogue. The second captures pragmatic considerations by situating the dialogue utterances in the context of the courtroom. Each utterance is attributed to a speaker, thus capturing meaningful patterns specific to individual speakers. Linguistic Resources (1) Named Entities provide strong indications of the topics discussed in the dialogue and help uncover relevant utterances, such as ones making claims associating individuals with locations. We use the Named Entity Recognizer (NER) described in (Finkel et al., 2005) to identify this information. (2) Subjective and Biased Language Equally important to understanding the topics of conversation is the way they are discussed. Expressions of subjectivity and sentiment are useful linguistic tools for changing the tone of the dialogue and are likely to attract opposition. We use several resources to capture this information. We use a lexicon of subjective and positive/negative sentiment expressions (Riloff and Wiebe, 2003). This resource can help identify subjective statements attempting to bias the discussion (e.g., “So he was driving negligently?”) We use a li"
E14-1069,C10-1131,0,0.0608629,"Missing"
E14-1069,D10-1040,0,0.0269786,"e and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Discriminative latent variables models have seen a surge of interest in recent years, both in the machine learning community (Yu and Joachims, 2009; Quattoni et al., 2007) as well as various application domains such as NLP (T¨ackstr¨om and McDonald, 2011) and computer vision (Felzenszwalb et al., 2010). In NLP, one of the most well"
E14-1069,D10-1102,0,0.0287443,"to the results in Table 2. 9 10 http://en.wikipedia.org/wiki/Foundation_ (evidence) 661 … MR. DARDEN THE COURT MS. KESTLER tured classification is to the Textual Entailment (TE) task (Chang et al., 2010; Wang and Manning, 2010). The TE task bears some resemblances ours, as both tasks require making a binary decision on the basis of a complex input object (i.e., the history of dialogue, pairs of paragraphs), creating the need for a learning framework that is flexible enough to model the complex latent structure that exists in the input. Another popular application domain is sentiment analysis (Yessenalina et al., 2010; T¨ackstr¨om and McDonald, 2011; Trivedi and Eisenstein, 2013). The latent variable model allows the learner to identify finer grained sentiment expression than annotated in the data. Your Honor, this is hearsay Overruled I don&apos;t recall if there was that day or not. I know at some point, we had a meeting as to what evidence we had and what was going to be tested and who was going to test it. MR. NEUFELD On the very next day, June 16th, did you participate in another meeting about this case? MS. KESTLER I don&apos;t recall. MR. NEUFELD Do you recall being at a meeting with Erin Reilly, Collin Yamau"
E14-1069,D10-1008,1,0.901996,"Missing"
E14-1069,P97-1013,0,0.0510918,"their ability to model plot structure. room dialogues. We adopted the structured latent variable model defined in (Chang et al., 2010), and use ILP to solve the structure prediction inference problem (Roth and Yih, 2007). Our prediction task, identifying the actionable result of a dialogue, requires capturing the dialogue and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Di"
E14-1069,prasad-etal-2008-penn,0,0.03594,"nderstanding was a significant detriment to their ability to model plot structure. room dialogues. We adopted the structured latent variable model defined in (Chang et al., 2010), and use ILP to solve the structure prediction inference problem (Roth and Yih, 2007). Our prediction task, identifying the actionable result of a dialogue, requires capturing the dialogue and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialog"
E14-1069,miltsakaki-etal-2004-penn,0,\N,Missing
E14-1069,P13-2014,0,\N,Missing
K16-2019,P09-1068,0,0.127561,"e et al., 2015; Wang and Lan, 2015). We followed the intuition that obtaining a significant increase in performance using traditional classifiers and feature engineering would be difficult given the effort that was previously spent on such systems. Neural-network-based classifiers present a different and less explored approach to the discourse sense problem, which can potentially lead to considerable improvement. Our system, described in this paper, takes a step in this direction. We explore different input representation types and introduce event vectors for this task. Following the work of (Chambers and Jurafsky, 2009), we look into event chains as a way to represent structure in the discourse arguments. Then, we adapt the skip-gram approach originally used to learn word vectors from sentences (Mikolov et al., 2013b) to learn event vector representations from event sequences. To do so, we draw a clear analogy between words and events, as well as between sentences and event chains. Finally, each input relation is represented with the pre-trained event and word vectors of its arguments and a multi-layer neural network is used to classify senses. Predicting the sense of a discourse relation is particularly cha"
K16-2019,prasad-etal-2008-penn,0,0.0755732,"ents (Xue et al., 2016). Identifying the sense is particularly challenging in the case of implicit relations, where explicit connective words (e.g., however, but, because) are not present. Last year, most submitted systems used algorithms traditionally applied for this task, such as Support Vector Machine (SVM) and Maximum Entropy classifiers learned over binary features as input representation. This included the best performing system, which reached an accuracy of 34.45 in the test data 2 System Description The dataset used in the CoNLL shared task corresponds to the Penn Discourse Treebank (Prasad et al., 2008), in which pairs of sentences are annotated with an optional discourse connective and a sense that best explains the discourse relation between them. The annotation was done over a set of Wall Street Journal articles. Each relation, either explicit or implicit, consists of two arguments, typically composed of short phrases and an associated sense. In the case of explicit relations, a connective is present in the 136 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 136–142, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
K16-2019,D09-1036,0,0.0745645,"Missing"
K16-2019,K15-2002,0,0.15171,"Missing"
K16-2019,P14-5010,0,0.00677989,"t al., 2013a). Instead of using word sequences as input to train the embeddings, we use event chains extracted by connecting events with co-referencing entities. Each entity has a chain of events and each event is represented in a form of verb and dependency pairs. Specifically, we represent an event e as a pair e = (v, d) where v denotes a verb and d denotes a grammatical dependency relation between the verb and its entity. Vector representations for events are learned from chains of events extracted from a large corpus (we used the Wikipedia dump). To start, we use Stanford CoreNLP toolkit (Manning et al., 2014) to extract dependency trees and resolve co-referent entities from the corpus. For each entity in the co-reference chain, events are extracted by looking at the adjacent verb v in the dependency tree and its correspondent grammatical dependency relation d, creating tuples (v, d) as described above. This way, chains of the form ei , ..., ek are extracted and are used as inputs to the embedding training model. Similar to the Word2Vec skip-gram model (Mikolov et al., 2013a), we use the following objective function. T 1X J= T X t=1 −c≤j≤c,j6=0 inputs, event chains are used as input for generating"
K16-2019,K15-2001,0,0.113842,"Missing"
K16-2019,N13-1090,0,0.202121,"at was previously spent on such systems. Neural-network-based classifiers present a different and less explored approach to the discourse sense problem, which can potentially lead to considerable improvement. Our system, described in this paper, takes a step in this direction. We explore different input representation types and introduce event vectors for this task. Following the work of (Chambers and Jurafsky, 2009), we look into event chains as a way to represent structure in the discourse arguments. Then, we adapt the skip-gram approach originally used to learn word vectors from sentences (Mikolov et al., 2013b) to learn event vector representations from event sequences. To do so, we draw a clear analogy between words and events, as well as between sentences and event chains. Finally, each input relation is represented with the pre-trained event and word vectors of its arguments and a multi-layer neural network is used to classify senses. Predicting the sense of a discourse relation is particularly challenging when connective markers are missing. To address this challenge, we propose a simple deep neural network approach that replaces manual feature extraction by introducing event vectors as an alt"
K16-2019,K16-2001,0,0.109446,"Missing"
K16-2019,W14-1606,0,0.020613,"ument pairs and there is a direct mapping to a finite and known set of labels. We use two different classifiers for sense identification: a SVM classifier with linear kernel for explicit relations that uses state-of-the-art features and a multi-layer neural network for the implicit relations, which is the main focus of our submission. The following sections describe each of the systems in detail. 2.1 we introduce the notion of an event to discourse parsing, inspired by the work of (Chambers and Jurafsky, 2009) as a way to represent structured knowledge and long range dependencies. Similar to (Modi and Titov, 2014; Pichotta and Mooney, 2016) we embed the event representation in a low dimension continuous space. More details on the definition of events and the derivation of the event vectors are given in section 2.2.1. The sense classification task is defined over two arguments. Each argument is represented as two single vectors: a series of concatenated event vectors and a series of concatenated word vectors. A multi-layer neural network architecture receives these inputs to predict senses. The specifications of the architecture used are outlined in section 2.3. Explicit Discourse Relations Explicit di"
K16-2019,P09-2004,0,0.0293737,"atenated event vectors and a series of concatenated word vectors. A multi-layer neural network architecture receives these inputs to predict senses. The specifications of the architecture used are outlined in section 2.3. Explicit Discourse Relations Explicit discourse relation detection depends on identifying explicit discourse connectives. In the sense classification task, the connective and the two corresponding arguments are supplied, therefore, we trained a linear SVM multi-class classifier to choose from 14 different senses. We used the syntactic features described in (Lin et al., 2009; Pitler and Nenkova, 2009). We also used the connective string, PoS tags, the connective’s previous word and PoS tag from Lin’s features in our classifier. The features described in (Pitler and Nenkova, 2009) are extracted using constituency parse trees and consist of self-category, parent-category, left-siblingcategory and right-sibling category. (Pitler and Nenkova, 2009) has shown that using only the syntactic features, ignoring the identity of the connective gives better result. As the discourse usage of a connective may strongly rely on the syntactic context it appeared, we have added Pitler’s pairwise interaction"
K16-2019,P09-1077,0,0.400459,"To alleviate some of these problems, we looked for a representation that can capture a higher level of abstraction of the input arguments. We propose to represent arguments as a set of events and use pre-trained event embeddings to facilitate this Implicit Discourse Relations Sense classification for implicit discourse relations is notoriously hard. For this reason, we focus our efforts on this task, and explore several types of input representation and neural net architectures to deal with the challenges. We move from the simple lexical representation of word pairs used in (Lin et al., 2009; Pitler et al., 2009), and explore the benefits of using pre-trained word vectors (Mikolov et al., 2013b) to capture combinations and similarities. Finally, 137 Class Comparison.Concession Comparison.Contrast Contingency.Cause.Reason Contingency.Cause.Result Contingency.Condition EntRel Expansion.Alternative Expansion.Alternative.Chosen alternative Expansion.Conjunction Expansion.Instantiation Expansion.Restatement Temporal.Asynchronous.Precedence Temporal.Asynchronous.Succession Temporal.Synchrony Total Words 0.000 0.022 0.256 0.036 0.637 0.000 0.520 0.000 0.173 0.000 0.000 0.000 0.315 Implicit Words + Events Num"
N09-1034,P07-1083,0,0.224595,"performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target la"
N09-1034,P07-1036,1,0.868408,"r supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b) that show how the feature representation of a word pair"
N09-1034,P08-2014,1,0.932369,"hes and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions ab"
N09-1034,D08-1037,1,0.91608,"hes and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions ab"
N09-1034,N06-1041,0,0.0335088,"del is typically done under supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b) that show how the feature represent"
N09-1034,P08-1045,0,0.0379901,"Missing"
N09-1034,C00-1056,0,0.0366664,"esource in conjunction with constraints provided us with a robust transliteration system which significantly outperforms existing unsupervised approaches and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in"
N09-1034,N06-1011,1,0.682814,"uages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 1 Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language while preserving its pronunciation in the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998). It might initially seem that transliteration is an easy task, requiring only finding a phonetic mapping between character sets. However simply matching every source language character to its target language counterpart is not likely to work well as in practice this mapping depends on the context the 299 characters appear in and on transliteration conventions which may change across domains. As a result, current approaches employ machine learning methods which, given enough labeled training data learn how to determine whether a pair of words const"
N09-1034,P06-1103,1,0.68478,"uages - Chinese, Russian and Hebrew. Our experiments show that constraint driven learning can significantly outperform existing unsupervised models and achieve competitive results to existing supervised models. 1 Introduction Named entity (NE) transliteration is the process of transcribing a NE from a source language to some target language while preserving its pronunciation in the original language. Automatic NE transliteration is an important component in many cross-language applications, such as Cross-Lingual Information Retrieval (CLIR) and Machine Translation(MT) (Hermjakob et al., 2008; Klementiev and Roth, 2006a; Meng et al., 2001; Knight and Graehl, 1998). It might initially seem that transliteration is an easy task, requiring only finding a phonetic mapping between character sets. However simply matching every source language character to its target language counterpart is not likely to work well as in practice this mapping depends on the context the 299 characters appear in and on transliteration conventions which may change across domains. As a result, current approaches employ machine learning methods which, given enough labeled training data learn how to determine whether a pair of words const"
N09-1034,P04-1021,0,0.051568,"ing this simple resource in conjunction with constraints provided us with a robust transliteration system which significantly outperforms existing unsupervised approaches and achieves comparable performance to supervised methods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for"
N09-1034,W06-1616,0,0.0203426,"ss data to converge. Training the transliteration model is typically done under supervised settings (Bergsma and Kondrak, 2007; Goldwasser and Roth, 2008b), or weakly supervised settings with additional temporal information (Sproat et al., 2006; Klementiev and Roth, 2006a). Our work differs from these works in that it is completely unsupervised and makes no assumptions about the training data. Incorporating knowledge encoded as constraints into learning problems has attracted a lot of attention in the NLP community recently. This has been shown both in supervised settings (Roth and Yih, 2004; Riedel and Clarke, 2006) and unsupervised settings (Haghighi and Klein, 2006; Chang et al., 2007) in which constraints are used to bootstrap the model. (Chang et al., 2007) describes an unsupervised training of a Constrained Conditional Model (CCM), a general framework for combining statistical models with declarative constraints. We extend this work to include constraints over possible assignments to latent variables which, in turn, define the underlying representation for the learning problem. In the transliteration community there are several works (Ristad and Yianilos, 1998; Bergsma and Kondrak, 2007; Goldwasser"
N09-1034,P06-1010,0,0.166253,"ethods. The rest of the paper is organized as follows. Sec. 2 briefly examines more related work. Sec. 3 explains our model and Sec. 4 provide a linguistic intuition for it. Sec. 5 describes our experiments and evaluates our results followed by sec. 6 which concludes our paper. 2 Related Works Transliteration methods typically fall into two categories: generative approaches (Li et al., 2004; Jung et al., 2000; Knight and Graehl, 1998) that try to produce the target transliteration given a source language NE, and discriminative approaches (Goldwasser and Roth, 2008b; Bergsma and Kondrak, 2007; Sproat et al., 2006; Klementiev and Roth, 2006a), that try to identify the correct translitera300 tion for a word in the source language given several candidates in the target language. Generative methods encounter the Out-Of-Vocabulary (OOV) problem and require substantial amounts of training data and knowledge of the source and target languages. Discriminative approaches, when used to for discovering NE in a bilingual corpora avoid the OOV problem by choosing the transliteration candidates from the corpora. These methods typically make very little assumptions about the source and target languages and require c"
N09-1034,W06-1630,0,0.155505,"f the temporal information. Our best model, the unsupervised learning with all constraints, outperforms both models in (Klementiev and Roth, 2006b), even though we do not use any temporal information. guages: Russian, Chinese, and Hebrew, and compared our results to previously published results. 5.1 Experimental Settings In our experiments the system is evaluated on its ability to correctly identify the gold transliteration for each source word. We evaluated the system’s performance using two measures adopted in many transliteration works. The first one is Mean Reciprocal Rank (MRR), used in (Tao et al., 2006; Sproat et al., 2006), which is the average of the multiplicative inverse of the rank of the correct answer. Formally, Let n be the number of source NEs. Let GoldRank(i) be the rank the algorithm assigns to the correct transliteration. Then, MRR is defined by: n MRR = 1 1X . n goldRank(i) i=1 Another measure is Accuracy (ACC) used in (Klementiev and Roth, 2006a; Goldwasser and Roth, 2008a), which is the percentage of the top rank candidates being the gold transliteration. In our implementation we used the support vector machine (SVM) learning algorithm with linear kernel as our underlying lea"
N09-1034,P07-1015,0,0.0372154,"e we have in these resources - the romanization table is a noisy mapping covering the character set and is therefore better suited as a feature. Constraints, represented by pervasive, correct character mapping, indicate the sound mapping tendency between source and target languages. For example, certain n-gram phonemic mappings, such as r → l 303 to Chinese transliteration (see Sec. 3.2 for more details). Constraints in boldface apply to all positions, the rest apply only to characters appearing in initial position. These patterns have been used by other systems as features or pseudofeatures (Yoon et al., 2007). However, in our system these language specific ruleof-thumbs are systematically used as constraints to exclude impossible alignments and therefore generate better features for learning. We listed in Table 1 all 20 language specific constraints we used for Chinese. There is a total of 24 constraints for Hebrew and 17 for Russian. The constraints in Table 1 indicate a systematic sound mapping between English and Chinese unigram character mappings. Arranged by manners of articulation each row of the table indicates the sound change tendency among vowels, nasals, approximants (retroflex and glid"
N09-1034,W04-2401,1,\N,Missing
N09-1034,J98-4003,0,\N,Missing
N10-1066,D08-1031,1,0.109195,"idden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the features include a WordNet based metric (WNSim), indicators for the POS tags and negation identifiers. We used the state-of-the-art coreference resolution system of (Bengtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.6"
N10-1066,P07-1083,0,0.0229497,"operties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning ta"
N10-1066,N09-1034,1,0.852715,"hus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constr"
N10-1066,2008.amta-papers.4,0,0.0547444,"atent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and provides a general platform for a wide range of NLP tasks. The optimization procedure in this work and (Felzenszwalb et al., 2009) are quite different. We use the coordinate descent and cutting-plane methods ensuring we have fewer parameters and the inference procedure can be easily parallelized. Our procedure also allows different loss functions. (Cherry and Quirk, 2008) adopts the Latent SVM algorithm to define a language model. Unfortunately, their implementation is not guaranteed to converge. In CRF-like models with latent variables (McCal436 lum et al., 2005), the decision function marginalizes over the all hidden states when presented with an input example. Unfortunately, the computational cost of applying their framework is prohibitive with constrained latent representations. In contrast, our framework requires only the best hidden representation instead of marginalizing over all possible representations, thus reducing the computational effort. 7 Conclu"
N10-1066,P09-1053,0,0.174568,"ing into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation. Our Learning Constrained Latent Represent"
N10-1066,C04-1051,0,0.692767,"intermediate representation. We evaluate our algorithm on three different NLP tasks – transliteration, paraphrase identification and textual entailment – and show that our joint method significantly improves performance. 1 Introduction Many NLP tasks can be phrased as decision problems over complex linguistic structures. Successful learning depends on correctly encoding these (often latent) structures as features for the learning system. Tasks such as transliteration discovery (Klementiev and Roth, 2008), recognizing textual entailment (RTE) (Dagan et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by definin"
N10-1066,P08-2014,1,0.934205,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,D08-1037,1,0.861012,"termediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show in this paper, this results in degraded performance for the actual classification task at hand. Several works have considered this issue (McCallum et al., 2005; Goldwasser and Roth, 2008b; Chang et al., 2009; Das and Smith, 2009); however, they provide solutions 1 In this paper, the phrases “intermediate representation” and “latent representation” are used interchangeably. 429 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 429–437, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics that do not easily generalize to new tasks. In this paper, we propose a unified solution to the problem of learning to make the classification decision jointly with determining the intermediate representation"
N10-1066,H05-1049,0,0.0443099,"a and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There are several exceptions to the two-stage approach in the NLP community (Haghighi et al., 2005; McCallum et al., 2005; Goldwasser and Roth, 2008b; Das and Smith, 2009); however, the intermediate representation and the inference for constructing it are closely coupled with the application task. In contrast, LCLR provides a general formulation that allows the use of expressive constraints, making it applicable to many NLP tasks. Unlike other latent variable SVM frameworks (Felzenszwalb et al., 2009; Yu and Joachims, 2009) which often use task-specific inference procedure, LCLR utilizes the declarative inference framework that allows using constraints over intermediate representation and"
N10-1066,D08-1084,0,0.222611,"n et al., 2006) and paraphrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning in"
N10-1066,J08-2005,1,0.216571,"b). 5.2 Textual Entailment Recognizing Textual Entailment (RTE) is an important textual inference task of predicting if a given text snippet, entails the meaning of another (the hypothesis). In many current RTE systems, the entailment decision depends on successfully aligning the constituents of the text and hypothesis, accounting for the internal linguistic structure of the input. The raw input – the text and hypothesis – are represented as directed acyclic graphs, where vertices correspond to words. Directed edges link verbs to the head words of semantic role labeling arguments produced by (Punyakanok et al., 2008). All other words are connected by dependency edges. The intermediate representation is an alignment between the nodes and edges of the graphs. We used three hidden variable types from Section 4 – wordmapping, word-deletion and edge-mapping, along with the associated constraints as defined earlier. Since the text is typically much longer than the hypothesis, we create word-deletion latent variables (and features) only for the hypothesis. The second column of Table 2 lists the resources used to generate features corresponding to each hidden variable type. For word-mapping variables, the feature"
N10-1066,W06-1603,0,0.101349,"e learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – specifying the latent representation, and then extracting features for learning. The latent representation is obtained by an inference process using predefined models or welldesigned heuristics. While these approaches often perform well, they ignore a useful resource when generating the latent structure – the labeled data for the final learning task. As we will show"
N10-1066,W04-3219,0,0.0114581,"ence formulation, which makes it easy to define the intermediate representation and to inject knowledge in the form of constraints. While ILP has been applied to structured output learning, to the best of our knowledge, this is the first work that makes use of ILP in formalizing the general problem of learning intermediate representations. 2 Preliminaries We introduce notation using the Paraphrase Identification task as a running example. This is the bi430 nary classification task of identifying whether one sentence is a paraphrase of another. A paraphrase pair from the MSR Paraphrase corpus (Quirk et al., 2004) is shown in Figure 1. In order to identify that the sentences paraphrase each other , we need to align constituents of these sentences. One possible alignment is shown in the figure, in which the dotted edges correspond to the aligned constituents. An alignment can be specified using binary variables corresponding to every edge between constituents, indicating whether the edge is included in the alignment. Different activations of these variables induce the space of intermediate representations. The notification was first reported Friday by MSNBC. MSNBC.com first reported the CIA request on F"
N10-1066,P09-2015,1,0.892648,"hrase identification (Dolan et al., 2004) are a few prototypical examples. However, the input to such problems does not specify the latent structures and the problem is defined in terms of surface forms only. Most current solutions transform the raw input into a meaningful intermediate representation1 , and then encode its structural properties as features for the learning algorithm. Consider the RTE task of identifying whether the meaning of a short text snippet (called the hypothesis) can be inferred from that of another snippet (called the text). A common solution (MacCartney et al., 2008; Roth et al., 2009) is to begin by defining an alignment over the corresponding entities, predicates and their arguments as an intermediate representation. A classifier is then trained using features extracted from the intermediate representation. The idea of using a intermediate representation also occurs frequently in other NLP tasks (Bergsma and Kondrak, 2007; Qiu et al., 2006). While the importance of finding a good intermediate representation is clear, emphasis is typically placed on the later stage of extracting features over this intermediate representation, thus separating learning into two stages – spec"
N10-1066,U06-1019,0,0.169702,"ngtson and Roth, 2008) to identify the canonical entities for pronouns and extract features accordingly. For word deletion, we use only the POS tags of the corresponding tokens (generated by the LBJ POS tagger3 ) to generate features. For edge 3 http://L2R.cs.uiuc.edu/˜cogcomp/software.php Hidden Variable word-mapping word-deletion edge-mapping RTE features WordNet, POS, Coref, Neg POS NODE-INFO edge-deletion N/A Paraphrase features WordNet, POS, NE, ED POS, NE NODE-INFO, DEP DEP Paraphrase System Acc Experiments using (Dolan et al., 2004) (Qiu et al., 2006) 72.00 (Das and Smith, 2009) 73.86 (Wan et al., 2006) 75.60 Alignment + Learning 76.23 LCLR 76.41 Experiments using Extended data set Alignment + Learning 72.00 LCLR 72.75 Table 2: Summary of latent variables and feature resources for the entailment and paraphrase identification tasks. See Section 4 for an explanation of the hidden variable types. The linguistic resources used to generate features are abbreviated as follows – POS: Part of speech, Coref: Canonical coreferent entities; NE: Named Entity, ED: Edit distance, Neg: Negation markers, DEP: Dependency labels, NODE-INFO: corresponding node alignment resources, N/A: Hidden variable not used"
N10-1066,P06-1051,0,0.0183547,"hat the similarity in performance between the joint LCLR algorithm and the two stage 4 Previous work (Das and Smith, 2009) has shown that combining the results of several systems improves performance. (Alignment + Learning) systems is due to the limited intermediate representation space for input pairs in this dataset. We evaluated these systems on the more difficult Extended dataset. Results indeed show that the margin between the two systems increases as the inference problem becomes harder. 6 Related Work Recent NLP research has largely focused on twostage approaches. Examples include RTE (Zanzotto and Moschitti, 2006; MacCartney et al., 2008; Roth et al., 2009); string matching (Bergsma and Kondrak, 2007); transliteration (Klementiev and Roth, 2008); and paraphrase identification (Qiu et al., 2006; Wan et al., 2006). (MacCartney et al., 2008) considered constructing a latent representation to be an independent task and used manually labeled alignment data (Brockett, 2007) to tune the inference procedure parameters. While this method identifies alignments well, it does not improve entailment decisions. This strengthens our intuition that the latent representation should be guided by the final task. There a"
N10-1066,W07-1401,0,\N,Missing
N19-1195,N16-1089,0,0.126554,"icial agent capable of understanding and executing human instructions is one of the oldest long-standing AI challenges (Winograd, 1972). This problem has numerous applications in various domains (planning, navigation and assembly) and can help accommodate seamless interaction with personal assistants in many environments. Due to its central role in AI and wide applicability, this problem has seen a surge of interest recently (MacMahon et al., 2006; Branavan et al., 2009; Chen and Mooney, 2011; Tellex et al., 2011; Matuszek et al., 2012; Kim and Mooney, 2013; Misra et al., 2017). Recent works (Bisk et al., 2016; Tan and Bansal, 2018) focus on exploring deep learning methods for grounding spatial language. In this popular setup, human communication with robots is viewed as a single-step process, in which a natural language (NL) instruction is provided, and an outcome is observed. Our goal in this paper is to explore different approaches for relaxing the single step assumption, and present initial results which we hope would motivate future work in this direction. Similar to interactive dialog systems (Allen et al., 1995; Rybski et al., 2007; Wen et al.), we view this problem as an interactive process"
N19-1195,P09-1010,0,0.0211425,"n. The ‘x’ represents an incorrect prediction, corrected by the provided advice (lower sentence). Introduction The problem of constructing an artificial agent capable of understanding and executing human instructions is one of the oldest long-standing AI challenges (Winograd, 1972). This problem has numerous applications in various domains (planning, navigation and assembly) and can help accommodate seamless interaction with personal assistants in many environments. Due to its central role in AI and wide applicability, this problem has seen a surge of interest recently (MacMahon et al., 2006; Branavan et al., 2009; Chen and Mooney, 2011; Tellex et al., 2011; Matuszek et al., 2012; Kim and Mooney, 2013; Misra et al., 2017). Recent works (Bisk et al., 2016; Tan and Bansal, 2018) focus on exploring deep learning methods for grounding spatial language. In this popular setup, human communication with robots is viewed as a single-step process, in which a natural language (NL) instruction is provided, and an outcome is observed. Our goal in this paper is to explore different approaches for relaxing the single step assumption, and present initial results which we hope would motivate future work in this directi"
N19-1195,H89-1033,0,0.643157,"ignificant performance improvements. To help reduce the effort involved in supplying the advice, we also explore model self-generated advice which can still improve results. 1 x The target is in the lower left. Figure 1: Based on the instruction (upper sentence) the model predicts the coordinates of the block and its target location. The ‘x’ represents an incorrect prediction, corrected by the provided advice (lower sentence). Introduction The problem of constructing an artificial agent capable of understanding and executing human instructions is one of the oldest long-standing AI challenges (Winograd, 1972). This problem has numerous applications in various domains (planning, navigation and assembly) and can help accommodate seamless interaction with personal assistants in many environments. Due to its central role in AI and wide applicability, this problem has seen a surge of interest recently (MacMahon et al., 2006; Branavan et al., 2009; Chen and Mooney, 2011; Tellex et al., 2011; Matuszek et al., 2012; Kim and Mooney, 2013; Misra et al., 2017). Recent works (Bisk et al., 2016; Tan and Bansal, 2018) focus on exploring deep learning methods for grounding spatial language. In this popular setup"
N19-1195,P13-1022,0,0.0322691,"tence). Introduction The problem of constructing an artificial agent capable of understanding and executing human instructions is one of the oldest long-standing AI challenges (Winograd, 1972). This problem has numerous applications in various domains (planning, navigation and assembly) and can help accommodate seamless interaction with personal assistants in many environments. Due to its central role in AI and wide applicability, this problem has seen a surge of interest recently (MacMahon et al., 2006; Branavan et al., 2009; Chen and Mooney, 2011; Tellex et al., 2011; Matuszek et al., 2012; Kim and Mooney, 2013; Misra et al., 2017). Recent works (Bisk et al., 2016; Tan and Bansal, 2018) focus on exploring deep learning methods for grounding spatial language. In this popular setup, human communication with robots is viewed as a single-step process, in which a natural language (NL) instruction is provided, and an outcome is observed. Our goal in this paper is to explore different approaches for relaxing the single step assumption, and present initial results which we hope would motivate future work in this direction. Similar to interactive dialog systems (Allen et al., 1995; Rybski et al., 2007; Wen e"
N19-1195,D17-1106,0,0.123293,"he problem of constructing an artificial agent capable of understanding and executing human instructions is one of the oldest long-standing AI challenges (Winograd, 1972). This problem has numerous applications in various domains (planning, navigation and assembly) and can help accommodate seamless interaction with personal assistants in many environments. Due to its central role in AI and wide applicability, this problem has seen a surge of interest recently (MacMahon et al., 2006; Branavan et al., 2009; Chen and Mooney, 2011; Tellex et al., 2011; Matuszek et al., 2012; Kim and Mooney, 2013; Misra et al., 2017). Recent works (Bisk et al., 2016; Tan and Bansal, 2018) focus on exploring deep learning methods for grounding spatial language. In this popular setup, human communication with robots is viewed as a single-step process, in which a natural language (NL) instruction is provided, and an outcome is observed. Our goal in this paper is to explore different approaches for relaxing the single step assumption, and present initial results which we hope would motivate future work in this direction. Similar to interactive dialog systems (Allen et al., 1995; Rybski et al., 2007; Wen et al.), we view this"
N19-1403,D18-1454,0,0.0616157,"2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017). While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled. Model Architecture Our model consists of three separate systems, one for each step, namely, the stand-alone question answering (QA) system, the Natural Language Inference (NLI) system and the inference framework connecting the two. First, we assign a true/false label to each question-choice pair using the standalone QA syst"
N19-1403,D15-1075,0,0.0299342,"ained using Bi-LSTMs. The hidden states of the Bi-LSTM are concatenated to generate the representation. This part of the model is similar to TriAN model proposed in Wang et al. (2018). The choice representations of ci and cj are passed as input to the decomposable attention layer proposed in Parikh et al. (2016). The architecture of the joint model is shown in figure 2. 3.4 Training We train the stand-alone QA system using the MultiRC and SemEval datasets for respective experiments. We experiment with 2 different training settings for the NLI system. In the first setting, we use SNLI dataset (Bowman et al., 2015) to train the NLI system. The sequence-attention layer is left untrained during this phase. Hence, we only use the answer choice and do not consider the question for NLI detection. Self-Training: Subsequently, to help the system adapt to our settings, we devise a self-training protocol over the RC datasets to train the NLI sys4012 tem. Self-training examples for the NLI system were obtained using the following procedure: if the SNLI-trained NLI model predicted entailment and the gold labels of the ordered choice pair were true-true, then the choice pair is labeled as entailment. Similarly, if"
N19-1403,P17-2057,0,0.0193742,"(2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017). While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled. Model Architecture Our model consists of three separate systems, one for each step, namely, the stand-alone question answering (QA) system, the Natural Language Inference (NLI) system and the inference framework connecting the two. First, we assign a true/false label to each que"
N19-1403,N18-1023,0,0.0539272,"o leverage such inferences for machine comprehension. Our approach contains three steps. First, we use a stand-alone QA system to classify the answer choices as true/false. Then, we classify the relation between each pair of choices for a given question as entailment, contradiction or neutral. Finally, we re-evaluate the labels assigned to choices using an Integer Linear Programming based inference procedure. We discuss different training protocols and representation choices for the combined decision problem. An overview is in figure 1. We empirically evaluate on two recent datasets, MultiRC (Khashabi et al., 2018) and SemEval4010 Proceedings of NAACL-HLT 2019, pages 4010–4015 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2018 task-11 (Ostermann et al., 2018) and show that it improves machine comprehension in both. 3 Model Formally, the task of machine comprehension can be defined as: given text P and a set of n related questions Q = {q1 , q2 , . . . , qn } each having m choices C = {ci1 , ci2 , . . . , cim }∀qi ∈ Q, the task is to assign true/false value for each choice cij . 3.1 Figure 1: Proposed Approach 2 Related Work Recently, several QA datasets h"
N19-1403,J81-4005,0,0.646714,"Missing"
N19-1403,E17-1001,0,0.0225608,"ions Q = {q1 , q2 , . . . , qn } each having m choices C = {ci1 , ci2 , . . . , cim }∀qi ∈ Q, the task is to assign true/false value for each choice cij . 3.1 Figure 1: Proposed Approach 2 Related Work Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016). Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences. Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et"
N19-1403,D18-1260,0,0.0122823,"o test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016). Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences. Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017). While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies bet"
N19-1403,P18-1076,0,0.0226775,"(2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017). While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled. Model Architecture Our model consists of three separate systems, one for each step, namely, the stand-alone question answering (QA) system, the Natural Language Inference (NLI) system and the inference framework connecting the two. First, we assign a true/false label to each question-choice pair using th"
N19-1403,P18-1160,0,0.0834436,". . , qn } each having m choices C = {ci1 , ci2 , . . . , cim }∀qi ∈ Q, the task is to assign true/false value for each choice cij . 3.1 Figure 1: Proposed Approach 2 Related Work Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016). Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences. Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylo"
N19-1403,C18-1198,0,0.0675255,"Missing"
N19-1403,S18-1119,0,0.0191224,"elation between each pair of choices for a given question as entailment, contradiction or neutral. Finally, we re-evaluate the labels assigned to choices using an Integer Linear Programming based inference procedure. We discuss different training protocols and representation choices for the combined decision problem. An overview is in figure 1. We empirically evaluate on two recent datasets, MultiRC (Khashabi et al., 2018) and SemEval4010 Proceedings of NAACL-HLT 2019, pages 4010–4015 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2018 task-11 (Ostermann et al., 2018) and show that it improves machine comprehension in both. 3 Model Formally, the task of machine comprehension can be defined as: given text P and a set of n related questions Q = {q1 , q2 , . . . , qn } each having m choices C = {ci1 , ci2 , . . . , cim }∀qi ∈ Q, the task is to assign true/false value for each choice cij . 3.1 Figure 1: Proposed Approach 2 Related Work Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016). Yatskar (2018) showed that a high perf"
N19-1403,D16-1244,0,0.0733711,"Missing"
N19-1403,D14-1162,0,0.0868965,"use the dependencies between different questions about the same paragraph. 3.3 Figure 2: Architecture of the Joint Model representation of both the answer choice and the question. Sequence-attention is defined in Wang n et al. (2018) as: X Attseq (u, {vi }ni=1 ) = αi vi (1) i=1 αi = sof tmaxi (f (W1 u)T f (W1 vi )) where u and vi are word embeddings, W1 is the associated weight parameter and f is non-linearity. Self-attention is Attseq of a vector onto itself. The embedding of each word in the answer choice is attended to by the sequence of question word embeddings. We use pre-trained GloVe (Pennington et al., 2014) embeddings to represent the words. The question-attended choices are then passed through the decomposable-attention layer proposed in Parikh et al. (2016). 3.2.1 Inference using DRAIL We use Deep Relational Learning (DRaiL) framework proposed by Zhang et al. (2016) to perform Joint Model The design of our joint model is motivated by the two objectives: 1) to obtain a better representation for the question-choice pair for NLI detection and 2) to leverage the benefit of multitask learning. Hence, in the joint model, choice representation from stand-alone QA system is input to the decomposable-a"
N19-1403,D13-1020,0,0.0157518,"ages 4010–4015 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2018 task-11 (Ostermann et al., 2018) and show that it improves machine comprehension in both. 3 Model Formally, the task of machine comprehension can be defined as: given text P and a set of n related questions Q = {q1 , q2 , . . . , qn } each having m choices C = {ci1 , ci2 , . . . , cim }∀qi ∈ Q, the task is to assign true/false value for each choice cij . 3.1 Figure 1: Proposed Approach 2 Related Work Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016). Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences. Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inf"
N19-1403,P16-2079,0,0.0261649,"ommonsense inferences. Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017). While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled. Model Architecture Our model consists of three separate systems, one for each step, namely, the stand-alone question answering (QA) system, the Natural Language Inference (NLI) system and the inferenc"
N19-1403,C18-1069,0,0.0189771,"Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There have been several attempts in literature to use inferences to answer questions. Most of the previous works either attempt to infer the answer from the given text (Sachan and Xing, 2016; Sun et al., 2018) or an external commonsense knowledge base (Das et al., 2017; Mihaylov and Frank, 2018; Bauer et al., 2018; Weissenborn et al., 2017). While neural models can capture some dependencies between choices through shared representations, to the best of our knowledge, inferences capturing the dependencies between answer choices or different questions have been not explicitly modeled. Model Architecture Our model consists of three separate systems, one for each step, namely, the stand-alone question answering (QA) system, the Natural Language Inference (NLI) system and the inference framework connect"
N19-1403,P16-1041,0,0.0186575,"2019 Association for Computational Linguistics 2018 task-11 (Ostermann et al., 2018) and show that it improves machine comprehension in both. 3 Model Formally, the task of machine comprehension can be defined as: given text P and a set of n related questions Q = {q1 , q2 , . . . , qn } each having m choices C = {ci1 , ci2 , . . . , cim }∀qi ∈ Q, the task is to assign true/false value for each choice cij . 3.1 Figure 1: Proposed Approach 2 Related Work Recently, several QA datasets have been proposed to test machine comprehension (Richardson, 2013; Weston et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016). Yatskar (2018) showed that a high performance on these datasets could be achieved without necessarily achieving the capability of making commonsense inferences. Trischler et al. (2016b), Kumar et al. (2016), Liu and Perez (2017), Min et al. (2018) and Xiong et al. (2016) proposed successful models on those datasets. To address this issue, new QA datasets which require commonsense reasoning have been proposed (Khashabi et al., 2018; Ostermann et al., 2018; Mihaylov et al., 2018). Using common sense inferences in Machine Comprehension is a far from solved problem. There"
N19-1403,S18-1120,0,0.0847591,"with an associated confidence score s2 . Then, we use a relational framework to perform inference using the information obtained from the stand-alone QA and the NLI systems. Each of the components is described in detail in the following sub-sections. We further propose a joint model whose parameters are trained jointly on both the tasks. The joint model uses the answer choice representation generated by the stand-alone QA system as input to the NLI detection system. The architecture of our joint model is shown in figure 2. 3.1.1 Stand-alone QA system We use the TriAN-single model proposed by Wang et al. (2018) for SemEval-2018 task-11 as our stand-alone QA system. We use the implementation1 provided by Wang et al. (2018) for our experiments. The system is a tri-attention model that takes passage-question-choice triplet as input and produces the probability of the choice being true as its output. 3.2 NLI System Our NLI system is inspired from decomposableattention model proposed by Parikh et al. (2016). We modified the architecture proposed in Parikh et al. (2016) to accommodate the question-choice pairs as opposed to sentence pairs in the original model. We added an additional sequence-attention la"
N19-1403,N19-1241,0,0.0259238,"Missing"
N19-1403,W16-5906,1,0.860291,"(1) i=1 αi = sof tmaxi (f (W1 u)T f (W1 vi )) where u and vi are word embeddings, W1 is the associated weight parameter and f is non-linearity. Self-attention is Attseq of a vector onto itself. The embedding of each word in the answer choice is attended to by the sequence of question word embeddings. We use pre-trained GloVe (Pennington et al., 2014) embeddings to represent the words. The question-attended choices are then passed through the decomposable-attention layer proposed in Parikh et al. (2016). 3.2.1 Inference using DRAIL We use Deep Relational Learning (DRaiL) framework proposed by Zhang et al. (2016) to perform Joint Model The design of our joint model is motivated by the two objectives: 1) to obtain a better representation for the question-choice pair for NLI detection and 2) to leverage the benefit of multitask learning. Hence, in the joint model, choice representation from stand-alone QA system is input to the decomposable-attention layer of the NLI system. The joint model takes two triplets (p, qi , ci ) and (p, qj , cj ) as input. It outputs a true/false for each choice and an NLI relation (entailment, contradiction or neutral) between the choices. The representations for passage, qu"
P08-2014,P06-1103,1,0.958269,"have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997). Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998). Unlike other approaches,our approach is based on minimizing the distance between the feature distribution of a comprehensive referenc"
P08-2014,P97-1017,0,0.114719,"ent in many linguistic applications such as machine translation and information retrieval, which require identifying out-of-vocabulary words. In our settings, we have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997). Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP application"
P08-2014,W06-1630,0,0.146807,"ntifying out-of-vocabulary words. In our settings, we have access to source language NE and the ability to label the data upon request. We introduce a new active sampling paradigm that aims to guide the learner toward informative samples, allowing learning from a small number of representative examples. After the data is obtained it is analyzed to identify repeating patterns which can be used to focus the training process of the model. Previous works usually take a generative approach, (Knight and Graehl, 1997). Other approaches exploit similarities in aligned bilingual corpora; for example, (Tao et al., 2006) combine two unsupervised methods. (Klementiev and Roth, 2006) bootstrap with a classifier used interchangeably with an unsupervised temporal alignment method. Although these approaches alleviate the problem of obtaining annotated data, other resources are still required, such as a large aligned bilingual corpus. The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998). Unlike other approaches,our approach is based on minimizing the distanc"
P11-1149,P09-1010,0,0.0651001,"interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different appro"
P11-1149,P07-1036,1,0.198445,"omingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as"
P11-1149,N09-1034,1,0.825093,"and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Re"
P11-1149,N03-1004,0,0.0122379,"iew, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to"
P11-1149,W10-2903,1,0.859398,"on top, preferably by a large margin to allow generalization.The structured learning algorithm can directly use the top ranking predictions of the model (line 8 in Alg. 1) as training data. In this case the underlying algorithm is a structural SVM with squared-hinge loss, using hamming distance as the distance function. We use the cuttingplane method to efficiently optimize the learning process’ objective function. 4 Model Semantic parsing as formulated in Eq. 1 is an inference procedure selecting the top ranked output logical formula. We follow the inference approach in (Roth and Yih, 2007; Clarke et al., 2010) and formalize this process as an Integer Linear Program (ILP). Due to space consideration we provide a brief description, and refer the reader to that paper for more details. Combined The two approaches defined above capture different views of the data, a natural question is then - can these two measures be combined to provide a more powerful estimation? We suggest a third approach which combines the first two approaches. It first uses the score produced by the latter approach 2 Without normalization longer sentences would have more to filter out unlikely candidates, and then ranks the influe"
P11-1149,W99-0613,0,0.0778258,"on: training the model using examples selected according to the model’s parameters (i.e., the top ranking structures) may not generalize much further beyond the existing model, as the training examples will simply reinforce the existing model. The statistics used for confidence estimation are different than those used by the model to create the output structures, and can therefore capture additional information unobserved by the prediction model. This assumption is based on the well established idea of multi-view learning, applied successfully to many NL applications (Blum and Mitchell, 1998; Collins and Singer, 1999). According to this idea if two models use different views of the data, each of them can enhance the learning process of the other. The success of our learning procedure hinges on finding good confidence measures, whose confidence prediction correlates well with the true quality of the prediction. The ability of unsupervised confidence estimation to provide high quality confidence predictions can be explained by the observation that prominent prediction patterns are more likely to be correct. If a non-random model produces a prediction pattern multiple times it is likely to be an indication of"
P11-1149,N04-4028,0,0.0109676,"riterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic pars"
P11-1149,W05-0602,0,0.271332,"result obtained by approximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et"
P11-1149,P06-1115,0,0.721507,"ged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learnin"
P11-1149,P09-1011,0,0.277238,"tracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011)"
P11-1149,P08-2055,0,0.0122683,"tion model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to the best of our knowledge. To compensat"
P11-1149,N06-1020,0,0.0106108,"direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works"
P11-1149,P06-2080,0,0.125091,"pproximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et a"
P11-1149,D09-1001,0,0.44694,"ervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al.,"
P11-1149,P07-1052,1,0.831409,"n unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mi"
P11-1149,P07-1078,1,0.786226,"n unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mi"
P11-1149,W10-2909,1,0.808796,"Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised learning algorithm for semantic parsing, the first for this task to the best of our knowledge. To compensate for the lack of training data we use a self-training protocol, driven by unsupervised confidence estimation. We demonstrate empirically that our approach results in a high preforming semantic parser and show that confidence"
P11-1149,P07-1076,0,0.0342205,"les for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to return the best performing model. 7 Conclusions We introduced an unsupervised lea"
P11-1149,W00-1317,0,0.0141848,"result obtained in any of the learning algorithm iterations (Best), the result obtained by approximating the best result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefor"
P11-1149,P11-1145,0,0.0349398,"y, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A"
P11-1149,P10-1098,0,0.159121,"rs. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation"
P11-1149,J07-1003,0,0.00798533,"timate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process, thus allowing our training procedure to ret"
P11-1149,N06-1056,0,0.367679,"result using the averaged prediction confidence (Conf. estim.) and the result of using the default convergence criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using"
P11-1149,P07-1121,0,0.586562,"vert NL into a formal MR has countless applications. The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling). We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language. Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. 1486 Current approaches for this task take a data driven approach (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately annotating sentences with their MR is a time consuming task which requi"
P11-1149,P95-1026,0,0.3364,"al step in this direction and suggest an unsupervised protocol. Other approaches to unsupervised semantic analysis (Poon and Domingos, 2009; Titov and Klementiev, 2011) take a different approach to semantic representation, by clustering semantically equivalent dependency tree fragments, and identifying their predicate-argument structure. While these approaches have been applied successfully to semantic tasks such as question answering, they do not ground the input in a well defined output language, an essential component in our task. Our unsupervised approach follows a self training protocol (Yarowsky, 1995; McClosky et al., 2006; Reichart and Rappoport, 2007b) enhanced with constraints restricting the output space (Chang et al., 2007; Chang et al., 2009). A Self training protocol uses its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored"
P11-1149,W06-1604,0,0.0219323,"es its own predictions for training. We estimate the quality of the predictions and use only high confidence examples for training. This selection criterion provides an additional view, different than the one used by the prediction model. Multi-view learning is a well established idea, implemented in methods such as co-training (Blum and Mitchell, 1998). Quality assessment of a learned model output was 1494 explored by many previous works (see (Caruana and Niculescu-Mizil, 2006) for a survey), and applied to several NL processing tasks such as syntactic parsing (Reichart and Rappoport, 2007a; Yates et al., 2006), machine translation (Ueffing and Ney, 2007), speech (Koo et al., 2001), relation extraction (Rosenfeld and Feldman, 2007), IE (Culotta and McCallum, 2004), QA (Chu-Carroll et al., 2003) and dialog systems (Lin and Weng, 2008). In addition to sample selection we use confidence estimation as a way to approximate the overall quality of the model and use it for model selection. This use of confidence estimation was explored in (Reichart et al., 2010), to select between models trained with different random starting points. In this work we integrate this estimation deeper into the learning process"
P11-1149,D07-1071,0,0.903992,"reasons, as the ability to convert NL into a formal MR has countless applications. The term semantic parsing has been used ambiguously to refer to several semantic tasks (e.g., semantic role labeling). We follow the most common definition of this task: finding a mapping between NL input and its interpretation expressed in a welldefined formal MR language. Unlike shallow semantic analysis tasks, the output of a semantic parser is complete and unambiguous to the extent it can be understood or even executed by a computer system. 1486 Current approaches for this task take a data driven approach (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), in which the learning algorithm is given a set of NL sentences as input and their corresponding MR, and learns a statistical semantic parser — a set of parameterized rules mapping lexical items and syntactic patterns to their MR. Given a sentence, these rules are applied recursively to derive the most probable interpretation. Since semantic interpretation is limited to the syntactic patterns observed in the training data, in order to work well these approaches require considerable amounts of annotated data. Unfortunately annotating sentences with their MR is a time co"
P11-1149,P09-1110,0,0.0810929,"nce criterion (Default). Results in parentheses are the result of using the U NIGRAM confidence to approximate the model’s performance. 6 Related Work Semantic parsing has attracted considerable interest in recent years. Current approaches employ various machine learning techniques for this task, such as Inductive Logic Programming in earlier systems (Zelle and Mooney, 1996; Tang and Mooney, 2000) and statistical learning methods in modern ones (Ge and Mooney, 2005; Nguyen et al., 2006; Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009). The difficulty of providing the required supervision motivated learning approaches using weaker forms of supervision. (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009; Titov and Kozhevnikov, 2010) ground NL in an external world state directly referenced by the text. The NL input in our setting is not restricted to such grounded settings and therefore we cannot exploit this form of supervision. Recent work (Clarke et al., 2010; Liang et al., 2011) suggest using response-based learning protocols, which alleviate some of the supervision effort. This work takes an additional ste"
P13-2082,D11-1131,0,0.0384776,"Missing"
P13-2082,N10-1066,1,0.841026,"ooney, 2008), describes robotic soccer events. The dataset was collected for the purpose of constructing semantic parsers from ambiguous supervision and consists of both “noisy” and gold labeled data. The noisy dataset Combined Model In order to consider both types of information we augment our decision model with the new variables, resulting in the following objective function (Eq. 2). P P Fw (x) = arg maxα,β c∈x s∈D αcs ·w1 T Φ1 (x, c, s)+ P P P T i j c,d∈x s,t∈D i,j βcsi ,dtj · w2 Φ2 (x, c, s , d, t ) + P P T T c∈x γc · w3 Φ3 (x, c) + c,d∈x δcd · w4 Φ4 (x, c, d) (2) 2 Details omitted, see (Chang et al., 2010) for more details. For example, a unary relation symbol for “He plays”, and a binary for “He plays with a ball”. 3 464 System Training Procedure D OM -I NIT P RED -A RGS C OMBINEDRL C OMBINEDRI+S w1 : Noisy probabilistic model, described below. Only w3 , w4 Trained over the Situ. dataset. w1 , w2 , w3 , w4 :learned from Robocup gold w3 , w4 : learned from the Situ. dataset, w1 uses the D OM -I NIT Robocup model. w3 , w4 : Initially learned over the Situ. dataset, updated jointly with w1 , w2 over Robocup gold C OMBINEDRL+S System P RED -A RGS D OM -I NIT C OMBINEDRI+S ¨ (B ORSCHINGER ET AL .,"
P13-2082,W10-2903,1,0.931959,"that the symbol t (associated with constituent d) is an argument of a function s (associated with constituent c). The overall inference problem (Eq. 1) is as follows: P P Fw (x) = arg maxα,β c∈x s∈D αcs · wT Φ1 (x, c, s) P P + c,d∈x s,t∈D βcs,dt · wT Φ2 (x, c, s, d, t) (1) pass(pink1, pink11) We restrict the possible assignments to the decision variables, forcing the resulting output formula to be syntactically legal, for example by restricting active β-variables to be type consistent, and forcing the resulting functional composition to be acyclic and fully connected (we refer the reader to (Clarke et al., 2010) for more details). We take advantage of the flexible ILP framework and encode these restrictions as global constraints. In order to overcome this difficulty, we suggest a flexible model that is able to leverage the supervision provided in one domain to learn an abstract intermediate layer, and show empirically that it learns a robust model, improving results significantly in a second domain. 2 Domain-Dependent Model Semantic Interpretation Model Our model consists of both domain-dependent (mapping between text and a closed set of symbols) and domain independent (abstract predicateargument str"
P13-2082,J02-3001,0,0.0402084,"w variables to semantic interpretation : This information is used to assist the overall learning process. We assume that these labels correspond to a binding to some logical symbol, and encode it as a constraint forcing the relations between the two models. Moreover, since learning this layer is a by-product of the learning process (as it does not use any labeled data) forcing the connection between the decisions is the mechanism that drives learning this model. Our domain-independent layer bears some similarity to other semantic tasks, most notably Semantic-Role Labeling (SRL) introduced in (Gildea and Jurafsky, 2002), in which identifying the predicate-argument structure is considered a preprocessing step, prior to assigning argument labels. Unlike SRL, which aims to identify linguistic structures alone, in our framework these structures capture both natural-language and domain-language considerations. ∀c ∈ x (γc → αc,s1 ∨ αc,s2 ∨ ... ∨ αc,sn ) ∀c ∈ x, ∀d ∈ x (δc,d → βc,s1 ,dt1 ∨βc,s2 ,dt1 ∨...∨βc,sn ,dtn ) (where n is the length of x). 2.3 Learning the Combined Model The supervision to the learning process is given via data consisting of pairs of sentences and (domain specific) semantic interpretation. G"
P13-2082,P11-1149,1,0.851705,"ar objective, which uses a vector w, mapping features to weights and a feature function Φ which maps the output decision to a feature vector. The output interpretation y is described using a subset of first order logic, consisting of typed constants (e.g., robotic soccer player), functions capturing relations between entities, and their properties (e.g., pass(x, y), where pass is a function symbol and x, y are typed arguments). We use data taken from two grounded domains, describing robotic soccer events and household situations. We begin by formulating the domain-specific process. We follow (Goldwasser et al., 2011; Clarke et al., 2010) and formalize semantic inference as an Integer Linear Program (ILP). Due to space consideration, we provide a brief description (see (Clarke et al., 2010) for more details). We then proceed to augment this model with domain-independent information, and connect the two models by constraining the ILP model. Features We use two types of feature, first-order Φ1 and second-order Φ2 . Φ1 depends on lexical information: each mapping of a lexical item c to a domain symbol s generates a feature. In addition each combination of a lexical item c and an symbol type generates a featu"
P13-2082,C10-2062,0,0.0239819,"Missing"
P13-2082,D10-1119,0,0.023594,"vel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains. 1 Introduction Natural Language (NL) understanding can be intuitively understood as a general capacity, mapping words to entities and their relationships. However, current work on automated NL understanding (typically referenced as semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Chen and Mooney, 2008; Kwiatkowski et al., 2010; B¨orschinger et al., 2011)) is restricted to a given output domain1 (or task) consisting of a closed set of meaning representation symbols, describing domains such as robotic soccer, database queries and flight ordering systems. In this work, we take a first step towards constructing a semantic interpreter that can leverage information from multiple tasks. This is not a straight forward objective – the domain specific nature of semantic interpretation, as described in the current literature, does not allow for an easy move between domains. For example, a system trained for the task of unders"
P13-2082,P07-1121,0,0.0297055,"nt and independent components, we suggest a novel interpretation model, which augments a domain dependent model with abstract information that can be shared by multiple domains. Our experiments show that this type of information is useful and can reduce the annotation effort significantly when moving between domains. 1 Introduction Natural Language (NL) understanding can be intuitively understood as a general capacity, mapping words to entities and their relationships. However, current work on automated NL understanding (typically referenced as semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Chen and Mooney, 2008; Kwiatkowski et al., 2010; B¨orschinger et al., 2011)) is restricted to a given output domain1 (or task) consisting of a closed set of meaning representation symbols, describing domains such as robotic soccer, database queries and flight ordering systems. In this work, we take a first step towards constructing a semantic interpreter that can leverage information from multiple tasks. This is not a straight forward objective – the domain specific nature of semantic interpretation, as described in the current literature, does not allow for an easy move between domains. For"
P14-1141,N12-1074,0,0.0587139,"Missing"
P14-1141,I13-1001,0,0.0260681,"(2014) use latent structure, aimed to identify relevant dialog segments, for predicting objections during courtroom deliberations. Other related work include speech act recognition in emails and forums but at a sentence level (Jeong et al., 2009), and using social network analysis to improve message classification into pre-determined types (Fortuna et al., 2007). Discussion forums data has also been used to address other interesting challenges such as extracting chatbox knowledge for use in general online forums (Huang et al., 2007) and automatically extracting answers from discussion forums (Catherine et al., 2013), subjectivity analysis of online forums (Biyani et al., 2013). Most of these methods use ideas similar to ours: identifying that threads (or discussions) have an underlying structure and that messages belong to categories. However, they operate in a different domain, which makes their goals and methods different from ours. Our work is most closely related to that of Backstrom et al. (2013) which introduced the re-entry prediction task —predicting whether a user who has participated in a thread will later contribute another comment to it. While seemingly related, their prediction task, focusin"
P14-1141,N10-1066,1,0.829513,"fw (tj , pj )) w 2 j (2) where λ is the regularization coefficient, tj is the j th thread with intervention decision rj and pj are the posts of this thread. w is the weight vector, l(·) is the squared hinge loss function and fw (tj , pj ) is defined in Equation 1. Replacing the term fw (tj , pj ) with the contents of Equation 1 in the minimization objective above, reveals the key difference from the traditional SVM formulation - the objective function has a maximum term inside the global minimization problem making it non-convex. We, therefore, employ the optimization algorithm presented in (Chang et al., 2010) to solve this problem. Exploiting the semi-convexity property (Felzenszwalb et al., 2010), the algorithm works in two steps, each executed iteratively. In the first step, it determines the latent variable assignments for positive examples. The algorithm then performs two step iteratively - first it determines the structural assignments for the negative examples, and then optimizes the fixed objective function using a cutting plane algorithm. Once this process converges for negative examples, the algorithm reassigns values to the latent variables for positive examples, and proceeds to the seco"
P14-1141,W02-1001,0,0.034724,"eneric EM style algorithm. The supervision in this model is provided only in form of the observed intervention decision, r and the post categories, hi are hid1504 den. The model uses the pseudocode shown in Algorithm 1 to iteratively refine the weight vectors. In each iteration, the model first uses viterbi algorithm to decode thread sequences with the current weights wt to find optimal highest scoring latent state sequences that agree with the observed intervention state (r = r0 ). In the next step, given the latent state assignments from the previous step, a structured perceptron algorithm (Collins, 2002) is used to update the weights wt+1 using weights from the previous step, wt , initialization. Algorithm 1 Training algorithm for LCMM 1: 2: 3: 4: 5: 6: 7: 8: Input: Labeled data D = {(t, p, r)i } Output: Weights w Initialization: Set wj randomly, ∀j for t : 1 to N do hˆi = arg maxh [wt · φ(p, r, h, t)] such that r = ri ∀i ˆ r) wt+1 = StructuredPerceptron(t, p, h, end for return w While testing, we use the learned weights and viterbi decoding to compute the intervention state and the best scoring latent category sequence. 3.3.2 Feature Engineering In addition to the ‘Thread Only Features’ and"
P14-1141,E14-1069,1,0.874407,"Missing"
P14-1141,D09-1130,0,0.018305,"al., 2009; Yano and Smith, 2010; Artzi et al., 2012) and rate of content diffusion (Kwak et al., 2010; Lerman and Ghosh, 2010; Bakshy et al., 2011; Romero et al., 2011; Artzi et al., 2012) and also question answering (Chaturvedi et al., 2014). Wang et al. (2007) incorporate thread structure of conversations using features in email threads while Goldwasser and Daum´e III (2014) use latent structure, aimed to identify relevant dialog segments, for predicting objections during courtroom deliberations. Other related work include speech act recognition in emails and forums but at a sentence level (Jeong et al., 2009), and using social network analysis to improve message classification into pre-determined types (Fortuna et al., 2007). Discussion forums data has also been used to address other interesting challenges such as extracting chatbox knowledge for use in general online forums (Huang et al., 2007) and automatically extracting answers from discussion forums (Catherine et al., 2013), subjectivity analysis of online forums (Biyani et al., 2013). Most of these methods use ideas similar to ours: identifying that threads (or discussions) have an underlying structure and that messages belong to categories."
P14-1141,P07-2019,0,0.0201449,"ed Work To the best of our knowledge, the problem of predicting instructor’s intervention in MOOC forums has not been addressed yet. Prior work deals with analyzing general online discussion forums of social media sites (Kleinberg, 2013): such as predicting comment volume (Backstrom et al., 2013; De Choudhury et al., 2009; Wang et al., 2012; Tsagkias et al., 2009; Yano and Smith, 2010; Artzi et al., 2012) and rate of content diffusion (Kwak et al., 2010; Lerman and Ghosh, 2010; Bakshy et al., 2011; Romero et al., 2011; Artzi et al., 2012) and also question answering (Chaturvedi et al., 2014). Wang et al. (2007) incorporate thread structure of conversations using features in email threads while Goldwasser and Daum´e III (2014) use latent structure, aimed to identify relevant dialog segments, for predicting objections during courtroom deliberations. Other related work include speech act recognition in emails and forums but at a sentence level (Jeong et al., 2009), and using social network analysis to improve message classification into pre-determined types (Fortuna et al., 2007). Discussion forums data has also been used to address other interesting challenges such as extracting chatbox knowledge for"
P17-1069,W11-0702,0,0.150399,"liticians. Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party. 2 et al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether t"
P17-1069,P13-2144,0,0.0812227,"rns over time by both party and individual politicians. Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party. 2 et al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expre"
P17-1069,L16-1591,0,0.163472,"iliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card 1 2 http://alt.qcri.org/semeval2016/ task6/ http://psl.cs.umd.edu 742 based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012). Several works from political and social science research have studied the role of Twitter and framing in shaping public opinion of certain events, e.g. the Vancouver riots (Burch et al., 2015) and the Egyptian protests (Harlow and Johnson, 2011; Meraz and Papacharissi, 2013)."
P17-1069,W13-1106,0,0.216923,"Missing"
P17-1069,D15-1008,0,0.177677,"cent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been"
P17-1069,N09-1057,0,0.0612147,"et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card 1 2 http://alt.qcri.org/semeval2016/ task6/ http://psl.cs.umd.edu 742 based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012). Several works from political and social science research have studied the role of Twitter and framing in s"
P17-1069,N15-1171,0,0.407435,"ntman, 1993; Chong and Druckman, 2007) is employed by politicians to bias the discussion towards their stance by emphasizing specific aspects of the issue. For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue. While the first frame supports increasing minimum wage because it improves workers’ lives, the second frame, by conversely emphasizing the costs involved, opposes the increase. Using framing to analyze political discourse has gathered significant interest over the last few years (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) as a way to automatically analyze political discourse in congressional speeches and political news articles. Different from previous works which focus on these longer texts or single issues, our dataset includes tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.). These tweets were annotated by adapting the annotation guidelines developed by Boydstun et al. (2014) for Twitter. Twitter issue framing is a challenging multilabel prediction task. Each tweet can be labeled as using one or more frames, out of 17 po"
P17-1069,W11-3702,0,0.723268,"Missing"
P17-1069,D14-1083,0,0.0393727,"nalyze framing patterns over time by both party and individual politicians. Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party. 2 et al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and des"
P17-1069,P15-2072,0,0.383305,"Twitter. Framing (Entman, 1993; Chong and Druckman, 2007) is employed by politicians to bias the discussion towards their stance by emphasizing specific aspects of the issue. For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue. While the first frame supports increasing minimum wage because it improves workers’ lives, the second frame, by conversely emphasizing the costs involved, opposes the increase. Using framing to analyze political discourse has gathered significant interest over the last few years (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) as a way to automatically analyze political discourse in congressional speeches and political news articles. Different from previous works which focus on these longer texts or single issues, our dataset includes tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.). These tweets were annotated by adapting the annotation guidelines developed by Boydstun et al. (2014) for Twitter. Twitter issue framing is a challenging multilabel prediction task. Each tweet can be labeled as using one or mor"
P17-1069,P14-1105,0,0.158679,"rable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several"
P17-1069,W12-3809,0,0.0912575,"vent extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card 1 2 http://alt.qcri.org/semeval2016/ task6/ http://psl.cs.umd.edu 742 based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012). Several works from political and social science research have studied the role o"
P17-1069,D14-1214,0,0.0466638,"Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens e"
P17-1069,P14-1016,0,0.0524056,"Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens e"
P17-1069,D16-1105,0,0.0571544,"rse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (We"
P17-1069,E12-1062,0,0.0702195,"Missing"
P17-1069,N13-1037,0,0.0261485,"; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional spe"
P17-1069,P15-1139,0,0.0867378,"more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) an"
P17-1069,P14-1018,0,0.0590158,"Missing"
P17-1069,N12-1072,0,0.0769315,"rty and individual politicians. Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party. 2 et al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an"
P17-1069,Q14-1024,0,0.0563689,"on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affi"
P17-1069,C14-1019,0,0.0933103,"sification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card 1 2 http://alt.qcri.org/semeval2016/ task6/ http://psl.cs.umd.edu 742 based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeato"
P17-1069,P13-1162,0,0.068506,"al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card 1 2 http://alt.qcri.org/semeval2016/ task6/ http://psl.cs.umd.edu 742 based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012). Several works from political and social science research have"
P17-1069,N10-1020,0,0.0567504,"nteractions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013). Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011). Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls Related Work Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 200"
P17-1069,D13-1010,0,0.0927583,"into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012). Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach. Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, includin"
P17-1069,P09-1026,0,0.0826889,"Missing"
P17-1069,W10-0214,0,0.0585396,"is provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party. 2 et al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other w"
P17-1069,P15-1012,0,0.0294808,"dataset of tweets to analyze framing patterns over time by both party and individual politicians. Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party. 2 et al., 2015; Baumer et al., 2015). Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance cl"
P17-1069,P14-1017,0,0.0541792,"r approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter. In recent years there has been growing interest in analyzing political discourse. Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009). Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue. Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (G"
P17-1069,P15-1157,0,0.362705,"ently discussed on Twitter. Framing (Entman, 1993; Chong and Druckman, 2007) is employed by politicians to bias the discussion towards their stance by emphasizing specific aspects of the issue. For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue. While the first frame supports increasing minimum wage because it improves workers’ lives, the second frame, by conversely emphasizing the costs involved, opposes the increase. Using framing to analyze political discourse has gathered significant interest over the last few years (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) as a way to automatically analyze political discourse in congressional speeches and political news articles. Different from previous works which focus on these longer texts or single issues, our dataset includes tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.). These tweets were annotated by adapting the annotation guidelines developed by Boydstun et al. (2014) for Twitter. Twitter issue framing is a challenging multilabel prediction task. Each tweet can be labeled"
P18-1067,W13-1106,0,0.203924,"Missing"
P18-1067,D15-1008,0,0.0603671,"s features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang and 1 The tweet refers to legislation proposed in 2016 concerning transgender bathroom access restrictions. 2 The data will be available at h"
P18-1067,N15-1171,0,0.0257372,"al slogans (for example “repeal and replace” when referring to the Affordable Care Act), and policy framing techniques (Boydstun et al., 2014; Johnson et al., 2017) as features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz an"
P18-1067,P14-1105,0,0.0770112,"nson et al., 2017) as features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang and 1 The tweet refers to legislation proposed in 2016 concerning transgender bathroom access restrictions. 2 The dat"
P18-1067,W11-3702,0,0.219338,", we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang and 1 The tweet refers to legislation proposed in 2016 concerning transgender bathroom access restrictions. 2 The data will be available at http://purduenlp. cs.purdue.edu/projects/twittermorals. 721 M ORAL F OUNDATION AND B RIEF D ESCRIPTION 1. Care/Harm: Care for others, generosity, compassion, ability to feel pain of others, sensiti"
P18-1067,C16-1279,1,0.815063,"rdable Care Act), and policy framing techniques (Boydstun et al., 2014; Johnson et al., 2017) as features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang and 1 The tweet refers to legislation proposed in 20"
P18-1067,P17-1069,1,0.871026,"Missing"
P18-1067,P15-2072,0,0.0189792,"nterplay of political slogans (for example “repeal and replace” when referring to the Affordable Care Act), and policy framing techniques (Boydstun et al., 2014; Johnson et al., 2017) as features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and J"
P18-1067,S17-2029,1,0.884734,"Missing"
P18-1067,L16-1591,0,0.276371,"ent morality on Twitter. We examine the interplay of political slogans (for example “repeal and replace” when referring to the Affordable Care Act), and policy framing techniques (Boydstun et al., 2014; Johnson et al., 2017) as features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and is"
P18-1067,N13-1092,0,0.0345779,"Missing"
P18-1067,C14-1019,0,0.0655465,"s. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang and 1 The tweet refers to legislation proposed in 2016 concerning transgender bathroom access restrictions. 2 The data will be available at http://purduenlp. cs.purdue.edu/projects/twittermorals. 721 M ORAL F OUNDATION AND B RIEF D ESC"
P18-1067,P17-1068,0,0.183644,"Missing"
P18-1067,D13-1010,0,0.158536,"g the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang and 1 The tweet refers to legislation proposed in 2016 concerning transgender bathroom access restrictions. 2 The data will be available at http://purduenlp. c"
P18-1067,P15-1157,0,0.015882,"r. We examine the interplay of political slogans (for example “repeal and replace” when referring to the Affordable Care Act), and policy framing techniques (Boydstun et al., 2014; Johnson et al., 2017) as features for predicting the underlying moral values which are expressed in politicians’ tweets. Additionally, we identify high-level themes characterizing the 2 Related Works In this paper, we explore how political ideology, language, framing, and morality interact on Twitter. Previous works have studied framing in longer texts, such as congressional speeches and news (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015), as well as issue-independent framing on Twitter (Johnson and Goldwasser, 2016; Johnson et al., 2017). Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing. The association between Twitter and framing in molding public opinion of events and issues (Burch et al.,"
P18-1067,P17-2102,0,0.0437378,"oncerning Morality and Sympathy are highly correlated with the Purity foundation, while Subversion is highly correlated with the Legal and Political frames. Table 1: Brief Descriptions of Moral Foundations. Hart, 2015) has also been studied. The connection between morality and political ideology has been explored in the fields of psychology and sociology (Graham et al., 2009, 2012). Moral foundations were also used to inform downstream tasks, by using the MFD to identify the moral foundations in partisan news sources (Fulgoni et al., 2016), or to construct features for other downstream tasks (Volkova et al., 2017). Several recent works have looked into using data-driven methods that go beyond the MFD to study tweets related to Hurricane Sandy (Garten et al., 2016; Lin et al., 2017). 3 Labeling tweets presents several challenges. First, tweets are short and thus lack the context often necessary for choosing a moral viewpoint. Tweets are often ambiguous, e.g., a tweet may express care for people who are being harmed by a policy. Another major challenge was overcoming the political bias of the annotator. For example, if a tweet discusses opposing Planned Parenthood because it provides abortion services, t"
P19-1055,W02-2004,0,0.326531,"Missing"
P19-1055,P17-1110,0,0.0205278,"tion function Z can be computed efficiently via the forward-backward algorithm. The term φ(x, yt ) corresponds to the score of a particular tag yt at position t in the sequence, and ψ(yt−1 , yt ) represents the score of transition from the tag at position t − 1 to the tag at position t. In the Neural CRF model, φ(x, yt ) is generated by the aforementioned Bi-LSTM while ψ(yt−1 , yt ) by a transition matrix. Architectures for Sequence Prediction Using neural networks to generate emission potentials in CRFs was applied successfully in several sequence prediction tasks, such as word segmentation (Chen et al., 2017), NER (Ma and Hovy, 2016; Lample et al., 2016), chunking and PoS tagging (Liu et al., 2018; Zhang et al., 2017). A sequence is represented as a sequence of L tokens: x = [x1 , x2 , . . . , xL ], each token corresponds to a label y ∈ Y, where Y is the set of all possible tags. An inference procedure is designed to find the most probable sequence y ∗ = [y1 , y2 , . . . , yL ] by solving, either exactly or approximately, the following optimization problem: 4 Functional Decomposition of Composite Tasks To accommodate our task decomposition approach, we first define the notion of partial labels, an"
P19-1055,W15-3904,0,0.0332508,"Missing"
P19-1055,D18-1504,0,0.18736,"ignored and learning is defined as a monolithic process, combining the tasks into a single learning problem. Our goal in this paper is to take a step towards modular learning architectures that exploit the learning tasks’ inner structure, and as a result simplify the learning process and reduce the 1 We also provide analysis for NER in the apendix 579 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 579–590 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018) We show that modular learning indeed helps simplify the learning task compared to traditional monolithic approaches. To answer the second question, we evaluate our model’s ability to leverage partial labels in two ways. First, by restricting the amount of full labels, and observing the improvement when providing increasing amounts of partial labels for only one of the sub-tasks. Second, we learn the sub-tasks using completely disjoint datasets of partial labels, and show that the knowledge learned by the sub-task modules can be integrated into the final decision module using a small amount of"
P19-1055,P16-1101,0,0.291814,"dently and then integrated into the final decision module. As we demonstrate in our experiments, this approach leads to better performance and increased flexibilOur experiments were designed to answer two questions. First, can the task structure be exploited to simplify a complex learning task by using a modular approach? Second, can partial labels be used effectively to reduce the annotation effort? To answer the first question, we conduct experiments over several sequence prediction tasks, and compare our approach to several recent models for deep structured prediction (Lample et al., 2016; Ma and Hovy, 2016; Liu et al., 2018), and when available, previously published results (Mitchell 580 3.1 ity, as it allows us to decouple the learning process and learn the tasks independently. Other modular neural architectures were recently studied for tasks combining vision and language analysis (Andreas et al., 2016; Hu et al., 2017; Yu et al., 2018), and were tailored for the grounded language setting. To help ensure the broad applicability of our framework, we provide a general modular network formulation for sequence labeling tasks by adapting a neural-CRF to capture the task structure. This family of m"
P19-1055,D13-1171,0,0.0685075,"Missing"
P19-1055,D15-1073,0,0.164529,"many learning tasks, it is typically ignored and learning is defined as a monolithic process, combining the tasks into a single learning problem. Our goal in this paper is to take a step towards modular learning architectures that exploit the learning tasks’ inner structure, and as a result simplify the learning process and reduce the 1 We also provide analysis for NER in the apendix 579 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 579–590 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics et al., 2013; Zhang et al., 2015; Li and Lu, 2017; Ma et al., 2018) We show that modular learning indeed helps simplify the learning task compared to traditional monolithic approaches. To answer the second question, we evaluate our model’s ability to leverage partial labels in two ways. First, by restricting the amount of full labels, and observing the improvement when providing increasing amounts of partial labels for only one of the sub-tasks. Second, we learn the sub-tasks using completely disjoint datasets of partial labels, and show that the knowledge learned by the sub-task modules can be integrated into the final deci"
P19-1055,D17-1179,1,0.857791,"to the score of a particular tag yt at position t in the sequence, and ψ(yt−1 , yt ) represents the score of transition from the tag at position t − 1 to the tag at position t. In the Neural CRF model, φ(x, yt ) is generated by the aforementioned Bi-LSTM while ψ(yt−1 , yt ) by a transition matrix. Architectures for Sequence Prediction Using neural networks to generate emission potentials in CRFs was applied successfully in several sequence prediction tasks, such as word segmentation (Chen et al., 2017), NER (Ma and Hovy, 2016; Lample et al., 2016), chunking and PoS tagging (Liu et al., 2018; Zhang et al., 2017). A sequence is represented as a sequence of L tokens: x = [x1 , x2 , . . . , xL ], each token corresponds to a label y ∈ Y, where Y is the set of all possible tags. An inference procedure is designed to find the most probable sequence y ∗ = [y1 , y2 , . . . , yL ] by solving, either exactly or approximately, the following optimization problem: 4 Functional Decomposition of Composite Tasks To accommodate our task decomposition approach, we first define the notion of partial labels, and then discuss different neural architectures capturing the dependencies between the modules trained over the d"
P19-1055,P15-1109,0,0.0217733,"the improvement when providing increasing amounts of partial labels for only one of the sub-tasks. Second, we learn the sub-tasks using completely disjoint datasets of partial labels, and show that the knowledge learned by the sub-task modules can be integrated into the final decision module using a small amount of full labels. Our contributions: (1) We provide a general modular framework for sequence learning tasks. While we focus on sentiment analysis task, the framework is broadly applicable to many other tagging tasks, for example, NER (Carreras et al., 2002; Lample et al., 2016) and SRL (Zhou and Xu, 2015), to name a few. (2) We introduce a novel weakly supervised learning approach, learning with partial labels, that exploits the modular structure to reduce the supervision effort. (3) We evaluated our proposed model, in both the fullysupervised and weakly supervised scenarios, over several sentiment analysis tasks. context they appear in semantically. By exploiting modularity, the entity segmentation partial labels can be used to help improve that specific aspect of the overall task. Our modular task decomposition approach is partially inspired by findings in cognitive neuroscience, namely the"
P19-1055,W09-1119,0,\N,Missing
P19-1055,W03-0419,0,\N,Missing
P19-1055,N16-1030,0,\N,Missing
P19-1055,P17-2012,0,\N,Missing
P19-1055,W02-2024,0,\N,Missing
P19-1055,S13-2052,0,\N,Missing
P19-1247,N15-1171,0,0.0156105,"set consisting of 594 documents describing the Israeli and Palestinian perspectives. More recently, in SemEval-2019, a hyperpartisian news article detection task was suggested1 . The current reported results on their dataset are comparable to ours, when using text information alone, demonstrating that it is indeed a challenging task. Other works use linguistic indicators of bias and expressions of implicit sentiment (Greene and Resnik, 2009; Recasens et al., 2013; Choi and Wiebe, 2014; Elfardy et al., 2015). In recent years several works looked at indications of framing bias in news articles (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018). We build on these work to help shape our text representation approach. Recent works looked at false content identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending"
P19-1247,P10-2047,0,0.0289952,"irectly about the comments made, while the second one focuses on negative reactions to these comments. Identifying the perspective difference and making it explicit can help strengthen trust in the newly-formed information landscape and ensure that all perspectives are represented. It can also help lay the foundation for the automatic detection of false content and rumors and help identify information echo-chambers in which only a single perspective is highlighted. Traditionally, identifying the author’s perspective is studied as a text-categorization problem (Greene and Resnik, 2009; Beigman Klebanov et al., 2010; Recasens et al., 2013; Iyyer et al., 2014; Johnson and Goldwasser, 2016), focusing on linguistic indicators of bias or issueframing phrases indicating their authors’ bias. These indicators can effectively capture bias in ideologically-charged texts, such as policy documents or political debates, which do not try to hide their political leaning and use a topic-focused vocabulary. Identifying the authors’ bias in news narratives can be more challenging. News articles, by their nature, cover a very large number of topics resulting in a diverse and dynamic vocabulary that is continuously updated"
P19-1247,D14-1125,0,0.0570347,"Missing"
P19-1247,S15-1015,0,0.0203983,"classifier is trained to differentiate between two specific perspectives. For example, the bitterlemons dataset consisting of 594 documents describing the Israeli and Palestinian perspectives. More recently, in SemEval-2019, a hyperpartisian news article detection task was suggested1 . The current reported results on their dataset are comparable to ours, when using text information alone, demonstrating that it is indeed a challenging task. Other works use linguistic indicators of bias and expressions of implicit sentiment (Greene and Resnik, 2009; Recasens et al., 2013; Choi and Wiebe, 2014; Elfardy et al., 2015). In recent years several works looked at indications of framing bias in news articles (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018). We build on these work to help shape our text representation approach. Recent works looked at false content identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when"
P19-1247,D16-1148,0,0.0185844,"g the Israeli and Palestinian perspectives. More recently, in SemEval-2019, a hyperpartisian news article detection task was suggested1 . The current reported results on their dataset are comparable to ours, when using text information alone, demonstrating that it is indeed a challenging task. Other works use linguistic indicators of bias and expressions of implicit sentiment (Greene and Resnik, 2009; Recasens et al., 2013; Choi and Wiebe, 2014; Elfardy et al., 2015). In recent years several works looked at indications of framing bias in news articles (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018). We build on these work to help shape our text representation approach. Recent works looked at false content identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending traditional approaches that rely on gra"
P19-1247,D18-1393,0,0.0286898,"alestinian perspectives. More recently, in SemEval-2019, a hyperpartisian news article detection task was suggested1 . The current reported results on their dataset are comparable to ours, when using text information alone, demonstrating that it is indeed a challenging task. Other works use linguistic indicators of bias and expressions of implicit sentiment (Greene and Resnik, 2009; Recasens et al., 2013; Choi and Wiebe, 2014; Elfardy et al., 2015). In recent years several works looked at indications of framing bias in news articles (Baumer et al., 2015; Budak et al., 2016; Card et al., 2016; Field et al., 2018; Morstatter et al., 2018). We build on these work to help shape our text representation approach. Recent works looked at false content identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending traditional approaches that rely on graph relations alone ("
P19-1247,P18-2029,0,0.0180025,"lly by emphasizing different aspects of the story. Our main insight in this paper is that the social context through which the information is propagated can be leveraged to alleviate the problem, by providing both a better representation for it, and when direct supervision is not available, a distantsupervision source based on information about users who endorse the textual content and spread it. Several recent works dealing with information dissemination analysis on social networks, focused on analyzing the interactions between news sources and users in social networks (Volkova et al., 2017; Glenski et al., 2018; Ribeiro et al., 2018). However, given the dynamic, and often adversarial setting of this domain, the true source of the news article might be hidden, unknown or masked by taking a different identity. Instead of analyzing the documents’ sources, our focus is to use social information, capturing how information is shared in the network, to help guide the text representation and provide additional support when making decisions over textual content. We construct a socially-infused textual representation, by embedding in a single space the news articles and the social circles in which these artic"
P19-1247,N09-1057,0,0.0900944,"rspectives. The first reporting directly about the comments made, while the second one focuses on negative reactions to these comments. Identifying the perspective difference and making it explicit can help strengthen trust in the newly-formed information landscape and ensure that all perspectives are represented. It can also help lay the foundation for the automatic detection of false content and rumors and help identify information echo-chambers in which only a single perspective is highlighted. Traditionally, identifying the author’s perspective is studied as a text-categorization problem (Greene and Resnik, 2009; Beigman Klebanov et al., 2010; Recasens et al., 2013; Iyyer et al., 2014; Johnson and Goldwasser, 2016), focusing on linguistic indicators of bias or issueframing phrases indicating their authors’ bias. These indicators can effectively capture bias in ideologically-charged texts, such as policy documents or political debates, which do not try to hide their political leaning and use a topic-focused vocabulary. Identifying the authors’ bias in news narratives can be more challenging. News articles, by their nature, cover a very large number of topics resulting in a diverse and dynamic vocabula"
P19-1247,P14-1105,0,0.517401,"ond one focuses on negative reactions to these comments. Identifying the perspective difference and making it explicit can help strengthen trust in the newly-formed information landscape and ensure that all perspectives are represented. It can also help lay the foundation for the automatic detection of false content and rumors and help identify information echo-chambers in which only a single perspective is highlighted. Traditionally, identifying the author’s perspective is studied as a text-categorization problem (Greene and Resnik, 2009; Beigman Klebanov et al., 2010; Recasens et al., 2013; Iyyer et al., 2014; Johnson and Goldwasser, 2016), focusing on linguistic indicators of bias or issueframing phrases indicating their authors’ bias. These indicators can effectively capture bias in ideologically-charged texts, such as policy documents or political debates, which do not try to hide their political leaning and use a topic-focused vocabulary. Identifying the authors’ bias in news narratives can be more challenging. News articles, by their nature, cover a very large number of topics resulting in a diverse and dynamic vocabulary that is continuously updated as new events unfold. Furthermore, unlike"
P19-1247,C16-1279,1,0.860226,"egative reactions to these comments. Identifying the perspective difference and making it explicit can help strengthen trust in the newly-formed information landscape and ensure that all perspectives are represented. It can also help lay the foundation for the automatic detection of false content and rumors and help identify information echo-chambers in which only a single perspective is highlighted. Traditionally, identifying the author’s perspective is studied as a text-categorization problem (Greene and Resnik, 2009; Beigman Klebanov et al., 2010; Recasens et al., 2013; Iyyer et al., 2014; Johnson and Goldwasser, 2016), focusing on linguistic indicators of bias or issueframing phrases indicating their authors’ bias. These indicators can effectively capture bias in ideologically-charged texts, such as policy documents or political debates, which do not try to hide their political leaning and use a topic-focused vocabulary. Identifying the authors’ bias in news narratives can be more challenging. News articles, by their nature, cover a very large number of topics resulting in a diverse and dynamic vocabulary that is continuously updated as new events unfold. Furthermore, unlike purely political texts, news na"
P19-1247,W06-2915,0,0.141646,"aning bias or no bias (center). Our experimental results demonstrate the strength of our approach. We compare direct text classification or node classification methods to our embedding-based approach in both the fully supervised and distant supervised settings, showing the importance of socially infused representations. 2595 Activity Link (share) Social Link (follow) Politically -Affiliated Figure 1: Information Flow Graph Sharing User 2 Articles -Left -Right -Center Sources Types Events Related Work The problem of perspective identification is typically studied as a supervised learning task (Lin et al., 2006; Greene and Resnik, 2009), in which a classifier is trained to differentiate between two specific perspectives. For example, the bitterlemons dataset consisting of 594 documents describing the Israeli and Palestinian perspectives. More recently, in SemEval-2019, a hyperpartisian news article detection task was suggested1 . The current reported results on their dataset are comparable to ours, when using text information alone, demonstrating that it is indeed a challenging task. Other works use linguistic indicators of bias and expressions of implicit sentiment (Greene and Resnik, 2009; Recasen"
P19-1247,P09-1113,0,0.0204536,"n when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending traditional approaches that rely on graph relations alone (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016) and information extraction and sentiment tasks (Yang et al., 2016a; West et al., 2014). In this work we focus on GCNs (Kipf and Welling, 2017; Schlichtkrull et al., 2018), a recent framework for representing relational data, that adapts the idea of convolutional networks to graphs. Distant supervision for NLP tasks typically relies on using knowledge-bases (Mintz et al., 2009), unlike our setting that uses social information. Using user activity and known user biases was explored in (Zhou et al., 2011), our settings are far more challenging as we do not have access to this information. https://webis.de/events/semeval-19/ 2 http://www.fakenewschallenge.org Twitter Users Pol. Users Left Pol. Users Right Pol. Users Center Pol. Users Avg # shared per Article Avg # pol. users followed 1,604 135 49 51 35 23.29 20.36 Table 1: Dataset Statistics 3 Dataset Description We collected 10,385 news articles from two news aggregation websites3 on 2,020 different events discussing"
P19-1247,D14-1162,0,0.0889826,"are content based features drawn from a wide range of approaches described in the literature on political bias, persuasion, and misinformation, capturing structure, sentiment, topic, complexity, bias and morality in the text. We used the resources in (Horne et al., 2018b) to generate 141 features based on the news article text, which were shown to work well for the binary hyper-partisan task (Horne et al., 2018a). Averaged Word Embedding (WE) The simplest approach for using pre-trained word embeddings. An averaged vector of all the document’s words using the pre-trained GloVe word embeddings (Pennington et al., 2014) were used to represent the entire article. Skip-Thought Embedding Unlike the Averaged word vector that does not capture context, we also used a sentence level encoder, Skip-Thought (Kiros et al., 2015), to generate text representations. We regard each document as a long sentence and map it directly to a 4800-dimension vector. Hierarchical LSTM over tokens and sentences We used a simplified version of the Hierarchical LSTM model (Yang et al., 2016b). In this case documents are first tokenized into sentences, then each sentence was tokenized into words. We used a word-level LSTM to construct a"
P19-1247,P13-1162,0,0.546547,"nts made, while the second one focuses on negative reactions to these comments. Identifying the perspective difference and making it explicit can help strengthen trust in the newly-formed information landscape and ensure that all perspectives are represented. It can also help lay the foundation for the automatic detection of false content and rumors and help identify information echo-chambers in which only a single perspective is highlighted. Traditionally, identifying the author’s perspective is studied as a text-categorization problem (Greene and Resnik, 2009; Beigman Klebanov et al., 2010; Recasens et al., 2013; Iyyer et al., 2014; Johnson and Goldwasser, 2016), focusing on linguistic indicators of bias or issueframing phrases indicating their authors’ bias. These indicators can effectively capture bias in ideologically-charged texts, such as policy documents or political debates, which do not try to hide their political leaning and use a topic-focused vocabulary. Identifying the authors’ bias in news narratives can be more challenging. News articles, by their nature, cover a very large number of topics resulting in a diverse and dynamic vocabulary that is continuously updated as new events unfold."
P19-1247,P17-2102,0,0.0996287,"d in subtle ways, usually by emphasizing different aspects of the story. Our main insight in this paper is that the social context through which the information is propagated can be leveraged to alleviate the problem, by providing both a better representation for it, and when direct supervision is not available, a distantsupervision source based on information about users who endorse the textual content and spread it. Several recent works dealing with information dissemination analysis on social networks, focused on analyzing the interactions between news sources and users in social networks (Volkova et al., 2017; Glenski et al., 2018; Ribeiro et al., 2018). However, given the dynamic, and often adversarial setting of this domain, the true source of the news article might be hidden, unknown or masked by taking a different identity. Instead of analyzing the documents’ sources, our focus is to use social information, capturing how information is shared in the network, to help guide the text representation and provide additional support when making decisions over textual content. We construct a socially-infused textual representation, by embedding in a single space the news articles and the social circle"
P19-1247,Q14-1024,0,0.0613251,"identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending traditional approaches that rely on graph relations alone (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016) and information extraction and sentiment tasks (Yang et al., 2016a; West et al., 2014). In this work we focus on GCNs (Kipf and Welling, 2017; Schlichtkrull et al., 2018), a recent framework for representing relational data, that adapts the idea of convolutional networks to graphs. Distant supervision for NLP tasks typically relies on using knowledge-bases (Mintz et al., 2009), unlike our setting that uses social information. Using user activity and known user biases was explored in (Zhou et al., 2011), our settings are far more challenging as we do not have access to this information. https://webis.de/events/semeval-19/ 2 http://www.fakenewschallenge.org Twitter Users Pol. Use"
P19-1247,D16-1152,0,0.274251,"ed at false content identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending traditional approaches that rely on graph relations alone (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016) and information extraction and sentiment tasks (Yang et al., 2016a; West et al., 2014). In this work we focus on GCNs (Kipf and Welling, 2017; Schlichtkrull et al., 2018), a recent framework for representing relational data, that adapts the idea of convolutional networks to graphs. Distant supervision for NLP tasks typically relies on using knowledge-bases (Mintz et al., 2009), unlike our setting that uses social information. Using user activity and known user biases was explored in (Zhou et al., 2011), our settings are far more challenging as we do not have access to this information. https://webis.de/events/semeval-19/ 2 http://www.fakenewschallenge.org T"
P19-1247,N16-1174,0,0.219166,"ed at false content identification (Volkova et al., 2017; Patwari et al., 2017), including a recent challenge2 identifying the relationship between an article’s title and its body. Unlike these, we do not assume the content is false, instead we ask if it reflects a different perspective. Using social information when learning text representations was studied in the context of graph embedding (Pan et al., 2016), extending traditional approaches that rely on graph relations alone (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016) and information extraction and sentiment tasks (Yang et al., 2016a; West et al., 2014). In this work we focus on GCNs (Kipf and Welling, 2017; Schlichtkrull et al., 2018), a recent framework for representing relational data, that adapts the idea of convolutional networks to graphs. Distant supervision for NLP tasks typically relies on using knowledge-bases (Mintz et al., 2009), unlike our setting that uses social information. Using user activity and known user biases was explored in (Zhou et al., 2011), our settings are far more challenging as we do not have access to this information. https://webis.de/events/semeval-19/ 2 http://www.fakenewschallenge.org T"
P19-1413,E12-1034,0,0.234766,"Missing"
P19-1413,W17-0906,0,0.048476,"observing the ranking of the correct answer over the entire event vocabulary, given the rest of the chain. However when complex event structures were considered, e.g., multi-argument events (Pichotta and Mooney, 2014), the large vocabulary size introduced both computational issues and ambiguity into the evaluation. As a result, Granroth-Wilding and Clark (2016) proposed a multiple-choice variation, called MCNC. It simplifies the evaluation process and reduces its computational burden. A similar choice of the multiple-choice adaptation could also be found in recent works, such as Story Cloze (Mostafazadeh et al., 2017) and SWAG (Zellers et al., 2018). In this paper, we evaluate our models on MCNC, and two recent variants (Lee and Goldwasser, 2018) turning MCNC into a sequential inference task. We also introduce relation-specific evaluation capturing the ability of our model to account for nuanced relations beyond co-occurrence. 3 Model We propose a learning framework, which accounts for the internal predicate-argument structure of events, tuning it to respect different relation types. Overview Our framework has two preprocessing phases: Event Extraction and Relational Triplet Extraction. In Event Extraction"
P19-1413,P18-1212,0,0.0658675,"Missing"
P19-1413,K16-2019,1,0.857008,"Missing"
P19-1413,P16-1028,0,0.0470941,"; Yoon et al., 2016) and capturing richer interactions (Nickel et al., 2016). In this paper, we adapt TransE and TransR for narrative script learning, which is an innovative generalization of relation embedding for commonsense inference. Several recent works looked at modeling specific relationships between events and extracting commonsense knowledge. Zhao et al. (2017) explored modeling cause-effect relations between 4215 events; Sap et al. (2018) focused on If-Then relations and showed that their joint multi-task model outperforms the models trained in isolation, based on human evaluations. Peng and Roth (2016) utilized discourse markers to extract relations between semantic frames and modeled them with prevalent language models. Event2Mind (Rashkin et al., 2018) created a dataset capturing the relationship between an event description and its participants’ intent and emotional reaction. This idea is related to our work, as the intent and reaction can correspond to Reason and Result discourse relations in our case. Our goal in this paper is to present a relational generalization over such relationships using a shared embedding space. The Narrative Cloze (NC) task (Chambers and Jurafsky, 2008) was in"
P19-1413,N18-1202,0,0.297156,"rts. The first focuses on comparing our models with previous work on several common script learning evaluation tasks. The second evaluates our model’s ability to capture different relation types between events. In the third, we apply our models to a related downstream task, implicit discourse sense classification, and achieve competitive results by combining our event embeddings 4218 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (a) MCNC (b) MCNS (c) MCNE Figure 3: Comparing single-step prediction (MCNC) and multiple-step inference (MCNS and MCNE). with ELMo (Peters et al., 2018), a contextualized word embedding model. We provide additional qualitative analysis, showing inferences made by our model, in the appendix. For training, we use the New York Times (NYT) section of the English Gigaword (Parker et al., 2011). It contains 2M newswire articles and splits into train/dev/test sets, replicating the setup given by Granroth-Wilding and Clark (2016). 500M triplets are extracted from the training set. All the experimental results are averaged over 5 runs. We leave the details about hyperparameter tuning in the appendix. The source code and pre-trained models are publicly"
P19-1413,E14-1024,0,0.0967822,"tion. This idea is related to our work, as the intent and reaction can correspond to Reason and Result discourse relations in our case. Our goal in this paper is to present a relational generalization over such relationships using a shared embedding space. The Narrative Cloze (NC) task (Chambers and Jurafsky, 2008) was introduced to evaluate statistical script models by removing an event from a chain, and observing the ranking of the correct answer over the entire event vocabulary, given the rest of the chain. However when complex event structures were considered, e.g., multi-argument events (Pichotta and Mooney, 2014), the large vocabulary size introduced both computational issues and ambiguity into the evaluation. As a result, Granroth-Wilding and Clark (2016) proposed a multiple-choice variation, called MCNC. It simplifies the evaluation process and reduces its computational burden. A similar choice of the multiple-choice adaptation could also be found in recent works, such as Story Cloze (Mostafazadeh et al., 2017) and SWAG (Zellers et al., 2018). In this paper, we evaluate our models on MCNC, and two recent variants (Lee and Goldwasser, 2018) turning MCNC into a sequential inference task. We also intro"
P19-1413,P16-1027,0,0.513183,"son, 1977) are structured knowledge representations capturing the relationships between prototypical event sequences and their participants in a given scenario. For example, given the event “John shot Jim with a gun”, we can infer that “he got arrested by police” is more probable than “he fell asleep”. In recent years, the problem of extracting script knowledge from text has attracted significant attention. Early works (Chambers and Jurafsky, 2008) focused on symbolic event representations and used Pointwise Mutual Information (PMI) between events to capture their relationships. Recent works (Pichotta and Mooney, 2016a; GranrothWilding and Clark, 2016; Wang et al., 2017; Lee and Goldwasser, 2018; Li et al., 2018) represent events using dense vectors, based on event cooccurrence, and use vector similarity over their embeddings to measure their relationship. Our main observation in this paper is that while models for learning script knowledge improved significantly over the last decade, these models can essentially represent only a single event relationship, co-occurrence. That is, events appearing in similar contexts tend to have similar representations. Although this idea works well for a lot of NLP tasks,"
P19-1413,P14-5010,0,0.0128608,"Missing"
P19-1413,P18-1043,0,0.0571121,"ch is an innovative generalization of relation embedding for commonsense inference. Several recent works looked at modeling specific relationships between events and extracting commonsense knowledge. Zhao et al. (2017) explored modeling cause-effect relations between 4215 events; Sap et al. (2018) focused on If-Then relations and showed that their joint multi-task model outperforms the models trained in isolation, based on human evaluations. Peng and Roth (2016) utilized discourse markers to extract relations between semantic frames and modeled them with prevalent language models. Event2Mind (Rashkin et al., 2018) created a dataset capturing the relationship between an event description and its participants’ intent and emotional reaction. This idea is related to our work, as the intent and reaction can correspond to Reason and Result discourse relations in our case. Our goal in this paper is to present a relational generalization over such relationships using a shared embedding space. The Narrative Cloze (NC) task (Chambers and Jurafsky, 2008) was introduced to evaluate statistical script models by removing an event from a chain, and observing the ranking of the correct answer over the entire event voc"
P19-1413,K16-2007,0,0.063745,"Missing"
P19-1413,D17-1006,0,0.212671,"the relationships between prototypical event sequences and their participants in a given scenario. For example, given the event “John shot Jim with a gun”, we can infer that “he got arrested by police” is more probable than “he fell asleep”. In recent years, the problem of extracting script knowledge from text has attracted significant attention. Early works (Chambers and Jurafsky, 2008) focused on symbolic event representations and used Pointwise Mutual Information (PMI) between events to capture their relationships. Recent works (Pichotta and Mooney, 2016a; GranrothWilding and Clark, 2016; Wang et al., 2017; Lee and Goldwasser, 2018; Li et al., 2018) represent events using dense vectors, based on event cooccurrence, and use vector similarity over their embeddings to measure their relationship. Our main observation in this paper is that while models for learning script knowledge improved significantly over the last decade, these models can essentially represent only a single event relationship, co-occurrence. That is, events appearing in similar contexts tend to have similar representations. Although this idea works well for a lot of NLP tasks, it is too coarse for modeling commonsense, which sho"
P19-1413,D18-1009,0,0.0154445,"answer over the entire event vocabulary, given the rest of the chain. However when complex event structures were considered, e.g., multi-argument events (Pichotta and Mooney, 2014), the large vocabulary size introduced both computational issues and ambiguity into the evaluation. As a result, Granroth-Wilding and Clark (2016) proposed a multiple-choice variation, called MCNC. It simplifies the evaluation process and reduces its computational burden. A similar choice of the multiple-choice adaptation could also be found in recent works, such as Story Cloze (Mostafazadeh et al., 2017) and SWAG (Zellers et al., 2018). In this paper, we evaluate our models on MCNC, and two recent variants (Lee and Goldwasser, 2018) turning MCNC into a sequential inference task. We also introduce relation-specific evaluation capturing the ability of our model to account for nuanced relations beyond co-occurrence. 3 Model We propose a learning framework, which accounts for the internal predicate-argument structure of events, tuning it to respect different relation types. Overview Our framework has two preprocessing phases: Event Extraction and Relational Triplet Extraction. In Event Extraction, we aim to identify events from"
Q16-1038,D14-1059,0,0.0319282,"impler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014). Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012). Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012), looking into common-sense causality identification prediction. In this work we focus on a different task, satire detection in news articles. We argue that this task is inherently a common-sense reasoning task, as identifying the satirical aspects in narrative text does not require any specialized training, but instead relies heavily on common expec"
Q16-1038,D14-1159,0,0.0228081,"(2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context. This paper follows a similar intuition; however, it looks into structured representations of this information, and studies their advantages. Our structured representation is related to several recent reading comprehension tasks (Richardson et al., 2013; Berant et al., 2014) and work on narrative representation such, as event-chains (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2008), plotunits (Goyal et al., 2010; Lehnert, 1981) and Story Intention Graphs (Elson, 2012). Unlike these works, narrative representation is not the focus of this work, but rather provides the basis for making inferences, and as result we choose a simpler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980;"
Q16-1038,P09-2041,0,0.351825,"mously fooled several leading news agencies1 . These misinterpretations can often 1 https://newrepublic.com/article/118013/ satire-news-websites-are-cashing-gullibleoutraged-readers be attributed to careless reading, as there is a clear line between unusual events finding their way to the news and satire, which intentionally places key political figures in unlikely humorous scenarios. The two can be separated by carefully reading the articles, exposing the satirical nature of the events described in such articles. In this paper we follow this intuition. We look into the satire detection task (Burfoot and Baldwin, 2009), predicting if a given news article is real or satirical, and suggest that this prediction task should be defined over common-sense inferences, rather than looking at it as a lexical text classification task (Pang and Lee, 2008; Burfoot and Baldwin, 2009), which bases the decision on word-level features. To further motivate this observation, consider the two excerpts in Figure 1. Both excerpts mention top-ranking politicians (the President and Vice President) in a drug-related context, and contain informal slang utterances, inappropriate for the subjects’ 537 Transactions of the Association f"
Q16-1038,P08-1090,0,0.813231,"ribed in satirical articles is often not unique to the specific individuals appearing in the narrative. In our example, both politicians are interchangeable: placing the president in the situation described in the first excerpt would not make it less absurd. It is therefore desirable to make a common-sense inference about high-ranking politicians in this scenario. We follow these intuitions and suggest a novel approach for the satire prediction task. Our model, C OM S ENSE, makes predictions by making common-sense inferences over a simplified narrative representation. Similarly to prior work (Chambers and Jurafsky, 2008; Goyal et al., 2010; Wang and McAllester, 2015) we represent the narrative structure by capturing the main entities (and tracking their mentions throughout the text), their activities, and their utterances. The result of this process is a Narrative Representation Graph (NRG). Figure 2 depicts examples of this representation for the excerpts in Figure 1. Given an NRG, our model makes inferences quantifying how likely are each of the represented events and interactions to appear in a real, or satirical context. Annotating the NRG for such inferences is a challenging task, as the space of possib"
Q16-1038,P09-1068,0,0.0301698,"xt classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context. This paper follows a similar intuition; however, it looks into structured representations of this information, and studies their advantages. Our structured representation is related to several recent reading comprehension tasks (Richardson et al., 2013; Berant et al., 2014) and work on narrative representation such, as event-chains (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2008), plotunits (Goyal et al., 2010; Lehnert, 1981) and Story Intention Graphs (Elson, 2012). Unlike these works, narrative representation is not the focus of this work, but rather provides the basis for making inferences, and as result we choose a simpler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual constructi"
Q16-1038,W10-2914,0,0.0300295,"e-art Convolutional Neural Network (Kim, 2014). Our experiments show that C OM S ENSE outperforms all other models. Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; Gonz´alezIb´anez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013). Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments. Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus cap"
Q16-1038,elson-2012-dramabank,0,0.103972,"c validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context. This paper follows a similar intuition; however, it looks into structured representations of this information, and studies their advantages. Our structured representation is related to several recent reading comprehension tasks (Richardson et al., 2013; Berant et al., 2014) and work on narrative representation such, as event-chains (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2008), plotunits (Goyal et al., 2010; Lehnert, 1981) and Story Intention Graphs (Elson, 2012). Unlike these works, narrative representation is not the focus of this work, but rather provides the basis for making inferences, and as result we choose a simpler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked i"
Q16-1038,filatova-2012-irony,0,0.0235593,"th a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; Gonz´alezIb´anez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013). Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments. Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context. This paper follows a similar intuition; however, it looks into structured representat"
Q16-1038,W10-0906,0,0.031812,"d Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014). Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012). Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012), looking into common-sense causality identification prediction. In this work we focus on a different task, satire detection in news articles. We argue that this task is inherently a common-sense reasoning task, as identifying the satirical aspects in narrative text does not require any specialized training, but instead relies heavily on common expectations of normative behavior and deviation from it in satirical text. We design our"
Q16-1038,P11-2102,0,0.0606664,"Missing"
Q16-1038,S12-1052,0,0.0135971,"ble to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014). Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012). Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012), looking into common-sense causality identification prediction. In this work we focus on a different task, satire detection in news articles. We argue that this task is inherently a common-sense reasoning task, as identifying the satirical aspects in narrative text does not require any specialized training, but instead relies heavily on common expectations of normative behavior and deviation from it in satirical text. We design our model to capture these behavioral expectations using (weighted) rules, instead of relying o"
Q16-1038,D10-1008,0,0.0574226,"Missing"
Q16-1038,P88-1012,0,0.683839,"ative representation such, as event-chains (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2008), plotunits (Goyal et al., 2010; Lehnert, 1981) and Story Intention Graphs (Elson, 2012). Unlike these works, narrative representation is not the focus of this work, but rather provides the basis for making inferences, and as result we choose a simpler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014). Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012). Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012"
Q16-1038,P15-2106,0,0.012385,"ural Network (Kim, 2014). Our experiments show that C OM S ENSE outperforms all other models. Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; Gonz´alezIb´anez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013). Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments. Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations u"
Q16-1038,D14-1181,0,0.00375236,"the more challenging sub-task of predicting if a quote is real given its speaker. We use two datasets collected 6 years apart. The first collected in 2009 (Burfoot and Baldwin, 2009) and an additional dataset collected recently. Since satirical articles tend to focus on current events, the two datasets describe different people and world events. To demonstrate the robustness of our C OM S ENSE approach we use the first dataset for training, and the second as out-of-domain test data. We compare C OM S ENSE to several competing systems including a state-of-the-art Convolutional Neural Network (Kim, 2014). Our experiments show that C OM S ENSE outperforms all other models. Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Bur"
Q16-1038,P12-2030,0,0.0153145,"re missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014). Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012). Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012), looking into common-sense causality identification prediction. In this work we focus on a different task, satire detection in news articles. We argue that this task is inherently a common-sense reasoning task, as identifying the satirical aspects in narrative text does not require any specialized training, but instead relies heavily on common expectations of normative behavior and deviation from it in satirical text. We design our model to capture these behavioral expectations using (weighted) rules"
Q16-1038,P11-2057,0,0.0213409,"rences is one of the core missions of AI, applicable to a wide range of tasks. Early work (Reiter, 1980; McCarthy, 1980; Hobbs et al., 1988) focused on logical inference, and manual construction of such knowledge repositories (Lenat, 1995; Liu and Singh, 2004). More recently, several researchers have looked into automatic common-sense knowledge construction and expansion using common-sense inferences (Tandon et al., 2011; Bordes et al., 2011; Socher et al., 2013; Angeli and Manning, 2014). Several works have looked into combining NLP with commonsense (Gerber et al., 2010; Gordon et al., 2011; LoBue and Yates, 2011; Labutov and Lipson, 2012; Gordon et al., 2012). Most relevant to our work is a SemEval-2012 task (Gordon et al., 2012), looking into common-sense causality identification prediction. In this work we focus on a different task, satire detection in news articles. We argue that this task is inherently a common-sense reasoning task, as identifying the satirical aspects in narrative text does not require any specialized training, but instead relies heavily on common expectations of normative behavior and deviation from it in satirical text. We design our model to capture these behavioral expectati"
Q16-1038,W13-1104,0,0.0171454,"restingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; Gonz´alezIb´anez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013). Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments. Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context. This paper follows a similar intuition; however, it looks into struct"
Q16-1038,P14-5010,0,0.00366601,"he text, their actions (represented as predicate nodes), their contextualizing information (location-modifiers, temporal modifiers, negations), and their utterances. We omitted from the graph other non-animate entity types. In Figure 2 we show an example of this representation. Similar in spirit to previous work (Goyal et al., 2010; Chambers and Jurafsky, 2008), we represent the relations between the entities that appear in the story using a Semantic Role Labeling system (Punyakanok et al., 2008) and collapse all the entity mentions into a single entity using a Co-Reference resolution system (Manning et al., 2014). We attribute utterances to their speaker based on a previously published rule based system (O’Keefe et al., 2012). Formally, we construct a graph G = {V, E}, where V consists of three types of vertices: A N IMATE E NTITY (e.g., people), P REDICATE (e.g., actions) and A RGUMENT (e.g., utterances, locations). The edges E capture the relationships between vertices. The graph contains several different edges. C O R EF edges collapse the mentions of the same entity into a single entity, A RGUMENT-T YPE edges connect A NIMATE E NTITY nodes to P RED ICATE nodes2 , and P REDICATE nodes to argument n"
Q16-1038,D12-1072,0,0.0398153,"Missing"
Q16-1038,J08-2005,0,0.0151987,"articles tend to focus on political figures, we design the NRG around animate entities that drive the events described in the text, their actions (represented as predicate nodes), their contextualizing information (location-modifiers, temporal modifiers, negations), and their utterances. We omitted from the graph other non-animate entity types. In Figure 2 we show an example of this representation. Similar in spirit to previous work (Goyal et al., 2010; Chambers and Jurafsky, 2008), we represent the relations between the entities that appear in the story using a Semantic Role Labeling system (Punyakanok et al., 2008) and collapse all the entity mentions into a single entity using a Co-Reference resolution system (Manning et al., 2014). We attribute utterances to their speaker based on a previously published rule based system (O’Keefe et al., 2012). Formally, we construct a graph G = {V, E}, where V consists of three types of vertices: A N IMATE E NTITY (e.g., people), P REDICATE (e.g., actions) and A RGUMENT (e.g., utterances, locations). The edges E capture the relationships between vertices. The graph contains several different edges. C O R EF edges collapse the mentions of the same entity into a single"
Q16-1038,D13-1020,0,0.0406526,"k is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in the article, thus capturing combinations unlikely to appear in a real context. This paper follows a similar intuition; however, it looks into structured representations of this information, and studies their advantages. Our structured representation is related to several recent reading comprehension tasks (Richardson et al., 2013; Berant et al., 2014) and work on narrative representation such, as event-chains (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2008), plotunits (Goyal et al., 2010; Lehnert, 1981) and Story Intention Graphs (Elson, 2012). Unlike these works, narrative representation is not the focus of this work, but rather provides the basis for making inferences, and as result we choose a simpler (and more robust) representation, most closely resembling event chains (Chambers and Jurafsky, 2008) Making common-sense inferences is one of the core missions of AI, applicable to a wide range of tasks. Ear"
Q16-1038,D13-1066,0,0.0269071,"Missing"
Q16-1038,P14-2084,0,0.012386,"C OM S ENSE to several competing systems including a state-of-the-art Convolutional Neural Network (Kim, 2014). Our experiments show that C OM S ENSE outperforms all other models. Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; Gonz´alezIb´anez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013). Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments. Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validit"
Q16-1038,P15-1100,0,0.0116004,"ncluding a state-of-the-art Convolutional Neural Network (Kim, 2014). Our experiments show that C OM S ENSE outperforms all other models. Most interestingly, it does so with a larger margin when tested over the out-of-domain dataset, demonstrating that it is more resistant to overfitting compared to other models. 2 Related Work The problem of building computational models dealing with humor, satire, irony and sarcasm has attracted considerable interest in the the Natural Language Processing (NLP) and Machine Learning (ML) communities in recent years (Wallace et al., 2014; Riloff et al., 2013; Wallace et al., 2015; Davidov et al., 2010; Karoui et al., 2015; Burfoot and Baldwin, 2009; Tepperman et al., 2006; Gonz´alezIb´anez et al., 2011; Lukin and Walker, 2013; Filatova, 2012; Reyes et al., 2013). Most work has looked into ironic expressions in shorter texts, such as tweets and forum comments. Most related to our work is Burfoot and Baldwin (2009) which focused on satirical articles. In that work the authors suggest a text classification approach for satire detection. In addition to using bag-of-words features, the authors also experiment with semantic validity features which pair entities mentioned in"
Q16-1038,P15-2115,0,0.0263025,"o the specific individuals appearing in the narrative. In our example, both politicians are interchangeable: placing the president in the situation described in the first excerpt would not make it less absurd. It is therefore desirable to make a common-sense inference about high-ranking politicians in this scenario. We follow these intuitions and suggest a novel approach for the satire prediction task. Our model, C OM S ENSE, makes predictions by making common-sense inferences over a simplified narrative representation. Similarly to prior work (Chambers and Jurafsky, 2008; Goyal et al., 2010; Wang and McAllester, 2015) we represent the narrative structure by capturing the main entities (and tracking their mentions throughout the text), their activities, and their utterances. The result of this process is a Narrative Representation Graph (NRG). Figure 2 depicts examples of this representation for the excerpts in Figure 1. Given an NRG, our model makes inferences quantifying how likely are each of the represented events and interactions to appear in a real, or satirical context. Annotating the NRG for such inferences is a challenging task, as the space of possible situations is extremely large. Instead, we fr"
S17-2029,P15-1150,0,0.0261426,"ilities of integer scores between 0-5). The final score is calculated by taking the mean of the 6 softmax outputs. This regression model is visualized in Figure 2. The PE and EE are concatenated to represent each input. They are fixed representations that will not be updated during the regression. The ”X” and ”-” shown in Figure 2 are element-wise products and element-wise differences between two input representations (Equation (1) and (2)). They represent the angles and distances between the input sentences. This regression objective has been shown to be very useful in text similarity tasks (Tai et al., 2015). e0 ∈C(e) = Regression exp(ve0 , ve ) , e∗ ∈E exp(ve∗ , ve ) P where e is the current event, C(e) is the contextual events of e, and ve is the embedding representation of e. To make the computation feasible, the negative sampling strategy is again used here. For each pair of event tokens in a sliding window, we sample k negative tokens. Other optimizing strategies for improve embedding quality used by Mikolov et al. 2013 are also applied here, such as subsampling for high-frequency tokens and filtering low-frequency tokens. The followings are the hyperparameters related to PE that are used in"
S17-2029,P08-1090,0,0.0516545,"for combining different embeddings for regression. More details will be discussed in Section 2.3. 2.2 Event Embeddings Word embeddings capture distributional semantics. It is a function that maps a word to a dense, low-dimension vector. With the same concept in mind, we can infer event semantics by exploring its contextual events to build EE. Similar ideas have be explored in several recent works (Granroth-Wilding and Clark, 2016; Pichotta and Mooney, 2016; Pacheco et al., 2016). Our EE model is constructed as follows: first, we extract event tokens, similar to narrative scripts construction (Chambers and Jurafsky, 2008). We resolve co-referent entities and run a dependency parser on all documents 2 . For each entity in a co-reference chain, we represent an event token e by its predicate p(e), a dependency relation to the entity d(e), and animacy of the entity a(e); resulting in a triplet ((p(e), d(e), a(e))). An event chain thus can be constructed by corresponding all the entities in a co-reference chain to event tokens. We extend the definition of the event predicate p(e) to include lemmatized verbs and predicative adjectives. These extensions are useful as they 1 2 δ is tuned over {0.4, 1} in our evaluatio"
S17-2029,Q15-1025,0,0.445174,"beddings I-Ta Lee, Mahak Goindani, Chang Li, Di Jin, Kristen Marie Johnson Xiao Zhang, Maria Leonor Pacheco, Dan Goldwasser Department of Computer Science, Purdue University West Lafayette, IN 47907 {lee2226,mgoindan,li1873,jind,john1187, zhang923,pachecog,dgoldwas}@purdue.edu Abstract sentence similarity: paraphrasing characteristics and sentence structures. The paraphrasing characteristics help identifying if two sentences share the same meaning. Our system incorporates it using an unsupervised learning step over the Paraphrase Database (PPDB; Ganitkevitch et al. 2013), which is inspired by Wieting et al. 2015a. The sentence structure, on the other hand, can detect structural differences, which reflect different aspects of the similarity between the input sentences. Our system employs a Convolutional Neural Network (CNN) to strengthen the embedding by including the sentence structure into our representation. The second type of embeddings, EE, conveys the distributional semantics of events in a narrative setting, associating a vector with each event. In the last part of our system, we build a regression model that associates the two distributed representations and predicts the similarity scores. Thi"
S17-2029,N13-1092,0,0.212961,"Missing"
S17-2029,P14-5010,0,0.00352158,"d run a dependency parser on all documents 2 . For each entity in a co-reference chain, we represent an event token e by its predicate p(e), a dependency relation to the entity d(e), and animacy of the entity a(e); resulting in a triplet ((p(e), d(e), a(e))). An event chain thus can be constructed by corresponding all the entities in a co-reference chain to event tokens. We extend the definition of the event predicate p(e) to include lemmatized verbs and predicative adjectives. These extensions are useful as they 1 2 δ is tuned over {0.4, 1} in our evaluation. we use Stanford CoreNLP library (Manning et al., 2014) capture important information about the state of the entity. For example, “Jim was hungry. He ate a sub”. The word “hungry” captures meaningful narrative information that should be included in the event chain of the entity “Jim”, so the resulting chain here should be: (hungry, subj, animate), (eat, subj, animate). Moreover, relying on verb predicates alone is sometimes insufficient, when the verbs are too ambiguous on their own, e.g., verbs like go, get, and have. For such weak verbs, we include their particles and clausal complement (xcomp) in the predicates, e.g., “have to sleep” will be re"
S17-2029,K16-2019,1,0.798679,"Missing"
S17-2029,P15-2070,0,0.0473595,"Missing"
S17-2029,D14-1162,0,0.0836834,"on for Computational Linguistics Figure 1: The convolutional neural network architecture consists of two networks that share the network parameters. The networks are constructed by a convolutional layer, a max-pooling layer, and two fully connected layers. Figure 1 describes our network architecture. Each input example consists of a pair of sentences/phrases. The initial input representation for each sentence is created by averaging the word vectors of the words in the sentence. The initial word vectors can rely on pre-trained word embeddings, such as Word2Vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014). This input layer is followed by a convolutional layer, a max-pooling layer, and two fully connected layers. The projected outputs (the embeddings layer in Figure 1) comprise the PE that will later be used for regression. Note that the two networks in Figure 1 share the network parameters. During training, the errors back-propagate not only to the network, but also to the embeddings. To train PE, we adopt a 2-step framework inspired by Wieting et al. 2015a and initialize our word embedding look-up table with the best performing embeddings they released–PARAGRAMPHRASE XXL. In the first step, w"
W10-2903,P09-1010,0,0.283359,"and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a Acknowledgements We are grateful to Rohit Kate and Raymond Mooney for their help with the Geoquery"
W10-2903,N10-1066,1,0.218067,"aining data and at a faster rate than D IRECT. Note that the performance of the D I RECT approach drops at the first iteration. We hypothesize this is due to imbalances in the binary feedback dataset (too many negative examples) in the first iteration. 7 It is relatively difficult to compare different approaches in the Geoquery domain given that many existing papers do not use the same data split. 25 grounded world state. Our learning framework closely follows recent work on learning from indirect supervision. The direct approach resembles learning a binary classifier over a latent structure (Chang et al., 2010a); while the aggressive approach has similarities with work that uses labeled structures and a binary signal indicating the existence of good structures to improve structured prediction (Chang et al., 2010b). 90 Accuracy on Response 250 80 70 60 50 40 30 Initialization 20 Direct Approach 10 0 0 Aggressive Approach 1 2 3 4 5 6 7 Learning Iterations 8 Conclusions Figure 2: Accuracy on training set as number of learning In this paper we tackle one of the key bottlenecks in semantic parsing — providing sufficient supervision to train a semantic parser. Our solution is two fold, first we present a"
W10-2903,N06-1056,0,0.865919,"gical symbol candidates per word (on average 13 logical symbols per word). 1. Is it possible to learn a semantic parser without annotated logical forms? 24 Algorithm N O L EARN D IRECT AGGRESSIVE S UPERVISED R250 22.2 75.2 82.4 87.6 Q250 — 69.2 73.2 80.4 To answer the second question, we compare a supervised version of our model to existing semantic parsers. The results are in Table 2. Although the numbers are not directly comparable due to different splits in the data7 , we can see that with a similar number of logical forms for training our S UPERVISED approach outperforms existing systems (Wong and Mooney, 2006; Wong and Mooney, 2007), while the AGGRESSIVE approach remains competitive without using any logical forms. Our S UPERVISED model is still very competitive with other approaches (Zettlemoyer and Collins, 2007; Wong and Mooney, 2007), which used considerably more annotated logical forms in the training phase. Table 1: Accuracy of learned models on R250 data and Q250 (testing) data. N O L EARN: using initialized weight vector, D IRECT : using feedback with the direct approach, AGGRESSIVE : using feedback with the aggressive approach, S UPERVISED: using gold 250 logical forms for training. Note"
W10-2903,P07-1121,0,0.92159,"the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm requires considerable amoun"
W10-2903,W05-0602,0,0.64828,"reformulate the semantic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training d"
W10-2903,P06-1115,0,0.848912,"can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an"
W10-2903,D07-1071,0,0.658715,"ntic parsing problem to reduce the dependency of the model on syntactic patterns, thus allowing our parser to scale better using less supervision. Our results surprisingly show that without using any annotated meaning representations learning with a weak feedback signal is capable of producing a parser that is competitive with fully supervised parsers. Example 1 Geoquery input text and output MR “What is the largest state that borders Texas?” largest(state(next to(const(texas)))) Previous works (Zelle and Mooney, 1996; Tang and Mooney, 2001; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007) employ machine learning techniques to construct a semantic parser. The learning algorithm is given a set of input sentences and their corresponding meaning representations, and learns a statistical semantic parser — a set of rules mapping lexical items and syntactic patterns to their meaning representation and a score associated with each rule. Given a sentence, these rules are applied recursively to derive the most probable meaning representation. Since semantic interpretation is limited to syntactic patterns identified in the training data, the learning algorithm req"
W10-2903,W08-2105,0,0.095047,"aning Representation Sentence 2: “What is Texas’ capital?” Following previous work, we capture the semantics of the Geoquery domain using a subset of first-order logic consisting of typed constants and functions. There are two types: entities E in the domain and numeric values N . Functions describe a functional relationship over types (e.g., population : E → N ). A complete logical form is constructed through functional composition; in our formalism this is perThe ability to adapt to unseen inputs is one of the key challenges in semantic parsing. Several works (Zettlemoyer and Collins, 2007; Kate, 2008) have addressed this issue explicitly by manually defining syntactic transformation rules that can help the learned parser generalize better. Unfortunately these are only partial solutions as a 3 Mistake driven algorithms that do not enforce margin constraints may not be able to generalize using this protocol since they will repeat the same prediction at training time and therefore will not update the model. 4 This is true for all meaning representations designed to be executed by a computer system. 22 stituent c). formed by the substitution operator. For example, given the function next to(x)"
W10-2903,P09-1110,0,0.436956,"ng to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operat"
W10-2903,P09-1011,0,0.131626,"s. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol. Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation. These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text. Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state. In our experiments the input text consists of generalized statements which describe some information need that does not correspond directly to a Acknowledgements We are grateful to Rohit Kate and Raymond Mooney for thei"
W10-2903,P06-2080,0,0.0713532,"better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements. In this work we take an additional step towards alleviating the difficulty o"
W10-2903,W00-1317,0,0.0100761,"ntic interpretation that does not rely on NL syntactic parsing rules, but rather uses the syntactic information to bias the interpretation process. This approach allows the model to generalize better and reduce the required amount of supervision. We demonstrate the effectiveness of our training paradigm and interpretation model over the Geoquery domain, and show that our model can outperform fully supervised systems. iterations increases. 7 Related Work Learning to map sentences to a meaning representation has been studied extensively in the NLP community. Early works (Zelle and Mooney, 1996; Tang and Mooney, 2000) employed inductive logic programming approaches to learn a semantic parser. More recent works apply statistical learning methods to the problem. In (Ge and Mooney, 2005; Nguyen et al., 2006), the input to the learner consists of complete syntactic derivations for the input sentences annotated with logical expressions. Other works (Wong and Mooney, 2006; Kate and Mooney, 2006; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models. Learning is then def"
W14-1804,E12-1021,0,0.20543,"Missing"
W14-1804,H05-2018,0,0.0129589,"elong to these three categories—a) course content, which include discussions about course material (COURSE), b) meta-level discussions about the course, including feedback and course logistics (LOGISTICS), and c) other general discussions, which include student introductions, discussions about online courses (GENERAL). In order to capture these categories automatically we provide seed words for each category. For example, we extract seed words for the COURSE topic from each course’s syllabus. In addition to the automatic topic assignment, we capture the sentiment polarity using Opinionfinder (Wilson et al., 2005). We use features derived from topic assignments and sentiment to predict student course completion (student survival). We measure course completion by examining if the student attempted the final exam/ last few assignments in the course. We follow the observation that LOGISTICS posts contain feedback about the course. Finding high-confidence LOGISTICS posts can give a better understanding of student opinion about the course. Similarly, posting in COURSE topic and receiving good feedback (i.e., votes) is an indicator of student success and might contribute to survival. We show that modeling th"
W16-1405,W12-2513,0,0.0198763,", focusing on social network analysis, looks at the network structure and information flow on it as means of inferring knowledge about the network. For example, works by (Leskovec et al., 2008; Kumar et al., 2010) model the evolution of network structure over time, and works such as (Xiang et al., 2010; Leskovec et al., 2010) use the network structure to predict properties of links (e.g., strength, sign). The second camp, focusing on natural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis resulted in a wide range of successful applications; however, they"
W16-1405,P10-1015,0,0.0113257,"nto two, almost completely disconnected, camps. The first, focusing on social network analysis, looks at the network structure and information flow on it as means of inferring knowledge about the network. For example, works by (Leskovec et al., 2008; Kumar et al., 2010) model the evolution of network structure over time, and works such as (Xiang et al., 2010; Leskovec et al., 2010) use the network structure to predict properties of links (e.g., strength, sign). The second camp, focusing on natural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis result"
W16-1405,D12-1135,0,0.0218114,"et al., 2008; Kumar et al., 2010) model the evolution of network structure over time, and works such as (Xiang et al., 2010; Leskovec et al., 2010) use the network structure to predict properties of links (e.g., strength, sign). The second camp, focusing on natural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis resulted in a wide range of successful applications; however, they neglect to model the interactions between the social and linguistic representations and how they complement one another. One of the few exceptions was discussed in (West et"
W16-1405,D12-1006,0,0.0149812,"e, works by (Leskovec et al., 2008; Kumar et al., 2010) model the evolution of network structure over time, and works such as (Xiang et al., 2010; Leskovec et al., 2010) use the network structure to predict properties of links (e.g., strength, sign). The second camp, focusing on natural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis resulted in a wide range of successful applications; however, they neglect to model the interactions between the social and linguistic representations and how they complement one another. One of the few exceptions was disc"
W16-1405,N13-1090,0,0.0258057,"presentations, one capturing the linguistic information, and the other, the network structure. Instead, in this paper we take the first step towards finding a joint representation over both linguistic and network information, rather than treating the two independently. We follow the intuition that interactions in a social network can be fully captured only by taking into account both types of information together. To achieve this goal, we embed the input social graph into a dense, continuous, low-dimensional vector space, capturing both network and linguistic similarities between nodes. Word (Mikolov et al., 2013; Pennington et al., 2014) and Network (Perozzi et al., 2014; Tang et al., 2015) embedding approaches that were recently proposed, aim to combat a similar problem in their respective domains–data sparsity. Both follow a similar approach–embed discrete objects (words or nodes in 29 Proceedings of the 2016 Workshop on Graph-based Methods for Natural Language Processing, NAACL-HLT 2016, pages 29–33, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics the graph) into a continuous vector representation, based on the context they appear in. Our approach aims to map"
W16-1405,D14-1162,0,0.0820729,"Missing"
W16-1405,N15-1153,0,0.0289789,"network structure over time, and works such as (Xiang et al., 2010; Leskovec et al., 2010) use the network structure to predict properties of links (e.g., strength, sign). The second camp, focusing on natural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis resulted in a wide range of successful applications; however, they neglect to model the interactions between the social and linguistic representations and how they complement one another. One of the few exceptions was discussed in (West et al., 2014), which inferred sentiment links between nodes in a"
W16-1405,D14-1121,0,0.0134534,"tural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis resulted in a wide range of successful applications; however, they neglect to model the interactions between the social and linguistic representations and how they complement one another. One of the few exceptions was discussed in (West et al., 2014), which inferred sentiment links between nodes in a social network by jointly modeling the local output probabilities of a sentiment analyzer looking at the textual interactions between the nodes and the global network structure. While resulting in bet"
W16-1405,W11-1708,0,0.0222196,"Missing"
W16-1405,P14-1018,0,0.0272384,"Missing"
W16-1405,Q14-1024,0,0.0335528,"l the evolution of network structure over time, and works such as (Xiang et al., 2010; Leskovec et al., 2010) use the network structure to predict properties of links (e.g., strength, sign). The second camp, focusing on natural language analysis, looks into tasks such as extracting social relationships from narrative text (Elson et al., 2010; Van De Camp and van den Bosch, 2011; Agarwal et al., 2012) and analyzing the contents of the information flowing through the network. For example, works by (Danescu-Niculescu-Mizil et al., 2012; Hassan et al., 2012; Filippova, 2012; Volkova et al., 2014; West et al., 2014; Rahimi et al., 2015; Volkova et al., 2015) extract attributes of, and social relationships between, nodes by analyzing the textual communication between them. Other works (Krishnan and Eisenstein, 2014; Sap et al., 2014) use the social network to inform language analysis. Both perspectives on social network analysis resulted in a wide range of successful applications; however, they neglect to model the interactions between the social and linguistic representations and how they complement one another. One of the few exceptions was discussed in (West et al., 2014), which inferred sentiment lin"
W16-5609,W11-0702,0,0.0319909,"ich were candidates for the U.S. 2016 presidential election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Works which focus on inferring signed social networks (West et al., 20"
W16-5609,P13-2144,0,0.0254003,"of 32 prominent U.S. politicians, some of which were candidates for the U.S. 2016 presidential election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Works which focus on inferri"
W16-5609,W13-1106,0,0.0606031,"Missing"
W16-5609,D15-1008,0,0.0115499,"framing in Twitter data. To do so we used the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fi"
W16-5609,N15-1171,0,0.059339,"es a lack of stance as well. To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we used the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platf"
W16-5609,W11-3702,0,0.0151663,"o analyze political discourse and influence has gained in popularity over recent years. Predicting characteristics of Twitter users, including political party affiliation has been explored (Volkova et al., 2015; Volkova et al., 2014; Conover et al., 2011). Previous works have also focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), analyzing types of tweets and Twitter network effects around political events (Maireder and Ausserhofer, 2013), automatic polls based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as uses of distant supervision (Marchetti-Bowick and Chambers, 2012). Analyzing political tweets specifically has also attracted considerable interest. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. Unlike this task and most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we combine tweet content and temporal indicators into a representation of a politician’s overall Twitter behavior, to determine if these features ar"
W16-5609,P15-2072,0,0.104715,"individual tweet (SemEval, 2016), we use the overall Twitter behavior to predict a politician’s stance on an issue. We argue that these settings are better suited for the political arena on Twitter. Given the limit of 140 characters, the stance relevance of a tweet is not independent of the social context in which it was generated. In an extreme case, even the lack of Twitter activity on certain topics can be indicative of a stance. Additionally, framing issues in order to create bias towards their stance is a tool often used by politicians to contextualize the discussion (Tsur et al., 2015; Card et al., 2015; Boydstun et al., 2014). Previous works exploring framing analyze text in traditional settings, such as congressional speeches or newspaper articles. To apply framing analysis to Twitter data, we allow tweets to hold multiple frames when necessary, as we find that on average many tweets are relevant to two frames per issue. This approach allows our model to make use of changing and similar framing patterns over 66 Proceedings of 2016 EMNLP Workshop on Natural Language Processing and Computational Social Science, pages 66–75, c Austin, TX, November 5, 2016. 2016 Association for Computational L"
W16-5609,N13-1037,0,0.0223781,"congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fiorina, Lindsey Graham, Mike Huckabee, Bobby Jindal, John Kasich, George Pataki, Rand Paul, Rick Perry, Marco Rubio, Rick Santorum, Donald Trump, Scott Walker D EMOCRATIC P OLITICIANS Joe Biden, Lincoln Chafee, Hillary Clinton, Kirsten Gillibrand, John Kerry, Ben Lujan, Ed Markey, Martin O’Malley, Nancy Pelosi, Harry Reid, Bernie Sanders, Chuck Schumer, Jon Tester, Mark Warner, Elizabeth Warren, Jim"
W16-5609,N09-1057,0,0.0722548,"dicators into a representation of a politician’s overall Twitter behavior, to determine if these features are indicative of a politician’s stance. This approach allows us to capture when politicians fail to tweet about a topic, which indicates a lack of stance as well. To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we used the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user pr"
W16-5609,D14-1083,0,0.0170492,"the Twitter activity of 32 prominent U.S. politicians, some of which were candidates for the U.S. 2016 presidential election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Work"
W16-5609,P14-1105,0,0.0436118,"pt to analyze issue framing in Twitter data. To do so we used the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Chri"
W16-5609,D14-1214,0,0.018919,"2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fiorina, Lindsey Graham, Mike Huckabee, Bobby Jindal, John Kasich, George Pataki, Rand Paul, Rick Perry, Marco Rubio, Rick Santorum, Donald Trump, Scott Walker D EMOCRATIC P OLITICIANS Joe Biden, Lincoln Chafee, Hillary Clinton, Kirsten Gillibrand, John Kerry, Ben Lujan, Ed Markey,"
W16-5609,P14-1016,0,0.0297927,"2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fiorina, Lindsey Graham, Mike Huckabee, Bobby Jindal, John Kasich, George Pataki, Rand Paul, Rick Perry, Marco Rubio, Rick Santorum, Donald Trump, Scott Walker D EMOCRATIC P OLITICIANS Joe Biden, Lincoln Chafee, Hillary Clinton, Kirsten Gillibrand, John Kerry, Ben Lujan, Ed Markey,"
W16-5609,E12-1062,0,0.0248449,"Twitter users, including political party affiliation has been explored (Volkova et al., 2015; Volkova et al., 2014; Conover et al., 2011). Previous works have also focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), analyzing types of tweets and Twitter network effects around political events (Maireder and Ausserhofer, 2013), automatic polls based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as uses of distant supervision (Marchetti-Bowick and Chambers, 2012). Analyzing political tweets specifically has also attracted considerable interest. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. Unlike this task and most related work on stance prediction (e.g., those mentioned above), we do not assume that each tweet expresses a stance. Instead, we combine tweet content and temporal indicators into a representation of a politician’s overall Twitter behavior, to determine if these features are indicative of a politician’s stance. This approach allows us to capture when politicians fail to tweet about a topic, which"
W16-5609,P15-1139,0,0.0541814,"e framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fiorina, Lindsey Graham, Mike Huckabee, Bobby Jindal, John Kasich, George Pataki, Rand Paul, Rick Perry"
W16-5609,C14-1019,0,0.022989,"g et al., 2012), and PSL collective classification (Bach et al., 2015) are closest to our work, but these typically operate in supervised settings. In this work, we use PSL without direct supervision, to assign soft values (in the range of 0 to 1) to output variables 1 . Using Twitter to analyze political discourse and influence has gained in popularity over recent years. Predicting characteristics of Twitter users, including political party affiliation has been explored (Volkova et al., 2015; Volkova et al., 2014; Conover et al., 2011). Previous works have also focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), analyzing types of tweets and Twitter network effects around political events (Maireder and Ausserhofer, 2013), automatic polls based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as uses of distant supervision (Marchetti-Bowick and Chambers, 2012). Analyzing political tweets specifically has also attracted considerable interest. Recently, SemEval Task 6 (SemEval, 2016) aimed to detect the stance of individual tweets. Unlike"
W16-5609,P13-1162,0,0.0522148,"ation of a politician’s overall Twitter behavior, to determine if these features are indicative of a politician’s stance. This approach allows us to capture when politicians fail to tweet about a topic, which indicates a lack of stance as well. To the best of our knowledge, this work is also the first attempt to analyze issue framing in Twitter data. To do so we used the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et"
W16-5609,N10-1020,0,0.0408395,"framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fiorina, Lindsey Graham, Mike Huckabee, Bobby Jindal, John Kasich, George Pataki, Rand Paul, Rick Perry, Marco Rubio, Rick Santorum, Donald Trump, Scott Walker D EMOCRATIC P OLITICIANS Joe Biden, Lincoln Chafee, Hillary Clinton, Kirsten Gillibrand, John Kerry, Ben Lujan, Ed Markey, Martin O’Malley, Nancy Pelosi, Harry Reid, Bernie Sanders, Chuck Schumer, Jon Tester, Mark Wa"
W16-5609,D13-1010,0,0.0496195,"To do so we used the frame guidelines developed by Boydstun et al. (2014). Issue framing is related to 1 Conversely, Markov Logic Networks assign hard (0 or 1) values to model variables. 68 both analyzing biased language (Greene and Resnik, 2009; Recasens et al., 2013) and subjectivity (Wiebe et al., 2004). Several previous works have explored topic framing of public statements, congressional speeches, and news articles (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) . Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Lewenberg et al., 2016) and policies (Gerrish and Blei, 2012; Nguyen et al., 2015; Grimmer, 2010). Finally, unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, such as user profile extraction (Li et al., 2014b), life event extraction (Li et al., 2014a), and conversation modeling (Ritter et al., 2010). Further, Eisenstein (2013) discusses methods for dealing with the unique language used in micro-blogging platforms. 3 Data and Problem Setting R EPUBLICAN P OLITICIANS Jeb Bush, Ben Carson, Chris Christie, Ted Cruz, Carly Fiorina, Lindsey Gra"
W16-5609,P09-1026,0,0.0376755,"election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Works which focus on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social gro"
W16-5609,W10-0214,0,0.0258499,"or the U.S. 2016 presidential election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Works which focus on inferring signed social networks (West et al., 2014), stance classification (Sr"
W16-5609,P15-1012,0,0.0137029,"liticians. We analyze the Twitter activity of 32 prominent U.S. politicians, some of which were candidates for the U.S. 2016 presidential election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structure"
W16-5609,P15-1157,0,0.0982989,"edicting stance per individual tweet (SemEval, 2016), we use the overall Twitter behavior to predict a politician’s stance on an issue. We argue that these settings are better suited for the political arena on Twitter. Given the limit of 140 characters, the stance relevance of a tweet is not independent of the social context in which it was generated. In an extreme case, even the lack of Twitter activity on certain topics can be indicative of a stance. Additionally, framing issues in order to create bias towards their stance is a tool often used by politicians to contextualize the discussion (Tsur et al., 2015; Card et al., 2015; Boydstun et al., 2014). Previous works exploring framing analyze text in traditional settings, such as congressional speeches or newspaper articles. To apply framing analysis to Twitter data, we allow tweets to hold multiple frames when necessary, as we find that on average many tweets are relevant to two frames per issue. This approach allows our model to make use of changing and similar framing patterns over 66 Proceedings of 2016 EMNLP Workshop on Natural Language Processing and Computational Social Science, pages 66–75, c Austin, TX, November 5, 2016. 2016 Association"
W16-5609,P14-1018,0,0.0686325,"Missing"
W16-5609,N12-1072,0,0.0153825,"liticians, some of which were candidates for the U.S. 2016 presidential election. We collected their recent tweets and stances on 16 different issues, which were used for evaluation purposes. Our experiments demonstrate the effectiveness of our global modeling approach which outperforms the weak learners that provide the initial supervision. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Works which focus on inferring signed social netw"
W16-5609,Q14-1024,0,0.0219026,"ion. 2 Related Work To the best of our knowledge this is the first work to use Twitter data, specifically content, frames, and temporal activity, to predict politicians’ stances. Previous works (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010; Somasundaran and Wiebe, 2009) have studied mining opinions and predicting stances in online debate forum data, exploiting argument and threaded conversation structures, or analyzed social interaction and group structure (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014). In our Twitter dataset, there were few “@” mention or retweet examples forming a conversation concerning the investigated issues, thus we did not have access to argument or conversation structures for analysis. Works which focus on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and PSL collective classification (Bach et al., 2015) are closest to our work, but these typically operate in supervised settings. In this work, we use PSL without direct supervision, to assign soft values (in the range of"
W16-5609,H05-2018,0,0.0501617,"tiple issues at once (e.g., religion may indicate the tweet refers to ISIS, Religion, or Marriage). The majority of matching keywords determines the issue of the tweet, with rare cases of ties manually resolved. The output of this classifier is all of the issue-related tweets of a politician, which are used as input for the PSL predicate T WEETS (P1, I SSUE ). This binary predicate indicates if politician P1 has tweeted about the issue or not. Sentiment Analysis: Based on the idea that the sentiment of a tweet can help expose a politician’s stance on a certain issue, we use OpinionFinder 2.0 (Wilson et al., 2005) to label each politician’s issue-related tweets as positive, negative, or neutral. We observed, however, that for all politicians, a majority of tweets will be labeled as neutral. This may be caused by the difficulty of labeling sentiment for Twitter data. If a politician has no positive or negative tweets, they are assigned their party’s majority sentiment assignment for that issue. This output is used as input to the PSL predicates T WEET P OS (P1, I SSUE ) and T WEET N EG (P1, I SSUE ). Agreement and Disagreement: To determine how well tweet content similarity can capture stance agreement,"
W16-5906,P16-1231,0,0.0213117,"(Goodman et al., 2012) were suggested for generative models, and MLN (Domingos et al., 2006), PSL (Bach et al., 2015), FACTORIE (McCallum et al., 2009), and CCM (Rizzolo and Roth, 2010; Kordjamshidi et al., 2015) were suggested for conditional models. In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models was studied by several works, typically in the context of a specific task or a specific inference procedure. These include dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016) named entity recognition and sequence labeling systems (Ma and Hovy, 2016; Lample et al., 2016), and models for combining deep learning and graphical models for vision tasks (Zheng et al., 2015; Chen et al., 2015). 3 DR AI L Modeling Language The DR AI L modeling language provides a general way to define relational learning problems that are highly structured. A relational model is specified in DR AI L using a set of weighted first-order logic rule templates that describe predictions and express the dependencies and constraints of a specific domain. Each rule is composed of: (1) a template de"
W16-5906,D14-1082,0,0.0153251,"been suggested. For example, BLOG (Milch et al., 2005) and CHURCH (Goodman et al., 2012) were suggested for generative models, and MLN (Domingos et al., 2006), PSL (Bach et al., 2015), FACTORIE (McCallum et al., 2009), and CCM (Rizzolo and Roth, 2010; Kordjamshidi et al., 2015) were suggested for conditional models. In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models was studied by several works, typically in the context of a specific task or a specific inference procedure. These include dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016) named entity recognition and sequence labeling systems (Ma and Hovy, 2016; Lample et al., 2016), and models for combining deep learning and graphical models for vision tasks (Zheng et al., 2015; Chen et al., 2015). 3 DR AI L Modeling Language The DR AI L modeling language provides a general way to define relational learning problems that are highly structured. A relational model is specified in DR AI L using a set of weighted first-order logic rule templates that describe predictions and express the dependencies and constraints of"
W16-5906,P11-2008,0,0.0167094,"Missing"
W16-5906,N16-1030,0,0.0301506,"PSL (Bach et al., 2015), FACTORIE (McCallum et al., 2009), and CCM (Rizzolo and Roth, 2010; Kordjamshidi et al., 2015) were suggested for conditional models. In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models was studied by several works, typically in the context of a specific task or a specific inference procedure. These include dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016) named entity recognition and sequence labeling systems (Ma and Hovy, 2016; Lample et al., 2016), and models for combining deep learning and graphical models for vision tasks (Zheng et al., 2015; Chen et al., 2015). 3 DR AI L Modeling Language The DR AI L modeling language provides a general way to define relational learning problems that are highly structured. A relational model is specified in DR AI L using a set of weighted first-order logic rule templates that describe predictions and express the dependencies and constraints of a specific domain. Each rule is composed of: (1) a template definition written in first order logic, (2) the neural network architecture that will be used to"
W16-5906,P16-1101,0,0.0141507,"gos et al., 2006), PSL (Bach et al., 2015), FACTORIE (McCallum et al., 2009), and CCM (Rizzolo and Roth, 2010; Kordjamshidi et al., 2015) were suggested for conditional models. In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models was studied by several works, typically in the context of a specific task or a specific inference procedure. These include dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016) named entity recognition and sequence labeling systems (Ma and Hovy, 2016; Lample et al., 2016), and models for combining deep learning and graphical models for vision tasks (Zheng et al., 2015; Chen et al., 2015). 3 DR AI L Modeling Language The DR AI L modeling language provides a general way to define relational learning problems that are highly structured. A relational model is specified in DR AI L using a set of weighted first-order logic rule templates that describe predictions and express the dependencies and constraints of a specific domain. Each rule is composed of: (1) a template definition written in first order logic, (2) the neural network architecture"
W16-5906,D14-1162,0,0.0807355,"tence(x,z) ∧ InSentence(y,z) ⇒ WorkFor(x,y) network: MultiLayer, Binary features: [""work_for_feats""] const: LiveIn(x,y) ⇒ Entity(x,""Per"") const: LiveIn(x,y) ⇒ Entity(y,""Loc"") const: WorkFor(y,z) ⇒ Entity(y,""Per"") const: WorkFor(y,z) ⇒ Entity(z,""Org"") Figure 2: Modeling the Relation Extraction problem using DR AI L Figure 1: Modeling POS Tagging using DR AI L In this example script, both rules are defined as multi-class prediction problems and are associated with Multi-layer neural network architectures (lines 3,7 respectively). We represent each word as a vector using Twitter Glove embedding (Pennington et al., 2014). We also use a vector representation for the previous POS tag (lines 4,8 respectively). Example 2: Entity-Relation Extraction Our second example focuses on a simplified version of the relation extraction task (Roth and Yih, 2007; Kordjamshidi et al., 2015), which identifies named entities and their categories (P ER , L OC , O RG) and two types of relations (L IVE I N , W ORK F OR) over these entities. In Figure 2 we illustrate the model definition for this task, and write the structural dependen56 DR AI L Elements The elementary units of the model are predicates, which represent relations in"
W16-5906,rizzolo-roth-2010-learning,0,0.204346,"ble of dealing with realistic problems require making predictions over multiple, often interdependent, variables. In such settings, correctly capturing the dependencies between these variables often takes precedence to the specific algorithm used for estimating the models’ parameters. Capturing these dependencies relies on compiling expert knowledge about the problem domain into the statistical model, and in recent years several machine learning systems offering intuitive interfaces for defining the dependencies between predictions were suggested (Domingos et al., 2006; McCallum et al., 2009; Rizzolo and Roth, 2010; Bach et al., 2015; Kordjamshidi et al., 2015). On the other hand, end-to-end deep learning methods, which are becoming increasingly popular, take an almost opposite approach. These methods map the complex input object to desired outputs ∗ * Equal contribution. directly, without decomposing the decision process into parts and modeling their dependencies. The recent advances in deep learning allow these methods to successfully learn such mappings over very high dimensional latent features space (Duchi et al., 2011; Srivastava et al., 2014; Bahdanau et al., 2014). At first glance these two tren"
W16-5906,W04-2401,0,0.180661,"Missing"
W16-5906,P15-1032,0,0.0276598,"ple, BLOG (Milch et al., 2005) and CHURCH (Goodman et al., 2012) were suggested for generative models, and MLN (Domingos et al., 2006), PSL (Bach et al., 2015), FACTORIE (McCallum et al., 2009), and CCM (Rizzolo and Roth, 2010; Kordjamshidi et al., 2015) were suggested for conditional models. In this paper we look into combining such declarative frameworks with deep learning models. Combining deep learning with structured models was studied by several works, typically in the context of a specific task or a specific inference procedure. These include dependency parsing (Chen and Manning, 2014; Weiss et al., 2015), transition systems (Andor et al., 2016) named entity recognition and sequence labeling systems (Ma and Hovy, 2016; Lample et al., 2016), and models for combining deep learning and graphical models for vision tasks (Zheng et al., 2015; Chen et al., 2015). 3 DR AI L Modeling Language The DR AI L modeling language provides a general way to define relational learning problems that are highly structured. A relational model is specified in DR AI L using a set of weighted first-order logic rule templates that describe predictions and express the dependencies and constraints of a specific domain. Ea"
W17-2913,W11-0702,0,0.0422794,"alysis with additional information. Our modeling approach is based on the observation that politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-J"
W17-2913,P13-2144,0,0.0263233,"2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and"
W17-2913,W13-1106,0,0.06263,"Missing"
W17-2913,D15-1008,0,0.0114544,"ic statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first tweet, several phrases indicate the frame: “exec. order”, “overreach of power”, “rights of law abiding Americans”, “our constitution”. In the second tweet, the relevant phrases are “this ruling” and “upheld a critical freedom”. All of these phrase"
W17-2913,N15-1171,0,0.100308,"microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012),"
W17-2913,W11-3702,0,0.0463612,"ze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first tweet, several phrases indicate the frame: “exec. order”, “overreach of power”, “rights of law abiding Americans”, “our constitution”. In the second tweet, the relevant phrases are “this ruling” and “upheld a critical freedom”. All of these phrases indicate that the same frame is being used in both tweets. However, analyzing the specific terminology in each case and the context in which it appears helps capture the ideological similarit"
W17-2913,P15-2072,0,0.0617029,"or handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Ge"
W17-2913,W12-3809,0,0.0264552,"s (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasja"
W17-2913,P17-1069,1,0.215074,"Missing"
W17-2913,D16-1105,0,0.0113408,"hat politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring"
W17-2913,N13-1037,0,0.0189705,"rums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is furt"
W17-2913,S17-2029,1,0.88045,"Missing"
W17-2913,D14-1214,0,0.0161573,"more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in"
W17-2913,L16-1591,0,0.0979848,"modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Ngu"
W17-2913,P14-1016,0,0.0269273,"more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in"
W17-2913,N13-1092,0,0.0309974,"Missing"
W17-2913,N09-1057,0,0.0215998,"2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also r"
W17-2913,P15-1139,0,0.0289716,"016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first tweet, several phrases indicate the frame: “exec. order”, “overreach of power”, “rights of law abiding Americans”, “our constitution”. In the second tweet, the relevant phrases are “this ruling” and “upheld a critical freedom”. All of these phrases indicate that the same frame is being used in both tweets. However, ana"
W17-2913,C14-1019,0,0.0179912,"nson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first tweet, several phrases indicate the frame: “exec. order”, “overreach of power”, “rights of law abiding Americans”, “our constitution”. In the"
W17-2913,D14-1083,0,0.0229526,"predictions that reflect different ideologies. ment the frame analysis with additional information. Our modeling approach is based on the observation that politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interaction"
W17-2913,P13-1162,0,0.0613022,"structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et"
W17-2913,N10-1020,0,0.0279823,"itical discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et"
W17-2913,P14-1105,0,0.0599535,"ored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first tweet, several phrases indicate the frame: “exec. order”, “overreach of power”, “rights of law abiding Americans”, “our constitution”. In the second tweet, the relevant phrases are “this ruling” and “upheld a critical freed"
W17-2913,D13-1010,0,0.0136351,"onal speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first tweet, several phrases indicate the frame: “exec. order”, “overreach of power”, “rights of law abiding Americans”, “our constitution”. In the second tweet, the relevant phrases are “this ruling” and “upheld a critical freedom”. All of these phrases indicate that th"
W17-2913,P09-1026,0,0.0860347,"Missing"
W17-2913,C16-1279,1,0.515419,"is based on the observation that politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works w"
W17-2913,W10-0214,0,0.0234386,"l information. Our modeling approach is based on the observation that politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al."
W17-2913,P15-1012,0,0.012589,"ts with similar frame predictions that reflect different ideologies. ment the frame analysis with additional information. Our modeling approach is based on the observation that politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling o"
W17-2913,P14-1017,0,0.0228161,"al networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015), voting patterns (Gerrish and Blei, 2012), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are also related to the study of framing on Twitter. Consider the two tweets in the example above. In the first twe"
W17-2913,P15-1157,0,0.0741049,"., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004). Important to the language analysis of our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter. The study of political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), ideology measurement and prediction (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), policies (Nguyen et al., 2015),"
W17-2913,P14-1018,0,0.0704594,"Missing"
W17-2913,N12-1072,0,0.019078,"es. ment the frame analysis with additional information. Our modeling approach is based on the observation that politicians often use slogans in both their tweets and speeches. These are key phrases used to indirectly indicate the political figures’ core beliefs and ideological stances. Identification of these phrases automatically decomposes the frames into more specific categories. 2 Related Work Previous computational works which analyze political discourse focus on opinion mining and stance prediction from forums and tweets (Sridhar et al., 2015; Hasan and Ng, 2014; AbuJbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridha"
W17-2913,Q14-1024,0,0.0125789,"Wiebe, 2010, 2009; Johnson and Goldwasser, 2016; Ebrahimi et al., 2016). A variety of social media based predictions have been studied including: prediction of political affiliation and other demographics of Twitter users (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011), profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), methods for handling unique microblog language (Eisenstein, 2013), and the modeling of social interactions and group structure in predictions (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014; Huang et al., 2012). Works which focus on inferring signed social networks (West et al., 2014) and collective classification using PSL (Bach et al., 2015) are similar to the modeling approach of Johnson et al. (2017b), which we extend in this paper. Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015). Framing is further related to works which analyze biased language (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wieb"
W19-2112,D15-1008,0,0.0135004,"over relational rep2 Related Works To the best of our knowledge, this is the first work to leverage the interaction of social networks and behavioral features on Twitter, in addition to language, for the task of weakly-supervised modeling and unsupervised classification of moral foundations implied in social media political discourse. Similar studies have used models which only employ language features for this task in a supervised setting (Johnson and Goldwasser, 2018). These language-based models serve as the baselines in our experimental analyses. Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are related to the study of abstract language, specifically political framing analysis which is a key feature in the language baseline of our approach. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang 101 M ORAL F OUNDATION AND D ES"
W19-2112,W11-3702,0,0.0539899,"guage, for the task of weakly-supervised modeling and unsupervised classification of moral foundations implied in social media political discourse. Similar studies have used models which only employ language features for this task in a supervised setting (Johnson and Goldwasser, 2018). These language-based models serve as the baselines in our experimental analyses. Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are related to the study of abstract language, specifically political framing analysis which is a key feature in the language baseline of our approach. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang 101 M ORAL F OUNDATION AND D ESCRIPTION 1. Care/Harm: Compassion for others, ability to empathize, prohibiting actions that harm. 2. Fairness/Cheating: Fairness, justice, reciprocity, rights, equality, proportionality, prohibit"
W19-2112,L16-1591,0,0.0280618,"by scraping for available tweet IDs, while still adhering to the terms of service, i.e., if a politician deletes a tweet, we are unable to recover it. This dataset will be made publicly available for use by the community. Table 1: Brief Descriptions of Moral Foundations. and Hart, 2015) has also been studied. Connections between morality dimensions and political ideology have been analyzed in the fields of psychology and sociology (Graham et al., 2009, 2012). Moral foundations have also been used via the Moral Foundations Dictionary (MFD) to identify the foundations in partisan news sources (Fulgoni et al., 2016) and to construct features for other downstream tasks (Volkova et al., 2017). Several recent works have explored using datadriven methods that go beyond the MFD to study tweets related to specific events, rather than policy issues, such as natural disasters (Garten et al., 2016; Lin et al., 2017). 3 CongressTweets. CongessTweets is a collection of the tweets of all congressional members in 2018 1 . To facilitate comparison with the Senate Tweets 2016 dataset, we used only the tweets of senators from this collection. This dataset and the Senate Tweets 2016 dataset (described previously) are use"
W19-2112,P14-1105,0,0.0258103,"ies high level rules over relational rep2 Related Works To the best of our knowledge, this is the first work to leverage the interaction of social networks and behavioral features on Twitter, in addition to language, for the task of weakly-supervised modeling and unsupervised classification of moral foundations implied in social media political discourse. Similar studies have used models which only employ language features for this task in a supervised setting (Johnson and Goldwasser, 2018). These language-based models serve as the baselines in our experimental analyses. Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are related to the study of abstract language, specifically political framing analysis which is a key feature in the language baseline of our approach. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang 101 M O"
W19-2112,W13-1106,0,0.0441646,"Missing"
W19-2112,P18-1067,1,0.879483,"ross the messages by similar party members on the same issues. We use Probabilistic Soft Logic (PSL) (Bach et al., 2013), which specifies high level rules over relational rep2 Related Works To the best of our knowledge, this is the first work to leverage the interaction of social networks and behavioral features on Twitter, in addition to language, for the task of weakly-supervised modeling and unsupervised classification of moral foundations implied in social media political discourse. Similar studies have used models which only employ language features for this task in a supervised setting (Johnson and Goldwasser, 2018). These language-based models serve as the baselines in our experimental analyses. Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are related to the study of abstract language, specifically political framing analysis which is a key feature in the language baseline of our approach. The association between Twitter and framing in molding pu"
W19-2112,C14-1019,0,0.0293457,"erage the interaction of social networks and behavioral features on Twitter, in addition to language, for the task of weakly-supervised modeling and unsupervised classification of moral foundations implied in social media political discourse. Similar studies have used models which only employ language features for this task in a supervised setting (Johnson and Goldwasser, 2018). These language-based models serve as the baselines in our experimental analyses. Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are related to the study of abstract language, specifically political framing analysis which is a key feature in the language baseline of our approach. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang 101 M ORAL F OUNDATION AND D ESCRIPTION 1. Care/Harm: Compassion for others, ability to empathize, prohibiting actions that h"
W19-2112,D13-1010,0,0.225414,"lated Works To the best of our knowledge, this is the first work to leverage the interaction of social networks and behavioral features on Twitter, in addition to language, for the task of weakly-supervised modeling and unsupervised classification of moral foundations implied in social media political discourse. Similar studies have used models which only employ language features for this task in a supervised setting (Johnson and Goldwasser, 2018). These language-based models serve as the baselines in our experimental analyses. Ideology measurement (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013; Djemili et al., 2014), political sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), and polls based on Twitter political sentiment (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010) are related to the study of abstract language, specifically political framing analysis which is a key feature in the language baseline of our approach. The association between Twitter and framing in molding public opinion of events and issues (Burch et al., 2015; Harlow and Johnson, 2011; Meraz and Papacharissi, 2013; Jang 101 M ORAL F OUNDATION AND D ESCRIPTION 1. Care/H"
W19-2112,P17-2102,0,0.0298216,"ervice, i.e., if a politician deletes a tweet, we are unable to recover it. This dataset will be made publicly available for use by the community. Table 1: Brief Descriptions of Moral Foundations. and Hart, 2015) has also been studied. Connections between morality dimensions and political ideology have been analyzed in the fields of psychology and sociology (Graham et al., 2009, 2012). Moral foundations have also been used via the Moral Foundations Dictionary (MFD) to identify the foundations in partisan news sources (Fulgoni et al., 2016) and to construct features for other downstream tasks (Volkova et al., 2017). Several recent works have explored using datadriven methods that go beyond the MFD to study tweets related to specific events, rather than policy issues, such as natural disasters (Garten et al., 2016; Lin et al., 2017). 3 CongressTweets. CongessTweets is a collection of the tweets of all congressional members in 2018 1 . To facilitate comparison with the Senate Tweets 2016 dataset, we used only the tweets of senators from this collection. This dataset and the Senate Tweets 2016 dataset (described previously) are used in Section 6 for the qualitative application of our models to the analysis"
