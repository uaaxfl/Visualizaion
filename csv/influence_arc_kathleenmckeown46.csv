1994.amta-1.11,P91-1022,0,0.565217,"Missing"
1994.amta-1.11,P93-1002,0,0.310201,"Missing"
1994.amta-1.11,P93-1001,0,0.614472,"Missing"
1994.amta-1.11,C94-2178,1,0.869399,"Missing"
1994.amta-1.11,J93-1004,0,0.449974,"Missing"
1994.amta-1.11,1992.tmi-1.7,0,0.436765,"Missing"
1994.amta-1.11,P94-1012,0,0.776427,"Missing"
1994.amta-1.11,1994.amta-1.26,0,0.835444,"Missing"
2011.mtsummit-papers.62,P05-1074,0,0.0230514,"Missing"
2011.mtsummit-papers.62,2000.iwpt-1.9,0,0.776082,"Missing"
2011.mtsummit-papers.62,W10-1742,0,0.0570337,"Missing"
2011.mtsummit-papers.62,W10-1747,0,0.147274,"ce. We then use a text-to-text generator operating on those lattices to generate those hypotheses. We filter ungrammatical combinations using a feature-based lexicalized tree adjoining grammars (FB-LTAG) and then use a TER-based metric to compute a consensus score function to select the best translation among grammatical hypotheses. The system combination gains 1.38 BLEU points over the best individual system. 1 Introduction Recently many MT combination approaches have been presented. Consensus network (CN) decoding (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Rosti et al. 2010; Leusch and Ney, 2010) is one of the most successful approaches, in which the words in all hypotheses are aligned with a backbone hypothesis. A word-based lattice is then formed with word alternatives, including nulls, each with associated scores from voting or other confidence scores. Then, the combined translation sentence(s) can be produced with the same word order as the backbone by selecting the path with the highest score(s) along the lattice. 546 In this paper, rather than use the CN decoding framework, we borrow the idea of text-to-text generation, which has been used successfully for sentence fusion as par"
2011.mtsummit-papers.62,W10-1746,0,0.0373586,"Missing"
2011.mtsummit-papers.62,N07-1029,0,0.463074,"ting all possible hypotheses for the same source sentence. We then use a text-to-text generator operating on those lattices to generate those hypotheses. We filter ungrammatical combinations using a feature-based lexicalized tree adjoining grammars (FB-LTAG) and then use a TER-based metric to compute a consensus score function to select the best translation among grammatical hypotheses. The system combination gains 1.38 BLEU points over the best individual system. 1 Introduction Recently many MT combination approaches have been presented. Consensus network (CN) decoding (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Rosti et al. 2010; Leusch and Ney, 2010) is one of the most successful approaches, in which the words in all hypotheses are aligned with a backbone hypothesis. A word-based lattice is then formed with word alternatives, including nulls, each with associated scores from voting or other confidence scores. Then, the combined translation sentence(s) can be produced with the same word order as the backbone by selecting the path with the highest score(s) along the lattice. 546 In this paper, rather than use the CN decoding framework, we borrow the idea of text-to-text generation, w"
2012.amta-papers.11,W07-0726,0,0.116305,"escribed above are based on the estimation of the degree of agreement between a phrase with another phrase. Now our question is: how can we estimate and utilize the agreement degree between a set of consecutive phrases with another set of consecutive phrases during decoding? In lattice-based combination, this issue has not been addressed before. Our solution is simple; –we consider N-gram consensus in addition to the confidence estimations for paraphrases during decoding. This idea of considering N-gram consensus was widely used in N-best list reranking (Chen et al., 2005; Zens and Ney, 2006; Chen et al., 2007). These years the technique has also been presented in some wordbased combination schemes and proven effectives. The approaches can be divided into two categories: one is based on a sentence-specific LM, built on translation hypotheses of multiple systems (Zhao and He 2009; Heafield and Lavie 2010); the other one is based on a corpus-based LM, built on the whole tuning/test corpus of all translation hypotheses of multiple systems (Matusov et al, 2008; Leusch et al, 2011). The strength of sentence-specific LM is that it considers the most specific data available while the corpus-based LM has th"
2012.amta-papers.11,W10-1746,0,0.044251,"Missing"
2012.amta-papers.11,P07-1040,0,0.248842,"based MT techniques in the combination framework. We show how phrase extraction rules and confidence estimations inspired from machine translation improve results. We also propose system-specific LMs for estimating N-gram consensus. Our results show that our approach yields a strong improvement over the best single MT system and competes with other stateof-the-art combination systems. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Confusion Network (CN) decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). A CN is a linear word lattice structure, in which the words in all translation hypotheses are aligned against the corresponding words of a selected backbone hypothesis. Each word in the CN is assigned a confidence score and the decoder Kathleen McKeown simply finds the path with the highest sum of these scores. In addition to word-level combination approaches, such as CN decoding, some phraselevel combination techniques have also recently been presented; their goal is to retain coherence and consistency between the words"
2012.amta-papers.11,2006.amta-papers.25,0,0.0432916,"d be produced as follows: Eb : w1 w2 w3 w4 w5 S P T D Eh : w 2 w 1 w 3 ε Y ε ε I [ w6 w7 I w4 w5 w6 w8] w9 P [ [ w10 w11] M w 8 w 7] w 10 P [ w9 ] Fig 2. The alignment between Eb and reordered Eh 3 Monolingual Word Alignment Our paraphrases are deduced from monolingual word alignment. Any monolingual word aligner can serve the purpose. Since in our implementation, we adopt TERp as our alignment tool, we briefly review it and use a virtual example to illustrate its alignment output format and how we slightly adjust the format to meet our needs. TERp (Snover et al. 2009) is an extension of TER (Snover et al. 2006). Both TERp and TER are automatic evaluation metrics for MT, based on measuring the ratio of the number of edit operations between the reference sentence and the MT system hypothesis. TERp uses all the edit operations of TER—Matches, Insertions, Deletions, Substitutions and Shifts—as well as three new edit operations: Stem Matches, Synonym Matches and Paraphrases. TERp identifies the Stem Matches and Synonym Matches using the Porter stemming algorithm (Porter, 1980) and WordNet (Fellbaum, 1998) respectively. Sequences of words in the reference are considered to be paraphrases of a sequence of"
2012.amta-papers.11,W11-2121,0,0.0389505,"Missing"
2012.amta-papers.11,D07-1029,0,\N,Missing
2012.amta-papers.11,E06-1005,0,\N,Missing
2012.amta-papers.11,W09-0441,0,\N,Missing
2012.amta-papers.11,W06-3110,0,\N,Missing
2012.amta-papers.11,P08-2021,0,\N,Missing
2012.amta-papers.11,W08-0329,0,\N,Missing
2012.amta-papers.11,D09-1115,0,\N,Missing
2012.amta-papers.11,P11-1125,0,\N,Missing
2012.amta-papers.11,N03-1017,0,\N,Missing
2012.amta-papers.11,N07-1029,0,\N,Missing
2012.amta-papers.11,N09-2052,0,\N,Missing
2012.amta-papers.11,D08-1011,0,\N,Missing
2012.amta-papers.11,W11-2118,0,\N,Missing
2012.amta-papers.11,2005.iwslt-1.11,0,\N,Missing
2012.amta-papers.11,W09-0405,0,\N,Missing
2012.amta-papers.11,W09-0407,0,\N,Missing
2012.amta-papers.11,P03-1021,0,\N,Missing
2012.amta-papers.11,2010.amta-papers.34,0,\N,Missing
2012.amta-papers.11,2010.amta-papers.9,0,\N,Missing
2012.amta-papers.12,D07-1090,0,0.0406081,"Missing"
2012.amta-papers.12,condon-etal-2010-evaluation,0,0.0288918,"Missing"
2012.amta-papers.12,J10-3008,0,0.0205943,"Missing"
2012.amta-papers.12,2008.amta-govandcom.8,0,0.106547,"Missing"
2012.amta-papers.12,P05-1045,0,0.00385752,"val accuracy, and focuses on sentences that should be relevant but are judged irrelevant due to errors in result translation. End-to-end (TLIR): MT is used for both retrieval and relevance annotation. This setting represents the full end-to-end TLIR system, where MT has an impact on both retrieval and result understanding. 2 Comparing query translation methods is not the focus of this paper, so query translations are identical across all experimental settings. HT is never used for query translation. 4 4.1 Experiments Query Extraction Queries were created by running the Stanford NE recognizer (Finkel et al., 2005) on one of the HTs. This list of all possible NE queries for the corpus was filtered to remove near-duplicates and incorrectly tagged phrases. After relevance annotation, queries that had one or zero results were filtered. The average number of relevant sentences per query was 8 for NW and 5 for WB. 4.2 MT systems We use two pre-existing state-of-the-art ArabicEnglish SMT systems with widely different implementations MT A was built using HiFST (de Gispert et al., 2010), a hierarchical phrase-based SMT system implemented using finite state transducers. It is trained on all the parallel corpora"
2012.amta-papers.12,P08-1045,0,0.0455666,"Missing"
2012.amta-papers.12,P07-2045,0,0.00489355,"e-of-the-art ArabicEnglish SMT systems with widely different implementations MT A was built using HiFST (de Gispert et al., 2010), a hierarchical phrase-based SMT system implemented using finite state transducers. It is trained on all the parallel corpora in the NIST MT08 Arabic Constrained Data track (5.9M parallel sentences). The first-pass 4-gram language model (LM) is trained on the English side of the parallel text and Gigaword 3. The second-pass 5-gram LM is a zerocutoff stupid-backoff (Brants et al., 2007) estimated using 6.6B words of English newswire text. MT B was built using Moses (Koehn et al., 2007), and is a non-hierarchical phrase-based system. It is trained on 3.2M sentences of parallel text using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using"
2012.amta-papers.12,P09-2084,0,0.0241542,"built from Wikipedia, the CIA world factbook and the NEs from the Buckwalter analyzer dictionary (Buckwalter, 2004). Since all of the queries are NEs, this dictionary is a high-precision, but low recall resource. If the translation is not found, a phrase table from MT B is used as a dictionary. The final back-off searches a large corpus of machine translated documents, which is a lower precision resource. This resource simulates the task context that a large CLIR corpus would provide; prior work has shown that words that are deleted in one sentence are often successfully translated in others (Ma and McKeown, 2009). If the translation is still not found, the original query is expanded using synonyms extracted from Wikipedia, and then the cascaded translation is applied again. 5 Analysis of Baselines Figure 2 shows results from all experimental settings; in this section we discuss the results for QT and DT. All of the models are evaluated on the newswire and web genres (the top and bottom charts, respectively) and MT A and MT B (left and 3 http://www.crowdflower.com right, respectively). Each setting is analyzed four different ways, summarized in Table 1. This analysis demonstrates the strengths and weak"
2012.amta-papers.12,P99-1027,0,0.060703,"end TLIR task. For instance, in the NW genre, QT has higher MAP than DT in the CLIR evaluation (Lost in Retrieval), but DT has higher (translated) MAP than QT in the TLIR evaluation (End-to-End). This highlights the limitations of evaluating CLIR models without taking result translation into account. 6 Lost and Found in Retrieval The baseline QT and DT models are both severely affected by errors in MT. A better retrieval model would retrieve all the relevant results, but rank the translations that appear relevant the highest. Several hybrid methods for combining QT and DT exist. For instance, McCarley (1999) and Chen and Gey (2003) describe methods for combining the results of separate QT and DT searches using re-ranking. We chose to use the simultaneous multilingual IR (SMLIR) model from (Parton et al., 2008) because it requires only a single index and a single search at runtime, and does not require tuning a re-ranker. SMLIR: In the SMLIR model, each sentence is indexed as a bilingual sentence with different fields for each language, and the structured query is composed of both query-language and documentlanguage terms. In a CLIR evaluation without result translation, SMLIR outperformed both QT"
2012.amta-papers.12,P02-1040,0,0.0821984,"Missing"
2012.amta-papers.12,2012.eamt-1.34,1,0.742334,"Missing"
2012.amta-papers.12,W07-0707,0,0.0223677,"Missing"
2012.amta-papers.12,P10-1063,0,0.0608833,"Missing"
2012.amta-papers.12,2011.mtsummit-papers.58,0,0.0630676,"Missing"
2012.amta-papers.12,stymne-ahrenberg-2010-using,0,0.0405724,"Missing"
2012.amta-papers.12,vilar-etal-2006-error,0,0.0338232,"Missing"
2012.amta-wptp.5,J10-3008,0,0.0251991,"Missing"
2012.amta-wptp.5,W11-2107,0,0.0190662,"adequacy metrics in certain cases, when compared with both the original MT output and heuristic insertion. 2 Related Work Our work builds on Parton et al. (2012) who compellingly show that a feedback and rule-based APE each have different advantages. The feedback post editor adds several potential corrections to the MT phrase table and feeds the updates back into another pass through the MT decoder, while the rulebased editor inserts the top-ranked correction directly into the original MT output. They found while the feedback system was preferred by the TERp (Snover et al., 2009) and Meteor (Denkowski and Lavie, 2011) automated adequacy metrics, the rule-based system was perceived to improve adequacy more often by human reviewers, often at the expense of fluency, noting that “with extra effort, the meaning of these sentences can usually be inferred, especially when the rest of the sentence is ﬂuent.” Our work attempts to increase adequacy through better insertion. Previous general APE systems target specific types of MT errors, like determiner selection (Knight and Chandler, 1994), grammatical errors (Doyon et al., 2008), and adequacy errors (Parton et al. 2012). In contrast, fully adaptive APE systems try"
2012.amta-wptp.5,2008.amta-govandcom.8,0,0.0140321,"the feedback system was preferred by the TERp (Snover et al., 2009) and Meteor (Denkowski and Lavie, 2011) automated adequacy metrics, the rule-based system was perceived to improve adequacy more often by human reviewers, often at the expense of fluency, noting that “with extra effort, the meaning of these sentences can usually be inferred, especially when the rest of the sentence is ﬂuent.” Our work attempts to increase adequacy through better insertion. Previous general APE systems target specific types of MT errors, like determiner selection (Knight and Chandler, 1994), grammatical errors (Doyon et al., 2008), and adequacy errors (Parton et al. 2012). In contrast, fully adaptive APE systems try to learn to correct all types of errors by example, and can be thought of as statistical MT systems that translate from bad text in the target language to good text in the target language (Simard et al., 2007; Ueffing et al., 2008; Kuhn et al., 2011). Similarly, Dugast et al. (2007) present the idea of statistical post editing, that is, using bad MT output and good reference output as training data for post editing. As their system proves more adept at correcting certain types of errors than others, they su"
2012.amta-wptp.5,W07-0732,0,0.017035,"the sentence is ﬂuent.” Our work attempts to increase adequacy through better insertion. Previous general APE systems target specific types of MT errors, like determiner selection (Knight and Chandler, 1994), grammatical errors (Doyon et al., 2008), and adequacy errors (Parton et al. 2012). In contrast, fully adaptive APE systems try to learn to correct all types of errors by example, and can be thought of as statistical MT systems that translate from bad text in the target language to good text in the target language (Simard et al., 2007; Ueffing et al., 2008; Kuhn et al., 2011). Similarly, Dugast et al. (2007) present the idea of statistical post editing, that is, using bad MT output and good reference output as training data for post editing. As their system proves more adept at correcting certain types of errors than others, they suggest the possibility of a hybrid post editing system, “breaking down the ‘statistical layer’ into different components/tools each specialized in a narrow and accurate area,” which is similar to the approach followed in this paper. Isabelle et al. (2007) also use learning methods to replace the need for a manually constructed post editing dictionary. While they study a"
2012.amta-wptp.5,P05-1045,0,0.00629001,"Missing"
2012.amta-wptp.5,2007.mtsummit-papers.34,0,0.026215,"anguage to good text in the target language (Simard et al., 2007; Ueffing et al., 2008; Kuhn et al., 2011). Similarly, Dugast et al. (2007) present the idea of statistical post editing, that is, using bad MT output and good reference output as training data for post editing. As their system proves more adept at correcting certain types of errors than others, they suggest the possibility of a hybrid post editing system, “breaking down the ‘statistical layer’ into different components/tools each specialized in a narrow and accurate area,” which is similar to the approach followed in this paper. Isabelle et al. (2007) also use learning methods to replace the need for a manually constructed post editing dictionary. While they study a corpus of MT output and manually post-edited text to derive a custom dictionary, our system attempts to learn the rules for a specific type of edit: missing word insertion. Taking a statistical approach to system combination, Zwarts and Dras (2008) built a classifier to analyze the syntax of candidate translations and use abnormalities to weed out bad options. Our classifier could be seen as a special case of this, looking for an area of bad syntax where a word was potentially"
2012.amta-wptp.5,N07-1008,0,0.026268,"Missing"
2012.amta-wptp.5,P07-2045,0,0.00367775,"ased on work discussed in Zheng et al. (2009). It was trained on 2.3 million parallel sentences, predominantly newswire with small amounts of forum, weblog, and broadcast news data. 3.3 Error Detection and Correction Errors are detected by locating mistranslated named entities (for Arabic only) and content words that are translated as function words or not translated at all, by looking at alignments and POS tags (Parton and McKeown, 2010). Arabic error corrections are looked up in a variety of dictionaries, including an MT phrase table with probabilities from a second Arabic MT system, Moses (Koehn et al., 2007), using data from the GALE program available from LDC (LDC2004T17, LDC2004E72, LDC2005E46, LDC2004T18, LDC2007T08, and LDC2004E13). Secondary sources include an English synonym dictionary from the CIA World Factbook1, and dictionaries extracted from Wikipedia and the Buckwalter analyzer (Buckwalter, 2004). Arabic additionally uses a large parallel background corpus of 120,000 Arabic newswire and web documents and their machine translations from a separate, third Arabic MT system, IBM’s Direct Translation Model 2 (Ittycheriah 2007). Chinese corrections are looked up in the phrase table of our C"
2012.amta-wptp.5,W11-2206,0,0.0169558,"de an English synonym dictionary from the CIA World Factbook1, and dictionaries extracted from Wikipedia and the Buckwalter analyzer (Buckwalter, 2004). Arabic additionally uses a large parallel background corpus of 120,000 Arabic newswire and web documents and their machine translations from a separate, third Arabic MT system, IBM’s Direct Translation Model 2 (Ittycheriah 2007). Chinese corrections are looked up in the phrase table of our Chinese MT, SRI&apos;s SRInterp system (Zheng et al., 2009), and also in a dictionary extracted from forum data, Wikipedia and similar sources (Ji et al., 2009; Lin et al., 2011). 3.4 Synthesizing a Gold Standard Once an error is detected and a high-probability replacement is found, it must be inserted into the existing MT output. The straightforward solution is to use standard machine learning techniques to adapt to the translation errors made by a specific MT system on a specific language, but doing this is 1 http://www.cia.gov/library/publications/the-world-factbook “tourism” Arabic: ... بلدا02 اعلنت وزارة السياحة المصرية اليوم الخميس ان مسؤولين سياحيين من حوالى MT: Egyptian Ministry of Tourism announced today , Thursday , that officials from about 20 countries"
2012.amta-wptp.5,P09-2084,0,0.0452038,"Missing"
2012.amta-wptp.5,W12-3122,0,0.011437,"lus metric (Snover et al., 2009) provides a variety of techniques for aligning a hypothesis to a reference translation, as well as determining translation adequacy amongst deletions and substitutions. While we use TER-Plus as a metric, we also use it as a guide for determining where a missing word should be inserted to maximize adequacy against a reference. While our effort focuses on learning the highest adequacy insertion from examples with reference translations, there is significant work in trying to assess adequacy directly from source and target, without references (Specia et al., 2011; Mehdad et al., 2012). 3 Method The APE has 3 major phases: error detection, correction, and insertion. The first two phases are performed identically as described in Parton et al. (2012) and will be summarized briefly here, while the third phase differs substantially and will be described in greater detail. 3.1 Input and Pre-processing We constructed two separate pipelines for Arabic and Chinese. The Arabic data was tagged using MADA+TOKEN (Habash et al., 2009). Translated English output was recased with Moses, and POS and NER tags were applied using the Stanford POS tagger (Toutanova et al., 2003) and NER tagger"
2012.amta-wptp.5,P02-1040,0,0.0839604,"Missing"
2012.amta-wptp.5,2012.eamt-1.34,1,0.769481,"Missing"
2012.amta-wptp.5,N07-1064,0,0.0306911,"ng of these sentences can usually be inferred, especially when the rest of the sentence is ﬂuent.” Our work attempts to increase adequacy through better insertion. Previous general APE systems target specific types of MT errors, like determiner selection (Knight and Chandler, 1994), grammatical errors (Doyon et al., 2008), and adequacy errors (Parton et al. 2012). In contrast, fully adaptive APE systems try to learn to correct all types of errors by example, and can be thought of as statistical MT systems that translate from bad text in the target language to good text in the target language (Simard et al., 2007; Ueffing et al., 2008; Kuhn et al., 2011). Similarly, Dugast et al. (2007) present the idea of statistical post editing, that is, using bad MT output and good reference output as training data for post editing. As their system proves more adept at correcting certain types of errors than others, they suggest the possibility of a hybrid post editing system, “breaking down the ‘statistical layer’ into different components/tools each specialized in a narrow and accurate area,” which is similar to the approach followed in this paper. Isabelle et al. (2007) also use learning methods to replace the"
2012.amta-wptp.5,W09-0441,0,0.289485,"m can improve automated and human adequacy metrics in certain cases, when compared with both the original MT output and heuristic insertion. 2 Related Work Our work builds on Parton et al. (2012) who compellingly show that a feedback and rule-based APE each have different advantages. The feedback post editor adds several potential corrections to the MT phrase table and feeds the updates back into another pass through the MT decoder, while the rulebased editor inserts the top-ranked correction directly into the original MT output. They found while the feedback system was preferred by the TERp (Snover et al., 2009) and Meteor (Denkowski and Lavie, 2011) automated adequacy metrics, the rule-based system was perceived to improve adequacy more often by human reviewers, often at the expense of fluency, noting that “with extra effort, the meaning of these sentences can usually be inferred, especially when the rest of the sentence is ﬂuent.” Our work attempts to increase adequacy through better insertion. Previous general APE systems target specific types of MT errors, like determiner selection (Knight and Chandler, 1994), grammatical errors (Doyon et al., 2008), and adequacy errors (Parton et al. 2012). In c"
2012.amta-wptp.5,2011.mtsummit-papers.58,0,0.0786567,"Missing"
2012.amta-wptp.5,N03-1033,0,0.00393888,"cia et al., 2011; Mehdad et al., 2012). 3 Method The APE has 3 major phases: error detection, correction, and insertion. The first two phases are performed identically as described in Parton et al. (2012) and will be summarized briefly here, while the third phase differs substantially and will be described in greater detail. 3.1 Input and Pre-processing We constructed two separate pipelines for Arabic and Chinese. The Arabic data was tagged using MADA+TOKEN (Habash et al., 2009). Translated English output was recased with Moses, and POS and NER tags were applied using the Stanford POS tagger (Toutanova et al., 2003) and NER tagger (Finkel et al.,2005). For Chinese data, POS tags were applied to both source and output using the Stanford POS tagger (Toutanova et al., 2003). 3.2 MT systems The Arabic MT system is an implementation of HiFST (de Gispert et al., 2010) trained on corpora from the NIST MT08 Arabic Constrained Data track (5.9M parallel sentences, 150M words per language). The Chinese MT system is the SRInterp system, developed by SRI for the DARPA BOLT project, based on work discussed in Zheng et al. (2009). It was trained on 2.3 million parallel sentences, predominantly newswire with small amoun"
2012.amta-wptp.5,C08-1145,0,0.0158321,"suggest the possibility of a hybrid post editing system, “breaking down the ‘statistical layer’ into different components/tools each specialized in a narrow and accurate area,” which is similar to the approach followed in this paper. Isabelle et al. (2007) also use learning methods to replace the need for a manually constructed post editing dictionary. While they study a corpus of MT output and manually post-edited text to derive a custom dictionary, our system attempts to learn the rules for a specific type of edit: missing word insertion. Taking a statistical approach to system combination, Zwarts and Dras (2008) built a classifier to analyze the syntax of candidate translations and use abnormalities to weed out bad options. Our classifier could be seen as a special case of this, looking for an area of bad syntax where a word was potentially dropped. As noted though, the MT system’s language model often “patches up” the syntax around the missing word, leading to areas that are syntactically valid, though inadequate. The TER-Plus metric (Snover et al., 2009) provides a variety of techniques for aligning a hypothesis to a reference translation, as well as determining translation adequacy amongst deletio"
2012.eamt-1.34,D07-1090,0,0.0231043,"urced human adequacy judgments. 5.1 MT Systems We used state-of-the art Arabic-English MT systems with widely different implementations. MT A was built using HiFST (de Gispert et al., 115 2010), a hierarchical phrase-based SMT system implemented using finite state transducers. It is trained on all the parallel corpora in the NIST MT08 Arabic Constrained Data track (5.9M parallel sentences, 150M words per language). The first-pass 4-gram language model (LM) is trained on the English side of the parallel text and a subset of Gigaword 3. The second-pass 5-gram LM is a zero-cutoff stupid-backoff (Brants et al., 2007) estimated using 6.6B words of English newswire text. MT B was built using Moses (Koehn et al., 2007), and is a non-hierarchical phrase-based system. It is trained on 3.2M sentences of parallel text (65M words on the English side) using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN"
2012.eamt-1.34,W07-0718,0,0.119192,"Missing"
2012.eamt-1.34,condon-etal-2010-evaluation,0,0.0924787,"Missing"
2012.eamt-1.34,J10-3008,1,0.869312,"Missing"
2012.eamt-1.34,W11-2107,0,0.0302277,"LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU, since they allow matching across stems, synonyms and paraphrases. Meteor-a and TERp-a are both tuned to have high correlation with human adequacy judgments. In contrast to automatic system-level metrics, human judgments can give a nuanced sentencelevel view of particular aspects of the MT. In order to"
2012.eamt-1.34,2008.amta-govandcom.8,0,0.625824,"Missing"
2012.eamt-1.34,2006.eamt-1.27,0,0.119724,"Missing"
2012.eamt-1.34,P05-1045,0,0.00482056,"ons for the errors, and 3) apply the suggestions. All the APEs use identical algorithms for steps 1 and 2, and only differ in how they apply the suggestions. The algorithms are language-pair independent, though we carried out all of our experiments on Arabic-English MT. 4.1 Pre-Processing The Arabic source text was analyzed and tokenized using MADA+TOKAN (Habash et al., 2009). Each MT system used a different tokenization scheme, so the source sentences were processed in two separate pipelines. Separate named entity recognizers (NER) were built for each pipeline using the Stanford NER toolkit (Finkel et al., 2005), by training on CoNLL and ACE data. Each translated English sentence was re-cased using Moses and then analyzed using the Stanford CoreNLP pipeline to get part-of-speech (POS) tags (Toutanova et al., 2003) and NER (Finkel et al., 2005). 4.2 Detecting Errors and Suggesting Corrections The APEs address specific adequacy errors that we have found to be most detrimental for the CLQA task: content words that are not translated at all, content words that are translated to function words, and mistranslated named entities. In the error detection step, these types of errors are detected via an algorit"
2012.eamt-1.34,2007.mtsummit-papers.34,0,0.285131,"Missing"
2012.eamt-1.34,P07-2045,0,0.0106204,"widely different implementations. MT A was built using HiFST (de Gispert et al., 115 2010), a hierarchical phrase-based SMT system implemented using finite state transducers. It is trained on all the parallel corpora in the NIST MT08 Arabic Constrained Data track (5.9M parallel sentences, 150M words per language). The first-pass 4-gram language model (LM) is trained on the English side of the parallel text and a subset of Gigaword 3. The second-pass 5-gram LM is a zero-cutoff stupid-backoff (Brants et al., 2007) estimated using 6.6B words of English newswire text. MT B was built using Moses (Koehn et al., 2007), and is a non-hierarchical phrase-based system. It is trained on 3.2M sentences of parallel text (65M words on the English side) using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used"
2012.eamt-1.34,P09-2084,0,0.399041,"ionary extracted from the Buckwalter analyzer (Buckwalter, 2004) and an English synonym dictionary from the CIA World Factbook.1 They are high precision and low recall: most errors do not have matches in the dictionaries, but when they do, they are often correct, particularly for NEs. 1 http://www.cia.gov/library/publications/the-world-factbook 113 Background MT corpus: Since our motivation is CLQA, we also draw on a resource specific to CLQA: a background corpus of about 120,000 Arabic newswire and web documents that have been translated into English by a state-of-the-art industry MT system. Ma and McKeown (2009) were able to exploit a similar pseudo-parallel corpus to correct deleted verbs, since words deleted in one sentence are frequently correctly translated in other sentences. For each error, the source-language phrase is converted into a query to search all three resources. Then the target-language results are aggregated and ranked by overall confidence scores. The confidence scores are a weighted combination of phrase translation probability, number of dictionary matches and term frequencies in the background corpus. The weights were set manually on a development corpus. 4.3 Rule-Based APE Tabl"
2012.eamt-1.34,P02-1040,0,0.0914811,"orpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU, since they allow matching across stems, synonyms and paraphrases. Meteor-a and TERp-a are both tuned to have high correlation with human adequacy judgments. In contrast to automatic system-level metrics, human judgments can give a nuanced sentencelevel view of part"
2012.eamt-1.34,C10-2109,1,0.93042,"was re-cased using Moses and then analyzed using the Stanford CoreNLP pipeline to get part-of-speech (POS) tags (Toutanova et al., 2003) and NER (Finkel et al., 2005). 4.2 Detecting Errors and Suggesting Corrections The APEs address specific adequacy errors that we have found to be most detrimental for the CLQA task: content words that are not translated at all, content words that are translated to function words, and mistranslated named entities. In the error detection step, these types of errors are detected via an algorithm from prior work that uses bilingual POS tags and word alignments (Parton and McKeown, 2010). Each flagged error consists of one or more source-language tokens and zero or more target-language tokens. In the error correction step, the source and target sentences and all the flagged errors are passed to the suggestion generator, which uses the following three resources. Phrase Table: The phrase table from MT B is used as a phrase dictionary (described in more detail in ??). Dictionaries: We also use a translation dictionary extracted from Wikipedia, a bilingual name dictionary extracted from the Buckwalter analyzer (Buckwalter, 2004) and an English synonym dictionary from the CIA Worl"
2012.eamt-1.34,W07-0707,0,0.119951,"Missing"
2012.eamt-1.34,P06-1001,1,0.825975,"is trained on 3.2M sentences of parallel text (65M words on the English side) using several LDC corpora including some available only through the GALE program (e.g., LDC2004T17, LDC2004E72, LDC2005E46 and LDC2004T18). The data includes some sentences from the ISI corpus (LDC2007T08) and UN corpus (LDC2004E13) selected to specifically add vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU,"
2012.eamt-1.34,N07-1064,0,0.200693,"Missing"
2012.eamt-1.34,W09-0441,0,0.0362319,"d vocabulary absent in the other resources. The Arabic text is tokenized and lemmatized using the MADA+TOKAN system (Habash et al., 2009). Lemmas are used for Giza++ alignment only. The tokenization scheme used is the Penn Arabic Treebank scheme (Habash, 2010; Sadat and Habash, 2006). The system uses a 5-gram LM that was trained on Gigaword 4. Both systems are tuned for BLEU score using MERT. 5.2 Automatic and Human Evaluation We ran several automatic metrics on the baseline MT output and the post-edited MT output: BLEU (Papineni et al., 2002), Meteor-a (Denkowski and Lavie, 2011) and TERp-a (Snover et al., 2009). BLEU is based on n-gram precision, while Meteor takes both precision and recall into account. TERp also implicitly takes precision and recall into account, since it is similar to edit distance. Both Meteor and TERp allow more flexible n-gram matching than BLEU, since they allow matching across stems, synonyms and paraphrases. Meteor-a and TERp-a are both tuned to have high correlation with human adequacy judgments. In contrast to automatic system-level metrics, human judgments can give a nuanced sentencelevel view of particular aspects of the MT. In order to compare adequacy across APEs, we"
2012.eamt-1.34,2011.mtsummit-papers.58,0,0.271781,"Missing"
2012.eamt-1.34,stymne-ahrenberg-2010-using,0,0.0853294,"Missing"
2012.eamt-1.34,2011.mtsummit-papers.16,0,0.0686158,"Missing"
2012.eamt-1.34,N03-1033,0,0.00674048,"though we carried out all of our experiments on Arabic-English MT. 4.1 Pre-Processing The Arabic source text was analyzed and tokenized using MADA+TOKAN (Habash et al., 2009). Each MT system used a different tokenization scheme, so the source sentences were processed in two separate pipelines. Separate named entity recognizers (NER) were built for each pipeline using the Stanford NER toolkit (Finkel et al., 2005), by training on CoNLL and ACE data. Each translated English sentence was re-cased using Moses and then analyzed using the Stanford CoreNLP pipeline to get part-of-speech (POS) tags (Toutanova et al., 2003) and NER (Finkel et al., 2005). 4.2 Detecting Errors and Suggesting Corrections The APEs address specific adequacy errors that we have found to be most detrimental for the CLQA task: content words that are not translated at all, content words that are translated to function words, and mistranslated named entities. In the error detection step, these types of errors are detected via an algorithm from prior work that uses bilingual POS tags and word alignments (Parton and McKeown, 2010). Each flagged error consists of one or more source-language tokens and zero or more target-language tokens. In"
2012.eamt-1.34,vilar-etal-2006-error,0,0.177624,"Missing"
2012.eamt-1.34,W11-2152,0,\N,Missing
2012.eamt-1.34,2010.iwslt-keynotes.2,0,\N,Missing
2012.eamt-1.34,P10-2033,1,\N,Missing
2020.acl-main.453,1998.amta-tutorials.1,0,0.755759,"Missing"
2020.acl-main.453,P18-1060,0,0.0374543,"Missing"
2020.acl-main.453,P18-1063,0,0.444653,"(e.g., (Cheng and Lapata, 2016; Grusky et al., 2018; Paulus et al., 2017)) focuses; chapters are on average seven times longer than news articles. There is no one-to-one correspondence between summary and chapter sentences, and the summaries in our dataset use extensive paraphrasing, while news summaries copy most of their information from the words used in the article. We focus on the task of content selection, taking an initial, extractive summarization approach given the task difficulty.1 As the reference sum∗ Equal contribution. Work done while at Amazon. We tried two abstractive models (Chen and Bansal, 2018; Liu and Lapata, 2019) but ROUGE was low and the output was poor with many repetitions and hallucinations. 1 maries are abstractive, training our model requires creating a gold-standard set of extractive summaries. We present a new approach for aligning chapter sentences with the abstractive summary sentences, incorporating weighting to ROUGE (Lin, 2004) and METEOR (Lavie and Denkowski, 2009) metrics to enable the alignment of salient words between them. We also experiment with BERT (Devlin et al., 2018) alignment. We use a stable matching algorithm to select the best alignments, and show tha"
2020.acl-main.453,W16-3617,0,0.0174879,"mmary pairs drawn from CliffsNotes and GradeSaver and developed an unsupervised system based on Meade (Radev et al., 2001) and TextRank (Mihalcea and Tarau, 2004) that showed promise. More recently, Zhang et al. (2019) developed an approach for summarizing characters within a novel. We hypothesize that our proposed task is more feasible than summarizing the full novel. Previous work has summarized documents using Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) to extract elementary discourse units (EDUs) for compression and more contentpacked summaries (Daum´e III and Marcu, 2002; Li et al., 2016; Arumae et al., 2019). Some abstractive neural methods propose attention to focus on phrases within a sentence to extract (Gehrmann et al., 2018). Fully abstractive methods are not yet appropriate for our task due to extensive paraphrasing and generalization. While previous work on semantic textual similarity is relevant to the problem of finding alignments between chapter and summary text, the data available (Cer et al., 2017; Dolan and Brockett, 2005) is not suitable for our domain, and the alignments we generated from this data were of a poorer quality than the other methods in our paper."
2020.acl-main.453,W04-1013,0,0.110069,"icle. We focus on the task of content selection, taking an initial, extractive summarization approach given the task difficulty.1 As the reference sum∗ Equal contribution. Work done while at Amazon. We tried two abstractive models (Chen and Bansal, 2018; Liu and Lapata, 2019) but ROUGE was low and the output was poor with many repetitions and hallucinations. 1 maries are abstractive, training our model requires creating a gold-standard set of extractive summaries. We present a new approach for aligning chapter sentences with the abstractive summary sentences, incorporating weighting to ROUGE (Lin, 2004) and METEOR (Lavie and Denkowski, 2009) metrics to enable the alignment of salient words between them. We also experiment with BERT (Devlin et al., 2018) alignment. We use a stable matching algorithm to select the best alignments, and show that enforcing one-toone alignments between reference summary sentences and chapter sentences is the best alignment method of those used in earlier work. We obtain a dataset of summaries from five study guide websites paired with chapter text from Project Gutenberg. Our dataset consists of 4,383 unique chapters, each of which is paired with two to five human"
2020.acl-main.453,D19-1387,0,0.0538778,"ta, 2016; Grusky et al., 2018; Paulus et al., 2017)) focuses; chapters are on average seven times longer than news articles. There is no one-to-one correspondence between summary and chapter sentences, and the summaries in our dataset use extensive paraphrasing, while news summaries copy most of their information from the words used in the article. We focus on the task of content selection, taking an initial, extractive summarization approach given the task difficulty.1 As the reference sum∗ Equal contribution. Work done while at Amazon. We tried two abstractive models (Chen and Bansal, 2018; Liu and Lapata, 2019) but ROUGE was low and the output was poor with many repetitions and hallucinations. 1 maries are abstractive, training our model requires creating a gold-standard set of extractive summaries. We present a new approach for aligning chapter sentences with the abstractive summary sentences, incorporating weighting to ROUGE (Lin, 2004) and METEOR (Lavie and Denkowski, 2009) metrics to enable the alignment of salient words between them. We also experiment with BERT (Devlin et al., 2018) alignment. We use a stable matching algorithm to select the best alignments, and show that enforcing one-toone a"
2020.acl-main.453,I05-5002,0,\N,Missing
2020.acl-main.453,D07-1040,0,\N,Missing
2020.acl-main.453,W04-3252,0,\N,Missing
2020.acl-main.453,W05-0909,0,\N,Missing
2020.acl-main.453,P14-5010,0,\N,Missing
2020.acl-main.453,P14-1002,0,\N,Missing
2020.acl-main.453,P16-1046,0,\N,Missing
2020.acl-main.453,S17-2001,0,\N,Missing
2020.acl-main.453,D18-1443,0,\N,Missing
2020.acl-main.453,N19-1072,0,\N,Missing
2020.acl-main.453,N19-1423,0,\N,Missing
2020.acl-main.453,D17-1238,0,\N,Missing
2020.acl-main.453,D19-5408,0,\N,Missing
2020.coling-main.131,J90-1003,0,0.255566,"e-step process that selects entity pairs related to a news event on a specific date. First, we collect a list of date-marked descriptions of events that occurred during the publication date range of our news corpus. These descriptions are available in many languages for every calendar year and can be easily scraped from Wikipedia. Next, we extract all pairs of entities from these descriptions, grouping them by their event dates. Finally, we use these groupings to select pairs of entities from our corpus, filtering them by their count and inarticle positive pointwise mutual information (PPMI, (Church and Hanks, 1990)) in the days following their event date. For comparison, we also evaluate random and date-window-grouped pairing. Random pairing selects pairs of entities at random. Date-window-grouped pairing groups pairs of entities by sliding date windows. As with event-guided pairing, it is informed by our observation regarding the local consistency of relations between entities associated with events in news. However, it does not restrict the selected entities to those found in Wikipedia event descriptions. For both these methods, we filter pairs by their counts and PPMIs exactly as in event-guided pair"
2020.coling-main.131,D18-1514,0,0.0272035,"ion extraction benchmarks and builds on Harris’ distributional hypothesis (Harris, 1954) and its extensions (Lin and Pantel, 2001). Soares et al. (2019) assume that the informational redundancy of very large text corpora (e.g., Wikipedia) results in sentences that contain the same pair of entities generally expressing the same relation. Thus, an encoder trained to collocate such sentences can be used to identify the relation between entities in any sentence s by finding the labeled relation example whose embedding is closest to s. While Soares et al. (2019) achieve state-of-the-art on FewRel (Han et al., 2018) and SemEval 2010 Task 8 (Hendrickx et al., 2019), their approach relies on a huge amount of data, making it difficult to retrain in English or any other language with standard computational resources: they fine-tune BERT large (Devlin et al., 2019), which has 340mil parameters, on 300mil+ relation pair statements with a batch size of 2, 048 for 1mil steps. In contrast our method, with only 50k relations statements and a language-model one-third the size, achieves comparable performance when fine-tuned on little to no task-specific data. This work is licensed under a Creative Commons Attributi"
2020.coling-main.131,P11-1115,0,0.0446444,"tributional structure of date-marked news articles to build a denoised corpus – the extraction process filters out low quality examples. We show that a smaller multilingual encoder trained on this corpus performs comparably to the current state-of-the-art (when both receive little to no fine-tuning) on few-shot and standard relation benchmarks in English and Spanish despite using many fewer examples (50k vs. 300mil+). 1 Introduction Multilingual relation extraction is an important problem in NLP, facilitating a diverse set of downstream tasks from the autopopulation of knowledge graphs (e.g., Ji and Grishman (2011)) to question answering (e.g., Xu et al. (2016)). While early efforts in relation extraction used supervised methods that rely on a fixed set of predetermined relations, research has since shifted to the identification of arbitrary unseen relations in any language. In this paper, we present a method for extracting high quality relation training examples from date-marked news articles. This technique leverages the predictable distributional structure of such articles to build a corpus that is denoised (i.e., where sentences with the same entities only express the same relation, see Figure 1). W"
2020.coling-main.131,P09-1113,0,0.121524,"cture of such articles to build a corpus that is denoised (i.e., where sentences with the same entities only express the same relation, see Figure 1). We use this corpus to learn general purpose relation representations and evaluate their quality on few-shot and standard relation extraction benchmarks in English and Spanish with little to no task-specific fine-tuning, achieving comparable results to a significantly more data-intensive approach that is the current state-of-the-art. The current state-of-the-art model, “Matching the Blanks” or MTB (Soares et al., 2019), is a distant supervision (Mintz et al., 2009) technique that provides large gains on many relation extraction benchmarks and builds on Harris’ distributional hypothesis (Harris, 1954) and its extensions (Lin and Pantel, 2001). Soares et al. (2019) assume that the informational redundancy of very large text corpora (e.g., Wikipedia) results in sentences that contain the same pair of entities generally expressing the same relation. Thus, an encoder trained to collocate such sentences can be used to identify the relation between entities in any sentence s by finding the labeled relation example whose embedding is closest to s. While Soares"
2020.coling-main.414,Q19-1038,0,0.0233204,"responding Sinhala or Odia dataset for evaluation. For the English classification task, we implement classifiers of different architectures adopting various embeddings including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020), which we use to extract features, then train a classifier that takes the contextualized representations of the tweets into account. For the cross-lingual classification task, we build classifiers using the same set of architectures, but deploying various cross-lingual embeddings that are constructed using different methods: LASER (Artetxe and Schwenk, 2019) and XLM-R (Conneau et al., 2020). For both tasks, we employ semi-supervised approaches by generating pseudo-labels for a large amount of unlabeled tweets that are crisis related, in order to improve system performance. Last but not least, we ensemble different classifiers to boost performance further. 2 Dataset Tweets about many natural and human-induced disasters such as earthquakes, typhoons, and landslides are collected by (Imran et al., 2016). We annotate a subset of them at the tweet-level on the Figure-Eight data annotation platform3 as seen in Figure 1. The annotation tag set comprises"
2020.coling-main.414,Q17-1010,0,0.0314269,"ion for Sinhala/Odia. 3.2 English Classification To start with, we build classifiers for detecting urgency given tweets in English to establish an understanding of the baseline performance of this task without the effect of transferring between languages. 3.2.1 Monolingual Embeddings For all of our classifiers, we first use word/sentence embeddings to extract features of the input tweets. We experiment with the following variations when choosing the English embeddings: contextual and non-contextual, and out-of-domain and in-domain. We choose two non-contextual embeddings: fastText embeddings (Bojanowski et al., 2017) and CrisisNLP embeddings (Nguyen et al., 2016). fastText embeddings are trained on texts from Wikipedia and Common Crawl (both out-of-crisis-domain) whereas CrisisNLP embeddings are trained on disaster related tweets, i.e. in-domain. Both embeddings project each word in a sentence to a 300 dimensional vector representation. We also use BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020) to generate contextual representations of the tweets for English.8 A list of embeddings and their availability for each language is shown in Table 3. Embeddings Lang. Dimens"
2020.coling-main.414,2020.acl-main.747,0,0.0749318,"Missing"
2020.coling-main.414,N19-1423,0,0.489571,"https://github.com/niless/urgency This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons. org/licenses/by/4.0/. 1 4693 Proceedings of the 28th International Conference on Computational Linguistics, pages 4693–4703 Barcelona, Spain (Online), December 8-13, 2020 classification, for which we use the entire English dataset for training and the corresponding Sinhala or Odia dataset for evaluation. For the English classification task, we implement classifiers of different architectures adopting various embeddings including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020), which we use to extract features, then train a classifier that takes the contextualized representations of the tweets into account. For the cross-lingual classification task, we build classifiers using the same set of architectures, but deploying various cross-lingual embeddings that are constructed using different methods: LASER (Artetxe and Schwenk, 2019) and XLM-R (Conneau et al., 2020). For both tasks, we employ semi-supervised approaches by generating pseudo-labels for a large amount of unlabeled tweets that are crisis related"
2020.coling-main.414,N13-1073,0,0.0100306,"fer approach is shown in Figure 3 and Algorithm 1. 3.3.1 Cross-lingual Embeddings To generate a cross-lingual embedding that can be used to transfer from English to Sinhala, we use a parallel corpus that contains English-Sinhala sentence pairs as well as pre-trained English embeddings and Sinhala embeddings. There are many approaches for generating cross-lingual embeddings given the above resources, but in our study we focus on the projection-based methods of training the embeddings: 4698 VecMap (Artetxe et al., 2018) and Proc-B (Glavaˇs et al., 2019). As a first step, we use fast-align tool (Dyer et al., 2013) to create symmetric word alignments between source and target words given the parallel corpus, then we choose the most frequent translation for each word (Rasooli et al., 2018). This generates a bilingual dictionary with 72K approximate vocabulary size for each language, which is used as a seed dictionary to generate the cross-lingual embeddings by projecting the pre-trained English and Sinhala monolingual embeddings to the same semantic space. We employ the same procedure to generate the English-Odia embeddings as well, given the English-Odia parallel corpus and the pre-trained English and O"
2020.coling-main.414,P19-1070,0,0.0326001,"Missing"
2020.coling-main.414,L18-1550,0,0.0565385,"Missing"
2020.coling-main.414,L16-1259,0,0.169168,"iers using the same set of architectures, but deploying various cross-lingual embeddings that are constructed using different methods: LASER (Artetxe and Schwenk, 2019) and XLM-R (Conneau et al., 2020). For both tasks, we employ semi-supervised approaches by generating pseudo-labels for a large amount of unlabeled tweets that are crisis related, in order to improve system performance. Last but not least, we ensemble different classifiers to boost performance further. 2 Dataset Tweets about many natural and human-induced disasters such as earthquakes, typhoons, and landslides are collected by (Imran et al., 2016). We annotate a subset of them at the tweet-level on the Figure-Eight data annotation platform3 as seen in Figure 1. The annotation tag set comprises the following four levels of urgency: Figure 1: Annotation interface • Extremely Urgent: aspects of the tweet refer to an extremely urgent and difficult situation; e.g. MT @SushmaSwaraj my uncle is in kathmandu, trapped, suffers from jaundice, chest infection, diabetes, his number #NepalQuake • Definitely Urgent: tweet contains content that is urgent but the level of urgency is not as high; e.g. @MountainGuides1 Please help us find my friends par"
2020.coling-main.414,W15-1521,0,0.0192207,"et al., 2019; Chaudhary et al., 2019). The work of Kejriwal and Zhou (2019) apply a manual feature based approach to transfer urgency labels from English to several low resource languages combined with active learning to increase the amount of labels. Recent successful techniques in transfer learning, however, use cross-lingual embeddings combined with deep learning based classifiers. Cross-lingual embeddings map words in different languages into same semantic space and among them, we use projection based approaches, i.e. VecMap and ProcB, rather than parallel corpora based ones e.g. BiSkip (Luong et al., 2015) due to their superior performance. This has been shown to work well for sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). In addition, after the success of contextual language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in many NLP tasks, their multilingual versions became available i.e. Multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not tri"
2020.coling-main.414,N06-1020,0,0.0718263,"utputs. Figure 2: MLP Architecture INPUT CROSS-LINGUAL LEARNING CROSS-LINGUAL OUTPUT Parallel Corpora Align words & extract dictionary Bilingual Dictionary Train cross-lingual embedding Cross-lingual Embedding Train cross-lingual classifier Cross-lingual Classifier IL Monolingual Embedding English Monolingual Embedding English Data with Labels Figure 3: System Architecture: Transfer Learning in Zero-shot setting 3.2.3 Data Augmentation We experiment with a semi-supervised training scheme to augment the training dataset (shown in Algorithm 1). We adopt self-training approaches (Yarowsky, 1995; McClosky et al., 2006), in which we add the best performing classifier’s predictions on unlabeled data to the initial training dataset, which is manually annotated. We sample the unlabeled tweets from the same collection of disaster related tweets (Imran et al., 2016) where we select and annotate a subset to create our English training dataset as described in Section 2, and we make sure the set of the unlabeled tweets and the set of training data 4697 Algorithm 1 Incremental Training Workflow Let source language training dataset be S Let unlabelled source language dataset be U Let target language testing set be T w"
2020.coling-main.414,P18-1096,0,0.0178834,"lingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not trivial and is not as reliable as in high resource settings. As such, we show how this can be achieved with our experimental setup. Specifically, we use a self-learning method by voting (Zhou and Goldman, 2004) to increase the size of the high resource language dataset on unlabelled Crisis NLP tweets. We decide not to use tri-training (Zhi-Hua Zhou and Ming Li, 2005; Ruder and Plank, 2018) due to the size of original English data despite the fact that tri-training has shown good results in domain-shift NLP tasks. 6 Conclusion In this study, we release an urgency dataset consisting of English tweets about natural crisis and their urgency status. In addition, we release two evaluation datasets for urgency detection in Sinhala and Odia. We train monolingual classifiers for English and cross-lingual classifiers for Sinhala and Odia that are zero-shot learners. For the design of our classifiers, beside exploring different architectures, we adopt different monolingual or cross-lingua"
2020.coling-main.414,N19-5004,0,0.114998,"de. During an emergent crisis, people post to report their well-being, ask for help, or give updates about the ongoing situation. This type of text data can be utilized to provide situational awareness to support missions such as humanitarian assistance/disaster relief, peacekeeping or infectious disease response. However, with the existence of more than 7,000 languages worldwide, automated human language technology does not exist for many languages.1 A possible solution to this problem is to transfer models learned in high resource language settings such as English to low resource languages (Ruder et al., 2019). In addition, there has been significant research in the use of transfer models in semantic analysis of texts such as sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). To this end, we collect and release English, Sinhala and Odia urgency datasets that consist of tweets relating to natural crises, annotated with urgency status. 2 To demonstrate that we are able to effectively transfer the task of urgency detection from English to low-resource languages, we use English annotated tweets for training, and Sinhala/Odia annotated tweets for evaluation only"
2020.coling-main.414,D13-1170,0,0.00954867,"ed approach to transfer urgency labels from English to several low resource languages combined with active learning to increase the amount of labels. Recent successful techniques in transfer learning, however, use cross-lingual embeddings combined with deep learning based classifiers. Cross-lingual embeddings map words in different languages into same semantic space and among them, we use projection based approaches, i.e. VecMap and ProcB, rather than parallel corpora based ones e.g. BiSkip (Luong et al., 2015) due to their superior performance. This has been shown to work well for sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). In addition, after the success of contextual language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in many NLP tasks, their multilingual versions became available i.e. Multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not trivial and is not as reliable as in high resource settings. As such, we show how this can be achieved wit"
2020.coling-main.414,C18-1246,1,0.803396,"everal low resource languages combined with active learning to increase the amount of labels. Recent successful techniques in transfer learning, however, use cross-lingual embeddings combined with deep learning based classifiers. Cross-lingual embeddings map words in different languages into same semantic space and among them, we use projection based approaches, i.e. VecMap and ProcB, rather than parallel corpora based ones e.g. BiSkip (Luong et al., 2015) due to their superior performance. This has been shown to work well for sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). In addition, after the success of contextual language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in many NLP tasks, their multilingual versions became available i.e. Multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not trivial and is not as reliable as in high resource settings. As such, we show how this can be achieved with our experimental setup. Specifically, we use a self-learnin"
2020.coling-main.414,P95-1026,0,0.61215,"ous CNN layers outputs. Figure 2: MLP Architecture INPUT CROSS-LINGUAL LEARNING CROSS-LINGUAL OUTPUT Parallel Corpora Align words & extract dictionary Bilingual Dictionary Train cross-lingual embedding Cross-lingual Embedding Train cross-lingual classifier Cross-lingual Classifier IL Monolingual Embedding English Monolingual Embedding English Data with Labels Figure 3: System Architecture: Transfer Learning in Zero-shot setting 3.2.3 Data Augmentation We experiment with a semi-supervised training scheme to augment the training dataset (shown in Algorithm 1). We adopt self-training approaches (Yarowsky, 1995; McClosky et al., 2006), in which we add the best performing classifier’s predictions on unlabeled data to the initial training dataset, which is manually annotated. We sample the unlabeled tweets from the same collection of disaster related tweets (Imran et al., 2016) where we select and annotate a subset to create our English training dataset as described in Section 2, and we make sure the set of the unlabeled tweets and the set of training data 4697 Algorithm 1 Incremental Training Workflow Let source language training dataset be S Let unlabelled source language dataset be U Let target lan"
2020.emnlp-main.419,D14-1179,0,0.0536004,"Missing"
2020.emnlp-main.419,W19-8652,0,0.0301649,"Missing"
2020.emnlp-main.419,W18-6505,0,0.0174714,"semantic correctness of generated utterances relative to a completely end-to-end neural NLG model. However, they do not test the systematicty of the neural generation components, i.e. the ability to perform correctly when given an arbitrary or random input from the preceding component, as we do here with the random permutation stress test. Other papers mention linearization order anecdottally but do quantify its impact. For example, Juraska et al. (2018) experiment with random linearization orderings during development, but do not use them in the final model or report results using them, and Gehrmann et al. (2018) report that using a consistent linearization strategy worked best for their models but do not specify the exact order. Juraska et al. (2018) also used sentence level data augmentation, i.e. splitting a multi-sentence example in multiple single sentence examples, similar in spirit to our proposed phrase based method, but they do not evaluate its effect independently. 10 Conclusion We present an empirical study on the effects of linearization order and phrase based data augmentation on controllable MR-to-text generation. Our findings support the importance of aligned linearization and phrase tr"
2020.emnlp-main.419,J95-2003,0,0.516922,"udy under the names sentence or micro-planning (Reiter and Dale, 2000; Walker et al., 2001; Stone et al., 2003). Contemporary practice, however, eschews modeling at this granularity, instead preferring to train an S2S model to directly map an input MR to a natural language utterance, with the utterance plan determined implicitly by the model which is learned from the training data (Duˇsek et al., 2020). We argue that robust and fine grained control in an S2S model is desirable because it enables neural implementations of various psycho-linguistic theories of discourse (e.g., Centering Theory (Grosz et al., 1995), or Accessibility Theory (Ariel, 2001)). This could, in turn, encourage the validation and/or refinement of additional psychologically plausible models of language production. In this paper, we study controllability in the context of task-oriented dialogue generation (Mairesse et al., 2010; Wen et al., 2015), where the input to the NLG model is an MR consisting of a dialogue act (i.e. a communicative goal) such as to R EQUEST E XPLANATION, and an unordered set of attribute-value pairs defining the semantics of the intended utterance (see Figure 1 for an example). The NLG model is expected to"
2020.emnlp-main.419,W19-8623,0,0.238363,"y attributes, e.g. from the phrase “is not on Mac,” which denotes has mac release = “no,” we obtain the phrase “on Mac” which denotes has mac release = “yes.” When presenting the linearized MR of phrase examples to the model encoder we prepend and append phrase specific start and stop tokens respectively (e.g., start-NP and stop-NP) to discourage the model from ever producing an incomplete sentence when generating for a complete MR. 3 Datasets We run our experiments on two English language, task-oriented dialogue datasets, the E2E Challenge corpus (Novikova et al., 2017) and the ViGGO corpus (Juraska et al., 2019). These datasets provide MR/utterance pairs from the restaurant and video game domains, respectively. Examples from the E2E corpus (33,523 train/1,426 dev/630 test) can have up to eight unique attributes. There is only one dialogue act for the corpus, I NFORM. Attributevalues are either binary or categorical valued. The ViGGO corpus (5,103 train/246 dev/359 test) contains 14 attribute types and nine dialogue acts. In addition to binary and categorical valued attributes, the corpus also features list-valued attributes (see the genres attribute in Figure 1) which can have a variable number of va"
2020.emnlp-main.419,N18-1014,0,0.0183985,"various planning tasks as separate components in a pipeline, where the components themselves are implemented with neural models, improves the overall quality and semantic correctness of generated utterances relative to a completely end-to-end neural NLG model. However, they do not test the systematicty of the neural generation components, i.e. the ability to perform correctly when given an arbitrary or random input from the preceding component, as we do here with the random permutation stress test. Other papers mention linearization order anecdottally but do quantify its impact. For example, Juraska et al. (2018) experiment with random linearization orderings during development, but do not use them in the final model or report results using them, and Gehrmann et al. (2018) report that using a consistent linearization strategy worked best for their models but do not specify the exact order. Juraska et al. (2018) also used sentence level data augmentation, i.e. splitting a multi-sentence example in multiple single sentence examples, similar in spirit to our proposed phrase based method, but they do not evaluate its effect independently. 10 Conclusion We present an empirical study on the effects of linea"
2020.emnlp-main.419,2020.acl-main.703,0,0.257241,"value pairs in µ to an ordered sequence, i.e. π(µ) = [a, x1 , x2 , . . . , x|µ |]. Regardless of the choice of π, the first token in π(µ) is always the dialogue act a. We experiment with both gated recurrent unit (GRU) (Cho et al., 2014) and Transformer (Vaswani et al., 2017) based S2S model variants to implement a conditional probability model p(·|π(µ); θ) : Y → (0, 1) over utterances. The model parameters, θ, are learned by approximately maximizing the log-likelihood L(θ) = P (µ,y)∈D log p(y|π(µ); θ) on the training set D. Additionally, we experiment with a pretrained S2S Transformer, BART (Lewis et al., 2020), with parameters θ0 fine-tuned on L(θ0 ). 2.1 Linearization Strategies Because of the recurrence in the GRU and position embeddings in the Transformer, it is usually the case that different linearization strategies, i.e. π(µ) 6= π 0 (µ), will result in different model internal representations and therefore different conditional probability distributions. These differences can be non-trivial, yielding changes in model behavior with respect to faithfulness and control. We study four linearization strategies, (i) random, (ii) increasing-frequency, (iii) fixed-position, and (iv) alignment trainin"
2020.emnlp-main.419,W04-1013,0,0.0146897,"Beam candidates are ranked by log likelihood. The final ordering we propose is the O RACLE ordering, i.e. the utterance plan implied by the human-authored test-set reference utterances. This 5 Experiments Test-Set Evaluation In our first experiment, we compare performance of the proposed models and linearization strategies on the E2E and ViGGO test sets. For the I F and AT +NUP models we also include variants trained on the union of original training data and phraseaugmented data (see §2.2), which we denote + P. Evaluation Measures For automatic quality measures, we report B LEU and ROUGE -L (Lin, 2004) scores.4 Additionally, we use the matching rules to automatically annotate the attribute-value spans of the model generated utterances, and then manually verify/correct them. With the attributevalue annotations in hand we compute the number of missing, wrong, or added attribute-values for each model. From these counts, we compute the semantic error rate (SER) (Duˇsek et al., 2020) where SER = #missing + #wrong + #added . #attributes On ViGGO, we do not include the rating attribute in this evaluation since we consider it part of the dialogue act. Additionally, for AT variants, we report the or"
2020.emnlp-main.419,D15-1166,0,0.0686993,"Missing"
2020.emnlp-main.419,P10-1157,0,0.0875457,"Missing"
2020.emnlp-main.419,W19-8645,0,0.0184398,"and they do not evaluate the degree to which a S2S model can follow realization orders not drawn from the training distribution. Castro Ferreira et al. (2017) compare a S2S NLG model using various linearizations of abstract meaning representation (AMR) graphs, including a model-based alignment very similar to the AT linearization presented in this work. However, they evaluate only on automatic quality measures and do not explicitly measure the semantic correctness of the generated text or the degree to which the model realizes the text in the order implied by the linearized input. Works like Moryossef et al. (2019a,b) and Castro Ferreira et al. (2019) show that treating various planning tasks as separate components in a pipeline, where the components themselves are implemented with neural models, improves the overall quality and semantic correctness of generated utterances relative to a completely end-to-end neural NLG model. However, they do not test the systematicty of the neural generation components, i.e. the ability to perform correctly when given an arbitrary or random input from the preceding component, as we do here with the random permutation stress test. Other papers mention linearization ord"
2020.emnlp-main.419,N19-1236,0,0.0185687,"and they do not evaluate the degree to which a S2S model can follow realization orders not drawn from the training distribution. Castro Ferreira et al. (2017) compare a S2S NLG model using various linearizations of abstract meaning representation (AMR) graphs, including a model-based alignment very similar to the AT linearization presented in this work. However, they evaluate only on automatic quality measures and do not explicitly measure the semantic correctness of the generated text or the degree to which the model realizes the text in the order implied by the linearized input. Works like Moryossef et al. (2019a,b) and Castro Ferreira et al. (2019) show that treating various planning tasks as separate components in a pipeline, where the components themselves are implemented with neural models, improves the overall quality and semantic correctness of generated utterances relative to a completely end-to-end neural NLG model. However, they do not test the systematicty of the neural generation components, i.e. the ability to perform correctly when given an arbitrary or random input from the preceding component, as we do here with the random permutation stress test. Other papers mention linearization ord"
2020.emnlp-main.419,W17-5525,0,0.0484753,"Missing"
2020.emnlp-main.419,P02-1040,0,0.107692,"e reasonable mappings to the utterance, so we treat it in practice like an addendum to the dialogue act, occurring directly after the dialogue act as part of a “header” section in any MR linearization strategy (see Figure 2 where rating = “N/A” occurs after the dialogue act regardless of choice of linearization strategy). 4 4.1 Models Generation Models We examine the effects of linearization strategy and data augmentation on a bidirectional GRU with attention (biGRU) and Transformer-based S2S models. Hyperparameters were found using grid-search, selecting the model with best validation B LEU (Papineni et al., 2002) score. We performed a separate grid-search for each architecture-linearization strategy pairing in case there was no one best hyperparameter setting. 5163 Additionally, we fine-tune BART (Lewis et al., 2020), a large pretrained Transformer-based S2S model. We stop fine-tuning after validation set cross-entropy stops decreasing. Complete architecture specification, hyperparameter search space, and validation results for all three models can be found in Appendix A. plan represents the model performance if it had a priori knowledge of the reference utterance plan. When a test example has multipl"
2020.emnlp-main.419,W18-6535,0,0.0221804,"arbitray order permutations independent of alignment model error. Also we should note that data cleaning can yield more substantial decreases in semantic errors (Duˇsek et al., 2019; Wang, 2019) and is an important consideration in any practical neural NLG. 9 Related Work MR linearizations for S2S models have been studied in a variety of prior works. Nayak et al. (2017) explore several ways of incorporating sentence planning into an MR linearization for S2S models, comparing a flat alignment order (equivalent to the alignment order used in this paper) against various sentence level groupings. Reed et al. (2018) add additional sentence and discourse structuring variables to indicate contrasts or sentential groupings. Balakrishnan et al. (2019) experiment both with tree structured MRs and encoders and compare them to linearized trees with standard S2S models. They also find that properly aligned linearization can lead to a controllable generator. These papers do not, however, explore how other linearization strategies compare in terms of faithfulness, and they do not evaluate the degree to which a S2S model can follow realization orders not drawn from the training distribution. Castro Ferreira et al."
2020.emnlp-main.419,W18-2509,0,0.0247693,"Missing"
2020.emnlp-main.419,N01-1003,0,0.250394,". Introduction In this work, we study the degree to which neural sequence-to-sequence (S2S) models exhibit finegrained controllability when performing natural language generation (NLG) from a meaning representation (MR). In particular, we focus on an S2S approach that respects the realization ordering constraints of a given utterance plan; such a model can generate utterances whose phrases follow the order of the provided plan. In non-neural NLG, fine-grained control for planning sentence structure has received extensive study under the names sentence or micro-planning (Reiter and Dale, 2000; Walker et al., 2001; Stone et al., 2003). Contemporary practice, however, eschews modeling at this granularity, instead preferring to train an S2S model to directly map an input MR to a natural language utterance, with the utterance plan determined implicitly by the model which is learned from the training data (Duˇsek et al., 2020). We argue that robust and fine grained control in an S2S model is desirable because it enables neural implementations of various psycho-linguistic theories of discourse (e.g., Centering Theory (Grosz et al., 1995), or Accessibility Theory (Ariel, 2001)). This could, in turn, encourag"
2020.emnlp-main.419,W19-8639,0,0.0146971,"w test-set SER for both corpora, we should caution that this required extensive manual development of matching rules to produce MR/utterance alignments, which in turn resulted in significant cleaning of the training datasets. We chose to do this over pursuing a model based strategy of aligning utterance subspans to attribute-values because we wanted to better understand how systematically S2S models can represent arbitray order permutations independent of alignment model error. Also we should note that data cleaning can yield more substantial decreases in semantic errors (Duˇsek et al., 2019; Wang, 2019) and is an important consideration in any practical neural NLG. 9 Related Work MR linearizations for S2S models have been studied in a variety of prior works. Nayak et al. (2017) explore several ways of incorporating sentence planning into an MR linearization for S2S models, comparing a flat alignment order (equivalent to the alignment order used in this paper) against various sentence level groupings. Reed et al. (2018) add additional sentence and discourse structuring variables to indicate contrasts or sentential groupings. Balakrishnan et al. (2019) experiment both with tree structured MRs"
2020.emnlp-main.419,D19-1412,0,0.0249308,"te-value pairs presented to the S2S encoder, and only in the case of alignment training does it correspond to the order in which the attribute-value pairs are realized in the utterance. When presenting a linearized MR to the model encoder, we always prepend and append distinguished start and stop tokens respectively. Random (R ND) In the random linearization (R ND), we randomly order the attribute-value pairs for a given MR. This strategy serves as a baseline for determining if linearization matters at all for faithfulness. R ND is similar to token level noise used in denoising auto-encoders (Wang et al., 2019) and might even improve faithfulness. During training, we resample the ordering for each example at every epoch. We do not resample the validation set in order to obtain stable results for model selection. Increasing Frequency (I F) In the increasing frequency linearization (I F), we order the attributevalue pairs by increasing frequency of occurrence in the training data (i.e., count(xi ) ≤ 5161 exp release date=N/A specifier=N/A on Linux=no on Mac=yes on Steam=no esrb=N/A has multiplayer=N/A developer=N/A release year=N/A perpspective1 =bird view perpspective2 =N/A perpspective3 =N/A platfor"
2020.emnlp-main.436,N19-1423,0,0.0239802,"6–20, 2020. 2020 Association for Computational Linguistics (or subword units for BERT-like models) i.e. {x0 , x1 , ..., xn−1 }, representing the input text. A subsequence spani is defined by starti , endi ∈ [0, n). Subsequences span1 and span2 represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2. First, the model embeds the input sequence into a vector representation using either static wang2vec representations (Ling et al., 2015), or contextualized representations from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or RoBERTa (Liu et al., 2019b). These embedded sequences are then optionally encoded with either LSTMs or Transformers. When BERT or RoBERTa is used to embed the input, we do not use any sequence encoders. The final sequence representation H[0,n) comprises of individual token representations i.e. {h0 , h1 , ..., hn−1 }. While the goal is to predict the temporal relation between span1 and span2 , the context around these two spans also has linguistic signals that connect the two arguments. To use this contextual information, we extract five constituent subsequences from the sequence represent"
2020.emnlp-main.436,P19-1433,0,0.38539,"facto standard for temporal ordering of events.1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Du"
2020.emnlp-main.436,P17-1085,0,0.0396269,"Missing"
2020.emnlp-main.436,Q18-1017,1,0.930198,"Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Durrett (2019). (3) A self-training method that incorporates the predictions of our model and learns from them; we test it jointly with the SMTL method. Our baseline model that uses RoBERTa (Liu et al., 2019b) already surpasses the state-of-the-art by 2 F1 points. Applying SMTL techniques affords further improvements with at least one of our auxiliary tasks. Finally, our self-training experiments, explore"
2020.emnlp-main.436,P14-1038,0,0.0658311,"Missing"
2020.emnlp-main.436,N15-1142,0,0.0314709,"Conference on Empirical Methods in Natural Language Processing, pages 5412–5417, c November 16–20, 2020. 2020 Association for Computational Linguistics (or subword units for BERT-like models) i.e. {x0 , x1 , ..., xn−1 }, representing the input text. A subsequence spani is defined by starti , endi ∈ [0, n). Subsequences span1 and span2 represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2. First, the model embeds the input sequence into a vector representation using either static wang2vec representations (Ling et al., 2015), or contextualized representations from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or RoBERTa (Liu et al., 2019b). These embedded sequences are then optionally encoded with either LSTMs or Transformers. When BERT or RoBERTa is used to embed the input, we do not use any sequence encoders. The final sequence representation H[0,n) comprises of individual token representations i.e. {h0 , h1 , ..., hn−1 }. While the goal is to predict the temporal relation between span1 and span2 , the context around these two spans also has linguistic signals that connect the two arguments. To use th"
2020.emnlp-main.436,W19-1917,0,0.108986,"al ordering of events.1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Durrett (2019). (3)"
2020.emnlp-main.436,2021.ccl-1.108,0,0.121946,"Missing"
2020.emnlp-main.436,P16-1105,0,0.0733547,"Missing"
2020.emnlp-main.436,W15-1506,0,0.0914746,"Missing"
2020.emnlp-main.436,D19-1642,0,0.395417,"It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering models designed by Goyal and Durrett (2019), Liu et al. (2019a) and Ning et al. (2019). Our main contributions are: (1) a neural architecture that can flexibly adapt different encoders and pretrained word embedders to form a contextual pairwise argument representation. Given the scarcity of training data, (2) we explore the application of an existing framework for Scheduled Multitask-Learning (henceforth SMTL) (Kiperwasser and Ballesteros, 2018) by leveraging complementary (temporal and non temporal) information to our models; this imitates pretraining and finetuning. This consumes timex information in a different way than Goyal and Durrett (2019). (3) A self-training method th"
2020.emnlp-main.436,P18-1122,0,0.51359,"ts establish a new state-of-the-art on this task. 1 Introduction The task of temporal ordering of events involves predicting the temporal relation between a pair of input events in a span of text (Figure 1). This task is challenging as it requires deep understanding of temporal aspects of language and the amount of annotated data is scarce. Albright (e1, came) to the State Department to (e2, offer) condolences. Figure 1: Example from the MATRES dataset. The relation between (e1, came) and (e2, offer) is Before. Note that for the same span there may be other relation pairs. The MATRES dataset (Ning et al., 2018) has become a de facto standard for temporal ordering of events.1 It contains 13,577 pairs of events annotated with a temporal relation (Before, After, Equal, Vague) within 256 English documents (and ∗ Kathleen McKeown is an Amazon Scholar and a Professor at Columbia University. 1 https://github.com/qiangning/MATRES 20 more for evaluation) from TimeBank2 (Pustejovsky et al., 2003), AQUAINT3 (Graff, 2002) and Platinum (UzZaman et al., 2013). In this paper, we present a set of neural architectures for temporal ordering of events. Our main model (Section 2) is similar to the temporal ordering mod"
2020.emnlp-main.436,N18-1202,0,0.0143787,"ages 5412–5417, c November 16–20, 2020. 2020 Association for Computational Linguistics (or subword units for BERT-like models) i.e. {x0 , x1 , ..., xn−1 }, representing the input text. A subsequence spani is defined by starti , endi ∈ [0, n). Subsequences span1 and span2 represent the input pair of argument events e1 and e2 respectively. The goal of the model is to predict the temporal relation between e1 and e2. First, the model embeds the input sequence into a vector representation using either static wang2vec representations (Ling et al., 2015), or contextualized representations from ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), or RoBERTa (Liu et al., 2019b). These embedded sequences are then optionally encoded with either LSTMs or Transformers. When BERT or RoBERTa is used to embed the input, we do not use any sequence encoders. The final sequence representation H[0,n) comprises of individual token representations i.e. {h0 , h1 , ..., hn−1 }. While the goal is to predict the temporal relation between span1 and span2 , the context around these two spans also has linguistic signals that connect the two arguments. To use this contextual information, we extract five constituent subsequences"
2020.emnlp-main.436,D19-1030,0,0.0433235,"Missing"
2020.emnlp-main.436,S13-2001,0,0.220663,"Missing"
2020.emnlp-main.436,N16-1174,0,0.0606923,"sequence before span1 i.e., H[0,start1 ) , (2) S2 , the subsequence corresponding to span1 i.e., H[start1 ,end1 ) , (3) S3 , the subsequence between span1 and span2 i.e, H[end1 ,start2 ) , (4) S4 , the subsequence corresponding to span2 i.e., H[start2 ,end2 ) and (5) S5 , the subsequence after span2 , i.e. H[end2 ,n) . Each of these subsequences Si has a variable number of tokens which are pooled to yield a fixed size representation si : si = pool(Si ) ∀i ∈ {1, ..., 5} (1) where pool is the result of concatenating the output of an attention mechanism (we use the word attention pooling method (Yang et al., 2016) for all tokens in a given span) and mean pooling. The final contextual pair representation c is formed by concatenating4 the five span representations si with a sequence representation r. For models with BERT and RoBERTa, r is the CLS and &lt;s&gt; token representation respectively while for other models r = pool(H[0,n) ). c = s1 s2 s3 s4 s5 r (2) This final contextual pair representation c is then projected with a fully connected layer followed by 4 is used to denote concatenation a softmax function to get a distribution over the output classes. The entire model is trained end-toend using the cros"
2020.emnlp-main.717,C18-1316,0,0.0132252,"ic-position stance, datasets primarily use text from news articles with headlines as topics (Thorne et al., 2018; Ferreira and Vlachos, 2016). In a similar vein, Habernal et al. (2018) use comments from news articles and manually construct position statements. These datasets, however, do not include clear, individuated topics and so we focus on the topic-phrase definition in our work. Many previous models for stance detection trained an individual classifier for each topic (Lin et al., 2006; Beigman Klebanov et al., 2010; Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Li et al., 2018; Hasan and Ng, 2014) or for a small number of topics common to both the training and evaluation sets (Faulkner, 2014; Du et al., 2017). In addition, a handful of models for the TwitterStance dataset have been designed for cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018), including a number of weakly supervised methods using unlabeled data related to the test topic (Zarrella and Marsh, 2016; Wei et al., 2016; Dias and Becker, 2016). In contrast, our models are trained jointly for all topics and are evaluated for zero-shot stance detection on a large number of new test t"
2020.emnlp-main.717,W06-2915,0,0.176516,"ns of the task (K¨uc¸u¨ k and Can, 2020). In the most common definition (topicphrase stance), stance (pro, con, neutral) of a text is detected towards a topic that is usually a nounphrase (e.g., ‘gun control’). In the second definition (topic-position stance), stance (agree, disagree, discuss, unrelated) is detected between a text and a topic that is an entire position statement (e.g., ‘We should disband NATO’). A number of datasets exist using the topicphrase definition with texts from online debate forums (Walker et al., 2012; Abbott et al., 2016; Hasan and Ng, 2014), information platforms (Lin et al., 2006; Murakami and Putra, 2010), student essays (Faulkner, 2014), news comments (Krejzl et al., 2017; Lozhnikov et al., 2018) and Twitter (K¨uc¸u¨ k, 2017; Tsakalidis et al., 2018; Taul´e et al., 2017; Mohammad et al., 2016). These datasets generally have a very small number of topics (e.g., Abbott et al. (2016) has 16) and the few with larger numbers of topics (Bar-Haim et al., 2017; Gottipati et al., 2013; Vamvas and Sennrich, 2020) still have limited topic coverage (ranging from 55 to 194 topics). The data used by Gottipati et al. (2013), articles and comments from an online debate site, has th"
2020.emnlp-main.717,S16-1003,0,0.109026,"Missing"
2020.emnlp-main.717,C10-2100,0,0.0168601,"uc¸u¨ k and Can, 2020). In the most common definition (topicphrase stance), stance (pro, con, neutral) of a text is detected towards a topic that is usually a nounphrase (e.g., ‘gun control’). In the second definition (topic-position stance), stance (agree, disagree, discuss, unrelated) is detected between a text and a topic that is an entire position statement (e.g., ‘We should disband NATO’). A number of datasets exist using the topicphrase definition with texts from online debate forums (Walker et al., 2012; Abbott et al., 2016; Hasan and Ng, 2014), information platforms (Lin et al., 2006; Murakami and Putra, 2010), student essays (Faulkner, 2014), news comments (Krejzl et al., 2017; Lozhnikov et al., 2018) and Twitter (K¨uc¸u¨ k, 2017; Tsakalidis et al., 2018; Taul´e et al., 2017; Mohammad et al., 2016). These datasets generally have a very small number of topics (e.g., Abbott et al. (2016) has 16) and the few with larger numbers of topics (Bar-Haim et al., 2017; Gottipati et al., 2013; Vamvas and Sennrich, 2020) still have limited topic coverage (ranging from 55 to 194 topics). The data used by Gottipati et al. (2013), articles and comments from an online debate site, has the potential to cover the wi"
2020.emnlp-main.717,D14-1162,0,0.100417,"Missing"
2020.emnlp-main.717,W10-0214,0,0.0466964,"w topics for both development and testing. For topic-position stance, datasets primarily use text from news articles with headlines as topics (Thorne et al., 2018; Ferreira and Vlachos, 2016). In a similar vein, Habernal et al. (2018) use comments from news articles and manually construct position statements. These datasets, however, do not include clear, individuated topics and so we focus on the topic-phrase definition in our work. Many previous models for stance detection trained an individual classifier for each topic (Lin et al., 2006; Beigman Klebanov et al., 2010; Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Li et al., 2018; Hasan and Ng, 2014) or for a small number of topics common to both the training and evaluation sets (Faulkner, 2014; Du et al., 2017). In addition, a handful of models for the TwitterStance dataset have been designed for cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018), including a number of weakly supervised methods using unlabeled data related to the test topic (Zarrella and Marsh, 2016; Wei et al., 2016; Dias and Becker, 2016). In contrast, our models are trained jointly for all topics and are evaluated for zero-shot stance dete"
2020.emnlp-main.717,P15-1012,0,0.0189553,"s a large number of new topics for both development and testing. For topic-position stance, datasets primarily use text from news articles with headlines as topics (Thorne et al., 2018; Ferreira and Vlachos, 2016). In a similar vein, Habernal et al. (2018) use comments from news articles and manually construct position statements. These datasets, however, do not include clear, individuated topics and so we focus on the topic-phrase definition in our work. Many previous models for stance detection trained an individual classifier for each topic (Lin et al., 2006; Beigman Klebanov et al., 2010; Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Li et al., 2018; Hasan and Ng, 2014) or for a small number of topics common to both the training and evaluation sets (Faulkner, 2014; Du et al., 2017). In addition, a handful of models for the TwitterStance dataset have been designed for cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018), including a number of weakly supervised methods using unlabeled data related to the test topic (Zarrella and Marsh, 2016; Wei et al., 2016; Dias and Becker, 2016). In contrast, our models are trained jointly for all topics and are evalu"
2020.emnlp-main.717,N18-1074,0,0.058567,"Missing"
2020.emnlp-main.717,walker-etal-2012-corpus,0,0.0212931,". 2 Related Work Previous datasets for stance detection have centered on two definitions of the task (K¨uc¸u¨ k and Can, 2020). In the most common definition (topicphrase stance), stance (pro, con, neutral) of a text is detected towards a topic that is usually a nounphrase (e.g., ‘gun control’). In the second definition (topic-position stance), stance (agree, disagree, discuss, unrelated) is detected between a text and a topic that is an entire position statement (e.g., ‘We should disband NATO’). A number of datasets exist using the topicphrase definition with texts from online debate forums (Walker et al., 2012; Abbott et al., 2016; Hasan and Ng, 2014), information platforms (Lin et al., 2006; Murakami and Putra, 2010), student essays (Faulkner, 2014), news comments (Krejzl et al., 2017; Lozhnikov et al., 2018) and Twitter (K¨uc¸u¨ k, 2017; Tsakalidis et al., 2018; Taul´e et al., 2017; Mohammad et al., 2016). These datasets generally have a very small number of topics (e.g., Abbott et al. (2016) has 16) and the few with larger numbers of topics (Bar-Haim et al., 2017; Gottipati et al., 2013; Vamvas and Sennrich, 2020) still have limited topic coverage (ranging from 55 to 194 topics). The data used b"
2020.emnlp-main.717,S16-1062,0,0.015407,"ned an individual classifier for each topic (Lin et al., 2006; Beigman Klebanov et al., 2010; Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Li et al., 2018; Hasan and Ng, 2014) or for a small number of topics common to both the training and evaluation sets (Faulkner, 2014; Du et al., 2017). In addition, a handful of models for the TwitterStance dataset have been designed for cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018), including a number of weakly supervised methods using unlabeled data related to the test topic (Zarrella and Marsh, 2016; Wei et al., 2016; Dias and Becker, 2016). In contrast, our models are trained jointly for all topics and are evaluated for zero-shot stance detection on a large number of new test topics (i.e., none of the zero-shot test topics occur in the training data). 3 VAST Dataset We collect a new dataset, VAST, for zero-shot stance detection that includes a large number of specific topics. Our annotations are done on comments collected from The New York Times ‘Room for Debate’ section, part of the Argument Reasoning Comprehension (ARC) Corpus (Habernal et al., 2018). Although the ARC corpus provides stance annotations"
2020.emnlp-main.717,S16-1074,0,0.0145239,"for stance detection trained an individual classifier for each topic (Lin et al., 2006; Beigman Klebanov et al., 2010; Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Hasan and Ng, 2013; Li et al., 2018; Hasan and Ng, 2014) or for a small number of topics common to both the training and evaluation sets (Faulkner, 2014; Du et al., 2017). In addition, a handful of models for the TwitterStance dataset have been designed for cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018), including a number of weakly supervised methods using unlabeled data related to the test topic (Zarrella and Marsh, 2016; Wei et al., 2016; Dias and Becker, 2016). In contrast, our models are trained jointly for all topics and are evaluated for zero-shot stance detection on a large number of new test topics (i.e., none of the zero-shot test topics occur in the training data). 3 VAST Dataset We collect a new dataset, VAST, for zero-shot stance detection that includes a large number of specific topics. Our annotations are done on comments collected from The New York Times ‘Room for Debate’ section, part of the Argument Reasoning Comprehension (ARC) Corpus (Habernal et al., 2018). Although the ARC corpus provides"
2020.findings-emnlp.315,Q18-1039,0,0.150352,"very costly (Sun and Saenko, 2014; Vazquez et al., 2013; Stark et al., 2010; Keung et al., 2019). For example, we often have sufficient labeled data for English, while very limited or even no labeled data are available for many other languages. Successfully transferring knowledge learned from the English domain to other languages is of great interest in solving many tasks in natural language processing. Many recent successes in unsupervised domain adaptation have been achieved by learning domain invariant features that are simultaneously being discriminative to the task in the source domain (Chen et al., 2018; Ganin and Lempitsky, 2014; Ganin et al., 2016; Tzeng et al., 2017). Following this line, Keung et al. (2019) propose a language-adversarial training approach for cross-lingual document classification and NER. They leverage the benefit of contextualized word embeddings by using multilingual BERT (Devlin et al., 2019) as the feature generator, and adopt the GAN framework (Goodfellow et al., 2014) to align the features from the two domains. Keung et al. (2019) show significant improvement over the baseline where the pretrained multilingual BERT is finetuned on the English data alone and testing"
2020.findings-emnlp.315,N19-1423,0,0.319014,"other languages is of great interest in solving many tasks in natural language processing. Many recent successes in unsupervised domain adaptation have been achieved by learning domain invariant features that are simultaneously being discriminative to the task in the source domain (Chen et al., 2018; Ganin and Lempitsky, 2014; Ganin et al., 2016; Tzeng et al., 2017). Following this line, Keung et al. (2019) propose a language-adversarial training approach for cross-lingual document classification and NER. They leverage the benefit of contextualized word embeddings by using multilingual BERT (Devlin et al., 2019) as the feature generator, and adopt the GAN framework (Goodfellow et al., 2014) to align the features from the two domains. Keung et al. (2019) show significant improvement over the baseline where the pretrained multilingual BERT is finetuned on the English data alone and testing on the same tasks in other languages. However, Keung et al. (2019), as well as the works mentioned above, are inspired by the pioneering work of Ben-David et al. (2010), which only rigorously studies domain adaptation in the setting of binary classification; there is a lack of theoretical guarantees when it comes to"
2020.findings-emnlp.315,D19-1138,0,0.0229207,"Missing"
2020.findings-emnlp.315,N18-1202,0,0.00882261,"eas the crossentropy loss often leads to poor margins (Liu et al., 2016; Elsayed et al., 2018). To tackle this problem, we augment the cross-entropy loss with Virtual Adversarial Training (VAT) (Miyato et al., 2018). As shown in Zhang et al. (2019a), the local consistency regularization introduced by VAT is capable of promoting large classification margin by optimizing the classification boundary error. This is further demonstrated in Section 4 that the incorporation of VAT leads to remarkable improvement over Zhang et al. (2019b). Although the pretrained language models (Devlin et al., 2019; Peters et al., 2018; Radford et al., 2019) have provided a good foundation for many downstream tasks, to leverage them for unsupervised domain adaptation, we need to tackle the potential overfitting problem, especially when we only have limited labeled data in the source domain but can require many training iterations to minimize the domain discrepancy. As shown in Section 4, VAT can efficiently prevent overfitting in the source domain, and hence significantly improve the generalization in the target domain. This matches the theoretical insights (Ben-David et al., 2010; Zhang et al., 2019b) that the generalizati"
2020.findings-emnlp.315,W03-0419,0,0.120011,"Missing"
2020.findings-emnlp.315,L18-1560,0,0.154376,"ψ(xs + δ)))] . δ;kδk≤ε This term regularizes the predictions being consistent within the ε norm ball of each input. As indicated in Zhang et al. (2019a), the local consistency regularization described in (6) can effectively 3530 promote large margin by optimizing the classification boundary error. As demonstrated in Miyato et al. (2018), the maximization in (6) can be well approximated by a pair of forward- and backwardpropagations. Note that, the input is discrete for the language data, hence we apply VAT to the embedding space and consider the following, tion, where we use the MLDoc corpus (Schwenk and Li, 2018); and named entity recognition, where we use the CoNLL 2002/2003 NER corpus (Tjong Kim Sang, 2002; Sang and De Meulder, 2003). We compare our regularized MDD approach against both Keung et al. (2019) and the baseline. For the baseline, we train the model on the English corpus only, while evaluating on the corpus of the other languages. We also do an ablation study to demone (7) RS strate VAT can yield remarkable performance boost := max KL [σ(f (ψ(e[xs ])))kσ(f (ψ(e[xs ] + δ)))] for all three approaches evaluated in this section. δ;kδk≤ε We implement all three approaches in PyTorch We use e[xs"
2020.findings-emnlp.315,W02-2024,0,0.124814,"input. As indicated in Zhang et al. (2019a), the local consistency regularization described in (6) can effectively 3530 promote large margin by optimizing the classification boundary error. As demonstrated in Miyato et al. (2018), the maximization in (6) can be well approximated by a pair of forward- and backwardpropagations. Note that, the input is discrete for the language data, hence we apply VAT to the embedding space and consider the following, tion, where we use the MLDoc corpus (Schwenk and Li, 2018); and named entity recognition, where we use the CoNLL 2002/2003 NER corpus (Tjong Kim Sang, 2002; Sang and De Meulder, 2003). We compare our regularized MDD approach against both Keung et al. (2019) and the baseline. For the baseline, we train the model on the English corpus only, while evaluating on the corpus of the other languages. We also do an ablation study to demone (7) RS strate VAT can yield remarkable performance boost := max KL [σ(f (ψ(e[xs ])))kσ(f (ψ(e[xs ] + δ)))] for all three approaches evaluated in this section. δ;kδk≤ε We implement all three approaches in PyTorch We use e[xs ] to denote the embedding of the dis(Paszke et al., 2017) with the HuggingFace library crete inp"
2020.findings-emnlp.360,D18-1045,0,0.0227931,"e we first fine-tune the mBART model for document level machine translation from the source language into English, and then we further fine-tune the model for cross-lingual summarization (DC+Synth+MT). Similar to above, since we only have a limited amount of parallel document pairs in our dataset, we translate English documents into the source language to create additional parallel data. This method of back-translation to create additional parallel data has been shown to be effective in improving the performance of neural machine translation systems (Sennrich et al., 2016; Hoang et al., 2018; Edunov et al., 2018).8 5 Results and Analysis Table 4 shows ROUGE scores (Lin, 2004) for the baselines and proposed cross-lingual approaches. We observe that the lead baseline performs poorly for this task, unlike in the news domain where it’s shown to be a strong baseline (Brandow et al., 8 While back-translation typically uses an intermediate training checkpoint to create synthetic data, we instead use AWS translate. 1995). When comparing the performance of Trans-Sum vs. Sum-Trans, we find that performance depends on the amount of summarization data available in the source language. Similar to previous work (Ou"
2020.findings-emnlp.360,W13-3102,0,0.0528277,"Missing"
2020.findings-emnlp.360,W13-3103,0,0.508918,"anslation at inference time) by leveraging synthetic data and Neural Machine Translation as a pre-training step. Our method significantly outperforms the baseline approaches, while being more cost efficient during inference. 1 Introduction Although there has been a tremendous amount of progress in abstractive summarization in recent years, most research has focused on monolingual summarization because of the lack of high quality multilingual resources (Lewis et al., 2019a; Song et al., 2020). While there have been a few studies to address the lack of resources for cross-lingual summarization (Giannakopoulos, 2013; Li et al., 2013; Elhadad et al., 2013; Nguyen and Daum´e III, 2019), the datasets employed are very limited in size. Scarcity in the availability of data for crosslingual abstractive summarization can largely be attributed to the difficulty of collecting high-quality, ∗ Equal contribution. https://www.wikihow.com 2 The data was collected in accordance with the terms and conditions listed on the website. 1 large-scale datasets via crowd-sourcing. It is a costly endeavor, since it requires humans to read, comprehend, condense, and paraphrase entire articles. Moreover, subjectivity in content s"
2020.findings-emnlp.360,W04-1013,0,0.196955,"n from the source language into English, and then we further fine-tune the model for cross-lingual summarization (DC+Synth+MT). Similar to above, since we only have a limited amount of parallel document pairs in our dataset, we translate English documents into the source language to create additional parallel data. This method of back-translation to create additional parallel data has been shown to be effective in improving the performance of neural machine translation systems (Sennrich et al., 2016; Hoang et al., 2018; Edunov et al., 2018).8 5 Results and Analysis Table 4 shows ROUGE scores (Lin, 2004) for the baselines and proposed cross-lingual approaches. We observe that the lead baseline performs poorly for this task, unlike in the news domain where it’s shown to be a strong baseline (Brandow et al., 8 While back-translation typically uses an intermediate training checkpoint to create synthetic data, we instead use AWS translate. 1995). When comparing the performance of Trans-Sum vs. Sum-Trans, we find that performance depends on the amount of summarization data available in the source language. Similar to previous work (Ouyang et al., 2019), we find that Tran-Sum works significantly be"
2020.findings-emnlp.360,D19-1387,0,0.112476,"in/dev/test splits. When splitting the English data, we ensure that all articles from the same topic as test articles in any of the four non-English languages, are included in the test set. This leaves us with ∼ 69K English articles that we randomly split into train and dev set (90/10 split). See Appendix A.2 for more information. We use large, pre-trained language models as a starting point for our experiments, given their success on a variety of downstream Natural Language Processing tasks (Devlin et al., 2019), including state of the art results for text summarization (Lewis et al., 2019b; Liu and Lapata, 2019). In particular, we use mBART (Liu et al., 2020), which is a multi-lingual language model that has been trained on large, monolingual corpora in 25 languages. The model uses a shared sub-word vocabulary, encoder, and decoder across all 25 languages, and is trained as a denoising auto-encoder during the pre-training step. Liu et al. (2020) showed that this pre-training method provides a good initialization for downstream machine translation tasks, particularly in lower resources settings, making this an ideal starting point for our cross-lingual summarization experiments. We also ran initial ex"
2020.findings-emnlp.360,2020.tacl-1.47,0,0.123669,"we ensure that all articles from the same topic as test articles in any of the four non-English languages, are included in the test set. This leaves us with ∼ 69K English articles that we randomly split into train and dev set (90/10 split). See Appendix A.2 for more information. We use large, pre-trained language models as a starting point for our experiments, given their success on a variety of downstream Natural Language Processing tasks (Devlin et al., 2019), including state of the art results for text summarization (Lewis et al., 2019b; Liu and Lapata, 2019). In particular, we use mBART (Liu et al., 2020), which is a multi-lingual language model that has been trained on large, monolingual corpora in 25 languages. The model uses a shared sub-word vocabulary, encoder, and decoder across all 25 languages, and is trained as a denoising auto-encoder during the pre-training step. Liu et al. (2020) showed that this pre-training method provides a good initialization for downstream machine translation tasks, particularly in lower resources settings, making this an ideal starting point for our cross-lingual summarization experiments. We also ran initial experiments with non-pretrained transformer models"
2020.findings-emnlp.360,K16-1028,0,0.0631803,"Missing"
2020.findings-emnlp.360,D18-1206,0,0.0239227,"em generated summaries are fluent, however DC+Synth+MT has better overlap with the content in the reference summary.10 9 The reference was only shown when evaluating for content overlap, and not for fluency evaluation. 4040 10 More examples are provided in Appendix A.4. Model Trans-Sum Trans-Sum-R DC+Synth+MT Fluency Content 2.61 2.62 2.67 2.07 2.09 2.19 Table 6: Human evaluation scores on a scale of 1-3. 6 Related Work Abstractive Summarization. The majority of research in abstractive summarization has focused on monolingual summarization in English (Gehrmann et al., 2018; Song et al., 2020; Narayan et al., 2018). Rush et al. (2015) proposes the first neural abstractive summarization model using an attentionbased convolutional neural network encoder and a feed-forward decoder. Chopra et al. (2016) shows improvements over this model using a recurrent neural network for the decoder. Nallapati et al. (2016) shows further improvements by incorporating embeddings for linguistic features such as part-of-speech tags and named-entity tags into their model, as well as a pointer network (Vinyals et al., 2015) to enable copying words from the source article. See et al. (2017) extends this model by further incorp"
2020.findings-emnlp.360,D19-5411,0,0.218419,"Missing"
2020.findings-emnlp.360,orasan-chiorean-2008-evaluation,0,0.11722,"Missing"
2020.findings-emnlp.360,N19-4009,0,0.0140513,"results were significantly worse than those with the pre-trained models. We fine-tune mBART for both monolingual and cross-lingual summarization as a standard sequence-to-sequence model, where the input document is represented as a sequence of tokens (subword units), with a special separator token between each sentence, and a language indicator token at the end of the document. The output summary is represented in a similar manner, with a language indicator token at the beginning of the sequence, to prime the decoder for generation in the target language, as shown in Figure 3. We use Fairseq (Ott et al., 2019) for all our experiments, and we follow the hyper-parameter settings that were used by Lewis et al. (2019b) to fine-tune BART for monolingual summarization in English. See Appendix A.1 for more details. 4.1 Baselines We evaluate the following baseline approaches for cross-lingual summarization on our data: leadn : copies first n sentences from the corresponding parallel English source articles. We report results for n = 3 since it performs the best. Summarize-then-translate (Sum-Trans): We 4037 Figure 3: An example showing the fine-tuning procedure for cross-lingual summarization from Spanish"
2020.findings-emnlp.360,N19-1204,0,0.545375,"glish. There are in total 141,457 English articlesummary pairs in our dataset. tude larger than Global Voices, which is the largest dataset to date for cross-lingual evaluation. The Data Statement (Bender and Friedman, 2018) for our dataset can be found in Appendix A.3. Train Validation Spanish 81,514 Russian 38,107 Vietnamese 9,473 Turkish 3,241 Test 9,057 22,643 4,234 10,586 1,052 2,632 360 901 Table 3: Number of examples in Train/Validation/Test splits per language. 4 Cross-lingual Experiments Following the prior work in cross-lingual abstractive summarization (Nguyen and Daum´e III, 2019; Ouyang et al., 2019), we aim to generate English summaries from non-English articles, as an initial study. We experiment with five languages (i.e. English, Spanish, Russian, Turkish, and Vietnamese) covering three language families (i.e. IndoEuropean, Ural-Altaic and Austroasiatic). We split the data for each of the four non-English languages into train/dev/test splits. When splitting the English data, we ensure that all articles from the same topic as test articles in any of the four non-English languages, are included in the test set. This leaves us with ∼ 69K English articles that we randomly split into train"
2020.findings-emnlp.360,D15-1044,0,0.0375053,"are fluent, however DC+Synth+MT has better overlap with the content in the reference summary.10 9 The reference was only shown when evaluating for content overlap, and not for fluency evaluation. 4040 10 More examples are provided in Appendix A.4. Model Trans-Sum Trans-Sum-R DC+Synth+MT Fluency Content 2.61 2.62 2.67 2.07 2.09 2.19 Table 6: Human evaluation scores on a scale of 1-3. 6 Related Work Abstractive Summarization. The majority of research in abstractive summarization has focused on monolingual summarization in English (Gehrmann et al., 2018; Song et al., 2020; Narayan et al., 2018). Rush et al. (2015) proposes the first neural abstractive summarization model using an attentionbased convolutional neural network encoder and a feed-forward decoder. Chopra et al. (2016) shows improvements over this model using a recurrent neural network for the decoder. Nallapati et al. (2016) shows further improvements by incorporating embeddings for linguistic features such as part-of-speech tags and named-entity tags into their model, as well as a pointer network (Vinyals et al., 2015) to enable copying words from the source article. See et al. (2017) extends this model by further incorporating a coverage p"
2020.findings-emnlp.360,N19-1380,0,0.0245473,"4.41/31.18† 36.48/14.29/30.96‡ DC+Synth+MT 40.60/16.89/34.06† 42.76/20.47/37.09‡ 37.09/14.81/31.67† 37.86/15.26/32.33† Table 4: Cross-lingual summarization results. The numbers correspond to ROUGE-1/ROUGE-2/ROUGE-L F1 scores respectively. † indicates where ROUGE-L F1 is significantly better than all baselines, and ‡ indicates where ROUGE-L F1 is significantly better than all baselines except Trans-Sum-R. We use Welch’s t-test, and use p < 0.01 to assess significance. data has been shown to be an effective strategy for cross-lingual transfer for text classification and sequence labeling tasks (Schuster et al., 2019). We note that while this method still relies on machine translation, the cost of translation is shifted to training time, and thus is a one-time cost. Since a cross-lingual summarization model needs to learn how to translate salient information from one language to another, we hypothesize that training the model for machine translation can improve performance of cross-lingual summarization. Therefore, we propose a two-step fine-tuning approach, where we first fine-tune the mBART model for document level machine translation from the source language into English, and then we further fine-tune t"
2020.findings-emnlp.360,P17-1099,0,0.0406669,"et al., 2018; Song et al., 2020; Narayan et al., 2018). Rush et al. (2015) proposes the first neural abstractive summarization model using an attentionbased convolutional neural network encoder and a feed-forward decoder. Chopra et al. (2016) shows improvements over this model using a recurrent neural network for the decoder. Nallapati et al. (2016) shows further improvements by incorporating embeddings for linguistic features such as part-of-speech tags and named-entity tags into their model, as well as a pointer network (Vinyals et al., 2015) to enable copying words from the source article. See et al. (2017) extends this model by further incorporating a coverage penalty to address the problem of repetitions in the generated summary. Chen and Bansal (2018) takes a two stage approach to abstractive summarization by learning an extractor to select salient sentences from the articles, and an abstractor to rewrite the sentences selected by the extractor. They further train the extractor and abstractor end-to-end with a policygradient method, using ROUGE-L F1 as the reward function. Recently, pre-trained language models have achieved the state of the art results in abstractive summarization (Lewis et a"
2020.findings-emnlp.360,P16-1009,0,0.0342526,"opose a two-step fine-tuning approach, where we first fine-tune the mBART model for document level machine translation from the source language into English, and then we further fine-tune the model for cross-lingual summarization (DC+Synth+MT). Similar to above, since we only have a limited amount of parallel document pairs in our dataset, we translate English documents into the source language to create additional parallel data. This method of back-translation to create additional parallel data has been shown to be effective in improving the performance of neural machine translation systems (Sennrich et al., 2016; Hoang et al., 2018; Edunov et al., 2018).8 5 Results and Analysis Table 4 shows ROUGE scores (Lin, 2004) for the baselines and proposed cross-lingual approaches. We observe that the lead baseline performs poorly for this task, unlike in the news domain where it’s shown to be a strong baseline (Brandow et al., 8 While back-translation typically uses an intermediate training checkpoint to create synthetic data, we instead use AWS translate. 1995). When comparing the performance of Trans-Sum vs. Sum-Trans, we find that performance depends on the amount of summarization data available in the sou"
2020.findings-emnlp.360,D15-1012,0,0.057299,"Missing"
2020.findings-emnlp.360,D18-1448,0,0.0143472,"m to account for potential translation noise. There is limited prior work in direct crosslingual summarization. Shen et al. (2018) propose zero-shot cross-lingual headline generation to generate Chinese headlines for English articles, via a teacher-student framework, using two teacher models. Duan et al. (2019) propose a similar approach for cross-lingual abstractive sentence summarization. We note that our approach is much simpler and also focuses on a different kind of summarization task. Zhu et al. (2019) use round-trip translation of large scale monolingual datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015) to generate synthetic training data for their models, and train a multi-task model to to learn both translation and cross-lingual summarization. We tried their approach on our data, using the code provided,11 but the results were worse than all baselines except lead.12 We suspect that this may be due to the amount of training data, as their synthetic dataset was much larger than ours (1.69M pairs for Zh-En). An extension of their approach would be to incorporate multi-task training for pre-trained mBART, which we leave for future work. Scarcity of crosslingual summarization"
2020.findings-emnlp.360,D19-1302,0,0.0469993,"o get noisy English articles. They then train on noisy article and clean summary pairs, which allows them to account for potential translation noise. There is limited prior work in direct crosslingual summarization. Shen et al. (2018) propose zero-shot cross-lingual headline generation to generate Chinese headlines for English articles, via a teacher-student framework, using two teacher models. Duan et al. (2019) propose a similar approach for cross-lingual abstractive sentence summarization. We note that our approach is much simpler and also focuses on a different kind of summarization task. Zhu et al. (2019) use round-trip translation of large scale monolingual datasets (Hermann et al., 2015; Zhu et al., 2018; Hu et al., 2015) to generate synthetic training data for their models, and train a multi-task model to to learn both translation and cross-lingual summarization. We tried their approach on our data, using the code provided,11 but the results were worse than all baselines except lead.12 We suspect that this may be due to the amount of training data, as their synthetic dataset was much larger than ours (1.69M pairs for Zh-En). An extension of their approach would be to incorporate multi-task"
2020.findings-emnlp.360,J11-3005,0,0.0236574,"odel to to learn both translation and cross-lingual summarization. We tried their approach on our data, using the code provided,11 but the results were worse than all baselines except lead.12 We suspect that this may be due to the amount of training data, as their synthetic dataset was much larger than ours (1.69M pairs for Zh-En). An extension of their approach would be to incorporate multi-task training for pre-trained mBART, which we leave for future work. Scarcity of crosslingual summarization data has limited prior work to a few languages, and mostly in the news domain (Wan et al., 2010; Wan, 2011; Yao et al., 2015; Zhang et al., 2016; Wan et al., 2019). While there is some existing work trying to address this (Nguyen and Daum´e III, 2019), the proposed dataset is still limited in size, and contains summaries only in English. We address this limitation by proposing a 11 https://github.com/ZNLP/NCLS-Corpora This model gets ROUGE-L F1 scores of 22.49, 23.38, 20.79, 19.45 for Spanish, Turkish, Russian and Vietnamese respectively. 4041 12 new benchmark dataset. 7 by sentence selection. Information Processing & Management, 31(5):675 – 685. Summarizing Text. Conclusion We present a benchmark"
2020.findings-emnlp.360,P10-1094,0,0.656157,"rce language. Similar to previous work (Ouyang et al., 2019), we find that Tran-Sum works significantly better when the amount of data in the source language is limited. However, as source language training data size increases, we see that the gap in performance decreases, as in the case of Spanish, which is similar in size to English, vs. Turkish, which is the lowest resource language for summarization in our dataset. This suggests that when the source language data is comparable in size or larger than the target language data, SumTrans approach may be worthwhile to consider, as suggested by Wan et al. (2010), since it is more cost effective (translating summaries instead of whole articles) and may avoid error propagation from translation systems. Amongst the baseline methods, Trans-Sum-R works the best. It consistently does better than Trans-Sum baseline, suggesting that round-trip translation to create noisy data can be an effective way to make the model more robust to translation errors at inference time. Since we have gold translations (Trans-Sum G) for each of the articles, we can measure the translation error in the Trans-Sum system. We see that on average, the round-trip translation method"
2020.vardial-1.15,C16-1207,1,0.928715,"ating these measures for all combinations of clustering algorithms and word embeddings on the Brown Corpus, we decide to move forward with GloVe embeddings and k-means clustering as the optimal combination on the two experimental corpora. The first corpus, TwitterAAE, was collected by the SLANG Lab at the University of Massachusetts at Amherst and slightly resembles SAE (Blodgett et al., 2016). The Gang Violence dataset, collected by the SAFE Lab at Columbia university, is the second corpus; it contains AAE combined with hyper-local slang that make the text especially difficult to understand (Blevins et al., 2016). We refer to the two AAE corpora as the experimental datasets. By clustering unknown words with related known words, our approach can expand existing resources for automatically learning slang and AAE words. Throughout our work, we learn that established lexical resources like WordNet and ConceptNet need to be augmented to support slang and AAE: these resources are unable to serve as gold standards for words that have never been seen before. We also provide results from human evaluations on subsets of our machine-generated clusters. An evaluation on Amazon Mechanical Turk (AMT) over the clust"
2020.vardial-1.15,D16-1120,0,0.0676517,"Missing"
2020.vardial-1.15,J92-4003,0,0.296302,"grim, Amytal + 36 words Sword [None] Both smile, grin, grinning, smiling blue, profane, dark + 6 words sword, steel, brand, blade ConceptNet action, smiler + 89 words blow, calypso, windows + 249 words tuck, swordbearing + 182 words Table 1: The gold clusters from WordNet and ConceptNet are shown in the table above. For a query word shown in the leftmost column, the synonyms in only WordNet, related words in only ConceptNet, and words in both resources are displayed in the next columns. Brown’s clustering is a type of agglomerative clustering that uses context to group similar words together (Brown et al., 1992). In Brown’s clustering, we use bigrams to account for context and pairwise combine clusters whose words share similar neighbors; Brown’s clustering groups individual words together based on context from bigrams. This clustering algorithm can be used to assign words to classes based on the clustering results, which would allow for the categorization of new words in the future. A class can function as the high-level label for a cluster of words. The original work presents classes and clusters built from a 260,000-word vocabulary, such as: Friday Monday Thursday Wednesday Tuesday Saturday Sunday"
2020.vardial-1.15,D18-1005,1,0.884213,"ultimately help in augmenting lexical resources for slang and AAE, • Exploration of clustering algorithms and word embeddings for clustering in the semantic space, and • Automatic and human evaluation of these machine-generated clusters from three different datasets, along with the Cluster Split Score as a new metric for evaluating the clusters. 2 Related Work Researchers have already started to work on applications of natural language processing for social media and nonstandard English. One approach includes adapting pretrained word embeddings for target domains, like social media. Recently, Chang et al. (2018) generated a lexicon and set of domain-specific word embeddings that were automatically induced from an unlabeled section of the Gang Violence dataset that is used in this work. Han and Eisenstein (2019)’s work on fine-tuning BERT embeddings for Early Modern English and Twitter supports the viability of using domain-adaptive fine-tuning for social media. Costa Bertaglia and Volpe Nunes (2016) propose an unsupervised, scalable, and language- and domain-independent method for learning word embeddings for Brazilian Portuguese. Other unsupervised methods include Hamilton et al. (2016)’s label prop"
2020.vardial-1.15,W16-3916,0,0.0599102,"Missing"
2020.vardial-1.15,N19-1423,0,0.0187355,"Missing"
2020.vardial-1.15,L16-1686,0,0.0282406,"from creating technology for analyzing dialects like these, which evolve quickly on social media. We already have lexical resources for understanding SAE (e.g., the Dictionary of Affect in Language for sentiment, WordNet for synonyms and antonyms, and ConceptNet for a variety of word relations), but equivalent resources for nonstandard English do not exist (Whissell, 1989; University, 2010; Speer et al., 2018). The small WordNet-like resource for slang, SlangNet, contains only 3000 words (compared to the 150000+ words in WordNet), and there are no resources for AAE at the time of this study (Dhuliawala et al., 2016). To develop tools for analyzing nonstandard English, we need lexical resources that can provide clues about the meaning of new words that appear. We also need approaches for evaluating whether the derived representations are accurate or not. In this paper, we present a comparison of methods that combine clustering algorithms and word embeddings to group unknown words in the semantic space. We explore the use of agglomerative and k-means clustering on GloVe and Word2Vec embeddings and Brown’s clustering on bigrams to create semantically related clusters of words which could then be used in dow"
2020.vardial-1.15,D16-1057,0,0.0217404,"edia. Recently, Chang et al. (2018) generated a lexicon and set of domain-specific word embeddings that were automatically induced from an unlabeled section of the Gang Violence dataset that is used in this work. Han and Eisenstein (2019)’s work on fine-tuning BERT embeddings for Early Modern English and Twitter supports the viability of using domain-adaptive fine-tuning for social media. Costa Bertaglia and Volpe Nunes (2016) propose an unsupervised, scalable, and language- and domain-independent method for learning word embeddings for Brazilian Portuguese. Other unsupervised methods include Hamilton et al. (2016)’s label propagation framework to induce domain-specific sentiment lexicons using seed words and Sinha and Mihalcea (2007)’s graph-based word sense disambiguation. Fine-tuning pretrained word embeddings is a promising foundation for developing completely unsupervised algorithms for semantics. In our work, we focus on simple methods using word embeddings to create clusters of semantically related words, an approach that enables interpretability of results as well as information about the meaning of new words that are just beginning to appear with low frequency. 161 Advancements in NLP methods r"
2020.vardial-1.15,D19-1433,0,0.0122481,"ion of these machine-generated clusters from three different datasets, along with the Cluster Split Score as a new metric for evaluating the clusters. 2 Related Work Researchers have already started to work on applications of natural language processing for social media and nonstandard English. One approach includes adapting pretrained word embeddings for target domains, like social media. Recently, Chang et al. (2018) generated a lexicon and set of domain-specific word embeddings that were automatically induced from an unlabeled section of the Gang Violence dataset that is used in this work. Han and Eisenstein (2019)’s work on fine-tuning BERT embeddings for Early Modern English and Twitter supports the viability of using domain-adaptive fine-tuning for social media. Costa Bertaglia and Volpe Nunes (2016) propose an unsupervised, scalable, and language- and domain-independent method for learning word embeddings for Brazilian Portuguese. Other unsupervised methods include Hamilton et al. (2016)’s label propagation framework to induce domain-specific sentiment lexicons using seed words and Sinha and Mihalcea (2007)’s graph-based word sense disambiguation. Fine-tuning pretrained word embeddings is a promisin"
2020.vardial-1.15,D14-1162,0,0.0833893,"ng sizes. Given the variety of language in the corpora, the clusters are not likely to be of uniform size and choosing the number of clusters beforehand requires additional domain knowledge. 4.2 Word Embeddings Word2Vec is a neural model trained on Google News (Mikolov et al., 2013). The older of the two word embeddings, Word2Vec represents a baseline for word embedding results. Like GloVe, Word2Vec is context-independent and combines all senses of a word into a single vector. GloVe embeddings are similar to Word2Vec, but they are trained on a cooccurrence matrix rather than a neural network (Pennington et al., 2014). Using a context-independent word embedding loses the distinction between different senses of a word but makes the vectors immediately available for downstream tasks. We use the 50-dimensional Twitter embeddings for this task. BERT embeddings are the current state of the art, but we do not use them for this work. These embeddings are trained on a context-dependent neural model at the subword level, which makes BERT more robust to out-of-vocabulary words and would be useful for the constantly changing language on social media (Devlin et al., 2019). BERT embeddings, however, separate the differ"
2020.vardial-1.15,L18-1046,0,0.0272828,"d communication reflects geographic proximity, population size, and racial demographics. Stewart and Eisenstein (2018) find that linguistic dissemination is a strong predictor of the longevity of a new word while social dissemination is not. Change in online language is driven by social dynamics and sociocultural influence, and using natural language processing techniques on large social media datasets yields important results for sociological studies (Goel et al., 2016). Along with tracking the evolution of language, our methods can help with the use of word embeddings for lexical discovery (Roberts and Egg, 2018). Our work aims to embrace the constantly evolving nature of language and improve the representation of meanings of new words introduced over time. 3 Corpora and Linguistic Tools This paper presents clusters of vocabulary from three corpora: Brown Corpus, TwitterAAE, and the Gang Violence dataset. We also use WordNet and ConceptNet to automatically calculate precision and recall of automatically generated clusters. We present example sentences from each corpus below. Brown Corpus “He’s all right, Craig,” Rachel said. TwitterAAE Whoever tryna do this tax thing to get more bread let me know Gang"
2020.vardial-1.15,D18-1467,0,0.0155759,"d to nonstandard English could inform research in sociolinguistics and dialectology, which typically use other methods (Meyerhoff, 2016). Timestamps on social media datasets also allow for observing and predicting the evolution of language. Robust systems for tracking the appearance of new words, association of new meanings with existing words, and disappearance of old words can help us understand how, when, and why language changes. Eisenstein et al. (2014) show that language evolution in computer-mediated communication reflects geographic proximity, population size, and racial demographics. Stewart and Eisenstein (2018) find that linguistic dissemination is a strong predictor of the longevity of a new word while social dissemination is not. Change in online language is driven by social dynamics and sociocultural influence, and using natural language processing techniques on large social media datasets yields important results for sociological studies (Goel et al., 2016). Along with tracking the evolution of language, our methods can help with the use of word embeddings for lexical discovery (Roberts and Egg, 2018). Our work aims to embrace the constantly evolving nature of language and improve the representa"
2020.wmt-1.141,P19-1294,0,0.653747,"cases where such a system would be beneficial. For example, content providers meticulously curate lists of terminologies for their domains that indicate preferred translations for technical terms. Lexically constrained APE would also be useful for cross-lingual information retrieval. When displaying snippets from retrieved documents, the query term should appear in the translation output (if it does in the source) as it can make relevance clear to the end user. Here, the query serves as the term. While recent approaches allow inference time adaptation of NMT systems using these terminologies (Dinu et al., 2019; Post and Vilar, 2018), postediting translations with a generic APE system may lead to dropped terms. A constraint-aware APE system would allow to fix systematic translation errors, while keeping the terminologies intact. Inspired by Dinu et al. (2019), we consider a range of representations which augment input sequences with constraint tokens and factors for use in an autoregressive Transformer (AT) APE model. Using this approach, the constraints are explicitly represented in the encoder input sequence, and the model learns to prefer translations that contain the supplied terminologies durin"
2020.wmt-1.141,P17-1141,0,0.0422942,"E pipeline. 3. We analyze the robustness of the constraint translation behavior and suggest a simple data augmentation technique that both improves translation quality and increases the number of correctly translated terms. 2 2.1 Related Work MT with Terminology Constraints Integrating terminology constraints into translation can be divided into two approaches: constrained decoding and input sequence modification. Constrained decoding modifies the decoding process to enforce the generation of the specified terminologies. This includes methods that modify beam search, such as grid beam search (Hokamp and Liu, 2017) and dynamic beam allocations (Post and Vilar, 2018). While these approaches are effective in including terminologies, they come with an increase in inference time due to the added overhead in the search algorithm. The LevT (Gu et al., 2019), which uses a nonautoregressive decoding procedure, can initialize its decoder with a partial or incomplete output sequence. By initializing the decoder output with terminology constraints, Susanto et al. (2020) train a LevT model to perform constrained decoding. Unlike constrained search methods in autoregressive models, this initialization technique does"
2020.wmt-1.141,W16-2378,0,0.0379075,"Missing"
2020.wmt-1.141,W18-6467,0,0.0137064,"roach has the benefit of not adding additional overhead during inference. 2.2 Automatic Post-Editing The APE task has gone through many iterations, since it was originally proposed by Simard et al. (2007). Initially, the task was to improve an unknown phrase-based machine transition (PBMT) system. An additional task to fix errors of an NMT system was introduced at WMT 2018 (Chatterjee et al., 2018). For the APE tasks, the use of the multi-source variant of the neural encoder-decoder model is the most popular approach (Bojar et al., 2017), with the Multi-source Transformer (MST) instantiation (Junczys-Dowmunt and Grundkiewicz, 2018) achieving state-of-the-art results in 2018. Based on the AT model (Vaswani et al., 2017), the MST model consists of two Transformer encoders and a single decoder. The source sentence and the MT system output are fed separately to the two encoders, where the outputs are concatenated and then fed into the decoder to perform post-editing. Recent work has explored alternative architectures for APE. The winner of 2019 APE tasks (Lopes et al., 2019), for example, uses a BERTbased encoder and decoder. Gu et al. (2019) both introduce the LevT model and demonstrate its utility on an APE task. 3 Constr"
2020.wmt-1.141,P07-2045,0,0.0168074,"e relatively small, we augment them with large synthetic datasets for pretraining: artificial (Junczys-Dowmunt and Grundkiewicz, 2016) and eSCAPE (Negri et al., 2018). The artificial dataset is generated using round-trip translation of two PBMT systems. It is already cleaned and tokenized. The eSCAPE dataset, containing 7,258,533 triplets, is created using NMT generated output from various parallel corpora. The data for eSCAPE is noisy, and we follow Lee et al. (2019)’s procedure to filter the dataset, which results in around 5 million triplets. We then tokenize the filtered data using Moses (Koehn et al., 2007).1 For pretraining on the synthetic corpora, we set aside 1,000 randomly sampled triplets as our validation set. Table 1 summarizes the statistics of both the evaluation and pretraining datasets. For both tasks, we use the same preprocessing steps. After tokenization, we truecase the data using Moses. We then use BPE with 32,000 merge operations on the joined vocabulary of source and target language. 5.2 Terminology Dataset We create terminology sets for each APE dataset using Wiktionary.2 We follow the procedure of Dinu et al. (2019), finding term translation pairs (ˇ x, y ˇ) in Wiktionary su"
2020.wmt-1.141,W19-5412,0,0.0224976,"Missing"
2020.wmt-1.141,W19-5413,0,0.0203122,"Missing"
2020.wmt-1.141,L18-1004,0,0.0390869,"Missing"
2020.wmt-1.141,N19-4009,0,0.0335909,"Missing"
2020.wmt-1.141,P02-1040,0,0.105896,"Missing"
2020.wmt-1.141,N18-1119,0,0.413604,"system would be beneficial. For example, content providers meticulously curate lists of terminologies for their domains that indicate preferred translations for technical terms. Lexically constrained APE would also be useful for cross-lingual information retrieval. When displaying snippets from retrieved documents, the query term should appear in the translation output (if it does in the source) as it can make relevance clear to the end user. Here, the query serves as the term. While recent approaches allow inference time adaptation of NMT systems using these terminologies (Dinu et al., 2019; Post and Vilar, 2018), postediting translations with a generic APE system may lead to dropped terms. A constraint-aware APE system would allow to fix systematic translation errors, while keeping the terminologies intact. Inspired by Dinu et al. (2019), we consider a range of representations which augment input sequences with constraint tokens and factors for use in an autoregressive Transformer (AT) APE model. Using this approach, the constraints are explicitly represented in the encoder input sequence, and the model learns to prefer translations that contain the supplied terminologies during decoding. We also exp"
2020.wmt-1.141,P16-1162,0,0.0810005,"uences, the source sentence and the MT output to be postedited. To accommodate these two sequences, we use the MST model of Tebbifakhr et al. (2018), which uses a separate Transformer to encode each sequence. The outputs of each encoder are concatenated and attended to by the decoder. We augment the encoder for the source sentence with the append and replace methods. Figure 1 shows an example of the inputs for the append and replace methods, x+ and x− respectively. To account for the additional input of MT, γ, for the source factors, we use 3 for each token in γ. For Byte-Pair Encoding (BPE) (Sennrich et al., 2016), the corresponding source factor token is applied for all subword units. We train three variants based on MST: an unconstrained version as the baseline (MST), and two constrained versions using the append (MST Append) and replace (MST Replace) methods as described in subsection 4.1. 4.3 Levenshtein Transformer The LevT follows the Transformer encoderdecoder architecture. However, instead of a regular Transformer decoder, the model uses three consecutive layers to simulate the edit operations. The first layer predicts whether each token should be deleted or kept. The second layer predicts how"
2020.wmt-1.141,2006.amta-papers.25,0,0.22635,"Missing"
2020.wmt-1.141,2020.acl-main.325,0,0.279038,"ecoding process to enforce the generation of the specified terminologies. This includes methods that modify beam search, such as grid beam search (Hokamp and Liu, 2017) and dynamic beam allocations (Post and Vilar, 2018). While these approaches are effective in including terminologies, they come with an increase in inference time due to the added overhead in the search algorithm. The LevT (Gu et al., 2019), which uses a nonautoregressive decoding procedure, can initialize its decoder with a partial or incomplete output sequence. By initializing the decoder output with terminology constraints, Susanto et al. (2020) train a LevT model to perform constrained decoding. Unlike constrained search methods in autoregressive models, this initialization technique does not add any significant overhead to the decoding process. When modified to disallow deletion of terms and insertion between consecutive terminology tokens, LevT is able to retain all terminologies without affecting the performance and speed. Alternatively, Dinu et al. (2019) propose modifying the encoder input sequence to represent terminology constraints. During training, the model learns to identify constraints in the input sequence, and translat"
2020.wmt-1.141,W18-6471,0,0.0571416,"cially, when a source side constraint x ˇ(i) matches a sub-sequence in x, it is required that the sub-sequence be translated as y ˇ(i) . See Figure 1 for an example. 4 Models While there are existing models to address the APE task, and the lexical constrained MT task, it is not clear how to represent lexical constraints for APE models which, unlike MT models, take two sequences as input. We propose several techniques to incorporate constraints as additional inputs to the APE encoder by combining the input sequence modification used in constrained MT (Dinu et al., 2019) with the MST method of (Tebbifakhr et al., 2018). For decoding, we experiment with both the AT and the LevT decoders. The LevT decoder can additionally take advantage of different decoder initialization strategies for constrained decoding. We first briefly show how we encode terminology constraints in the input sequence, before deModel Input Init. MST MST Append MST Replace x, γ x+ , γ x− , γ – – – LevT LevT Append LevT Replace MS LevT x x+ x− x, γ γ γ γ y ˇ1 , . . . , y ˇ(t) Figure 2: Setup for the models by the input and initialization at inference. scribing how they are incorporated into the MST and LevT APE models specifically. 4.1 Enco"
2021.acl-long.133,P15-1034,0,0.0223896,"ccuracy on fake documents. A third of the fake news documents were predicted incorrectly by over half of the human subjects. This indicates that our automatically generated fake documents are also very hard for humans to detect. The most common clues humans used to detect fake news include linguistic style, topic coherence, specific event details and novel entities. 6 Related Work Fake News Detection. Traditional approaches to fake news detection are largely based on factchecking, text-style, or context from a single modality (Ciampaglia et al., 2015; Shi and Weninger, 2016; Pan et al., 2018; Angeli et al., 2015). Other approaches include detecting previously factchecked claims (Shaar et al., 2020), retrieving sentences that explain fact-checking (Nadeem et al., 2019; Atanasova et al., 2020), and leveraging context and discourse information (Nakov et al., 2019). 1689 Image Caption Body Text Misinformative KEs Aerial view of Fort McHenry. The battle of Fort McHenry, which took place in September of 1814, was a pivotal moment in the U.S. War of Independence...When the British finally left, they left behind a trail of destruction, including the destruction of the twin towers of the World Trade Center ..."
2021.acl-long.133,2020.findings-emnlp.89,0,0.011227,"stan, the Taliban released to the media this picture, which it said shows the suicide bombers who attacked the army base in Mazar-i-Sharif, April 21, 2017 Fake caption: On 21 April 2017 the Taliban released this picture to the army in Afghanistan which they said was a suicide bomber hiding at a media base in the city of Mazar-i-Sharif Figure 4: Example of AMR-to-text fake caption generation. The roles of army and media (in blue) are switched and the node corresponding to the event trigger (in red) attacked is negated. To obtain the AMR graphs, we use the stacktransformer based AMR parser from Astudillo et al. (2020) and train it on AMR 3.03 . Given the AMR graph, we vary the manipulation as follows: (1) Role switching - we randomly select two entity mentions that are present in different argument subgraphs of the AMR root node and interchange their positions in the AMR graph. (2) Predicate negation - we randomly pick predicates in the AMR graph corresponding to event triggers and other verbs, and replace them with their antonyms, which we obtain from WordNet (Fellbaum, 1998). This manipulation also includes reverting nodes with negative polarity, thereby negating the sentence. 1687 3 https://catalog.ldc."
2021.acl-long.133,2020.acl-main.656,0,0.0166883,"ocuments are also very hard for humans to detect. The most common clues humans used to detect fake news include linguistic style, topic coherence, specific event details and novel entities. 6 Related Work Fake News Detection. Traditional approaches to fake news detection are largely based on factchecking, text-style, or context from a single modality (Ciampaglia et al., 2015; Shi and Weninger, 2016; Pan et al., 2018; Angeli et al., 2015). Other approaches include detecting previously factchecked claims (Shaar et al., 2020), retrieving sentences that explain fact-checking (Nadeem et al., 2019; Atanasova et al., 2020), and leveraging context and discourse information (Nakov et al., 2019). 1689 Image Caption Body Text Misinformative KEs Aerial view of Fort McHenry. The battle of Fort McHenry, which took place in September of 1814, was a pivotal moment in the U.S. War of Independence...When the British finally left, they left behind a trail of destruction, including the destruction of the twin towers of the World Trade Center ... <British, Conflict.Attack, twin towers> Table 4: An example fake document which Tan et al. (2020) misses, but InfoSurgeon successfully detects. Text Features P´erez-Rosas et al. (20"
2021.acl-long.133,D18-1389,0,0.0330051,"Missing"
2021.acl-long.133,W13-2322,0,0.0137395,"Missing"
2021.acl-long.133,D14-1162,0,0.0850423,"mbeddings: We define an attribute function, A : Nt , Er|a ) F , that transforms each of the nodes and edges to its initial representation by concatenating the following features: • Background Embeddings - For the entity nodes Nt that can be linked to Freebase, we use data dump from Google Developers resources2 to map them to their respective Wikipedia pages, which serve as a rich source of established background knowledge. Background node embedding features are initialized from passing a Long Short Term Memory networks (LSTM) based architecture (Gers et al., 2000) through the word embeddings (Pennington et al., 2014) of the first paragraph in the Wikipedia page, which usually starts with a mention of the Wiki page’s title. Background edge embedding features are initialized from passing the LSTM through the paragraphs that contain the mentions of both the head and tail nodes. These embeddings are set to a default zero vector for unlinkable nodes. 1685 2 https://developers.google.com/freebase/ • News Embeddings - These are the surface-level features circumstantial to the entities, relations, and events extracted. News-based node features are initialized from passing an LSTM through the word embeddings of th"
2021.acl-long.133,C18-1287,0,0.040838,"Missing"
2021.acl-long.133,N18-1119,0,0.0250419,"Missing"
2021.acl-long.133,D19-1216,0,0.0412103,"Missing"
2021.acl-long.133,2020.emnlp-main.163,0,0.565405,"the node features. We feed the body text and each caption through the summarization-based BERT encoder from Liu and Lapata (2019), which averages the encoded token embeddings across sentences through a weighted mechanism. For metadata, we run the text encoder on a string containing the article domain, publication date, author, and headline. For images, we concatenate object-based (Anderson et al., 2018) and event-based (Pratt et al., 2020) visual features. Features for the edges between global context nodes are initialized by the attention-based semantic similarity between the node features (Tan et al., 2020). 3.3 Local KG Representation Constructing a KG from each Multimedia News Article: We leverage a publicly available multimedia Information Extraction (IE) system (Li et al., 2020; Lin et al., 2020) to construct a withindocument knowledge graph KG = (Nt , Er|a ) for each multimedia article. The IE system can extract 197 types of entities, 61 types of relations, and 144 types of events from text and images. Then, it performs entity linking (Pan et al., 2017) to map entities extracted from both text and images to a background knowledge base e.g. Freebase (Bollacker et al., 2008) and NIL (unlinkab"
2021.acl-long.133,N18-1074,0,0.0540313,"Missing"
2021.acl-long.133,P17-2067,0,0.0247961,"guage processing (Zellers et al., 2019) and computer vision (Choi et al., 2018) have become the frontier for malicious actors to controllably generate misinformation at scale. These realistic-looking AI-generated “fake news” have been shown to easily deceive humans, and it is, thus, critical for us to develop robust verification techniques against machine-generated fake news (Tan et al., 2020; Zellers et al., 2019; Kaliyar et al., 2020). Current misinformation detection approaches mainly focus on document-level fake news detection using lexical features and semantic embedding representations (Wang, 2017; Karimi et al., 2018; Tan et al., 2020). However, fake news is often generated based on manipulating (misusing, exaggerating, or falsifying) only a small part of the true information, namely the knowledge 1 The code, data and resources related to the misinformation detector are made publicly available at https://github. com/yrf1/InfoSurgeon for research purposes. elements (KEs, including entities, relations and events). Moreover, recent news oftentimes makes claims that do not have verified evidence yet, and evaluating the truthfulness of these real-time claims depends more on their consisten"
2021.acl-long.300,D19-1352,0,0.0174858,"26 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tokens (Somali, Swahili, Tagalog) in the parallel corpus. Lang. Pair Augmented Dataset Size EN-SO EN-SW EN-TL 1,649,484 2,014,838 2,417,448 Table 2: Augmented dataset statistics; “augmented dataset size” refers to total number of positive and negative query-sentence samples in the augmented dataset. bel, 1997; Wade and Allan, 2005; Fan et al., 2018; Inel et al., 2018; Akkalyoncu Yilmaz et al., 2019). Given a document D = [S1 , . . . , S|D |], which is a sequence of sentences, and a query Q, following Liu and Croft (2002) we assign a relevance score by: rˆ = max p(r = 1|Q, S; W ) S∈D 4.3 We initialize English word embeddings with word2vec (Mikolov et al., 2013), and initialize SO/SW/TL word embeddings with FastText (Grave et al., 2018). For training we use a SparseAdam (Kingma and Ba, 2015) optimizer with learning rate 0.001. The hyperparameter λ2 in Section 3.3 is set to be 3 so that Lrel and λ2 Lrat are approximately on the same scale during training. More details on experiments are inc"
2021.acl-long.300,P18-1073,0,0.0160745,". In other work, Xu and Weischedel (2000) use a 2-state hidden Markov model (HMM) to estimate the probability that a passage is relevant given the query. Cross-lingual Word Embeddings Crosslingual embedding methods perform cross-lingual relevance prediction by representing query and passage terms of different languages in a shared semantic space (Vuli´c and Moens, 2015; Litschko et al., 2019, 2018; Joulin et al., 2018). Both supervised approaches trained on parallel sentence corpora (Levy et al., 2017; Luong et al., 2015) and unsupervised approaches with no parallel data (Lample et al., 2018; Artetxe et al., 2018) have been proposed to train cross-lingual word embeddings. Our approach differs from previous cross-lingual word embedding methods in two aspects. First, the focus of previous work has mostly been on learning a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work. Second, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approa"
2021.acl-long.300,D18-1216,0,0.0231306,"provide detailed comparisons of performance with other sentence selection approaches. Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat3882 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016). 3 Methods We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT. E 0 are in the same language, so checking whether q or a synon"
2021.acl-long.300,P19-3004,0,0.0145662,"a distributional word representation where translation across languages is primarily shaped by syntactic or shallow semantic similarity; it has not been tuned specifically for cross-language sentence selection tasks, which is the focus of our work. Second, in contrast to previous supervised approaches that train embeddings directly on a parallel corpus or bilingual dictionary, our approach trains embeddings on an artificial labeled dataset augmented from a parallel corpus and directly represents relevance across languages. Our data augmentation scheme to build a relevance model is inspired by Boschee et al. (2019), but we achieve significant performance improvement by incorporating rationale information into the embedding training process and provide detailed comparisons of performance with other sentence selection approaches. Trained Rationale Previous research has shown that models trained on classification tasks sometimes do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorpor"
2021.acl-long.300,P17-1171,0,0.022581,"’s applicability to even lower-resource settings and mitigation of hubness issues (Dinu and Baroni, 2015; Radovanovi´c et al., 2010). These findings are validated by empirical results of experiments in a low-resource sentence selection task, with English queries over sentence collections of text and speech in Somali, Swahili, and Tagalog. 2 Related Work Query-focused Sentence Selection Sentencelevel query relevance prediction is important for various downstream NLP tasks such as queryfocused summarization (Baumel et al., 2016, 2018; Feigenblat et al., 2017) and open-domain question answering (Chen et al., 2017; Dhingra et al., 2017; Kale et al., 2018). Such applications often depend on a sentence selection system to provide attention signals on which sentences to focus upon to generate a query-focused summary or answer a question. Cross-language Sentence Selection A common approach to cross-language sentence selection is to use MT to first translate either the query or the sentence to the same language and then perform standard monolingual IR (Nie, 2010). The risk of this approach is that errors in translation cascade to the IR system. As an alternative to generating full translations, PSQ (Darwish"
2021.acl-long.300,2016.amta-researchers.10,0,0.0221643,"do not use the correct rationale when making predictions, where a rationale is a mechanism of the classification model that is expected to correspond to human intuitions about salient features for the decision function (Jain and Wallace, 2019). Research has also shown that incorporating human rationales to guide a model’s attention distribution can potentially improve model performance on classification tasks (Bao et al., 2018). Trained rationales have also been used in neural MT (NMT); incorporat3882 ing alignments from SMT to guide NMT attention yields improvements in translation accuracy (Chen et al., 2016). 3 Methods We first describe our synthetic training set generation process, which converts a parallel sentence corpus for MT into cross-lingual query-sentence pairs with binary relevance judgements for training our SECLR model. Following that, we detail our SECLR model and finish with our method for rationale training with word alignments from SMT. E 0 are in the same language, so checking whether q or a synonym can be found in E 0 is a monolingual task. If we can verify that there is no direct match or synonym equivalent of q in E 0 then by transitivity it is unlikely there exists a translat"
2021.acl-long.300,2020.acl-main.747,0,0.0940514,"Missing"
2021.acl-long.300,N19-1423,0,0.0174726,"T) and speech (S) for Tagalog. data as our SECLR models. The 1-best output from each MT system is then scored with Indri (Strohman et al., 2005) to obtain relevance scores. Details of NMT and SMT systems are included in Appendix C.2. PSQ. To implement the PSQ model of Darwish and Oard (2003), we use the same alignment matrix as in rationale training (see Section 3.3) exMultilingual XLM-RoBERTa. We compare our model to the cross-lingual model XLM-RoBERTa (Conneau et al., 2020), which in previous research has been shown to have better performance on lowresource languages than multilingual BERT (Devlin et al., 2019). We use the Hugging Face implementation (Wolf et al., 2019) of XLM-RoBERTa (Base). We fine-tuned the model on the same augmented dataset of labeled query-sentence pairs as the SECLR models, but we apply the XLMRoBERTa tokenizer before feeding examples to the model. We fine-tuned the model for four epochs using an AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate 2 × 10−5 . Since XLMRoBERTa is pretrained on Somali and Swahili but not Tagalog, we only compare our models to XLMRoBERTa on Somali and Swahili. 3886 5 Results and Discussion 6 We report Mean Average Precision (MAP) of"
2021.acl-long.300,W11-2123,0,0.217567,"l corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space. While our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired"
2021.acl-long.300,N19-1357,0,0.104894,"rnational Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevance (SECLR) model trained on this data for low-resource sentence selection tasks on text and speech. Additionally, (iii) we propose a rationale training secondary objective to further improve SECLR performance, which we call SECLR-RT. Finally, (iv) we conduct training data ablation and h"
2021.acl-long.300,D18-1330,0,0.0676993,"Missing"
2021.acl-long.300,W18-6478,0,0.0350646,"Missing"
2021.acl-long.300,kamholz-etal-2014-panlex,0,0.0210779,"none of the translations of the query in the matrix are present in the source sentence. 4 Experiments 4.1 Dataset Generation from Parallel Corpus The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A. 1 https://www.iarpa.gov/index.php/ research-programs/material 2 https://dumps.wikimedia.org/ 3884 # sents. EN tkn. LR tkn. EN-SO EN-SW EN-TL 69,818 1,827,826 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tokens (Somali, Swahili, Tagalog) in the parallel corpus."
2021.acl-long.300,P07-2045,0,0.0263233,"these noisy parallel corpora. With this synthetic training set in hand, we can learn a supervised cross-lingual embedding space. While our approach is competitive with pipelines of MT-IR, it is still sensitive to noise in the parallel sentence data. We can mitigate the negative effects of this noise if we first train a phrase-based statistical MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale tr"
2021.acl-long.300,E17-1072,0,0.303623,"allel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and"
2021.acl-long.300,W15-1521,0,0.157957,"MT (SMT) model on the same parallel sentence corpus and use the extracted word alignments as additional supervision. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with bin"
2021.acl-long.300,J03-1002,0,0.0454723,"n task: Lrel = − log p(r|q, S; W ) 3.3 Guided Alignment with Rationale Training We can improve SECLR by incorporating additional alignment information as a secondary training objective, yielding SECLR-RT. Our intuition is that after training, the word sˆ = arg maxs∈S ws |wq should correspond to a translation of q. However, it is possible that sˆ simply co-occurs frequently with the true translation in our parallel data but its association is coincidental or irrelevant outside the training contexts. We use alignment information to correct for this. We run two SMT word alignment models, GIZA++ (Och and Ney, 2003) and Berkeley Aligner (Haghighi et al., 2009), on the orginal parallel sentence corpus. The two resulting alignments are concatenated as in Zbib et al. (2019) to estimate a unidirectional probabilistic word translation matrix A ∈ [0, 1]|VQ |×|VS |, such that A maps each word in the query language vocabulary to a list of document language words with different probabilities, i.e. P Aq,s is the probability of translating q to s and s∈VS Aq,s = 1. For each relevant training sample, i.e. (q, S, r = 1), we create a rationale distribution ρ ∈ [0, 1]|S| αs = P for s ∈ S. To encourage α to match ρ, we"
2021.acl-long.300,D17-1039,0,0.0752852,"Missing"
2021.acl-long.300,2020.clssts-1.1,0,0.0136766,"system development, and the latter being a larger evaluation corpus. In our main experiments we do not use Analysis or Dev for development and so we report results for all three (the ground truth relevance judgements for the TL Eval collection have not been released yet so we do not report Eval for TL). See Table 3 for evaluation statistics. All queries are text. The speech documents are first transcribed with an ASR system (Ragni and Gales, 2018), and the 1-best ASR output is used in the sentence selection task. Examples of the evaluation datasets are shown in Appendix B. We refer readers to Rubino (2020) for further details about MATERIAL test collections used in this work. While our model and baselines work at the sentence-level, the MATERIAL relevance judgements are only at the document level. Following previous work on evaluation of passage retrieval, we aggregate our sentence-level relevance scores to obtain document-level scores (Kaszkiel and ZoExperiment Settings Baselines Cross-Lingual Word Embeddings. We compare our model with three other cross-lingual embedding methods, Bivec (Luong et al., 2015), MUSE (Lample et al., 2018), and SID-SGNS (Levy et al., 2017). Bivec and SID-SGNS are tr"
2021.acl-long.300,tiedemann-2012-parallel,0,0.049924,"he translation matrix, and positive samples where none of the translations of the query in the matrix are present in the source sentence. 4 Experiments 4.1 Dataset Generation from Parallel Corpus The parallel sentence data for training our proposed method and all baselines includes the parallel data provided in the BUILD collections of both the MATERIAL1 and LORELEI (Christianson et al., 2018) programs for three low resource languages: Somali (SO), Swahili (SW), and Tagalog (TL) (each paired with English). Additionally, we include in our parallel corpus publicly available resources from OPUS (Tiedemann, 2012), and lexicons mined from Panlex (Kamholz et al., 2014) and Wiktionary.2 Statistics of these parallel corpora and augmented data are shown in Table 1 and Table 2, respectively. Other preprocessing details are in Appendix A. 1 https://www.iarpa.gov/index.php/ research-programs/material 2 https://dumps.wikimedia.org/ 3884 # sents. EN tkn. LR tkn. EN-SO EN-SW EN-TL 69,818 1,827,826 1,804,428 251,928 1,946,556 1,848,184 232,166 2,553,439 2,682,076 Table 1: Parallel corpus statistics; “EN tkn.” refers to number of English tokens in the parallel corpus; “LR tkn.” refers to number of low-resource tok"
2021.acl-long.300,W00-1312,0,0.613969,"on. With these alignment hints, we demonstrate consistent and significant improvements over neural and statistical MT+IR (Niu et al., 2018; Koehn et al., 2007; Heafield, 2011), 3881 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3881–3895 August 1–6, 2021. ©2021 Association for Computational Linguistics three strong cross-lingual embedding-based models (Bivec (Luong et al., 2015), SID-SGNS (Levy et al., 2017), MUSE (Lample et al., 2018)), a probabilistic occurrence model (Xu and Weischedel, 2000), and a multilingual pretrained model XLMRoBERTa (Conneau et al., 2020). We refer to this secondary training objective as rationale training, inspired by previous work in text classification that supervises attention over rationales for classification decisions (Jain and Wallace, 2019). To summarize, our contributions are as follows. We (i) propose a data augmentation and negative sampling scheme to create a synthetic training set of cross-lingual query-sentence pairs with binary relevance judgements, and (ii) demonstrate the effectiveness of a Supervised Embedding-based Cross-Lingual Relevanc"
2021.acl-long.536,N19-4009,0,0.0292469,"e - each summary sentence usually corresponds to an existing sentence in the input document. Evaluation metrics: We use the ROUGE (Lin, 2004) to measure general summarizaiton quality. For factual consistency, we use the QAGS protocol (see Appendix for more details) as well as the FactCC model (Kry´sci´nski et al., 2019) downloaded directly from the official website.4 In contrast to QAGS, FactCC is a BERT-based classification model that makes a binary prediction if the given claim sentence is factually consistent or not with the given input document. Implementation details: We use the Fairseq (Ott et al., 2019) implementation of BART-large (Lewis et al., 2019) for the summarization model as it is shown to achieve the state-of-the-art ROUGE scores for this task. We fine-tune the BART-large model with the standard learning rate of 3 × 10−5 4 https://github.com/salesforce/factCC Figure 3: Correlation between Q UALS and QAGS on XSUM (left) and CNNDM (right). The average QAGS tend to increase with the increase in Q UALS. on XSUM and CNNDM respectively to establish the MLE baselines. We then initialize C ON S EQ with the MLE baseline models. In C ON S EQ we use a learning rate of 3 × 10−6 . For evaluation"
2021.acl-long.536,2020.emnlp-main.439,1,0.828378,"Missing"
2021.acl-long.536,W17-2623,0,0.0355424,"Missing"
2021.acl-long.536,2020.acl-main.450,0,0.134957,"rnational Joint Conference on Natural Language Processing, pages 6881–6894 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 2: QAGen model: for an input text (p), it generates a question (q) followed by an answer (a). Figure 1: Comparison between QAGS (top) and Q UALS (bottom) protocols. Q UALS uses only one QAGen model instead of the AE, QG and QA models used in QAGS. factualness. Our main contributions lie in both areas. First, we propose an efficient automatic evaluation metric for factual consistency that is a simplification of the recently published QAGS protocol (Wang et al., 2020). Evaluating QAGS is computationally expensive and ill-suited for being part of the model training process. Our proposed protocol achieves a 55x speedup while correlating closely with QAGS1 . Second, we propose a new contrastive learning method that uses factualness as a training objective. We demonstrate through experiments that our method improves the factual consistency of summarization models measured by both automatic metrics such as QAGS as well as human evaluation. 2 An Efficient Metric for Factual Consistency In order to improve factual consistency of summarization models, we must have"
2021.acl-long.536,P18-2124,0,0.113053,"Missing"
2021.acl-long.536,D16-1264,0,0.104627,"Missing"
2021.acl-tutorials.2,P08-1090,0,0.0818516,"tical role in understanding events (Cybulska and Vossen, 2014). This part should last for 35 minutes. Background of Events and Their Representations [30min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic sali"
2021.acl-tutorials.2,D17-1168,1,0.911618,"alization, coreference resolution and prediction of events and their relations, (iii) induction of event processes and properties, and (iv) a wide range of NLP and commonsense understanding tasks that benefit from aforementioned techniques. We will conclude the tutorial by outlining emerging research problems in this area. 1 Introduction Human languages always involve the description of real-world events. Therefore, understanding events plays a critical role in NLP. For example, narrative prediction benefits from learning the causal relations of events to predict what happens next in a story (Chaturvedi et al., 2017a); machine comprehension of documents may involve understanding of events that affect the stock market (Ding et al., 2015), describe natural phenomena (Berant et al., 2014) or identify disease phenotypes (Zhang et al., 2020d). In fact, event understanding also widely finds its important use cases in tasks such as opendomain question answering (Yang et al., 2003), intent prediction (Rashkin et al., 2018), timeline construction (Do et al., 2012), text summarization (Daum´e III and Marcu, 2006) and misinformation detection (Fung et al., 2021). Since events are not just simple, standalone predica"
2021.acl-tutorials.2,2021.acl-long.133,1,0.81067,"Missing"
2021.acl-tutorials.2,glavas-etal-2014-hieve,0,0.0604228,"Missing"
2021.acl-tutorials.2,2020.conll-1.43,1,0.898616,"many efforts have been devoted into modeling event narratives (Peng et al., 2017; Chaturvedi et al., 2017b; Lee and Goldwasser, 2019) such that they can successfully predict missing events in an event process. Besides, another important event understanding angle is conceptualization (Zhang et al., 2020a), which aims at understanding the super-sub relations between a coarse-grained event and a fine-grained event process (Glavaˇs et al., 2014). In this context, the machine could also be expected to generate the event process given a goal (Zhang et al., 2020a), infer the goal given the process (Chen et al., 2020), and capture the recurrence of events in a process (Zhu et al., 2021). Last but not least, event coreference, which links references to the same event together, also plays a critical role in understanding events (Cybulska and Vossen, 2014). This part should last for 35 minutes. Background of Events and Their Representations [30min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk abou"
2021.acl-tutorials.2,P19-1433,0,0.0195631,"ng the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend informa"
2021.acl-tutorials.2,D15-1076,0,0.0239549,"min] We will start the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based enc"
2021.acl-tutorials.2,P18-1201,1,0.822982,"lined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress"
2021.acl-tutorials.2,P06-1039,0,0.0890017,"Missing"
2021.acl-tutorials.2,D18-1208,1,0.845181,"Missing"
2021.acl-tutorials.2,2020.acl-main.713,1,0.724423,"presentation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and expensive human annotations, which are not feasible on a large sca"
2021.acl-tutorials.2,P15-1155,1,0.800367,"movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present differen"
2021.acl-tutorials.2,Y18-1046,0,0.0145132,"ls (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and ex"
2021.acl-tutorials.2,W17-2701,0,0.0175787,"also interested in large-scale events that unfold over time. Over the past year, we saw many examples of such events, including COVID-19, the vaccine roll-out, the Black Lives Matter movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g.,"
2021.acl-tutorials.2,P19-1413,0,0.0244441,"to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress of events from different angles. For example, many efforts have been devoted into modeling event narratives (Peng et al., 2017; Chaturvedi et al., 2017b; Lee and Goldwasser, 2019) such that they can successfully predict missing events in an event process. Besides, another important event understanding angle is conceptualization (Zhang et al., 2020a), which aims at understanding the super-sub relations between a coarse-grained event and a fine-grained event process (Glavaˇs et al., 2014). In this context, the machine could also be expected to generate the event process given a goal (Zhang et al., 2020a), infer the goal given the process (Chen et al., 2020), and capture the recurrence of events in a process (Zhu et al., 2021). Last but not least, event coreference, which"
2021.acl-tutorials.2,P19-1500,0,0.0234852,"Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present different perspectives on the same event as well as identify salient information that is highlighted many in different ways across the set of input articles. This part is scheduled for 30 minutes • Emmon Bach. The algebra of events. Linguistics and philosophy. 9(1):5-16, 1986. • Nathanael Chambers. Event Schema Induction with a Probabilistic Entity-Driven Model. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process"
2021.acl-tutorials.2,2021.emnlp-main.422,1,0.760995,"tracted eventuality knowledge, we will explain how various prediction tasks, including the completion of an event complex, conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cros"
2021.acl-tutorials.2,N18-2089,0,0.0177113,"t the tutorial by introducing the essential background knowledge about events and their relations, including the definitions, categorizations, and applications (P. D. Mourelatos, 1978; Bach, 1986). In the last part of the introduction, we will talk about widely used event representation methods, including event schemas (Baker et al., 1998; Li et al., 2020b, 2021a), event knowledge graphs (Zhang et al., 2020c), event processes (Chambers and Jurafsky, 2008), event language models (Peng et al., 2017), and more recent work on event meaning representation via questionanswer pairs (He et al., 2015; Michael et al., 2018), event network embeddings (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for"
2021.acl-tutorials.2,2020.acl-main.230,1,0.883102,"Missing"
2021.acl-tutorials.2,D18-1483,0,0.0226446,"Missing"
2021.acl-tutorials.2,2020.emnlp-main.50,1,0.742388,"es, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequences. Specifically, people are trying to understand the progress of events from different angles. For example, many efforts have been devoted"
2021.acl-tutorials.2,C14-1114,0,0.0297635,"ents was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many articles covering the same event. By using multiple articles as input, a summarizer can present different perspectives on the same event as well as identify salient information that is highlighted many in different ways across the set of input articles. This part is scheduled for 30 minutes • Emmon Bach. The algebra of events. Li"
2021.acl-tutorials.2,P18-1122,1,0.835058,"conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motiv"
2021.acl-tutorials.2,2021.naacl-main.69,1,0.705823,"tracted eventuality knowledge, we will explain how various prediction tasks, including the completion of an event complex, conceptualization and consolidation of event processes, can be resolved. We will also discuss commonsense understanding of events, with a focus on the temporal and cognitive aspects. Moreover, we will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cros"
2021.acl-tutorials.2,D19-1405,0,0.0220875,"e will exemplify the use of aforementioned technologies in NLP applications of various domains, and will outline emerging research challenges that may catalyze further investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of prese"
2021.acl-tutorials.2,P18-1043,0,0.0258892,"Therefore, understanding events plays a critical role in NLP. For example, narrative prediction benefits from learning the causal relations of events to predict what happens next in a story (Chaturvedi et al., 2017a); machine comprehension of documents may involve understanding of events that affect the stock market (Ding et al., 2015), describe natural phenomena (Berant et al., 2014) or identify disease phenotypes (Zhang et al., 2020d). In fact, event understanding also widely finds its important use cases in tasks such as opendomain question answering (Yang et al., 2003), intent prediction (Rashkin et al., 2018), timeline construction (Do et al., 2012), text summarization (Daum´e III and Marcu, 2006) and misinformation detection (Fung et al., 2021). Since events are not just simple, standalone predicates, frontier research 2 Outline of Tutorial Content This half-day tutorial presents a systematic overview of recent advances in event-centric NLP technologies. We will begin with motivating this topic with several real-world applications, and introduce the main research problems. Then, we will introduce methods for automated extraction of events as well as their participants, properties and relations fr"
2021.acl-tutorials.2,2021.eacl-main.198,1,0.68319,"past year, we saw many examples of such events, including COVID-19, the vaccine roll-out, the Black Lives Matter movement and the US presidential election. In this tutorial, we will present methods for tracking such events over time and generating summaries that provide updates as an event unfolds. The task of identifying and tracking events was first introduced in the Topic Detection and Tracking challenge (Allan et al., 1998). Recent work has explored new methods for tracking and visualizing such events over time (e.g., (Laban and Hearst, 2017; Miranda et al., 2018; Staykovski et al., 2019; Saravanakumar et al., 2021)), in some cases generating summaries that contain information on what is new (e.g., (Kedzie et al., 2015, 2018)) and in other cases, exploring timeline summarization, ordering events and generating summaries that are placed along a timeline (e.g., (Wang et al., 2015; Binh Tran et al., 2013; Chen et al., 2019; Nguyen et al., 2014)) We will also consider how these are related to summarization of an event that takes place within a single day, a problem that falls within the category of multidocument summarization (e.g., (Liu and Lapata, 2019; Fabbri et al., 2019)), as typically there may be many"
2021.acl-tutorials.2,2021.naacl-main.4,1,0.738456,"gs (Zeng et al., 2021) and event time expression embeddings (Goyal and Durrett, 2019). This part is estimated to take 30 minutes. 2.3 Understanding Event Processes [35min] Event-centric Information Extraction [40min] 2.5 We will introduce unsupervised and zero-shot techniques for parsing the internal structures of verb and nominal events from natural language text, which also involves methods for automatic salient event detection (Liu et al., 2018), joint entity, relation and event extraction (Lin et al., 2020), and graph neural networks based encoding and decoding for information extraction (Zhang and Ji, 2021). Then we will discuss the recent research trend to extend information extraction from sentence-level Event-centric Commonsense Knowledge Acquisition [35min] Commonsense reasoning is a challenging yet important research problem in the AI community and one key challenge we are facing is the lack of satisfactory commonsense knowledge resources about events. Previous resources (Liu and Singh, 2004) typically require laborious and expensive human annotations, which are not feasible on a large scale. In this tutorial, we introduce recent 7 2.7 progress on modeling commonsense knowledge with high-or"
2021.acl-tutorials.2,D19-1030,1,0.89034,"Missing"
2021.acl-tutorials.2,2020.acl-main.678,1,0.834145,"d expensive human annotations, which are not feasible on a large scale. In this tutorial, we introduce recent 7 2.7 progress on modeling commonsense knowledge with high-order selectional preference over event knowledge and demonstrates that how to convert relatively cheap event knowledge, which can be easily acquired from raw documents with linguistic patterns, to precious commonsense knowledge defined in ConceptNet (Zhang et al., 2020b). Beyond that, we will also introduce how to automatically acquire other event-centric commonsense knowledge including but not limited to temporal properties (Zhou et al., 2020), intentions (Chen et al., 2020), effects (Sap et al., 2019) and graph schemas (Li et al., 2020c) of events. This part is estimated to be 35 minutes. 2.6 Emerging Research Problems [20min] Event-centric NLP impacts on a wide spectrum of knowledge-driven AI tasks, and is particularly knotted with commonsense understanding. We will conclude the tutorial using 20 minutes by presenting some challenges and potential research topics in applying eventuality knowledge in downstream tasks (e.g., reading comprehension, dialogue generation, and event timeline generation), and grounding eventuality knowle"
2021.acl-tutorials.2,2020.emnlp-main.51,1,0.784661,"er investigation on this topic. The detailed contents are outlined below. 2.1 to document-level (Du and Cardie, 2020; Li et al., 2021b). Besides, we will also discuss methods that identify temporal and causal relations of primitive events (Ning et al., 2018), and membership relations of multi-granular events (Aldawsari and Finlayson, 2019). Specifically, for data-driven extraction methods, we will present how constrained learning (Li et al., 2019) and structured prediction are incorporated to improve the tasks by enforcing logic consistency among different categories of event-event relations (Wang et al., 2020). We will also cover various cross-domain (Huang et al., 2018), cross-lingual (Subburathinam et al., 2019) and cross-media (Li et al., 2020a) structure transfer approaches for event extraction. This part is estimated to be 40 minutes. Motivation [20min] We will define the main research problem and motivate the topic by presenting several real-world applications based on event-centric NLP. This seeks to provide 20 minutes of presented content to motivate the main topic of this tutorial. 2.2 2.4 We will then present recent works on machine comprehension and prediction on event processes/sequence"
2021.acl-tutorials.2,N15-1112,0,0.062705,"Missing"
2021.eacl-main.184,J88-3010,0,0.214272,"Missing"
2021.eacl-main.184,W18-5507,0,0.0176706,"s to nouns within the context of certain verbs. Our work is unique in directly defining detailed aspects for nouns and adjectives. Early work on stance detection applied topicspecific models to various genres, including online debate forums (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Murakami and Putra, 2010; Hasan and Ng, 2013, 2014) and student essays (Faulkner, 2014). More recent studies have used a single model for many topics to predict stance in Tweets (Mohammad et al., 2016; Augenstein et al., 2016; Xu et al., 2018) and as part of the fact extraction and verification pipeline (Conforti et al., 2018; Ghanem et al., 2018; Riedel et al., 2017; Hanselowski et al., 2018). Klenner et al. (2017) explore the relationship between connotations and stance through verb frames. In contrast, our work studies stance using connotation representations from a learned joint embedding space for words from all parts of speech. Recently, Webson et al. (2020) examine representations of political 3 3.1 Connotation Lexicon Definitions We use w to indicate a word and w0 to indicate the person, thing or attribute signified by w. For each w, we define (1) Social Value: whether w0 is considered valuable by society,"
2021.eacl-main.184,P13-1025,0,0.0262152,". More recently ‘power’ and ‘agency’, components of Social Value, have been defined for verbs in connotation frames and for nouns in context (Field et al., 2019) and have been used to analyze bias and framing in a variety of texts, illustrating the applications and importance of Social Value in connotations. (2) Politeness follows the definition of Lakoff (1973) in noting words that make the addressee feel good but also includes notions of formality. These notions have been previously studied within the context of politeness as a set of behaviors and linguistic cues (Brown and Levinson, 1987; Danescu-Niculescu-Mizil et al., 2013; Aubakirova and Bansal, 2016). We focus on purely lexical distinctions because how one comprehends these distinctions affects one’s “attitude towards the speaker ... or some issue” as well as whether one feels insulted by the exchange (Colston and Katz, 2005). This aspect of perspective is a component of verb connotation frames and we extend it to nouns and adjectives in our lexicon through Politeness. (3) Impact and effect have been studied in verb connotation frames and other verb lexica (Choi and Wiebe, 2014), capturing notions of implicit benefit or harm on the arguments of the verb. We e"
2021.eacl-main.184,C18-1158,0,0.0132845,"e in directly defining detailed aspects for nouns and adjectives. Early work on stance detection applied topicspecific models to various genres, including online debate forums (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Murakami and Putra, 2010; Hasan and Ng, 2013, 2014) and student essays (Faulkner, 2014). More recent studies have used a single model for many topics to predict stance in Tweets (Mohammad et al., 2016; Augenstein et al., 2016; Xu et al., 2018) and as part of the fact extraction and verification pipeline (Conforti et al., 2018; Ghanem et al., 2018; Riedel et al., 2017; Hanselowski et al., 2018). Klenner et al. (2017) explore the relationship between connotations and stance through verb frames. In contrast, our work studies stance using connotation representations from a learned joint embedding space for words from all parts of speech. Recently, Webson et al. (2020) examine representations of political 3 3.1 Connotation Lexicon Definitions We use w to indicate a word and w0 to indicate the person, thing or attribute signified by w. For each w, we define (1) Social Value: whether w0 is considered valuable by society, (2) Politeness (Polite): whether w is a socially polite term, (3) Im"
2021.eacl-main.184,N19-1142,0,0.0275668,"urakami and Putra, 2010). For example, the sentence “the people opposed gun control” conveys no information about the author’s opinion. However, by adding just one word, “the selfish people opposed gun control”, the author can convey their stance on both gun control (against) and the people who support it (not valuable and disliked). Discerning such subtle meaning is crucial for fully understanding and recognizing the hidden influences behind everyday content. Recent studies in NLP have begun to examine these hidden influences through framing in social media and news (Asur and Huberman, 2010; Hartmann et al., 2019; Klenner, 2017) and style detection in hyperpartisan news (Potthast et al., 2018). Lexical connotations provide a method to study these influences, including stance, in more detail. Connotations are implied cultural and emotional associations for words that augment their literal meanings (Carpuat, 2015; Feng et al., 2011). Connotation values are associated with a phrase (e.g., fear is associated with “cancer”) (Feng et al., 2011) and capture a range of nuances, such as whether a phrase is an insult or implies value (see Figure 1). In this paper, we define six new fine-grained connotation aspe"
2021.eacl-main.184,P14-2050,0,0.102212,"Missing"
2021.eacl-main.184,I13-1191,0,0.027445,"ts for verbs (Rashkin et al., 2016, 2017; Sap et al., 2017; Klenner, 2017) or single polarities for many parts of speech (Feng et al., 2011, 2013; Kang et al., 2014). One exception is the work of Field et al. (2019), which extends limited detailed connotation dimensions from verbs to nouns within the context of certain verbs. Our work is unique in directly defining detailed aspects for nouns and adjectives. Early work on stance detection applied topicspecific models to various genres, including online debate forums (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Murakami and Putra, 2010; Hasan and Ng, 2013, 2014) and student essays (Faulkner, 2014). More recent studies have used a single model for many topics to predict stance in Tweets (Mohammad et al., 2016; Augenstein et al., 2016; Xu et al., 2018) and as part of the fact extraction and verification pipeline (Conforti et al., 2018; Ghanem et al., 2018; Riedel et al., 2017; Hanselowski et al., 2018). Klenner et al. (2017) explore the relationship between connotations and stance through verb frames. In contrast, our work studies stance using connotation representations from a learned joint embedding space for words from all parts of speech. Re"
2021.eacl-main.184,N19-1065,0,0.0581789,"Missing"
2021.eacl-main.184,P14-1145,0,0.384615,"cultural and emotional associations for words that augment their literal meanings (Carpuat, 2015; Feng et al., 2011). Connotation values are associated with a phrase (e.g., fear is associated with “cancer”) (Feng et al., 2011) and capture a range of nuances, such as whether a phrase is an insult or implies value (see Figure 1). In this paper, we define six new fine-grained connotation aspects for nouns and adjectives, filling a gap in the literature on connotation lexica, which has focused on verbs (Sap et al., 2017; Rashkin et al., 2016, 2017), and coarse-grained polarity (Feng et al., 2011; Kang et al., 2014). We create a new distantly labeled English lexicon that maps nouns and adjectives to our six aspects and show that it aligns well with human judgments. In addition, we show that our lexicon confirms existing hypotheses about subtle semantic differences between synonyms. We then learn a single connotation embedding space for words from all parts of speech, combining our lexicon with existing verb lexica and contributing to the literature on unifying lexica (Hoyle et al., 2019). Intrinsic evaluation shows that our embedding space captures clusters of connotativelysimilar words. In addition, our"
2021.eacl-main.184,S13-2053,0,0.266956,"’s, and reader’s perspectives, effect, value, and mental state) in connotation frames (e.g., “suffer” ; negative effect on the agent) and Sap et al. (2017) extend these aspects to include power and agency. We first define the six new aspects of connotation for nouns and adjectives (§3.1) in our work, then we describe our distant labeling procedure (§3.2) and human evaluation of the final lexicon (§3.3). Related Work Studies of connotation build upon the literature examining subtle language nuances, including good and bad effects of verbs (Choi and Wiebe, 2014), evoked sentiments and emotions (Mohammad et al., 2013a; Mohammad and Turney, 2010; Mohammad, 2018b), multi-dimensional sentiment (Whissell, 2009; Mohammad, 2018a; Whissell, 1989), offensiveness (Klenner et al., 2018), and psychosociological properties of words (Stone and Hunt, 1963; Tausczik and Pennebaker, 2009). Work explicitly on connotations has focused primarily on detailed aspects for verbs (Rashkin et al., 2016, 2017; Sap et al., 2017; Klenner, 2017) or single polarities for many parts of speech (Feng et al., 2011, 2013; Kang et al., 2014). One exception is the work of Field et al. (2019), which extends limited detailed connotation dimens"
2021.eacl-main.184,P18-1017,0,0.0163572,"ental state) in connotation frames (e.g., “suffer” ; negative effect on the agent) and Sap et al. (2017) extend these aspects to include power and agency. We first define the six new aspects of connotation for nouns and adjectives (§3.1) in our work, then we describe our distant labeling procedure (§3.2) and human evaluation of the final lexicon (§3.3). Related Work Studies of connotation build upon the literature examining subtle language nuances, including good and bad effects of verbs (Choi and Wiebe, 2014), evoked sentiments and emotions (Mohammad et al., 2013a; Mohammad and Turney, 2010; Mohammad, 2018b), multi-dimensional sentiment (Whissell, 2009; Mohammad, 2018a; Whissell, 1989), offensiveness (Klenner et al., 2018), and psychosociological properties of words (Stone and Hunt, 1963; Tausczik and Pennebaker, 2009). Work explicitly on connotations has focused primarily on detailed aspects for verbs (Rashkin et al., 2016, 2017; Sap et al., 2017; Klenner, 2017) or single polarities for many parts of speech (Feng et al., 2011, 2013; Kang et al., 2014). One exception is the work of Field et al. (2019), which extends limited detailed connotation dimensions from verbs to nouns within the context"
2021.eacl-main.184,W10-0214,0,0.318473,"bedding space and show that using the embeddings provides a statistically significant improvement on the task of stance detection when data is limited. 1 Stance: against gun control Selfish people support gun control. negative impact not tangible Figure 1: Connotations of the word “selfish” and the resulting implied stance on the topic “gun control”. Introduction Expressions of ideological attitudes are widespread in today’s online world, influencing how we perceive and react to events and people on a daily basis. These attitudes are often expressed through subtle expressions or associations (Somasundaran and Wiebe, 2010; Murakami and Putra, 2010). For example, the sentence “the people opposed gun control” conveys no information about the author’s opinion. However, by adding just one word, “the selfish people opposed gun control”, the author can convey their stance on both gun control (against) and the people who support it (not valuable and disliked). Discerning such subtle meaning is crucial for fully understanding and recognizing the hidden influences behind everyday content. Recent studies in NLP have begun to examine these hidden influences through framing in social media and news (Asur and Huberman, 201"
2021.eacl-main.184,P15-1012,0,0.0219486,"2009). Work explicitly on connotations has focused primarily on detailed aspects for verbs (Rashkin et al., 2016, 2017; Sap et al., 2017; Klenner, 2017) or single polarities for many parts of speech (Feng et al., 2011, 2013; Kang et al., 2014). One exception is the work of Field et al. (2019), which extends limited detailed connotation dimensions from verbs to nouns within the context of certain verbs. Our work is unique in directly defining detailed aspects for nouns and adjectives. Early work on stance detection applied topicspecific models to various genres, including online debate forums (Sridhar et al., 2015; Somasundaran and Wiebe, 2010; Murakami and Putra, 2010; Hasan and Ng, 2013, 2014) and student essays (Faulkner, 2014). More recent studies have used a single model for many topics to predict stance in Tweets (Mohammad et al., 2016; Augenstein et al., 2016; Xu et al., 2018) and as part of the fact extraction and verification pipeline (Conforti et al., 2018; Ghanem et al., 2018; Riedel et al., 2017; Hanselowski et al., 2018). Klenner et al. (2017) explore the relationship between connotations and stance through verb frames. In contrast, our work studies stance using connotation representations"
2021.eacl-main.198,W06-0901,0,0.0270483,"r time. This goal of identifying and tracking topics from a news stream was first introduced in the Topic Detection and Tracking (TDT) task (Allan et al., 1998). Topics in the news stream setting usually correspond to real-world events, while news articles may also be categorized thematically into ∗ sports, politics, etc. We focus on the task of clustering news on the basis of event-based story chains. We make a distinction between our definition of an event topic, which follows TDT and refers to large-scale real-world events, and the fine-grained events used in trigger-based event detection (Ahn, 2006). Given the non-parametric nature of our task (the number of events is not known beforehand and evolves over time), the two primary approaches have been topic modeling using Hierarchical Dirichlet Processes (HDPs) (Teh et al., 2005; Beykikhoshk et al., 2018) and Stream Clustering (MacQueen, 1967; Laban and Hearst, 2017; Miranda et al., 2018). While HDPs use word distributions within documents to infer topics, stream clustering models use representation strategies to encode and cluster documents. Contemporary models have adopted stream clustering using TF-IDF weighted bag of words representatio"
2021.eacl-main.198,N19-1423,0,0.17652,"n, 1967; Laban and Hearst, 2017; Miranda et al., 2018). While HDPs use word distributions within documents to infer topics, stream clustering models use representation strategies to encode and cluster documents. Contemporary models have adopted stream clustering using TF-IDF weighted bag of words representations to achieve state-of-the-art results (Staykovski et al., 2019). In this paper, we present a model for event topic detection and tracking from news streams that leverages a combination of dense and sparse document representations. Our dense representations are obtained from BERT models (Devlin et al., 2019) finetuned using the triplet network architecture (Hoffer and Ailon, 2015) on the event similarity task, which we describe in Section 3. We also use an adaptation of the triplet loss to learn a Support Vector Machine (SVM) (Boser et al., 1992) based document-cluster similarity model and handle the non-parametric cluster creation using a shallow neural network. We empirically show consistent improvement in clustering performance across many clustering metrics and significantly less cluster fragmentation. The main contributions of this paper are: Work done during internship at Amazon • We presen"
2021.eacl-main.198,N18-1098,0,0.0464114,"Missing"
2021.eacl-main.198,W17-2701,0,0.236159,"politics, etc. We focus on the task of clustering news on the basis of event-based story chains. We make a distinction between our definition of an event topic, which follows TDT and refers to large-scale real-world events, and the fine-grained events used in trigger-based event detection (Ahn, 2006). Given the non-parametric nature of our task (the number of events is not known beforehand and evolves over time), the two primary approaches have been topic modeling using Hierarchical Dirichlet Processes (HDPs) (Teh et al., 2005; Beykikhoshk et al., 2018) and Stream Clustering (MacQueen, 1967; Laban and Hearst, 2017; Miranda et al., 2018). While HDPs use word distributions within documents to infer topics, stream clustering models use representation strategies to encode and cluster documents. Contemporary models have adopted stream clustering using TF-IDF weighted bag of words representations to achieve state-of-the-art results (Staykovski et al., 2019). In this paper, we present a model for event topic detection and tracking from news streams that leverages a combination of dense and sparse document representations. Our dense representations are obtained from BERT models (Devlin et al., 2019) finetuned"
2021.eacl-main.198,P19-1335,0,0.019525,"similarity = 1), while those from different events are dissimilar (with similarity = 0). Given the embeddings of an anchor document da , a positive document dp (from the same event as the anchor) and a negative document dn (from a different event), triplet loss is computed as ltriplet = sim(da , dn ) − sim(da , dp ) + m (1) where sim is the cosine similarity function and m is the hyper-parameter margin. Providing External Entity Knowledge In line with TDT’s definition, entities are central to events and thus need to be highlighted in document representations for our clustering task. We follow Logeswaran et al. (2019) to introduce entity awareness to BERT by leveraging knowledge from an external NER system. Apart from token, position and token type embeddings, we also add an entity presence-absence embedding for each token depending on whether it corresponds to an entity or not. The entity aware BERT model architecture is shown in Figure 2. This enhanced entity-aware model can then be coupled with the event similarity (E-S-BERT) objective for fine-tuning. 3.1.3 Temporal Representation Documents are also represented with the timestamp of publication. Unlike TF-IDF and dense embeddings, which are vector valu"
2021.eacl-main.198,H05-1004,0,0.104726,"hile mistakes on smaller clusters can fall through without incurring much penalty. In our experiments, we observed that this property of the metric prevents it from capturing cluster fragmentation errors on smaller events. In the news stream clustering setting, small events may correspond to recent salient events and thus, we want our metric 6 The mean and standard deviation of the cluster count over five independent training and evaluations of our model are 312 ± 27. 2337 to be agnostic to the size of the clusters. We thus use an additional metric that weights every cluster equally - CEAF-e (Luo, 2005). The CEAF-e metric creates a one-to-one mapping between the clustering output and gold clusters using the Kuhn-Munkres algorithm. The similarity between a gold cluster G and an output cluster O is computed as the fraction of articles that are common to the clusters. Once the clusters are aligned, precision and recall are computed using the aligned pairs of clusters. This ensures that unaligned clusters contribute to a penalty in the score and cluster fragmentation and coalescing is captured by the metric. In order to ensure that our model’s better performance is metric-agnostic, we also empir"
2021.eacl-main.198,D18-1483,0,0.115479,"Missing"
2021.eacl-main.198,D15-1225,0,0.0160655,"and cluster creation. Similarity between a document and cluster is computed along multiple document representations and then aggregated using a Rank-SVM model (Joachims, 2002). The decision to merge a document with a cluster or create a new cluster is taken by an SVM classifier. Our model also follows this architecture, but critically adds dense document representations, an SVM trained on the adapted triplet loss for aggregating document-cluster similarities and a shallow neural network for cluster creation. News event tracking has also been framed as a non-parametric topic modeling problem (Zhou et al., 2015) and HDPs that share parameters across temporal batches have been used for this task (Beykikhoshk et al., 2018). Dense document representations have been shown to be useful in the parametric variant of our problem, with neural LDA (Dieng et al., 2019a; Keya et al., 2019; Dieng et al., 2019b; Bianchi et al., 2020), temporal topic evolution models (Zaheer et al., 2017; Gupta et al., 2018; Zaheer et al., 2019; Brochier et al., 2020) and embedding space clustering (Momeni et al., 2018; Sia et al., 2020) being some prominent approaches in the literature. 2331 Figure 1: The architecture of the news"
2021.eacl-main.198,D19-1410,0,0.0276536,"sify a document into one of the events in the output space. Fine-tuning on Event Similarity Fine-tuning on the task of event classification constrains the embedding of documents corresponding to different events to be non-linearly separable. Semantics about events can be better captured if the vector similarity between document embeddings encode whether they are from the same event or not. For this, we adapt the triplet network architecture (Hoffer and Ailon, 2015) and fine-tune on the task of event similarity. Triplet BERT networks were introduced for the semantic text similarity (STS) task (Reimers and Gurevych, 2019), where the vector similarity between sentence embeddings was tuned to reflect the semantic similarity between them. We formulate the event similarity task, where the term “similarity” refers to whether two documents are from the same event cluster or not. In our task, documents from the same event are similar (with similarity = 1), while those from different events are dissimilar (with similarity = 0). Given the embeddings of an anchor document da , a positive document dp (from the same event as the anchor) and a negative document dn (from a different event), triplet loss is computed as ltrip"
2021.eacl-main.235,N19-1423,0,0.0139778,"opose a set of simple metrics to quantify factual consistency at the entitylevel. We analyze the factual quality of summaries produced by the state-of-the-art BART model (Lewis et al., 2019) on three news datasets. We then propose several techniques including data filtering, multi-task learning and joint sequence generation to improve performance on these metrics. We leave the relation level consistency to future work. 2 Related work Large transformer-based neural architectures combined with pre-training have set new records across many natural language processing tasks (Vaswani et al., 2017; Devlin et al., 2019; 2727 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2727–2733 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Radford et al., 2019). In particular, the BART model (Lewis et al., 2019) has shown superior performance in many text generation tasks including abstractive summarization. In contrast to encoder-only pre-training such as in BERT (Devlin et al., 2019) or decoder-only pre-training such as in GPT-2 (Radford et al., 2019), BART is an encoder-decoder transformer-based neural translation model jointly"
2021.eacl-main.235,W18-2706,0,0.0232806,"uthors in (Kry´sci´nski et al., 2019) proposed to train a neural network model to classify if a summary is factually consistent with a given source document, similar to a natural language inference task. In the dialogue generation setting, authors in (Li et al., 2019) proposed using unlikelihood to surpress logically inconsistent responses. Our work is complementary to such existing approaches as we focus on simple entity-level metrics to quantify and improve factual consistency. Our goal of improving entity-level metrics of summaries is also related to controllable abstractive summarization (Fan et al., 2018), where a list of named-entities that a user wants to see in the summary can be passed as input to influence the generated summary. In contrast, our goal is to predict which entities are summary-worthy while generating the summary that contains them. In this view we are trying to solve a more challenging problem. 3 Entity-level factual consistency metrics We propose three new metrics that rely on off-theshelf tools to perform Named-Entity Recognition (NER). 1 We use N (t) and N (h) to denote the number of named-entities in the target (gold summary) and hypothesis (generated summary), respectiv"
2021.eacl-main.235,W19-8665,0,0.0630315,"Missing"
2021.eacl-main.235,N18-1065,0,0.0448655,"Missing"
2021.eacl-main.235,D19-1051,0,0.0373461,"Missing"
2021.eacl-main.235,2020.acl-main.703,0,0.032094,"Missing"
2021.eacl-main.235,K16-1028,1,0.883013,"Missing"
2021.eacl-main.235,D18-1206,0,0.045514,"Missing"
2021.eacl-main.235,N19-4009,0,0.0147707,"rthy named-entities, followed by a special token, and then the summary. We call this approach JAENS (Join sAlient ENtity and Summary generation). Similar to the multitask learning approach discussed earlier, JAENS encourages the model to jointly learn to identify the summary-worthy named-entities while learning to generate summaries. Since the decoder generates the salient named-entities first, the summaries that JAENS generate can further attend to these salient named-entities through decoder self-attention. 6 Experiment results We use the pre-trained BART-large model in the Fairseq library (Ott et al., 2019) to fine-tune on the 3 summarization datasets.3 The appendix contains additional details of experimental setup. In Table 3, we show the effect of the entitybased data filtering. For each dataset, we train two separate models: using the training data before and after entity-based data filtering as shown in Table 2. We evaluate both models on the “clean” test set after entity-based data 3 Our code is available at https://github.com/ amazon-research/fact-check-summarization 2729 train Newsroom val test train CNNDM val test train XSUM val test original 922,500 (1.58) 100,968 (1.60) 100,933 (1.59)"
2021.eacl-main.235,P19-1363,0,0.0279786,"the BART model (Lewis et al., 2019) has shown superior performance in many text generation tasks including abstractive summarization. In contrast to encoder-only pre-training such as in BERT (Devlin et al., 2019) or decoder-only pre-training such as in GPT-2 (Radford et al., 2019), BART is an encoder-decoder transformer-based neural translation model jointly pre-trained to reconstruct corrupted input sequences of text. Several authors have pointed out the problem of factual inconsistency in abstractive summarization models (Kryscinski et al., 2019; Kry´sci´nski et al., 2019; Cao et al., 2018; Welleck et al., 2019). The authors in (Kry´sci´nski et al., 2019) proposed to train a neural network model to classify if a summary is factually consistent with a given source document, similar to a natural language inference task. In the dialogue generation setting, authors in (Li et al., 2019) proposed using unlikelihood to surpress logically inconsistent responses. Our work is complementary to such existing approaches as we focus on simple entity-level metrics to quantify and improve factual consistency. Our goal of improving entity-level metrics of summaries is also related to controllable abstractive summariz"
2021.eacl-main.235,2020.emnlp-demos.6,0,0.0351401,"Missing"
2021.eacl-main.248,2020.iwslt-1.3,0,0.0132603,"ed data. To our knowledge, such datasets do not exist for the languages we are interested in. Wan et al. (2020) develop a segmentation model in our setting using subtitles; however, they do not take into account explicit modeling of segmentation errors and show only minimal and intermittent improvements in downstream tasks. Recent work has increasingly focused on endto-end models of SLT in a high-resource setting, since these systems reduce error propagation and latency when compared to cascaded approaches (Weiss et al., 2017; Cross Vila et al., 2018; Sperber et al., 2019; Gaido et al., 2020; Bahar et al., 2020; Lakumarapu et al., 2020). In spite of these advantages, end-to-end systems have only very recently achieved competitive results due to the limited amount of parallel data for speech translation as compared to the data that is available to train ASR systems and translation systems separately (Gaido et al., 2020; Ansari et al., 2020). 3 Problem Definition We treat the ASR acoustic segmentation problem as a sequence tagging problem (Stolcke and Shriberg, 1996). Unlike a typical tagging problem, which aims to tag a single input sequence, our input is a pair of aligned sequences of n items, x = ["
2021.eacl-main.248,2012.eamt-1.60,0,0.0173392,"07; Rao et al., 2007) and part-of-speech features derived from a fixed window size (Rangarajan Sridhar et al., 2013). Other work has modeled the problem using hidden markov models (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen et al., 2001; Kim and Woodland, 2001) and conditional random fields (Liu et al., 2005; Lu and Ng, 2010). More recent segmentation work uses neural architectures, such as LSTM (Sperber et al., 2018) and Transformer models (Pham et al., 2019). These models benefit from the large training data available for high-resource languages. For example, the TED corpus (Cettolo et al., 2012) for SLT from English to German includes about 340 hours of welltranscribed data. To our knowledge, such datasets do not exist for the languages we are interested in. Wan et al. (2020) develop a segmentation model in our setting using subtitles; however, they do not take into account explicit modeling of segmentation errors and show only minimal and intermittent improvements in downstream tasks. Recent work has increasingly focused on endto-end models of SLT in a high-resource setting, since these systems reduce error propagation and latency when compared to cascaded approaches (Weiss et al.,"
2021.eacl-main.248,2020.iwslt-1.8,0,0.0631458,"Missing"
2021.eacl-main.248,W18-1820,0,0.0264012,"entation corrections produced by our model, we re-segment the ASR output tokens and hand the resulting segments off to the MT component where they are individually translated. 7.2.3 MT Systems We evaluate with three different MT systems. We use the neural MT model developed by the University of Edinburgh (EDI-NMT) and the neural and phrase-based statistical MT systems from the University of Maryland (UMD-NMT and UMD-SMT, respectively). The EDI-NMT and UMD-NMT systems are Transformer-based models (Vaswani et al., 2017) trained using the Marian Toolkit (JunczysDowmunt et al., 2018) and Sockeye (Hieber et al., 2018), respectively. UMD-NMT trains a single model for both directions of a language pair (Niu et al., 2018), while EDI-NMT has a separate model for each direction. UMD-SMT is trained using the Moses SMT Toolkit (Koehn et al., 2003), where the weights were optimized using MERT (Och, 2003). 7.2.4 IR System For the IR system, we use the bag-of-words language model implemented in Indri (Strohman et al., 2005). Documents and queries are both tokenized and normalized on the character level to avoid potential mismatch in the vocabulary. The queries are relatively short, typically consisting of only a few"
2021.eacl-main.248,P07-2045,0,0.0348514,"speech from multiple low-resource languages. Some speech documents have two speakers, with each speaker on a separate channel, i.e., completely isolated from the other speaker. When performing segmentation we treat each channel independently, creating a separate (resegmented) ASR output for each channel. To create the document transcript for MT, we merge the two 3 Experiments The official MATERIAL collections are named ANALYSIS+DEV and EVAL, but we refer to them as Test (Small) and Test (Large) to avoid confusion. Segmentation Model Training For all datasets, we tokenize all data with Moses (Koehn et al., 2007). To improve performance on out of vocabulary words, we use Byte-PairEncoding (Sennrich et al., 2016) with 32,000 merge operations to create subwords for each language. We then train the segmentation model on the subtitle dataset. When creating γ sequences on the subtitles data, we set under- and over-segmentation noise to α ˇ = 0.25 and α ˆ = 0.25 respectively.4 We use the Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.001. We use early stopping on the validation loss of the OpenSubtitles validation set to select the best stopping epoch for the segmentation model. We further fin"
2021.eacl-main.248,N03-1017,0,0.192442,"Missing"
2021.eacl-main.248,L16-1147,0,0.0613786,"Missing"
2021.eacl-main.248,P05-1056,0,0.0854126,"omplex segments. 2 Related Work Segmentation in SLT has been studied quite extensively in high-resource settings. Early work used kernel-based SVM models to predict sentence boundaries using language model probabilities along with prosodic features such as pause duration (Matusov et al., 2007; Rao et al., 2007) and part-of-speech features derived from a fixed window size (Rangarajan Sridhar et al., 2013). Other work has modeled the problem using hidden markov models (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen et al., 2001; Kim and Woodland, 2001) and conditional random fields (Liu et al., 2005; Lu and Ng, 2010). More recent segmentation work uses neural architectures, such as LSTM (Sperber et al., 2018) and Transformer models (Pham et al., 2019). These models benefit from the large training data available for high-resource languages. For example, the TED corpus (Cettolo et al., 2012) for SLT from English to German includes about 340 hours of welltranscribed data. To our knowledge, such datasets do not exist for the languages we are interested in. Wan et al. (2020) develop a segmentation model in our setting using subtitles; however, they do not take into account explicit modeling o"
2021.eacl-main.248,W18-2709,0,0.0306382,"Missing"
2021.eacl-main.248,D10-1018,0,0.0266873,"Related Work Segmentation in SLT has been studied quite extensively in high-resource settings. Early work used kernel-based SVM models to predict sentence boundaries using language model probabilities along with prosodic features such as pause duration (Matusov et al., 2007; Rao et al., 2007) and part-of-speech features derived from a fixed window size (Rangarajan Sridhar et al., 2013). Other work has modeled the problem using hidden markov models (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen et al., 2001; Kim and Woodland, 2001) and conditional random fields (Liu et al., 2005; Lu and Ng, 2010). More recent segmentation work uses neural architectures, such as LSTM (Sperber et al., 2018) and Transformer models (Pham et al., 2019). These models benefit from the large training data available for high-resource languages. For example, the TED corpus (Cettolo et al., 2012) for SLT from English to German includes about 340 hours of welltranscribed data. To our knowledge, such datasets do not exist for the languages we are interested in. Wan et al. (2020) develop a segmentation model in our setting using subtitles; however, they do not take into account explicit modeling of segmentation err"
2021.eacl-main.248,L18-1008,0,0.032774,"gmentation sequence γ and produce the corrected sequence y. 5 Model We employ a Long Short-Term Memory (LSTM)based model architecture for this task (Hochreiter and Schmidhuber, 1997). Given an input sequence of ASR tokens x = [x1 , . . . , xn ] along with corresponding ASR segmentation sequence 2844 γ = [γ1 , . . . , γn ], we first get an embedding representation ei ∈ R316 for each token as follows: ei = G(xi ) ⊕ F (γi ) where G ∈ R|V|×300 and F ∈ R2×16 are embedding lookup tables, and ⊕ is the concatenation operator. We initialized G with FastText embeddings pre-trained on Common Crawl data (Mikolov et al., 2018). F is randomly initialized. We pass the embedding sequence through a twolayer bi-directional LSTM, with 512 hidden units each, to get the contextual representation hi ∈ R1024 for each token as follows: −−−−→ ←−−−− hi = LSTM(ei ) ⊕ LSTM(ei ) −−−−→ ←−−−− where LSTM and LSTM are the forward direction and backward direction LSTMs respectively. Each output state hi is then passed through a linear projection layer with a logistic sigmoid to compute the probability of a segment boundary p(yi = 1|hi ; θ). The log-likelihood of a corrected segmentation Pn boundary sequence is log p(y|x, γ; θ) = i=1 lo"
2021.eacl-main.248,P03-1021,0,0.0484756,"Missing"
2021.eacl-main.248,P02-1040,0,0.109118,"., 2005). Documents and queries are both tokenized and normalized on the character level to avoid potential mismatch in the vocabulary. The queries are relatively short, typically consisting of only a few words, and they define two types of relevancy – the conceptual queries require the relevant documents to be topically relevant to the query, while the simple queries require the relevant document to contain the translation of the query. However, no specific processing is used for these two relevance types in our experiments. 7.3 MT Evaluation Our first extrinsic evaluation measures the BLEU (Papineni et al., 2002) score of the MT output on the Test (Small) sets after running our segmentation correction model, where we have ground truth reference English translations. We refer to our model trained only on the BUILD data as Seg, and our subtitle-trained model as Seg + Sub. As our baseline, we compare the same pipeline using the segmentation produced by the acoustic model of the ASR system, denoted Acous. Since each segmentation model produces segments with different boundaries, we are unable to use BLEU directly to compare to the reference sentences. Therefore, we concatenate all segments of a document a"
2021.eacl-main.248,W18-6319,0,0.0260018,"Missing"
2021.eacl-main.248,N13-1023,0,0.0273217,"l tagging model for correcting ASR acoustic segmentation before use in an MT pipeline. (iv) Finally, we show downstream performance increases on MT and document-level CLIR tasks, especially for more syntactically complex segments. 2 Related Work Segmentation in SLT has been studied quite extensively in high-resource settings. Early work used kernel-based SVM models to predict sentence boundaries using language model probabilities along with prosodic features such as pause duration (Matusov et al., 2007; Rao et al., 2007) and part-of-speech features derived from a fixed window size (Rangarajan Sridhar et al., 2013). Other work has modeled the problem using hidden markov models (Shriberg et al., 2000; Gotoh and Renals, 2000; Christensen et al., 2001; Kim and Woodland, 2001) and conditional random fields (Liu et al., 2005; Lu and Ng, 2010). More recent segmentation work uses neural architectures, such as LSTM (Sperber et al., 2018) and Transformer models (Pham et al., 2019). These models benefit from the large training data available for high-resource languages. For example, the TED corpus (Cettolo et al., 2012) for SLT from English to German includes about 340 hours of welltranscribed data. To our knowle"
2021.eacl-main.248,W19-6101,0,0.0408841,"Missing"
2021.eacl-main.248,P16-1162,0,0.0320373,"peaker on a separate channel, i.e., completely isolated from the other speaker. When performing segmentation we treat each channel independently, creating a separate (resegmented) ASR output for each channel. To create the document transcript for MT, we merge the two 3 Experiments The official MATERIAL collections are named ANALYSIS+DEV and EVAL, but we refer to them as Test (Small) and Test (Large) to avoid confusion. Segmentation Model Training For all datasets, we tokenize all data with Moses (Koehn et al., 2007). To improve performance on out of vocabulary words, we use Byte-PairEncoding (Sennrich et al., 2016) with 32,000 merge operations to create subwords for each language. We then train the segmentation model on the subtitle dataset. When creating γ sequences on the subtitles data, we set under- and over-segmentation noise to α ˇ = 0.25 and α ˆ = 0.25 respectively.4 We use the Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.001. We use early stopping on the validation loss of the OpenSubtitles validation set to select the best stopping epoch for the segmentation model. We further fine-tune this model on the BUILD partition to expose the model to some in-domain training data. The dat"
2021.eacl-main.248,Q19-1020,0,0.0538976,"includes about 340 hours of welltranscribed data. To our knowledge, such datasets do not exist for the languages we are interested in. Wan et al. (2020) develop a segmentation model in our setting using subtitles; however, they do not take into account explicit modeling of segmentation errors and show only minimal and intermittent improvements in downstream tasks. Recent work has increasingly focused on endto-end models of SLT in a high-resource setting, since these systems reduce error propagation and latency when compared to cascaded approaches (Weiss et al., 2017; Cross Vila et al., 2018; Sperber et al., 2019; Gaido et al., 2020; Bahar et al., 2020; Lakumarapu et al., 2020). In spite of these advantages, end-to-end systems have only very recently achieved competitive results due to the limited amount of parallel data for speech translation as compared to the data that is available to train ASR systems and translation systems separately (Gaido et al., 2020; Ansari et al., 2020). 3 Problem Definition We treat the ASR acoustic segmentation problem as a sequence tagging problem (Stolcke and Shriberg, 1996). Unlike a typical tagging problem, which aims to tag a single input sequence, our input is a pai"
2021.eacl-main.248,2020.clssts-1.11,1,0.782061,"g et al., 2000; Gotoh and Renals, 2000; Christensen et al., 2001; Kim and Woodland, 2001) and conditional random fields (Liu et al., 2005; Lu and Ng, 2010). More recent segmentation work uses neural architectures, such as LSTM (Sperber et al., 2018) and Transformer models (Pham et al., 2019). These models benefit from the large training data available for high-resource languages. For example, the TED corpus (Cettolo et al., 2012) for SLT from English to German includes about 340 hours of welltranscribed data. To our knowledge, such datasets do not exist for the languages we are interested in. Wan et al. (2020) develop a segmentation model in our setting using subtitles; however, they do not take into account explicit modeling of segmentation errors and show only minimal and intermittent improvements in downstream tasks. Recent work has increasingly focused on endto-end models of SLT in a high-resource setting, since these systems reduce error propagation and latency when compared to cascaded approaches (Weiss et al., 2017; Cross Vila et al., 2018; Sperber et al., 2019; Gaido et al., 2020; Bahar et al., 2020; Lakumarapu et al., 2020). In spite of these advantages, end-to-end systems have only very r"
2021.eacl-main.248,2020.clssts-1.8,0,0.0451636,"Missing"
2021.emnlp-main.519,D19-5816,1,0.892822,"Missing"
2021.emnlp-main.519,P99-1071,1,0.465211,"is on the size of input event graph. 4 Related Work etc. However, they ignore relations between event arguments, or only use hierarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework."
2021.emnlp-main.519,D19-5602,0,0.0273129,"Missing"
2021.emnlp-main.519,N19-1240,0,0.0414547,"Missing"
2021.emnlp-main.519,N19-1423,0,0.177587,"ences measured by A time-aware optimal transport distance is the inter-similarity of these articles. In these meththen introduced for learning the compression ods, the document representations are limited to model in an unsupervised manner. We show local text features, ignoring the global context of that our approach significantly improves the the news collection. The applications of neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major"
2021.emnlp-main.519,P19-1259,0,0.0422027,"Missing"
2021.emnlp-main.519,P19-1102,0,0.084656,"e of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Processing (NLP) techniques: (1) In contrast to multi-document summarization (MDS) we construct a single event graph from the dealing with tens of documents (Fabbri et al., 2019), input documents, with co-referential entities (e.g., house, mansion in Figure 1) and co1 The programs, data and resources are publicly available referential events (e.g., die, collapsed) merged for research purpose in https://github.com/limanling/ event-graph-summarization. across documents. Our comprehensive event 6443 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6443–6456 c November 7–11, 2021. 2021 Association for Computational Linguistics Input Documents 2009-06-27 There was no sign of foul play in the death of Michael Jackson......A recor"
2021.emnlp-main.519,W04-1017,0,0.0911918,"ptimal transport (OT): We propose a new for- minimal distance with only m events to be kept, a global decision is learned to select salient but mulation of timeline summarization, by selecting also diverse events. The summary graphs are genevent nodes from the input graph to form a smaller summary graph. Under a certain summary size con- erated using a differentiable compression model according to a hyperparameter of compression rate, straint, a summary graph with high coverage has a instead of using annotated timelines. Thus, our small information loss, compared to the one with low coverage (Filatova and Hatzivassiloglou, 2004). objective allows model training in an end-to-end unsupervised way. We constrain the total number of event nodes to be kept in the summary, and optimize the summary (3) Time-aware Gromov-Wasserstein distance: graph to be close to the original graph using opti- The distance between two graphs should capture 6444 the following criteria: i) Semantic relevance: each node first has its initial local context encoded via a pre-trained BERT model and node type embeddings. For example, S TART P OSITION event is not closely related to the T RANSPORT event in Figure 1 though they have temporal dependenc"
2021.emnlp-main.519,C10-1039,0,0.0590717,"erarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timel"
2021.emnlp-main.519,N09-1041,0,0.0241803,"Work etc. However, they ignore relations between event arguments, or only use hierarchical or temporal relations to connect events. Also, cross-document entity coreference and event coreference resolution are critical for large corpora understanding, while previous work focuses on a single document. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracp"
2021.emnlp-main.519,2020.acl-main.457,1,0.840441,"is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timeline Summarization. Due to the lack of to hierarchically generate the timeline. training data, timeline summarization focuses on extractive methods with heuristics (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Ac"
2021.emnlp-main.519,2021.naacl-main.274,1,0.693645,"ion from a node to its mentions v An event node in an event graph e An entity node in an event graph hvi , vl i A temporal ordering edge (vl happens after vi ) hvi , a, ej i An argument edge (the entity ej plays argument role a in event vi ) hej , r, ek i An entity relation edge between ej and ek , and r is the relation type Table 1: List of symbols. We apply OneIE (Lin et al., 2020), a state-ofthe-art Information Extraction (IE) system, to extract entities, relations and events; then perform 2 Method cross-document entity and event coreference res2.1 Overview olution (Pan et al., 2015, 2017; Lai et al., 2021) Our approach aims at finding the graph that has over the document cluster of each timeline topic. minimal distance from the input graph (Filatova We apply (Ning et al., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the be"
2021.emnlp-main.519,2020.acl-main.230,1,0.885492,"Missing"
2021.emnlp-main.519,2020.emnlp-main.50,1,0.76894,"ral attribute accuracy if there is a tie. The events with temporal attributes extracted directly from the context are of highest priority, followed by events having temporal attributes propagated from neighbor events in §2.2, and then the ones using document publication date. 4 https://wwconw.voanews.com 5 https://www.reuters.com following §2.2. 6 We use the ACE event ontology7 , with 7 entity types, 6 relation types, 33 event types, and 22 argument roles. For the (unsupervised) training of our event graph compression model, we use event graphs constructed from VoA news between 2011 and 2017 (Li et al., 2020a). The statistics are shown in Table 2. Dataset Split #Doc #Event #Entity #Rel Timeline17 Input Timeline 4,650 19 74,320 115,585 136,509 974 1,936 1,134 Crisis Input 20,463 325,695 551,228 610,410 Timeline 22 736 1,184 1,309 Timeline100 Input 10,379 178,581 301,132 306,975 Timeline 100 3,296 8,901 23,732 Unlabeled Input 72,576 913,679 381,735 1,046,066 (for OT) Timeline - Table 2: Data statistics, including the number of documents, events, entities, and temporal relations. Evaluation Metrics. We use the conventional metrics for timeline summarization (Martschat and Markert, 2018) to evaluate"
2021.emnlp-main.519,2020.acl-main.713,1,0.86566,"2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Processing (NLP) techniques: (1) In contrast to multi-document summarization (MDS) we construct a single event graph from the dealing with tens of documents (Fabbri et al., 2019), input documents, with co-referential entities (e.g., house, mansion in Figure 1) and co1 The programs, data and resources are publicly available referential events (e.g., die, collapsed) merged for research purpose in https://github.com/limanling/ event-graph-summarization. across documents. Our comprehensive event 6443 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process"
2021.emnlp-main.519,P19-1500,0,0.0578843,"Missing"
2021.emnlp-main.519,P14-5010,0,0.00254608,"l., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the best transport plan that has a minimal distance event, We extract and normalize time expressions between two graphs. To apply optimal transport using publication date (Manning et al., 2014), and to timeline summarization, the key is to design the then apply (Wen et al., 2021) to extract the event distance to evaluate the information loss, and thus temporal attributes from the context. If the tem6445 poral attributes can not be decided according to the context, we propagate the temporal attributes from neighbor events based on their shared arguments (?). After that, we use the document publication date to populate the remaining missing dates. For example, in Figure 1, the date 2009-0625 of the collapse (D IE) event is extracted from context last Thursday, and the date of the unco"
2021.emnlp-main.519,K18-1023,0,0.0660956,"orld datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction (IE) systems (Lin et al., 2020), ral Language Pr"
2021.emnlp-main.519,C14-1114,0,0.0646958,"neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline summariza- (1) Event graph construction for multi-doc tion task poses several challenges to existing Natu- encoding: With state-of-the-art Information Extraction"
2021.emnlp-main.519,D19-1642,0,0.0443163,"Missing"
2021.emnlp-main.519,N15-1119,1,0.81622,"s type w A mapping function from a node to its mentions v An event node in an event graph e An entity node in an event graph hvi , vl i A temporal ordering edge (vl happens after vi ) hvi , a, ej i An argument edge (the entity ej plays argument role a in event vi ) hej , r, ek i An entity relation edge between ej and ek , and r is the relation type Table 1: List of symbols. We apply OneIE (Lin et al., 2020), a state-ofthe-art Information Extraction (IE) system, to extract entities, relations and events; then perform 2 Method cross-document entity and event coreference res2.1 Overview olution (Pan et al., 2015, 2017; Lai et al., 2021) Our approach aims at finding the graph that has over the document cluster of each timeline topic. minimal distance from the input graph (Filatova We apply (Ning et al., 2019) to extract temporal and Hatzivassiloglou, 2004), so that when only a relations for events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtai"
2021.emnlp-main.519,P17-1178,1,0.766016,"Missing"
2021.emnlp-main.519,2021.textgraphs-1.4,0,0.0817398,"Missing"
2021.emnlp-main.519,D19-5403,0,0.210679,"hey pendency across key stories, which, compared to determine the key dates of events. These standard MDS, poses additional challenges in remethods overlook the events’ intra-structures (arguments) and inter-structures (event-event constructing temporal order. (3) Manual labeling connections). Following a different route, of timeline summaries is costly; thus the labeled we propose to represent the news articles data for model training is very limited. as an event-graph, thus the summarization As a result, previous studies (Martschat and task becomes compressing the whole graph Markert, 2018; Steen and Markert, 2019) usually to its salient sub-graph. The key hypothesis take an unsupervised approach. Specifically, these is that the events connected through shared arguments and temporal order depict the skelemethods first identify the key dates from the pubton of a timeline, containing events that are lication time distribution. Then for each key date semantically related, structurally salient, and and its associated news articles, a summary is gentemporally coherent in the global event graph. erated based on the salient sentences measured by A time-aware optimal transport distance is the inter-similarity o"
2021.emnlp-main.519,2020.acl-main.553,0,0.0178075,"ument. Our approach is unique in building event-centric graphs across documents, with rich argument and temporal information. 5 Conclusions and Future Work Multi-Document Summarization. Graph-based We propose a novel event graph compression frameMDS methods (Barzilay et al., 1999; Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; work for timeline summarization and achieve stateGanesan et al., 2010; Banerjee et al., 2015; Ya- of-the-art on multiple real-world datasets. Our ussunaga et al., 2017; Fabbri et al., 2019; Liu and La- age of event graphs allows for efficient joint enpata, 2019; Wang et al., 2020; Huang et al., 2020) coding of a large number of documents; and our proposed time-aware optimal transport allows unare closely related to timeline summarization but cannot be directly applied, due to the lack of tem- supervised training of the entire framework. Future work includes extending our approach to abstracporal dimensions. tive summarization, and adding subevent relation Timeline Summarization. Due to the lack of to hierarchically generate the timeline. training data, timeline summarization focuses on extractive methods with heuristics (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh"
2021.emnlp-main.519,N16-1008,0,0.0607266,"Missing"
2021.emnlp-main.519,2021.naacl-main.6,1,0.500178,"r events in the same paragraph or having limited number of nodes is selected, the summary shared arguments. For example, clashes happen graph can have menial information loss. Optimal before wound given the sentence fifty wounded are transport is solving this exact problem by finding reported in the clashes. To obtain the date of each the best transport plan that has a minimal distance event, We extract and normalize time expressions between two graphs. To apply optimal transport using publication date (Manning et al., 2014), and to timeline summarization, the key is to design the then apply (Wen et al., 2021) to extract the event distance to evaluate the information loss, and thus temporal attributes from the context. If the tem6445 poral attributes can not be decided according to the context, we propagate the temporal attributes from neighbor events based on their shared arguments (?). After that, we use the document publication date to populate the remaining missing dates. For example, in Figure 1, the date 2009-0625 of the collapse (D IE) event is extracted from context last Thursday, and the date of the unconscious (I NJURE) event is propagated along with their shared argument Michael Jackson."
2021.emnlp-main.519,J81-4005,0,0.679601,"Missing"
2021.emnlp-main.519,D11-1040,0,0.138735,"ised manner. We show local text features, ignoring the global context of that our approach significantly improves the the news collection. The applications of neural state of the art on three real-world datasets, including two public standard benchmarks models, especially advanced pre-trained language and our newly collected Timeline100 dataset. 1 models, such as BERT (Devlin et al., 2019a) and GPT-2 (Budzianowski and Vuli´c, 2019), are re1 Introduction stricted in terms of both representation capacity and memory efficiency when handling the global Timeline summarization (Chieu and Lee, 2004; Yan et al., 2011a,b; Binh Tran et al., 2013; Tran et al., context within such input document size. We propose an event graph representation along 2013, 2015; Nguyen et al., 2014; Wang et al., 2016; Martschat and Markert, 2018; Steen and Markert, with compression to deal with the representation difficulties in global graph contextualization, scal2019) aims at generating a sequence of major news ability, and time-awareness. Our solution consists events with their key dates from a large collection of the following key ideas. of related news from multiple perspectives (see Figure 1 for an example). The timeline s"
2021.emnlp-main.519,K17-1045,0,0.0478793,"Missing"
2021.emnlp-main.519,2021.textgraphs-1.5,1,0.837732,"Missing"
2021.emnlp-main.519,P19-1628,0,0.0232982,"e summaries of all selected dates; (2) agree F1 to compute ROUGE only between the summaries which have the same dates; (3) align F1 to first align summaries in the output with those in the reference based on similarity and the distance between their dates, then compute the ROUGE score between aligned summaries. Distant alignments are punished. Baselines. We compare with: (1) (Chieu and Lee, 2004), a typical extractive model based on sentence similarity; and (2) (Martschat and Markert, 2018), the state-of-the-art extractive timeline sumarization model based on submodular functions. (3) PacSum (Zheng and Lapata, 2019), the state-of-the-art unsupervised graph-based ranking summarization baseline, which utilizes BERT to encode sentences for sentence centrality ranking in a sentence graph. We use the publication date of the selected sentence as key dates. (4) SummPip (Zhao et al., 2020), the state-of-the-art unsupervised multi-document summarization baseline, which constructs a sentence graph and performs spectral clustering. After that, a summary is generated for each sentence cluster 6 The preprocessed event graphs are released together with the dataset. 7 https://www.ldc.upenn.edu/collaborations/ past-proj"
2021.emnlp-main.631,2020.semeval-1.45,0,0.0275106,"n did not seem to be a problem, and that BART was able to handle multiple negations very well. Therefore marking negation scopes could have introduced unneeded noise into the model, causing the observed performance drop. 3.5 Reasoning conduct such reasoning by relying only on the reference summaries (this difficulty is exacerbated by the fact that SAMSum is of a relatively small size). Multi-task learning (MTL) enables knowledge transfer across relevant tasks. For instance Li et al. (2019) improved their summarization performance by jointly learning summarization and topic segmentation. Also, Konar et al. (2020) improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table 7 in the Appendix A). We use the ROC stories dataset (Mostafazadeh et al., 2016). • Commonsense Generat"
2021.emnlp-main.631,2020.acl-main.703,0,0.0365465,"MSum, a benchmark for abstractive everyday dialogue summarization. Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 3 Challenges 3.1 Experimental Setup For all our experiments, we use BART large architecture (Lewis et al., 2020).1 All our experiments are run using fairseq (Ott et al., 2019). 3.2 Baselines • Vanilla BART: Fine-tuning the original BART large checkpoint model on SAMSum. • Multi-view Seq2Seq (Chen and Yang, 2020) : This is based on BART, as well, but during the summarization, the model considers multiple views, each of which defines a certain structure for the dialogue. We compare to their best model which combines topic and stage views. 3.3 Multiple Speakers We hypothesize that uncommon (less frequent in the original pretraining data) or new names could be an issue to a pretrained model, especially if s"
2021.emnlp-main.631,P19-1210,0,0.0213006,"investigate the negation challenge dialogues put together in (Chen and Yang, 2020). We found that in all examples, negation did not seem to be a problem, and that BART was able to handle multiple negations very well. Therefore marking negation scopes could have introduced unneeded noise into the model, causing the observed performance drop. 3.5 Reasoning conduct such reasoning by relying only on the reference summaries (this difficulty is exacerbated by the fact that SAMSum is of a relatively small size). Multi-task learning (MTL) enables knowledge transfer across relevant tasks. For instance Li et al. (2019) improved their summarization performance by jointly learning summarization and topic segmentation. Also, Konar et al. (2020) improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See exam"
2021.emnlp-main.631,2020.findings-emnlp.165,0,0.0186249,"multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table 7 in the Appendix A). We use the ROC stories dataset (Mostafazadeh et al., 2016). • Commonsense Generation: Generative commonsense reasoning (Lin et al., 2020) is a task involving generating an everyday scenario description given basic concepts. We assume such task could help the model reason more about conversations, which is certainly needed in many dialogues (see example 3 in Table 7 in Appendix A). • Commonsense Knowledge Base Construction: The task here is to generate relation triplets similar to (Bosselut et al., 2019). More specifically, we train our model to predict relation objects given both relation and subject. We use ConceptNet (Liu and Singh, 2004). Table 4 shows the summarization performance after multi-task fine-tuning of BART. We al"
2021.emnlp-main.631,S12-1035,0,0.0264525,"observe that the more participants in the summary, the more effect this technique has. Notably, the average number of speakers per dialogue in SAMSum is only ~2.4. and we expect name substitution to work even better with datasets that have many more speakers per dialogue. 3.4 Negation Understanding Chen and Yang (2020) argue that negations represent a challenge for dialogues. We experiment with marking negation scopes in the input dialogues before feeding them to BART. To do that, we fine-tune a RoBERTa base model on the CD-SCO dataset from SEM Shared Task 2012 for negation scope prediction (Morante and Blanco, 2012). Then, we mark negation scope using two designated special tokens to mark the start and the end of the negation scope. For example, the sentence “I don’t know what to do” becomes “I don’t <NEG> know what to do <NEG>” after negation scope highlighting. We initialize the embeddings of the special tokens <NEG> and <NEG> randomly. Results are shown in Table 3. While we expected to see a performance boost due to negation scope highlighting, we actually saw a performance drop except on ROUGE-L on the test set. To understand why, we investigate the negation challenge dialogues put together in (Che"
2021.emnlp-main.631,N16-1098,0,0.022276,"and topic segmentation. Also, Konar et al. (2020) improved commonsense reasoning through multi-task learning on relevant datasets. Similarly, we propose to simultaneously learn summarization and other reasoning-based tasks. More specifically, we jointly fine-tune BART on the following tasks : • Short Story Ending Prediction: this task could be helpful as predicting story ending requires intuitive understanding of the events. Also, conversation endings could be essential to understand the point of the dialogue (See examples 1 and 2 in Table 7 in the Appendix A). We use the ROC stories dataset (Mostafazadeh et al., 2016). • Commonsense Generation: Generative commonsense reasoning (Lin et al., 2020) is a task involving generating an everyday scenario description given basic concepts. We assume such task could help the model reason more about conversations, which is certainly needed in many dialogues (see example 3 in Table 7 in Appendix A). • Commonsense Knowledge Base Construction: The task here is to generate relation triplets similar to (Bosselut et al., 2019). More specifically, we train our model to predict relation objects given both relation and subject. We use ConceptNet (Liu and Singh, 2004). Table 4"
2021.emnlp-main.631,N19-4009,0,0.0179545,". Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 3 Challenges 3.1 Experimental Setup For all our experiments, we use BART large architecture (Lewis et al., 2020).1 All our experiments are run using fairseq (Ott et al., 2019). 3.2 Baselines • Vanilla BART: Fine-tuning the original BART large checkpoint model on SAMSum. • Multi-view Seq2Seq (Chen and Yang, 2020) : This is based on BART, as well, but during the summarization, the model considers multiple views, each of which defines a certain structure for the dialogue. We compare to their best model which combines topic and stage views. 3.3 Multiple Speakers We hypothesize that uncommon (less frequent in the original pretraining data) or new names could be an issue to a pretrained model, especially if such names were seen very few times, or not at all, during pretr"
2021.emnlp-main.631,P18-1062,0,0.0197847,"and multi-tasked. The summary generated by the vanilla model indicates that the rat is the cheater, pointing to a lack of commonsense reasoning on the model side. The output of our multi-tasked model (section 3.5) clearly shows better understanding of the dialogue. We compare our techniques to two summarization baselines: 2 Related Work Early work on dialogue summarization focused more on extractive than abstractive techniques for summarization of meetings (Murray et al., 2005; Riedhammer et al., 2008) or random conversations (Murray and Renals, 2007). In the context of meeting summarization, Shang et al. (2018) proposed an unsupervised graph-based sentence compression approach for meeting summarization on the AMI (McCowan et al., 2005) and ICSI (Janin et al., 2003) benchmarks. Goo and Chen (2018) leveraged hidden representations from a dialogue act classifier through a gated attention mechanism to guide the summary decoder. More recently, Gliwa et al. (2019) proposed SAMSum, a benchmark for abstractive everyday dialogue summarization. Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang"
2021.emnlp-main.631,J02-4003,0,0.297889,"zer model must keep track of the different lines of thoughts of individual speakers, distinguish salient from non-salient utterances, and finally produce a coherent, monologue summary of the dialogue. Dialogues usually include unfinished sentences where speakers were interrupted or repetitions, where a speaker expresses their thoughts more than once and possibly in different styles. Moreover, a single dialogue could touch on many topics without a clear boundary between the different topics. All the aforementioned phenomena certainly add to the difficulty of the task (Zechner and Waibel, 2000; Zechner, 2002; Chen and Yang, 2020). Our work focuses on SAMSum (Gliwa et al., 2019), which is a dialogue summarization dataset comprised of ~16K everyday dialogues with their ∗ We propose a combination of techniques to tackle a set of dialogue summarization challenges. The first challenge is having multiple speakers (generally, more than 2), where it becomes harder for the model to keep track of different utterances and determine their saliency. The second challenge is multiple negations, which is thought by Chen and Yang (2020) to pose some difficulty to dialogue understanding. The third of these challen"
2021.emnlp-main.631,C00-2140,0,0.319141,"n many speakers, a summarizer model must keep track of the different lines of thoughts of individual speakers, distinguish salient from non-salient utterances, and finally produce a coherent, monologue summary of the dialogue. Dialogues usually include unfinished sentences where speakers were interrupted or repetitions, where a speaker expresses their thoughts more than once and possibly in different styles. Moreover, a single dialogue could touch on many topics without a clear boundary between the different topics. All the aforementioned phenomena certainly add to the difficulty of the task (Zechner and Waibel, 2000; Zechner, 2002; Chen and Yang, 2020). Our work focuses on SAMSum (Gliwa et al., 2019), which is a dialogue summarization dataset comprised of ~16K everyday dialogues with their ∗ We propose a combination of techniques to tackle a set of dialogue summarization challenges. The first challenge is having multiple speakers (generally, more than 2), where it becomes harder for the model to keep track of different utterances and determine their saliency. The second challenge is multiple negations, which is thought by Chen and Yang (2020) to pose some difficulty to dialogue understanding. The third o"
2021.emnlp-main.631,P18-1205,0,0.0265998,"1, ROUGE-2, and ROUGE-L on SAMSum with and without names substitution. Results are shown on the validation and test splits from (Gliwa et al., 2019). Figure 1: ROUGE values against the number of participants per dialogue on the development set of SAMSum. Performance boost is more clear in dialogues with more participants logue domain. Therefore, we adapt BART to dialogue inputs by further pretraining of BART on a dialogue corpus and with dialogue-specific objectives. 4 3.6.1 Pretraining Corpora We consider the following 2 corpora for further pretraining of BART: PersonaChat (140K utterances) (Zhang et al., 2018), and a collection of 12M Reddit comments. We experiment with both whole word masking and span masking (masking random contiguous tokens). Our experimental setup is described in the Appendix in section B.1. Table 5 shows the results of fine-tuning BART pretrained on dialogue corpora.5 The best model (PersonaChat, word masking) outperforms the vanilla BART on all metrics and the Multiview SS baseline on test set ROUGE-2 and ROUGEL. We can see that in general, BART pretrained on PersonaChat is better than pretraining on both PersonaChat and Reddit, which is surprising since more pretraining data"
2021.emnlp-main.631,2020.coling-main.39,0,0.0429498,"meetings (Murray et al., 2005; Riedhammer et al., 2008) or random conversations (Murray and Renals, 2007). In the context of meeting summarization, Shang et al. (2018) proposed an unsupervised graph-based sentence compression approach for meeting summarization on the AMI (McCowan et al., 2005) and ICSI (Janin et al., 2003) benchmarks. Goo and Chen (2018) leveraged hidden representations from a dialogue act classifier through a gated attention mechanism to guide the summary decoder. More recently, Gliwa et al. (2019) proposed SAMSum, a benchmark for abstractive everyday dialogue summarization. Zhao et al. (2020) modeled dialogues using a graph structure of words and utterances and summaries are generated using a graph-to-sequence architecture. Chen and Yang (2020) proposed a multi-view summarization model, where views can include topic or stage. They also pointed out to seven different challenges to dialogue summarization and analysed the effect each challenge can have on summarization performance using examples from SAMSum. 3 Challenges 3.1 Experimental Setup For all our experiments, we use BART large architecture (Lewis et al., 2020).1 All our experiments are run using fairseq (Ott et al., 2019). 3"
2021.naacl-main.230,N19-1213,0,0.0164695,"et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo e"
2021.naacl-main.230,2020.acl-main.372,0,0.119555,"es the stress classification problem in terms of the author and the time–i.e., a post is labeled stressful only if the Pre-training and fine-tuning are another type of poster themselves is currently expressing stress. transfer learning where multiple tasks are trained in sequence rather than at the same time. Pre-trained Because this dataset is small for training a deep language models are perhaps the most widely used learning model, we also experiment with larger example, where a large neural language model can datasets to provide auxiliary information. We se2896 lect the GoEmotions dataset (Demszky et al., 2020), which consists of 58,009 Reddit comments labeled by crowd workers with one or more of 27 emotions (or Neutral), for its larger size and genre similarity to Dreaddit. In this paper, we refer to the dataset in this form as GoEmotionsall or GoEmotionsA . The authors also published two relabelings of this dataset, achieved by agglomerative clustering: one where labels are clustered together into the Ekman 6 basic emotions (anger, disgust, fear, joy, sadness, surprise, neutral) (Ekman, 1992) (GoEmotionsEkman/E ), and one into simple polarity (positive, negative, ambiguous, neutral) (GoEmotionssen"
2021.naacl-main.230,N19-1423,0,0.0194252,"rning has been successfully applied to many domains across NLP (Sun et al., 2019; Kiperwasser and Ballesteros, 2018; Liu et al., 2019); we are especially interested in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji det"
2021.naacl-main.230,D17-1169,0,0.0326887,"e-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo et al. (2012); Allen et al. (2014); Al-Shargie et al. (2016); Kumar et al. (2020); Jaiswal et al. (2020)). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts. 3 Data A comparison of all the datasets we use in this work can be seen in Table 1. T"
2021.naacl-main.230,2020.acl-main.740,0,0.014103,"nd emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to s"
2021.naacl-main.230,P18-1031,0,0.0204595,"sted in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou et al. (2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better st"
2021.naacl-main.230,2020.lrec-1.187,0,0.0428233,"sk learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo et al. (2012); Allen et al. (2014); Al-Shargie et al. (2016); Kumar et al. (2020); Jaiswal et al. (2020)). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts. 3 Data A comparison of all the datasets we use in this work can be seen in Table 1. The primary dataset we use for this work is Dreaddit (Turcan and McKeown, 2019), a dataset of 3,553 segments of Reddit posts from various support communities where the authors believe posters are likely to express stress. The stress detection problem as expressed in this dataset is a binary classification problem, with crowdsourced annotations aggregated as the majority vote from five ann"
2021.naacl-main.230,2020.louhi-1.16,0,0.0417352,"a et al. (2018); Lin et al. (2017)) to assign labels. Much of the work that has been done on psychological stress detection focuses either on establishing baseline models with little advancement in computational modeling, or on using external information about the text (e.g., author, time of posting, number of replies), which is usually, but not always available and may differ in meaning or importance across platforms and domains. There has also been a substantial amount of work on detecting related mental health concerns such as anxiety (e.g., Shen and Rudzicz (2017); Gruda and Hasan (2019); Jiang et al. (2020)), but these are distinct from the generalized experience of stress. The most similar work to ours is Turcan and McKeown (2019), our prior work publishing a dataset of psychological stress collected from the social media website Reddit and labeled by crowd workers, and presenting baselines with several basic non-neural and BERT-based models on this data. We use this dataset in our current work; however, we focus on exploring interpretable frameworks for this sensitive task and connecting the stress detection task concretely with emotion detection. The models we propose in this work rely on two"
2021.naacl-main.230,Q18-1017,0,0.0177964,"ents to the neural representation learned by models like BERT: multi-task learning and pre-training or fine-tuning. Multi-task learning is an increasingly popular framework in which some parameters in a model are shared between or used to inform multiple different tasks. Hard parameter sharing (Caruana, 1993), the variant we employ, uses some set of parameters as a shared base representation and then allows each task to have some private parameters on top and perform their own separate predictions. Multi-task learning has been successfully applied to many domains across NLP (Sun et al., 2019; Kiperwasser and Ballesteros, 2018; Liu et al., 2019); we are especially interested in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target d"
2021.naacl-main.230,D19-6213,1,0.903496,"on focuses either on establishing baseline models with little advancement in computational modeling, or on using external information about the text (e.g., author, time of posting, number of replies), which is usually, but not always available and may differ in meaning or importance across platforms and domains. There has also been a substantial amount of work on detecting related mental health concerns such as anxiety (e.g., Shen and Rudzicz (2017); Gruda and Hasan (2019); Jiang et al. (2020)), but these are distinct from the generalized experience of stress. The most similar work to ours is Turcan and McKeown (2019), our prior work publishing a dataset of psychological stress collected from the social media website Reddit and labeled by crowd workers, and presenting baselines with several basic non-neural and BERT-based models on this data. We use this dataset in our current work; however, we focus on exploring interpretable frameworks for this sensitive task and connecting the stress detection task concretely with emotion detection. The models we propose in this work rely on two types of enhancements to the neural representation learned by models like BERT: multi-task learning and pre-training or fine-t"
2021.naacl-main.230,2020.emnlp-demos.6,0,0.0310945,"Missing"
2021.naacl-main.230,W18-6243,0,0.0230015,"framework in which some parameters in a model are shared between or used to inform multiple different tasks. Hard parameter sharing (Caruana, 1993), the variant we employ, uses some set of parameters as a shared base representation and then allows each task to have some private parameters on top and perform their own separate predictions. Multi-task learning has been successfully applied to many domains across NLP (Sun et al., 2019; Kiperwasser and Ballesteros, 2018; Liu et al., 2019); we are especially interested in instances where it has improved semantic and emotion-related tasks, such as Xu et al. (2018), who perform emotion detection with a suite of secondary semantic tasks including personality classification. Dataset Dreaddit GoEmotionsA,E,S GoEmotionsF SJ Vent Size 3,553 58K 4,136 1.6M Table 1: The datasets we use in this work and their relative sizes (in terms of total number of data points). be fine-tuned for many different tasks (Devlin et al., 2019). Additionally, continuing to pre-train the language model itself on language from the target domain has been shown to improve performance (Howard and Ruder, 2018; Chakrabarty et al., 2019; Gururangan et al., 2020) (also note Chronopoulou e"
2021.naacl-main.230,zuo-etal-2012-multilingual,0,0.0324675,"2019), who perform this task at the same time as the target task, in a form of multi-task learning). This methodology has been successfully extended to other domains, in which a model is first finetuned on some large, broadly useful task and then further fine-tuned for a smaller target task (e.g., Felbo et al. (2017), who first fine-tuned on emoji detection and then fine-tuned on target semantic tasks including emotion and sentiment detection). It should be noted that the psychological stress is much better studied in settings where researchers have access to some physiological signals (e.g., Zuo et al. (2012); Allen et al. (2014); Al-Shargie et al. (2016); Kumar et al. (2020); Jaiswal et al. (2020)). This work is not as relevant to our task, since we have only text data available when detecting stress from online posts. 3 Data A comparison of all the datasets we use in this work can be seen in Table 1. The primary dataset we use for this work is Dreaddit (Turcan and McKeown, 2019), a dataset of 3,553 segments of Reddit posts from various support communities where the authors believe posters are likely to express stress. The stress detection problem as expressed in this dataset is a binary classifi"
2021.naacl-main.379,S16-1062,0,0.0278392,"at uses adversarial learning to generalize to unseen topics on Twitter. Our model achieves state-of-the-art zero-shot performance on the majority of topics in the standard dataset for English stance detection on Twitter (Mohammad et al., 2016) and also provides benchmark results on two new topics in this dataset. Most prior work on English social media stance detection uses the SemEval2016 Task 6 (SemT6) dataset (Mohammad et al., 2016) which consists of six topics. While early work trained using five topics and evaluated on the sixth (e.g., Augenstein et al. (2016); Zarrella and Marsh (2016); Wei et al. (2016)), they used only one topic, ‘Donald Trump’ ⇤ Denotes equal contribution. (DT), for evaluation and did not experiment with others. Furthermore, recent work on SemT6 has focused on cross-target stance detection (Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020): training on one topic and evaluating on one different unseeen topic that has a known relationship with the training topic (e.g., “legalization of abortion” to “feminist movement”). These models are typically evaluated on four different test topics (each with a different training topic). In contrast, our work is a hybrid of these t"
2021.naacl-main.379,S16-1074,0,0.0286882,"del for stance detection that uses adversarial learning to generalize to unseen topics on Twitter. Our model achieves state-of-the-art zero-shot performance on the majority of topics in the standard dataset for English stance detection on Twitter (Mohammad et al., 2016) and also provides benchmark results on two new topics in this dataset. Most prior work on English social media stance detection uses the SemEval2016 Task 6 (SemT6) dataset (Mohammad et al., 2016) which consists of six topics. While early work trained using five topics and evaluated on the sixth (e.g., Augenstein et al. (2016); Zarrella and Marsh (2016); Wei et al. (2016)), they used only one topic, ‘Donald Trump’ ⇤ Denotes equal contribution. (DT), for evaluation and did not experiment with others. Furthermore, recent work on SemT6 has focused on cross-target stance detection (Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020): training on one topic and evaluating on one different unseeen topic that has a known relationship with the training topic (e.g., “legalization of abortion” to “feminist movement”). These models are typically evaluated on four different test topics (each with a different training topic). In contrast, our work is"
2021.naacl-main.379,2020.acl-main.291,0,0.038666,"nchmark results on two new topics in this dataset. Most prior work on English social media stance detection uses the SemEval2016 Task 6 (SemT6) dataset (Mohammad et al., 2016) which consists of six topics. While early work trained using five topics and evaluated on the sixth (e.g., Augenstein et al. (2016); Zarrella and Marsh (2016); Wei et al. (2016)), they used only one topic, ‘Donald Trump’ ⇤ Denotes equal contribution. (DT), for evaluation and did not experiment with others. Furthermore, recent work on SemT6 has focused on cross-target stance detection (Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020): training on one topic and evaluating on one different unseeen topic that has a known relationship with the training topic (e.g., “legalization of abortion” to “feminist movement”). These models are typically evaluated on four different test topics (each with a different training topic). In contrast, our work is a hybrid of these two settings: we train on five topics and evaluate on one other, but unlike prior work we do not assume a relationship between training and test topics and so we use each topic in turn as the test topic. This illustrates the robustness of our model across topics and"
2021.naacl-main.379,Q17-1036,0,0.39356,"topics when learning with many topics and only a few examples per topic, there are no datasets for social media with this setup. Specifically, current datasets for stance detection on Twitter (Mohammad et al., 2016; Taulé et al., 2017; Küçük, 2017; Tsakalidis et al., 2018; Lai et al., 2020) have only a few topics but many examples per topic. Therefore, zero-shot stance detection on social media is best modeled as a domain adaptation task. To model zero-shot topic transfer as domainadaptation, we treat each topic as a domain. Following the success of adversarial learning for domain adaptation (Zhang et al., 2017; Ganin and Lempitsky, 2015), we use a discriminator (adversary) to learn topic-invariant representations that allow better generalization across topics. Although, Wei and Mao (2019) also proposed adversarial learning 4756 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4756–4767 June 6–11, 2021. ©2021 Association for Computational Linguistics for stance detection, their model relies on knowledge transfer between topics (domains) and so is only suited to the cross-target, not zero-shot, task."
2021.naacl-main.427,W19-4322,0,0.268196,"rastive loss. We assess the performance of SCCL on short text clustering, which has become increasingly important due to the popularity of social media such as Twitter and Instagram. It benefits many real-world applications, including topic discovery (Kim et al., 2013), recommendation (Bouras and Tsogkas, 2017), and visualization (Sebrechts et al., 1999). However, the weak signal caused by noise and sparsity poses a significant challenge for clustering short texts. Although some improvement has been achieved by leveraging shallow neural networks to enrich the representations (Xu et al., 2017; Hadifar et al., 2019), there is still large room for improvement. We address this challenge with our SCCL model. Our main contributions are the following: • We propose a novel end-to-end framework for unsupervised clustering, which advances the state-of-the-art results on various short text clustering datasets by a large margin. Furthermore, our model is much simpler than the existing deep neural network based short text clustering approaches that often require multistage independent training. Early work focuses on solving different artificially designed pretext tasks, such as predicting masked tokens (Devlin et a"
2021.naacl-main.427,N19-1423,0,0.102265,"al., 2019), there is still large room for improvement. We address this challenge with our SCCL model. Our main contributions are the following: • We propose a novel end-to-end framework for unsupervised clustering, which advances the state-of-the-art results on various short text clustering datasets by a large margin. Furthermore, our model is much simpler than the existing deep neural network based short text clustering approaches that often require multistage independent training. Early work focuses on solving different artificially designed pretext tasks, such as predicting masked tokens (Devlin et al., 2019), generating future tokens (Radford et al., 2018), or denoising corrupted tokens (Lewis et al., 2019) for textual data, and predicting colorization (Zhang et al., 2016), rotation (Gidaris et al., 2018), or relative patch position (Doersch et al., 2015) for image data. Nevertheless, the resulting representations are tailored to the specific pretext tasks with limited generalization. Many recent successes are largely driven by instance-wise contrastive learning. Inspired by the pioneering work of Becker and Hinton (1992); Bromley et al. (1994), Instance-CL treats each data instance and its augme"
2021.naacl-main.427,N18-2072,0,0.222978,"˜i2 apart from all negative instances in B a by minimizing the following Here 1j6=i1 is an indicator function and τ denotes the temperature parameter which we set as 0.5. Following Chen et al. (2020a), we choose sim(·) as the dot product between a pair of normalized outputs, i.e., sim(˜ zi , z˜j ) = z˜iT z˜j /k˜ zi k2 k˜ zj k 2 . The Instance-CL loss is then averaged over all instances in B a , LInstance-CL = 2M X `Ii /2M . (2) i=1 To explore the above contrastive loss in the text domain, we explore three different augmentation strategies in Section 4.3.1, where we find contextual augmenter (Kobayashi, 2018; Ma, 2019) consistently performs better than the other two. 3.2 Clustering We simultaneously encode the semantic categorical structure into the representations via unsupervised clustering. Unlike Instance-CL, clustering focuses on the high-level semantic concepts and tries to bring together instances from the same semantic category together. Suppose our data consists of K semantic categories, and each category is characterized by its centroid in the representation space, denoted as µk , k ∈ {1, . . . , K}. Let ej = ψ(xj ) denote the representation of instance xj in the original set B. Followi"
2021.naacl-main.427,2020.acl-main.703,0,0.0388608,"Missing"
2021.naacl-main.427,H89-1033,0,0.527203,"Missing"
2021.naacl-main.427,2020.emnlp-demos.16,0,0.0437775,"Missing"
2021.naacl-main.427,N18-1202,0,0.0303245,"Hadifar et al., 2019), where word embedcluster distance and intra-cluster distance. dings (Mikolov et al., 2013b; Arora et al., 2017) • We explore various text augmentation techare adopted to further enhance the performance. niques for SCCL, showing that, unlike the However, the above approaches divide the learnimage domain (Chen et al., 2020a), using coming process into multiple stages, each requiring position of augmentations is not always beneindependent optimization. On the other hand, deficial in the text domain. spite the tremendous successes achieved by contextualized word embeddings (Peters et al., 2018; 2 Related Work Devlin et al., 2019; Radford et al., 2018; Reimers Self-supervised learning Self-supervised learn- and Gurevych, 2019b), they have been left largely ing has recently become prominent in providing ef- unexplored for short text clustering. In this work, fective representations for many downstream tasks. we leverage the pretrained transformer as the back5420 Figure 2: Training framework SCCL. During training, we jointly optimize a clustering loss over the original data instances and an instance-wise contrastive loss over the associated augmented pairs. bone, which is optimized in"
2021.naacl-main.427,D19-1410,0,0.305385,"hed clustering methods such as K-means (MacQueen et al., 1967; Lloyd, 1982) and Gaussian Mixture Models (Celeux and Govaert, 1995) rely on distance measured in the data space, which tends to be ineffective for highdimensional data. On the other hand, deep neural networks are gaining momentum as an effective way to map data to a low dimensional and hopefully better separable representation space. Many recent research efforts focus on integrating clustering with deep representation learning Figure 1: TSNE visualization of the embedding space learned on SearchSnippets using Sentence Transformer (Reimers and Gurevych, 2019a) as backbone. Each color indicates a ground truth semantic category. by optimizing a clustering objective defined in the representation space (Xie et al., 2016; Jiang et al., 2016; Zhang et al., 2017a; Shaham et al., 2018). Despite promising improvements, the clustering performance is still inadequate, especially in the presence of complex data with a large number of clusters. As illustrated in Figure 1, one possible reason is that, even with a deep neural network, data still has significant overlap across categories before clustering starts. Consequently, the clusters learned by optimizing"
2021.naacl-main.427,P19-1103,0,0.0280766,"ing space. In contrast, we leverage the strengths of both Clustering and Instance-CL to compliment each other. Consequently, Figure 4 shows SCCL leads to better separated clusters with each cluster being less dispersed. 4.3 4.3.1 Figure 4: Cluster-level evaluation on SearchSnippets. Each plot is summarized over five random runs. Data Augmentation Exploration of Data Augmentations To study the impact of data augmentation, we explore three different unsupervised text augmentations: (1) WordNet Augmenter5 transforms an input text by replacing its words with WordNet synonyms (Morris et al., 2020; Ren et al., 2019). (2) Contextual Augmenter6 leverages the pretrained transformers to find top-n suitable words of the input text for insertion or substitution (Kobayashi, 2018; Ma, 2019). We augment the data via word substitution, and we choose Bertbase and Roberta to generate the augmented pairs. (3) Paraphrase via back translation7 generates paraphrases of the input text by first translating it to another language (French) and then back to English. When translating back to English, we used the mixture of experts model (Shen et al., 2019) to generate ten candidate paraphrases per input to increase diversity."
2021.sustainlp-1.10,2020.aacl-main.28,0,0.0301663,"Missing"
2021.sustainlp-1.10,2020.emnlp-main.54,0,0.0644881,"Missing"
2021.sustainlp-1.10,W18-2607,0,0.0282866,"cally, we match keywords in the questions and use the Spacy model (Honnibal et al., 2020) to detect subjective and objective in context sentences. 2 Once the mapped types are obtained for each SocialIQA question, we transfer such information to QA models by simply concatenating the tags to original QA examples in the format of [Context, SEP, Question, T ag, Answer] as input to a PLM for fine-tuning. 3 Although we Commonsense Categorization in NLP LoBue and Yates (2011) proposed form-based and contentbased categories for commonsense knowledge that is involved in recognizing textual entailment. Boratko et al. (2018) refined the categorization method 2 Take the fourth instance in Figure 1 as an example; we firstly match the word “need"" in question to the ""Need"" relation, then detect the name “Taylor"" is subjective in the context, so we assign “xNeed"" to this question. 3 All the tags are added into the model’s vocabulary as spe1 https://github.com/posuer/social-commonsenseknolwedge 80 use RoBERTa model in this work, our method is generic and can be applied to any PLMs. 3.2 Social Knowledge Categorization Although utilizing ATOMIC to obtain relation type is straightforward and simple, it is restricted to da"
2021.sustainlp-1.10,2020.deelio-1.9,0,0.0479513,"ocial knowledge needed by the SocialIQA task. 2 3.1 3 Methodology This section presents two approaches to model the underlying semantics and knowledge of the SocialIQA task, together with a simple yet effective method that leverages these knowledge types to improve QA models. Related Work SocialIQA Task Most previous works on SocialIQA task involve either large size of pre-trained models, and datasets (Khashabi et al., 2020; Lourie et al., 2021) or complicated models that heavily rely on external knowledge bases (Shen et al., 2020; Shwartz et al., 2020; Mitra et al., 2019; Ji et al., 2020a,b; Chang et al., 2020). Among them, UnifiedQA (Khashabi et al., 2020) achieved impressive performance by fine-tuning 11B T5 model (Raffel et al., 2019) with 17 existing QA datasets. Unlike previous efforts, our work achieves comparable performance with a relatively small model and simple knowledge extraction method that does not rely on knowledge bases nor require additional pretraining. Question Relation Type The SocialIQA dataset was derived from the ATOMIC (Sap et al., 2019a). ATOMIC is a knowledge base that focuses on everyday social commonsense knowledge organized as ten types of ifthen relations. Based on thi"
2021.sustainlp-1.10,2021.ccl-1.108,0,0.072545,"Missing"
2021.sustainlp-1.10,P11-2057,0,0.0421997,"this observation, we tag each question in SocialIQA according to its relation types in ATOMIC by conducting rule-based mapping between them. Specifically, we match keywords in the questions and use the Spacy model (Honnibal et al., 2020) to detect subjective and objective in context sentences. 2 Once the mapped types are obtained for each SocialIQA question, we transfer such information to QA models by simply concatenating the tags to original QA examples in the format of [Context, SEP, Question, T ag, Answer] as input to a PLM for fine-tuning. 3 Although we Commonsense Categorization in NLP LoBue and Yates (2011) proposed form-based and contentbased categories for commonsense knowledge that is involved in recognizing textual entailment. Boratko et al. (2018) refined the categorization method 2 Take the fourth instance in Figure 1 as an example; we firstly match the word “need"" in question to the ""Need"" relation, then detect the name “Taylor"" is subjective in the context, so we assign “xNeed"" to this question. 3 All the tags are added into the model’s vocabulary as spe1 https://github.com/posuer/social-commonsenseknolwedge 80 use RoBERTa model in this work, our method is generic and can be applied to a"
2021.sustainlp-1.10,N16-1098,0,0.0304205,"ward. Why did Tracy do this? (a) get very close to Austin (b) squeeze into the elevator ✔ (c) get flirty with Austin. Daily Events xWant Alex spilled the food she just prepared all over the floor and it made a huge mess. What will Alex want to do next? Introduction (a) taste the food (b) mop up ✔ (c) run around in the mess Knowledge, Norm, and Rules Recently, large pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2019; Liu et al., 2019) have been widely used on various commonsense QA tasks such as CommonsenseQA (Malaviya et al., 2020), SocialIQA (Sap et al., 2019b), and Mostafazadeh et al. (2016); Huang et al. (2019); Boratko et al. (2020); Levesque et al. (2012); Roemmele et al. (2011). One line of work (Khashabi et al., 2020) improved the performances of these QA tasks by aggregating more QA data and using even bigger PLM T5 (Raffel et al., 2019). Other line of work tried to supplement the question context with retrieval of related knowledge from external knowledge bases (KB), or re-trained PLMs under the guidance of KBs (Shen et al., 2020; Shwartz et al., 2020; Mitra et al., 2019; Ji et al., 2020a,b). However, very little past research has paid attention to the specific question/co"
2021.sustainlp-1.10,D19-1454,0,0.18,"elevator and it was awkward. Why did Tracy do this? (a) get very close to Austin (b) squeeze into the elevator ✔ (c) get flirty with Austin. Daily Events xWant Alex spilled the food she just prepared all over the floor and it made a huge mess. What will Alex want to do next? Introduction (a) taste the food (b) mop up ✔ (c) run around in the mess Knowledge, Norm, and Rules Recently, large pre-trained language models (PLMs) (Devlin et al., 2019; Raffel et al., 2019; Liu et al., 2019) have been widely used on various commonsense QA tasks such as CommonsenseQA (Malaviya et al., 2020), SocialIQA (Sap et al., 2019b), and Mostafazadeh et al. (2016); Huang et al. (2019); Boratko et al. (2020); Levesque et al. (2012); Roemmele et al. (2011). One line of work (Khashabi et al., 2020) improved the performances of these QA tasks by aggregating more QA data and using even bigger PLM T5 (Raffel et al., 2019). Other line of work tried to supplement the question context with retrieval of related knowledge from external knowledge bases (KB), or re-trained PLMs under the guidance of KBs (Shen et al., 2020; Shwartz et al., 2020; Mitra et al., 2019; Ji et al., 2020a,b). However, very little past research has paid att"
2021.sustainlp-1.10,2020.emnlp-main.722,0,0.25481,"have been widely used on various commonsense QA tasks such as CommonsenseQA (Malaviya et al., 2020), SocialIQA (Sap et al., 2019b), and Mostafazadeh et al. (2016); Huang et al. (2019); Boratko et al. (2020); Levesque et al. (2012); Roemmele et al. (2011). One line of work (Khashabi et al., 2020) improved the performances of these QA tasks by aggregating more QA data and using even bigger PLM T5 (Raffel et al., 2019). Other line of work tried to supplement the question context with retrieval of related knowledge from external knowledge bases (KB), or re-trained PLMs under the guidance of KBs (Shen et al., 2020; Shwartz et al., 2020; Mitra et al., 2019; Ji et al., 2020a,b). However, very little past research has paid attention to the specific question/context knowledge types that are needed for these commonsense QA tasks. Therefore, in this paper, we go deeper into xNeed Taylor taught math in the schools after studying to be a teacher for 4 years. What does Taylor need to do before this? (a) get a certificate ✔ (b) teach small children (c) work in a school Figure 1: SocialIQA Examples for Social Knowledge Categories and Question Relation Type the QA task context and take a closer look at the semanti"
2021.sustainlp-1.10,2020.emnlp-main.373,0,0.0282592,"Missing"
A00-2024,P98-1099,1,0.712227,"eduction module makes decisions based on multiple sources of knowledge: (1) G r a m m a r c h e c k i n g . In this step, we mark which components of a sentence or a phrase are obligatory to keep it grammatically correct. To do this, we traverse the sentence parse tree, produced by the English Slot G r a m m a r ( E S G ) parser developed at IBM (McCord, 1990), in top-down order and m a r k for each node in the parse tree, which of its children are obligatory. The main source of knowledge the system relies on in this step is a large-scale, reusable lexicon we combined from multiple resources (Jing and McKeown, 1998). The lexicon contains subcategorizations for over 5,000 verbs. This information is used to m a r k the obligatory arguments of verb phrases. (2) C o n t e x t i n f o r m a t i o n . We use an extracted sentence&apos;s local context in the article to decide which components in the sentence are likely to be most relevant to the main topic. We link the words in the extracted sentence with words in its local context, if they are repetitions, morphologically related, or linked with each other in WordNet through certain type of lexical relation, such as synonymy, antonymy, or meronymy. Each word in the"
A00-2024,A00-1043,1,0.768869,"sentences in the input article. It also divided the s u m m a r y sentence into phrases and pinpointed the exact document origin of each phrase. A phrase in the s u m m a r y sentence is annotated as (FNUM:SNUM actual-text), where F N U M is the sequential number of the phrase and S N U M is the number of the document sentence where the phrase comes from. SNUM = -1 means that the component does not come from the original document. The phrases in the document sentences are annotated as (FNUM actual-text). 4.2 Sentence reduction The task of the sentence reduction module, described in detail in (Jing, 2000), is to remove extraneous phrases from extracted sentences. The goal of 181 reduction is to ""reduce without m a j o r loss""; that is, we want to remove as m a n y extraneous phrases as possible from an extracted sentence so that it can be concise, but without detracting from the main idea that the sentence conveys. Ideally, we want to remove a phrase from an extracted sentence only if it is irrelavant to the main topic. Our reduction module makes decisions based on multiple sources of knowledge: (1) G r a m m a r c h e c k i n g . In this step, we mark which components of a sentence or a phras"
A00-2024,P99-1072,0,0.0160526,"Missing"
A00-2024,C98-1096,1,\N,Missing
A94-1002,C90-3059,0,0.134773,"Missing"
A94-1002,C92-3158,0,0.165128,"Missing"
A94-1002,H90-1009,1,0.818414,"Missing"
A94-1002,P89-1025,0,0.0212695,"Missing"
A94-1002,A92-1006,0,0.0431655,"Missing"
A94-1002,C90-1021,0,\N,Missing
A94-1002,P83-1022,1,\N,Missing
A94-1002,H90-1086,1,\N,Missing
A97-1033,M92-1024,0,0.0328184,"d for the message understanding conferences (MUC, 1992), and use of extracted information for question answering. Techniques for proper noun extraction include the use of regular g r a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We u"
A97-1033,A88-1019,0,0.0154053,"Missing"
A97-1033,M92-1031,0,0.308897,"Missing"
A97-1033,M92-1032,0,0.03486,"use in summarization. In contrast with previous work on information extraction, our work has the following features: • It builds a database of profiles for entities by storing descriptions from a collected corpus of • past news. • It operates in real time, allowing for connections with the latest breaking, online news to extract information about the most recently mentioned individuals and organizations. Introduction In our work to date on news summarization at Columbia University (McKeown and Radev, 1995; Radev, 1996), information is extracted from a series of input news articles (MUC, 1992; Grishman et al., 1992) and is analyzed by a generation component to produce a s u m m a r y that shows how perception of the event has changed over time. In this s u m m a r i z a t i o n paradigm, problems arise when information needed for the s u m m a r y is either missing from the input article(s) or not extracted by the information extraction system. In such cases, the information m a y be readily available in other current news stories, in past news, or in online databases. If the summarization system can find the needed information in other online sources, then it can produce an improved s u m m a r y by mer"
A97-1033,M93-1023,0,0.0839026,"a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular g r a m m a r encoding part-ofspeech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to provide semantic filtering. Our work on extract"
A97-1033,W93-0105,0,0.269583,"construction of knowledge sources for generation. 2.1 Information Extraction Work on information extraction is quite broad and covers far more topics and problems than the information extraction problem we address. We restrict our comparison here to work on proper noun extraction, extraction of people descriptions in various information extraction systems developed for the message understanding conferences (MUC, 1992), and use of extracted information for question answering. Techniques for proper noun extraction include the use of regular g r a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored"
A97-1033,W93-0104,0,0.0851769,"tles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular g r a m m a r encoding part-ofspeech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to provide semantic filtering. Our work on extracting descriptions is quite similar to the work carried out under the D A R P A message understanding p r o g r a m for extracting desc"
A97-1033,M93-1010,0,0.0165286,"e the use of regular g r a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular g r a m m a r encoding part-ofspeech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to provide semantic filteri"
A97-1033,W96-0512,1,0.716907,"L E that tracks prior references to a given entity by extracting descriptions for later use in summarization. In contrast with previous work on information extraction, our work has the following features: • It builds a database of profiles for entities by storing descriptions from a collected corpus of • past news. • It operates in real time, allowing for connections with the latest breaking, online news to extract information about the most recently mentioned individuals and organizations. Introduction In our work to date on news summarization at Columbia University (McKeown and Radev, 1995; Radev, 1996), information is extracted from a series of input news articles (MUC, 1992; Grishman et al., 1992) and is analyzed by a generation component to produce a s u m m a r y that shows how perception of the event has changed over time. In this s u m m a r i z a t i o n paradigm, problems arise when information needed for the s u m m a r y is either missing from the input article(s) or not extracted by the information extraction system. In such cases, the information m a y be readily available in other current news stories, in past news, or in online databases. If the summarization system can find th"
A97-1033,M91-1028,0,\N,Missing
A97-1033,H93-1062,0,\N,Missing
A97-1033,M92-1030,0,\N,Missing
A97-1041,P88-1023,0,0.487239,"Missing"
A97-1041,P95-1053,1,0.867464,"t generates a sentence in order to provide information to the media coordinator for negotiation with graphics. Our development of MAGIC is very much an ongoing research project. We are continuing to work on improved coordination of media, use of the syntactic and semantic structure of generated language to improve the quality of the synthesized speech, and analysis of a corpus of radio speech to identify characteristics of formal, spoken language. 4 6 R e l a t e d Work There is considerable interest in producing fluent and concise sentences. EPICURE (Dale, 1992), PLANDOC(Kukich et al., 1994; Shaw, 1995), and systems developed by Dalianis and Hovy (Dalianis and Hovy, 1993) all use various forms of conjunction and ellipsis to generate more concise sentences. In (Horacek, 1992) aggregation is performed at text-structure level. In addition to conjoining VP and NPs, FLowDoc(Passonneau et al., 1996) uses ontological generalization to combine descriptions of a set of objects into a more general description. Based on a corpus analysis in the basketball domain, (Robin, 1994) catalogued a set of revision operators such as adjoin and nominalization in his system STREAK. Unlike STREAK, MAGIC does not us"
A97-1041,A94-1002,1,0.829094,"nt its decisions as it generates a sentence in order to provide information to the media coordinator for negotiation with graphics. Our development of MAGIC is very much an ongoing research project. We are continuing to work on improved coordination of media, use of the syntactic and semantic structure of generated language to improve the quality of the synthesized speech, and analysis of a corpus of radio speech to identify characteristics of formal, spoken language. 4 6 R e l a t e d Work There is considerable interest in producing fluent and concise sentences. EPICURE (Dale, 1992), PLANDOC(Kukich et al., 1994; Shaw, 1995), and systems developed by Dalianis and Hovy (Dalianis and Hovy, 1993) all use various forms of conjunction and ellipsis to generate more concise sentences. In (Horacek, 1992) aggregation is performed at text-structure level. In addition to conjoining VP and NPs, FLowDoc(Passonneau et al., 1996) uses ontological generalization to combine descriptions of a set of objects into a more general description. Based on a corpus analysis in the basketball domain, (Robin, 1994) catalogued a set of revision operators such as adjoin and nominalization in his system STREAK. Unlike STREAK, MAGI"
andreas-etal-2012-annotating,W04-2319,0,\N,Missing
andreas-etal-2012-annotating,D08-1027,0,\N,Missing
andreas-etal-2012-annotating,N03-2012,0,\N,Missing
andreas-etal-2012-annotating,P04-1085,1,\N,Missing
andreas-etal-2012-annotating,W10-0701,0,\N,Missing
andreas-etal-2012-annotating,P11-2065,0,\N,Missing
andreas-etal-2012-annotating,N06-2014,0,\N,Missing
C00-2104,W97-0703,0,0.0257697,"s within five-word windows. Smadja (1992) gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Hatzivassiloglou and McKeown (1993) clustered adjectives into semantic classes, and Pereira et al. (1993) clustered nouns on their appearance in verb-object pairs. We are trying to be less restrictive in learning multiple salient relationships between words rather than seeking a particular relationship. In a way, our idea is the mirror image of Barzilay and Elhadad (1997), who used Wordnet to identify lexical chains that would coincide with cohesive text segments. We assumed that documents are cohesive and that co-occurrence patterns can uncover word relationships. 3 Experiments The focus of our experiment was on units of text in which the constituents must fit together in order for the discourse to be coherent. We made the assumption that the documents in our corpus were coherent and reasoned that if we had enough text, covering a broad range of topics, we could pick out domainindependent associations. For example, testimony can be about virtually anything, s"
C00-2104,P99-1008,0,0.0159788,"ent basis. Harabagiu and Maiorano (1999) argued that indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction. In the same vein, researchers at Brown University (Caraballo and Charniak, 1999), (Berland and Charniak, 1999), (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different perspective than these lines of inquiry. They were specifying various semantic relationships and seeking ways to collect similar pairs. We have a less restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been developed and refined. The focus of"
C00-2104,W99-0609,0,0.0129417,"was considered on a whole-document basis. Harabagiu and Maiorano (1999) argued that indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction. In the same vein, researchers at Brown University (Caraballo and Charniak, 1999), (Berland and Charniak, 1999), (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different perspective than these lines of inquiry. They were specifying various semantic relationships and seeking ways to collect similar pairs. We have a less restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been devel"
C00-2104,P99-1016,0,0.0147348,"ano (1999) argued that indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction. In the same vein, researchers at Brown University (Caraballo and Charniak, 1999), (Berland and Charniak, 1999), (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different perspective than these lines of inquiry. They were specifying various semantic relationships and seeking ways to collect similar pairs. We have a less restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been developed and refined. The focus of much of this work"
C00-2104,P89-1010,0,0.019334,"on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different perspective than these lines of inquiry. They were specifying various semantic relationships and seeking ways to collect similar pairs. We have a less restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been developed and refined. The focus of much of this work was to develop the methods themselves. Church and Hanks (1989) explored the use of mutual information statistics in ranking co-occurrences within five-word windows. Smadja (1992) gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Hatzivassiloglou and McKeown (1993) clustered adjectives into semantic classes, and Pereira et al. (1993) clustered nouns on their appearance in verb-object pairs. We are trying to be less restrictive in learning multiple salient relationships between words rather than se"
C00-2104,P93-1023,1,0.813641,"ess restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been developed and refined. The focus of much of this work was to develop the methods themselves. Church and Hanks (1989) explored the use of mutual information statistics in ranking co-occurrences within five-word windows. Smadja (1992) gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Hatzivassiloglou and McKeown (1993) clustered adjectives into semantic classes, and Pereira et al. (1993) clustered nouns on their appearance in verb-object pairs. We are trying to be less restrictive in learning multiple salient relationships between words rather than seeking a particular relationship. In a way, our idea is the mirror image of Barzilay and Elhadad (1997), who used Wordnet to identify lexical chains that would coincide with cohesive text segments. We assumed that documents are cohesive and that co-occurrence patterns can uncover word relationships. 3 Experiments The focus of our experiment was on units of text"
C00-2104,P90-1034,0,0.0151234,"They were specifying various semantic relationships and seeking ways to collect similar pairs. We have a less restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been developed and refined. The focus of much of this work was to develop the methods themselves. Church and Hanks (1989) explored the use of mutual information statistics in ranking co-occurrences within five-word windows. Smadja (1992) gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Hatzivassiloglou and McKeown (1993) clustered adjectives into semantic classes, and Pereira et al. (1993) clustered nouns on their appearance in verb-object pairs. We are trying to be less restrictive in learning multiple salient relationships between words rather than seeking a particular relationship. In a way, our idea is the mirror image of Barzilay and Elhadad (1997), who used Wordnet to identify lexical chains that would coincide with cohesive text segments. We assumed that documents are cohesiv"
C00-2104,W99-0212,0,0.0154338,"erspectives, from information retrieval to the development of statistical methods for investigating word similarity and classification. Our efforts fall somewhere in the middle. Compared with document retrieval tasks, we are more closely focused on the words themselves and on specific concepts than on document “aboutness.” Jing and Croft (1994) examined words and phrases in paragraph units, and found that the association data improves retrieval performance. Callan (1994) compared paragraph units and fixed windows of text in examining passage-level retrieval. In the question-answering context, Morton (1999) collected document co-occurrence statistics to uncover part-whole and synonymy relationships to use in a question-answering system. The key difference here was that co-occurrence was considered on a whole-document basis. Harabagiu and Maiorano (1999) argued that indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of r"
C00-2104,P93-1024,0,0.00915503,". For more than a decade, a variety of statistical techniques have been developed and refined. The focus of much of this work was to develop the methods themselves. Church and Hanks (1989) explored the use of mutual information statistics in ranking co-occurrences within five-word windows. Smadja (1992) gathered co-occurrences within fiveword windows to find collocations, particularly in specific domains. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Hatzivassiloglou and McKeown (1993) clustered adjectives into semantic classes, and Pereira et al. (1993) clustered nouns on their appearance in verb-object pairs. We are trying to be less restrictive in learning multiple salient relationships between words rather than seeking a particular relationship. In a way, our idea is the mirror image of Barzilay and Elhadad (1997), who used Wordnet to identify lexical chains that would coincide with cohesive text segments. We assumed that documents are cohesive and that co-occurrence patterns can uncover word relationships. 3 Experiments The focus of our experiment was on units of text in which the constituents must fit together in order for the discourse"
C00-2104,W97-0313,0,0.030915,"raph units and fixed windows of text in examining passage-level retrieval. In the question-answering context, Morton (1999) collected document co-occurrence statistics to uncover part-whole and synonymy relationships to use in a question-answering system. The key difference here was that co-occurrence was considered on a whole-document basis. Harabagiu and Maiorano (1999) argued that indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction. In the same vein, researchers at Brown University (Caraballo and Charniak, 1999), (Berland and Charniak, 1999), (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different per"
C00-2104,P98-2182,0,0.013203,"indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction. In the same vein, researchers at Brown University (Caraballo and Charniak, 1999), (Berland and Charniak, 1999), (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different perspective than these lines of inquiry. They were specifying various semantic relationships and seeking ways to collect similar pairs. We have a less restrictive focus and are relying on surface syntactic information about clauses. For more than a decade, a variety of statistical techniques have been developed and refined. The focus of much of this work was to develop the methods them"
C00-2104,C96-2157,0,0.0250031,"xamining passage-level retrieval. In the question-answering context, Morton (1999) collected document co-occurrence statistics to uncover part-whole and synonymy relationships to use in a question-answering system. The key difference here was that co-occurrence was considered on a whole-document basis. Harabagiu and Maiorano (1999) argued that indexing in question answering should be based on paragraphs. One recent approach to automatic lexicon building has used seed words to build up larger sets of semantically similar words in one or more categories (Riloff and Shepherd, 1997). In addition, Strzalkowski and Wang (1996) used a bootstrapping technique to identify types of references, and Riloff and Jones (1999) adapted bootstrapping techniques to lexicon building targeted to information extraction. In the same vein, researchers at Brown University (Caraballo and Charniak, 1999), (Berland and Charniak, 1999), (Caraballo, 1999) and (Roark and Charniak, 1998) focused on target constructions, in particular complex noun phrases, and searched for information not only on identifying classes of nouns, but also hypernyms, noun specificity and meronymy. We have a different perspective than these lines of inquiry. They"
C00-2104,J93-1007,0,\N,Missing
C00-2104,C98-2177,0,\N,Missing
C04-1128,J96-2004,0,0.066509,"this writing. GR has completed work on 200 threads of which there are 81 QA threads (threads with question and answer pairs), 98 question segments, and 142 question and answer pairs. DB has completed work on 138 threads of which there are 62 QA threads, 72 question segments, and 92 question and answer pairs. We consider a segment to be a question segment if a sentence in that segment has been highlighted as a question. Similarly, we consider a segment to be an answer segment if a sentence in that segment has been paired with a question to form a question and answer pair. The kappa statistic (Carletta, 1996) for identifying question segments is 0.68, and for linking question and answer segments given a question segment is 0.81. 4.2 Features For each question segment in an email message, we make a list of candidate answer segments. This is basically just a list of original (content that is not quoted from past emails) 4 segments in all the messages in the thread subsequent to the message of the question segment. Let the thread in consideration be called t, the container message of the question segment be called mq, the container message of the candidate answer segment be called ma, the question se"
C04-1128,W01-0719,0,0.024768,"il thread and being able to associate the answers with their questions is a necessity in order to generate this type of summary. In this paper, we present our work on the detection of question and answer pairs in email conversations. The question-answer detection system we present will ultimately serve as one component of a full email summarization system, providing a portion of summary content. We developed one approach for the detection of questions in email messages, and a separate approach to detect the corresponding answers. These are described in turn below. 2 Previous and Related Work (Muresan et al., 2001) describe work on summarizing individual email messages using machine learning approaches to learn rules for salient noun phrase extraction. In contrast, our work aims at summarizing whole threads and at capturing the interactive nature of email. (Nenkova and Bagga, 2003) present work on generating extractive summaries of threads in archived discussions. A sentence from the root message and from each response to the root is extracted using ad-hoc algorithms crafted by hand. This approach works best when the subject of the root email best describes the “issue” of the thread, and when the root e"
C04-1128,N04-4027,1,\N,Missing
C04-1129,E99-1042,0,0.0328042,"our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge. 1 Introduction Syntactic simplification is an NLP task, the goal of which is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content. Text simplification is a useful task for varied reasons. Chandrasekar et al. (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. The PSET project (Carroll et al., 1999), on the other hand, focused its research on simplifying newspaper text for aphasics, who have trouble with long sentences and complicated grammatical constructs. We have previously (Siddharthan, 2002; Siddharthan, 2003) developed a shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction. In this paper, we explore the use of syntactic simplification in multi-document summarization. 1.1 Sentence Shortening for Summarization It is interesting to survey the literature in sentence shortening, a task related to syntactic simp"
C04-1129,C96-2183,0,0.412753,"summary is a reference-generation task rather than a content-selection one, and implement a baseline reference rewriting module. We perform our evaluations on the test sets from the 2003 and 2004 Document Understanding Conference and report that simplifying parentheticals results in significant improvement on the automated evaluation metric Rouge. 1 Introduction Syntactic simplification is an NLP task, the goal of which is to rewrite sentences to reduce their grammatical complexity while preserving their meaning and information content. Text simplification is a useful task for varied reasons. Chandrasekar et al. (1996) viewed text simplification as a preprocessing tool to improve the performance of their parser. The PSET project (Carroll et al., 1999), on the other hand, focused its research on simplifying newspaper text for aphasics, who have trouble with long sentences and complicated grammatical constructs. We have previously (Siddharthan, 2002; Siddharthan, 2003) developed a shallow and robust syntactic simplification system for news reports, that simplifies relative clauses, apposition and conjunction. In this paper, we explore the use of syntactic simplification in multi-document summarization. 1.1 Se"
C04-1129,grover-etal-2000-lt,0,0.0107854,"o improved content selection in summaries. We therefore also need to evaluate our summarizer. We do this in $ 3, but first we describe the summarizer in more detail. 2.2 Description of our Summarizer Our summarizer has four stages—preprocessing of original documents to remove parentheticals, clustering of the simplified sentences, selecting of one representative sentence from each cluster and deciding which of these selected sentences to incorporate in the summary. We use our syntactic simplification software (Siddharthan, 2002; Siddharthan, 2003) to remove parentheticals. It uses the LT TTT (Grover et al., 2000) for POS-tagging and simple noun-chunking. It then performs apposition and relative clause identification and attachment using shallow techniques based on local context and animacy information obtained from WordNet (Miller et al., 1993). We then cluster the simplified sentences with SimFinder (Hatzivassiloglou et al., 1999). To further tighten the clusters and ensure that their size is representative of their importance, we post-process them as follows. SimFinder implements an incremental approach to clustering. At each incremental step, the similarity of a new sentence to an existing cluster"
C04-1129,W99-0625,0,0.0695217,"at the clustering is not always accurate. Clusters can contain spurious sentences, and a cluster’s size might then exaggerate its importance. Improving the quality of the clustering can thus be expected to improve the content of the summary. We now describe our experiments on syntactic simplification and sentence clustering. Our hypothesis is that simplifying parenthetical units (relative clauses and appositives) will improve the performance of our clustering algorithm, by preventing it from clustering on the basis of background information. 2.1 Simplification and Clustering We use SimFinder (Hatzivassiloglou et al., 1999) for sentence clustering and its similarity metric to evaluate cluster quality; SimFinder outputs similarity values (simvals) between 0 and 1 for pairs of sentences, based on word overlap, synonymy and n-gram matches. We use the average of the simvals for each pair of sentences in a cluster to evaluate a quality-score for the cluster. Table 1 below shows the quality-scores averaged over all clusters when the original document set is and is not preprocessed using our syntactic simplification software (described in $ 2.2). We use 30 document sets from the 2003 Document Understanding Conference ("
C04-1129,A00-1043,0,0.0157875,", the sentence: Former Democratic National Committee finance director Richard Sullivan faced more pointed questioning from Republicans during his second day on the witness stand in the Senate’s fund-raising investigation. got shortened (with different levels of reduction) to: # Richard Sullivan Republicans Senate. # Richard Sullivan faced pointed questioning. # Richard Sullivan faced pointed questioning from Republicans during day on stand in Senate fundraising investigation. Grefenstette (1998) provided a rule based approach to telegraphic reduction of the kind illustrated above. Since then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that"
C04-1129,N03-1020,0,0.045946,"evaluation methods and also started providing participants with multiple human-written models needed for reliable evaluation. Participating generic multidocument summarizers were tested on 30 eventbased sets in 2003 and 50 sets in 2004, all 80 containing roughly 10 newswire articles each. There were four human-written summaries for each set, created for evaluation purposes. In DUC’03, the task was to generate 100 word summaries, while in DUC’04, the limit was changed to 665 bytes. 3.2 Evaluation Metric We evaluated our summarizer on the DUC test sets using the Rouge automatic scoring metric (Lin and Hovy, 2003). The experiments in Lin and Hovy (2003) show that among n-gram approaches to scoring, Rouge-1 (based on unigrams) has the highest correlation with human scores. In 2004, an additional automatic metric based on longest common subsequence was included (Rouge-L), that aims to overcome some deficiencies of Rouge-1, such as its susceptibility to ungrammatical keyword packing by dishonest summarizers2 . For our evaluations, we use the Rouge settings from DUC’04: stop words are included, words are Porter-stemmed, and all four human model summaries are used. 3.3 DUC’04 Evaluation We entered our syste"
C04-1129,W03-1101,0,0.0123843,"ince then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that statistical sentence-shortening approaches like Knight and Marcu (2000) do not improve content selection in summaries. Indeed he reported that syntax-based sentence-shortening resulted in significantly worse content selection by their extractive summarizer NeATS. Lin (2003) concluded that pure syntaxbased compression does not improve overall summarizer performance, even though the compression algorithm performs well at the sentence level. 1.2 Simplifying Syntax for Summarization A problem with using statistical sentenceshortening for summarization is that syntactic form does not a"
C04-1129,N03-1026,0,0.014123,"e: Former Democratic National Committee finance director Richard Sullivan faced more pointed questioning from Republicans during his second day on the witness stand in the Senate’s fund-raising investigation. got shortened (with different levels of reduction) to: # Richard Sullivan Republicans Senate. # Richard Sullivan faced pointed questioning. # Richard Sullivan faced pointed questioning from Republicans during day on stand in Senate fundraising investigation. Grefenstette (1998) provided a rule based approach to telegraphic reduction of the kind illustrated above. Since then, Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. These sentence-shortening approaches have been evaluated by comparison with human-shortened sentences and have been shown to compare favorably. However, the use of sentence shortening for the multi-document summarization task has been largely unexplored, even though intuitively it appears that sentence-shortening can allow more important information to be included in a summary. Recently, Lin (2003) showed that statistical sentence-s"
C04-1129,N03-2024,1,\N,Missing
C04-1129,A97-1030,0,\N,Missing
C04-1129,E99-1029,0,\N,Missing
C08-1110,N03-1003,0,0.0375553,"this approach but extends it to identify all detectable redundancies within a document set. Another approach does identify small subsentential units of information within text called “Basic Elements” and uses these for evaluating summarizations (Hovy et al., 2006). Our approach, in contrast, does not make assumptions about the size or structure of redundant information since this is uncovered through alignments. We thus require the use of an alignment algorithm to extract the common information between two pieces of text. This is related to the wellstudied problem of identifying paraphrases (Barzilay and Lee, 2003; Pang et al., 2003) and the more general variant of recognizing textual entailment, which explores whether information expressed in a hypothesis can be inferred from a given premise. Entailment problems have also been approached with a wide variety of techniques, one of which is dependency tree alignment (Marsi et al., 2006), which we utilize as well to align segments of text while respecting syntax. However, our definition of redundancy does not extend to include unidirectional entailment, and the alignment process is simply required to identify equivalent information. 3 Levels of Informatio"
C08-1110,J05-3002,1,0.873555,"snippets In order to obtain the concept graph representation, the common information between each pair of snippets in the document must first be discovered by aligning all pairs of snippets with each other. We make use of dependency parsing and alignment of dependency parse trees to obtain intersections between each pair of snippets, where each intersection may be a discontiguous span of text corresponding to an aligned subtree within each snippet. In our experiments, dependency parsing is accomplished with Minipar (Lin, 1998) and alignment is done using a bottom-up tree alignment algorithm (Barzilay and McKeown, 2005) modified to account for the shallow semantic role labels produced by the parser. The alignment implementation is not the focus of this work, however, and the framework described here could by applied using any alignment technique between segments of text in potentially any language. As seen in Figure 1, the intersections that can be extracted solely by pairwise comparisons are not unique and may contain multiple concepts. A truly information-preserving approach requires the explicit identification of concepts as in the concept graph from Figure 2, but efficiently converting the former into th"
C08-1110,C04-1057,0,0.228183,"entences that are largely repetitive; however, as noted earlier, this is not well-suited to the redundancy task. The use of sen873 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 873–880 Manchester, August 2008 tence simplification in conjunction with clustering (Siddharthan et al., 2004) could help alleviate this problem by effectively clustering smaller units, but this issue cannot be avoided unless sentences are simplified to atomic elements of information. Other research has introduced the notion of identifying concepts in the input text (Filatova and Hatzivassiloglou, 2004), using a set cover algorithm to attempt to include as many concepts as possible. However, this approach uses tf-idf to approximate concepts and thus doesn’t explicitly identify redundant text. Our work draws on this approach but extends it to identify all detectable redundancies within a document set. Another approach does identify small subsentential units of information within text called “Basic Elements” and uses these for evaluating summarizations (Hovy et al., 2006). Our approach, in contrast, does not make assumptions about the size or structure of redundant information since this is un"
C08-1110,hovy-etal-2006-automated,0,0.0633563,"ments of information. Other research has introduced the notion of identifying concepts in the input text (Filatova and Hatzivassiloglou, 2004), using a set cover algorithm to attempt to include as many concepts as possible. However, this approach uses tf-idf to approximate concepts and thus doesn’t explicitly identify redundant text. Our work draws on this approach but extends it to identify all detectable redundancies within a document set. Another approach does identify small subsentential units of information within text called “Basic Elements” and uses these for evaluating summarizations (Hovy et al., 2006). Our approach, in contrast, does not make assumptions about the size or structure of redundant information since this is uncovered through alignments. We thus require the use of an alignment algorithm to extract the common information between two pieces of text. This is related to the wellstudied problem of identifying paraphrases (Barzilay and Lee, 2003; Pang et al., 2003) and the more general variant of recognizing textual entailment, which explores whether information expressed in a hypothesis can be inferred from a given premise. Entailment problems have also been approached with a wide v"
C08-1110,N03-1024,0,0.0124077,"ds it to identify all detectable redundancies within a document set. Another approach does identify small subsentential units of information within text called “Basic Elements” and uses these for evaluating summarizations (Hovy et al., 2006). Our approach, in contrast, does not make assumptions about the size or structure of redundant information since this is uncovered through alignments. We thus require the use of an alignment algorithm to extract the common information between two pieces of text. This is related to the wellstudied problem of identifying paraphrases (Barzilay and Lee, 2003; Pang et al., 2003) and the more general variant of recognizing textual entailment, which explores whether information expressed in a hypothesis can be inferred from a given premise. Entailment problems have also been approached with a wide variety of techniques, one of which is dependency tree alignment (Marsi et al., 2006), which we utilize as well to align segments of text while respecting syntax. However, our definition of redundancy does not extend to include unidirectional entailment, and the alignment process is simply required to identify equivalent information. 3 Levels of Information In describing the"
C08-1110,D07-1043,0,0.041264,"d by sampling from distributions over these properties derived from the statistics of the actual SCU graph for that document. For evaluation, these artificial concepts are randomly mapped to SCUs using m to control the likelihood of mapping. The best scores from 100 evaluation runs were considered. The second baseline used for comparison is a clustering algorithm, since clustering is the most common approach to dealing with redundancy. For this purpose, we use a recursive spectral partitioning algorithm, a variant of spectral clustering (Shi and Malik, 2000) which obtains an average Vmeasure (Rosenberg and Hirschberg, 2007) of 0.93 when clustering just pyramid contributors labeled by their SCUs. The algorithm requires a parameter that controls the homogeneity of each cluster; we run it over the entire range of settings of this parameter. We consider the clustering that maximizes F-measure in order to avoid any uncertainty regarding optimal parameter selection and to implicitly compare our algorithm against an entire hierarchy of possible clusterings. 879 6.4 Results Table 1 shows the F1 scores over evaluation runs using the random concept assignment, clustering and concept graph techniques. These results are obt"
C08-1110,C04-1129,1,0.836037,"y preferring sentences that differ from the sentences already selected for the summary. However, this approach does not attempt to identify sub-sentential redundancy. Alternative approaches to identifying redundancy use clustering at the sentence level (Lin and Hovy, 2001) to remove sentences that are largely repetitive; however, as noted earlier, this is not well-suited to the redundancy task. The use of sen873 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 873–880 Manchester, August 2008 tence simplification in conjunction with clustering (Siddharthan et al., 2004) could help alleviate this problem by effectively clustering smaller units, but this issue cannot be avoided unless sentences are simplified to atomic elements of information. Other research has introduced the notion of identifying concepts in the input text (Filatova and Hatzivassiloglou, 2004), using a set cover algorithm to attempt to include as many concepts as possible. However, this approach uses tf-idf to approximate concepts and thus doesn’t explicitly identify redundant text. Our work draws on this approach but extends it to identify all detectable redundancies within a document set."
C08-1110,P02-1058,0,\N,Missing
C10-1129,P08-1092,0,0.0141358,"eature set that consists of interesting task-specific features. For example, they monitor the number of previously submitted edits from the same author or IP, which is a good feature to model author contribution. Their other contributions are the use of a logistic regression classifier, as well as the use of lexical features. They successfully demonstrate the use of lexical features like vulgarism frequency. Using all features, they reach an average precision of 0.83 and recall of 0.77. In addition to previous work on vandalism detection, there is also earlier work using the web for modeling. Biadsy et al. (2008) extract patterns in Wikipedia to generate biographies automatically. In their experiment, they show that when using Wikipedia as the only resource for extracting named entities and corresponding collocational patterns, although the precision is typically high, recall can be very low. For that reason, they choose to use Google to retrieve training data from the Web. In our approach, instead of using Wikipedia edits and historical revisions, we also select the Web as a resource to train our shallow syntactic and semantic models. 3 Analysis of Types of Vandalism In order to better understand the"
C10-1129,P05-1063,0,0.0900022,"kle all the above types of vandalism, and our focus is on the syntactically ill-formed and semantically illintentioned types that could not be detected by rule-based systems and straightforward lexical features. 1148 4 Our System We propose a shallow syntactic-semantic focused classification approach for vandalism detection (Table 2). In contrast to previous work, our approach concentrates on the aspect of using natural language techniques to model vandalism. Our shallow syntactic and semantic modeling approaches extend the traditional n-gram language modeling method with topic-specific ntag (Collins et al., 2005) syntax models and topic-specific syntactic n-gram semantic models. Moreover, in the Wikipedia vandalism detection task, since we do not have a sufficient amount of training data to model the topic of each edit, we propose the idea of using the Web as corpus by retrieving search engine results to learn our topic-specific n-tag syntax and syntactic n-gram semantic models. The difference between our syntactic and semantic modeling is that n-tag syntax models only model the order of sentence constituents, disregarding the corresponding words. Conversely, for our syntactic n-gram models, we do kee"
C10-1129,W00-1308,0,0.122941,"ams for Shallow Syntactic and Semantic Modeling In Figure 1, we present the overview of our approach, which uses Web-trained topic-specific training for both: (1) n-tag syntax models for shallow syntactic modeling and (2) syntactic ngram models for shallow semantic modeling. For each Wikipedia edit, we consider its title as an approximate semantic representation, using it as a query to build topic-specific models. In addition, we also use the title information to model the syntax of this topic. Given Rdiff, we produce the syntactic version of the diff-ed text using a probabilistic POS tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The edit title is extracted from the corpus (either Rnew or Rold) and is used to query multiple Web search engines in order to collect the n-tag and n-gram training data from the top-k results. Before we start training language models, we tag the top-k results using the POS tagger. Note that when modeling n-tag syntax models, it is necessary to remove all the words. With the POS-only sequences, we train topic-specific ntag models to describe the syntax of normal text on the same topic associated with this edit. With the original tagged sequences, we train syntactic n"
C10-1129,N03-1033,0,0.250537,"Semantic Modeling In Figure 1, we present the overview of our approach, which uses Web-trained topic-specific training for both: (1) n-tag syntax models for shallow syntactic modeling and (2) syntactic ngram models for shallow semantic modeling. For each Wikipedia edit, we consider its title as an approximate semantic representation, using it as a query to build topic-specific models. In addition, we also use the title information to model the syntax of this topic. Given Rdiff, we produce the syntactic version of the diff-ed text using a probabilistic POS tagger (Toutanova and Manning, 2000; Toutanova et al., 2003). The edit title is extracted from the corpus (either Rnew or Rold) and is used to query multiple Web search engines in order to collect the n-tag and n-gram training data from the top-k results. Before we start training language models, we tag the top-k results using the POS tagger. Note that when modeling n-tag syntax models, it is necessary to remove all the words. With the POS-only sequences, we train topic-specific ntag models to describe the syntax of normal text on the same topic associated with this edit. With the original tagged sequences, we train syntactic n-gram models to represent"
C10-2109,D09-1030,0,0.0349201,"Missing"
C10-2109,P06-1039,0,0.0620122,"Missing"
C10-2109,2006.eamt-1.27,0,0.47249,"Missing"
C10-2109,P08-1045,0,0.0927906,"Missing"
C10-2109,N07-1008,0,0.0127534,"guage NE recognition) were ranked highest, since our particular CLQA task is focused on NEs. The final output of the algorithm is a list of sentences with MT errors, ranked by relevance to the query and importance of the error. 949 5 Experimental Setup We begin by describing the MT systems, which motivate the need for time-constrained MT. Then we describe the CLQA task and the baseline CLQA system, and finally how the error detection algorithm is used by the CLQA system. 5.1 MT Systems Both the research and production MT systems used in our evaluation were based on Direct Translation Model 2 (Ittycheriah and Roukos, 2007), which uses a maximum entropy approach to extract minimal translation blocks (one-toM phrases with optional variable slots) and train system parameters over a large number of source- and target-language features. The research system incorporates many additional syntactic features and does a deeper (and slower) beam search, both of which cause it to be much slower than the production system. In addition, the research MT system filters the training data to match the test data, as is customary in MT evaluations, whereas the production system must be able to handle a wide range of input data. Par"
C10-2109,2007.mtsummit-papers.39,0,0.0925137,"Missing"
C10-2109,P09-2084,0,0.0487582,"Missing"
C10-2109,D08-1077,0,0.0390189,"Missing"
C10-2109,P02-1040,0,0.0858198,"Missing"
C10-2109,P09-1048,1,0.864506,"Missing"
C10-2109,D08-1027,0,0.0111806,"Missing"
C10-2109,vilar-etal-2006-error,0,0.154877,"Missing"
C10-2109,2009.eamt-1.31,0,0.0652074,"Missing"
C10-2109,W08-0301,0,0.0382452,"Missing"
C16-1207,W11-0705,1,0.550627,"time since the publication of (Jørgensen et al., 2016), we have not been able to obtain their data or system to compare to ours, which we intend to do in the future. Other research has used statistical approaches to automatically characterize dialect variation in Twitter across cities and to show how the geographical distribution of lexical variation changes over time (Eisenstein, 2015). There has been quite a bit of work examining other kinds of phenomena on Twitter; researchers have developed systems to analyze accommodation (Danescu-Niculescu-Mizil et al., 2011), sentiment analysis (e.g., (Agarwal et al., 2011; Rosenthal et al., 2015)) and clues to geolocation (Dredze et al., 2016). 3 Our Corpus 3.1 Data Collection, Corpus, and Qualitative Analysis To create our corpus, we analyzed publicially available Twitter communication from Gakirah Barnes, who became a gang member in Chicago at age 13 and was killed at age 17, as well as tweets from people who communicated with her. Barnes changed her Twitter handle to @TyquanAssassin in memory of her friend Tyquan Tyler, who was killed in 2012. She subsequently swore to avenge Tyler’s death and became a known gang leader with 9 killings to her name before sh"
C16-1207,P07-1033,0,0.0401474,"Missing"
C16-1207,R13-1026,0,0.0154157,"d network data (Radil et al., 2010) and automated the analysis of graffiti style features (Piergallini et al., 2014) to predict gang affiliation. Research has also studied the psychological impact of crime on urban populations by analyzing social media, finding that crime exposure over a year can result in negative emotion and anxiety (Valdes et al., 1 The dataset is available at http://dx.doi.org/10.7916/D84F1R07 . 2197 2015). Yet others are analyzing news reports to build a database of gun violence incidents (Pavlick and Callison-Burch, 2016). There has been work on POS tagging for Twitter (Derczynski et al., 2013; Owoputi et al., 2013), including for other languages (Rehbein, 2013). We discuss (Owoputi et al., 2013) in detail in Section 4.1 The most closely related work is (Jørgensen et al., 2016), which studies African American Vernacular English in three genres (movie scripts, lyrics, tweets). They use a very large unlabeled corpus. In contrast, we use a small labeled corpus and investigate domain adaptation using additional data. Given the short amount of time since the publication of (Jørgensen et al., 2016), we have not been able to obtain their data or system to compare to ours, which we intend"
C16-1207,N16-1122,0,0.0192531,"able to obtain their data or system to compare to ours, which we intend to do in the future. Other research has used statistical approaches to automatically characterize dialect variation in Twitter across cities and to show how the geographical distribution of lexical variation changes over time (Eisenstein, 2015). There has been quite a bit of work examining other kinds of phenomena on Twitter; researchers have developed systems to analyze accommodation (Danescu-Niculescu-Mizil et al., 2011), sentiment analysis (e.g., (Agarwal et al., 2011; Rosenthal et al., 2015)) and clues to geolocation (Dredze et al., 2016). 3 Our Corpus 3.1 Data Collection, Corpus, and Qualitative Analysis To create our corpus, we analyzed publicially available Twitter communication from Gakirah Barnes, who became a gang member in Chicago at age 13 and was killed at age 17, as well as tweets from people who communicated with her. Barnes changed her Twitter handle to @TyquanAssassin in memory of her friend Tyquan Tyler, who was killed in 2012. She subsequently swore to avenge Tyler’s death and became a known gang leader with 9 killings to her name before she was in turn shot and killed at age 17. We focus on Gakirah because she"
C16-1207,N16-1130,0,0.0184473,"l impact of crime on urban populations by analyzing social media, finding that crime exposure over a year can result in negative emotion and anxiety (Valdes et al., 1 The dataset is available at http://dx.doi.org/10.7916/D84F1R07 . 2197 2015). Yet others are analyzing news reports to build a database of gun violence incidents (Pavlick and Callison-Burch, 2016). There has been work on POS tagging for Twitter (Derczynski et al., 2013; Owoputi et al., 2013), including for other languages (Rehbein, 2013). We discuss (Owoputi et al., 2013) in detail in Section 4.1 The most closely related work is (Jørgensen et al., 2016), which studies African American Vernacular English in three genres (movie scripts, lyrics, tweets). They use a very large unlabeled corpus. In contrast, we use a small labeled corpus and investigate domain adaptation using additional data. Given the short amount of time since the publication of (Jørgensen et al., 2016), we have not been able to obtain their data or system to compare to ours, which we intend to do in the future. Other research has used statistical approaches to automatically characterize dialect variation in Twitter across cities and to show how the geographical distribution o"
C16-1207,P07-2045,0,0.00528189,"Missing"
C16-1207,N13-1039,0,0.0868543,"Missing"
C16-1207,E14-1012,0,0.394258,"Missing"
C16-1207,S13-2079,0,0.0207509,"n tweets depends on identifying the emotion expressed. We use the Dictionary of Affect in Language (DAL) in order to obtain the emotional content of individual words. The DAL is a lexicon that maps over 8000 English words to a three dimensional score. The three dimensions of this score are pleasantness; activation, which is a measure of a word’s intensity; and imagery, which is a measure of the ease with which a word can be visualized. Our system extends the DAL with WordNet in order to identify the emotional content of Standard English words in our data that do not occur in the DAL following Rosenthal and McKeown (2013). For each word that is not found in the DAL and is found in WordNet, the synonyms from the first (most common) synset are searched against the DAL. We assume that the emotion of a synonym will be similar to that of the original word. Thus, if there is a match between the synonyms and the DAL, the emotion score of the synonym is used for the original word. A more difficult task is to apply the DAL to the nonstandard English and Twitter-specific elements of the tweets. We assume each token that is not found in the DAL or WordNet is not a Standard English word. We considered various lexicons for"
C16-1207,S15-2078,0,0.0470825,"Missing"
C90-3018,P84-1055,0,0.0458839,"from the meaning of each conjunct. In our work, we use a similar approach for the definition of connectives, but, since we work on generation (as opposed to interpretation), we describe the meaning of connectives as sets of constraints that must be satisfied between the conjuncts as opposed to &quot;instructions.&quot; We use the notion of thematization procedure to account for the homogeneousness condition (of. Section 5). In this paper, we concentrate on the distinctions between similar connectives rather than on the general properties of the class of connectives. Work on the structure of discourse (Cohen, 1984, Reichman, 1985, Grosz & Sidner, 1986) has identified the role of connectives in marking structural shifts. This work generally relies on the notion that hearers maintain a discourse model (which is often represented using stacks). Connectives give instructions to the hearer on how to update the discourse model. For example, &quot; n o w &quot; (Hirschberg & Litman, 1987) can indicate that the hearer needs to push or pop the current stack of the model. When used in this manner, connectives are called &quot;cue (or clue) words.&quot; This work indicates that the role of connectives is not only to indicate a logic"
C90-3018,P90-1020,1,0.904586,"n: Propositional Content, Argumentative Derivation, Functional Status, Speech Act and Utterance Act. In a complete text generation system, the &quot;deep component ''4 given certain information to convey, decides when it is possible to make some of it implicit by using a certain thematization procedure. The effect is to remove certain discoupse entities from the propositional content to be generated. Using a non-PC thematization procedure therefore allows to implicitly discuss certain features of an utterance that may be difficult to address explicitly. The deep module we are currently developing (Elhadad, 1990a) will use politeness constraints (Brown & Levinson, 1987) to decide which thematization is most appropriate. CUE VS. NON-CUE USAGE: Thematization procedures allow us to distinguish cue and non-cue usages of connectives. When a connective links on a feature that is not the propositional content, it does not affect the truth conditions of the propositions, at least in the traditional view. This suggests that non-content linking is in some ways similar to the cue/non-cue distinction discussed in section 2. Our approach does therefore capture this distinction, but with several differences. It de"
C90-3018,J86-3001,0,0.205758,"junct. In our work, we use a similar approach for the definition of connectives, but, since we work on generation (as opposed to interpretation), we describe the meaning of connectives as sets of constraints that must be satisfied between the conjuncts as opposed to &quot;instructions.&quot; We use the notion of thematization procedure to account for the homogeneousness condition (of. Section 5). In this paper, we concentrate on the distinctions between similar connectives rather than on the general properties of the class of connectives. Work on the structure of discourse (Cohen, 1984, Reichman, 1985, Grosz & Sidner, 1986) has identified the role of connectives in marking structural shifts. This work generally relies on the notion that hearers maintain a discourse model (which is often represented using stacks). Connectives give instructions to the hearer on how to update the discourse model. For example, &quot; n o w &quot; (Hirschberg & Litman, 1987) can indicate that the hearer needs to push or pop the current stack of the model. When used in this manner, connectives are called &quot;cue (or clue) words.&quot; This work indicates that the role of connectives is not only to indicate a logical or conceptual relation, but also to"
C90-3018,P87-1023,0,0.0955341,"cedure to account for the homogeneousness condition (of. Section 5). In this paper, we concentrate on the distinctions between similar connectives rather than on the general properties of the class of connectives. Work on the structure of discourse (Cohen, 1984, Reichman, 1985, Grosz & Sidner, 1986) has identified the role of connectives in marking structural shifts. This work generally relies on the notion that hearers maintain a discourse model (which is often represented using stacks). Connectives give instructions to the hearer on how to update the discourse model. For example, &quot; n o w &quot; (Hirschberg & Litman, 1987) can indicate that the hearer needs to push or pop the current stack of the model. When used in this manner, connectives are called &quot;cue (or clue) words.&quot; This work indicates that the role of connectives is not only to indicate a logical or conceptual relation, but also to indicate the structural organization of discourse. The distinction between cue and non-cue usages is an important one, and we also attempt to capture cue usages, but the structural indication (which often has the form of just push or pop) under-constrains the choice of a cue word -. it does not control how to choose among th"
C98-1096,C94-1042,0,0.200325,"Missing"
C98-1096,W97-0210,1,0.754795,"Missing"
C98-1096,W98-0718,1,0.841534,"Subcategorizations and alternations from the lexicon are then applied at the syntactic level. After three levels of paraphrasing, each message in PlanDOC on average has over 10 paraphrases. For a specific domain such as PlanDOC, an enormous proportion of a general lexicon like the one we constructed is unrelated thus unused at all. On the other hand, domain-specific knowledge may need to be added to the lexicon. The problem of how to adapt a general lexicon to a particular application domain and merge domain ontologies with a general lexicon is out of the scope of this paper but discussed in (Jing, 1998). 612 4 Conclusion In this paper, we present research oil building a rich, large-scale, and reusable lexicon for generation by combining multiple heterogeneous finguistic resources. Novel semi-automatic transformation and integration were used in combining resources to ensure reliability of the resulting lexicon. The lexicon, together with a multilevel feedback architecture, is used in a practical generation system, PlanDOC. The application of the lexicon in a generation system such as PlanDOC has many advantages. First, paraphrasing power of the system can be greatly improved due to the intro"
C98-1096,H93-1061,0,0.0794223,"is an extensive linguistic study of diathesis alternations, which are variations in the realization of verb arguments. For example, the alternation &quot;there.insertion&quot; transforms A ship appeared on the horizon to There appeared a ship on the horizon. Knowledge of alternations facilitates the generation of paraphrases. (Levin, 1993) studies 80 alternations. 3. The COMLEX syntax dictionary (Grishm a n et al., 1994). COMLEX contains syntactic information for 38,000 English words. The information includes subcategorization and complement restrictions. 4. The Brown Corpus tagged with WordNet senses (Miller et al., 1993). The original 1As of Version 1.6, released in December 1997. Combining linguistic resources 2.2.1 M e r g i n g C O M L E X a n d E V C A Alternations involve syntactic transformations of verb arguments. T h e y are thus a means to alleviate the usual lack of alternative ways to express the same concept in current generation systems. EVCA has been designed for use by humans, not computers. We need therefore to convert the information present in Levin&apos;s book (Levin, 1993) to a format that can be automatically analyzed. We extracted the relevant information for each verb using the verb classes"
C98-2160,J90-3003,0,0.373069,"Missing"
C98-2160,P88-1023,0,0.0897645,"Missing"
C98-2160,A97-1041,1,0.843513,"tem to tailor, reorganize, or summarize lengthy database responses. For example, in our work on a multimedia generation system where speech and graphics generation techniques are used to automatically summarize patient&apos;s pre-, during, and post-, operation status to different caregivers (Dalal et al., 1996), records relevant to patient status can easily number in the thousands. Through content planning, sentence planning and lexical selection, ,the NLG component is able to provide a concise, yet inforlnative, briefing automatically through spoken and written language coordinated with graphics (McKeown et al., 1997) . Integrating language generation with speech synthesis within a Concept-to-Speech (CTS) system not only brings the individual benefits of each; as an integrated system, CTS can take advantage of the availability of rich structural intbrmation constructed by the underlying NLG component to improve the quality of synthesized speech. Together, they have the potential of generating better speech than Text-to-Speech (TTS) systems. In this paper, we present a series of experiments that use machine learning to identify correlation between intonation and features produced by a robust language genera"
C98-2160,W97-1204,1,0.845012,"Missing"
C98-2160,J94-3001,0,\N,Missing
C98-2160,J94-1002,0,\N,Missing
C98-2160,J93-4004,0,\N,Missing
C98-2160,E89-1023,0,\N,Missing
C98-2160,H90-1006,0,\N,Missing
C98-2160,W94-0302,0,\N,Missing
C98-2160,P84-1018,0,\N,Missing
C98-2160,H92-1089,0,\N,Missing
C98-2160,P96-1025,0,\N,Missing
C98-2160,W98-1415,0,\N,Missing
C98-2160,J86-3001,0,\N,Missing
C98-2160,A97-1039,0,\N,Missing
D15-1122,W07-0726,0,0.0193598,"et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 200"
D15-1122,N12-1059,0,0.0671925,"le MT engines through its hierarchical paraphrases, which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, en"
D15-1122,2010.amta-papers.9,0,0.316637,"hen et al., 2009b). One problem with these approaches is that, just with a new phrase table, existing information about word ordering present in the target hypotheses is not utilized; thus the approaches are likely to make new mistakes of word reordering which do not appear in the target hypotheses of MT engines. Huang and Papineni (2007) attacked this issue through a reordering cost function that encourages search along with decoding paths from all MT engines’ decoders. Another phrase-level combination approach relies on a lattice decoding model to carry out the combination (Feng et al 2009; Du and Way 2010; Ma and McKeown 2012). In a lattice, each edge is associated with a phrase (a single word or a sequence of words) rather than a single word. The construction of the lattice is based on the extraction of phrase pairs from word alignments between a selected best MT system hypothesis (the backbone) and the other translation hypotheses. One challenge of the lattice decoding model is that it is difficult to consider structural consensus among target hypotheses from multiple MT engines, i.e, the consensus among occurrences of discontinuous words. In this paper, we propose another phrase-level combi"
D15-1122,D13-1111,0,0.0207575,", which non-hierarchical paraphrases are not able to do. 2. It can utilize existing information about word ordering present in the target hypotheses. 3. It can retain coherence and consistency between the words in a phrase. 4. The hybrid combination architecture enables us to consider a diverse set of plausible fused translations produced by different fusing techniques. 2 Hybrid Combination Architecture In the context of system combination, discriminative reranking or post editing, MT researchers (Rosti et al., 2007a; Huang and Papineni, 2007; Devlin and Matsoukas, 2012, Matusov et al., 2008; Gimpel et al., 2013) have recently shown many positive results if more diverse translations are considered. Inspired by them, we develop a hybrid combination architecture in order to consider more diverse fused translations. We paraphrase every target hypothesis to obtain the corresponding fused translation, and then make the final selection among all fused translations through a sentence-level selection-based model, shown in Figure 1. In the architecture, different fusing techniques can be used to generate fused translations for the further sentence-level selection, enabling us to exploit more sophisticated info"
D15-1122,W11-2118,0,0.0360959,"Missing"
D15-1122,W10-1746,0,0.0204563,"target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines. 1 Introduction In the past several years, many machine translation (MT) combination approaches have been developed. Word-level combination approaches, such as the confusion network decoding model, have been quite successful (Matusov et al., 2006; Rosti et al., 2007a; He et al. 2008; Karakos et al. 2008; Chen et al. 2009a; Narsale 2010; Leusch 2011; Freitag et al. 2014). In addition to word-level combination approaches, some phrase-level combination approaches have also recently been developed; the goal is to retain coherence and consistency between the words in a phrase. The most common phrase-level combination approaches are redecoding methods: by constructing a new phrase table from each MT system’s source-to-target phrase alignments, the source sentence can also be re-decoded using the new translation table (Rosti et al., 2007b; Huang and Papineni, 2007; Chen et al., 2007; Chen et al., 2009b). One problem with these app"
D15-1122,2006.amta-papers.25,0,0.232378,"Missing"
D15-1122,D07-1029,0,\N,Missing
D15-1122,E06-1005,0,\N,Missing
D15-1122,P08-2021,0,\N,Missing
D15-1122,P07-1040,0,\N,Missing
D15-1122,D09-1115,0,\N,Missing
D15-1122,N04-1022,0,\N,Missing
D15-1122,E14-2008,0,\N,Missing
D15-1122,N07-1029,0,\N,Missing
D15-1122,P09-1106,0,\N,Missing
D15-1122,D08-1011,0,\N,Missing
D15-1122,J07-2003,0,\N,Missing
D15-1122,W09-0405,0,\N,Missing
D15-1122,W09-0407,0,\N,Missing
D15-1122,P03-1021,0,\N,Missing
D15-1230,H01-1065,1,0.603186,"at using such a model significantly improves the quality of the generated text as judged by humans. 1 Introduction Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of disco"
D15-1230,W15-4612,1,0.873288,"Missing"
D15-1230,I05-1055,0,0.0358037,"he discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and"
D15-1230,W03-2304,0,0.0272613,"model significantly improves the quality of the generated text as judged by humans. 1 Introduction Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of con"
D15-1230,P01-1023,1,0.667465,"We show that using such a model significantly improves the quality of the generated text as judged by humans. 1 Introduction Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of disco"
D15-1230,P14-1048,0,0.0319655,"03; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and discourse structure was shown to be as important for text coherence as entity-based content structure (Lin et al., 2011; Feng et al., 2014). Surprisingly, so far, discourse sequential information from existing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has not been used in generation. In this paper, we present an NLG framework that generates texts from existing semantic web ontologies. We use an n-gram model of discourse relations to perform discourse planning for these stories. Through a crowd-sourced human evaluation, we show that the"
D15-1230,C14-1089,0,0.0143704,"relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and discourse structure was shown to be as important for text coherence as entity-based content structure (Lin et al., 2011; Feng et al., 2014). Surprisingly, so far, discourse sequential information from existing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has not been used in generation. In this paper, we present an NLG framework that generates texts from existing semantic web ontologies. We use an n-gram model of discourse relations to perform discourse planning for these stories. Through a crowd-sourced human evaluation, we show that the ordering of our documents and the choice of discourse relations is significantly better when using this model. 2 Generation Framework In concept-"
D15-1230,I11-1120,0,0.0200355,"l., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and discourse structure was shown to be as important for text coherence as entity-based content structure (Lin et al., 2011; Feng et al., 2014). Surprisingly, so far, discourse sequential information from existing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has not been used in generation. In this paper, we present an NLG framework that generates texts from existing semantic web ontologies. We use an n-gram model of discourse relations to perform discourse planning for these stories. Through a crowd-sourced human evalua"
D15-1230,D13-1157,0,0.0259463,"udged by humans. 1 Introduction Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (We"
D15-1230,P03-1069,0,0.0568347,"document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Fe"
D15-1230,P11-1100,0,0.0478735,"Missing"
D15-1230,C08-2022,0,0.0251819,"tistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and discourse structure was shown to be as important for text coherence as entity-based content structure (Lin et al., 2011; Feng et al., 2014). Surprisingly, so far, discourse sequential information from existing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has not been used in generation. In this paper, we present an NLG framework that generates texts from existing semantic web ontologies. We use an n-gram model of dis"
D15-1230,C00-2093,0,0.0821168,"such as n-gram models of discourse relations learned from an annotated corpus. We show that using such a model significantly improves the quality of the generated text as judged by humans. 1 Introduction Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, resear"
D15-1230,prasad-etal-2008-penn,0,0.0683354,"mbiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambiguating adjacent relations (Wellner et al., 2006; Pitler et al., 2008). Sequential discourse information has been used successfully in discourse parsing (Ghosh et al., 2011; Feng and Hirst, 2014), and discourse structure was shown to be as important for text coherence as entity-based content structure (Lin et al., 2011; Feng et al., 2014). Surprisingly, so far, discourse sequential information from existing discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has not been used in generation. In this paper, we present an NLG framework that generates texts from existing semantic web ontologies. We use an n-gram model of discourse relations to perform discourse planning for these stories. Through a crowd-sourced human evaluation, we show that the ordering of our documents and the choice of discourse relations is significantly better when using this model. 2 Generation Framework In concept-to-text generation pipelines, discourse planning typically occurs after the content selection stage. The input, therefore, is an unordered set of messages that are"
D15-1230,P06-2103,0,0.0113466,"f the generated text as judged by humans. 1 Introduction Discourse planning is a subtask of Natural Language Generation (NLG), concerned with determining the ordering of messages in a document and the discourse relations that hold among them (Reiter and Dale, 2000). Early approaches to discourse planning used manually written rules, often based on schemas (McKeown, 1985) or on Rhetorical Structure Theory (RST) (Mann and Thompson, 1987; Hovy, 1993; Power, 2000). In the past decade, various statistical approaches have emerged (Duboue and McKeown, 2001; Dimitromanolaki and Androutsopoulos, 2003; Soricut and Marcu, 2006; Konstas and Lapata, 2013). Other relevant statistical approaches to content ordering can also be found in the summarization literature (Barzilay et al., 2001; Lapata, 2003; Bollegala et al., 2005). These approaches overwhelmingly focus on determining the best order of messages using semantic conent, while discourse relations are in most cases either determined by manuallywritten derivation rules or completely ignored. Meanwhile, researchers working on discourse relation disambiguation have observed that the sequence of discourse relations itself, independently of content, helps in disambigua"
D15-1230,W06-1317,0,0.163647,"Missing"
D15-1257,E09-1004,1,0.799315,"emantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expect the MRE to occur at an emotional peak. We use the Dictionary of Affect in Language (DAL) (Whissell, 1989), augmented with WordNet for coverage (Miller, 1995). The DAL represents lexical affect with three scores: evaluation (ee, hereafter ‘pleasantness’ to avoid confusion with Labov’s Evaluation), activation (aa, activeness), and imagery (ii, concreteness). We also use a fourth score, the activationevaluation (AE) norm, a measure of subjectivity defined by Agarwal et al. (2009): √ ee2 + aa2 norm = ii For each of these four word-level scores, we calculate a sentence-level score by averaging across the words in the sentence using the finite state machine described by Agarwal et al. We expect the sentences surrounding an MRE to be more subjective and emotional as the impact of the MRE becomes clear. We also expect a build-up in activeness and intensity, peaking at the MRE. To model change over the course of a narrative, we look for changes in the syntactic, semantic, and affectual scores. To illustrate this, Figure 1 shows the activeness and pleasantness DAL scores for"
D15-1257,P12-1091,0,0.032763,"e use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Our semantic scores are the bag-of-words cosine and the latent semantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expect the MRE to occur at an emotional peak. We use the Dictionary of Affect in Language (DAL) (Whissell, 1989), augmented with WordNet for coverage (Miller, 1995). The DAL represents lexical affect with three scores: evaluation (ee, hereafter ‘pleasantness’ to avoid confusion with Labov’s Evaluation), activation (aa, activeness), and imagery (ii, concreteness). We also use a fourth score, the activationevaluation (AE) norm, a measure of subjectivity defined by Agarwal et al. (2009): √ ee2 + aa2 norm = ii For each of these four w"
D15-1257,D09-1087,0,0.0124849,"combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 40 0.56 30 0.54 0.52 20 0.50 Performance Training Size 0.48 0.46 0 2 4 6 Training Round 8 10 0 10 Figure 2: Learning and training set size curves. Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the curThe results of the self-training experiment are shown in Table 6. We achieve the best performance, f1 = 0.635, a"
D15-1257,P03-1054,0,0.00952039,"rding to three views of narrative: syntax, semantics, and affect. Syntax. We model Polanyi’s claim that a change in formality marks the changing point by including metrics of sentential syntax; we use the syntactic complexity of a sentence as an approximation for formality. The complexity of a sentence also reflects emphasis – short, staccato sentences bear more emphasis than long, complicated ones. We use the length of the sen2152 0.6 Activeness Pleasantness MRE 0.4 0.2 DAL Score tence, the length of its verb phrase, and the ratio of these two lengths; the depth of the sentence’s parse tree (Klein and Manning, 2003), the depth of its verb phrase’s subtree, and the ratio of these two depths. We also use the average word length for the sentence and the syntactic complexity formula proposed by Botel and Granowsky (1972), which scores sentences on specific structures, such as passives, appositives, and clausal subjects. Finally, we use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases i"
D15-1257,Q13-1028,0,0.0468624,"Missing"
D15-1257,N06-1020,0,0.0184633,"n transcribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 40 0.56 30 0.54 0.52 20 0.50 Performance Training Size 0.48 0.46 0 2 4 6 Training Round 8 10 0 10 Figure 2: Learning and training set size curves. Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the curThe results of the self-train"
D15-1257,P08-2026,0,0.0161735,"t, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 40 0.56 30 0.54 0.52 20 0.50 Performance Training Size 0.48 0.46 0 2 4 6 Training Round 8 10 0 10 Figure 2: Learning and training set size curves. Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the curThe results of the self-training experiment are shown in Table 6. We achieve the best pe"
D15-1257,P09-1025,0,0.0811107,"Missing"
D15-1257,W04-2405,0,0.0141063,"gle sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 40 0.56 30 0.54 0.52 20 0.50 Performance Training Size 0.48 0.46 0 2 4 6 Training Round 8 10 0 10 Figure 2: Learning and training set size curves. Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the curThe results of the self-training experiment are shown in Table 6. We achieve the best performance, f1 = 0.635, after 9 rounds of self-training. Self-training terminated afte"
D15-1257,N13-1095,0,0.0238828,"Missing"
D15-1257,P09-1113,0,0.055834,"Missing"
D15-1257,P11-2048,0,0.056217,"Missing"
D15-1257,ouyang-mckeown-2014-towards,1,0.709045,"Missing"
D15-1257,N15-1023,0,0.0237917,"We use the length of the sen2152 0.6 Activeness Pleasantness MRE 0.4 0.2 DAL Score tence, the length of its verb phrase, and the ratio of these two lengths; the depth of the sentence’s parse tree (Klein and Manning, 2003), the depth of its verb phrase’s subtree, and the ratio of these two depths. We also use the average word length for the sentence and the syntactic complexity formula proposed by Botel and Granowsky (1972), which scores sentences on specific structures, such as passives, appositives, and clausal subjects. Finally, we use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Our semantic scores are the bag-of-words cosine and the latent semantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expe"
D15-1257,E12-1049,0,0.0263171,"Missing"
D15-1257,P07-1078,0,0.0169973,"ooking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 40 0.56 30 0.54 0.52 20 0.50 Performance Training Size 0.48 0.46 0 2 4 6 Training Round 8 10 0 10 Figure 2: Learning and training set size curves. Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the curThe results of the self-training experiment are shown in Ta"
D15-1257,W10-2606,0,0.0140661,"of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 40 0.56 30 0.54 0.52 20 0.50 Performance Training Size 0.48 0.46 0 2 4 6 Training Round 8 10 0 10 Figure 2: Learning and training set size curves. Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the curThe results of the self-training experiment are shown in Table 6. We achieve the best performance, f1 = 0.635, after 9 rounds"
D15-1257,W14-4323,0,0.127032,"Missing"
D15-1257,P13-2117,0,0.025584,"Missing"
D15-1257,P11-1055,0,\N,Missing
D15-1257,D10-1099,0,\N,Missing
D18-1208,P17-1190,0,0.0607382,"Missing"
D18-1208,K17-1045,0,0.0330819,"sequence-to-sequence model to predict which sentences to include in the summary. Subsequently, Nallapati et al. (2017) proposed a different model using word-level bidirectional RNNs along with a sentence level bidirectional RNN for predicting which sentences should be extracted. Their sentence extractor creates representations of the whole document and computes separate scores for salience, novelty, and location. These works represent the state-of-the-art for deep learningbased extractive summarization and we analyze them further in this paper. Other recent neural network approaches include, Yasunaga et al. (2017), who learn a graphconvolutional network (GCN) for multi-document summarization. They do not closely examine the choice of sentence encoder, which is one of the focuses of the present paper; rather, they study the best choice of graph structure for the GCN, which is orthogonal to this work. Non-neural network learning-based approaches have also been applied to summarization. Typically they involve learning n-gram feature weights in linear models along with other non-lexical word or structural features (Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Durrett et al., 2016). In this paper, we"
D18-1208,W14-3348,0,\N,Missing
D18-1208,W05-0908,0,\N,Missing
D18-1208,D14-1162,0,\N,Missing
D18-1208,P16-1046,0,\N,Missing
D18-1208,P16-1188,0,\N,Missing
D18-1208,E17-2008,0,\N,Missing
D18-1208,P11-1049,0,\N,Missing
D18-1208,E12-1023,0,\N,Missing
E09-1004,C90-3018,1,0.385792,"Missing"
E09-1004,P97-1023,1,0.269706,"on contextual phrasal level sentiment analysis was pioneered by Nasukawa and Yi (2003), who used manually developed patterns to identify sentiment. Their approach had high precision, but low recall. Wilson et al., (2005) also explore contextual phrasal level sentiment analysis, using a machine learning approach that is closer to the one we present. Both of these researchers also follow the traditional approach and first set priors on words using a prior polarity lexicon. Wilson et al. (2005) use a lexicon of over 8000 subjectivity clues, gathered from three sources ((Riloff and Wiebe, 2003); (Hatzivassiloglou and McKeown, 1997) and The General Inquirer2 ). Words that were not tagged as positive or negative were manually labeled. Yi et al. (2003) acquired words from GI, DAL and WordNet. From DAL, only words whose pleasantness score is one standard deviation away from the mean were used. Nasukawa as well as other researchers (Kamps and Marx, 2002)) also manually tag words with prior polarities. All of these researchers use categorical tags for prior lexical polarity; in contrast, we use quantitative scores, making it possible to use them in computation of scores for the full phrase. While Wilson et al. (2005) aim at p"
E09-1004,C04-1200,0,0.351523,"Missing"
E09-1004,W03-1014,0,0.129504,"tence sentiment. Research on contextual phrasal level sentiment analysis was pioneered by Nasukawa and Yi (2003), who used manually developed patterns to identify sentiment. Their approach had high precision, but low recall. Wilson et al., (2005) also explore contextual phrasal level sentiment analysis, using a machine learning approach that is closer to the one we present. Both of these researchers also follow the traditional approach and first set priors on words using a prior polarity lexicon. Wilson et al. (2005) use a lexicon of over 8000 subjectivity clues, gathered from three sources ((Riloff and Wiebe, 2003); (Hatzivassiloglou and McKeown, 1997) and The General Inquirer2 ). Words that were not tagged as positive or negative were manually labeled. Yi et al. (2003) acquired words from GI, DAL and WordNet. From DAL, only words whose pleasantness score is one standard deviation away from the mean were used. Nasukawa as well as other researchers (Kamps and Marx, 2002)) also manually tag words with prior polarities. All of these researchers use categorical tags for prior lexical polarity; in contrast, we use quantitative scores, making it possible to use them in computation of scores for the full phras"
E09-1004,W00-1308,0,0.0543122,"n classifying the target phrase. These include n-grams of chunks from the all sentences, minimum and maximum pleasantness scores from the chunks in the target phrase itself, and the syntactic categories that occur in the context of the target phrase. In the remainder of this section, we describe how these features are extracted. We extract unigrams, bigrams and trigrams of chunks from all the sentences. For example, we may extract a bigram from Figure 1 of [V P ]neu followed by [P P ]target neg . Similar to the lexical 3 We use the Stanford Tagger to assign parts of speech tags to sentences. (Toutanova and Manning, 2000) 4 Xuan-Hieu Phan, “CRFChunker: CRF English Phrase Chunker”, http://crfchunker.sourceforge.net/, 2006. (3) They haven’t succeeded and will never succeed in breaking the will of this valiant people. As another example, AE space scores of goodies and good turn out to be the same. What differentiates one from the another is the imagery score, which is higher for the former. Therefore, value of the norm is lower for goodies than for good. Unsurprisingly, this feature always appears in the top 10 features when the classification task contains neutral expressions as one of the classes. 5.3 Lexical F"
E09-1004,P02-1053,0,0.0268988,"Missing"
E09-1004,H05-1044,0,0.20759,"e a prior polarity lexicon of words to first set priors on target phrases and then make use of the syntactic and semantic information in and around the sentence to make the final prediction. As in earlier approaches, we also use a lexicon to set priors, but we explore new uses of a Dictionary of Affect in Language (DAL) (Whissel, 1989) extended using WordNet (Fellbaum, 1998). We augment this approach with n-gram analysis to capture the effect of context. We present a system for classification of neutral versus positive versus negative and positive versus negative polarity (as is also done by (Wilson et al., 2005)). Our approach is novel in the use of following features: Introduction Sentiment analysis is a much-researched area that deals with identification of positive, negative and neutral opinions in text. The task has evolved from document level analysis to sentence and phrasal level analysis. Whereas the former is suitable for classifying news (e.g., editorials vs. reports) into positive and negative, the latter is essential for question-answering and recommendation systems. A recommendation system, for example, must be able to recommend restaurants (or movies, books, etc.) based on a variety of f"
E09-1004,W03-1017,0,0.0681421,"5). It contains numeric scores assigned along axes of pleasantness, activeness and concreteness. We introduce a method for setting numerical priors on words using these three axes, which we refer to as a “scoring scheme” throughout the paper. This scheme has high coverage of the phrases for classification and requires no manual intervention when tagging words with prior polarities. Literature Survey The task of sentiment analysis has evolved from document level analysis (e.g., (Turney., 2002); (Pang and Lee, 2004)) to sentence level analysis (e.g., (Hu and Liu., 2004); (Kim and Hovy., 2004); (Yu and Hatzivassiloglou, 2003)). These researchers first set priors on words using a prior polarity lexicon. When classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment. Research on contextual phrasal level sentiment analysis was pioneered by Nasukawa and Yi (2003), who used manually developed patterns to identify sentiment. Their approach had high precision, but low recall. Wilson et al., (2005) also explore contextual phrasal level sentiment analysis, using a machine learning approach that is closer to the one we pr"
E09-1004,P04-1035,0,\N,Missing
elson-mckeown-2010-building,kingsbury-palmer-2002-treebank,0,\N,Missing
elson-mckeown-2010-building,W01-1605,0,\N,Missing
elson-mckeown-2010-building,P09-4003,1,\N,Missing
elson-mckeown-2010-building,P07-1060,0,\N,Missing
elson-mckeown-2010-building,prasad-etal-2008-penn,0,\N,Missing
elson-mckeown-2010-building,W05-1602,0,\N,Missing
H01-1065,P99-1071,1,0.935785,"chronological order of events and cohesion. This strategy was derived from empirical observations based on experiments asking humans to order information. Evaluation of our augmented algorithm shows a signi cant improvement of the ordering over the two naive techniques we used as baseline. 1. INTRODUCTION Multidocument summarization poses a number of new challenges over single document summarization. Researchers have already investigated issues such as identifying repetitions or contradictions across input documents and determining which information is salient enough to include in the summary [1, 3, 6, 11, 15, 19]. One issue that has received little attention is how to organize the selected information so that the output summary is coherent. Once all the relevant pieces of information have been selected across the input documents, the summarizer has to decide in which order to present them so that the whole text makes sense. In single document summarization, one possible ordering of the extracted information is provided by the input document itself. However, [10] observed that, in single document summaries written by professional summarizers, extracted sentences do not retain their precedence orders in"
H01-1065,W00-1426,0,0.0470244,"ure 8: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering. summary sentences are typically arranged in the same order that they were found in the full document (although [10] reports that human summarizers do sometimes change the original order). In multidocument summarization, the summary consists of fragments of text or sentences that were selected from di erent texts. Thus, there is no complete ordering of summary sentences that can be found in the original documents. The ordering task has been extensively investigated in the generation community [14, 17, 9, 2, 16]. One approach is top-down, using schemas [14] or plans [5] to determine the organizational structure of the text. This appproach postulates a rhetorical structure which can be used to select information from an underlying knowledge base. Because the domain is limited, an encoding can be developed of the kinds of propositional content that match rhetorical elements of the schema or plan, thereby allowing content to be selected and ordered. Rhetorical Structure Theory (RST) allows for more exibility in ordering content. The relations occur between pairs of propositions. Constraints based on int"
H01-1065,W99-0625,0,0.0137047,"nformation across documents. In the case of multidocument summarization of articles about the same event, source articles can contain both repetitions and contradictions. Extracting all the similar sentences would produce a verbose and repetitive summary, while extracting only some of the similar sentences would produce a summary biased towards some sources. MultiGen uses a comparison of extracted similar sentences to select the appropriate phrases to include in the summary and reformulates them as a new text. MultiGen consists of an analysis and a generation component. The analysis component [7] identi es units of text which convey similar information across the input documents using statistical techniques and shallow text analysis. Once similar text units are identi ed, we cluster them into themes. Themes are sets of sentences from di erent documents that contain repeated information and do not necessarily contain sentences from all the documents. For each theme, the generation component [1] identi es phrases which are in the intersection of the theme sentences, and selects them as part of the summary. The intersection sentences are then ordered to produce a coherent text. 3. NAIVE"
H01-1065,P94-1002,0,0.0176866,"1 T2 T3 are summarized by the Chronological Ordering (S1 ) or by the Augmented algorithm (S2 ). 5.1 The Algorithm Our goal is to remove dis uencies from the summary by grouping together topically related themes. This can be achieved by integrating cohesion as an additional constraint to the CO algorithm. The main technical diÆculty in incorporating cohesion in our ordering algorithm is to identify and to group topically related themes across multiple documents. In other words, given two themes, we need to determine if they belong to the same cohesion block. For a single document, segmentation [8] could be used to identify blocks, but we cannot use such a technique to identify cohesion between sentences across multiple documents. The main reason is that segmentation algorithms exploit the linear structure of an input text; in our case, we want to group together sentences belonging to di erent texts. Our solution consists of the following steps. In a preprocessing stage, we segment each input text, so that given two sentences within the same text, we can determine if they are topically related. Assume the themes A and B , where A contains sentences (A1 : : : An ), and B contains sentenc"
H01-1065,P00-1010,0,0.011804,"ished on di erent dates, and articles themselves cover events occurring over a wide range in time. Using chronological order in the summary to describe the main events helps the user understand what has happened. It seems like a natural and appropriate strategy. As mentioned earlier, in our framework, we are ordering themes; in this strategy, we therefore need to assign a date to themes. To identify the date an event occured requires a detailed interpretation of temporal references in articles. While there have been recent developments in disambiguating temporal expressions and event ordering [12], correlating events with the date on which they occurred is a hard task. In our case, we approximate the theme time by its rst publication date; that is, the rst time the theme has been reported in our set of input articles. It is an acceptable approximation for news events; the rst publication date of an event usually corresponds to its occurrence in real life. For instance, in a terrorist attack story, the theme conveying the attack itself will have a date previous to the date of the theme describing a trial following the attack. Articles released by news agencies are marked with a publicat"
H01-1065,C90-2048,0,0.210194,"ure 8: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering. summary sentences are typically arranged in the same order that they were found in the full document (although [10] reports that human summarizers do sometimes change the original order). In multidocument summarization, the summary consists of fragments of text or sentences that were selected from di erent texts. Thus, there is no complete ordering of summary sentences that can be found in the original documents. The ordering task has been extensively investigated in the generation community [14, 17, 9, 2, 16]. One approach is top-down, using schemas [14] or plans [5] to determine the organizational structure of the text. This appproach postulates a rhetorical structure which can be used to select information from an underlying knowledge base. Because the domain is limited, an encoding can be developed of the kinds of propositional content that match rhetorical elements of the schema or plan, thereby allowing content to be selected and ordered. Rhetorical Structure Theory (RST) allows for more exibility in ordering content. The relations occur between pairs of propositions. Constraints based on int"
H01-1065,J93-4004,0,0.0492256,"ure 8: Evaluation of the the Majority Ordering, the Chronological Ordering and the Augmented Ordering. summary sentences are typically arranged in the same order that they were found in the full document (although [10] reports that human summarizers do sometimes change the original order). In multidocument summarization, the summary consists of fragments of text or sentences that were selected from di erent texts. Thus, there is no complete ordering of summary sentences that can be found in the original documents. The ordering task has been extensively investigated in the generation community [14, 17, 9, 2, 16]. One approach is top-down, using schemas [14] or plans [5] to determine the organizational structure of the text. This appproach postulates a rhetorical structure which can be used to select information from an underlying knowledge base. Because the domain is limited, an encoding can be developed of the kinds of propositional content that match rhetorical elements of the schema or plan, thereby allowing content to be selected and ordered. Rhetorical Structure Theory (RST) allows for more exibility in ordering content. The relations occur between pairs of propositions. Constraints based on int"
H01-1065,W00-0403,0,0.0399355,"system has to choose in which order to present the output sentences. In this section, we describe two algorithms for ordering sentences suitable for domain independent multidocument summarization. The rst algorithm, Majority Ordering (MO), relies only on the original orders of sentences in the input documents. It is the rst solution one can think of when addressing the ordering problem. The second one, Chronological Ordering (CO) uses time related features to order sentences. We analyze this strategy because it was originally implemented in MultiGen and followed by other summarization systems [18]. In the MultiGen framework, ordering sentences is equivalent to ordering themes and we describe the algorithms in terms of themes, but the concepts can be adapted to other summarization systems such as [3]. Our evaluation shows that these methods alone do not provide an adequate strategy for ordering. 3.1 is NP-complete; this can be shown by reducing the traveling salesman problem to this problem. Despite this fact, we still can apply this ordering, because typically the length of the output summary is limited to a small number of sentences. For longer summaries, the approximation algorithm d"
H01-1065,J98-3005,1,0.821107,"chronological order of events and cohesion. This strategy was derived from empirical observations based on experiments asking humans to order information. Evaluation of our augmented algorithm shows a signi cant improvement of the ordering over the two naive techniques we used as baseline. 1. INTRODUCTION Multidocument summarization poses a number of new challenges over single document summarization. Researchers have already investigated issues such as identifying repetitions or contradictions across input documents and determining which information is salient enough to include in the summary [1, 3, 6, 11, 15, 19]. One issue that has received little attention is how to organize the selected information so that the output summary is coherent. Once all the relevant pieces of information have been selected across the input documents, the summarizer has to decide in which order to present them so that the whole text makes sense. In single document summarization, one possible ordering of the extracted information is provided by the input document itself. However, [10] observed that, in single document summaries written by professional summarizers, extracted sentences do not retain their precedence orders in"
H05-1005,W99-0625,0,\N,Missing
H05-1005,J90-1003,0,\N,Missing
H05-1005,P02-1040,0,\N,Missing
H05-1005,N04-1019,0,\N,Missing
H05-1005,W97-0704,0,\N,Missing
H05-1005,X98-1026,0,\N,Missing
H05-1005,grover-etal-2000-lt,0,\N,Missing
H05-1005,N03-1020,0,\N,Missing
H05-1031,J98-3005,1,\N,Missing
H05-1031,J98-2001,0,\N,Missing
H05-1031,N03-2024,1,\N,Missing
H05-1031,C04-1129,1,\N,Missing
H05-1031,J95-2003,0,\N,Missing
H05-1031,grover-etal-2000-lt,0,\N,Missing
H05-1031,J96-2004,0,\N,Missing
H05-1031,J86-3001,0,\N,Missing
H05-1090,W00-0405,0,0.0531833,"Missing"
H05-1090,J86-3001,0,0.0797326,"lready-seen content words to a separate threshold, T old . If the previous focus was novel, this means the focus has shifted to an old segment. 3. For any remaining sentences, the classification is based on context: (a) If the sentence does not have a sufficient number of content words, use the classification in the focus variable. This adds the sums of both new and old content words and compares that to a threshold, Tkeep . (b) If the first noun phrase is a third person personal pronoun, use the classification in the focus variable. Pronouns are known to signal that the same focus continues (Grosz and Sidner, 1986). (c) If the sentence has not met any of the above tests but has a minimum number of content words, shift the focus. If all tests above fail and there are a minimum number of content words, with a sum of Tshif t shift the focus. 4. Default This rarely occurs but the default is to continue the focus, whether novel or old. We examined the 2003 Novelty Track data and found that more than half the novel sentences appear in sequences of consecutive sentences (See Table 1). This circumstance creates an opportunity to make principled classifications on some sentences that have few, if any, clearly no"
H89-2049,J83-3003,0,\N,Missing
H89-2049,P87-1014,1,\N,Missing
H89-2049,P83-1019,0,\N,Missing
H90-1009,H89-1033,0,0.0633861,"Missing"
H90-1009,P90-1020,0,0.026989,"graphics are generated with coordinated breaks, it will be necessary to lay them out so that relationships between corresponding material in different media are clearly visible. Although COMET's current media layout component does not take these relationships into account, we have begun to design a new one that will, building on our previous work on automated layout [7]. Text Generation One focus in the text generation component has been on selection of appropriate vocabulary for the explanation. We have developed a framework for lexical choice using the Ftmctional Unification Formalism (FUF)[9, 4, 5]. In addition, we have identified how previous discourse and the underlying knowledge sources influence lexical choice and implemented these influences as part of the lexieal chooser. The lexical chooser is part of the text surface generator. It receives its input from the media coordinator and passes its output to the surface generator, which contains COMET's grammar and constructs the grammatical slxucture of the sentence. As output, the lexieal chooser produces a list of pardally specified functional descriptions (PSFDs) that are passed as input to the surface generator. Thus, a PSFD is bas"
H90-1009,E87-1001,0,\N,Missing
H90-1009,H89-2055,1,\N,Missing
H90-1086,E87-1001,0,\N,Missing
H90-1086,H89-2055,1,\N,Missing
H90-1086,P90-1020,0,\N,Missing
H92-1059,J80-1001,0,0.136959,"Missing"
H92-1059,H91-1034,0,0.0602132,"Missing"
H93-1053,J92-4003,0,0.0329406,"Missing"
H93-1053,M92-1004,0,0.0447634,"Missing"
H93-1053,J91-1001,0,\N,Missing
H93-1053,P89-1015,0,\N,Missing
H94-1027,P93-1002,0,0.0517426,"igned sentence wise and a list of collocations to be translated must be provided in the source language. Aligning the database corpus Champollion requires that the data base corpus be aligned so that sentences that are translations of one another are co-indexed. Most bilingual corpora are given as two separate (sets of) files. The problem of identifying which sentences in one language correspond to which sentences in the other is complicated by the fact that sentence order may be reversed or several sentences may translate a single sentence. Sentence alignment programs (i.e., [10], [2], [11], [1], [4]) insert identifiers before each sentence in the source and the target text so that translations are given the same identifier. For Champollion, we used corpora that had been aligned by Church&apos;s sentence alignment program [10] as our input data. Providing Champolllon with a list o f s o u r c e collocations A list of source collocations can be compiled manually by experts, but it can also be compiled automatically by tools such as Xtract [17], [12]. Xtract produces a wide range of coUocations, including flexible collocations of the type &quot;to make a decision,&quot; in which the words can be infl"
H94-1027,P93-1001,0,0.0444688,"must be aligned sentence wise and a list of collocations to be translated must be provided in the source language. Aligning the database corpus Champollion requires that the data base corpus be aligned so that sentences that are translations of one another are co-indexed. Most bilingual corpora are given as two separate (sets of) files. The problem of identifying which sentences in one language correspond to which sentences in the other is complicated by the fact that sentence order may be reversed or several sentences may translate a single sentence. Sentence alignment programs (i.e., [10], [2], [11], [1], [4]) insert identifiers before each sentence in the source and the target text so that translations are given the same identifier. For Champollion, we used corpora that had been aligned by Church&apos;s sentence alignment program [10] as our input data. Providing Champolllon with a list o f s o u r c e collocations A list of source collocations can be compiled manually by experts, but it can also be compiled automatically by tools such as Xtract [17], [12]. Xtract produces a wide range of coUocations, including flexible collocations of the type &quot;to make a decision,&quot; in which the words"
H94-1027,H91-1026,0,0.0333536,"Missing"
H94-1027,P91-1034,0,0.099152,"Missing"
H94-1027,P91-1017,0,0.0208145,"lion. 4. Related Work. The recent availabilityof large amounts of bilingualdata has attracted interest in several areas, including sentence alignment [10], [2], [11], [1], [4], word alignment [6], alignment of groups of words [3], [7], and statistical translation [8]. Of these, aligning groups of words is most similar to the work reported here, although we consider a greater variety of groups. Note that additional research using bilingual corpora is less related to ours, addressing, for example, word sense disambiguation in the source language by examining different translations in the target [9], [8]. One line of research uses statistical techniques only for machine translation [8]. Brown et. al. use a stochastic language model based on the techniques used in speech recognition [19], combined with translation probabilities compiled on the aligned corpus in order to do sentence translation. The project produces high quality 154 English advance notice additional cost apartheid ... South Africa affirmative action collective agreement free trade freer trade head office health insurance employment equity make a decision to take steps to demonstrate support French Equivalent prtvenu avance"
H94-1027,P91-1023,0,0.0252358,"corpus must be aligned sentence wise and a list of collocations to be translated must be provided in the source language. Aligning the database corpus Champollion requires that the data base corpus be aligned so that sentences that are translations of one another are co-indexed. Most bilingual corpora are given as two separate (sets of) files. The problem of identifying which sentences in one language correspond to which sentences in the other is complicated by the fact that sentence order may be reversed or several sentences may translate a single sentence. Sentence alignment programs (i.e., [10], [2], [11], [1], [4]) insert identifiers before each sentence in the source and the target text so that translations are given the same identifier. For Champollion, we used corpora that had been aligned by Church&apos;s sentence alignment program [10] as our input data. Providing Champolllon with a list o f s o u r c e collocations A list of source collocations can be compiled manually by experts, but it can also be compiled automatically by tools such as Xtract [17], [12]. Xtract produces a wide range of coUocations, including flexible collocations of the type &quot;to make a decision,&quot; in which the w"
H94-1027,P91-1022,0,0.0624651,"be aligned sentence wise and a list of collocations to be translated must be provided in the source language. Aligning the database corpus Champollion requires that the data base corpus be aligned so that sentences that are translations of one another are co-indexed. Most bilingual corpora are given as two separate (sets of) files. The problem of identifying which sentences in one language correspond to which sentences in the other is complicated by the fact that sentence order may be reversed or several sentences may translate a single sentence. Sentence alignment programs (i.e., [10], [2], [11], [1], [4]) insert identifiers before each sentence in the source and the target text so that translations are given the same identifier. For Champollion, we used corpora that had been aligned by Church&apos;s sentence alignment program [10] as our input data. Providing Champolllon with a list o f s o u r c e collocations A list of source collocations can be compiled manually by experts, but it can also be compiled automatically by tools such as Xtract [17], [12]. Xtract produces a wide range of coUocations, including flexible collocations of the type &quot;to make a decision,&quot; in which the words can be"
H94-1027,J93-1007,1,0.681072,"t that sentence order may be reversed or several sentences may translate a single sentence. Sentence alignment programs (i.e., [10], [2], [11], [1], [4]) insert identifiers before each sentence in the source and the target text so that translations are given the same identifier. For Champollion, we used corpora that had been aligned by Church&apos;s sentence alignment program [10] as our input data. Providing Champolllon with a list o f s o u r c e collocations A list of source collocations can be compiled manually by experts, but it can also be compiled automatically by tools such as Xtract [17], [12]. Xtract produces a wide range of coUocations, including flexible collocations of the type &quot;to make a decision,&quot; in which the words can be inflected, the word order might change and the number of additional words can vary. Xtract also produces compounds, such as &quot;The Dow Jones average of 30 industrial stock,&quot; which are rigid collocations. We used Xtract to produce a list of input collocations for Champollion. 2.2. Statistics used: The Dice coefficient. There are several ways to measure the correlation of two events. In information retrieval, measures such as the cosine measure, the Dice coeffi"
H94-1027,P90-1032,1,0.841389,"he fact that sentence order may be reversed or several sentences may translate a single sentence. Sentence alignment programs (i.e., [10], [2], [11], [1], [4]) insert identifiers before each sentence in the source and the target text so that translations are given the same identifier. For Champollion, we used corpora that had been aligned by Church&apos;s sentence alignment program [10] as our input data. Providing Champolllon with a list o f s o u r c e collocations A list of source collocations can be compiled manually by experts, but it can also be compiled automatically by tools such as Xtract [17], [12]. Xtract produces a wide range of coUocations, including flexible collocations of the type &quot;to make a decision,&quot; in which the words can be inflected, the word order might change and the number of additional words can vary. Xtract also produces compounds, such as &quot;The Dow Jones average of 30 industrial stock,&quot; which are rigid collocations. We used Xtract to produce a list of input collocations for Champollion. 2.2. Statistics used: The Dice coefficient. There are several ways to measure the correlation of two events. In information retrieval, measures such as the cosine measure, the Dice"
H94-1027,A88-1019,0,0.023816,"Missing"
H94-1027,J93-1004,0,\N,Missing
H94-1027,P93-1003,0,\N,Missing
I11-1032,P11-1135,0,0.0208302,"labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event identification, and improve recall as well as precision.The pseudo code of this algorithm is provided in Algorithm 1. Algorithm 1 The co-training algorithm Given: (1) a set S of labeled seed training examples; (2) a set U of unlabeled news summary examples; Initialize the iteration parameter k, pool size u, and leap si"
I11-1032,P09-1068,0,0.0250238,"ing sections detail the data, features and techniques used. 3.1 Related Work Corpora Seed Training Data and Test Data Annotation The problem of identifying and understanding events in natural language text has been explored in many ways that have produced a variety of perspectives on the challenges involved. Tasks explored have included the mapping of verblevel events into aspectual classes (Siegel and McKeown, 2000), detection of specific atomic events (Filatova and Hatzivassiloglou, 2003), supervised event classification (Bethard and Martin, 2006) and unsupervised learning of event schemas (Chambers and Jurafsky, 2009). The notion of what constitutes an event has similarly 1 Our Approach Our primary corpus of news documents and queries is derived from the DARPA Global Autonomous Language Exploitation (GALE) distillation training and evaluation sets. Fifty event queries provided for the GALE task were used for our experiments (randomly divided into 30 training queries and 20 test queries), and the set of documents was restricted to those containing at least one keyword bigram from any query in the set2 . For each query, we consider all sentences from the 2 As determined by the information-retrieval pipeline"
I11-1032,N03-2019,0,0.0913712,"Missing"
I11-1032,W06-1618,0,0.379417,"sentence “The discussions between Israelis and Palestinians follow last month’s Annapolis meeting where Israeli Prime Minister and Palestinian President Mahmoud Abbas met and agreed to try to negotiate a deal before the end of 2008.” is eventrelevant, whereas the sentence “Turkey has close ties to both Israel and the Palestinians.” is not. Yet both sentences feature the named entities from the query. The term “peace talks” does not appear in the event-relevant sentence, but is implied by “negotiate”. Previous approaches have addressed this problem using supervised learning (Mani et al., 2003; Bethard and Martin, 2006; Manshadi et al., 2008), where the relation between the query and eventrelevant sentences is learned. Still, gathering a large amount of training data is difficult and people often do not agree on what counts as relevant to an event description (Filatova and Hatzivassiloglou, 2003), making the process of manually labeling sentences as events both time consuming and expensive. This is supported by our own experiments with Amazon Mechanical Turk (AMT); we were only able to obtain 685 labeled event-relevant sentences on which 3 or more AMT users agreed after 16 days of posting. This is a strikin"
I11-1032,J00-4004,1,0.756329,"Missing"
I11-1032,W04-1000,0,0.330511,", our results show that co-training yields improvement even though are classifiers are not independent. Semi-Supervised Approach The original co-training framework (Blum and Mitchell, 1998) was introduced in the context of Web page classification where one typically has access to a limited number of labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event iden"
I11-1032,W04-2405,0,0.492392,". However, our results show that co-training yields improvement even though are classifiers are not independent. Semi-Supervised Approach The original co-training framework (Blum and Mitchell, 1998) was introduced in the context of Web page classification where one typically has access to a limited number of labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event iden"
I11-1032,H01-1056,0,0.0796951,"Missing"
I11-1032,P05-1045,0,0.00359298,"tators and robots. 5 Fleiss’ κ = 0.417, generally assumed to indicate moderate agreement between annotators 4 284 classifiers over a balanced corpus created by directly adding some summary data to the seed corpus. We observed the performance of these classifiers was poorer than that of classifiers trained only on the unaugmented seed corpus, suggesting that the assumption was too strong and that the summaries consist of both query-related and query-unrelated sentences. This supports the use of such a corpus in a semi-supervised setting. 3.2 the query. We used the Stanford Named Entity tagger (Finkel et al., 2005) to obtain named entities features from the unlabeled data. Note that the second view of our approach is clearly not independent of the first view, as the named entity matching is based on lexical matching. However, named entities are known as an informative source for selecting relevant sentences for a query since they serve as unique identifiers (Parton et al., 2008). Previous studies (Krogel and Scheffer, 2004) show that if the independence assumption of co-training approach is violated, the co-training approach can yield negative results. However, our results show that co-training yields i"
I11-1032,D08-1027,0,0.038708,"Missing"
I11-1032,rosenthal-etal-2010-towards,1,0.811558,"Missing"
I11-1032,P09-1027,0,0.0252743,"classifiers are not independent. Semi-Supervised Approach The original co-training framework (Blum and Mitchell, 1998) was introduced in the context of Web page classification where one typically has access to a limited number of labeled pages but to a potentially unlimited number of unlabeled pages. Co-training involves building two independent views (classifiers), letting them automatically label the unlabeled data and incorporating this self-labeled data into the seed training set to improve system performance. Mihalcea (2004) shows that co-training is useful in word sense disamgibuation, Wan (2009) uses SVM for cotraining and shows improvement for cross-lingual sentiment analysis, and recent research has established the effectiveness of co-training for a variety of NLP tasks (Yu and K¨ubler, 2011; Li et al., 2011; Bergsma et al., 2011). In this section, we present our co-training classifiers, as well as an algorithm that is specifically tailored to automatic event identification. The Co-training Algorithm The motivation behind our co-training algorithm is to make use of the online summarization data for event identification, and improve recall as well as precision.The pseudo code of thi"
I11-1032,W11-0323,0,0.0656265,"Missing"
I11-1032,W04-1017,0,\N,Missing
I13-1095,J90-1003,0,0.382996,"n the incoming links of the articles 7. The ratio of incoming links in the first article shared by the second article 8. The ratio of incoming links in the second article shared by the first article Features from the text of the articles For each article, we build a bag-of-words vector. These vectors are used to compute the cosine similarity between the two articles of a pair, which we use as a feature. The intuition behind this central feature is that articles having a semantic similarity will also have a higher lexical similarity. This is the same intuition behind distributional similarity (Church and Hanks, 1990), which is that terms surrounded by similar context tend to be semantically related. In this case, the context does not surround the terms but is in the body of the articles corresponding to them. Lexical similarity between Wikipedia articles has been used successfully to link articles, for example in (Yazdani and Popescu-Belis, 2010). One of the powerful aspects of Wikipedia is its hyperlink structure. Based on the simple assumption that article A links to article B only if the information in B is related to or somehow assists in understanding the information in A, the intuition is that two a"
I13-1095,D10-1107,0,0.144721,"Missing"
I13-1095,W98-0718,0,0.269191,"problem of creating a taxonomy as a classification task of the potential relations between individual Wikipedia article pairs, and show that a supervised algorithm can achieve high precision in this task with very little training data. 1 Introduction Thesauri are useful resources for many NLP applications. In particular, taxonomic thesauri which contain synonymy and hypernymy relations are important for natural language generation (NLG) systems which must make decisions regarding lexical choice and aggregation. WordNet (Fellbaum, 1998) is one such thesaurus which has many uses in generation (Jing, 1998), but its set of concepts (called synsets) is quite limited. It does not contain many domain-specific concepts, nor does it contain technical concepts that emerged very recently. This work is motivated by the needs of a NLG system in the scientific literature domain, where these missing concepts are absolutely necessary for any practical application. Our goal is to generate a thesaurus containing synonomy and hypernomy relations between scientific terms which a generation system can use to select the most appropriate term given a context. The English Wikipedia has over 4 million articles, and"
I13-1095,N07-1025,0,0.0667606,"Missing"
I13-1095,W10-0910,0,0.169935,"ere have been many attempts to extend WordNet with concepts from Wikipedia. Because WordNet has some of the properties of an ontology, most work on extending WordNet with Wikipedia concepts was in the context of creating an ontology. Although our work is different in that we focus on extending only the taxonomic relations between the terms, this related work is still very relevant. There have also been attempts to create ontologies directly from Wikipedia in various ways, and we discuss those as well. Yago (Suchanek et al., 2007) is a large ontology (over 10 million concepts) based on WordNet Syed and Finin (2010) match each Wikipedia article to a WordNet synset as a hypernymlike superclass. Their method relies on the synset-category mappings of (Ponzetto and Navigli, 2009), extending it with information obtained from the hyperlink structure of the Wikipedia articles. However, this approach is still limited by the choice of categories for each article. In addition, it does not work as well for articles with a small number of hyperlinks, which is typical of the more specialized scientific articles. There have also been attempts (Auer et al., 789 3 2007; Wu and Weld, 2008) to build ontologies from the in"
I13-1159,N03-1003,0,0.0554989,"Missing"
I13-1159,E09-1097,0,0.0221985,"Missing"
I13-1159,W02-1001,0,0.0659203,"Missing"
I13-1159,D08-1019,0,0.0746728,"Missing"
I13-1159,C10-1037,0,0.0372938,"Missing"
I13-1159,N12-1015,0,0.021611,"Missing"
I13-1159,W04-1013,0,0.0127533,"Missing"
I13-1198,J05-3002,1,0.934856,"long been a high-level goal of natural language processing. Although progress in text-to-text (T2T) generation tasks such as sentence compression and paraphrase generation has been steady, the fusion of multiple sentences offers a particularly formidable challenge. Sentence fusion refers to the task of combining two or more sentences which overlap in information content, avoiding extraneous details and preserving common information. This procedure has been observed in human summarization (Jing and McKeown, 2000) and has been shown to be a valuable component of automated summarization systems (Barzilay and McKeown, 2005). However, research in sentence fusion has long been hampered by the absence of datasets for the task, and the difficulty of generating one has cast doubt on the viability of automated fusion (Daum´e III and Marcu, 2004). This paper presents a new fusion dataset generated from existing human annotations and also introduces a discriminative T2T system that generalizes the single sentence compression approach of Thadani and McKeown (2013) to n-way sentence fusion. Our fusion dataset is constructed from evaluation data for summarization shared tasks in the Document Understanding Conference (DUC)1"
I13-1198,P99-1071,1,0.339337,"tion, recent work by Filippova (2010) has also addressed fusion—referred to as multi-sentence compression—within a cluster of sentences. 7 Table 3: Examples of system outputs for instances from the corpus. Contributors are indicated by boldfaced text spans. working with multiple input sentences. We are currently working on extending this approach to produce richer formulations of syntax that will be more appropriate for this task. 6 Related Work Sentence fusion is the general label applied to tasks which take multiple sentences as input to produce a single output sentence. Barzilay & McKeown (Barzilay et al., 1999; Barzilay and McKeown, 2005) first introduced fusion in the context of multidocument summarization as a way to better capture the information in a cluster of related sentences than just using the centroid. The fusion task has since expanded to include other forms of sentence combination, such as the merging of overlapping sentences in a multidocument context (Marsi and Krahmer, 2005; Krahmer et al., 2008; Filippova and Strube, 2008b) and the combination of two (usually contiguous) sentences from a single document (Daum´e III and Marcu, 2004; Elsner and Santhanam, 2011). Variations on the fusi"
I13-1198,P11-1049,0,0.0535733,"formulation The starting point for this work is the sequential structured transduction5 model of Thadani and McKeown (2013), originally devised for single sentence compression. This approach relies on integer linear programming (ILP) to find a globally optimal solution to generation problems involving heterogenous substructures. ILP has been used frequently in recent T2T generation systems including many for sentence fusion (Filippova and Strube, 2008b; Elsner and Santhanam, 2011), intersection (Thadani and McKeown, 2011) and compression (Clarke and Lapata, 2008; Filippova and Strube, 2008a; Berg-Kirkpatrick et al., 2011), as well as other natural language processing tasks. Although LPs with integer constraints are NP-hard in the general case, the availability of optimized general-purpose ILP solvers and the natural limits on English sentence length make ILP inference attractive for sentence-level optimization problems. Consider a single fusion instance involving k source sentences S , {S1 , . . . , Sk }. The notation FS is used to denote a fusion of the sentences in S. The inference step aims to retrieve the output sentence FS∗ that is the most likely fusion of S, i.e., the sentence that maximizes p(FS |S) or"
I13-1198,P06-4020,0,0.0211981,"n metric is n-gram F1 , used in numerous tasks and evaluation scenarios; we consider all 1 ≤ n ≤ 4. In addition, since n-gram metrics do not distinguish between content words and function words, we also include an evaluation metric that observes the precision, recall and Fmeasure of only nouns and verbs as a proxy for the informativeness of a given fusion. In addition to the direct measures discussed above, we consider syntactic metrics that act as surrogates for grammaticality. Napoles et al. (2011) indicates that F1 metrics over syntactic relations such as those produced by the RASP parser (Briscoe et al., 2006) correlate significantly with human judgments of compression quality; we expect that the same holds for our fusion scenario. Output fusions were therefore parsed with RASP as well as the Stanford dependency parser and their resulting dependency graphs were compared to those of the gold fusions. 4.3 Results Table 2 summarizes the results from the fusion experiments. We first observe that the proposed fusion system as well as the contributor+compression baseline outperform the source+compression baseline significantly on all metrics evaluated. We also observe a significant gain for the fusion sy"
I13-1198,W04-1016,0,0.408709,"Missing"
I13-1198,W11-1607,0,0.255314,"cKeown (2013) to n-way sentence fusion. Our fusion dataset is constructed from evaluation data for summarization shared tasks in the Document Understanding Conference (DUC)1 and the Text Analysis Conference (TAC).2 Specifically, we use human-generated annotations produced for the pyramid method (Nenkova et al., 2007) for summarization evaluation to produce a dataset of natural human fusions with quantifiable agreement. This offers advantages over previous datasets used for standalone English sentence fusion which contain annotator-induced noise (McKeown et al., 2010) or cannot be distributed (Elsner and Santhanam, 2011). In addition, both these datasets contain approximately 300 instances of fusion while the new dataset presented here contains 1858 instances. Crucially, this larger corpus encourages supervised approaches to sentence fusion and we leverage this to explore new strategies for the task. Previous approaches to fusion have generally relied on variations of dependency graph combination (Barzilay and McKeown, 2005; Filippova and Strube, 2008b; Elsner and Santhanam, 2011) for content selection with a separate step for linearization that is usually based on a language model (LM). In contrast, we exper"
I13-1198,W05-1612,0,0.900151,"annotators reading these summaries. Each SCU comprises a label which is a concise English sentence that states the meaning of the SCU3 and a list of contributors which are discontinuous character spans from the summary sentences—herafter referred to as source sentences—in which that SCU is realized. Table 1 contains examples of SCUs drawn from DUC 2005–2007 and TAC 2008–2011 data. Our fusion corpus is constructed by taking the source sentences of an SCU as input and the SCU labels as the gold fusion output. The fusion task posed by this corpus is similar to sentence intersection as defined by Marsi and Krahmer (2005) although it does not fit the criteria for strict intersection as addressed in Thadani and McKeown (2011) since source sentences do not always expressly mention all the information in an SCU label due to unresolved anaphora and entail3 An SCU annotation guide from DUC 2005 is available at http://www1.cs.columbia.edu/∼ani/DUC2005/ AnnotationGuide.htm. 1411 ment. The following procedure was used to extract meaningful fusion instances from the SCUs. 1. SCUs that have no more than one contributor which covers a single summary sentence are dropped. In addition, we chose to restrict the number of in"
I13-1198,P09-1039,0,0.0307329,"Missing"
I13-1198,E06-1038,0,0.336398,"s. This approach is also known as overgenerate-and-rank and is often found to be a source of errors in T2T problems (Barzilay and McKeown, 2005). Although syntactic representations are natural for assembling text across sentences, recent work in unsupervised multi-sentence fusion has shown that well-formed output can often be constructed 4 This is accomplished by removing additional contributors that share the fewest words with the SCU label. purely on the basis of adjacency relationships in a word graph (Filippova, 2010). Similarly, systems for related T2T tasks such as sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and strict sentence intersection (Thadani and McKeown, 2011) have also seen promising results by linearizing n-grams without explicitly relying on syntactic representations. Our framework takes a similar perspective and assembles output text directly from n-grams over input tokens, but we employ a discriminative structured prediction approach in which likelihood under an LM is one of many features of output quality and parameters for all features are learned from a training corpus. Moreover, rather than rely on pipelined stages to first select the output content and"
I13-1198,N10-1044,1,0.892036,"e sentence compression approach of Thadani and McKeown (2013) to n-way sentence fusion. Our fusion dataset is constructed from evaluation data for summarization shared tasks in the Document Understanding Conference (DUC)1 and the Text Analysis Conference (TAC).2 Specifically, we use human-generated annotations produced for the pyramid method (Nenkova et al., 2007) for summarization evaluation to produce a dataset of natural human fusions with quantifiable agreement. This offers advantages over previous datasets used for standalone English sentence fusion which contain annotator-induced noise (McKeown et al., 2010) or cannot be distributed (Elsner and Santhanam, 2011). In addition, both these datasets contain approximately 300 instances of fusion while the new dataset presented here contains 1858 instances. Crucially, this larger corpus encourages supervised approaches to sentence fusion and we leverage this to explore new strategies for the task. Previous approaches to fusion have generally relied on variations of dependency graph combination (Barzilay and McKeown, 2005; Filippova and Strube, 2008b; Elsner and Santhanam, 2011) for content selection with a separate step for linearization that is usually"
I13-1198,W11-1611,0,0.0215847,"Missing"
I13-1198,W11-1606,1,0.930013,"states the meaning of the SCU3 and a list of contributors which are discontinuous character spans from the summary sentences—herafter referred to as source sentences—in which that SCU is realized. Table 1 contains examples of SCUs drawn from DUC 2005–2007 and TAC 2008–2011 data. Our fusion corpus is constructed by taking the source sentences of an SCU as input and the SCU labels as the gold fusion output. The fusion task posed by this corpus is similar to sentence intersection as defined by Marsi and Krahmer (2005) although it does not fit the criteria for strict intersection as addressed in Thadani and McKeown (2011) since source sentences do not always expressly mention all the information in an SCU label due to unresolved anaphora and entail3 An SCU annotation guide from DUC 2005 is available at http://www1.cs.columbia.edu/∼ani/DUC2005/ AnnotationGuide.htm. 1411 ment. The following procedure was used to extract meaningful fusion instances from the SCUs. 1. SCUs that have no more than one contributor which covers a single summary sentence are dropped. In addition, we chose to restrict the number of input sentences to at most four4 since larger SCUs are very infrequent. 2. Although SCU descriptions are re"
I13-1198,W13-3508,1,0.218083,"procedure has been observed in human summarization (Jing and McKeown, 2000) and has been shown to be a valuable component of automated summarization systems (Barzilay and McKeown, 2005). However, research in sentence fusion has long been hampered by the absence of datasets for the task, and the difficulty of generating one has cast doubt on the viability of automated fusion (Daum´e III and Marcu, 2004). This paper presents a new fusion dataset generated from existing human annotations and also introduces a discriminative T2T system that generalizes the single sentence compression approach of Thadani and McKeown (2013) to n-way sentence fusion. Our fusion dataset is constructed from evaluation data for summarization shared tasks in the Document Understanding Conference (DUC)1 and the Text Analysis Conference (TAC).2 Specifically, we use human-generated annotations produced for the pyramid method (Nenkova et al., 2007) for summarization evaluation to produce a dataset of natural human fusions with quantifiable agreement. This offers advantages over previous datasets used for standalone English sentence fusion which contain annotator-induced noise (McKeown et al., 2010) or cannot be distributed (Elsner and Sa"
I13-1198,W08-1105,0,0.264672,"or this, we ignored SCUs without contributors that are at least half the length of their source sentences as well as SCUs whose labels are less than half the length of the smallest contributor. 4. Finally, we chose to retain only SCUs whose labels contain terms present in at least one source sentence, thus ensuring that gold fusions are reachable without paraphrasing. This yields 1858 fusion instances of which 873 have two inputs, 569 have three and 416 have four. 3 Single-stage Fusion Previous approaches to fusion have often relied on dependency graph combination (Barzilay and McKeown, 2005; Filippova and Strube, 2008b; Elsner and Santhanam, 2011) to produce an intermediate syntactic representation of the information in the sentence. Linearization of output fusions is usually performed by ranking hypotheses with a language model (LM), sometimes with language-specific heuristics to filter out illformed sentences. This approach is also known as overgenerate-and-rank and is often found to be a source of errors in T2T problems (Barzilay and McKeown, 2005). Although syntactic representations are natural for assembling text across sentences, recent work in unsupervised multi-sentence fusion has shown that well-f"
I13-1198,D08-1019,0,0.188457,"or this, we ignored SCUs without contributors that are at least half the length of their source sentences as well as SCUs whose labels are less than half the length of the smallest contributor. 4. Finally, we chose to retain only SCUs whose labels contain terms present in at least one source sentence, thus ensuring that gold fusions are reachable without paraphrasing. This yields 1858 fusion instances of which 873 have two inputs, 569 have three and 416 have four. 3 Single-stage Fusion Previous approaches to fusion have often relied on dependency graph combination (Barzilay and McKeown, 2005; Filippova and Strube, 2008b; Elsner and Santhanam, 2011) to produce an intermediate syntactic representation of the information in the sentence. Linearization of output fusions is usually performed by ranking hypotheses with a language model (LM), sometimes with language-specific heuristics to filter out illformed sentences. This approach is also known as overgenerate-and-rank and is often found to be a source of errors in T2T problems (Barzilay and McKeown, 2005). Although syntactic representations are natural for assembling text across sentences, recent work in unsupervised multi-sentence fusion has shown that well-f"
I13-1198,C10-1037,0,0.843937,"model (LM), sometimes with language-specific heuristics to filter out illformed sentences. This approach is also known as overgenerate-and-rank and is often found to be a source of errors in T2T problems (Barzilay and McKeown, 2005). Although syntactic representations are natural for assembling text across sentences, recent work in unsupervised multi-sentence fusion has shown that well-formed output can often be constructed 4 This is accomplished by removing additional contributors that share the fewest words with the SCU label. purely on the basis of adjacency relationships in a word graph (Filippova, 2010). Similarly, systems for related T2T tasks such as sentence compression (McDonald, 2006; Clarke and Lapata, 2008) and strict sentence intersection (Thadani and McKeown, 2011) have also seen promising results by linearizing n-grams without explicitly relying on syntactic representations. Our framework takes a similar perspective and assembles output text directly from n-grams over input tokens, but we employ a discriminative structured prediction approach in which likelihood under an LM is one of many features of output quality and parameters for all features are learned from a training corpus."
I13-1198,A00-2024,1,0.166624,"identified concept spans for perfect content selection. 1 Introduction Abstractive text summarization has long been a high-level goal of natural language processing. Although progress in text-to-text (T2T) generation tasks such as sentence compression and paraphrase generation has been steady, the fusion of multiple sentences offers a particularly formidable challenge. Sentence fusion refers to the task of combining two or more sentences which overlap in information content, avoiding extraneous details and preserving common information. This procedure has been observed in human summarization (Jing and McKeown, 2000) and has been shown to be a valuable component of automated summarization systems (Barzilay and McKeown, 2005). However, research in sentence fusion has long been hampered by the absence of datasets for the task, and the difficulty of generating one has cast doubt on the viability of automated fusion (Daum´e III and Marcu, 2004). This paper presents a new fusion dataset generated from existing human annotations and also introduces a discriminative T2T system that generalizes the single sentence compression approach of Thadani and McKeown (2013) to n-way sentence fusion. Our fusion dataset is c"
I13-1198,P08-2049,0,0.151272,"Missing"
I13-1198,W02-1001,0,\N,Missing
I17-1031,D10-1049,0,0.0192548,"rated text. End-to-end concept-totext systems were proposed by Galanis et al. (2009), Androutsopoulos et al. (2013) and Cimiano et al. (2013), among others. For a survey of the Work done while at Columbia University 306 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 306–315, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 3.1 history of generation from semantic web data and its difficulties, see (Bouayad-Agha et al., 2014). Generation meta-systems which can be automatically adapted to a new domain have been explored in recent years. Angeli et al. (2010) learn to make decisions about content selection and (separately) template selection from an aligned corpus of database records and text describing them. Kondadadi et al. (2013) describe a framework that learns domain-specific templates, content selection, ordering and template selection from an aligned corpus. Both approaches rely on supervised learning from an aligned corpus of data and sample texts generated from the data, which is a rare resource that does not exist for most domains. Other recent work has focused on domain adaptation for existing generation systems (as opposed to creating"
I17-1031,W10-4324,0,0.0324175,"d corpus of database records and text describing them. Kondadadi et al. (2013) describe a framework that learns domain-specific templates, content selection, ordering and template selection from an aligned corpus. Both approaches rely on supervised learning from an aligned corpus of data and sample texts generated from the data, which is a rare resource that does not exist for most domains. Other recent work has focused on domain adaptation for existing generation systems (as opposed to creating adaptable meta-systems). There has been work on adapting generated text for different user groups (Janarthanam and Lemon, 2010; Gkatzia et al., 2014); adapting summarization systems to new genres (Lloret and Boldrini, 2015); adapting dialog generation systems to new applications (Rieser and Lemon, 2011) and domains (Walker et al., 2007); and parameterizing existing handcrafted systems to increase the range of domains they can handle (Lukin et al., 2015). In comparison, hybrid C2T-T2T generation is fairly unexplored territory. One recent example is Saldanha et al. (2016), which evaluated two approaches to generating company descriptions - one with Wikipedia structured data, the other utilizing web search results - and"
I17-1031,H05-1042,0,0.0497196,"∈C |C| Domain Message Selection The set of core messages gives us the core entities which participate in the core messages. We also have the set of domain messages for the domain which are prepared (extracted from the domain corpus) ahead of time as described in Section 4. The set P of potential domain messages for generation is the subset of domain messages which contain the instance entity. In this stage of the pipeline, we select a subset of these potential domain messages to include in the generated text. To select domain messages, we utilize the energy minimization framework described by Barzilay and Lapata (2005). They describe a formulation that allows efficient optimization of what they call independent scores of content units and link scores among them through the energy minimization framework. The function to minimize is: X X X indN (p) + indS (p) + link(pi , pj ) p∈S p∈N Finally, we define indS (p) and indN (p) as ( Bp(p) × ind(p) if Bp(p) ≥ 0 indS (p) = 0 otherwise ( Bp(p) indN (p) = pi ∈S pj ∈N 10 if Bp(p) &lt; 0 0 otherwise The link scores link(pi , pj ) are defined using a type similarity score. In contrast to the individual preference scores, where we maximize the entity overlap with core messa"
I17-1031,P16-1180,1,0.855498,"me from Wikipedia. 4.1 Extracting Domain STTs and Messages Given a new domain corpus, we first extract definitional sentences: sentences in the corpus which contain an entity which is an instance of the domain. For example, in the Company Description application, in the Computer Hardware domain, definitional sentences for the entity Apple may include “Apple is an American multinational technology company” and “In 1984, Apple launched the Macintosh, the first computer to be sold without a programming language at all”. To templatize the sentence and find its paraphrases, we use the approach of (Biran et al., 2016). Each definitional sentence is parsed, and NNPs that match an entity in DBPedia become typed slots, resulting in a template and a set of entities that match the slot types. The slot types are determined in two stages - sense disambiguation and hierarchical positioning - both achieved by leveraging the DBPedia ontology in combination with vector representations. We then use the templated paraphrase detection method described in (Biran et al., 2016) to compare the template with existing STTs that match the entities’ types and relations (all of which are known from the RDF ontology). The paraphr"
I17-1031,P13-1138,0,0.0192077,"he Work done while at Columbia University 306 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 306–315, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 3.1 history of generation from semantic web data and its difficulties, see (Bouayad-Agha et al., 2014). Generation meta-systems which can be automatically adapted to a new domain have been explored in recent years. Angeli et al. (2010) learn to make decisions about content selection and (separately) template selection from an aligned corpus of database records and text describing them. Kondadadi et al. (2013) describe a framework that learns domain-specific templates, content selection, ordering and template selection from an aligned corpus. Both approaches rely on supervised learning from an aligned corpus of data and sample texts generated from the data, which is a rare resource that does not exist for most domains. Other recent work has focused on domain adaptation for existing generation systems (as opposed to creating adaptable meta-systems). There has been work on adapting generated text for different user groups (Janarthanam and Lemon, 2010; Gkatzia et al., 2014); adapting summarization sys"
I17-1031,D15-1230,1,0.935572,"s born and raised in [v2 ]”, ...} Domain message 1: ST T = ST T1 E = {Candice Bergen, Beverly Hills} ST T2 (new, no RDF relation): V = {Model, Fashion Magazine} R = {∅} L = { “[v1 ] began her career as a fashion model and appeared on the front cover of [v2 ]” } Domain message 2: ST T = ST T2 E = {Candice Bergen, Vogue Magazine} Figure 1: An example of the domain STT and message extraction process. 308 4.2 Extracting the Discourse Planning Model the estimates of nearby lengths: P#S+3 i=#S−3 E[#T |i] ˜ E[#T |#S] = 7 A discourse planning model is extracted from the domain corpus as described in (Biran and McKeown, 2015). The model provides prior and transition probabilities for the four top-level Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) discourse relations: expansion, comparison, contingency and temporal. These probabilities reflect the discourse style that characterizes the domain, and will be used in Section 5 to determine the ordering of, and relations between, generated messages. 4.3 Based on this smoothed expectation, we define the probability of a template T given a selected previous sentence S: ∆ PLM3 (T |S) = This definition is not intended to have a true probabilistic interpretation, but"
I17-1031,W15-4627,0,0.0250036,"a rare resource that does not exist for most domains. Other recent work has focused on domain adaptation for existing generation systems (as opposed to creating adaptable meta-systems). There has been work on adapting generated text for different user groups (Janarthanam and Lemon, 2010; Gkatzia et al., 2014); adapting summarization systems to new genres (Lloret and Boldrini, 2015); adapting dialog generation systems to new applications (Rieser and Lemon, 2011) and domains (Walker et al., 2007); and parameterizing existing handcrafted systems to increase the range of domains they can handle (Lukin et al., 2015). In comparison, hybrid C2T-T2T generation is fairly unexplored territory. One recent example is Saldanha et al. (2016), which evaluated two approaches to generating company descriptions - one with Wikipedia structured data, the other utilizing web search results - and determined that the best results were achieved by combining the two. However, the hybrid system in this case was only a concatenation of two independent approaches. 3 Semantic Data Structures Our main data structure is the Semantic Typed Template (STT). An STT is a tuple hV, R, Li consisting of a set of vertices labeled with ent"
I17-1031,W13-2102,0,0.0737999,"Missing"
I17-1031,prasad-etal-2008-penn,0,0.026362,"{Model, Fashion Magazine} R = {∅} L = { “[v1 ] began her career as a fashion model and appeared on the front cover of [v2 ]” } Domain message 2: ST T = ST T2 E = {Candice Bergen, Vogue Magazine} Figure 1: An example of the domain STT and message extraction process. 308 4.2 Extracting the Discourse Planning Model the estimates of nearby lengths: P#S+3 i=#S−3 E[#T |i] ˜ E[#T |#S] = 7 A discourse planning model is extracted from the domain corpus as described in (Biran and McKeown, 2015). The model provides prior and transition probabilities for the four top-level Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) discourse relations: expansion, comparison, contingency and temporal. These probabilities reflect the discourse style that characterizes the domain, and will be used in Section 5 to determine the ordering of, and relations between, generated messages. 4.3 Based on this smoothed expectation, we define the probability of a template T given a selected previous sentence S: ∆ PLM3 (T |S) = This definition is not intended to have a true probabilistic interpretation, but it preserves an order of likelihood since it increases monotonically as the length of T approaches the expected values. These mode"
I17-1031,J11-1006,0,0.0204858,"lection from an aligned corpus. Both approaches rely on supervised learning from an aligned corpus of data and sample texts generated from the data, which is a rare resource that does not exist for most domains. Other recent work has focused on domain adaptation for existing generation systems (as opposed to creating adaptable meta-systems). There has been work on adapting generated text for different user groups (Janarthanam and Lemon, 2010; Gkatzia et al., 2014); adapting summarization systems to new genres (Lloret and Boldrini, 2015); adapting dialog generation systems to new applications (Rieser and Lemon, 2011) and domains (Walker et al., 2007); and parameterizing existing handcrafted systems to increase the range of domains they can handle (Lukin et al., 2015). In comparison, hybrid C2T-T2T generation is fairly unexplored territory. One recent example is Saldanha et al. (2016), which evaluated two approaches to generating company descriptions - one with Wikipedia structured data, the other utilizing web search results - and determined that the best results were achieved by combining the two. However, the hybrid system in this case was only a concatenation of two independent approaches. 3 Semantic D"
I17-1031,W03-1016,1,0.715677,"Missing"
I17-1031,P16-2040,1,0.82725,"ng generation systems (as opposed to creating adaptable meta-systems). There has been work on adapting generated text for different user groups (Janarthanam and Lemon, 2010; Gkatzia et al., 2014); adapting summarization systems to new genres (Lloret and Boldrini, 2015); adapting dialog generation systems to new applications (Rieser and Lemon, 2011) and domains (Walker et al., 2007); and parameterizing existing handcrafted systems to increase the range of domains they can handle (Lukin et al., 2015). In comparison, hybrid C2T-T2T generation is fairly unexplored territory. One recent example is Saldanha et al. (2016), which evaluated two approaches to generating company descriptions - one with Wikipedia structured data, the other utilizing web search results - and determined that the best results were achieved by combining the two. However, the hybrid system in this case was only a concatenation of two independent approaches. 3 Semantic Data Structures Our main data structure is the Semantic Typed Template (STT). An STT is a tuple hV, R, Li consisting of a set of vertices labeled with entity types V = {v1 , . . . , vn }, a set of edges labeled with relations among the vertices R = {r1 , . . . , rm } and a"
I17-1031,W13-0108,0,0.0225862,"ard to capture with simple relations. Second, RDF data spans many domains, and presents the difficulty of handling specific domains in generation. Generally speaking, there are three approaches: domain-specific approaches (with hand-written or other rules relevant to each domain), which are not scalable; generic approaches (generating in ∗ 2 Related Work Generation from RDF data is not a new topic. Duboue and McKeown (2003) described a content selection approach for generation from RDF data. Sun and Mellish (2006) present a domain-independent approach for sentence generation from RDF triples. Duma and Klein (2013) propose an architecture for learning end-to-end generation systems from aligned RDF data and sampled generated text. End-to-end concept-totext systems were proposed by Galanis et al. (2009), Androutsopoulos et al. (2013) and Cimiano et al. (2013), among others. For a survey of the Work done while at Columbia University 306 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 306–315, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 3.1 history of generation from semantic web data and its difficulties, see (Bouayad-Agha et al., 2014). Gener"
I17-1031,E09-2005,0,0.0346761,"hes: domain-specific approaches (with hand-written or other rules relevant to each domain), which are not scalable; generic approaches (generating in ∗ 2 Related Work Generation from RDF data is not a new topic. Duboue and McKeown (2003) described a content selection approach for generation from RDF data. Sun and Mellish (2006) present a domain-independent approach for sentence generation from RDF triples. Duma and Klein (2013) propose an architecture for learning end-to-end generation systems from aligned RDF data and sampled generated text. End-to-end concept-totext systems were proposed by Galanis et al. (2009), Androutsopoulos et al. (2013) and Cimiano et al. (2013), among others. For a survey of the Work done while at Columbia University 306 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 306–315, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 3.1 history of generation from semantic web data and its difficulties, see (Bouayad-Agha et al., 2014). Generation meta-systems which can be automatically adapted to a new domain have been explored in recent years. Angeli et al. (2010) learn to make decisions about content selection and (separately"
I17-1031,W14-4422,0,0.0209773,"and text describing them. Kondadadi et al. (2013) describe a framework that learns domain-specific templates, content selection, ordering and template selection from an aligned corpus. Both approaches rely on supervised learning from an aligned corpus of data and sample texts generated from the data, which is a rare resource that does not exist for most domains. Other recent work has focused on domain adaptation for existing generation systems (as opposed to creating adaptable meta-systems). There has been work on adapting generated text for different user groups (Janarthanam and Lemon, 2010; Gkatzia et al., 2014); adapting summarization systems to new genres (Lloret and Boldrini, 2015); adapting dialog generation systems to new applications (Rieser and Lemon, 2011) and domains (Walker et al., 2007); and parameterizing existing handcrafted systems to increase the range of domains they can handle (Lukin et al., 2015). In comparison, hybrid C2T-T2T generation is fairly unexplored territory. One recent example is Saldanha et al. (2016), which evaluated two approaches to generating company descriptions - one with Wikipedia structured data, the other utilizing web search results - and determined that the be"
J00-4004,A88-1019,0,0.0214396,"Missing"
J00-4004,P93-1023,1,0.93641,"encies of additional Siegel and McKeown Improving Aspectual Classification aspectual markers. Furthermore, indicators measured over multiple clausal constituents (e.g., main verb-object pairs) alleviate verb ambiguity and sparsity and improve classification performance (Siegel 1998b). Manual analysis reveals linguistic insights. As summarized below in Section 9, our analysis reveals linguistic insights that can be used to inform future work. 6. Unsupervised Learning Unsupervised methods for clustering words have been developed that do not require manually marked examples (Hatzivassiloglou and McKeown 1993; Schfitze 1992). These methods automatically determine the number of groups and the number of verbs in each group. This section evaluates an approach to clustering verbs developed and implemented by Hatzivassiloglou, based on previous work for semantically clustering adjectives (Hatzivassiloglou and McKeown 1993; Hatzivassiloglou 1997). This system automatically places verbs into semantically related groups based on the distribution of cooccurring direct objects. Such a system avoids the need for a set of manually marked examples for the training process. Manual marking is time consuming and"
J00-4004,P95-1027,1,0.834207,"994). An overview of psycholinguistic issues behind learning for natural language problems in particular is given by Powers and Turk (1989). Models resulting from machine induction have beeen manually inspected to discover linguistic insights for disambiguating clue words (Siegel and McKeown 1994). However, machine learning techniques have not previously been applied to aspectual disambiguation. Previous efforts have applied machine induction methods to coordinate corpusbased linguistic indicators in particular, for example, to classify adjectives according to markedness (Hatzivassiloglou and McKeown 1995), to perform accent restoration (Yarowsky 1994), for sense disambiguation problems (Luk 1995), and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee 1993; Hatzivassiloglou and McKeown 1993; Schi.itze 1992). 8. Future Work Parallel bilingual corpora are potential sources of supervised examples for training and testing aspectual classification systems. For example, since many languages have explicit markings corresponding to completedness (as described in Section 2.6), the category of a clause can be determined by its translation. Additional machi"
J00-4004,C92-4177,0,0.102614,"4 Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments (Moens and Steedman 1988; Dorr 1992; Klavans 1994) and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks. Each of these applications requires the ability to reason about time. A verb's aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers (Klavans and Chodorow 1992). These frequency measures are called linguistic indicators. The choice of indicators is guided by linguistic insights that describe how the aspectual category of a clause is constrained by the presence of these modifiers. For example, an event can be placed in the progressive, as in, She was jogging, but many stative clauses cannot, e.g., *She was resembling her father (Dowty 1979). One advantage of linguistic indicators is that they can be measured automatically. However, individual linguistic indicators are predictively incomplete, and are therefore insufficient when used in isolation. As d"
J00-4004,P95-1025,0,0.0116016,"ticular is given by Powers and Turk (1989). Models resulting from machine induction have beeen manually inspected to discover linguistic insights for disambiguating clue words (Siegel and McKeown 1994). However, machine learning techniques have not previously been applied to aspectual disambiguation. Previous efforts have applied machine induction methods to coordinate corpusbased linguistic indicators in particular, for example, to classify adjectives according to markedness (Hatzivassiloglou and McKeown 1995), to perform accent restoration (Yarowsky 1994), for sense disambiguation problems (Luk 1995), and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee 1993; Hatzivassiloglou and McKeown 1993; Schi.itze 1992). 8. Future Work Parallel bilingual corpora are potential sources of supervised examples for training and testing aspectual classification systems. For example, since many languages have explicit markings corresponding to completedness (as described in Section 2.6), the category of a clause can be determined by its translation. Additional machine learning methods should be evaluated for combining linguistic indicators. For example, neu"
J00-4004,J88-2003,0,0.664692,"nation or completion point at which a new state is introduced. For example, I made afire is culminated, since a new state is introduced--something is made, whereas I gazed at the sunset is nonculminated. * Computer Science Dept., 1214 Amsterdam Ave., New York NY 10027. E-mail: evs@cs.columbia.edu t Computer Science Dept., 1214 Amsterdam Ave., New York, NY 10027. E-mail: kathy@cs.columbia.edu @ 2001 Association for Computational Linguistics Computational Linguistics Volume 26, Number 4 Aspectual classification is necessary for interpreting temporal modifiers and assessing temporal entailments (Moens and Steedman 1988; Dorr 1992; Klavans 1994) and is therefore a required component for applications that perform certain natural language interpretation, generation, summarization, information retrieval, and machine translation tasks. Each of these applications requires the ability to reason about time. A verb's aspectual category can be predicted by co-occurrence frequencies between the verb and linguistic phenomena such as the progressive tense and certain temporal modifiers (Klavans and Chodorow 1992). These frequency measures are called linguistic indicators. The choice of indicators is guided by linguistic"
J00-4004,J88-2005,0,0.884179,"input clauses have been aspectually classified. Then we describe how aspect influences the interpretation of temporal connectives and modifiers. Aspectual transformations are described, and we introduce the concept of a clause's fundamental aspectual category. Finally, we describe several natural language applications that require an aspectual classification component. 2.1 Aspectual Markers and Constraints Certain features of a clause, such as the presence of adjuncts and tense, are constrained by and contribute to the aspectual class of the clause (Vendler 1967; Dowty 1979; Pustejovsky 1991; Passonneau 1988; Klavans 1994; Resnik 1996; Olsen and Resnik 1997). Table 2 illustrates an array of linguistic constraints, as more comprehensively 597 Computational Linguistics Volume 26, Number 4 Table 2 Several aspectual markers and associated constraints on aspectual class. If a clause can occur: then it must be: with a temporal adverb (e.g., then) in progressive as a complement of force~persuade after &quot;What happened was...&quot; with a duration in-PP (e.g., in an hour) in the perfect tense Event Extended Event Event Event Culminated Event Culminated Event or State summarized by Klavans (1994) and Siegel (199"
J00-4004,P93-1024,0,0.0858689,"Missing"
J00-4004,H93-1054,0,0.0264478,"pioneered the application of statistical corpus analysis to aspectual classification by placing verbs on a scale according to the frequency with which they occur with certain aspectual markers from Table 2. This way, verbs are automatically ranked according to their &quot;degree of stativity.&quot; Machine learning has become instrumental in the development of robust natural language understanding systems in general (Cardie and Mooney 1999). For example, decision tree induction has been applied to word sense disambiguation (Black 1988), determiner prediction (Knight et al. 1995), coordination parsing (Resnik 1993), syntactic parsing (Magerman 1993), and disambiguating clue phrases (Siegel 1994; Siegel and McKeown 1994; Litman 1994). An overview of psycholinguistic issues behind learning for natural language problems in particular is given by Powers and Turk (1989). Models resulting from machine induction have beeen manually inspected to discover linguistic insights for disambiguating clue words (Siegel and McKeown 1994). However, machine learning techniques have not previously been applied to aspectual disambiguation. Previous efforts have applied machine induction methods to coordinate corpusbased lin"
J00-4004,H90-1008,0,0.11198,"ow. 598 Siegel and McKeown Improving Aspectual Classification Table 3 Several aspectual entailments. If a clause occurring: necessarily entails: then it m u s t be: in past progressive tense as argument of stopped in simple present tense past tense reading past tense reading the habitual reading Nonculminated Event Nonculminated Event or State Event 2.3 I n t e r p r e t i n g T e m p o r a l C o n n e c t i v e s a n d M o d i f i e r s Several researchers have developed models that incorporate aspectual class to assess temporal constraints between connected clauses (Hwang and Schubert 1991; Schubert and Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman 1988; Hitzeman, Moens, and Grover 1994). For example, stativity must be identified to detect temporal constraints between clauses connected with when. For example, in interpreting, (7) She h a d good strength when objectively tested. 2 the have state began before or at the beginning of the test event, and ended after or at the end of the test event: have I¸ I t2z2_ However, in interpreting, (8) Phototherapy was d i s c o n t i n u e d when the bilirubin c a m e down to 13. the discontinue event began at the end of the come event: come I I discon"
J00-4004,W97-0318,1,0.337244,"he aspectual category of a clause. For example, Passonneau (1998) describes an algorithm that depends on what is called lexical aspect, the aspectual information stored in the lexicon for each verb, and Dorr (1992) augments Jackendoff's lexical entries with aspectual information. Combining linguistic indicators with machine learning automatically produces domain-specialized aspectual lexicons. 3. Linguistic Indicators Aspectually categorizing verbs is the first step towards aspectually classifying clauses, since many clauses in certain domains can be categorized based on their main verb only (Siegel 1997, 1998b, 1999). However, the most frequent category of a verb is often domain dependent, so it is necessary to perform a specialized analysis for each domain. 3 These example sentences are modificationsof samples from the corpus of medical discharge summaries described below. 601 Computational Linguistics Volume 26, Number 4 Table 4 Fourteen linguistic indicators evaluated for aspectual classification. Linguistic Indicator frequency not or never temporal adverb no subject past/pres participle duration in-PP perfect present tense progressive manner adverb evaluation adverb past tense duration f"
J00-4004,W98-0702,1,0.466308,"ve aspectual categories. In addition to the two distinctions described in the previous section, atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) from extended events, which have a time duration (e.g., She ran to the store). Therefore, four classes of events are derived: culmination, culminated process, process, and point. These aspectual distinctions are motivated by a series of syntactic and entailment constraints described in the first three subsections below. Further cognitive and philosophical rationales behind these semantic distinctions are surveyed by Siegel (1998b). First we describe aspectual constraints that linguistically motivate the design of several of the linguistic indicators. Next we describe an array of semantic entailments and temporal constraints that can be put to use by an understanding system once input clauses have been aspectually classified. Then we describe how aspect influences the interpretation of temporal connectives and modifiers. Aspectual transformations are described, and we introduce the concept of a clause's fundamental aspectual category. Finally, we describe several natural language applications that require an aspectual"
J00-4004,P99-1015,1,0.7472,"Missing"
J00-4004,W97-0320,0,0.0549551,"Missing"
J00-4004,P94-1013,0,0.0130239,"ehind learning for natural language problems in particular is given by Powers and Turk (1989). Models resulting from machine induction have beeen manually inspected to discover linguistic insights for disambiguating clue words (Siegel and McKeown 1994). However, machine learning techniques have not previously been applied to aspectual disambiguation. Previous efforts have applied machine induction methods to coordinate corpusbased linguistic indicators in particular, for example, to classify adjectives according to markedness (Hatzivassiloglou and McKeown 1995), to perform accent restoration (Yarowsky 1994), for sense disambiguation problems (Luk 1995), and for the automatic identification of semantically related groups of words (Pereira, Tishby, and Lee 1993; Hatzivassiloglou and McKeown 1993; Schi.itze 1992). 8. Future Work Parallel bilingual corpora are potential sources of supervised examples for training and testing aspectual classification systems. For example, since many languages have explicit markings corresponding to completedness (as described in Section 2.6), the category of a clause can be determined by its translation. Additional machine learning methods should be evaluated for com"
J00-4004,W91-0222,0,\N,Missing
J00-4004,H93-1027,0,\N,Missing
J02-4001,P99-1071,1,0.535805,"Missing"
J02-4001,J96-2004,0,0.0414546,"Missing"
J02-4001,W00-0408,0,0.00656006,"Missing"
J02-4001,J02-4006,0,0.00628879,"om the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build a corpus representing the cut-and-paste process used by humans; such a corpus can then be used to train an automated summarizer. Other researchers focus on the reduction process. In an attempt to learn rules for reduction, Knight and Marcu (2000) use expectation maximization to train a system to compress the syntactic parse tree of a sentence in order to produce a shorter but 401 Computational Linguistics Volume 28, Number 4 still maximally grammatical version. Ultimately, this approach can likely be used for shortening two sentences into one, three in"
J02-4001,A97-1042,1,0.514115,"rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. Aone et al. (1999) and Lin (1999) experimented with other forms of machine learning and its effectiveness. Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measure"
J02-4001,W01-0100,0,0.107473,"ms that can automatically summarize one or more documents become increasingly desirable. Recent research has investigated types of summaries, methods to create them, and methods to evaluate them. Several evaluation competitions (in the style of the National Institute of Standards and Technology’s [NIST’s] Text Retrieval Conference [TREC]) have helped determine baseline performance levels and provide a limited set of training material. Frequent workshops and symposia reflect the ongoing interest of researchers around the world. The volume of papers edited by Mani and Maybury (1999) and a book (Mani 2001) provide good introductions to the state of the art in this rapidly evolving subfield. A summary can be loosely defined as a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc. The main goal of a summary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would no"
J02-4001,P99-1072,0,0.0527337,"Missing"
J02-4001,W97-0713,0,0.0944504,"y the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some re"
J02-4001,C94-1056,0,0.0181083,"Missing"
J02-4001,W02-0404,1,0.419938,"Missing"
J02-4001,2001.mtsummit-papers.68,0,0.0198146,"Missing"
J02-4001,W00-0403,1,0.391204,"Missing"
J02-4001,J98-3005,1,0.16809,"ems from different source documents. In an early approach to multidocument summarization, information extraction was used to facilitate the identification of similarities and differences (McKeown and Radev 1995). As for single-document summarization, this approach produces more of a briefing than a summary, as it contains only preidentified information types. Identity of slot values are used to determine when information is reliable enough to include in the summary. Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similari"
J02-4001,J02-4005,0,0.0345746,"Missing"
J02-4001,J02-4004,0,0.00917955,"Missing"
J02-4001,J02-4002,0,0.0637939,", words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some recent work (Conroy and O’Leary 2001) has turned to the use of hidden Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the probability of inclusion of a sentence in an extract depends"
J02-4001,W02-0402,0,0.0221134,"Missing"
J02-4001,J02-4003,0,0.00735213,"Missing"
J02-4001,W97-0703,0,\N,Missing
J02-4001,E99-1011,0,\N,Missing
J02-4001,P02-1058,1,\N,Missing
J02-4001,P02-1040,0,\N,Missing
J02-4001,W97-0704,1,\N,Missing
J02-4001,W02-0406,1,\N,Missing
J02-4001,X98-1024,0,\N,Missing
J02-4001,H01-1065,1,\N,Missing
J02-4001,C67-1037,0,\N,Missing
J05-3002,C02-1134,0,0.00888176,"Missing"
J05-3002,C00-1007,0,0.0163532,"2) originated during linearization of the lattice and were caused either by incompleteness of the linearizer or by suboptimal scoring of the language model. Mistakes of the first type are caused by missing rules for generating auxiliaries given node features. An example of this phenomenon is the sentence The coalition to have play a central role, which verbalizes the verb construction will have to play incorrectly. Our linearizer lacks the completeness of existing application-independent linearizers, such as the unificationbased FUF/SURGE (Elhadad and Robin 1996) and the probabilistic Fergus (Bangalore and Rambow 2000). Unfortunately, we were unable to reuse any of the existing largescale linearizers because of significant structural differences between input expected by these linearizers and the format of a fusion lattice. We are currently working on adapting Fergus for the sentence fusion task. Mistakes related to suboptimal scoring were the most common (33 out of 42); in these cases, a language model selected ill-formed sentences, assigning a worse score to a better sentence. The sentence The diplomats were given to leave the country in 10 days illustrates a suboptimal linearization of the fusion lattice"
J05-3002,N04-4001,0,0.0207226,"ved from input documents. Sentence fusion is a significant first step toward the generation of abstracts, as opposed to extracts (Borko and Bernier 1975), for multidocument summarization. Unlike extraction methods (used by the vast majority of summarization researchers), sentence fusion allows for the true synthesis of information from a set of input documents. It has been shown that combining information from several sources is a natural strategy for multidocument summarization. Analysis of human-written summaries reveals that most sentences combine information drawn from multiple documents (Banko and Vanderwende 2004). Sentence fusion achieves this goal automatically. Our evaluation shows that our approach is promising, with sentence fusion outperforming sentence extraction for the task of content selection. This article focuses on the implementation and evaluation of the sentence fusion method within the multidocument summarization system MultiGen, which daily summarizes multiple news articles on the same event as part1 of Columbia’s news browsing system Newsblaster (http://newsblaster.cs.columbia.edu/). In the next section, we provide an overview of MultiGen, focusing on components that produce input or"
J05-3002,W97-0703,1,0.234019,"ent at an army base in the area. Fusion sentence: Palestinians fired an antitank missile at a bulldozer. 300 Barzilay and McKeown Sentence Fusion for Multidocument News Summarization 2.2 Theme Selection To generate a summary of predetermined length, we induce a ranking on the themes and select the n highest.2 This ranking is based on three features of the theme: size measured as the number of sentences, similarity of sentences in a theme, and salience score. The first two of these scores are produced by Simfinder, and the salience score is computed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) as described below. Combining different rankings further filters common information in terms of salience. Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chains—sequences of semantically related words—are tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining"
J05-3002,W02-1022,1,0.538495,"Missing"
J05-3002,P01-1008,1,0.274258,"indicate the relationships between nodes). An edge is labeled by the syntactic function of the two nodes it connects (e.g., subject– verb). It is unlikely that an edge connecting a subject and verb in one sentence, for example, corresponds to an edge connecting a verb and an adjective in another sentence. The word similarity measures take into account more than word identity: They also identify pairs of paraphrases, using WordNet and a paraphrasing dictionary. We automatically constructed the paraphrasing dictionary from a large comparable news corpus using the co-training method described in Barzilay and McKeown (2001). The dictionary contains pairs of word-level paraphrases as well as phrase-level paraphrases.4 Several examples of automatically extracted paraphrases are given in Table 2. During alignment, each pair of nonidentical words that do not comprise a synset in 4 The comparable corpus and the derived dictionary are available at http://www.cs.cornell.edu/˜regina/thesis-data/comp/input/processed.tbz2 and http://www.cs.cornell.edu/˜regina/thesis-data/comp/output/comp2-ALL.txt. For details on the corpus collection and evaluation of the paraphrase quality, see Barzilay (2003). 304 Barzilay and McKeown S"
J05-3002,P99-1071,1,0.479926,"Missing"
J05-3002,J93-2003,0,0.0120045,"ficantly distorting its meaning. While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999), more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003). The summary sentences, which have been manually compressed, are aligned with the original sentences from which they were drawn. Knight and Marcu (2000) treat reduction as a translation process using a noisychannel model (Brown et al. 1993). In this model, a short (compressed) string is treated as a source, and additions to this string are considered to be noise. The probability of a source string s is computed by combining a standard probabilistic context-free grammar score, which is derived from the grammar rules that yielded tree s, and a word-bigram score, computed over the leaves of the tree. The stochastic channel model creates a large tree t from a smaller tree s by choosing an extension template for each node based on the labels of the node and its children. In the decoding stage, the system searches for the short string"
J05-3002,C96-2183,0,0.194743,"Missing"
J05-3002,N03-1004,0,0.0141413,"t collections, such as the Web, creates both problems and opportunities for natural language systems. On the one hand, the presence of numerous sources conveying the same information causes difficulties for end users of search engines and news providers; they must read the same information over and over again. On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization. A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multidocument summarization (Carbonell and Goldstein 1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy 2002). Once a system finds a set of senten"
J05-3002,J03-4003,0,0.032625,"Missing"
J05-3002,W02-2102,0,0.00743534,"s still a gap between the performance of our system and human performance. An important goal for future work on sentence fusion is to increase the flexibility of content selection and realization. We believe that the process of aligning theme sentences can be greatly improved by having the system learn the similarity function, instead of using manually assigned weights. An interesting question is how such a similarity function can be induced in an unsupervised fashion. In addition, we can improve the flexibility of the fusion algorithm by using a more powerful language model. Recent research (Daume et al. 2002) has show that syntax-based language models are more suitable for language generation tasks; the study of such models is a promising direction to explore. An important feature of the sentence fusion algorithm is its ability to generate multiple verbalizations of a given fusion lattice. In our implementation, this property is utilized only to produce grammatical texts in the changed syntactic context, but it can also be used to increase coherence of the text at the discourse level by taking context into account. In our current system, each sentence is generated in isolation, independently from"
J05-3002,H01-1065,1,0.308984,"rasing, then discuss our implementation. For the word-ordering task, we do not have to consider all the possible traversals, since the number of valid traversals is limited by ordering constraints encoded in the fusion lattice. However, the basis lattice does not uniquely determine the ordering: The placement of trees inserted in the basis lattice from other theme sentences is not restricted by the original basis tree. While the ordering of many sentence constituents is determined by their syntactic roles, some constituents, such as time, location and manner circumstantials, are free to move (Elhadad et al. 2001). Therefore, the algorithm still has to select an appropriate order from among different orders of the inserted trees. The process so far produces a sentence that can be quite different from the extracted sentence; although the basis sentences provides guidance for the generation process, constituents may be removed, added in, or reordered. Wording can also be modified during this process. Although the selection of words and phrases which appear in the basis tree is a safe choice, enriching the fusion sentence with alternative verbalizations has several benefits. In applications such as summar"
J05-3002,W96-0501,0,0.0641651,"ulldozer to build a new embankment in the area—is not a well-formed sentence; however, our language model gave it a better score than its well-formed alternatives, the second and the third sentences (see Section 4 for further discussion). Despite these shortcomings, we preferred entropy-based scoring to symbolic linearization. In the next section, we motivate our choice. 3.3.1 Statistical versus Symbolic Linearization. In the previous version of the system (Barzilay, McKeown, and Elhadad 1999), we performed linearization of a fusion dependency structure using the language generator FUF/SURGE (Elhadad and Robin 1996). As a large-scale linearizer used in many traditional semantic-to-text generation systems, FUF/SURGE could be an appealing solution to the task of surface realization. Because the input structure and the requirements on the linearizer are quite different in text-to-text generation, we had to design rules for mapping between dependency structures produced by the fusion component and FUF/SURGE input. For instance, FUF/SURGE requires that the input contain a semantic role for prepositional phrases, such as manner, purpose, or location, which is not present in our dependency representation; thus"
J05-3002,W99-0625,0,0.011143,"Missing"
J05-3002,P93-1023,1,0.139349,"nce and peer sentences were divided into clauses by the authors. The judges assessed overlap on the clause level between reference and peer sentences. The wording of the instructions was inspired by the DUC instructions for clause comparison. For each clause in the reference sentence, the judge decided whether the meaning of a corresponding clause was conveyed in a peer sentence. In addition to 0 score for no overlap and 1 for full overlap, this framework allows for partial overlap with a score of 0.5. From the overlap data, we computed weighted recall and precision based on fractional count (Hatzivassiloglou and McKeown 1993). Recall is a ratio of weighted clause overlap between a peer and a reference sentence, and the number of clauses in a reference sentence. Precision is a ratio of weighted clause overlap between a peer and a reference sentence, and the number of clauses in a peer sentence. 4.1.5 Grammaticality Assessment. Grammaticality was rated in three categories: grammatical (3), partially grammatical (2), and not grammatical (1). The judge was instructed to rate a sentence in the grammatical category if it contained no grammatical mistakes. Partially grammatical included sentences that contained at most o"
J05-3002,A00-2024,1,0.873365,"which are not part of the intersection are pruned off the basis tree. However, removing all such subtrees may result in an ungrammatical or semantically flawed sentence; for example, we might create a sentence without a subject. This overpruning may happen if either the input to the fusion algorithm is noisy or the alignment has failed to recognize similar subtrees. Therefore, we perform a more conservative pruning, deleting only the self-contained components which can be removed without leaving ungrammatical sentences. As previously observed in the literature (Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000), such components include a clause in the clause conjunction, relative clauses, and some elements within a clause (such as adverbs and prepositions). For example, this procedure transforms the lattice in Figure 6 into the pruned basis lattice shown in Figure 7 by deleting the clause the clash erupted and the verb phrase to better protect Israeli forces. These phrases are eliminated because they do not appear in the other sentences of the theme and at the same time their removal does not interfere with the well-formedness of the fusion sentence. Once these subtrees are removed, the fusion latti"
J05-3002,P95-1034,0,0.0287335,"ion algorithm needs to select among available alternatives. 311 Computational Linguistics Volume 31, Number 3 Linearization of the fusion sentence involves the selection of the best phrasing and placement of auxiliaries as well as the determination of optimal ordering. Since we do not have sufficient semantic information to perform such selection, our algorithm is driven by corpus-derived knowledge. We generate all possible sentences10 from the valid traversals of the fusion lattice and score their likelihood according to statistics derived from a corpus. This approach, originally proposed by Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used in statistical generation. We trained a trigram model with Good–Turing smoothing over 60 megabytes of news articles collected by Newsblaster using the second version CMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997). The sentence with the lowest length-normalized entropy (the best score) is selected as the verbalization of the fusion lattice. Table 4 shows several verbalizations produced by our algorithm from the central tree in Figure 7. Here, we can see that the lowestscoring sentence is both grammati"
J05-3002,lacatusu-etal-2004-multi,0,0.0494796,"Missing"
J05-3002,P98-1116,0,0.00683125,"ilable alternatives. 311 Computational Linguistics Volume 31, Number 3 Linearization of the fusion sentence involves the selection of the best phrasing and placement of auxiliaries as well as the determination of optimal ordering. Since we do not have sufficient semantic information to perform such selection, our algorithm is driven by corpus-derived knowledge. We generate all possible sentences10 from the valid traversals of the fusion lattice and score their likelihood according to statistics derived from a corpus. This approach, originally proposed by Knight and Hatzivassiloglou (1995) and Langkilde and Knight (1998), is a standard method used in statistical generation. We trained a trigram model with Good–Turing smoothing over 60 megabytes of news articles collected by Newsblaster using the second version CMU–Cambridge Statistical Language Modeling toolkit (Clarkson and Rosenfeld 1997). The sentence with the lowest length-normalized entropy (the best score) is selected as the verbalization of the fusion lattice. Table 4 shows several verbalizations produced by our algorithm from the central tree in Figure 7. Here, we can see that the lowestscoring sentence is both grammatical and concise. Table 4 also il"
J05-3002,N03-1020,0,0.162156,"this function is presented in the Appendix. In the resulting tree mapping, the pairs of nodes whose NodeSimilarity positively contributed to the alignment are considered parallel. Figure 5 shows two dependency trees and their alignment. As is evident from the Sim definition, we are considering only one-to-one node “matchings”: Every node in one tree is mapped to at most one node in another tree. This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the fusion task, when tested against two reference outputs. This is to be expected: As lexical variability across input sentences grows, the number of possible ways to fuse them by machine as well by human also grows. The accuracy of match between the system output and the reference sentences largely depends on the features of the input sentences, rather than on the underlying fusion method. 6 Pairs of phrases that form an entry in the paraphrasing dictionary are compared as pairs of atomic entries. 306 Barzilay and McKeown Sen"
J05-3002,P99-1072,0,0.0488868,"Missing"
J05-3002,C96-1078,0,0.0688245,"Missing"
J05-3002,N03-2024,1,0.59867,"evel by taking context into account. In our current system, each sentence is generated in isolation, independently from what is said before and what will be said after. Clear evidence of the limitation of this approach is found in the selection of referring expressions. For example, all summary sentences may contain the full description of a named entity (e.g., President of Columbia University Lee Bollinger), while the use of shorter descriptions such as Bollinger or anaphoric expressions in some summary sentences would increase the summary’s readability (Schiffman, Nenkova, and McKeown 2002; Nenkova and McKeown 2003). These constraints can be incorporated into the sentence fusion algorithm, since our alignment-based representation of themes often contains several alternative descriptions of the same object. Beyond the problem of referring-expression generation, we found that by selecting appropriate paraphrases of each summary sentence, we can significantly improve the coherence of an output summary. An important research direction for future work is to develop a probabilistic text model that can capture properties of well-formed texts, just as a language model captures properties of sentence grammaticali"
J05-3002,N03-1024,0,0.014046,"Missing"
J05-3002,P02-1040,0,0.0809299,"f input trees. The pseudocode of this function is presented in the Appendix. In the resulting tree mapping, the pairs of nodes whose NodeSimilarity positively contributed to the alignment are considered parallel. Figure 5 shows two dependency trees and their alignment. As is evident from the Sim definition, we are considering only one-to-one node “matchings”: Every node in one tree is mapped to at most one node in another tree. This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the fusion task, when tested against two reference outputs. This is to be expected: As lexical variability across input sentences grows, the number of possible ways to fuse them by machine as well by human also grows. The accuracy of match between the system output and the reference sentences largely depends on the features of the input sentences, rather than on the underlying fusion method. 6 Pairs of phrases that form an entry in the paraphrasing dictionary are compared as pairs of atomic entries"
J05-3002,W00-0403,0,0.0109965,"Missing"
J05-3002,J98-3005,1,0.416226,"the input documents and can synthesize information across sources. 1. Introduction Redundancy in large text collections, such as the Web, creates both problems and opportunities for natural language systems. On the one hand, the presence of numerous sources conveying the same information causes difficulties for end users of search engines and news providers; they must read the same information over and over again. On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). Clearly, it would be highly desirable to have a mechanism that could identify common information among multiple related documents and fuse it into a coherent text. In this article, we present a method for sentence fusion that exploits redundancy to achieve this task in the context of multidocument summarization. A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multidocument summarization (Carbonell and Goldstein 1998; Rade"
J05-3002,A00-1021,0,0.0132615,"Missing"
J05-3002,N03-1026,0,0.0113693,"the modifier is dropped, we get an anomaly. The second, more unusual problem is in the equation of Clinton/Dole, Dole/Clinton, and Clinton and Dole. 5. Related Work 5.1 Text-to-Text Generation Unlike traditional concept-to-text generation approaches, text-to-text generation methods take text as input and transform it into a new text satisfying some constraints (e.g., length or level of sophistication). In addition to sentence fusion, compression algorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003) and methods for expansion of a multiparallel corpus (Pang, Knight, and Marcu 2003) are other instances of such methods. Compression methods have been developed for single-document summarization, and they aim to reduce a sentence by eliminating constituents which are not crucial for understanding the sentence and not salient enough to include in the summary. These approaches are based on the observation that the “importance” of a sentence constituent can often be determined based on shallow features, such as its syntactic role and the words it contains. For example, in many cases a relative cl"
J05-3002,N03-1029,0,0.0150977,"procedure), the judge was asked to ignore punctuation. 4.2 Data To evaluate our sentence fusion algorithm, we selected 100 themes following the procedure described in the previous section. Each set varied from three to seven sentences, 13 We were unable to develop a set of rules which works in most cases. Punctuation placement is determined by a variety of features; considering all possible interactions of these features is hard. We believe that corpus-based algorithms for automatic restoration of punctuation developed for speech recognition applications (Beeferman, Berger, and Lafferty 1998; Shieber and Tao 2003) could help in our task, and we plan to experiment with them in the future. 315 Computational Linguistics Volume 31, Number 3 with 4.22 sentences on average. The generated fusion sentences consisted of 1.91 clauses on average. None of the sentences in the test set were fully extracted; on average, each sentence fused fragments from 2.14 theme sentences. Out of 100 sentence, 57 sentences produced by the algorithm combined phrases from several sentences, while the rest of the sentences comprised subsequences of one of the theme sentences. (Note that compression is different from sentence extract"
J05-3002,J02-4004,0,0.00615464,"s of salience. Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chains—sequences of semantically related words—are tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining which sentences are important for single-document summarization (Barzilay and Elhadad 1997; Silber and McCoy 2002). In the multidocument scenario, lexical chains can be adapted for theme ranking based on the salience of theme sentences within their original documents. Specifically, a theme that has many sentences ranked high by lexical chains as important for a single-document summary is, in turn, given a higher salience score for the multidocument summary. In our implementation, a salience score for a theme is computed as the sum of lexical chain scores of each sentence in a theme. 2.3 Theme Ordering Once we filter out the themes that have a low rank, the next task is to order the selected themes into co"
J05-3002,J91-1002,0,\N,Missing
J05-3002,P02-1058,0,\N,Missing
J05-3002,C98-1112,0,\N,Missing
J11-4007,W08-1107,0,0.0255227,"Missing"
J11-4007,J05-3002,1,0.864994,"eport an evaluation of the effect of reference rewriting on summary quality in Section 6, including a discussion of its scope and limitations in Section 6.2. 812 Siddharthan, Nenkova, and McKeown Information Status and References to People 2. Related Work Related research into summarization, information status distinctions, and generating referring expressions is reviewed here. 2.1 Extractive and Abstractive Summarization Multi-document summarization has been an active area of research over the past two decades and yet, barring a few exceptions (Radev and McKeown 1998; Daum´e III et al. 2002; Barzilay and McKeown 2005), most systems still use shallow features to produce an extractive summary, an age-old technique (Luhn 1958) that has well-known problems. Extractive summaries may contain phrases that the reader cannot understand out of context (Paice 1990) and irrelevant phrases that happen to occur in a relevant sentence (Knight and Marcu 2000; Barzilay 2003). Referring expressions in extractive summaries illustrate this, as sentences compiled from different documents might contain too little, too much, or repeated information about the referent. In a study of how summary revisions could be used to improve"
J11-4007,W09-2817,0,0.0534453,"Missing"
J11-4007,W07-2302,0,0.0297396,"stance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff 2005) have not been used for generation or summarization tasks. Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, and Viethen 2009). The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both ﬁrst and subsequent mentions of the main subject of the article, consists of 2,000 texts in ﬁve different domains (cities, countries, rivers, people, and mountains). The more recent GREC-People corpus consists of 1,000 texts in just one domain (people) but references to all people mentioned in a text have been annotated. The GREC challenges require systems to pick the most appropriate reference in context from a list of all references in"
J11-4007,J96-2004,0,0.191249,"Missing"
J11-4007,A00-2018,0,0.0190877,"containing 651,000 words and coming from 876 news reports from six different news agencies. The variety of sources is important because working with text from a single source could lead to the learning of paper-speciﬁc editorial rules. The reference characteristics we were interested in were number of pre-modiﬁers, presence and type of post-modiﬁers, and the form of name used to refer to people. The corpus was automatically annotated for person name occurrence and co-reference using Nominator (Wacholder, Ravin, and Choi 1997). Syntactic form of references was obtained using Charniak’s parser (Charniak 2000). This automatically annotated corpus contains references to 6,240 distinct people. The distribution of forms for discourse-new and discourse-old references are shown in Table 1. For discourse-old references, computing the probability of a syntactic realization is not as straightforward as for discourse-new references, because the form of the reference is inﬂuenced by the form of previous references, among other factors. To capture this relationship, we used the data from discourse-old mentions to form a Markov chain, which captures exactly the probability of transitioning from one form of ref"
J11-4007,W09-0609,0,0.0146935,"uations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evaluation exercises such as the TUNA challenge (Gatt, Belz, and Kow 2008) therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent. In a similar vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, an"
J11-4007,P02-1057,0,0.0118805,"rse-old is the only information status distinction 815 Computational Linguistics Volume 37, Number 4 that participating systems model, with other features derived from lexical and syntactic context; for instance, Greenbacker and McCoy (2009) consider subjecthood, parallelism, recency, and ambiguity. 2.2.4 Applications of Information Status Distinctions to GRE. The main application of theories of information status has been in anaphora resolution. Information status distinctions are not normally used in work on generating referring expressions, with a few notable exceptions. Krahmer and Theune (2002) show that the relative salience of discourse entities can be taken into account to produce less-informative descriptions (including fewer attributes than those necessary to uniquely identify the referent using a discourse model that does not incorporate salience). In contrast, Jordan and Walker (2005) show that, in task-oriented dialogs, over-speciﬁed references (including more attributes than needed to uniquely identify the intended referent) are more likely for certain dialog states and communicative goals. Some participating teams in the GREC challenges use the discourse-new vs. discourse-"
J11-4007,W10-4203,0,0.0181468,"tractors are added to the referring expression until its interpretation contains only the intended referent. Subsequent work on referring expression generation has (a) expanded the logical framework to allow reference by negation (the dog that is not black) and references to multiple entities (the brown or black dogs) (van Deemter 2002; Gatt and Van Deemter 2007), (b) explored different search algorithms for ﬁnding a minimal description (e.g., Horacek 2003), and (c) offered different representation frameworks such as graph theory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) as alternatives for representing referring characteristics. This body of research assumes a limited domain where the semantics of attributes and their allowed values can be formalized, though semantic representations and inference mechanisms are getting increasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz 2008; Ren, van Deemter, and Pan 2010). In contrast, Siddharthan and Copestake (2004) consider open-domain generation of referring expressions in a regeneration task (text simpliﬁcation); they take a different approach, approximating the hand-coded do"
J11-4007,D08-1019,0,0.0261054,"linguistic rules (e.g., Zajic et al. 2007) often combined with corpus-based information (Jing and McKeown 2000), whereas other approaches use statistical compression applied to news (Knight and Marcu 2000; Daum´e III and Marcu 2002) and to spoken dialogue (Galley and McKeown 2007). Other researchers addressed the problem of generating new sentences to include in a summary. Information fusion, which uses bottom–up multi-sequence alignment of the parse trees of similar sentences, generates new summary sentences from phrases extracted from different document sentences (Barzilay and McKeown 2005; Filippova and Strube 2008). 2.1.2 Summary Revision. Research in single-document summarization on improving summaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work. Three types of ad hoc revision rules are deﬁned—elimination (removing parentheticals, sentence initial prepositional phrases, and adverbial phrases), aggregation (combining constituents from two sentences), and smoothing. The smoothing operators cover some reference editing operations. They include substitution of a proper name with a name alias if the name is mentioned earlier, expansion of a pronoun with co-referential proper name"
J11-4007,N07-1023,1,0.491252,"h that unclear references in summaries pose serious problems for users (Paice 1990). 2.1.1 Sentence Compression and Fusion. Research in abstractive summarization has largely focused on the problem of compression, developing techniques to edit sentences by removing information that is not salient from extracted sentences. Some approaches use linguistic rules (e.g., Zajic et al. 2007) often combined with corpus-based information (Jing and McKeown 2000), whereas other approaches use statistical compression applied to news (Knight and Marcu 2000; Daum´e III and Marcu 2002) and to spoken dialogue (Galley and McKeown 2007). Other researchers addressed the problem of generating new sentences to include in a summary. Information fusion, which uses bottom–up multi-sequence alignment of the parse trees of similar sentences, generates new summary sentences from phrases extracted from different document sentences (Barzilay and McKeown 2005; Filippova and Strube 2008). 2.1.2 Summary Revision. Research in single-document summarization on improving summaries through revision (Mani, Gates, and Bloedorn 1999) is closer to our work. Three types of ad hoc revision rules are deﬁned—elimination (removing parentheticals, sente"
J11-4007,W08-1131,0,0.160759,"Missing"
J11-4007,J95-2003,0,0.277775,"Missing"
J11-4007,J86-3001,0,0.385105,"48, with W EKA parameters: “J48 -U -M 4”). We now discuss what features we used for our two classiﬁcation tasks (see the list of features in Table 2). Our hypothesis is that features capturing the frequency and syntactic and lexical forms of references are sufﬁcient to infer the desired distinctions. The frequency features are likely to give a good indication of the global salience of a person in the document set. Pronominalization indicates that an entity was particularly salient at a speciﬁc point of the discourse, as has been widely discussed in attentional status and centering literature (Grosz and Sidner 1986; Gordon, Grosz, and Gilliom 1993). Modiﬁed noun phrases (with apposition, relative clauses, or pre-modiﬁcation) can also signal different information status; for instance, we expect post-modiﬁcation to be more prevalent for characters who are less familiar. For our lexical features, we used two months worth of news articles collected over the Web (and independent of the DUC collection) to collect unigram and bigram lexical models of discourse-new references of people. The names themselves were removed from the discourse-new reference noun phrases and the counts were collected over the pre-mod"
J11-4007,C00-1045,0,0.728214,"Missing"
J11-4007,A00-2024,1,0.290386,"Missing"
J11-4007,P03-1054,0,0.0123544,"cision Section Prediction Accuracy Discourse-new references Include Name Section 5.1 Include Role & temporal mods Include Afﬁliation Include Post-Modiﬁcation Section 5.3.1 Section 5.3.2 Section 5.2 .74 (rising to .92 when there is unanimity among human summarizers) .79 .75 to .79 (depending on rule) .72 (rising to 1.00 when there is unanimity among human summarizers) Discourse-old references Include Only Surname Section 3 .70 833 Computational Linguistics Volume 37, Number 4 Implementation of Algorithm 4. Our reference rewrite module operates on parse trees obtained using the Stanford Parser (Klein and Manning 2003). For each person automatically identiﬁed using the techniques described in Section 4.1.1, we matched every mention of their surname in the parse trees of MEAD summary sentences. We then replaced the enclosing NP (includes all pre- and post-modifying constituents) with a new NP generated using Algorithm 4. The regenerated summary was produced automatically, without any manual correction of parses, semantic analyses, or information status classiﬁcations. We now enumerate implementation details not covered in Section 4.1.1: 1. Although our algorithm determines when to include role and afﬁliation"
J11-4007,J03-1003,0,0.0161679,"Missing"
J11-4007,P99-1072,0,0.248696,"Missing"
J11-4007,W99-0108,0,0.237814,"identify the referent using a discourse model that does not incorporate salience). In contrast, Jordan and Walker (2005) show that, in task-oriented dialogs, over-speciﬁed references (including more attributes than needed to uniquely identify the intended referent) are more likely for certain dialog states and communicative goals. Some participating teams in the GREC challenges use the discourse-new vs. discourse-old distinction as a feature to help select the most likely reference in context. Different interpretations of Centering Theory have also been used to generate pronominal references (McCoy and Strube 1999; Henschel, Cheng, and Poesio 2000). Our research is substantially different in that we model a much richer set of information status distinctions. Also, our choice of the news genre makes our studies complementary to the GREC challenges, which use Wikipedia articles about people or other named entities. News stories tend to be about events, not people, and the choice of initial references to participants is particularly important to help the reader understand the news. Our research is thus largely focused on the generation of initial references. Due to their short length, summaries do not gen"
J11-4007,I08-1016,1,0.86528,"f pronoun replacement), is meant to work for all entities, not just mentions to people, and does not incorporate distinctions inferred from the input to the summarizer. Although the rules and the overall approach are based on reasonable intuitions, in practice entity rewrites for summarization do introduce errors, some due to the rewrite rules themselves, others due to problems with co-reference resolution and parsing. 813 Computational Linguistics Volume 37, Number 4 Readers are very sensitive to these errors and prefer extractive summaries to summaries where all references have been edited (Nenkova 2008). Automatic anaphora resolution for all entities mentioned in the input and summary text is also errorful, with about one third of all substitutions in the summary being incorrect (Steinberger et al. 2007). In contrast, when editing references is restricted to references to people alone, as we do in the work presented here, there are fewer edits per summary but the overall result is perceived as better than the original by readers (Nenkova and McKeown 2003). 2.1.3 Reference in Summaries. There has been little investigation of the phenomenon of reference in news summaries. In addition to the re"
J11-4007,N03-2024,1,0.898914,"Missing"
J11-4007,H05-1031,1,0.891249,"Missing"
J11-4007,W06-1612,0,0.0862228,"r vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 2008), narrow focus (Calhoun 2007; Nenkova and Jurafsky 2007), and rheme and theme distinctions (Postolache, Kruijff-Korbayova, and Kruijff 2005) have not been used for generation or summarization tasks. Current efforts in the language generation community aim at providing a corpus and evaluation task (the GREC challenge) to address just this issue (Belz and Varges 2007; Belz, Kow, and Viethen 2009). The GREC-2.0 corpus, extracted from Wikipedia articles and annotated for the task of referring expression generation for both ﬁrst and subsequent mentions of the main subject of t"
J11-4007,W02-0404,0,0.147936,"Missing"
J11-4007,H05-1002,0,0.0614896,"Missing"
J11-4007,A97-1033,1,0.647061,"o errorful, with about one third of all substitutions in the summary being incorrect (Steinberger et al. 2007). In contrast, when editing references is restricted to references to people alone, as we do in the work presented here, there are fewer edits per summary but the overall result is perceived as better than the original by readers (Nenkova and McKeown 2003). 2.1.3 Reference in Summaries. There has been little investigation of the phenomenon of reference in news summaries. In addition to the revision of subsequent references described in Mani, Gates, and Bloedorn (1999), we are aware of Radev and McKeown (1997), who built a prototype system called PROFILE that extracted references to people from news, merging and recording information about people mentioned in various news articles. The idea behind the system was that the rich proﬁles collected for people could be used in summaries of later news in order to generate informative descriptions. However, the collection of information about entities from different contexts and different points in time leads to complications in description generation; for example, past news can refer to Bill Clinton as Clinton, an Arkansas native, the democratic president"
J11-4007,J98-3005,1,0.379455,"m was that the rich proﬁles collected for people could be used in summaries of later news in order to generate informative descriptions. However, the collection of information about entities from different contexts and different points in time leads to complications in description generation; for example, past news can refer to Bill Clinton as Clinton, an Arkansas native, the democratic presidential candidate Bill Clinton, U.S. President Clinton, or former president Clinton and it is not clear which of these descriptions are appropriate to use in a summary of a novel news item. In later work, Radev and McKeown (1998) developed an approach to learn correlations between linguistic indicators and semantic constraints to address such problems, but this line of research has not been pursued further. Next, we review related work on reference outside the ﬁeld of summarization. 2.2 Information Status and Generating Referring Expressions Research on information status distinctions closely relates to work on generating referring expressions. We now overview the two ﬁelds and how they interact. 2.2.1 Information Status Distinctions. Information status distinctions depend on two parameters related to the referent’s p"
J11-4007,W10-4212,0,0.079416,"Missing"
J11-4007,W03-2602,1,0.897915,"Missing"
J11-4007,P04-1052,1,0.822018,"nding a minimal description (e.g., Horacek 2003), and (c) offered different representation frameworks such as graph theory (Krahmer, van Erk, and Verleg 2003) or reference domain theory (Denis 2010) as alternatives for representing referring characteristics. This body of research assumes a limited domain where the semantics of attributes and their allowed values can be formalized, though semantic representations and inference mechanisms are getting increasingly sophisticated (e.g., the use of description logic: Areces, Koller, and Striegnitz 2008; Ren, van Deemter, and Pan 2010). In contrast, Siddharthan and Copestake (2004) consider open-domain generation of referring expressions in a regeneration task (text simpliﬁcation); they take a different approach, approximating the hand-coded domainknowledge of earlier systems with a measure of relatedness for attribute-values that is derived from WordNet synonym and antonym links. 2.2.3 Recent Trends: Data Collection and Evaluations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evalu"
J11-4007,C04-1129,1,0.871401,"Missing"
J11-4007,P98-2204,0,0.0630138,"hearer-old O R the person’s organization (country/ state/ afﬁliation) has been already mentioned A ND is the most salient organization in the discourse at the point where the reference needs to be generated T HEN the afﬁliation of a person can be omitted in the discourse-new reference. Algorithm 2: Omitting the afﬁliation in a discourse-new reference. Based on our intuitions about discourse salience and information status, we initially postulated the decision procedure in Algorithm 2. We described how we make the hearer-new/hearer-old judgment in Section 4.2. We used a salience-list (S-List) (Strube 1998) to determine the salience of organizations. This is a shallow attentional-state model and works as follows: 1. Within a sentence, entities are added to the salience-list from left to right. 2. Within the discourse, sentences are considered from right to left. In other words, entities in more recent sentences are more salient than those in previous ones, and within a sentence, earlier references are more salient than later ones. Results. To make the evaluation meaningful, we only considered examples where there was an afﬁliation mentioned for the person in the input documents, ruling out the t"
J11-4007,J02-1003,0,0.0174239,"Missing"
J11-4007,W03-0508,0,0.0286631,"Missing"
J11-4007,W06-1410,0,0.0243741,"ness for attribute-values that is derived from WordNet synonym and antonym links. 2.2.3 Recent Trends: Data Collection and Evaluations. There is now increasing awareness that factors other than conciseness are important when planning referring expressions and that considerable variation exists between humans generating referring expressions in similar contexts. Recent evaluation exercises such as the TUNA challenge (Gatt, Belz, and Kow 2008) therefore consider metrics other than length of a reference, such as humanness and the time taken by hearers to identify the referent. In a similar vein, Viethen and Dale (2006) examine how similar references produced by well-known algorithms are to human-produced references, and Dale and Viethen (2009) examine differences in human behavior when generating referring expressions. There is also growing collaboration between psycholinguists and computational linguists on the topic of generating referring expressions; for instance, the PRE-CogSci workshop (van Deemter et al. 2010). Recently, several corpora marked for various information status aspects have been made available. Subsequent studies concerned with predicting givenness status (Nissim 2006; Sridhar et al. 200"
J11-4007,A97-1030,0,\N,Missing
J11-4007,W09-0629,0,\N,Missing
J11-4007,grover-etal-2000-lt,0,\N,Missing
J11-4007,C98-2199,0,\N,Missing
J11-4007,radev-etal-2004-mead,0,\N,Missing
J11-4007,E03-1017,0,\N,Missing
J83-1001,T78-1009,0,0.0261293,"Missing"
J83-1001,P79-1016,1,0.117575,"Missing"
J83-1001,T75-2001,0,\N,Missing
J83-1001,J78-3015,0,\N,Missing
J96-1001,H94-1028,0,0.0620285,"Missing"
J96-1001,J90-2002,0,0.483539,"n) input, in which periods may not be noticeable (Church 1993), or languages where insertions or deletions are common (Shemtov 1993; Fung and McKeown 1994). These algorithms were adequate for our purposes, but could be replaced by algorithms more appropriate for noisy input corpora, if necessary. Sentence alignment techniques are generally used as a preprocessing stage, before the main processing component that proposes actual translations, whether of words, phrases, or full text, and they are used this way in our work as well. Translation can be approached using statistical techniques alone. Brown et al. (1990, 1993) use a stochastic language model based on techniques used in speech recognition, combined with translation probabilities compiled on the aligned corpus, to do sentence translation. Their system, Candide, uses little linguistic and no semantic information and currently produces good quality translations for short sentences containing high frequency vocabulary, as measured by individual human evaluators (see Berger et al. [1994] for information on recent results). While they also align groups of words across languages in the process of translation, they are careful to point out that such"
J96-1001,P91-1022,0,0.838326,"er a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it is less related to ours, addressing, for example, word sense disambiguation in the source language by statistically examining context (e.g., collocations) in the source language, thus allowing appropriate word selection in the target language. (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994). Our use of bilingual corpora assumes a prealigned corpus. Thus, we draw on work done at AT&T Bell Laboratories by Gale and Church (1991a, 1991b, 1993) and at IBM by Brown, Lai, and Mercer (1991) on bilingual sentence alignment. Sentence alignment programs take a paired bilingual corpus as input and determine which sentences in the target language translate which sentences in the source language. Both the AT&T and the IBM groups use purely statistical techniques based on sentence length to identify sentence pairing in corpora such as the Ha"
J96-1001,P91-1034,0,0.273877,"er a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it is less related to ours, addressing, for example, word sense disambiguation in the source language by statistically examining context (e.g., collocations) in the source language, thus allowing appropriate word selection in the target language. (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994). Our use of bilingual corpora assumes a prealigned corpus. Thus, we draw on work done at AT&T Bell Laboratories by Gale and Church (1991a, 1991b, 1993) and at IBM by Brown, Lai, and Mercer (1991) on bilingual sentence alignment. Sentence alignment programs take a paired bilingual corpus as input and determine which sentences in the target language translate which sentences in the source language. Both the AT&T and the IBM groups use purely statistical techniques based on sentence length to identify sentence pairing in corpora such as the Ha"
J96-1001,J93-2003,0,0.0659713,"sion of the limitations of our approach and of future work. 1 None of the authors is affiliated with Boitet&apos;s research center on machine translation in Grenoble, France, which is also n a m e d &quot;Champollion&apos;. Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons 2. Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al. 1993). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional resea"
J96-1001,A88-1019,0,0.00950705,"r results than by applying the same technique to all constructions uniformly. Our work is part of a paradigm of research that focuses on the development of tools using statistical analysis of text corpora. This line of research aims at producing tools 34 Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons that satisfactorily handle relatively simple tasks. These tools can then be used b y other systems to address more complex tasks. For example, previous w o r k has addressed low-level tasks such as tagging a free-style corpus with part-of-speech information (Church 1988), aligning a bilingual corpus (Gale and Church 1991b; Brown, Lai, and Mercer 1991), and producing a list of collocations (Smadja 1993). While each of these tools is based on simple statistics and tackles elementary tasks, we have demonstrated with our w o r k on Champollion that by combining them, one can reach new levels of complexity in the automatic treatment of natural languages. Acknowledgments This work was supported jointly by the Advanced Research Projects Agency and the Office of Naval Research under grant N00014-89-J-1782, by the Office of Naval Research under grant N00014-95-1-0745,"
J96-1001,P93-1001,0,0.0879418,"can be used for a variety of applications, closing with a discussion of the limitations of our approach and of future work. 1 None of the authors is affiliated with Boitet&apos;s research center on machine translation in Grenoble, France, which is also n a m e d &quot;Champollion&apos;. Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons 2. Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al. 1993). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe"
J96-1001,J90-1003,0,0.172845,"Missing"
J96-1001,A94-1006,0,0.256458,"nt, often forming part of a sublanguage. For example, Mr. Speaker is the proper way to refer to the Speaker of the House in the Canadian Parliament when speaking English. The French equivalent, Monsieur le Prdsident, is not the literal translation but instead uses the translation of the term President. While this is an appropriate translation for the Canadian Parliament, in different contexts another translation would be better. Note that these problems are quite similar to the difficulties in translating technical terminology, which also is usually part of a particular technical sublanguage (Dagan and Church 1994). The ability to automatically acquire collocation translations is thus a definite advantage for sublanguage translation. When moving to a new domain and sublanguage, translations that are appropriate can be acquired by running Champollion on a new corpus from that domain. Since in some instances parts of a sentence can be translated on a word-by-word basis, a translator must know when a full phrase or pair of words must be considered for translation and when a word-by-word technique will suffice. Two tasks must therefore be considered: . Identify collocations, or phrases which cannot be trans"
J96-1001,W93-0301,0,0.305494,"Missing"
J96-1001,J94-4003,0,0.748972,"other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it is less related to ours, addressing, for example, word sense disambiguation in the source language by statistically examining context (e.g., collocations) in the source language, thus allowing appropriate word selection in the target language. (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994). Our use of bilingual corpora assumes a prealigned corpus. Thus, we draw on work done at AT&T Bell Laboratories by Gale and Church (1991a, 1991b, 1993) and at IBM by Brown, Lai, and Mercer (1991) on bilingual sentence alignment. Sentence alignment programs take a paired bilingual corpus as input and determine which sentences in the target language translate which sentences in the source language. Both the AT&T and the IBM groups use purely statistical techniques based on sentence length to identify sentence pairing in corpora such as the Hansards. The AT&T group (Gale and Church 1993) defines"
J96-1001,P91-1017,0,0.00597627,"Missing"
J96-1001,P93-1022,0,0.0111739,"Missing"
J96-1001,E93-1015,0,0.0363986,"Missing"
J96-1001,W95-0114,0,0.0103365,"e authors is affiliated with Boitet&apos;s research center on machine translation in Grenoble, France, which is also n a m e d &quot;Champollion&apos;. Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons 2. Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al. 1993). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it i"
J96-1001,P95-1032,0,0.0695791,"e authors is affiliated with Boitet&apos;s research center on machine translation in Grenoble, France, which is also n a m e d &quot;Champollion&apos;. Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons 2. Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al. 1993). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and statistical translation, showing how these goals differ from our own, and then describe work on aligning groups of words. Note that there is additional research using statistical approaches to bilingual problems, but it i"
J96-1001,1994.amta-1.11,1,0.596001,"Missing"
J96-1001,H91-1026,0,0.840392,"Missing"
J96-1001,P91-1023,0,0.52032,"Next, we turn to a description of the results and evaluation. Finally, we show how the results can be used for a variety of applications, closing with a discussion of the limitations of our approach and of future work. 1 None of the authors is affiliated with Boitet&apos;s research center on machine translation in Grenoble, France, which is also n a m e d &quot;Champollion&apos;. Smadja, McKeown, and Hatzivassiloglou Translating Collocations for Bilingual Lexicons 2. Related Work The recent availability of large amounts of bilingual data has attracted interest in several areas, including sentence alignment (Gale and Church 1991b; Brown, Lai, and Mercer 1991; Simard, Foster and Isabelle 1992; Gale and Church 1993; Chen 1993), word alignment (Gale and Church 1991a; Brown et al. 1993; Dagan, Church, and Gale 1993; Fung and McKeown 1994; Fung 1995b), alignment of groups of words (Smadja 1992; Kupiec 1993; van der Eijk 1993), and statistical translation (Brown et al. 1993). Of these, aligning groups of words is most similar to the work reported here, although, as we shall show, we consider a greater variety of groups than is typical in other research. In this section, we describe work on sentence and word alignment and s"
J96-1001,J93-1004,0,0.451254,"which periods may not be noticeable (Church 1993), or languages where insertions or deletions are common (Shemtov 1993; Fung and McKeown 1994). These algorithms were adequate for our purposes, but could be replaced by algorithms more appropriate for noisy input corpora, if necessary. Sentence alignment techniques are generally used as a preprocessing stage, before the main processing component that proposes actual translations, whether of words, phrases, or full text, and they are used this way in our work as well. Translation can be approached using statistical techniques alone. Brown et al. (1990, 1993) use a stochastic language model based on techniques used in speech recognition, combined with translation probabilities compiled on the aligned corpus, to do sentence translation. Their system, Candide, uses little linguistic and no semantic information and currently produces good quality translations for short sentences containing high frequency vocabulary, as measured by individual human evaluators (see Berger et al. [1994] for information on recent results). While they also align groups of words across languages in the process of translation, they are careful to point out that such groups"
J96-1001,1994.amta-1.26,0,0.0282526,"Missing"
J96-1001,H93-1052,0,0.180162,"ion of single words (Klavans and Tzoukermann 1990; Dorr 1992; Klavans and Tzoukermann 1996), since for single words polysemy cannot be ignored; indeed, the problem of sense disambiguation has been linked to the problem of translating ambiguous words (Brown et al. 1991; Dagan, Itai, and Schwall 1991; Dagan and Itai 1994). The assumption of a single meaning per collocation was based on our previous experience with English collocations (Smadja 1993), is supported for less opaque collocations by the fact that their constituent words tend to have a single sense when they appear in the collocation (Yarowsky 1993), and was verified during our evaluation of Champollion (Section 7). We construct a mathematical model of the events we want to correlate, namely, the appearance of any word or group of words in the sentences of our corpus, as follows: To each group of words G, in either the source or the target language, we map a binary random variable Xc that takes the value &quot;1&quot; if G appears in a particular sentence and &quot;0&quot; if not. Then, the corpus of paired sentences comprising our database represents a collection of samples for the various random variables X for the various groups of words. Each new senten"
J96-1001,J93-1007,1,\N,Missing
J96-1001,E93-1054,0,\N,Missing
J96-1001,P91-1036,1,\N,Missing
J96-1001,C90-3031,0,\N,Missing
J96-1001,P93-1003,0,\N,Missing
J96-1001,P90-1032,1,\N,Missing
J96-1001,C90-3077,0,\N,Missing
J96-1001,P94-1033,0,\N,Missing
J96-1001,P93-1002,0,\N,Missing
J96-1001,P93-1023,1,\N,Missing
J97-2001,P83-1011,0,0.171801,"Missing"
J97-2001,C90-3059,0,0.0723417,"Missing"
J97-2001,W96-0501,1,0.133279,"determining the form of that text (McDonald 1983; McKeown 1985). Typically, a generator has two modules, each corresponding to one of these two tasks, a content planner and a linguistic realizer. While many systems allow for interaction across these components, there is general consensus that these two components can be separated (Reiter 1994). Furthermore, within the linguistic component, there appears to be further consensus that the task of syntactic realization can be isolated. As evidence, note that a number of dedicated syntactic realization components have been developed such as SURGE (Elhadad and Robin 1996), NICEL (Matthiessen 1991), MUMBLE(Meteer et al. 1987), and TAGs (Yang, McCoy, and Vijay-Shanker 1991; Harbusch 1994). Such components expect as input a specification of the thematic structure of the sentence to generate, with the syntactic category and open-class words of each thematic role. 3 Thematic structure involves roles such as agent, p a t i e n t , instrument, etc. It is opposed to surface syntactic structure which involves roles such as s u b j e c t , o b j e c t , adjunct, etc. Due to general syntactic alternations (Levin 1993) such as passive, dative, it-extraposition, or cleftin"
J97-2001,J85-4002,0,0.057011,"lexical choice is not part of the syntactic realization component, then all decisions regarding open-class word selection must be made before the grammar is invoked# They then must occur either as part of content planning or after all content has been determined and expressed in a language-independent manner. While some researchers have directly associated words with each concept in the domain-knowledge base (e.g., Reiter 1991; Swartout 1983), this approach does not allow for consideration of syntactic and lexical constraints unless a phrasal lexicon is used (e.g., Kukich 1983b; Danlos 1986; Jacobs 1985; Hovy 1988). Using a phrasal lexicon, however, means hand-encoding the many mappings of multiple constraints onto multiword phrasings. It thus does not allow for compositional lexical representation (Pustejovsky and Boguraev 1993), and entails a combinatorial explosion in the number of entries to cover the variations of phrases that are possible in different contexts. This approach thus does not allow for scaling up paraphrasing power (see Robin and McKeown [1996] for a quantitative evaluation of the scalability gains resulting from the compositional word-based approach). By waiting until con"
J97-2001,P83-1022,0,0.033265,"between semantic units. If lexical choice is not part of the syntactic realization component, then all decisions regarding open-class word selection must be made before the grammar is invoked# They then must occur either as part of content planning or after all content has been determined and expressed in a language-independent manner. While some researchers have directly associated words with each concept in the domain-knowledge base (e.g., Reiter 1991; Swartout 1983), this approach does not allow for consideration of syntactic and lexical constraints unless a phrasal lexicon is used (e.g., Kukich 1983b; Danlos 1986; Jacobs 1985; Hovy 1988). Using a phrasal lexicon, however, means hand-encoding the many mappings of multiple constraints onto multiword phrasings. It thus does not allow for compositional lexical representation (Pustejovsky and Boguraev 1993), and entails a combinatorial explosion in the number of entries to cover the variations of phrases that are possible in different contexts. This approach thus does not allow for scaling up paraphrasing power (see Robin and McKeown [1996] for a quantitative evaluation of the scalability gains resulting from the compositional word-based appr"
J97-2001,P81-1014,0,0.26012,"Missing"
J97-2001,A94-1002,1,0.877314,"Missing"
J97-2001,P93-1031,1,0.880409,"Missing"
J97-2001,W94-0319,0,0.260874,"ossible placements of lexical choice within a generator's architecture. 2.1 Lexical Choice within a Generation System Architecture Generation systems perform two types of tasks: one conceptual, determining the content of the text to be generated, and one linguistic, determining the form of that text (McDonald 1983; McKeown 1985). Typically, a generator has two modules, each corresponding to one of these two tasks, a content planner and a linguistic realizer. While many systems allow for interaction across these components, there is general consensus that these two components can be separated (Reiter 1994). Furthermore, within the linguistic component, there appears to be further consensus that the task of syntactic realization can be isolated. As evidence, note that a number of dedicated syntactic realization components have been developed such as SURGE (Elhadad and Robin 1996), NICEL (Matthiessen 1991), MUMBLE(Meteer et al. 1987), and TAGs (Yang, McCoy, and Vijay-Shanker 1991; Harbusch 1994). Such components expect as input a specification of the thematic structure of the sentence to generate, with the syntactic category and open-class words of each thematic role. 3 Thematic structure involve"
J97-2001,J83-1005,0,0.323545,"Missing"
J97-2001,W90-0104,0,\N,Missing
J97-2001,C90-1021,0,\N,Missing
J97-2001,E87-1001,0,\N,Missing
J97-2001,C92-3158,0,\N,Missing
J98-3005,M92-1024,0,0.0460828,"Missing"
J98-3005,W97-0703,0,0.146356,"that these sentences function together coherently as a full paragraph. As with the other statistical approaches, this work is aimed at summarization of single articles. Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summarization primarily focused on the use of sentence extraction. Alternatives to the use 473 Computational Linguistics Volume 24, Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997). While most of the work in this category focuses on summarization of single articles, early work is beginning to emerge on summarization across multiple documents. In ongoing work at Carnegie Mellon, Carbonell (personal communication) is developing statistical techniques to identify similar sentences and phrases across articles. The aim is to identify sentences that are r"
J98-3005,W97-0702,0,0.029309,"Sentence Extraction To allow summarization in arbitrary domains, researchers have traditionally applied statistical techniques (Luhn 1958; Paice 1990; Preston and Williams 1994; Rau, Brandow, and Mitze 1994). This approach can be better termed extraction rather than summarization, since it attempts to identify and extract key sentences from an article using statistical techniques that locate important phrases using various statistical measures. This has been successful in different domains (Preston and Williams 1994) and is, in fact, the approach used in recent commercial summarizers (Apple [Boguraev and Kennedy 1997], Microsoft, and inXight). Rau, Brandow, and Mitze (1994) report that statistical summaries of individual news articles were rated lower by evaluators than summaries formed by simply using the lead sentence or two from the article. This follows the principle of the ""inverted pyramid"" in news writing, which puts the most salient information in the beginning of the article and leaves elaborations for later paragraphs, allowing editors to cut from the end of the text without compromising the readability of the remaining text. Paice (1990) also notes that problems for this approach center around"
J98-3005,C90-3059,0,0.0199317,"kich et al. 1997) generates Web traffic summaries for advertisement management software. It makes use of an ontology over the domain to combine information at the conceptual level. All of these systems take tabular data as input. The research focus has been on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al. 1994). All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles. In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. 3. System Overview The overall architecture of our summarization system given earlier in Figure 1 draws on research in software agents (Genesereth and Ketchpel 1994) to"
J98-3005,A88-1019,0,0.024822,"Missing"
J98-3005,M92-1031,0,0.0190995,"fic information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction. These are domain-independent techniques that can also be used to extract information for a summary. Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et al. 1994), the use of extensive name lists, place names, titles and ""gazetteers"" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al. 1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al. 1993; Lehnert et al. 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald 1993; Coates-Stephens 1991) and the larger discourse context (Mani et al. 1993) to improve the accuracy of p"
J98-3005,M95-1011,0,0.0032534,". By relying on these systems, the task we have addressed to date is happily more restricted than direct summarization of full text. This has allowed us to focus on issues related to the combination of information in the templates and the generation of text to express them. In order to port our system to other domains, we would need to develop new templates and the information extraction rules required for them. While this is a task we leave to those working in the information extraction field, we note that there do exist tools for semi-automatically acquiring such rules (Lehnert et al. 1993; Fisher et al. 1995). This helps to alleviate the otherwise knowledge-intensive nature of the task. We are working on the development of tools for domain-independent types of information extraction. For example, our work on extracting descriptions of individuals and organizations and representing them in a formalism that facilitates reuse of the descriptions in summaries can be used in any domain. In the remainder of this section, we highlight the novel techniques of SUMMONS and explain why they are important for our work. 1.1 Summarization of Multiple Articles With a few exceptions (cf. Section 2), all existing"
J98-3005,P88-1020,0,0.00800527,"Missing"
J98-3005,W97-0704,0,0.251839,"ical approaches, this work is aimed at summarization of single articles. Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summarization primarily focused on the use of sentence extraction. Alternatives to the use 473 Computational Linguistics Volume 24, Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997). While most of the work in this category focuses on summarization of single articles, early work is beginning to emerge on summarization across multiple documents. In ongoing work at Carnegie Mellon, Carbonell (personal communication) is developing statistical techniques to identify similar sentences and phrases across articles. The aim is to identify sentences that are representative of more than one article. Mani and Bloedorn (1997) link similar words and p"
J98-3005,P83-1022,0,0.0414662,"1995). ZEDDoc (Passonneau et al. 1997; Kukich et al. 1997) generates Web traffic summaries for advertisement management software. It makes use of an ontology over the domain to combine information at the conceptual level. All of these systems take tabular data as input. The research focus has been on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al. 1994). All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles. In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. 3. System Overview The overall architecture of our summarization system given earlier in Figure 1 draws on research in"
J98-3005,W97-0903,1,0.472801,"rwise appear as separate sentences gets added in as modifiers of the existing sentences, or new words that can simultaneously convey both pieces of information are selected. PLANDoc (McKeown, Kukich, and Shaw 1994a; McKeown, Robin, and Kukich 1995; Shaw 1995) generates summaries of the activities of telephone planning engineers, using linguistic summarization both to order its input messages and to combine them into single sentences. Focus has been on the combined use of conjunction, ellipsis, and paraphrase to result in concise, yet fluent reports (Shaw 1995). ZEDDoc (Passonneau et al. 1997; Kukich et al. 1997) generates Web traffic summaries for advertisement management software. It makes use of an ontology over the domain to combine information at the conceptual level. All of these systems take tabular data as input. The research focus has been on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1"
J98-3005,M93-1023,0,0.0907063,"on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al. 1994). All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles. In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. 3. System Overview The overall architecture of our summarization system given earlier in Figure 1 draws on research in software agents (Genesereth and Ketchpel 1994) to allow connections to a variety of different types of data sources. Facilities are used to provide a transparent interface to heterogeneous data sources that run on several machines and may be written in different programming languages. Curren"
J98-3005,W93-0105,0,0.0248138,"future work. Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree. In addition to domain-specific information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction. These are domain-independent techniques that can also be used to extract information for a summary. Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et al. 1994), the use of extensive name lists, place names, titles and ""gazetteers"" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al. 1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al. 1993; Lehnert et al. 1993). In"
J98-3005,W97-0713,0,0.105914,"ey do not address the task of summarization since they do not combine and rephrase extracted information as part of a textual summary. A recent approach to symbolic summarization is being carried out at Cambridge University on identifying strategies for summarization (Sparck Jones 1993). This work studies how various discourse processing techniques (e.g., rhetorical structure relations) can be used to both identify important information and form the actual summary. While promising, this work does not involve an implementation as of yet, but provides a framework and strategies for future work. Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree. In addition to domain-specific information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction. These are domain-independent techniques that can also be used to extract information for a summary. Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et"
J98-3005,W93-0104,0,0.0103133,"ial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al. 1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al. 1993; Lehnert et al. 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald 1993; Coates-Stephens 1991) and the larger discourse context (Mani et al. 1993) to improve the accuracy of proper noun extraction when large known-word lists are not available. In a way similar to this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular grammar encoding part-of-speech categories to extract certain text patterns (descriptions) and we use WordNet (Miller et al. 1990) to provide semantic filtering. Another system, called MURAX (Kupiec 1993), is similar to ours from a different perspective. MURAX also extracts information from"
J98-3005,A94-1002,1,0.277667,"Missing"
J98-3005,P93-1031,1,0.514309,"Missing"
J98-3005,W97-0707,0,0.0473643,"Missing"
J98-3005,W96-0512,1,0.456172,"iate historical references and the context of prior news. Briefings focus on certain types of information that are present in the source text in which the reader has expressed interest. They deliberately ignore facts that are tangential to the user&apos;s interests, whether or not these facts are the focus of the article. In other words, briefings are more user-centered than general summaries; the latter convey information that the writer has considered important, whereas briefings are based on information that the user is looking for. We present a system, called SUMMONS 1 (McKeown and Radev 1995; Radev 1996; Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques in the following areas: • It briefs the user on information of interest using tools related to information extraction, conceptual combination, and text generation. • It combines information from multiple news articles into a coherent summary using symbolic techniques. • It augments the resulting summaries using descriptions of entities obtained from on-line sources. As can be expected from a knowledge-based summarization system, SUMMONS works in a restricted domain. We have chosen the domain of news on terrorism fo"
J98-3005,A97-1033,1,0.832566,"nces and the context of prior news. Briefings focus on certain types of information that are present in the source text in which the reader has expressed interest. They deliberately ignore facts that are tangential to the user&apos;s interests, whether or not these facts are the focus of the article. In other words, briefings are more user-centered than general summaries; the latter convey information that the writer has considered important, whereas briefings are based on information that the user is looking for. We present a system, called SUMMONS 1 (McKeown and Radev 1995; Radev 1996; Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques in the following areas: • It briefs the user on information of interest using tools related to information extraction, conceptual combination, and text generation. • It combines information from multiple news articles into a coherent summary using symbolic techniques. • It augments the resulting summaries using descriptions of entities obtained from on-line sources. As can be expected from a knowledge-based summarization system, SUMMONS works in a restricted domain. We have chosen the domain of news on terrorism for several reasons. First,"
J98-3005,P95-1053,0,0.0436206,"Missing"
J98-3005,C90-1021,0,\N,Missing
J98-3005,H93-1062,0,\N,Missing
J98-3005,M92-1030,0,\N,Missing
J98-3005,A97-1030,0,\N,Missing
J98-3005,C92-3158,0,\N,Missing
J98-3005,X98-1026,0,\N,Missing
J98-3005,M95-1006,0,\N,Missing
kan-etal-2002-using,J98-3005,1,\N,Missing
kan-etal-2002-using,P96-1025,0,\N,Missing
N03-2024,A00-2018,0,\N,Missing
N03-2024,A97-1030,0,\N,Missing
N04-3001,P99-1044,0,0.0343031,"to English text are replaced (or augmented with) the similar English sentence. This first system using full machine translation over the sentences and English similarity detection will be extended using simple features for multilingual similarity detection in SimFinder MultiLingual (SimFinderML), a multilingual version of SimFinder (Hatzivassiloglou et al., 2001). We also plan an experiment evaluating the usefulness of noun phrase detection and noun phrase variant detection as a primitive for multilingual similarity detection, using tools such as Christian Jacquemin’s FASTR (Jacquemin, 1994; Jacquemin, 1999). 4.2 Summary presentation Multilingual Newsblaster presents multiple views of a cluster of documents to the user, broken down by language and by country. Summaries are generated for the entire cluster, as well as sub-sets of the articles based on the country of origin and language of the original articles. Users are first presented with a summary of the entire cluster using all documents, and then have the ability to focus on countries or languages of their choosing. We also allow the user to view two summaries side-by-side so they can easily compare differences between summaries from differe"
N04-3001,1999.mtsummit-1.45,0,0.0216909,"iments. The phases in the multilingual version of Columbia Newsblaster have been modified to take language and character encoding into account, and a new phase, translation, has been added. Figure 1 depicts the multilingual Columbia Newsblaster architecture. We will describe the system, in particular a method using machine learning to extract article text from web pages that is applicable to different languages, and a baseline approach to multilingual multi-document summarization. 1.1 Related Research Previous work in multilingual document summarization, such as the SUMMARIST system (Hovy and Lin, 1999) 1 http://newsblaster.cs.columbia.edu/ Figure 1: Architecture of the multilingual Columbia Newsblaster system. extracts sentences from documents in a variety of languages, and translates the resulting summary. This system has been applied to Information Retrieval in the MuST System (Lin, 1999) which uses query translation to allow a user to search for documents in a variety of languages, summarize the documents using SUMMARIST, and translate the summary. The Keizei system (Ogden et al., 1999) uses query translation to allow users to search Japanese and Korean documents in English, and displays"
N04-3001,N03-2024,1,0.738908,"o one of two multidocument summarization systems based on the similarity of the documents in the cluster. If the documents are highly similar, the Multigen summarization system (McKeown et al., 1999) is used. Multigen clusters sentences based on similarity, and then parses and fuses information from similar sentences to form a summary. The second summarization system used is DEMS, the Dissimilarity Engine for Multi-document Summarization (Schiffman et al., 2002), which uses a sentence extraction approach to summarization. The resulting summary is then run through a named entity recovery tool (Nenkova and McKeown, 2003), which repairs named entity references in the summary by making the first reference descriptive, and shortening subsequent reference mentions in the summary. Using an unmodified version of DEMS, summaries might contain sentences from translated documents which are not grammatically correct. The DEMS summarization system was modified to prefer choosing a sentence from an English article if there are sentences that express similar content in multiple languages. By setting different weight penalties we can take the quality of the translation system for a given language pair into Figure 2: A scre"
N04-3001,W97-0704,0,\N,Missing
N07-1023,P05-1022,0,0.0126392,"ction. We feel it is important to use a relatively large development corpus, since we will provide in Section 5 detailed analyses of model selection on the development set (e.g., by evaluating different Markov structures), and we want these findings to be as significant as possible. Finally, we used the same test data as K&M for human evaluation purposes (32 sentence pairs). 4 Tree Alignment and Synchronous Grammar Inference We now describe methods to train SCFG models from sentence pairs. Given a tree pair (f , c), whose respective parses (πf , πc ) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency estimates for our Markovized models from observed SCFG productions. Clearly, the two trees may sometimes be structurally quite different (e.g., a given PP may attach to an NP in πf , while attaching to VP in πc ), and it is not always possible to build an SCFG derivation given the constraints in (πf , πc ). The approach taken by K&M is to analyze both trees and count an SCFG rule whenever two nodes are “deemed to correspond”, i.e., roots are the same, and αc is a sub-sequence of αf . This leads"
N07-1023,W03-0501,0,0.0520647,"Missing"
N07-1023,A00-1043,0,0.891657,"Missing"
N07-1023,J98-4004,0,0.0282372,"from DT the . PP JJ NN year-ago period IN because of NP VBG NN NN slowing microchip demand Figure 1: Penn Treebank tree with adjuncts in italic. occurrences are sparsely seen (e.g., “fell”-“from”). At a lower level, lexicalization is clearly desirable for pre-terminals. Indeed, current SCFG models such as K&M have no direct way of preventing highly improbable single word removals, such as deletions of adverbs “never” or “nowhere”, which may turn a negative statement into a positive one.4 A second type of annotation that can be added to syntactic categories is the so-called parent annotation (Johnson, 1998), which was effectively used in syntactic parsing to break unreasonable context-free assumptions. For instance, a PP with a VP parent is marked as PPˆVP. It is reasonable to assume that, e.g., that constituents deep inside a PP have more chances to be removed than otherwise expected, and one may seek to increase the amount of vertical context that is available for conditioning each constituent deletion. To achieve the above desiderata for better SCFG probability estimates—i.e., reduce the amount of sister annotation within each SCFG production, by conditioning deletions on a context smaller th"
N07-1023,P03-1054,0,0.205977,"data, which greatly benefited the lexical probabilities incorporated into our Markovized SCFGs. Our work provides three main contributions: 180 Proceedings of NAACL HLT 2007, pages 180–187, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics (1) Our lexicalized head-driven Markovization yields more robust probability estimates, and our compressions outperform (Knight and Marcu, 2000) according to automatic and human evaluation. (2) We provide a comprehensive analysis of the impact of different Markov orders for sentence compression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the available training resources for syntax-directed sentence compression systems. 2 Synchronous Grammars for Sentence Compression One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&M) relies on synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand s"
N07-1023,E06-1038,0,0.626032,"Missing"
N07-1023,P05-1036,0,0.571044,"ion to accurately distinguish adjuncts from complements, and that produces sentences that were judged more grammatical than those generated by previous work. 1 Introduction Sentence compression addresses the problem of removing words or phrases that are not necessary in the generated output of, for instance, summarization and question answering systems. Given the need to ensure grammatical sentences, a number of researchers have used syntax-directed approaches that perform transformations on the output of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical approach, relying on formalisms equivalent to probabilistic synchronous context-free grammars (SCFG) ∗ This material is based on research supported in part by the U.S. National Science Foundation (NSF) under Grant No. IIS-05-34871 and the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or DARPA. In this paper, we present a head-driven Markovization of SCFG compression rules, an"
N07-1023,J93-2004,0,\N,Missing
N07-1023,J03-4003,0,\N,Missing
N07-1054,W05-0613,0,0.0291209,"henomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter training instances based on Part-of-Speech (POS) tags, and Soricut and Marcu (2003) use syntactic features t"
N07-1054,P96-1025,0,0.017207,"Missing"
N07-1054,P03-1071,1,0.82182,"Missing"
N07-1054,W03-1210,0,0.0380148,"of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter training instances based on Part-of-Speech (POS) tags, and Soricut and Marc"
N07-1054,P02-1047,0,0.9041,"timizing a set of basic parameters such as smoothing weights, vocabulary size and stoplisting. We then focus on improving the quality of the automaticallymined training examples, using topic segmentation and syntactic heuristics to filter out training instances which may be wholly or partially invalid. We find that the parameter optimization and segmentation-based filtering techniques achieve significant improvements in classification performance. We report results of experiments which build and refine models of rhetoricalsemantic relations such as Cause and Contrast. We adopt the approach of Marcu and Echihabi (2002), using a small set of patterns to build relation models, and extend their work by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. Using human-annotated and automatically-extracted test sets, we find that each of these techniques results in improved relation classification accuracy. 2 Related Work 1 Introduction Relations such as Cause and Contrast, which we call rhetorical-semantic relations (RSRs), may be signaled in text by cue phrases like because or however which join clauses or sentences and explicitly express the r"
N07-1054,J96-3006,0,0.0147759,"uthors and do not necessarily reflect the views of DARPA. The first author performed most of the research reported in this paper while at Columbia University. Rhetorical and discourse theory has a long tradition in computational linguistics (Moore and WiemerHastings, 2003). While there are a number of different relation taxonomies (Hobbs, 1979; McKeown, 1985; Mann and Thompson, 1988; Martin, 1992; Knott and Sanders, 1998), many researchers have found that, despite small differences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of"
N07-1054,N03-1030,0,0.0692976,"t and Sanders, 1998), many researchers have found that, despite small differences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a"
N07-1054,W06-1317,0,0.138548,"ferences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter training instances based on Par"
N07-1054,J05-2005,0,0.0111075,"ve found that, despite small differences, these theories have wide agreement in terms of the core phenomena for which they account (Hovy and Maier, 1993; Moser and Moore, 1996). Work on automatic detection of rhetorical and discourse relations falls into two categories. Marcu and Echihabi (2002) use a pattern-based approach in mining instances of RSRs such as Contrast and Elaboration from large, unannotated corpora. We discuss this work in detail in Section 3. Other work uses human-annotated corpora, such as the RST Bank (Carlson et al., 2001), used by Soricut and Marcu (2003), the GraphBank (Wolf and Gibson, 2005), used by Wellner et al. (2006), or adhoc annotations, used by (Girju, 2003; Baldridge and Lascarides, 2005). In the past year, the ini428 Proceedings of NAACL HLT 2007, pages 428–435, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics tial public release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. Some work in RSR detection has enlisted syntactic analysis as a tool. Marcu and Echihabi (2002) filter"
N07-1054,C04-1020,0,\N,Missing
N07-1054,W01-1605,0,\N,Missing
N07-1054,N04-1020,0,\N,Missing
N10-1044,J05-3002,1,0.824766,"must transform input text to produce useful output text, condensing an input document or document set in the case of summarization and selecting text that meets the question constraints in the case of question answering. While many systems use sentence extraction to facilitate the task, this approach risks including additional, irrelevant or non-salient information in the output, and the original sentence wording may be inappropriate for the new context in which it appears. Instead, recent research has investigated methods for generating new sentences using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Lapata, 2008"
N10-1044,C08-1018,0,0.179572,"Missing"
N10-1044,P02-1057,0,0.0754356,"Missing"
N10-1044,W04-1016,0,0.264921,"Missing"
N10-1044,D08-1019,0,0.370427,"xt, condensing an input document or document set in the case of summarization and selecting text that meets the question constraints in the case of question answering. While many systems use sentence extraction to facilitate the task, this approach risks including additional, irrelevant or non-salient information in the output, and the original sentence wording may be inappropriate for the new context in which it appears. Instead, recent research has investigated methods for generating new sentences using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Lapata, 2008), we expect that the creation of this corpus might en"
N10-1044,A00-1043,0,0.20753,"nces using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Lapata, 2008), we expect that the creation of this corpus might encourage the development of supervised learning techniques for automated sentence fusion. In this work, we present a methodology for creating such a corpus using Amazon’s Mechanical Turk1 , a widely used online marketplace for crowdsourced task completion. Our goal is the generation of accurate fusions between pairs of sentences that have some information in common. To ensure that the task is performed consistently, we abide by the distinction proposed by Marsi and Krahmer (2005)"
N10-1044,P08-2049,0,0.629925,"Missing"
N10-1044,W05-1612,0,0.51232,"produce useful output text, condensing an input document or document set in the case of summarization and selecting text that meets the question constraints in the case of question answering. While many systems use sentence extraction to facilitate the task, this approach risks including additional, irrelevant or non-salient information in the output, and the original sentence wording may be inappropriate for the new context in which it appears. Instead, recent research has investigated methods for generating new sentences using a technique called sentence fusion (Barzilay and McKeown, 2005; Marsi and Krahmer, 2005; Filippova and Strube, 2008) where output sentences are generated by fusing together portions of related sentences. While algorithms for automated fusion have been developed, there is no corpus of human-generated fused sentences available to train and evaluate such systems. The creation of such a dataset could provide insight into the kinds of fusions that people produce. Furthermore, since research in the related task of sentence compression has benefited from the availability of training data (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Cohn and Lapata, 2008), we expect that the cre"
N10-1044,E06-1038,0,0.166938,"Missing"
N10-1044,D08-1027,0,0.0186861,"Missing"
N10-1044,xie-etal-2008-extracting,0,0.0488656,"Missing"
N13-1045,J99-2004,0,0.668943,"he other is translation selection or reranking (Hildebrand and Vogel 2008; Callison-Burch et al., 2012), where candidate translations are generated by different decoding processes or different decoders. This paper belongs to the latter; the goal is to identify ungrammatical hypotheses from given candidate translations using grammatical knowledge in the target language that expresses syntactic dependencies between words. To achieve that, we propose a novel Structured Language Model (SLM) - Supertagged Dependency Language Model (SDLM) to model the syntactic dependencies between words. Supertag (Bangalore and Joshi, 1999) is an elementary syntactic structure based on Lexicalized Tree Adjoining Grammar (LTAG). Traditional supertagged n-gram LM predicts the next supertag based on the immediate words to the left with supertags, so it can not explicitly model long-distance dependency relations. In contrast, SDLM predicts the next supertag using the words with supertags on which it syntactically depend, and these words could be anywhere and arbitrarily far apart in a sentence. A candidate translation’s grammatical degree or “fluency” can be measured by simply calculating the SDLM likelihood of the supertagged depen"
N13-1045,2000.iwpt-1.9,0,0.0405998,"ity to extend to the remaining directions in the dependency list. 435 In contrast to the LTAG parsing and supertagging-based approaches, we propose an alternative mechanism: first we use a state-of-theart constituent parser to obtain the parse of a sentence, and then we extract elementary trees with dependencies from the parse to assign each word with an elementary tree. The second step is similar to the approach used in extracting elementary trees from the TreeBank (Xia, 1999; Chen and VijayShanker, 2000). 4.1 Elementary Tree Extraction We use an elementary tree extractor, a modification of (Chen and Vijay-Shanker, 2000), to serve our purpose. Heuristic rules were used to distinguish arguments from adjuncts, and the extraction process can be regarded as a process that gradually decomposes a constituent parse to multiple elementary trees and records substitutions and adjunctions. From elementary trees, we can obtain supertags by only considering syntactic structure and ignoring anchor words. Take the sentence – “The hungry boys ate dinner” as an example; the constituent parse and extracted supertags are shown in Figure 1. In Figure 1 (b), dotted lines represent the operations of substitution and adjunction. No"
N13-1045,D09-1076,0,0.0142569,"ns in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination. 1 Introduction In recent years, there has been a burgeoning interest in incorporating syntactic structure into Statistical machine translation (SMT) models (e..g, Galley et al., 2006; DeNeefe and Knight 2009; Quirk et al., 2005). In addition to modeling syntactic structure in the decoding process, a methodology for candidate translation selection has also emerged. This methodology first generates multiple candidate translations followed by rescoring using global sentence-level syntactic features to select the final translation. The advantage of this methodology is that it allows for easy integration of complex syntactic features that would be too expensive to use during the decoding process. The methodology is usually applied in two scenarios: one is as part of an n-best reranking (Och et al., 20"
N13-1045,P08-1009,0,0.0327969,"Missing"
N13-1045,E06-1005,0,0.0386225,"at integrates supertags into the target side of the translation model and the target n-gram LM. Two kinds of supertags are employed: those from LTAG and Combinatory Categorial Grannar (CCG), and both yield similar improvements. They found that using both or either of the supertag-based translation model and supertagged LM can achieve significant improvement. Again, the supertagged LM is a class-based n-gram LM and does not model explicit syntactic dependencies during decoding. In the field of MT system combination, wordlevel confusion network decoding is one of the most successful approaches (Matusov et al., 2006; Rosti et al., 2007; He et al. 2008; Karakos et al. 2008; Sim et al. 2007; Xu et al. 2011). It is capable of generating brand new translations but it is difficult to consider more complex syntax such as dependency LM during decoding since it adds one word at a time while a dependency based LM must parse a complete sentence. Typically, a confusion network approach selects one translation as the best and uses this as the backbone for the confusion network. The work we present here could provide a more sophisticated mechanism for selecting the backbone. Alternatively, one can enhance confusion n"
N13-1045,W11-2121,0,0.25327,"Missing"
N13-1045,C94-1024,0,\N,Missing
N13-1045,W12-3102,0,\N,Missing
N13-1045,P08-2021,0,\N,Missing
N13-1045,P07-1040,0,\N,Missing
N13-1045,C88-2121,0,\N,Missing
N13-1045,W12-3112,0,\N,Missing
N13-1045,P05-1034,0,\N,Missing
N13-1045,P06-1121,0,\N,Missing
N13-1045,2008.amta-srw.3,0,\N,Missing
N13-1045,P07-1037,0,\N,Missing
N13-1045,N04-1021,0,\N,Missing
N13-1045,W06-2606,0,\N,Missing
O12-1014,O06-1023,0,0.0201501,"propagate the failure information to relevant words. We call the modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through loo"
O12-1014,O10-2001,0,0.0283016,"pagate the failure information to relevant words. We call the modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through look-u"
O12-1014,stymne-ahrenberg-2010-using,0,0.0243733,"to relevant words. We call the modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through look-up in the XTAG grammar, and fina"
O12-1014,J93-4006,0,0.295024,"modified unification a fail propagation unification. Our approach features: 1) the use of XTAG grammar [2], a rule-based English grammar developed by linguists using the FB-LTAG formalism, 2) the ability to simultaneously detect multiple ungrammatical types and their corresponding words by using FB-LTAG’s feature unifications, and 3) the ability to simultaneously correct multiple ungrammatical types based on the detection information. Grammar checking methods are usually divided into three classes: statistic-based checking [3][4][5][6], rule-based checking [7][8][9] and syntax-based checking [10]. Our approach is a mix of rule-based checking and syntax-based checking: The XTAG English grammar is designed by linguists while the detecting procedure is based on syntactic operations which 142 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) dynamically reference the grammar. In our procedure for syntactic error detection, we first decomposes each sentence hypothesis parse tree into elementary trees, followed by associating each elementary tree with AVMs through look-up in the XTAG grammar, and finally reconstruct the original pa"
O12-1014,C88-2147,0,\N,Missing
O12-1014,P03-1054,0,\N,Missing
O12-1014,C88-2121,0,\N,Missing
O12-5001,2000.iwpt-1.9,0,0.133896,"Missing"
O12-5001,O10-2001,0,0.0639591,"Missing"
O12-5001,J93-4006,0,0.352581,"Missing"
O12-5001,P03-1054,0,0.0120037,"Missing"
O12-5001,2011.mtsummit-papers.62,1,0.832835,"Missing"
O12-5001,C88-2121,0,0.280262,"Missing"
O12-5001,stymne-ahrenberg-2010-using,0,0.056021,"Missing"
O12-5001,O06-1023,0,0.0543074,"Missing"
O12-5001,C88-2147,0,\N,Missing
P01-1008,W99-0625,0,\N,Missing
P01-1008,J93-1004,0,\N,Missing
P01-1008,W97-0703,1,\N,Missing
P01-1008,N01-1009,0,\N,Missing
P01-1008,J93-2003,0,\N,Missing
P01-1008,P97-1004,0,\N,Missing
P01-1008,P95-1026,0,\N,Missing
P01-1008,P98-1116,0,\N,Missing
P01-1008,C98-1112,0,\N,Missing
P01-1008,P98-2127,0,\N,Missing
P01-1008,C98-2122,0,\N,Missing
P01-1008,P93-1023,1,\N,Missing
P01-1008,W99-0613,0,\N,Missing
P01-1008,P93-1024,0,\N,Missing
P01-1023,C00-1007,0,0.0795519,"k and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6 Conclusions In this paper we"
P01-1023,H01-1065,1,0.890734,"Missing"
P01-1023,W00-1426,0,0.0474065,"Missing"
P01-1023,P95-1034,0,0.031415,"itable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6 Conclusions In this paper we presented a technique for extractin"
P01-1023,W98-1426,0,0.0405835,"nd at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of sentence generation, including syntactic and lexical realization (e.g., (Langkilde and Knight, 1998; Bangalore and Rambow, 2000; Knight and Hatzivassiloglou, 1995)). Concurrent work analyzing constraints on ordering of sentences in summarization found that a coherence constraint that ensures that blocks of sentences on the same topic tend to occur together (Barzilay et al., 2001). This results in a bottomup approach for ordering that opportunistically groups sentences together based on content features. In contrast, our work attempts to automatically learn plans for generation based on semantic types of the input clause, resulting in a top-down planner for selecting and ordering content. 6"
P01-1023,A97-1041,1,0.903953,"Missing"
P01-1023,P97-1039,0,0.0112496,"t that the obtained constraints mostly occur in the existing plan is very encouraging. 5 Related work As explained in (Hudak and McClure, 1999), motif detection is usually targeted with alignment techniques (as in (Durbin et al., 1998)) or with combinatorial pattern discovery techniques such as the ones we used here. Combinatorial pattern discovery is more appropriate for our task because it allows for matching across patterns with permutations, for representation of wild cards and for use on smaller data sets. Similar techniques are used in NLP. Alignments are widely used in MT, for example (Melamed, 1997), but the crossing problem is a phenomenon that occurs repeatedly and at many levels in our task and thus, this is not a suitable approach for us. Pattern discovery techniques are often used for information extraction (e.g., (Riloff, 1993; Fisher et al., 1995)), but most work uses data that contains patterns labelled with the semantic slot the pattern fills. Given the difficulty for humans in finding patterns systematically in our data, we needed unsupervised techniques such as those developed in computational genomics. Other stochastic approaches to NLG normally focus on the problem of senten"
P01-1023,J93-4004,0,0.196343,"Missing"
P01-1023,M95-1006,0,\N,Missing
P01-1023,M95-1011,0,\N,Missing
P03-1071,A00-2004,0,0.864903,"the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001)"
P03-1071,J86-3001,0,0.614029,"mber of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25 meetings to be segmented, each by at least three subjects. We opted for a linear representation of discourse, since finer-grained discourse structures (e.g. (Grosz and Sidner, 1986)) are generally considered to be difficult to mark reliably. Subjects were asked to mark each speaker change (potential boundary) as either boundary or non-boundary. In the resulting annotation, the agreed segmentation based on majority 1 While it would be desirable to have a broader variety of meetings, we hope that experiments on this corpus will still carry some generality. opinion contained 7.5 segments per meeting on average, while the average number of potential boundaries is 770. We used Cochran’s Q (1950) to evaluate the agreement among annotators. Cochran’s test evaluates the null hyp"
P03-1071,P98-2145,0,0.0135692,"are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003). This corpus is one of a growing number of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25"
P03-1071,P94-1002,0,0.983157,"ing approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg a"
P03-1071,P96-1038,0,0.663903,"91; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corp"
P03-1071,W98-1123,1,0.766858,"ule. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses,"
P03-1071,P93-1041,0,0.823497,"r features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Na"
P03-1071,P95-1015,0,0.0924914,"e counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed χ2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose χ2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is χ2 ≥ 6.635). Finally, induced cue phrases whose usage has never been described in other work were removed (marked with ∗ in Table 3). Indeed, there is a risk that the automatically derived list of cue phrases could be too specific to the word usage in 9 As in (Litman and Passonneau, 1995), we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word. okay Other Near boundary 64 657 Distant 740 25896 Table 2: okay (χ2 = 89.11, df = 1, p &lt; 0.01). okay shall ∗ anyway we’re ∗ alright let’s ∗ 93.05 27.34 23.95 17.67 16.09 14.54 but so and should ∗ good ∗ 13.57 11.65 10.99 10.21 7.70 to the discussion can greatly change from one discourse unit to the next. We try to capture significant changes in speakership by measuring the dissimilarity between two analysis windows. For each potential boundary, we count for each speaker"
P03-1071,J91-1002,0,0.963738,"speaker change, and other features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hi"
P03-1071,P93-1020,0,0.0314282,"wo analysis windows. For each potential boundary, we count for each speaker i the number of words that are uttered before (Li ) and after (Ri ) the potential boundary (we limit our analysis to a window of fixed size). The two distributions are normalized to form two probability distributions l and r, and significant changes of speakership are detected by computing their Jensen-Shannon divergence: JS(l, r) = 12 [D(l||avgl,r ) + D(r||avgl,r )] Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983). A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech. Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion. As an approximation of this distinction, we classified a silence that follows a question or in"
P03-1071,J97-1005,0,0.950475,"roadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997;"
P03-1071,J02-1002,0,0.625567,"ng the average (µ) and standard deviation (σ) of the p(mi ) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorithms. We reuse the same test corpus for our evaluation, in addition to two other test corpora we constructed to test how segmenters scale across genres and how they perform with texts with various 6 Concatenated documents correspond to reference segments. number of segments.7 We designed two test corpora, each of"
P03-1071,P94-1050,0,0.200681,"features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur"
P03-1071,P99-1046,0,0.0706811,"provement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range"
P03-1071,J01-1002,0,0.457388,"Missing"
P03-1071,P01-1064,0,0.942001,"es lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991). We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end. While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input document is first tokenized, non-content words are removed, 2 Four other meetings failed short the significance test, while there was little agreement on the two last ones (p > 0.1). and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics. Stemming will allow our algorithm to more accurately relate terms that are semantically close. The core algorithm of LCseg has two main parts: a method to identify and weight stro"
P03-1071,J93-3003,0,\N,Missing
P03-1071,C98-2140,0,\N,Missing
P04-1085,J96-1002,0,0.00482603,"Missing"
P04-1085,P97-1023,1,0.16656,"rk of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated wi"
P04-1085,N03-2012,1,0.55458,"sus decision is reached. Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task. For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision. In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither. Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available. We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational and structural features that look both forward and backward in the discourse. This allows us to acquire, and subsequently process, knowledge about who speaks to whom. We hypothesize that pragmatic features that center around previous agreemen"
P04-1085,W02-1002,0,0.0100279,"beling. Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data. In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations. Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)). Conditional models address these concerns. Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets. In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags + !      is decomposed as: 1        E4            !   . is the vector of observations and each is the index of a spurt. The probability distribution  @  / !- associated with each state of the Markov chain only depends on the preceding tag F  and the local observation "" . However, in order to incorporate more than one label dependency and, in"
P04-1085,W03-1209,0,0.011538,"Missing"
P04-1085,W04-2319,1,0.693642,"Missing"
P04-1085,J00-3003,1,0.795799,"Missing"
P04-1085,J93-3003,1,\N,Missing
P06-2027,H05-1015,1,0.931089,"quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the e"
P06-2027,J02-3001,0,0.0342983,"h 25, 2002), India (January 26, 2001), Iran (December 26, 2003), Japan (October 26, 2004), and Peru (June 23, 2001). Using this procedure we retrieve training document collections for 9 instances of airplane crashes, 5 instances of earthquakes, 13 instances of presidential elections, and 6 instances of terrorist attacks. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e"
P06-2027,harabagiu-etal-2002-multidocument,0,0.0613173,"in information extraction, there has been little work on their automatic design. In the Conceptual Case Frame Acquisition project (Riloff and Schmelzenbach, 1998), extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions. The system requires two sets of documents: those relevant to the domain and those irrelevant. Our approach does not require any domain-specific knowledge and uses only corpus-based statistics. The GIST exter summarization system (Harabagiu and Maiorano, 2002) used statistics over an arbitrary document collection together with semantic relations from WordNet. The created templates heavily depend on the topical relations encoded in WordNet. The template models an input collection of documents. If there is only one domain instance described in the input than the template is created for this particular instance rather than for a domain. In our work, we learn domain templates by cross-examining several collections of documents on the same topic, aiming for a general domain template. We rely on relations cross-mentioned in different instances of the dom"
P06-2027,H94-1032,0,0.0385689,"s. 207 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 207–214, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 Related Work mantic roles for the semantically identical verb phrases (e.g., arrests and placed under arrest). Our system automatically generates a template that captures the generally most important information for a particular domain and is reusable across multiple instances of that domain. Deciding what slots to include in the template, and what restrictions to place on their potential fillers, is a knowledge representation problem (Hobbs and Israel, 1994). Templates were used in the main IE competitions, the Message Understanding Conferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE,2 uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought. Although templates have been extensively used in information extraction, there has been little work on their automatic design."
P06-2027,C02-1165,0,0.0214531,"ck domain we also compared the lists of questions we got from our subjects with the terrorist attack template created by experts for the MUC competition. In this template we treated every slot as a separate question, excluding the first two slots which captured information about the text from which the template fillers were extracted and not about the domain. The results for this comparison are included in Table 2. Differences in domain complexity were studied by IE researchers. Bagga (1997) suggests a classification methodology to predict the syntactic complexity of the domain-related facts. Huttunen et al. (2002) analyze how component subevents of the domain are linked together and discuss the factors which contribute to the domain complexity. subj2 and MUC 0.59 Table 2: Creating gold standard. Jaccard metric values for interannotator agreement. analysis. Thus, for the earthquake and terrorist attack domains we got two lists of questions; for the airplane crash and presidential election domains we got three lists of questions. After the questions lists were created we studied the agreement among annotators on what information they consider is important for the domain and thus, should be included in th"
P06-2027,H94-1031,0,0.374121,"tational Linguistics 2 Related Work mantic roles for the semantically identical verb phrases (e.g., arrests and placed under arrest). Our system automatically generates a template that captures the generally most important information for a particular domain and is reusable across multiple instances of that domain. Deciding what slots to include in the template, and what restrictions to place on their potential fillers, is a knowledge representation problem (Hobbs and Israel, 1994). Templates were used in the main IE competitions, the Message Understanding Conferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE,2 uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought. Although templates have been extensively used in information extraction, there has been little work on their automatic design. In the Conceptual Case Frame Acquisition project (Riloff and Schmelzenbach, 1998), extraction patterns, a domain semantic lexicon,"
P06-2027,J05-1004,0,0.0585111,", 2004), and Peru (June 23, 2001). Using this procedure we retrieve training document collections for 9 instances of airplane crashes, 5 instances of earthquakes, 13 instances of presidential elections, and 6 instances of terrorist attacks. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e.g., the collection containing all the instances in the domain of airplane cr"
P06-2027,N04-1030,0,0.0284188,"26, 2001), Iran (December 26, 2003), Japan (October 26, 2004), and Peru (June 23, 2001). Using this procedure we retrieve training document collections for 9 instances of airplane crashes, 5 instances of earthquakes, 13 instances of presidential elections, and 6 instances of terrorist attacks. 5 Creating Templates In this work we build domain templates around verbs which are estimated to be important for the domains. Using verbs as the starting point we identify semantic dependencies within sentences. In contrast to deep semantic analysis (Fillmore and Baker, 2001; Gildea and Jurafsky, 2002; Pradhan et al., 2004; Harabagiu and L˘ac˘atus¸u, 2005; Palmer et al., 2005) we rely only on corpus statistics. We extract the most frequent syntactic subtrees which connect verbs to the lexemes used in the same subtrees. These subtrees are used to create domain templates. For each of the four domains described in Section 4, we automatically create domain templates using the following algorithm. Step 1: Estimate what verbs are important for the domain under investigation. We initiate our algorithm by calculating the probabilities for all the verbs in the document collection for one domain — e.g., the collection co"
P06-2027,J98-3005,1,0.720868,"ems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the eighties is different from the description of today’s terrorist events. How does one determine what facts are important for different kinds of responses? Often the types of facts that are important are hand en1 Unfortunately, NLP terminology is not standardized across different tasks. Two NLP tasks most close"
P06-2027,W98-1106,0,0.285899,"the Message Understanding Conferences (Hobbs and Israel, 1994; Onyshkevych, 1994; Marsh and Perzanowski, 1997). One of the recent evaluations, ACE,2 uses pre-defined frames connecting event types (e.g., arrest, release) to a set of attributes. The template construction task was not addressed by the participating systems. The domain templates were created manually by experts to capture the structure of the facts sought. Although templates have been extensively used in information extraction, there has been little work on their automatic design. In the Conceptual Case Frame Acquisition project (Riloff and Schmelzenbach, 1998), extraction patterns, a domain semantic lexicon, and a list of conceptual roles and associated semantic categories for the domain are used to produce multiple-slot case frames with selectional restrictions. The system requires two sets of documents: those relevant to the domain and those irrelevant. Our approach does not require any domain-specific knowledge and uses only corpus-based statistics. The GIST exter summarization system (Harabagiu and Maiorano, 2002) used statistics over an arbitrary document collection together with semantic relations from WordNet. The created templates heavily d"
P06-2027,P03-1029,0,0.219201,"in different instances of the domain to automatically prioritize roles and relationships for selection. Topic Themes (Harabagiu and L˘ac˘atus¸u, 2005) used for multi-document summarization merge various arguments corresponding to the same se2 Atomic events also model an input document collection (Filatova and Hatzivassiloglou, 2003) and are created according to the statistics collected for co-occurrences of named entity pairs linked through actions. GISTexter, atomic events, and Topic Themes were used for modeling a collection of documents rather than a domain. In other closely related work, Sudo et al. (2003) use frequent dependency subtrees as measured by TF*IDF to identify named entities and IE patterns important for a given domain. The goal of their work is to show how the techniques improve IE pattern acquisition. To do this, Sudo et al. constrain the retrieval of relevant documents for a MUC scenario and then use unsupervised learning over descriptions within these documents that match specific types of named entities (e.g., Arresting Agency, Charge), thus enabling learning of patterns for specific templates (e.g., the Arrest scenario). In contrast, the goal of our work is to show how similar"
P06-2027,H01-1054,0,0.0467244,"esponse containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A description of a terrorist event in Latin America in the eighties is different from the description of today’s terrorist events. How does one determine what facts are important for different kinds of responses? Often the types of facts that are important are hand en1 Unfortunately, NLP terminology is not standardized across different tasks. Two NLP tasks most close to our research are"
P06-2027,W04-3256,0,0.0669799,"e for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of inventions. A descr"
P06-2027,O97-1012,0,0.0196532,"dates, while another subject was interested only in the outcome of the presidential election. For the terrorist attack domain we also compared the lists of questions we got from our subjects with the terrorist attack template created by experts for the MUC competition. In this template we treated every slot as a separate question, excluding the first two slots which captured information about the text from which the template fillers were extracted and not about the domain. The results for this comparison are included in Table 2. Differences in domain complexity were studied by IE researchers. Bagga (1997) suggests a classification methodology to predict the syntactic complexity of the domain-related facts. Huttunen et al. (2002) analyze how component subevents of the domain are linked together and discuss the factors which contribute to the domain complexity. subj2 and MUC 0.59 Table 2: Creating gold standard. Jaccard metric values for interannotator agreement. analysis. Thus, for the earthquake and terrorist attack domains we got two lists of questions; for the airplane crash and presidential election domains we got three lists of questions. After the questions lists were created we studied t"
P06-2027,N03-1003,0,0.0976723,"instances of airplane crashes, 3 instances of earthquakes, 6 instances of presidential elections and 3 instances of terrorist attacks. The number of the documents corresponding to the instances varies greatly (from two documents for one of the earthquakes up to 156 documents for one of the terrorist attacks). This variation in the number of documents per topic is typical for the TDT corpus. Many of the current approaches of domain modeling collapse together different instances and make the decision on what information is important for a domain based on this generalized corpus (Collier, 1998; Barzilay and Lee, 2003; Sudo et al., 2003). We, on the other hand, propose to cross-examine these instances keeping them separated. Our goal is to eliminate dependence on how well the corpus is balanced and to avoid the possibility of greater impact on the domain template of those instances which have more documents. After reading about presidential elections in different countries on different years, a reader has a general picture of this process. Later, when reading about a new presidential election, the reader already has in her mind a set of questions for which she expects answers. This process can be called do"
P06-2027,W03-1016,1,0.184761,"s no well-defined procedure for the evaluation of the domain template quality. Thus, we propose a methodology for identifying what information should be present in the template. Using this information we evaluate the automatically created domain templates through the text snippets retrieved according to the created templates. 1 Introduction Open-ended question-answering (QA) systems typically produce a response containing a variety of specific facts proscribed by the question type. A biography, for example, might contain the date of birth, occupation, or nationality of the person in question (Duboue and McKeown, 2003; Zhou et al., 2004; Weischedel et al., 2004; Filatova and Prager, 2005). A definition may contain the genus of the term and characteristic attributes (Blair-Goldensohn et al., 2004). A response to a question about a terrorist attack might include the event, victims, perpetrator and date as the templates designed for the Message Understanding Conferences (Radev and McKeown, 1998; White et al., 2001) predicted. Furthermore, the type of information included varies depending on context. A biography of an actor would include movie names, while a biography of an inventor would include the names of"
P06-2027,M98-1002,0,\N,Missing
P06-2027,M98-1003,0,\N,Missing
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P09-4003,kingsbury-palmer-2002-treebank,0,0.0918135,"lations and other aspects of narrative. A built-in naturallanguage generation component regenerates text from the formal structures, which eases the annotation process. We have run collection experiments with the tool and shown that non-experts can easily create semantic encodings of short fables. We present this tool as a stand-alone, reusable resource for research in semantics in which formal encoding of text, especially in a narrative form, is required. 1 Introduction Research in language processing has benefited greatly from the collection of large annotated corpora such as Penn PropBank (Kingsbury and Palmer, 2002) and Penn Treebank (Marcus et al., 1993). Such projects typically involve a formal model (such as a controlled vocabulary of thematic roles) and a corpus of text that has been annotated against the model. One persistent tradeoff in building such resources, however, is that a model with a wider scope is more challenging for annotators. For example, part-of-speech tagging is an easier task than PropBank annotation. We believe that careful user interface design can alleviate difficulties in annotating texts against deep semantic models. In this demonstration, we present a tool we have developed,"
P09-4003,J93-2004,0,0.0306791,"-in naturallanguage generation component regenerates text from the formal structures, which eases the annotation process. We have run collection experiments with the tool and shown that non-experts can easily create semantic encodings of short fables. We present this tool as a stand-alone, reusable resource for research in semantics in which formal encoding of text, especially in a narrative form, is required. 1 Introduction Research in language processing has benefited greatly from the collection of large annotated corpora such as Penn PropBank (Kingsbury and Palmer, 2002) and Penn Treebank (Marcus et al., 1993). Such projects typically involve a formal model (such as a controlled vocabulary of thematic roles) and a corpus of text that has been annotated against the model. One persistent tradeoff in building such resources, however, is that a model with a wider scope is more challenging for annotators. For example, part-of-speech tagging is an easier task than PropBank annotation. We believe that careful user interface design can alleviate difficulties in annotating texts against deep semantic models. In this demonstration, we present a tool we have developed, S CHEHERAZADE, for deep annotation of te"
P09-4003,prasad-etal-2008-penn,0,0.0347979,"on allows for machine learning on the thematic dimension of narrative – that is, the aspects that unite a series of related facts into an engaging and fulfilling experience for a reader. Our methodology is novel in its synthesis of several annotation goals and its focus on content rather than expression. We aim to separate the narrative’s fabula, the content dimension of the story, from the rhetorical presentation at the textual surface (sjuˇzet) (Bal, 1997). To this end, our model incorporates formal elements found in other discourse-level annotation projects such as Penn Discourse Treebank (Prasad et al., 2008) and temporal markup languages such as TimeML (Mani and Pustejovsky, 2004). We call the representation a story graph, because these elements are embodied by nodes and connected by arcs that represent relationships such as temporal order and motivation. More specifically, our annotation process involves the construction of propositions to best approximate each of the events described in the textual story. Every element of the representation is formally defined from controlled vocabularies: the verb frames, with their thematic roles, are adapted from VerbNet (Kipper et al., 2006), the largest ve"
P09-4003,W04-0208,0,\N,Missing
P10-1015,P08-1090,0,0.0169555,"the writers of anonymous texts by comparing their style to that of a corpus of known authors (Mostellar and Wallace, 1984). Determining instances of “text reuse,” a type of paraphrasing, is also a form of analysis at the lexical level, and it has recently been used to validate theories about the lineage of ancient texts (Lee, 2007). Analysis of literature using more semanticallyoriented techniques has been rare, most likely because of the difficulty in automatically determining meaningful interpretations. Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al., 2004), a knowledge-based strategy. On the other hand, literary theorists, linguists and others have long developed symbolic but non-computational models for novels. For example, Moretti (2005) has graphically mapped out texts according to geography, social connections and other variables. While researchers have not attempted the automatic construction of social networks representing connections between characters in a corpus of novels, the ACE prog"
P10-1015,doddington-etal-2004-automatic,0,0.00893145,"event calculus for characterizing stories written by children (Halpin et al., 2004), a knowledge-based strategy. On the other hand, literary theorists, linguists and others have long developed symbolic but non-computational models for novels. For example, Moretti (2005) has graphically mapped out texts according to geography, social connections and other variables. While researchers have not attempted the automatic construction of social networks representing connections between characters in a corpus of novels, the ACE program has involved entity and relation extraction in unstructured text (Doddington et al., 2004). Other recent work in social network construction has explored the use of structured data such as email headers (McCallum et al., 2007) and U.S. Senate bill cosponsorship (Cho and Fowler, 2010). In an analysis of discussion forums, Gruzd and Haythornthwaite (2008) explored the use of message text as well as posting data to infer who is talking to whom. In this paper, we also explore how to build a network based on conversational interaction, but we analyze the reported dialogue found in novels to determine the links. The kinds of language that is used to signal such information is quite diffe"
P10-1015,P05-1045,0,0.00653843,"Missing"
P10-1015,W04-3217,0,0.0093312,",” a type of paraphrasing, is also a form of analysis at the lexical level, and it has recently been used to validate theories about the lineage of ancient texts (Lee, 2007). Analysis of literature using more semanticallyoriented techniques has been rare, most likely because of the difficulty in automatically determining meaningful interpretations. Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al., 2004), a knowledge-based strategy. On the other hand, literary theorists, linguists and others have long developed symbolic but non-computational models for novels. For example, Moretti (2005) has graphically mapped out texts according to geography, social connections and other variables. While researchers have not attempted the automatic construction of social networks representing connections between characters in a corpus of novels, the ACE program has involved entity and relation extraction in unstructured text (Doddington et al., 2004). Other recent work in social network construction has expl"
P10-1015,P07-1060,0,0.00964682,"mputational Linguistics 2 Related Work 3 Computer-assisted literary analysis has typically occurred at the word level. This level of granularity lends itself to studies of authorial style based on patterns of word use (Burrows, 2004), and researchers have successfully “outed” the writers of anonymous texts by comparing their style to that of a corpus of known authors (Mostellar and Wallace, 1984). Determining instances of “text reuse,” a type of paraphrasing, is also a form of analysis at the lexical level, and it has recently been used to validate theories about the lineage of ancient texts (Lee, 2007). Analysis of literature using more semanticallyoriented techniques has been rare, most likely because of the difficulty in automatically determining meaningful interpretations. Some exceptions include recent work on learning common event sequences in news stories (Chambers and Jurafsky, 2008), an approach based on statistical methods, and the development of an event calculus for characterizing stories written by children (Halpin et al., 2004), a knowledge-based strategy. On the other hand, literary theorists, linguists and others have long developed symbolic but non-computational models for n"
P11-1077,de-marneffe-etal-2006-generating,0,0.0403107,"Missing"
P11-1077,P03-1054,0,0.0152029,"as written by a unique individual and includes a user profile and up to 25 recent posts written between 2000-2010 with the most recent post being written in 2009-2010. The birth dates of the bloggers range in years from 1940 to 2000 and thus, their age ranges from 10 to 70 in 2010. Figure 1 shows the number of bloggers per age in our group with birth dates from 1950 to 1996. The majority of bloggers on LiveJournal were born between 1978-1989. 4 Methods We pre-processed the data to add Part-ofSpeech tags (POS) and dependencies (de Marneffe et al., 2006) between words using the Stanford Parser (Klein and Manning, 2003a; Klein and Manning, 2003b). The POS and syntactic dependencies were only found for approximately the first 90 words in each sentence. Our classification method investigates 17 different features that fall into three categories: online behavior, lexical-stylistic and lexical-content. All of the features we used are explained in Table 1 along with their trend as age decreases where applicable. Any feature that increased, decreased, or fluctuated should have some positive impact on the accuracy of predicting age. 4.1 Online Behavior and Interests Online behavior features are blog specific, such"
P11-1077,J93-1007,0,0.0294768,"rds increased as bloggers got younger. Slang and punctuation, which excludes the emoticons and acronyms counted in the other features, increased as well, but not as significantly. The length of sentences decreased as bloggers got younger and the number of links/images varied across all years as shown in Figure 2(c). 4.3 Lexical - Content The last category of features described in Table 1.3 consists of collocations and words, which are content based lexical terms. The top words are produced using a typical “bag-of-words” approach. The top collocations are computed using a system called Xtract (Smadja, 1993). 767 We use Xtract to obtain important lexical collocations, syntactic collocations, and POS collocations as features from our text. Syntactic collocations refer to significant word pairs that have specific syntactic dependencies such as subject/verb and verb/object. Due to the length of time it takes to run this program, we ran Xtract on 1500 random blogs from each age group and examined the first 1000 words per blog. We looked at 1.5 million words in total and found approximately 2500-2700 words that were repeated more than 50 times. We extracted the top 200 words and collocations sorted by"
P11-2044,N09-2047,0,0.0747056,"Missing"
P11-2044,W02-1022,0,0.0330703,"(Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment r"
P11-2044,W07-1427,0,0.473166,"Missing"
P11-2044,P08-2007,0,0.0410148,"Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1 , e2 , . . .}, each belonging to one of the following types: • INS and DEL edits covering unaligned words in T1 and T2 respectively • SUB and EQ edits connecting a phrase in T1 to a phrase in T2 . EQ"
P11-2044,D08-1019,0,0.0199246,"which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1 , e2 , . . .}, each belonging t"
P11-2044,N06-1014,0,0.0464115,"niques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work (MacCartney et al., 2008) has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006). However, MANLI’s use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, MacCartney et al. (2008) employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage o"
P11-2044,D08-1084,0,0.656657,"Missing"
P11-2044,P09-1039,0,0.0277239,"al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1 , e2 , . . .}, each belonging to one of the following types: • INS and DEL edits covering unaligned"
P11-2044,N10-1069,0,0.0256976,"larity and WordNet for determining semantic relatedness, forgoing NomBank and the distributional similarity resources used in the original MANLI implementation. 3.3 Parameter Inference Feature weights are learned using the averaged structured perceptron algorithm (Collins, 2002), an intuitive structured prediction technique. We deviate from MacCartney et al. (2008) and do not introduce L2 normalization of weights during learning as this could have an unpredictable effect on the averaged parameters. For efficiency reasons, we parallelize the training procedure using iterative parameter mixing (McDonald et al., 2010) in our experiments. 3.4 Decoding The decoding problem is that of finding the highestscoring alignment under some parameter values for the model. MANLI’s phrase-based representation makes decoding more complex because the segmentation of T1 and T2 into phrases is not known beforehand. Every pair of phrases considered for inclusion in an alignment must adhere to some consistent segmentation so that overlapping edits and uncovered words are avoided. Consequently, the decoding problem cannot be factored into a number of independent decisions and MANLI searches for a good alignment using a stochas"
P11-2044,N10-1044,1,0.874236,"g alignment tasks over various corpora on the same machine using the models trained as per §4.1. We observe a twenty-fold improvement in performance with ILPbased decoding. It is important to note that the specific implementations being compared2 may be responsible for the relative speed of decoding. The short hypotheses featured in the RTE2 corpus (averaging 11 words) dampen the effect of the quadratic growth in number of edits with sentence length. For this reason, we also run the aligners on a corpus of 297 related sentence pairs which don’t have a particular disparity in sentence lengths (McKeown et al., 2010). The large difference in decoding time illustrates the scaling limitations of the searchbased decoder. 5 Syntactically-Informed Constraints The use of an integer program for decoding provides us with a convenient mechanism to prevent common alignment errors by introducting additional constraints on edits. For example, function words such as determiners and prepositions are often misaligned just because they occur frequently in many different contexts. Although MANLI makes use of contextual features which consider the similarity of neighboring words around phrase pairs, outof-context alignment"
P11-2044,J03-1002,0,0.0198752,"y on alignment techniques to establish the overlap between the given premise and a hypothesis before determining if the former entails the latter. Such monolingual alignment techniques are also frequently employed in systems for paraphrase generation, multi-document summarization, sentence fusion and question answering. Previous work (MacCartney et al., 2008) has presented a phrase-based monolingual aligner for NLI (MANLI) that has been shown to significantly outperform a token-based NLI aligner (Chambers et al., 2007) as well as popular alignment techniques borrowed from machine translation (Och and Ney, 2003; Liang et al., 2006). However, MANLI’s use of a phrase-based alignment representation appears to pose a challenge to the decoding task, i.e. the task of recovering the highest-scoring alignment under some parameters. Consequently, MacCartney et al. (2008) employ a stochastic search algorithm to decode alignments approximately while remaining consistent with regard to phrase segmentation. In this paper, we propose an exact decoding technique for MANLI that retrieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (IL"
P11-2044,W04-3219,0,0.0551956,"rk Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et"
P11-2044,C96-2141,0,0.320615,"rieves the globally optimal alignment for a sentence pair given some parameters. Our approach is based on integer linear programming (ILP) and can leverage optimized general-purpose LP solvers to recover exact solutions. This strategy boosts decoding speed by an order of magnitude over stochastic search in our experiments. Additionally, we introduce hard syntactic constraints on alignments produced by the model, yielding better precision and a large increase in the number of perfect alignments produced over our evaluation corpus. 2 Related Work Alignment is an integral part of statistical MT (Vogel et al., 1996; Och and Ney, 2003; Liang et al., 2006) but the task is often substantively different from monolingual alignment, which poses unique challenges depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 20"
P11-2044,D10-1050,0,0.0451577,"s depending on the application (MacCartney et al., 2008). Outside of NLI, prior research has also explored the task of monolingual word align254 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 254–259, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ment using extensions of statistical MT (Quirk et al., 2004) and multi-sequence alignment (Barzilay and Lee, 2002). ILP has been used extensively for applications ranging from text-to-text generation (Clarke and Lapata, 2008; Filippova and Strube, 2008; Woodsend et al., 2010) to dependency parsing (Martins et al., 2009). It has also been recently employed for finding phrase-based MT alignments (DeNero and Klein, 2008) in a manner similar to this work; however, we further build upon this model through syntactic constraints on the words participating in alignments. 3 The MANLI Aligner Our alignment system is structured identically to MANLI (MacCartney et al., 2008) and uses the same phrase-based alignment representation. An alignment E between two fragments of text T1 and T2 is represented by a set of edits {e1 , e2 , . . .}, each belonging to one of the following t"
P11-2044,W02-1001,0,\N,Missing
P11-2118,C00-1072,0,0.331437,"al structure of various datasets (Hofmann, 1999; Blei et al., 2010; Li et al., 2007). Our purpose here is more specialized and similar to that of Labeled LDA (Ramage et al., 2009a) or Fixed hLDA (Reisinger and Pa¸sca, 2009) where the set of topics associated with a document is known a priori. In both cases, document labels are mapped to constraints on the set of topics on which the - otherwise unaltered - topic inference algorithm is to be applied. Lastly, while most recent developments have been based on unsupervised data, it is also worth mentioning earlier approaches like Topic Signatures (Lin and Hovy, 2000) where words (or phrases) characteristic of a topic are identified using a statistical test of dependence. Our first model extends this approach to the hierarchical setting, building actual topic models based on the selected vocabulary. 3 Information-Theoretic Approach The assumption that topics are known a-priori allows us to extend the concept of Topic Signatures to a hierarchical setting. Lin and Hovy (2000) describe a Topic Signature as a list of words highly correlated with a target concept, and use a χ2 estimator over labeled data to decide as to the allocation of a word to a topic. Here"
P11-2118,D09-1026,0,0.226084,"low the derivation of both p(wi |cd,j ) and θ and present early experimental results showing that explicit hierarchical information of content can indeed be used as a basis for content modeling purposes. 2 Related Work While several efforts have focused on the DMOZ corpus, often as a reference for Web summarization 670 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 670–675, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics tasks (Berger and Mittal, 2000; Delort et al., 2003) or Web clustering tasks (Ramage et al., 2009b), very little research has attempted to make use of its hierarchy as is. The work by Sun et al. (2005), where the DMOZ hierarchy is used as a basis for a hierarchical lexicon, is closest to ours although their contribution is not a full-fledged content model, but a selection of highly salient vocabulary for every category of the hierarchy. The problem considered in this paper is connected to the area of Topic Modeling (Blei and Lafferty, 2009) where the goal is to reduce the surface complexity of text documents by modeling them as mixtures over a finite set of topics2 . While the inferred mo"
P11-2118,P09-1070,0,0.0492514,"Missing"
P13-2013,D09-1036,0,0.588391,"Missing"
P13-2013,P02-1047,0,0.913208,"their experiments were run on data that is unnatural in two ways. First, it is balanced. Second, it is constructed with the same unsupervised method they use to extract the word pairs by assuming that the patterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. 3.1 Word Pairs The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs will not be seen in training. In fact, even the larger corpus of Marcu and Echihabi (2002) may not be quite large"
P13-2013,W12-1614,0,0.444069,"our knowledge the only other work utilizing a similar approach. They used aggregated word pair set features to predict whether or not a sentence is argumentative. Their method is to group together word pairs that have been collected around the same explicit discourse marker: for every discourse marker such as therefore or however, they have a single feature whose value depends only on the word pairs Zhou et al. (2010) used a similar method and added features that explicitly try to predict the implicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 70 collected around that marker. This is reasonable given the intuition that the marker pattern is unambiguous and points at a particular relation. Using one feature per marker can be seen as analogous (yet complementary) to Zhou et al. (2010)’s approach of trying to predict the implicit connective by giving a score to each marker using a language model. This work uses binary features which o"
P13-2013,P09-2004,0,0.420711,"Missing"
P13-2013,P09-1077,0,0.724224,"s 69–73, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3 some success, their experiments were run on data that is unnatural in two ways. First, it is balanced. Second, it is constructed with the same unsupervised method they use to extract the word pairs by assuming that the patterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. 3.1 Word Pairs The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many important word pairs wi"
P13-2013,prasad-etal-2008-penn,0,0.668333,"which is counter-intuitive. They attribute this finding to the sparsity of the feature space. An analysis in (Pitler et al., 2009) also shows that the top word pairs (ranked by information gain) all contain common functional words, and are not at all the semantically-related content words that were imagined. In the case of some reportedly useful word pairs (the-and; inthe; the-of...) it is hard to explain how they might affect performance except through overfitting. More recently, implicit relation prediction has been evaluated on annotated implicit relations from the Penn Discourse Treebank (Prasad et al., 2008). PDTB uses hierarchical relation types which abstract over other theories of discourse such as RST (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003). It contains 40, 600 annotated relations from the WSJ corpus. Each relation has two arguments, Arg1 and Arg2, and the annotators decide whether it is explicit or implicit. The first to evaluate directly on PDTB in a realistic setting were Pitler et al. (2009). They used word pairs as well as additional features to train four binary classifiers, each corresponding to one of the high-level PDTB relation classes. Although other feature"
P13-2013,H05-1044,0,0.012843,"Negation Length Comp. 20.07 14.24 23.84 17.49 16.46 18.62 20.68 8.28 20.75 Cont. 34.07 24.84 38.58 28.92 26.36 31.59 34.5 22.47 31.28 Exp. 52.96 49.6 49.97 13.84 65.15 59.8 43.16 75.87 65.72 Temp. 11.58 10.04 13.16 10.72 11.58 13.37 12.1 11.1 10.19 fined as the highest level Levin verb class (Levin, 1993) from the LCS database (Dorr, 2001). Money, Percentages and Numbers (MPN): The counts of currency symbols/abbreviations, percentage signs or cues (“percent”, “BPS”...) and numbers in each argument. Modality: Presence or absence of each English modal in each argument. Polarity: Based on MPQA (Wilson et al., 2005). We include the counts of positive and negative words according to the MPQA subjectivity lexicon for both arguments. Unlike Pitler et al. (2009), we do not use neutral polarity features. We also do not explicitly group negation with polarity (although we do have separate negation features). Affect: Based on the Dictionary of Affect in Language (Whissell, 1989). Each word in the DAL gets a score for three dimensions - pleasantness (pleasant - unpleasant), activation (passive - active) and imagery (hard to imagine - easy to imagine). We use the average score for each dimension in each argument"
P13-2013,C10-2172,0,0.74876,"aggregated sets of words. For this approach to be effective, the pairs we choose to group together should have similar meaning with regard to predicting the relation. Biran and Rambow (2011) is to our knowledge the only other work utilizing a similar approach. They used aggregated word pair set features to predict whether or not a sentence is argumentative. Their method is to group together word pairs that have been collected around the same explicit discourse marker: for every discourse marker such as therefore or however, they have a single feature whose value depends only on the word pairs Zhou et al. (2010) used a similar method and added features that explicitly try to predict the implicit marker in the relation, increasing performance. Most recently to the best of our knowledge, Park and Cardie (2012) achieved the highest performance by optimizing the feature set. Another work evaluating on PDTB is (Lin et al., 2009), who are unique in evaluating on the more fine-grained second-level relation classes. 70 collected around that marker. This is reasonable given the intuition that the marker pattern is unambiguous and points at a particular relation. Using one feature per marker can be seen as ana"
P13-2013,N07-1054,1,0.842631,"Computational Linguistics, pages 69–73, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 3 some success, their experiments were run on data that is unnatural in two ways. First, it is balanced. Second, it is constructed with the same unsupervised method they use to extract the word pairs by assuming that the patterns correspond to a particular relation and collecting the arguments from an unannotated corpus. Even if the assumption is correct, these arguments are really taken from explicit relations with their markers removed, which as others have pointed out (Blair-Goldensohn et al., 2007; Pitler et al., 2009) may not look like true implicit relations. 3.1 Word Pairs The Problem: Sparsity While Marcu and Echihabi (2002)’s approach of training a classifier from an unannotated corpus provides a relatively large amount of training data, this data does not consist of true implicit relations. However, the approach taken by Pitler et al. (2009) and repeated in more recent work (training directly on PDTB) is problematic as well: when training a model with so many sparse features on a dataset the size of PDTB (there are 22, 141 non-explicit relations overall), it is likely that many i"
P15-1155,W04-1017,0,0.0543541,"disaster management (Starbird and Palen, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and Fernando Diaz Microsoft Research fdiaz@microsoft.com salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large. While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (Allan et al., 1998; Filatova and Hatzivassiloglou, 2004; Wang et al., 2011), these experiments were not developed to handle streaming data from a heterogeneous environment at web scale. These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information. In this paper, we present an update summarization system to track events across time. Our system predicts sentence salience in the context of a large-scale event, such as a disaster, and integrates these predictions into a clustering based multidocument summarization system. We demonstrate that combining salience with clus"
P15-1155,S12-1086,0,0.00773789,". Each event has anywhere from 50 to several hundred nuggets in total in our gold dataset. We describe how these nuggets are found in Section 4. Throughout our treatment of our algorithm, the salience of an update captures the degree to which it reflects an event’s unobserved nuggets. Assuming that we have a text representation for each of our nuggets, the salience of an update u with respect to a set of nuggets N is defined as, salience(u) = maxn∈N sim(u, n) (1) where sim(·) is the semantic similarity such as the cosine similarity of latent vectors associated with the update and nugget text (Guo and Diab, 2012). 3.1 Update Summarization Our system architecture follows a simple pipeline design where each stage provides an additional level of processing or filtering of the input sentences. We begin with an empty update summary U . At each hour we receive a new batch of sentences bt from the stream of event relevant documents and perform the following actions: 1. predict the salience of sentences in bt (Section 3.2), 2. select a set of exemplar sentences in bt by combining clustering with salience predictions (Section 3.3), Salience Prediction 3.2.1 Features We want our model to be predictive of senten"
P15-1155,N09-1041,0,0.0347539,"are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed na¨ıve Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the expressiveness of their features is limited. Our model expands on these basic features to account for geographic, temporal, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences, i.e. there are large portions of the corpora that we specifically want to avoid. Several researchers have recognized the importance of summarization during natural disasters. (Guo et al., 2013) developed a system for detecting novel, relevant, and comprehensive sentences immediately"
P15-1155,X98-1026,0,0.0488443,"or input centroid). However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite beneficial. The second category, predictive approaches, includes ranking and classification based methods. Sentences have been ranked by the average word probability, average TF*IDF score, and the number of topically related words (topic-signatures in the summarization literature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed na¨ıve Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent"
P15-1155,P11-1052,0,0.0142848,"l in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed na¨ıve Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the expressiveness of their features is limited. Our model expands on these basic features to account for geographic, temporal, and language model features. The last category includes probabilistic (Haghighi and Vanderwende, 2009), information theoretic, and set cover (Lin and Bilmes, 2011) approaches. While these methods are focused on producing diverse summaries, they are difficult to adapt to the streaming setting, where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences, i.e. there are large portions of the corpora that we specifically want to avoid. Several researchers have recognized the importance of summarization during natural disasters. (Guo et al., 2013) developed a system for detecting novel, relevant, and comprehensive sentences immediately after a natural disaster. (Wang and Li, 2010) present a clust"
P15-1155,I11-1032,1,0.781978,"len, 2013). Multi-document summarization has the potential to assist the crisis informatics community. Automatic summarization could deliver relevant and Fernando Diaz Microsoft Research fdiaz@microsoft.com salient information at regular intervals, even when human volunteers are unable to. Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large. While methods for identifying, tracking, and summarizing events from text based input have been explored extensively (Allan et al., 1998; Filatova and Hatzivassiloglou, 2004; Wang et al., 2011), these experiments were not developed to handle streaming data from a heterogeneous environment at web scale. These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information. In this paper, we present an update summarization system to track events across time. Our system predicts sentence salience in the context of a large-scale event, such as a disaster, and integrates these predictions into a clustering based multidocument summarization system. We demonstrate that combining salience with clustering produces more"
P15-1155,C00-1072,0,0.0682749,"However, they rely chiefly on redundancy. When applied to an unfolding event, there may not exist enough redundant content at the event onset for these methods to exploit. Once the event onset has passed, however, the redundancy reduction of these methods is quite beneficial. The second category, predictive approaches, includes ranking and classification based methods. Sentences have been ranked by the average word probability, average TF*IDF score, and the number of topically related words (topic-signatures in the summarization literature) (Nenkova and Vanderwende, 2005; Hovy and Lin, 1998; Lin and Hovy, 2000). The first two statistics are easily computable from the input sentences, while the third only requires an additional, generic background corpus. In classification based methods, model features are usually derived from human generated summaries, and are non-lexical in nature (e.g., sentence starting position, number of topic-signatures, number of unique words). Seminal work in this area has employed na¨ıve Bayes and logistic regression classifiers to identify sentences for summary inclusion (Kupiec et al., 1995; Conroy et al., 2001). While these methods are less dependent on redundancy, the e"
P15-1155,W04-1013,0,0.0346554,"er to accomplish this, for each event, assessors were provided with the revision history of the Wikipedia page associated with the event. For example, the 1612 revision history for the Wikipedia page for ‘Hurricane Sandy’ will contain text additions including those related to individual nuggets. The assessment task involves reviewing the Wikipedia revisions in the evaluation time frame and marking the text additions capturing a new, unique nugget. More detail on this process can be found in the track description (Aslam et al., 2013). 5 Experiments We evaluate our system on two metrics: ROUGE (Lin, 2004), an automatic summarization method and an evaluation of system expected gain and comprehensiveness—metrics adapted from the TREC TS track (Aslam et al., 2013). 5.1 Training and Testing Of the 25 events in the TREC TS data, 24 are covered by the news portion of the TREC KBA Stream Corpus. From these 24, we set aside three events to use as a development set. All system salience and similarity threshold parameters are tuned on the development set to maximize ROUGE-2 F1 scores. We train a salience model for each event using 1000 sentences randomly sampled from the event’s document stream. We perf"
P16-1180,D10-1049,0,0.170472,"Missing"
P16-1180,P05-1074,0,0.0156069,"Table 4: Comparison with the precision and paraphrases generated per input sentence (PPS) of relevant prior work While we wanted to show a meaningful comparison with another method from previous work, none of them do what we are doing here - extraction of sentence-size paraphrasal templates from a non-aligned corpus - and so a comparison using the same data would not be fair (and in most cases, not possible). While it seems that providing the results of human evaluation without comparison to prior methods is the norm in most relevant prior work (Ibrahim et al., 2003; Pas¸ca and Dienes, 2005; Bannard and Callison-Burch, 2005; Fujita et al., 2012), we wanted to at least get some sense of where we stand in comparison to other methods, and so we provide a list of (not directly comparable) results reported by other authors in Table 4.4 While it is impossible to meaningfully compare and rate such different methods, these numbers support the conclusion that our singlecorpus, domain-agnostic approach achieves a precision that is similar to or better than other methods. We also include the paraphrase per sentence (PPS) value - the ratio of paraphrases extracted to the number of input sentences of the corpus - for each me"
P16-1180,J10-4006,0,0.0390703,"orpus (Dolan et al., 2004). Mihalcea et al. (2006) evaluated a wide range of lexical and semantic measures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast literature on semantics, including but not limited to lexical, distributional and knowledge-based semantics. Of particular interest to us is the work of Blacoe and Lapata (2012"
P16-1180,N03-1003,0,0.0572372,"dently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not directly comparable. In our approach, it is precisely the fact that we use a rich type system"
P16-1180,P01-1008,1,0.585979,"paraphrasal templates is not important (Chambers and Jurafsky, 2011). One exception which expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. H"
P16-1180,P08-1077,0,0.0249921,"are not, by themselves, paraphrases and avoid using a comparable corpus. Sekine (2005) produces typed phrase templates, but the approach does not allow learning non-trivial paraphrases (that is, paraphrases that do not share the exact same keywords) from sentences that do not share the same entities (thus remaining dependent on a comparable corpus), and the type system is not very rich. In addition, that approach is limited to learning short paraphrases of relations between two entities. Another line of research is based on contextual similarity (Lin and Pantel, 2001; Pas¸ca and Dienes, 2005; Bhagat and Ravichandran, 2008). Here, shorter (phrase-level) paraphrases are extracted from a single corpus when they appear in a similar lexical (and in later approaches, also syntactic) context. The main drawbacks of these methods are their inability to handle longer paraphrases and their tendency to find phrase pairs that are semantically related but not real paraphrases (e.g. antonyms or taxonomic siblings). More recent work on paraphrase detection has, for the most part, focused on classifying provided sentence pairs as paraphrases or not, using the Microsoft Paraphrase Corpus (Dolan et al., 2004). Mihalcea et al. (20"
P16-1180,D12-1050,0,0.113131,"Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast literature on semantics, including but not limited to lexical, distributional and knowledge-based semantics. Of particular interest to us is the work of Blacoe and Lapata (2012), which show that simple combination methods (e.g., vector multiplication) in classic vector space representations outperform more sophisticated alternatives which take into account syntax and which use deep representations (e.g. word embeddings, or the distributional memory approach). This finding is appealing since classic vector space representation (distributional vectors) are easy to obtain and are interpretable, making it possible to drill into errors. 3 Taxonomy Our method relies on a type system which links entities to one another in a taxonomy. We use a combination of WordNet (Fellbau"
P16-1180,P11-1098,0,0.0344343,"emplates from non-aligned text for general NLG, as aligned corpora are difficult to obtain for most domains. While template extraction has been a relatively small part of NLG research, it is very prominent in the field of Information Extraction (IE), beginning with Hearst (1992). There, however, the goal is to extract good data and not to extract templates that are good for generation. Many pattern extraction (as it is more commonly referred to in IE) approaches focus on semantic patterns that are not coherent lexically or syntactically, and the idea of paraphrasal templates is not important (Chambers and Jurafsky, 2011). One exception which expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003;"
P16-1180,P09-1053,0,0.119334,"paraphrases or not, using the Microsoft Paraphrase Corpus (Dolan et al., 2004). Mihalcea et al. (2006) evaluated a wide range of lexical and semantic measures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast literature on semantics, including but not limited to lexical, distributional and knowledge-based semantics. Of particul"
P16-1180,C04-1051,0,0.25581,"es, 2005; Bhagat and Ravichandran, 2008). Here, shorter (phrase-level) paraphrases are extracted from a single corpus when they appear in a similar lexical (and in later approaches, also syntactic) context. The main drawbacks of these methods are their inability to handle longer paraphrases and their tendency to find phrase pairs that are semantically related but not real paraphrases (e.g. antonyms or taxonomic siblings). More recent work on paraphrase detection has, for the most part, focused on classifying provided sentence pairs as paraphrases or not, using the Microsoft Paraphrase Corpus (Dolan et al., 2004). Mihalcea et al. (2006) evaluated a wide range of lexical and semantic measures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) an"
P16-1180,W13-0108,0,0.115061,"paraphrases found and the precision, which makes it easy to adjust our approach to the needs of various applications. 2 Related Work To our knowledge, although several works exist which utilize paraphrasal templates in some way, the task of extracting them has not been defined as such in the literature. The reason seems to be a difference in priorities. In the context of NLG, Angeli et al. (2010) as well as Kondadadi et al. (2013) used paraphrasal templates extracted from aligned corpora of text and data representations in specific domains, which were grouped by the data types they relate to. Duma and Klein (2013) extract templates from Wikipedia pages aligned with RDF information from DBPedia, and although they do not explicitly mention aligning multiple templates to the same set of RDF templates, the possibility seems to exist in their framework. In contrast, we are interested in extracting paraphrasal templates from non-aligned text for general NLG, as aligned corpora are difficult to obtain for most domains. While template extraction has been a relatively small part of NLG research, it is very prominent in the field of Information Extraction (IE), beginning with Hearst (1992). There, however, the g"
P16-1180,D12-1058,0,0.0772634,"expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not d"
P16-1180,P12-1091,0,0.0232603,"s). More recent work on paraphrase detection has, for the most part, focused on classifying provided sentence pairs as paraphrases or not, using the Microsoft Paraphrase Corpus (Dolan et al., 2004). Mihalcea et al. (2006) evaluated a wide range of lexical and semantic measures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast"
P16-1180,C92-2082,0,0.155755,"hey relate to. Duma and Klein (2013) extract templates from Wikipedia pages aligned with RDF information from DBPedia, and although they do not explicitly mention aligning multiple templates to the same set of RDF templates, the possibility seems to exist in their framework. In contrast, we are interested in extracting paraphrasal templates from non-aligned text for general NLG, as aligned corpora are difficult to obtain for most domains. While template extraction has been a relatively small part of NLG research, it is very prominent in the field of Information Extraction (IE), beginning with Hearst (1992). There, however, the goal is to extract good data and not to extract templates that are good for generation. Many pattern extraction (as it is more commonly referred to in IE) approaches focus on semantic patterns that are not coherent lexically or syntactically, and the idea of paraphrasal templates is not important (Chambers and Jurafsky, 2011). One exception which expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has impl"
P16-1180,W03-1608,0,0.248755,"important (Chambers and Jurafsky, 2011). One exception which expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely o"
P16-1180,D13-1090,0,0.0141171,"on paraphrase detection has, for the most part, focused on classifying provided sentence pairs as paraphrases or not, using the Microsoft Paraphrase Corpus (Dolan et al., 2004). Mihalcea et al. (2006) evaluated a wide range of lexical and semantic measures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast literature on semantics, i"
P16-1180,P13-1138,0,0.211399,"Missing"
P16-1180,N12-1019,0,0.0123909,"yntactic) context. The main drawbacks of these methods are their inability to handle longer paraphrases and their tendency to find phrase pairs that are semantically related but not real paraphrases (e.g. antonyms or taxonomic siblings). More recent work on paraphrase detection has, for the most part, focused on classifying provided sentence pairs as paraphrases or not, using the Microsoft Paraphrase Corpus (Dolan et al., 2004). Mihalcea et al. (2006) evaluated a wide range of lexical and semantic measures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable"
P16-1180,D14-1079,0,0.0140814,"t outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast literature on semantics, including but not limited to lexical, distributional and knowledge-based semantics. Of particular interest to us is the work of Blacoe and Lapata (2012), which show that simple combination methods (e.g., vector multiplication) in classic vector space representations outperform more sophisticated alternativ"
P16-1180,I05-1011,0,0.0273006,"Missing"
P16-1180,N03-1024,0,0.0811037,"Missing"
P16-1180,W04-3219,0,0.0441556,"One exception which expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyp"
P16-1180,P09-1094,0,0.0277602,"t, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not directly comparable. In our approach, it is precisely the fact that we use a rich type system that allows us to extract paraphrasal templates from"
P16-1180,D12-1084,0,0.0156871,"paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not directly comparable. In ou"
P16-1180,I05-5011,0,0.0216028,"ecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not directly comparable. In our approach, it is precisely the fact that we use a rich type system that allows us"
P16-1180,P06-2094,0,0.0247449,"s. While template extraction has been a relatively small part of NLG research, it is very prominent in the field of Information Extraction (IE), beginning with Hearst (1992). There, however, the goal is to extract good data and not to extract templates that are good for generation. Many pattern extraction (as it is more commonly referred to in IE) approaches focus on semantic patterns that are not coherent lexically or syntactically, and the idea of paraphrasal templates is not important (Chambers and Jurafsky, 2011). One exception which expicitly contains a paraphrase detection component is (Sekine, 2006). Meanwhile, independently of templates, detecting paraphrases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shi"
P16-1180,P06-2096,0,0.0168261,"ases is an important, difficult and wellresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not directly comparable. In our approach, it is precisely the fact that we use a rich type system that allows us to extract paraphr"
P16-1180,D11-1014,0,0.0192281,"sures of similarity and introduced a combined metric that outperformed all previous measures. Madnani et al. (2012) showed that metrics from Machine Translation can be used 1914 to find paraphrases with high accuracy. Another line of research uses the similarity of texts in a latent space created through matrix factorization (Guo and Diab, 2012; Ji and Eisenstein, 2013). Other approaches that have been explored are explicit alignment models (Das and Smith, 2009), distributional memory tensors (Baroni and Lenci, 2010) and syntax-aware representations of multiword phrases using word embeddings (Socher et al., 2011). Word embeddings were also used by Milajevs et al. (2014). These approaches are not comparable to ours because they focus on classification, as opposed to mining, of paraphrases. Detecting paraphrases is closely related to research on the mathematical representation of sentences and other short texts, which draws on a vast literature on semantics, including but not limited to lexical, distributional and knowledge-based semantics. Of particular interest to us is the work of Blacoe and Lapata (2012), which show that simple combination methods (e.g., vector multiplication) in classic vector spac"
P16-1180,W11-1208,0,0.0138025,"llresearched problem of Natural Language Processing. It has implications for the general study of semantics as well as many specific applications such as Question Answering and Summarization. Research that focuses on mining paraphrases from large text corpora is especially relevant for our work. Typically, these approaches utilize a parallel (Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003; Quirk et al., 2004; Fujita et al., 2012; Regneri and Wang, 2012) or comparable corpus (Shinyama et al., 2002; Barzilay and Lee, 2003; Sekine, 2005; Shen et al., 2006; Zhao et al., 2009; Wang and Callison-Burch, 2011), and there have been approaches that leverage bilingual aligned corpora as well (Bannard and CallisonBurch, 2005; Madnani et al., 2008). Of the above, two are particularly relevant. Barzilay and Lee (2003) produce slotted lattices that are in some ways similar to templates, and their work can be seen as the most closely related to ours. However, as they rely on a comparable corpus and produce untyped slots, it is not directly comparable. In our approach, it is precisely the fact that we use a rich type system that allows us to extract paraphrasal templates from sentences that are not, by them"
P16-2040,D10-1049,0,0.166285,"years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of target"
P16-2040,W11-2810,0,0.0313148,"he generation pipeline paradigm (Reiter and Dale, 1997), with content selection determined by the relation in the company’s DBpedia entry while microplanning and realization are carried out through template generation. While some generation systems, particularly in early years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on s"
P16-2040,H05-1115,0,0.0322632,"Missing"
P16-2040,P06-2020,0,0.1083,"Missing"
P16-2040,P09-1024,0,0.0333998,"company and combines 243 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 243–248, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tence selection system (Blair-Goldensohn et al., 2003; Weischedel et al., 2004; Schiffman et al., 2001). In our approach, we target both relevance and variety of expression, driving content by selecting sentences that match company/entity pairs and inducing multiple patterns of expression. Sentence selection has also been used in prior work on generating Wikipedia overall articles (Sauper and Barzilay, 2009). Their focus is more on learning domain-specific templates that control the topic structure of an overview, a much longer text than we generate. ically used to express the relations on the web. In the process, it uses bootstrapping (Agichtein and Gravano, 2000) to learn new ways of expressing the relations corresponding to each company/entity pair, alternating with learning new pairs that match the learned expression patterns. Since the bootstrapping process is driven only by company/entity pairs and lexical patterns, it has the potential to learn a wider variety of expressions for each pair"
P16-2040,P01-1059,0,0.200363,"Missing"
P16-2040,I11-1032,1,0.806686,"bstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of targeted and data-driven methods for a pure sen244 pattern.1 The entities are therefore assumed to be related since they are expressed in the same way as the seed pair. Unlike the TD approach, the actual relationship between the entities is unknown (since the only data we use is the web text, not the structured RDF data); all we need to know here is that a relationship exists. We alternate learning the patterns and generating entity pairs over our development set of 100 companies. We then tak"
P16-2040,H05-1015,0,0.040684,"eration has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of targeted and data-driven methods for a pure sen244 pattern.1 The entities are therefore assumed to be related since they are expressed in the same way as the see"
P16-2040,E14-4041,0,0.0206945,"digm (Reiter and Dale, 1997), with content selection determined by the relation in the company’s DBpedia entry while microplanning and realization are carried out through template generation. While some generation systems, particularly in early years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant s"
P16-2040,W14-4424,0,0.0271504,", relevant facts were discarded due to phrases that mentioned lesser facts we did not have the data to replace. We therefore added a postprocessing step to remove, if possible, any phrases Related Work The TD approach falls into the generation pipeline paradigm (Reiter and Dale, 1997), with content selection determined by the relation in the company’s DBpedia entry while microplanning and realization are carried out through template generation. While some generation systems, particularly in early years, used sophisticated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occup"
P16-2040,P13-1138,0,0.14605,"cated grammars for realization (Matthiessen and Bateman, 1991; Elhadad, 1991; White, 2014), in recent years, template-based generation has shown a resurgence. In some cases, authors focus on document planning and sentences in the domain are stylized enough that templates suffice (Elhadad and Mckeown, 2001; Bouayad-Agha et al., 2011; Gkatzia et al., 2014; Biran and McKeown, 2015). In other cases, learned models that align database records with text snippets and then abstract out specific fields to form templates have proven successful for the generation of various domains (Angeli et al., 2010; Kondadadi et al., 2013). Others, like us, target atomic events (e.g., date of birth, occupation) for inclusion in biographies (Filatova and Prager, 2005) but the templates used in other work are manually encoded. Sentence selection has also been used for question answering and query-focused summarization. Some approaches focus on selection of relevant sentences using probabilistic approaches (Daum´e III and Marcu, 2005; Conroy et al., 2006), semisupervised learning (Wang et al., 2011) and graphbased methods (Erkan and Radev, 2004; Otterbacher et al., 2005). Yet others use a mixture of targeted and data-driven method"
P16-2040,W07-0734,0,0.0595219,"ompare the three versions of our output - generated by the TD, DD, and hybrid approach - against multi-document summaries generated (from the same search results used by our DD approach) by TextRank (Mihalcea and Tarau, 2004). For each one of the approaches as well as the baseline, we generated descriptions for all companies that were part of the S&P500 as of January 2016. We used our development set of 100 companies for tuning, and the evaluation results are based on the remaining 400. We conducted two types of experiments. The first is an automated evaluation, where we use the METEOR score (Lavie and Agarwal, 2007) between the description generated by one of our approaches or by the baseline and the first section of the Wikipedia article for the company. In Wikipedia articles, the first section typically serves as an introduction or overview of the most important information about the company. METEOR scores capture the content overlap between the generated description and the Wikipedia text. To avoid bias from different text sizes, we set the same size limit for all descriptions when comparing them. We experimented with three settings: 150 words, 500 words, and no size limit. In addition, we conducted a"
P16-2040,W04-3252,0,\N,Missing
P16-2040,D15-1230,1,\N,Missing
P79-1016,T78-1009,0,\N,Missing
P79-1016,T75-2001,0,\N,Missing
P79-1016,J78-3015,0,\N,Missing
P82-1028,P82-1029,0,\N,Missing
P84-1043,P82-1007,0,\N,Missing
P84-1043,P82-1009,0,\N,Missing
P84-1065,P83-1022,0,0.107224,"Missing"
P84-1065,J81-1002,0,0.0902811,"Missing"
P84-1065,P83-1012,0,0.0141511,"een mentioned. Kay&apos;s work provides the basis for McKeown&apos;s and Appelt&apos;s and emphasizes the development of a formalism and grammar for generation that allows for the use of functional information. Both McKeown and Appelt make direct use of Kay&apos;s formalism, with McKeown&apos;s emphasis being on the influence of focus information on syntax and Appelt&apos;s emphasis being on the development of a facility that allows interaction between the grammar and an underlying planning component. id del ( prop (Verb, Prot, Goall, Ben, Foe), prop (Verb, Prot, Goal2, Ben, Foe), prop (Verb, Prot, Goal, Ben, Foe)) Nigel (Mann, 1983) is a fourth system that makes use of functional information and is based on systemic grammar (Hudson, 1974). A systemic grammar contains choice points that query the environment to decide between alternatives (the environment may include functional, discourse, semantic, or contextual information). Mann&apos;s emphasis, so far, has been on the development of the system, on the development of a large linguistically justified grammar, and on the influence of underlying semantics on choices. The influence of functional information on syntactic choice as well as the generation of complex propositions a"
P84-1065,P83-1021,0,0.0151511,"cture and a resulting surface string is returned in the second argument. Though not always practical, grammar rules can be designed to work in both directions (as were the rules in Figure 1). A generation goal and the sentence it produces are shown in Figure I. 4.3 Combining the Formalisms We have implemented a surface generator based on both the DCG formalism and the functional grammar 4. Functional grammar has also been referred to as unification grammar (Appett, 1983). 5. The functional grammar unification operation is similar to set union. A description of the algorithm is given in Appelt (1983). It is not to be confused with the unification process used in resolution theorem proving, though a similarity has been noted by Pereira and Warren (1983). 4.2 Functional Grammars Another formalism that has been used in previous .generation systems (McKeown, 1982; Appelt, 1983) is the functional grammar formalism (Kay, 1979) 4. In a 322 formalism. The result is a generator with the best features of both grammars: simplification of input by using functional information and efficiency of execution through Prolog. Functional information, supplied as part of the generation goal&apos;s input argument,"
P84-1065,P83-1011,0,\N,Missing
P87-1014,H86-1019,0,\N,Missing
P87-1014,C86-1137,0,\N,Missing
P87-1014,P85-1029,1,\N,Missing
P87-1014,P84-1065,1,\N,Missing
P90-1032,A88-1019,0,0.0694396,"Missing"
P90-1032,P89-1010,0,0.0613509,"Missing"
P90-1032,E87-1001,0,0.0590223,"Missing"
P90-1032,P90-1020,0,0.0696789,"Missing"
P90-1032,J85-4002,0,0.0321413,"Missing"
P90-1032,C88-2166,0,0.0167547,"Missing"
P90-1032,P87-1026,0,\N,Missing
P93-1023,J92-4003,0,0.0296166,"third stage combines the opinions of the various similarity modules in a single dissimilarity measure for any pair of adjectives. Finally, the fourth stage clusters the adjectives into groups according to the dissimilarity measure, so that adjectives with a high degree of pairwise similarity fall in the same cluster (and, consequently, adjectives with a low degree of similarity fall in different clusters). addition to statistical measures, is a unique property of our work and significantly improves the accuracy of our results. One other published model for grouping semantically related words (Brown et al., 1992), is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge, but no evaluation of the results is reported. 3.1. Stage One: Extracting Word Pairs During the first stage, the system extracts adjective-noun and adjective-adjective pairs from the corpus. To determine the syntactic category of each word, and identify the NP boundaries and the syntactic relations among the words, we used the Fidditch parser (Hindle, 1989). For each NP, we then determine its minimal NP, that part of an NP consisting of the head noun and its adjectival pre-modifiers"
P93-1023,P89-1015,0,0.0097067,"nd significantly improves the accuracy of our results. One other published model for grouping semantically related words (Brown et al., 1992), is based on a statistical model of bigrams and trigrams and produces word groups using no linguistic knowledge, but no evaluation of the results is reported. 3.1. Stage One: Extracting Word Pairs During the first stage, the system extracts adjective-noun and adjective-adjective pairs from the corpus. To determine the syntactic category of each word, and identify the NP boundaries and the syntactic relations among the words, we used the Fidditch parser (Hindle, 1989). For each NP, we then determine its minimal NP, that part of an NP consisting of the head noun and its adjectival pre-modifiers 2. We match a set of regular expressions, consisting of syntactic categories and representing the different forms a minimal NP can take, against the NPs. From the minimal NP, we produce the different pairs of adjectives and nouns, assuming that all adjectives modify the head noun 3. This assumption is rarely invalid, because a minimal NP with multiple adjectives all modifying the head noun is far more common than a minimal NP with multiple adjectives where one of the"
P93-1023,H93-1026,0,0.0216233,"Missing"
P93-1023,H93-1054,0,0.00971017,"rk, N.Y. 10027 Internet: v h @ c s . c o l u m b i a . e d u kathy @ cs.columbia.edu ABSTRACT One type of lexical knowledge which is useful for many natural language (NL) tasks is the semantic relatedness between words of the same or different syntactic categories. Semantic relatedness subsumes hyponymy, synonymy, and antonymyincompatibility. Special forms of relatedness are represented in the lexical entries of the WordNet lexical database (Miller et al., 1990). Paradigmatic semantic relations in WordNet have been used for diverse NL problems, including disambiguation of syntactic structure (Resnik, 1993) and semiautomatic construction of a large-scale ontology for machine translation (Knight, 1993). In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to an"
P93-1023,H93-1036,0,0.00913616,"e type of lexical knowledge which is useful for many natural language (NL) tasks is the semantic relatedness between words of the same or different syntactic categories. Semantic relatedness subsumes hyponymy, synonymy, and antonymyincompatibility. Special forms of relatedness are represented in the lexical entries of the WordNet lexical database (Miller et al., 1990). Paradigmatic semantic relations in WordNet have been used for diverse NL problems, including disambiguation of syntactic structure (Resnik, 1993) and semiautomatic construction of a large-scale ontology for machine translation (Knight, 1993). In this paper we present a method to group adjectives according to their meaning, as a first step towards the automatic identification of adjectival scales. We discuss the properties of adjectival scales and of groups of semantically related adjectives and how they imply sources of linguistic knowledge in text corpora. We describe how our system exploits this linguistic knowledge to compute a measure of similarity between two adjectives, using statistical techniques and without having access to any semantic information about the adjectives. We also show how a clustering algorithm can use the"
P93-1023,H93-1049,0,0.0152342,"Missing"
P93-1023,J91-1001,0,\N,Missing
P93-1023,M92-1004,0,\N,Missing
P93-1023,P93-1024,0,\N,Missing
P93-1031,E87-1001,0,0.123192,"Missing"
P93-1031,P90-1032,1,0.839397,"Missing"
P93-1031,P90-1020,0,0.0224488,"Missing"
P95-1027,P93-1023,1,0.921802,"Missing"
P95-1027,J91-1001,0,0.150716,"riables are closely related and their number is so high that it impedes the task of modeling semantic markedness in terms of them, we combined several of them, keeping 14 variables for the statistical analysis. 4 Data Collection In order to measure the performance of the markedness tests discussed in the previous section, we collected a fairly large sample of pairs of antonymous gradable adjectives that can appear in howquestions. The Deese antonyms (Deese, 1964) is the prototypical collection of pairs of antonymous adjectives that have been used for similar analyses in the past (Deese, 1964; Justeson and Katz, 1991; Grefenstette, 1992). However, this collection contains only 75 adjectives in 40 pairs, some of which cannot be used in our study either because they are primarily adverbials (e.g., inside-outside) or not gradable (e.g., alive-dead). Unlike previous studies, the nature of the statistical analysis reported in this paper requires a higher number of pairs. Consequently, we augmented the Deese set with the set of pairs used in the largest manual previous study of markedness in adjective pairs (Lehrer, 1985). In addition, we included all gradable adjectives which appear 50 times or more in the Bro"
P95-1027,C82-1027,0,0.132897,"Missing"
P95-1027,H93-1054,0,0.143284,"Missing"
P95-1027,J93-1007,0,\N,Missing
P97-1023,J92-4003,0,0.0963657,"Missing"
P97-1023,A88-1019,0,0.0642097,"In the following sections, we first present the set of adjectives used for training and evaluation. We next validate our hypothesis that conjunctions constrain the orientation of conjoined adjectives and then describe the remaining three steps of the algorithm. After presenting our results and evaluation, we discuss simulation experiments that show how our method performs under different conditions of sparseness of data. 3 Data Collection For our experiments, we use the 21 million word 1987 Wall Street Journal corpus 4, automatically annotated with part-of-speech tags using the PARTS tagger (Church, 1988). In order to verify our hypothesis about the orientations of conjoined adjectives, and also to train and evaluate our subsequent algorithms, we need a 3Certain words inflected with negative affixes (such as in- or un-) tend to be mostly negative, but this rule applies only to a fraction of the negative words. Furthermore, there are words so inflected which have positive orientation, e.g., independent and unbiased. 4Available form the ACL Data Collection Initiative as CD ROM 1. 175 Positive: adequate central clever famous intelligent remarkable reputed sensitive slender thriving Negative: cont"
P97-1023,C90-3018,1,0.105468,"Missing"
P97-1023,P93-1023,1,0.0836704,"Missing"
P97-1023,P95-1027,1,0.126296,"Missing"
P97-1023,J91-1001,0,0.120529,"Missing"
P97-1023,P93-1024,0,0.0408899,"Missing"
P98-1099,C94-1042,0,0.0993676,"ds, and because it provides several semantic relationships (hyponymy, antonymy, meronymy, entailment) which are beneficial to lexical choice. 2. English Verb Classes and Alternations (EVCA) (Levin, 1993). EVCA is an extensive linguistic study of diathesis alternations, which are variations in the realization of verb arguments. For example, the alternation ""there-insertion"" transforms A ship appeared on the horizon to There appeared a ship on the horizon. Knowledge of alternations facilitates the generation of paraphrases. (Levin, 1993) studies 80 alternations. 3. The COMLEX syntax dictionary (Grishman et al., 1994). COMLEX contains syntactic information for 38,000 English words. The information includes subcategorization and complement restrictions. 4. The Brown Corpus tagged with WordNet senses (Miller et al., 1993). The original 1As of Version 1.6, released in December 1997. Brown corpus (Ku~era and Francis, 1967) has been used as a reference corpus in many computational applications. Part of Brown Corpus has been tagged with WordNet senses manually by the WordNet group. We use this corpus for frequency measurements and exacting selectional constraints. 2.2 C o m b i n i n g l i n g u i s t i c r e s"
P98-1099,W97-0210,1,0.76519,"Missing"
P98-1099,W98-0718,1,0.841513,"bcategorizations and alternations from the lexicon are then applied at the syntactic level. After three levels of paraphrasing, each message in PlanDOC on average has over 10 paraphrases. For a specific domain such as PlanDOC, an enormous proportion of a general lexicon like the one we constructed is unrelated thus unused at all. On the other hand, domain-specific knowledge may need to be added to the lexicon. T h e problem of how to adapt a general lexicon to a particular application domain and merge domain ontologies with a general lexicon is out of the scope of this paper but discussed in (Jing, 1998). The third stage produces a structure represented as a high level sentence structure, with subcategorizations and words associated with each sentence. At this stage, information in the lexical resource about subcategorization and alternations are applied in order to generate syntactic paraphrases. O u t p u t of this stage is then fed directly to the surface realization pack612 4 Conclusion In this paper, we present research on building a rich, large-scale, and reusable lexicon for generation by combining multiple heterogeneous linguistic resources. Novel semi-automatic transformation and int"
P98-1099,H93-1061,0,0.0731871,"A is an extensive linguistic study of diathesis alternations, which are variations in the realization of verb arguments. For example, the alternation ""there-insertion"" transforms A ship appeared on the horizon to There appeared a ship on the horizon. Knowledge of alternations facilitates the generation of paraphrases. (Levin, 1993) studies 80 alternations. 3. The COMLEX syntax dictionary (Grishman et al., 1994). COMLEX contains syntactic information for 38,000 English words. The information includes subcategorization and complement restrictions. 4. The Brown Corpus tagged with WordNet senses (Miller et al., 1993). The original 1As of Version 1.6, released in December 1997. Brown corpus (Ku~era and Francis, 1967) has been used as a reference corpus in many computational applications. Part of Brown Corpus has been tagged with WordNet senses manually by the WordNet group. We use this corpus for frequency measurements and exacting selectional constraints. 2.2 C o m b i n i n g l i n g u i s t i c r e s o u r c e s In this section, we present an algorithm for merging data from the four resources in a manner that achieves high accuracy and completeness. We focus on verbs, which play the most important role"
P98-2165,J90-3003,0,0.38041,"Missing"
P98-2165,P88-1023,0,0.0891637,"Missing"
P98-2165,A97-1041,1,0.894423,"Missing"
P98-2165,W97-1204,1,0.845239,"Missing"
P98-2165,J94-3001,0,\N,Missing
P98-2165,J94-1002,0,\N,Missing
P98-2165,J93-4004,0,\N,Missing
P98-2165,E89-1023,0,\N,Missing
P98-2165,H90-1006,0,\N,Missing
P98-2165,W94-0302,0,\N,Missing
P98-2165,P84-1018,0,\N,Missing
P98-2165,H92-1089,0,\N,Missing
P98-2165,P96-1025,0,\N,Missing
P98-2165,W98-1415,0,\N,Missing
P98-2165,J86-3001,0,\N,Missing
P98-2165,A97-1039,0,\N,Missing
P99-1071,W97-0703,1,0.18292,"document summarization systems, then we will detail our sentence comparison technique, and describe the sentence generation component. We provide examples of generated summaries and conclude with a discussion of evaluation. 2 Related Work Automatic summarizers typically identify and extract the most important sentences from an input article. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et al., 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadad, 1997). Extraction techniques can work only if summary sentences already appear in the article. Extraction cannot handle the task we address, because summarization of multiple documents requires information about similarities and differences across articles. While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents. In constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, informati"
P99-1071,P96-1025,0,0.0195348,"opositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: Content p l a n n i n g o p e r a t e s o v e r full sentences, producing s e n t e n c e fragm e n t s . Thus, content planning straddles the border between interpretation and generation. We preprocess the similar sentences using an existing shallow parser (Collins, 1996) and a mapping to predicateargument structure. The content planner finds an intersection of phrases by comparing the predicate-argument structures; through this process it selects the phrases that can adequately convey the common information of the theme. It also orders selected phrases and augments t h e m with 550 On 3th of September 1995, 120 hostages were released by Bosnian Serbs. Serbs were holding over 250 U.N. personnel. Bosnian serb leader Radovan Karadjic said he expected ""a sign of goodwill"" from the international community. U.S. F-16 fighter jet was shot down by Bosnian ! Serbs. El"
P99-1071,P97-1004,0,0.00607779,"Missing"
P99-1071,A97-1042,0,0.0830727,"fusion of similar information across multiple documents using language generation to produce a concise summary. We propose a m e t h o d for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for s u m m a r y generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while exMichael Elhadad Dept. of C o m p u t e r Science B e n - G u r i o n University Beer-Sheva, Israel tracting s o m e similar sentences could produce a s u m m a r y biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that shou"
P99-1071,W97-0713,0,0.158286,"nformation across multiple documents using language generation to produce a concise summary. We propose a m e t h o d for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for s u m m a r y generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while exMichael Elhadad Dept. of C o m p u t e r Science B e n - G u r i o n University Beer-Sheva, Israel tracting s o m e similar sentences could produce a s u m m a r y biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be include"
P99-1071,J98-3005,1,0.606113,"tic way. However, some of the rules can only be approximated to a certain degree. For example, identification of similarity based on semantic relations between words depends on the coverage of the thesaurus. We 553 identify word similarity using synonym relations from WordNet. Currently, paraphrasing using part of speech transformations is not supported by the system. All other paraphrase classes we identified are implemented in our algorithm for theme intersection. 3.3 T e m p o r a l O r d e r i n g A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown, 1998). When reading an original text, it is possible to retrieve the correct temporal sequence of events which is usually available explicitly. However, when we p u t pieces of text from different sources together, we must provide the correct time perspective to the reader, including the order of events, the temporal distance between events and correct temporal references. In single-document summarization, one of the possible orderings of the extracted information is provided by the input document itself. However, in the case of multiple-document summarization, some events may not be described in t"
rosenthal-etal-2010-towards,W04-2705,0,\N,Missing
rosenthal-etal-2010-towards,W97-1016,0,\N,Missing
rosenthal-etal-2010-towards,A00-2018,0,\N,Missing
rosenthal-etal-2010-towards,W97-0109,0,\N,Missing
rosenthal-etal-2010-towards,J93-2004,0,\N,Missing
rosenthal-etal-2010-towards,D08-1027,0,\N,Missing
rosenthal-etal-2010-towards,H94-1048,0,\N,Missing
rosenthal-etal-2010-towards,J07-4002,0,\N,Missing
rosenthal-etal-2010-towards,J03-4003,0,\N,Missing
rosenthal-etal-2010-towards,N09-4006,0,\N,Missing
rosenthal-etal-2010-towards,W00-0726,0,\N,Missing
rosenthal-etal-2010-towards,P08-1037,0,\N,Missing
rosenthal-etal-2010-towards,J95-4004,0,\N,Missing
rosenthal-etal-2010-towards,P98-2234,0,\N,Missing
rosenthal-etal-2010-towards,C98-2229,0,\N,Missing
rosenthal-etal-2010-towards,J05-1004,0,\N,Missing
W00-1414,J87-1005,0,0.0496675,"o generate concise references to entities at the discourse level. For example, a sentence such as ""'The patient has an infusion line in each arm."" is a more concise version of ""The patient has an infusion line ir~ his left arm. The patient has an infusion line in his right a r m . "" Quantification is an active research topic in logic, language, and philosophy(Carpenter, 1997; de Swart. 1998). Since natural language understanding systems need to obtain as few interpretations as possible from text, researchers have studied quantifier scope ambiguity extensively (Woods~ 1978;-Grosz et al., 1987; Hobbs and Shieber, 1987: Pereira, 1990; Moran and Pereira, 1992: Park, 1995). Research in quantification interpretation first transforms a sentence into predicate logic, raises tim quantifiers to the sentential level, and permutes these quantifiers {o obtain as many readings as possible relaled to quantifier scoping. Then, invalid readings are eliminated using various consl raints. Ambiguity in quantified expressions is caused by two main culprits. T h e first type of ambiguity involves the distributive reading versus the collective reading. In universal quantification, a referring ex100 , :, ~.*~ . pression refers"
W00-1414,J81-4006,0,0.363955,"Missing"
W00-1414,A97-1041,1,0.896266,"Missing"
W00-1414,P95-1028,0,0.0237774,"For example, a sentence such as ""'The patient has an infusion line in each arm."" is a more concise version of ""The patient has an infusion line ir~ his left arm. The patient has an infusion line in his right a r m . "" Quantification is an active research topic in logic, language, and philosophy(Carpenter, 1997; de Swart. 1998). Since natural language understanding systems need to obtain as few interpretations as possible from text, researchers have studied quantifier scope ambiguity extensively (Woods~ 1978;-Grosz et al., 1987; Hobbs and Shieber, 1987: Pereira, 1990; Moran and Pereira, 1992: Park, 1995). Research in quantification interpretation first transforms a sentence into predicate logic, raises tim quantifiers to the sentential level, and permutes these quantifiers {o obtain as many readings as possible relaled to quantifier scoping. Then, invalid readings are eliminated using various consl raints. Ambiguity in quantified expressions is caused by two main culprits. T h e first type of ambiguity involves the distributive reading versus the collective reading. In universal quantification, a referring ex100 , :, ~.*~ . pression refers to multiple entities. There is a potential ambiguity"
W00-1414,J90-1001,0,0.0375952,"nces to entities at the discourse level. For example, a sentence such as ""'The patient has an infusion line in each arm."" is a more concise version of ""The patient has an infusion line ir~ his left arm. The patient has an infusion line in his right a r m . "" Quantification is an active research topic in logic, language, and philosophy(Carpenter, 1997; de Swart. 1998). Since natural language understanding systems need to obtain as few interpretations as possible from text, researchers have studied quantifier scope ambiguity extensively (Woods~ 1978;-Grosz et al., 1987; Hobbs and Shieber, 1987: Pereira, 1990; Moran and Pereira, 1992: Park, 1995). Research in quantification interpretation first transforms a sentence into predicate logic, raises tim quantifiers to the sentential level, and permutes these quantifiers {o obtain as many readings as possible relaled to quantifier scoping. Then, invalid readings are eliminated using various consl raints. Ambiguity in quantified expressions is caused by two main culprits. T h e first type of ambiguity involves the distributive reading versus the collective reading. In universal quantification, a referring ex100 , :, ~.*~ . pression refers to multiple ent"
W00-1414,A92-1006,0,0.0765959,"Missing"
W00-1414,C92-1038,0,0.0311166,"th previous work in the generation of quantified expressions. In Section 3, we will describe the application where the need for concise output motivated our research in quantification. The algorithm for generating universal quantifiers is detailed in Section 4, including how the system handles ambiguity between distributive and collective readings. Section 5 describes how our algorithm generates sentences with multiple quantifiers. 2 Related Work Because a quantified expression refers to multiple entities in a domain, our work can be categorized as referring expression generation (Dale, 1992; Reiter and Dale, 1992; Horacek, 1997). Previous work in this area did not address the generation of quantified expressions directly. In this paper, we are interested in how to systematically derive quantifiers from input propositions, discourse history, and ontological information. Recent work on the generation ofquantifiers (Gailly, 1988; Creaney, 1996; Creaney, 1999) follows the analysis viewpoint, discussing scope anabiguities extensively. Though our algorithm generates different sentences for different scope orderings, we do not achieve this through scoping operations as they did. Creaney also discussed variou"
W00-1414,P98-2199,1,0.85288,"ntified expressions can replace entities in a text, making the text more fluent and concise. In addition to avoiding ambiguities between distributive and collective readings in universal quantification generation, we will also show how different scope orderings between universal and existential quantitiers will result in different quantified expressions in our algorithm. 1 Introduction To convey information concisely and fluently, text generation systems often perform opportunistic text planning (Robin, 1995; Mellish et al., 1998) and employ advanced linguistic constructions such as ellipsis (Shaw, 1998). But a system can also take advantage of quantification and ontological information to generate concise references to entities at the discourse level. For example, a sentence such as ""'The patient has an infusion line in each arm."" is a more concise version of ""The patient has an infusion line ir~ his left arm. The patient has an infusion line in his right a r m . "" Quantification is an active research topic in logic, language, and philosophy(Carpenter, 1997; de Swart. 1998). Since natural language understanding systems need to obtain as few interpretations as possible from text, researchers"
W00-1414,W98-1404,0,\N,Missing
W00-1414,C88-1037,0,\N,Missing
W00-1414,P97-1027,0,\N,Missing
W00-1414,W96-0413,0,\N,Missing
W01-0813,C00-1007,0,0.0268105,"C ENTRIFUSER is fully implemented; it produces the sample summary in Figure 1. We have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived Content Types and Special Content and the Title metadata). Future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features. We are also exploring the use of stochastic corpus modeling (Langkilde, 2000; Bangalore and Rambow, 2000) to replace our template-based realizer with a probabilistic one that can produce felicitous sentence patterns based on contextual analysis. 8 Conclusion We have presented a model for indicative multidocument summarization based on natural language generation. In our model, summary content is based on document features describing topic and structure instead of extracted text. Given these features, a generation model uses a text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describing document topics as typical,"
W01-0813,W98-1123,1,0.900195,"Missing"
W01-0813,A00-2023,0,0.0400379,"s and future work C ENTRIFUSER is fully implemented; it produces the sample summary in Figure 1. We have concentrated on implementing the most commonly occuring document feature, topicality, and have additionally incorporated three other document features into our framework (document-derived Content Types and Special Content and the Title metadata). Future work will include extending our document feature analysis to model context (to model adding features only when appropriate), as well as incorporating additional document features. We are also exploring the use of stochastic corpus modeling (Langkilde, 2000; Bangalore and Rambow, 2000) to replace our template-based realizer with a probabilistic one that can produce felicitous sentence patterns based on contextual analysis. 8 Conclusion We have presented a model for indicative multidocument summarization based on natural language generation. In our model, summary content is based on document features describing topic and structure instead of extracted text. Given these features, a generation model uses a text plan, derived from analysis of naturally occurring indicative summaries plus guidelines for summarization, to guide the system in describin"
W01-0813,P97-1013,0,0.0223606,"A topic tree for an article about coronary artery disease from The Merck manual of medical information, constructed automatically from its section headers. care article. Each document in the collection is represented by such a tree, which breaks each document’s topic into subtopics. We build these document topic trees automatically for structured documents using a simple approach that utilizes section headers, which suffices for our current domain and genre. Other methods such as layout identification (Hu et al., 1999) and text segmentation / rhetorical parsing (Yaari, 1999; Kan et al., 1998; Marcu, 1997) can serve as the basis for constructing such trees in both structured and unstructured documents, respectively. 4.1 Normative topicality as composite topic trees As stated in rule 1, the summarizer needs normative values calculated for each document feature to properly compute differences between documents. The composite topic tree embodies this paradigm. It is a data structure that compiles knowledge about all possible topics and their structure in articles of the same intersection of domain and genre, (i.e., rule 1’s notion of “document type”). Figure 4 shows a partial view of such a tree c"
W01-0813,W94-0319,0,0.0106391,"multidocument summary generation. We address the problem of “what to say” in Section 2, by examining what document features are important for indicative summaries, starting from a single document context and generalizing to a multidocument, querybased context. This yields two rules-of-thumb for guiding content calculation: 1) reporting differences from the norm and 2) reporting information relevent to the query. We have implemented these rules as part of the content planning module of our C ENTRIFUSER summarization system. The summarizer’s architecture follows the consensus NLG architecture (Reiter, 1994), including the stages of content calculation and content planning. We follow the generation of a sample indicative multidocument query-based summary, shown in the bottom half of Figure 1, focusing on these two stages in the remainder of the paper. 2 Document features as potential summary content Information about topics and structure of the document may be based on higher-level document features. Such information typically does not occur as strings in the document text. Our approach, therefore, is to identify and extract the document features that are relevant for indicative summaries. These"
W02-1023,J96-1001,1,\N,Missing
W02-1023,W01-0508,1,\N,Missing
W02-1023,A88-1019,1,\N,Missing
W02-2101,C94-2174,0,0.0953842,"Missing"
W02-2101,P97-1005,0,0.0327727,"Missing"
W02-2101,A00-2023,0,0.0474466,"es and their ordering, the task of surface realization is to convey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audie"
W02-2101,C00-1007,0,0.066472,"Missing"
W02-2101,P01-1008,1,0.830652,"Missing"
W02-2101,H01-1065,1,0.876325,"Missing"
W02-2101,P96-1025,0,0.0331878,"Missing"
W02-2101,P01-1023,1,0.801212,"hus have different predicate attribute values. In addition, some predicates are present in some summaries and not in others (e.g., author or editor). Tables 1 and 2 list the predicates and their frequency in the training corpus. The presence of the predicate may also be dependent on its value (e.g., Edition only occurs after the first edition). Content ordering. The presence or absence of particular predicates depends greatly on the presence or absence of its peers. Thus it is important to encode content structuring information, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou, 1999), in which pairs of premodifiers were observed and used to find order"
W02-2101,W01-0813,1,0.886958,"predicate attribute values. In addition, some predicates are present in some summaries and not in others (e.g., author or editor). Tables 1 and 2 list the predicates and their frequency in the training corpus. The presence of the predicate may also be dependent on its value (e.g., Edition only occurs after the first edition). Content ordering. The presence or absence of particular predicates depends greatly on the presence or absence of its peers. Thus it is important to encode content structuring information, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou, 1999), in which pairs of premodifiers were observed and used to find order"
W02-2101,W00-0306,0,0.121094,"Missing"
W02-2101,P98-2176,0,0.0582913,"Missing"
W02-2101,A00-2026,0,0.18552,"Missing"
W02-2101,P99-1018,0,0.0969699,"formation, represented as local preferences rather than predefined schemas. Duboue and McKeown (2001) detail an approach for this problem which we initially tried that uses techniques from computational biology, but which is best suited for summaries with multiple instances of the same predicate. Instead, we calculated bigram statistics on pairs of adjacent predicates, recording which occurred before another. These statistics are used to find an ordering of the predicates that maximizes agreement with training observations. This approach was also utilized in work done on premodifier ordering (Shaw and Hatzivassiloglou, 1999), in which pairs of premodifiers were observed and used to find ordering constraints. The technique is also referred to as Majority Ordering in (Barzilay et al., 2001), in which bigram orderings were elicited from human subjects. (Background j Language) Overview Topic Size Media Types Authority Collection Size (Comparison j Detail j Content Types j Navigation j Query Relevance) Subjective Difficulty Author Purpose Style (Publisher j Award j Readability j Audience j Contributor j Copyright) Figure 2: Highest agreement full orderings of the predicates using harmonic penalties. Predicates are swa"
W02-2101,N01-1001,0,0.17174,"vey the predicates as natural language. In traditional NLG, surface realization is often broken down into three separate tasks: (1) sentence planning, which takes individual messages or propositions and assigns them to specific sentences and determines the sentences’ basic syntactic structure; (2) lexical choice, which determines the words used, and (3) syntactic realization, which uses a grammar to produce the sentence. We are concerned with tasks 1 and 2. While there has been work concentrating on inducing syntactic generators (Langkilde, 2000; Bangalore and Rambow, 2000; Ratnaparkhi, 2000; Varges and Mellish, 2001), for specific domains and general language, there has been less work on other generation components (Oh and Rudnicky, 2000). Certain predicates, such as those that are contentor topic-based (e.g., Overview and Detail features), which are highly specific to the resource being summarized are best handled by existing techniques of sentence extraction or domain- and genre-specific text grammars (Liddy, 1991; Rama and Srinivasan, 1993). However, many other predicates are more domain-independent (e.g., Content Types and Audience). We focus on these metadata predicates, as they comprise a large port"
W02-2101,C98-2171,0,\N,Missing
W02-2112,W02-1022,0,0.0510906,"Missing"
W02-2112,P01-1023,1,0.677368,"s are provided: mutations (that produces a new chromosome by modifying an old one) and cross-over (that produces a new chromosome by combining two existing ones, its ‘parents’). Each chromosome has an associated fitness value, that specifies how well or promising the chromosome looks. A main contribution of our work is the use of two corpus-based fitness functions, F C and FA . We use an approximate evaluation function, F C that allows us to efficiently determine whether order constraints over plan operators are met in the current chromosome. We use the constraints we acquired on this domain (Duboue and McKeown, 2001),2 Figure 4). These constraints relate sets of 2 In particular, we set f itness = −1 ∗ N , where N is the number of violated constraints over on the training set. patterns by specifying strict restrictions on their relative placements. Note that a chromosome that violates any of these constraints ought to be considered invalid. However, instead of discarding it completely, we follow Richardson et al. (1989) and provide a penalty function, in order to allow the useful information contained in it to be preserved in future generations. Once a tree has been evolved so that it conforms to all order"
W02-2112,W98-1411,0,0.384918,"Missing"
W02-2112,W99-0619,1,0.794205,"rds, as measured in a 1M-token corpus of related discourse, to estimate the goodness of substituting one word by another. An important point to note here is that both F C and FA are data-dependent, as they analyze the goodness or badness of output plans, i.e., sequences of instantiated atomic operators. They require running the planner multiple times in order to do the 3 We employ global alignments because we are comparing two discourses derived from identical semantic input. 4 or they will align only the As or the Cs, depending on the score of aligning correctly any of them. 5 as computed by Pan and McKeown (1999). plan A A B A A ? A C D C D C F order constraint Figure 4: Fitness function: Constraints. evaluation. We do this because, for one instance, as the planning process may delete (because there is no data available) or duplicate nodes (because of multivalued data). An advantage of FC is that it can be tested on a much wider range of semantic inputs than it is trained on6 . 2.1 Operations over chromosomes We define three mutation operators and one crossover operation. The mutations include node insertion, which picks an internal node at random and moves a subset of its children to a newly created"
W02-2112,2001.mtsummit-papers.68,0,0.0177595,"se planners,such as SYNERGY (Muslea, 1997); or to induce grammars (Smith and Witten, 1996). In general, all these approaches are deeply tied to Genetic Programming (Koza, 1994), that deals with the issue of how to let a computer program itself. Our two-level fitness-function employs a lowerorder function for the initial approximation of solutions in a process similar to the one taken by Haupt (1995) in a very different domain. This technique is appropriate for dealing with expensive fitness functions. Finally, our approach with respect to humanproduced discourse as gold standard is similar to Papinini et al. (2001) as it avoids adhering to the particularities of one specific person or discourse. 5 Conclusions and Further Work The task of learning a general content planner such as Moore and Paris (1992) is well beyond the state of the art. We have identified reduced content planning tasks that are feasible for learning. In learning for traditional AI planning, e.g., learning of planning operators (Garc´ıa-Mart´ınez and Borrajo, 1997), the focus is on reduced planning environments. The kind of discourse targeted by our current techniques has been identified in the past as rich in domain communication know"
W02-2112,W02-2101,1,0.87595,"Missing"
W02-2112,A97-1039,0,0.0267873,"d as tightly coupled with the semantics and idiosyncrasies of each particular domain. Kathleen R. McKeown Department of Computer Science Columbia University kathy@cs.columbia.edu The AI planning community is aware that machine learning techniques can bring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997)"
W02-2112,A97-1041,1,0.77145,"oie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997). The overall architecture for our learning of content planners is shown in Figure 1. As input we utilize an aligned corpus of semantic inputs aligned with human-produced discourse. We also take advantage of the definition of the atomic operators (messages) from our existing system. We learn these tree-like planners by means of a genetic search process. The plan produced as output by such planners is a sequence of semantic structures, defined by the atomic operators. The learning technique is complementary to approaches proposed for generation in summarization (Kan and McKeown, 2002), that uti"
W02-2112,W94-0319,0,0.0379172,"Missing"
W02-2112,W98-1415,0,0.0203657,"syncrasies of each particular domain. Kathleen R. McKeown Department of Computer Science Columbia University kathy@cs.columbia.edu The AI planning community is aware that machine learning techniques can bring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997). The overall architecture for our"
W02-2112,W94-0302,0,0.020568,"ring a general solution to problems that require customization for every particular instantiation (Minton, 1993). The automatic (or semi-automatic) construction of a complete content planner for unrestricted domains is a highly desirable goal. While there are general tools and techniques to deal with surface realization (Elhadad and Robin, 1996; Lavoie and Rambow, 1997) and sentence planning (Shaw, 1998), the inherent dependency on each domain makes the content planning problem difficult to deal with in a unified framework; it requires sophisticated planning methodologies, for example, DPOCL (Young and Moore, 1994). The main problem is that the space of possible planners is so large. For example, in the experiments reported here, it contains all the possible orderings of 82 units of information. In this paper, we present a technique for learning the structure of tree-like planners, similar to the one manually built for our MAGIC system (McKeown et al., 1997). The overall architecture for our learning of content planners is shown in Figure 1. As input we utilize an aligned corpus of semantic inputs aligned with human-produced discourse. We also take advantage of the definition of the atomic operators (me"
W02-2112,W96-0501,0,\N,Missing
W02-2112,J93-4004,0,\N,Missing
W02-2112,P02-1040,0,\N,Missing
W03-1016,J97-1004,0,\N,Missing
W03-1016,J93-4004,0,\N,Missing
W03-1016,W00-1429,0,\N,Missing
W03-1016,W94-0318,0,\N,Missing
W03-1016,W02-1022,0,\N,Missing
W03-1016,P01-1059,0,\N,Missing
W03-1016,W01-0802,0,\N,Missing
W03-1016,W02-1011,0,\N,Missing
W03-1016,W02-2111,0,\N,Missing
W06-1401,N03-1020,0,0.0587462,"rge amounts of data, including both input document sets and multiple models of good output for each input set, which has spurred studies both on evaluation and summarization. Halteren and Teufel, for example, provide a method for annotation of content units and study consensus across summarizers (van Halteren and Teufel, 2003; Teufel and van Halteren, 2004b). Nenkova studies significant differences across DUC04 systems (Nenkova, 2005) as well as the properties of human and system summaries (Nenkova, 2006). We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE (Lin and Hovy, 2003; Lin, 2004) which allow quick measurement of systems during development and enable evaluation of larger amounts of data. We have seen the development of manual methods for evaluation developed both within DUC (Harman and Over, 2004) and without. The Pyramid method (Nenkova and Passonneau, 2004) provides a annotation method and metric that addresses the issues of reliability and stability of scoring. Thus, research on evaluation of summarization has become a field in its own right result• Given the different ways in which evaluation can be carried out and the fact that different researchers ma"
W06-1401,N04-1019,0,0.0221097,"summarizers (van Halteren and Teufel, 2003; Teufel and van Halteren, 2004b). Nenkova studies significant differences across DUC04 systems (Nenkova, 2005) as well as the properties of human and system summaries (Nenkova, 2006). We can credit DUC with the emergence of automatic methods for evaluation such as ROUGE (Lin and Hovy, 2003; Lin, 2004) which allow quick measurement of systems during development and enable evaluation of larger amounts of data. We have seen the development of manual methods for evaluation developed both within DUC (Harman and Over, 2004) and without. The Pyramid method (Nenkova and Passonneau, 2004) provides a annotation method and metric that addresses the issues of reliability and stability of scoring. Thus, research on evaluation of summarization has become a field in its own right result• Given the different ways in which evaluation can be carried out and the fact that different researchers may be biased towards methods which favor their own approach, it is important the evaluation be overseen by a neutral party which is not deeply involved in research on the task itself. On the other hand, some knowledge is necessary if the evaluation is to be well-designed. While my talk will focus"
W06-1401,W04-3254,0,0.0262129,"Missing"
W06-1401,W03-0508,0,0.0576239,"Missing"
W06-1401,W04-1013,0,\N,Missing
W06-1401,W04-1003,0,\N,Missing
W10-0702,D09-1021,0,0.0133432,"ety of tasks such as information extraction (Hong and Davison, 2009), social networking (Gruhl et al., 2004), and sentiment analysis (Leshed and Kaye, 2006), we are not aware of any previous efforts to gather syntactic data (such as PP attach14 ments) in the genre. Syntactic methods such as POS tagging, parsing and structural disambiguation are commonly used when analyzing well-structured text. Including the use of syntactic information has yielded improvements in accuracy in speech recognition (Chelba and Jelenik, 1998; Collins et al., 2005) and machine translation (DeNeefe and Knight, 2009; Carreras and Collins, 2009). We anticipate that datasets such as ours could be useful for such tasks as well. Amazon’s Mechanical Turk (MTurk) has become very popular for manual annotation tasks and has been shown to perform equally well over labeling tasks such as affect recognition, word similarity, recognizing textual entailment, event temporal ordering and word sense disambiguation, when compared to annotations from experts (Snow et al., 2008). While these tasks were small in scale and intended to demonstrate the viability of annotation via MTurk, it has also proved effective in large-scale tasks including the colle"
W10-0702,P05-1063,0,0.0131239,"text, such as discussion forums and emails, have been studied for a variety of tasks such as information extraction (Hong and Davison, 2009), social networking (Gruhl et al., 2004), and sentiment analysis (Leshed and Kaye, 2006), we are not aware of any previous efforts to gather syntactic data (such as PP attach14 ments) in the genre. Syntactic methods such as POS tagging, parsing and structural disambiguation are commonly used when analyzing well-structured text. Including the use of syntactic information has yielded improvements in accuracy in speech recognition (Chelba and Jelenik, 1998; Collins et al., 2005) and machine translation (DeNeefe and Knight, 2009; Carreras and Collins, 2009). We anticipate that datasets such as ours could be useful for such tasks as well. Amazon’s Mechanical Turk (MTurk) has become very popular for manual annotation tasks and has been shown to perform equally well over labeling tasks such as affect recognition, word similarity, recognizing textual entailment, event temporal ordering and word sense disambiguation, when compared to annotations from experts (Snow et al., 2008). While these tasks were small in scale and intended to demonstrate the viability of annotation v"
W10-0702,D09-1076,0,0.0172095,"ve been studied for a variety of tasks such as information extraction (Hong and Davison, 2009), social networking (Gruhl et al., 2004), and sentiment analysis (Leshed and Kaye, 2006), we are not aware of any previous efforts to gather syntactic data (such as PP attach14 ments) in the genre. Syntactic methods such as POS tagging, parsing and structural disambiguation are commonly used when analyzing well-structured text. Including the use of syntactic information has yielded improvements in accuracy in speech recognition (Chelba and Jelenik, 1998; Collins et al., 2005) and machine translation (DeNeefe and Knight, 2009; Carreras and Collins, 2009). We anticipate that datasets such as ours could be useful for such tasks as well. Amazon’s Mechanical Turk (MTurk) has become very popular for manual annotation tasks and has been shown to perform equally well over labeling tasks such as affect recognition, word similarity, recognizing textual entailment, event temporal ordering and word sense disambiguation, when compared to annotations from experts (Snow et al., 2008). While these tasks were small in scale and intended to demonstrate the viability of annotation via MTurk, it has also proved effective in large-sc"
W10-0702,H94-1048,0,0.0410564,"ntially applicable to any task which can be reliably decomposed into independent judgments that untrained annotators can tackle (e.g., quantifier scoping, conjunction scope). This work is intended as an initial step towards the development of efficient hybrid annotation tools that seamlessly incorporate aggregate human wisdom alongside effective algorithms. 2 Related Work Identifying PP attachments is an essential task for building syntactic parse trees. While this task has been studied using fully-automated systems, many of them rely on parse tree output for predicting potential attachments (Ratnaparkhi et al., 1994; Yeh and Vilain, 1998; Stetina and Nagao, 1997; Zavrel et al., 1997). However, systems that rely on good parses are unlikely to perform well on new genres such as blogs and machine translated texts for which parse tree training data is not readily available. Furthermore, the predominant dataset for evaluating PP attachment is the RRR dataset (Ratnaparkhi et al., 1994) which consists of PP attachment cases from the Wall Street Journal portion of the Penn Treebank. Instead of complete sentences, this dataset consists of sets of the form {V,N1 ,P,N2 } where {P,N2 } is the PP and {V,N1 } are the"
W10-0702,rosenthal-etal-2010-towards,1,0.814923,"disambiguation, when compared to annotations from experts (Snow et al., 2008). While these tasks were small in scale and intended to demonstrate the viability of annotation via MTurk, it has also proved effective in large-scale tasks including the collection of accurate speech transcriptions (Gruenstein et al., 2009). In this paper we explore a method for corpus building on a large scale in order to extend annotation into new domains and genres. We previously evaluated crowdsourced PP attachment annotation by using MTurk workers to reproduce PP attachments from the Wall Street Journal corpus (Rosenthal et al., 2010). The results demonstrated that MTurk workers are capable of identifying PP attachments in newswire text, but the approach used to generate attachment options is dependent on the existing gold-standard parse trees and cannot be used on corpora where parse trees are not available. In this paper, we build on the semiautomated annotation principle while avoiding the dependency on parsers, allowing us to apply this technique to the noisy and informal text found in blogs. 3 System Description Our system must both identify PPs and generate a list of potential attachments for each PP in this section."
W10-0702,D08-1027,0,0.313945,"Missing"
W10-0702,W97-0109,0,0.0275117,"ably decomposed into independent judgments that untrained annotators can tackle (e.g., quantifier scoping, conjunction scope). This work is intended as an initial step towards the development of efficient hybrid annotation tools that seamlessly incorporate aggregate human wisdom alongside effective algorithms. 2 Related Work Identifying PP attachments is an essential task for building syntactic parse trees. While this task has been studied using fully-automated systems, many of them rely on parse tree output for predicting potential attachments (Ratnaparkhi et al., 1994; Yeh and Vilain, 1998; Stetina and Nagao, 1997; Zavrel et al., 1997). However, systems that rely on good parses are unlikely to perform well on new genres such as blogs and machine translated texts for which parse tree training data is not readily available. Furthermore, the predominant dataset for evaluating PP attachment is the RRR dataset (Ratnaparkhi et al., 1994) which consists of PP attachment cases from the Wall Street Journal portion of the Penn Treebank. Instead of complete sentences, this dataset consists of sets of the form {V,N1 ,P,N2 } where {P,N2 } is the PP and {V,N1 } are the potential attachments. This simplification of t"
W10-0702,P98-2234,0,0.0383594,"task which can be reliably decomposed into independent judgments that untrained annotators can tackle (e.g., quantifier scoping, conjunction scope). This work is intended as an initial step towards the development of efficient hybrid annotation tools that seamlessly incorporate aggregate human wisdom alongside effective algorithms. 2 Related Work Identifying PP attachments is an essential task for building syntactic parse trees. While this task has been studied using fully-automated systems, many of them rely on parse tree output for predicting potential attachments (Ratnaparkhi et al., 1994; Yeh and Vilain, 1998; Stetina and Nagao, 1997; Zavrel et al., 1997). However, systems that rely on good parses are unlikely to perform well on new genres such as blogs and machine translated texts for which parse tree training data is not readily available. Furthermore, the predominant dataset for evaluating PP attachment is the RRR dataset (Ratnaparkhi et al., 1994) which consists of PP attachment cases from the Wall Street Journal portion of the Penn Treebank. Instead of complete sentences, this dataset consists of sets of the form {V,N1 ,P,N2 } where {P,N2 } is the PP and {V,N1 } are the potential attachments."
W10-0702,W97-1016,0,0.0176698,"pendent judgments that untrained annotators can tackle (e.g., quantifier scoping, conjunction scope). This work is intended as an initial step towards the development of efficient hybrid annotation tools that seamlessly incorporate aggregate human wisdom alongside effective algorithms. 2 Related Work Identifying PP attachments is an essential task for building syntactic parse trees. While this task has been studied using fully-automated systems, many of them rely on parse tree output for predicting potential attachments (Ratnaparkhi et al., 1994; Yeh and Vilain, 1998; Stetina and Nagao, 1997; Zavrel et al., 1997). However, systems that rely on good parses are unlikely to perform well on new genres such as blogs and machine translated texts for which parse tree training data is not readily available. Furthermore, the predominant dataset for evaluating PP attachment is the RRR dataset (Ratnaparkhi et al., 1994) which consists of PP attachment cases from the Wall Street Journal portion of the Penn Treebank. Instead of complete sentences, this dataset consists of sets of the form {V,N1 ,P,N2 } where {P,N2 } is the PP and {V,N1 } are the potential attachments. This simplification of the PP attachment task"
W10-0702,C98-2229,0,\N,Missing
W10-0702,D07-1112,0,\N,Missing
W10-4205,C02-1138,0,0.0117878,"ctive, online encoding tool. We found that developing a custom realizer as a module to our Java-based system was preferable to integrating a large, general purpose system such as KPML/Nigel (Matthiessen and Bateman, 1991) or FUF/SURGE (Elhadad and Robin, 1996). These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). The statistically trained FERGUS (Chen et al., 2002) contrasts with our rule-based approach. Dorr and Gaasterland (1995) and Grote (1998) focus on generating temporal connectives, such as before, based on the relative times and durations of two events; Gagnon and Lapalme (1996) focus on temporal adverbials (e.g., when to insert a known time of day for an event). By comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which they do not report implementing. While our work focuses on English, Yang and Bateman (2009) describe a recent system for generating Chinese aspect expressions based on a ti"
W10-4205,W96-0501,0,0.0310112,"shot of our story encoding interface. recent work has centered on markup languages for complex temporal information (Mani, 2004) and corpus-based (statistical) models for predicting temporal relationships on unseen text (Mani et al., 2006; Lapata and Lascarides, 2006). Our annotation interface requires a fast realizer that can be easily integrated into an interactive, online encoding tool. We found that developing a custom realizer as a module to our Java-based system was preferable to integrating a large, general purpose system such as KPML/Nigel (Matthiessen and Bateman, 1991) or FUF/SURGE (Elhadad and Robin, 1996). These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). The statistically trained FERGUS (Chen et al., 2002) contrasts with our rule-based approach. Dorr and Gaasterland (1995) and Grote (1998) focus on generating temporal connectives, such as before, based on the relative times and durations of two events; Gagnon and Lapalme (1996) focus on tempor"
W10-4205,J96-1004,0,0.0668307,"1) or FUF/SURGE (Elhadad and Robin, 1996). These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). The statistically trained FERGUS (Chen et al., 2002) contrasts with our rule-based approach. Dorr and Gaasterland (1995) and Grote (1998) focus on generating temporal connectives, such as before, based on the relative times and durations of two events; Gagnon and Lapalme (1996) focus on temporal adverbials (e.g., when to insert a known time of day for an event). By comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which they do not report implementing. While our work focuses on English, Yang and Bateman (2009) describe a recent system for generating Chinese aspect expressions based on a time interval representation, using KPML as their surface realizer. Several other projects run tangential to our interactive narrative encoding project. Callaway and Lester’s STORYBOOK (2002) aims to improve fluency and discours"
W10-4205,W98-0304,0,0.051124,"Java-based system was preferable to integrating a large, general purpose system such as KPML/Nigel (Matthiessen and Bateman, 1991) or FUF/SURGE (Elhadad and Robin, 1996). These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). The statistically trained FERGUS (Chen et al., 2002) contrasts with our rule-based approach. Dorr and Gaasterland (1995) and Grote (1998) focus on generating temporal connectives, such as before, based on the relative times and durations of two events; Gagnon and Lapalme (1996) focus on temporal adverbials (e.g., when to insert a known time of day for an event). By comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which they do not report implementing. While our work focuses on English, Yang and Bateman (2009) describe a recent system for generating Chinese aspect expressions based on a time interval representation, using KPML as their surface realizer. Several other proje"
W10-4205,P87-1002,0,0.528156,"ecting tense and aspect for single events in Section 3. Section 4 follows with more complex cases involving multiple events and shifts in temporal focus. We then discuss the results. 2 Related Work There has been intense interest in the interpretation of tense and aspect into a formal understanding of the ordering and duration of events. This work has been in both linguistics (Dowty, 1979; Nerbonne, 1986; Vlach, 1993) and natural language understanding. Early systems investigated rule-based approaches to parsing the durations and orderings of events from the tenses and aspects of their verbs (Hinrichs, 1987; Webber, 1987; Song and Cohen, 1988; Passonneau, 1988). Allen (1984) and Steedman (1995) focus on distinguishing between achievements (when an event culminates in a result, such as John builds a house) and processes (such as walking). More Figure 1: Screenshot of our story encoding interface. recent work has centered on markup languages for complex temporal information (Mani, 2004) and corpus-based (statistical) models for predicting temporal relationships on unseen text (Mani et al., 2006; Lapata and Lascarides, 2006). Our annotation interface requires a fast realizer that can be easily inte"
W10-4205,kingsbury-palmer-2002-treebank,0,0.0413514,"ntences in various temporal scenarios through a graphical interface. 3 3.1 Expressing single events Temporal knowledge The propositions that we aim to realize take the form of a predicate, one or more arguments, zero or more attached modifiers (either a negation operator or an adverbial, which is itself a proposition), and an assignment in time. Each argument is associated with a semantic role (such as Agent or Experiencer), and may include nouns (such as characters) or other propositions. In our implemented system, the set of predicates available to the annotator is adapted from the VerbNet (Kingsbury and Palmer, 2002) and WordNet (Fellbaum, 1998) linguistic databanks. These provide both durative actions and statives (Dowty, 1979); we will refer to both as events as they occur over intervals. For example, here are an action and a stative: walk(Mary, store, 2, 6) (1) hungry(Julia, 1,∞) (2) The latter two arguments in (1) refer to time states in a totally ordered sequence; Mary starts walking to the store at state 2 and finishes walking at state 6. (2) begins at state 1, but is unbounded (Julia never ceases being hungry). While this paper does not address the use of reference times (1) and (2), depending on t"
W10-4205,P06-1095,0,0.276875,"arsing the durations and orderings of events from the tenses and aspects of their verbs (Hinrichs, 1987; Webber, 1987; Song and Cohen, 1988; Passonneau, 1988). Allen (1984) and Steedman (1995) focus on distinguishing between achievements (when an event culminates in a result, such as John builds a house) and processes (such as walking). More Figure 1: Screenshot of our story encoding interface. recent work has centered on markup languages for complex temporal information (Mani, 2004) and corpus-based (statistical) models for predicting temporal relationships on unseen text (Mani et al., 2006; Lapata and Lascarides, 2006). Our annotation interface requires a fast realizer that can be easily integrated into an interactive, online encoding tool. We found that developing a custom realizer as a module to our Java-based system was preferable to integrating a large, general purpose system such as KPML/Nigel (Matthiessen and Bateman, 1991) or FUF/SURGE (Elhadad and Robin, 1996). These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from s"
W10-4205,A97-1039,0,0.0348673,"on markup languages for complex temporal information (Mani, 2004) and corpus-based (statistical) models for predicting temporal relationships on unseen text (Mani et al., 2006; Lapata and Lascarides, 2006). Our annotation interface requires a fast realizer that can be easily integrated into an interactive, online encoding tool. We found that developing a custom realizer as a module to our Java-based system was preferable to integrating a large, general purpose system such as KPML/Nigel (Matthiessen and Bateman, 1991) or FUF/SURGE (Elhadad and Robin, 1996). These realizers, along with RealPro (Lavoie and Rambow, 1997), accept tense as a parameter, but do not calculate it from a semantic representation of overlapping time intervals such as ours (though the Nigel grammar can calculate tense from speech, event, and reference time orderings, discussed below). The statistically trained FERGUS (Chen et al., 2002) contrasts with our rule-based approach. Dorr and Gaasterland (1995) and Grote (1998) focus on generating temporal connectives, such as before, based on the relative times and durations of two events; Gagnon and Lapalme (1996) focus on temporal adverbials (e.g., when to insert a known time of day for an"
W10-4205,C92-2073,0,0.162002,"known time of day for an event). By comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which they do not report implementing. While our work focuses on English, Yang and Bateman (2009) describe a recent system for generating Chinese aspect expressions based on a time interval representation, using KPML as their surface realizer. Several other projects run tangential to our interactive narrative encoding project. Callaway and Lester’s STORYBOOK (2002) aims to improve fluency and discourse cohesion in realizing formally encoded narratives; Ligozat and Zock (1992) allow users to interactively construct sentences in various temporal scenarios through a graphical interface. 3 3.1 Expressing single events Temporal knowledge The propositions that we aim to realize take the form of a predicate, one or more arguments, zero or more attached modifiers (either a negation operator or an adverbial, which is itself a proposition), and an assignment in time. Each argument is associated with a semantic role (such as Agent or Experiencer), and may include nouns (such as characters) or other propositions. In our implemented system, the set of predicates available to t"
W10-4205,W04-0208,0,0.0153311,"20 = E2 ), we assign the perspective End. To realize this, we reassign the perspective to that between R and E 0 , and reassign the verb predicate to stop (or finish for cumulative achievements). Similarly, the predicate’s argument is the original proposition rendered in the infinitive. 4 Alternate timelines and modalities This section covers more complex situations involving alternate timelines– the feature of our representation by which a proposition in the main timeline can refer to a second frame of time. Other models of time have supported similar encapsulations (Crouch and Pulman, 1993; Mani and Pustejovsky, 2004). The alternate timeline can contain references to actual events or modal events (imagined, obligated, desired, planned, etc.) in the past the future with respect to its point of attachment on Espeech R S Ehunger E′hunger alternate R′ E′buy Figure 2: Schematic of a speech act attaching to a alternate timeline with a hypothetical action. R0 and Espeech are attachment points. the main timeline. This is primarily used in practice for modeling dialogue acts, but it can also be used to place real events at uncertain time states in the past (e.g., the present perfect is used in a reference story bei"
W10-4205,P09-4003,1,0.840471,"ns. The second result is a set of syntactic constructions for realizing these permutations in our story encoding interface. Our focus here, as we have noted, is not fluency, but a surface-level rendering that reflects the relationships (and, at times, the ambiguities) present in the conceptual encoding. We consider variations in modality, such as an indicative reading as opposed to a conditional or subjunctive reading, to be at the level of the realizer and not another class of tenses. We have run a collection project with our encoding interface and can report success in the tool’s usability (Elson and McKeown, 2009). Two annotators each encoded 20 fables into the formal representation, with their only exposure to the semantic encodings being through the reference text generator (as in Figure 1). Both annotators became comfortable with the tool after a period of training; in surveys that they completed after each task, they gave Likert-scale usability scores of 4.25 and 4.30 (averaged over 20 tasks, with 5 meaning “easiest to use”). These scores are not specific to the generation component, but they suggest that annotators could derive satisfactory tenses from their semantic structures. The most frequentl"
W10-4205,elson-mckeown-2010-building,1,0.809156,"el shows the original fable, and the left-hand panel shows a graphical timeline with buttons for constructing new propositions at certain intervals. The left-hand and bottom-right panels contain automatically generated text of the encoded story, as the system understands it, from different points of view. Users rely on these realizations to check that they have assigned the formal connections correctly. The tenses and aspects of these sentences are a key component of this feedback. We describe the general purpose of the system, its data model, and the encoding methodology in a separate paper (Elson and McKeown, 2010). The paper is organized as follows: After discussing related work in Section 2, we describe our method for selecting tense and aspect for single events in Section 3. Section 4 follows with more complex cases involving multiple events and shifts in temporal focus. We then discuss the results. 2 Related Work There has been intense interest in the interpretation of tense and aspect into a formal understanding of the ordering and duration of events. This work has been in both linguistics (Dowty, 1979; Nerbonne, 1986; Vlach, 1993) and natural language understanding. Early systems investigated rule"
W10-4205,J88-2003,0,0.838442,"s a sequence of tokens that are either cue words (began, stopped, etc.) or conjugations of the predicate’s verb. These constructions emphasize precision over fluency. As we have noted, theorists have distinguished between “statives” that are descriptive (John was hungry), “achievement” actions that culminate in a state change (John built the house), and “activities” that are more continuous and divisible (John read a book for an hour) (Dowty, 1979). Prior work in temporal connectives has taken advantage of lexical information to determine the correct situation and assign aspect appropriately (Moens and Steedman, 1988; Dorr and Gaasterland, 1995). In our case, we only distinguish between actions and statives, based on information from WordNet and VerbNet. We use a separate table for statives; it is similar to Table 2, except the constructions replace verb conjugations with insertions of be, been, being, was, were, felt, and so on (with the latter applying to affective states). We do not currently distinguish between achievements and activities in selecting tense and aspect, except that the annotator is tasked with “manually” indicating a new state when an event culminates in one (e.g., The house was comple"
W10-4205,J88-2005,0,0.471884,"3. Section 4 follows with more complex cases involving multiple events and shifts in temporal focus. We then discuss the results. 2 Related Work There has been intense interest in the interpretation of tense and aspect into a formal understanding of the ordering and duration of events. This work has been in both linguistics (Dowty, 1979; Nerbonne, 1986; Vlach, 1993) and natural language understanding. Early systems investigated rule-based approaches to parsing the durations and orderings of events from the tenses and aspects of their verbs (Hinrichs, 1987; Webber, 1987; Song and Cohen, 1988; Passonneau, 1988). Allen (1984) and Steedman (1995) focus on distinguishing between achievements (when an event culminates in a result, such as John builds a house) and processes (such as walking). More Figure 1: Screenshot of our story encoding interface. recent work has centered on markup languages for complex temporal information (Mani, 2004) and corpus-based (statistical) models for predicting temporal relationships on unseen text (Mani et al., 2006; Lapata and Lascarides, 2006). Our annotation interface requires a fast realizer that can be easily integrated into an interactive, online encoding tool. We fo"
W10-4205,P87-1021,0,0.837456,"aspect for single events in Section 3. Section 4 follows with more complex cases involving multiple events and shifts in temporal focus. We then discuss the results. 2 Related Work There has been intense interest in the interpretation of tense and aspect into a formal understanding of the ordering and duration of events. This work has been in both linguistics (Dowty, 1979; Nerbonne, 1986; Vlach, 1993) and natural language understanding. Early systems investigated rule-based approaches to parsing the durations and orderings of events from the tenses and aspects of their verbs (Hinrichs, 1987; Webber, 1987; Song and Cohen, 1988; Passonneau, 1988). Allen (1984) and Steedman (1995) focus on distinguishing between achievements (when an event culminates in a result, such as John builds a house) and processes (such as walking). More Figure 1: Screenshot of our story encoding interface. recent work has centered on markup languages for complex temporal information (Mani, 2004) and corpus-based (statistical) models for predicting temporal relationships on unseen text (Mani et al., 2006; Lapata and Lascarides, 2006). Our annotation interface requires a fast realizer that can be easily integrated into an"
W10-4205,P09-1071,0,0.0247135,"nt, and reference time orderings, discussed below). The statistically trained FERGUS (Chen et al., 2002) contrasts with our rule-based approach. Dorr and Gaasterland (1995) and Grote (1998) focus on generating temporal connectives, such as before, based on the relative times and durations of two events; Gagnon and Lapalme (1996) focus on temporal adverbials (e.g., when to insert a known time of day for an event). By comparison, we extend our approach to cover direct/indirect speech and the subjunctive/conditional forms, which they do not report implementing. While our work focuses on English, Yang and Bateman (2009) describe a recent system for generating Chinese aspect expressions based on a time interval representation, using KPML as their surface realizer. Several other projects run tangential to our interactive narrative encoding project. Callaway and Lester’s STORYBOOK (2002) aims to improve fluency and discourse cohesion in realizing formally encoded narratives; Ligozat and Zock (1992) allow users to interactively construct sentences in various temporal scenarios through a graphical interface. 3 3.1 Expressing single events Temporal knowledge The propositions that we aim to realize take the form of"
W11-1606,N09-2047,0,0.0219519,"and introduce additional single commodity flow constraints (Magnanti and Wolsey, 1994) adapted from Martins et al. (2009) over the interleaving paths to guarantee that the output will only involve a linear sequence of aligned phrases and paths. 5 Evaluation We now turn to the design of experiments for the strict sentence intersection task and discuss the performance of the proposed models using the corpus provided by McKeown et al. (2010). We use a beam size of 50 for the beam search decoder and a 4-gram LM for all experiments. Dependency parsing is accomplished with MICA, a TAG-based parser (Bangalore et al., 2009). Our primary considerations for studying system-generated fusions are validity (whether the output contains only the information common to each sentence), coverage (whether the output contains all the common information in the input sentences) and the fluency of the output. 49 5.1 Evaluating Validity and Fluency Evaluating the validity of an intersection involves determining whether it contains only the information contained in each sentence and nothing else. In order to do this, we make use of the interpretation of valid intersections as being mutually entailed by the input sentences. It fol"
W11-1606,J05-3002,1,0.931482,"(2006) lets you directly connect the QA150-EXAT to a file server and issue a command from any workstation to back up the server Human compression #1 TapeWare supports DOS and NetWare 286 Human compression #2 TapeWare lets you connect the QA150-EXAT to a file server (hypothesized) Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ significantly in semantic content. Italicized text is used to indicate fragments that are semantically identical. valid intersections that follows the basic framework of previous unsupervised fusion systems (Barzilay and McKeown, 2005; Filippova and Strube, 2008b). In our approach, the input sentences are first aligned using a modified version of a recent phrase-based alignment approach (MacCartney et al., 2008). We assume the alignments that are produced define aspects of the input that must appear in the output fusion and consider decoding strategies to recover intersections that preserve these alignments. In addition to a search-based decoding strategy, we propose a constrained integer linear programming (ILP) formulation that attempts to decode the most fluent sentence covering all these aspects while minimizing the si"
W11-1606,bensley-hickl-2008-unsupervised,0,0.0164629,"pressions in which the redundancy of information content across input sentences in a multidocument setting is assumed to directly indicate its salience, thereby consigning it to the output. Additionally, in this work, we frequently consider the sentence intersection task from the perspective of textual entailment (cf. §5.1). The textual entailment task involves automatically determining whether a given hypothesis can be inferred from a textual premise (Dagan et al., 2005; Bar-Haim et al., 2006). Automatic construction of positive and negative entailment examples has been explored in the past (Bensley and Hickl, 2008) to provide training data for entailment systems; however the production of text that is simultaneously entailed by two (or more) sentences is a far more constrained and difficult challenge. ILP has been used extensively for text-to-text generation problems in recent years (Clarke and Lapata, 2008; Filippova and Strube, 2008b; Woodsend et al., 2010), including techniques which incorporate syntax directly into the decoding to imporove the fluency of the resulting text. In this paper, we focus on generating valid intersections and do not incorporate syntactic and semantic constraints into our IL"
W11-1606,W10-0735,0,0.0297776,"n of valid intersections as being mutually entailed by the input sentences. It follows that the task of judging the validity of an intersection can simply be decomposed into two tasks that judge whether the intersection is entailed by each input sentence. We make use of Amazon’s Mechanical Turk (AMT) platform to have humans evaluate the intersections produced. Crowdsourcing annotations and judgments in this manner has been shown to be cheap and effective for natural language tasks (Snow et al., 2008) and has recently been employed in similar entailment-detection tasks (Negri and Mehdad, 2010; Buzek et al., 2010). Since we only seek judgments on produced intersections and avoid presenting both input sentences to users, we do not anticipate the noisiness that was noted by McKeown et al. (2010) when asking AMT users to generate intersections. Each entailment task is framed as a multiple choice question. An AMT user is shown just one input sentence (the premise in entailment terminology) along with a potential intersection (the hypothesis) and is required to respond to whether there is any new or different information in the latter that is not in the former. They can respond on a 3-point scale (yes/no/ma"
W11-1606,W04-1016,0,0.608474,"Missing"
W11-1606,C04-1057,0,0.0345255,"e intersection to sentence union. This makes the comparison of different fusion systems dependent on task-based utility2 . In addition, intersection comprises an interesting problem in its own right. It necessitates the use of generalization over phrases in order to convey only the content of the input sentences when different wording is used and therefore involves more than just word deletion. The analogy to set-theoretic intersection in this task implies an underlying consideration of each sentence as a set of informational concepts, similar to previous work in summarization and redundancy (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008). While we don’t commit to any semantic representation for such elements of information, we can nevertheless attempt to identify repeated information using well-studied natural language analysis techniques such as alignment and paraphrase recognition, and furthermore isolate this information through text-to-text generation techniques. Consider, for example, the first sentence pair from the examples in Table 2. A valid intersection for these sentences must not contain any information that is not substantiated by both of them, so a fusion that mentions “Mr Litvinenko’"
W11-1606,W08-1105,0,0.249744,"nnect the QA150-EXAT to a file server and issue a command from any workstation to back up the server Human compression #1 TapeWare supports DOS and NetWare 286 Human compression #2 TapeWare lets you connect the QA150-EXAT to a file server (hypothesized) Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ significantly in semantic content. Italicized text is used to indicate fragments that are semantically identical. valid intersections that follows the basic framework of previous unsupervised fusion systems (Barzilay and McKeown, 2005; Filippova and Strube, 2008b). In our approach, the input sentences are first aligned using a modified version of a recent phrase-based alignment approach (MacCartney et al., 2008). We assume the alignments that are produced define aspects of the input that must appear in the output fusion and consider decoding strategies to recover intersections that preserve these alignments. In addition to a search-based decoding strategy, we propose a constrained integer linear programming (ILP) formulation that attempts to decode the most fluent sentence covering all these aspects while minimizing the size and disfluency of interle"
W11-1606,D08-1019,0,0.210343,"nnect the QA150-EXAT to a file server and issue a command from any workstation to back up the server Human compression #1 TapeWare supports DOS and NetWare 286 Human compression #2 TapeWare lets you connect the QA150-EXAT to a file server (hypothesized) Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ significantly in semantic content. Italicized text is used to indicate fragments that are semantically identical. valid intersections that follows the basic framework of previous unsupervised fusion systems (Barzilay and McKeown, 2005; Filippova and Strube, 2008b). In our approach, the input sentences are first aligned using a modified version of a recent phrase-based alignment approach (MacCartney et al., 2008). We assume the alignments that are produced define aspects of the input that must appear in the output fusion and consider decoding strategies to recover intersections that preserve these alignments. In addition to a search-based decoding strategy, we propose a constrained integer linear programming (ILP) formulation that attempts to decode the most fluent sentence covering all these aspects while minimizing the size and disfluency of interle"
W11-1606,N09-2057,0,0.0157035,"ms that mistakenly align faraway fragments both cause spans of unaligned text that 51 must be then abstracted over. We hypothesize that these issues could be tackled with the use of joint models: a system that aligns as it decodes could reduce the need for abstraction over long unaligned spans, although care would have to be taken to ensure that coverage is maintained. Additionally, richer lexical resources such as wider-coverage ontologies (Snow et al., 2006) and entailment/paraphrase dictionaries could aid in improving coverage. Finally, previous work in fusion (Filippova and Strube, 2008b; Filippova and Strube, 2009) has noted that models based on syntax outperform techniques that rely solely on LM scores to determine fluency, and strict intersection appears to be well-suited for further exploration in this vein. 7 Conclusion We have examined the text-to-text generation task of strict sentence intersection, which restricts semantic variation in the output and necessarily invokes the problems of generalization and abstraction in addition to the usual challenge of producing fluent text. We tackle the task as lattice decoding and discuss two decoding strategies for producing valid intersections. In addition,"
W11-1606,N03-1013,0,0.082667,"es which have already been aligned using additional information (such as their neighboring context), thus avoiding generalizations that do not fit the alignment. For our experiments, we make use of the Wordnet ontology (Miller, 1995) to find the hypernyms common to every aligned pair of non-identical phrases, and only attempt to detect entailments which are comprised of specific instances that entail general concepts. This approach can be augmented by the use of entailment corpora and distributional clustering which we intend to explore in future work. We also use the lexical resource CatVar (Habash and Dorr, 2003) to try to generate morphological variants of aligned words that enable them to be interchanged without creating disfluencies. 4.3 Pragmatic abstraction Our strategy assumes that aligned text must be preserved in output intersections whereas unaligned text must be minimized. However, unaligned text cannot simply be dropped as it may contain vital portions for generating fluent text. In addition, unaligned phrases can be caused by paraphrased or metaphorical text that the aligner is not capable of identifying. For example, the phrases “polonium” and “radioactive substances” in the second senten"
W11-1606,A00-2024,1,0.823132,"ion between intersection and union of text was introduced in the context of sentence fusion (Krahmer et al., 2008; Marsi and Krahmer, 2005) in order to distinguish between traditional fusion strategies that attempted to include only common content and fusions that attempted to include all non-redundant content from the input. We focus here on strict sentence intersection, explicitly incorporating a constraint that requires that a produced fusion must not contain information that is not present in all input sentences. This distinguishes our approach from traditional sentence fusion approaches (Jing and McKeown, 2000; Barzilay and McKeown, 2005; Filippova and Strube, 2008b) which generally attempt to retain common information but are typically evaluated in an abstractive summarization context in which additional information in the fusion output does not negatively impact judgments. This task is also related to the field of sentence compression which has received much attention in recent years (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008a; Cohn and Lapata, 2009; Marsi et al., 2010). Intersections can be viewed as guided compressions in which the redundancy"
W11-1606,P08-2049,0,0.54689,"Missing"
W11-1606,D08-1084,0,0.0307858,"Missing"
W11-1606,W05-1612,0,0.563801,"le 1. In this work, we examine the task of sentence intersection: a variant of sentence fusion that does not permit semantic variation in the output. A strict1 intersection system is expected to produce a fused sentence that contains all the information common to its input sentences and avoid information that is in just one of the inputs. In other words, a valid intersection should only contain information that is substantiated by all input sentences. The set-theoretic notions of intersection (along with union) have been employed to describe variants of sentence fusion tasks in previous work (Marsi and Krahmer, 2005; Krahmer et al., 2008) but, to our knowledge, this work is the first to explicitly tackle and evaluate the strict intersection task. We focus on the case of unsupervised pairwise sentence intersection and propose a strategy to yield 1 We use the term strict to make explicit the distinction from traditional fusion systems, which generally aim at notions of intersection but are not formally evaluated with respect to it. 43 Workshop on Monolingual Text-To-Text Generation, pages 43–53, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43–53, c Portland"
W11-1606,P09-1039,0,0.0418696,"hrases that they connect together are themselves present using the following set of constraints. aki − |Ak | X pk∗ i∗ = 1 ∀aki ∈ Ak , Ak ∈ I p∗l ∗j = 1 ∀alj ∈ Al , Al ∈ I i=0 |Al | alj − X j=0 pkl ij pkl ij − aki &lt;= 0 ∀i, j, k, l − alj &lt;= 0 ∀i, j, k, l As we don’t restrict the structure of the lattice in any way and allow crossing alignment links, the program as defined thus far is capable of generating cyclic and fragmented solutions. To combat this, we add dummy start and end phrase variables and introduce additional single commodity flow constraints (Magnanti and Wolsey, 1994) adapted from Martins et al. (2009) over the interleaving paths to guarantee that the output will only involve a linear sequence of aligned phrases and paths. 5 Evaluation We now turn to the design of experiments for the strict sentence intersection task and discuss the performance of the proposed models using the corpus provided by McKeown et al. (2010). We use a beam size of 50 for the beam search decoder and a 4-gram LM for all experiments. Dependency parsing is accomplished with MICA, a TAG-based parser (Bangalore et al., 2009). Our primary considerations for studying system-generated fusions are validity (whether the outpu"
W11-1606,E06-1038,0,0.258883,"most text generation systems is that a given input can map to a range of lexically diverse outputs. However, text-to-text tasks defined with vague criteria such as the preservation of the “important” information in text can also permit outputs that are semantically distinct. This can make evaluation difficult; for instance, systemgenerated sentences may differ (partially or completely) in informational content from reference human-annotated text. This phenomenon has been noted and discussed in the task of pairwise sentence fusion (Daum´e III and Marcu, 2004) and also in sentence compression (McDonald, 2006). Some examples are listed in Table 1. In this work, we examine the task of sentence intersection: a variant of sentence fusion that does not permit semantic variation in the output. A strict1 intersection system is expected to produce a fused sentence that contains all the information common to its input sentences and avoid information that is in just one of the inputs. In other words, a valid intersection should only contain information that is substantiated by all input sentences. The set-theoretic notions of intersection (along with union) have been employed to describe variants of sentenc"
W11-1606,N10-1044,1,0.318858,"arch-based decoding strategy, we propose a constrained integer linear programming (ILP) formulation that attempts to decode the most fluent sentence covering all these aspects while minimizing the size and disfluency of interleaving text. This is a fairly general model which can also be extended to other alignment-based tasks such as pairwise union and difference. As this is a substantially more constrained task than generic sentence fusion, we also present a novel evaluation approach that avoids out-of-context salience judgments. We make use of a recentlyreleased corpus of fusion candidates (McKeown et al., 2010) and propose a crowdsourced entailmentstyle evaluation to determine the validity of generated intersections, as well as the grammaticality of the sentences produced. Additionally, automated machine translation (MT) metrics are explored to quantify the amount of information missing from valid intersections. Our decoding strategies show 44 promise under these experiments and we discuss potential directions for improving intersection performance. 2 Related Work The distinction between intersection and union of text was introduced in the context of sentence fusion (Krahmer et al., 2008; Marsi and"
W11-1606,W10-0734,0,0.0238585,"use of the interpretation of valid intersections as being mutually entailed by the input sentences. It follows that the task of judging the validity of an intersection can simply be decomposed into two tasks that judge whether the intersection is entailed by each input sentence. We make use of Amazon’s Mechanical Turk (AMT) platform to have humans evaluate the intersections produced. Crowdsourcing annotations and judgments in this manner has been shown to be cheap and effective for natural language tasks (Snow et al., 2008) and has recently been employed in similar entailment-detection tasks (Negri and Mehdad, 2010; Buzek et al., 2010). Since we only seek judgments on produced intersections and avoid presenting both input sentences to users, we do not anticipate the noisiness that was noted by McKeown et al. (2010) when asking AMT users to generate intersections. Each entailment task is framed as a multiple choice question. An AMT user is shown just one input sentence (the premise in entailment terminology) along with a potential intersection (the hypothesis) and is required to respond to whether there is any new or different information in the latter that is not in the former. They can respond on a 3-p"
W11-1606,J03-1002,0,0.00831047,"-search technique and an ILP Alignment link k Phrases from S1 Shared phrases Alignment link l m m m m Phrases from S1 Shared phrases Phrases from S2 Phrases from S2 Figure 1: The general structure of one segment of the alignment lattice, illustrating the potential interleaving paths between aligned phrases. Solid lines indicate paths derived from sentence 1 and dashed lines indicate paths derived from sentence 2 strategy that leverages our initial assumption that all aligned phrases must appear in the output. 4.4.1 Beam search Search-based decoding is often employed in phrasebased MT systems (Och and Ney, 2003) and is implemented in the Moses toolkit5 ; similar approaches have also been used in text-to-text generation tasks (Barzilay and McKeown, 2005; Soricut and Marcu, 2006). This technique attempts to find the highest-scoring sentence string under the LM by unwrapping and searching through a lattice. Since the dynamic programming search could require an exponential number of search states, a fixed-width beam can be used to control the number of search states being actively considered at each step. In order to decode an intersection problem, we first pick a beam size B and initialize the list of c"
W11-1606,P02-1040,0,0.0821693,"uation for coverage of intersections described in §5.3. without annotators who understand the concept of intersection. We instead attempt to utilize the high-quality human-generated union dataset from McKeown et al. (2010) in evaluating the coverage of our intersection systems. Using the simple absorption law A ∩ (A ∪ B) = A, we assume that the coverage of intersection systems can be judged by how well they can recover an input sentence from humangenerated unions. The resulting outputs are compared to the original input sentences in an MTstyle evaluation under two commonly-used metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). The results of this automated evaluation are shown in Table 5. The aligned words system here always considers words from the union sentence and can therefore be seen as a baseline system. We observe that the segmented decoder produces output that is judged most similar to the input sentences under BLEU, which measures n-gram overlap, although results under NIST (which gives additional weight to rarer n-grams) are less conclusive. 6 Discussion The experimental results indicate that the two systems we describe, particularly the segmented decoder, do a reasonable job"
W11-1606,P06-1101,0,0.0103407,"approach. Furthermore, we notice that the quality of alignment also factors in to this problem: systems that miss phrases which should be aligned or systems that mistakenly align faraway fragments both cause spans of unaligned text that 51 must be then abstracted over. We hypothesize that these issues could be tackled with the use of joint models: a system that aligns as it decodes could reduce the need for abstraction over long unaligned spans, although care would have to be taken to ensure that coverage is maintained. Additionally, richer lexical resources such as wider-coverage ontologies (Snow et al., 2006) and entailment/paraphrase dictionaries could aid in improving coverage. Finally, previous work in fusion (Filippova and Strube, 2008b; Filippova and Strube, 2009) has noted that models based on syntax outperform techniques that rely solely on LM scores to determine fluency, and strict intersection appears to be well-suited for further exploration in this vein. 7 Conclusion We have examined the text-to-text generation task of strict sentence intersection, which restricts semantic variation in the output and necessarily invokes the problems of generalization and abstraction in addition to the u"
W11-1606,D08-1027,0,0.0129336,"Missing"
W11-1606,P06-1139,0,0.0278163,"2 Figure 1: The general structure of one segment of the alignment lattice, illustrating the potential interleaving paths between aligned phrases. Solid lines indicate paths derived from sentence 1 and dashed lines indicate paths derived from sentence 2 strategy that leverages our initial assumption that all aligned phrases must appear in the output. 4.4.1 Beam search Search-based decoding is often employed in phrasebased MT systems (Och and Ney, 2003) and is implemented in the Moses toolkit5 ; similar approaches have also been used in text-to-text generation tasks (Barzilay and McKeown, 2005; Soricut and Marcu, 2006). This technique attempts to find the highest-scoring sentence string under the LM by unwrapping and searching through a lattice. Since the dynamic programming search could require an exponential number of search states, a fixed-width beam can be used to control the number of search states being actively considered at each step. In order to decode an intersection problem, we first pick a beam size B and initialize the list of candidate search states with the first interleaving paths in each sentence. At every iteration, we consider the B candidates with the highest normalized scores under the"
W11-1606,C08-1110,1,0.849863,"s makes the comparison of different fusion systems dependent on task-based utility2 . In addition, intersection comprises an interesting problem in its own right. It necessitates the use of generalization over phrases in order to convey only the content of the input sentences when different wording is used and therefore involves more than just word deletion. The analogy to set-theoretic intersection in this task implies an underlying consideration of each sentence as a set of informational concepts, similar to previous work in summarization and redundancy (Filatova and Hatzivassiloglou, 2004; Thadani and McKeown, 2008). While we don’t commit to any semantic representation for such elements of information, we can nevertheless attempt to identify repeated information using well-studied natural language analysis techniques such as alignment and paraphrase recognition, and furthermore isolate this information through text-to-text generation techniques. Consider, for example, the first sentence pair from the examples in Table 2. A valid intersection for these sentences must not contain any information that is not substantiated by both of them, so a fusion that mentions “Mr Litvinenko’s poisoning”, “Britain” or “"
W11-1606,P11-2044,1,0.834037,"; Filippova and Strube, 2008b) and sentence compression (Clarke and Lapata, 2008; Filippova and Strube, 2008a). Additionally, we focus on the case of pairwise sentence intersection and assume that the common information between the input sentence pair can be represented within a single output sentence. As a result, although the McKeown et al. (2010) corpus cannot be used for training an intersection model, we can make use of the sentence pairs it contains for evaluation. decoding with exact ILP-based alignment decoding and incorporates syntactic constraints to produce more precise alignments (Thadani and McKeown, 2011). The aligner is trained on a corpus of human-generated alignment annotations produced by Microsoft Research (Brockett, 2007) for inference problems from the second Recognizing Textual Entailment (RTE2) challenge (Bar-Haim et al., 2006). Entailment problems are inherently asymmetric because premise text is generally larger than hypothesis text; however, this does not apply to our intersection problems and consequently our MANLI implementation drops asymmetric indicator features. The absence of these features impacts alignment performance on RTE2 data but our reimplementation performs comparabl"
W11-1606,P05-1036,0,0.0177788,"raint that requires that a produced fusion must not contain information that is not present in all input sentences. This distinguishes our approach from traditional sentence fusion approaches (Jing and McKeown, 2000; Barzilay and McKeown, 2005; Filippova and Strube, 2008b) which generally attempt to retain common information but are typically evaluated in an abstractive summarization context in which additional information in the fusion output does not negatively impact judgments. This task is also related to the field of sentence compression which has received much attention in recent years (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008a; Cohn and Lapata, 2009; Marsi et al., 2010). Intersections can be viewed as guided compressions in which the redundancy of information content across input sentences in a multidocument setting is assumed to directly indicate its salience, thereby consigning it to the output. Additionally, in this work, we frequently consider the sentence intersection task from the perspective of textual entailment (cf. §5.1). The textual entailment task involves automatically determining whether a given hypothesis can be inferred from a tex"
W11-1606,D10-1050,0,0.0279089,"k involves automatically determining whether a given hypothesis can be inferred from a textual premise (Dagan et al., 2005; Bar-Haim et al., 2006). Automatic construction of positive and negative entailment examples has been explored in the past (Bensley and Hickl, 2008) to provide training data for entailment systems; however the production of text that is simultaneously entailed by two (or more) sentences is a far more constrained and difficult challenge. ILP has been used extensively for text-to-text generation problems in recent years (Clarke and Lapata, 2008; Filippova and Strube, 2008b; Woodsend et al., 2010), including techniques which incorporate syntax directly into the decoding to imporove the fluency of the resulting text. In this paper, we focus on generating valid intersections and do not incorporate syntactic and semantic constraints into our ILP models; these are areas we intend to explore in the future. 3 The Intersection Task The need for strict variants of fusion is motivated by considerations of evaluation and utility in text-totext generation tasks. Without explicit constraints on the semantic content of valid output, the operational definition of fusion can encompass the full spectr"
W11-1606,W07-1401,0,\N,Missing
W11-1606,2003.mtsummit-systems.9,0,\N,Missing
W12-2105,andreas-etal-2012-annotating,1,0.874501,"fall into that category of conversational behavior and these Language Uses are used directly as features in a supervised machine learning model to predict whether or not a participant is an influencer. For example, Dialog Patterns contributes the Language Uses Initiative, Irrelevance, Incitation, Investment and Interjection. The Language Uses of the Persuasion and Agreement/Disagreement components are not described in detail in this paper, and instead are treated as black boxes (indicated by solid boxes in Figure 1). We have previously published work on some of these (Biran and Rambow, 2011; Andreas et al., 2012). The remainder of this section describes them briefly and provides the results of evaluations of their performance (in Table 2). The next section describes the features of the Dialog Patterns component. 41 Persuasion This component identifies three Language Uses: Attempt to Persuade, Claims and Argumentation. We define an attempt to persuade as a set of contributions made by a single participant which may be made anywhere within the thread, and which are all concerned with stating and supporting a single claim. The subject of the claim does not matter: an opinion may seem trivial, but the arg"
W13-3413,C12-1109,0,0.0128479,"t practices and metrics. • Natural Language Processing in Watson Murdock et al. (2012a), McCord et al. (2012). • Structured Knowledge in Watson Murdock et al. (2012b), Kalyanpur et al. (2012), Fan et al. (2012). • Semantic Web OWL, RDF, Semantic Web resources. • Domain Adaptation Ferrucci et al. (2012). • UIMA The UIMA framework, Annotators, Types, Descriptors, tools. Hands-on exercise with the class project architecture (Epstein et al., 2012). • Midterm Workshop Presentations of each team’s project idea and their research into related work and the state of the art. • Distributional Semantics Miller et al. (2012), Gliozzo and Isabella (2005). • Machine Learning and Strategy in Watson 2 87 http://uima.apache.org Figure 2: Overview of the class project framework 1. Query Analysis their project by extending this framework. Even though we built the framework to perform semantic search over a text corpus, many of the teams in this course had projects that went far beyond just semantic search. Our hope was that each team would be able to able independently develop interesting new components for the processing stages of the pipeline, and at the end of the course we would be able to merge the most interesting"
W13-3508,N07-1023,1,0.818261,"s for compression and other text-to-text generation problems. Related Work An early notion of compression was proposed by Dras (1997) as reluctant sentence paraphrasing under length constraints. Jing and McKeown (2000) analyzed human-generated summaries and identified a heavy reliance on sentence reduction (Jing, 2000). The extraction by Knight and Marcu (2000) of a dataset of natural compression instances from the Ziff-Davis corpus spurred interest in supervised approaches to the task (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Unno et al., 2006; Galley and McKeown, 2007; Nomoto, 2007). In particular, McDonald (2006) expanded on Knight & Marcu’s (2002) transitionbased model by using dynamic programming to recover optimal transition sequences, and Clarke and Lapata (2006a) used ILP to replace pairwise transitions with trigrams. Other recent work (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010) has used dependency trees directly as sentence representations for compression. Another line of research has attempted to broaden the notion of compression beyond mere word deletion (Cohn and Lapata, 2009; Ganitkevitch et al., 2011; Napoles et al., 2011a)."
W13-3508,P11-1049,0,0.621037,"over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti and Wolsey, 1994) between all pairs of tokens, inspired by recent work in dependency parsing (Martins et al., 2009). Linear token ordering is maintained by defining realadj valued adjacency commodity flow variables γij which must be non-zero w"
W13-3508,D11-1108,0,0.0576179,"Missing"
W13-3508,P06-4020,0,0.0193092,"rpus. Dependency parses were retrieved using the Stanford parser6 and ILPs were solved using Gurobi.7 As a state-of-the-art baseline for these experiments, we used a reimplementation of the LM-based system of Clarke and Lapata (2008), which we henceforth refer to as CL08. This is equivalent to a variant of our proposed model that excludes variables for syntactic structure, uses LM log-likelihood as a feature for trigram variables and a tf*idf -based significance score for token variables, and incorporates several targeted syntactic constraints based on grammatical relations derived from RASP (Briscoe et al., 2006) designed to encourage fluent output. Due to the absence of word reordering in the gold compressions, trigram variables y that were considered in the structured transduction approach were restricted to only those for which tokens appear in the same order as the input as is the case with CL08. Furthermore, in order to reduce computational overhead for potentially-expensive ILPs, we also excluded dependency arc variables which inverted an existing governor-dependent relationship from the input sentence parse. A recent analysis of approaches to evaluating compression (Napoles et al., 2011b) has s"
W13-3508,N10-1066,0,0.0131563,"zed general-purpose solvers exist for solving ILPs thereby making them tractable for sentencelevel natural language problems in which the number of variables and constraints is described by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti and Wolsey, 1994) be"
W13-3508,P06-2019,0,0.0534877,"endencies. Sentence generation is treated as a discriminative structured prediction task in which rich linguistically-motivated Introduction Recent years have seen increasing interest in textto-text generation tasks such as paraphrasing and text simplification, due in large part to their direct utility in high-level natural language tasks such as abstractive summarization. The task of sentence compression in particular has benefited from the availability of a number of useful resources such as the the Ziff-Davis compression corpus (Knight and Marcu, 2000) and the Edinburgh compression corpus (Clarke and Lapata, 2006b) which make compression problems highly relevant for datadriven approaches involving language generation. The sentence compression task addresses the problem of minimizing the lexical footprint of a 1 To further the analogy, compression is most often formulated as a word deletion task which parallels the popular view of summarization as a sentence extraction task. 65 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65–74, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics features can be used to predict the informativ"
W13-3508,A00-2024,1,0.633449,"Missing"
W13-3508,P06-1048,0,0.367846,"endencies. Sentence generation is treated as a discriminative structured prediction task in which rich linguistically-motivated Introduction Recent years have seen increasing interest in textto-text generation tasks such as paraphrasing and text simplification, due in large part to their direct utility in high-level natural language tasks such as abstractive summarization. The task of sentence compression in particular has benefited from the availability of a number of useful resources such as the the Ziff-Davis compression corpus (Knight and Marcu, 2000) and the Edinburgh compression corpus (Clarke and Lapata, 2006b) which make compression problems highly relevant for datadriven approaches involving language generation. The sentence compression task addresses the problem of minimizing the lexical footprint of a 1 To further the analogy, compression is most often formulated as a word deletion task which parallels the popular view of summarization as a sentence extraction task. 65 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 65–74, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics features can be used to predict the informativ"
W13-3508,A00-1043,0,0.100039,"Missing"
W13-3508,D07-1001,0,0.0192623,"rke and Lapata (2006a) used ILP to replace pairwise transitions with trigrams. Other recent work (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010) has used dependency trees directly as sentence representations for compression. Another line of research has attempted to broaden the notion of compression beyond mere word deletion (Cohn and Lapata, 2009; Ganitkevitch et al., 2011; Napoles et al., 2011a). Finally, progress on standalone compression tasks has also enabled document summarization techniques that jointly address sentence selection and compression (Daum´e and Marcu, 2002; Clarke and Lapata, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012), a number of which also rely on ILP-based inference. Monolingual text-to-text generation research also faces many obstacles common to MT. Re5 Conclusion We have presented a supervised discriminative approach to sentence compression that elegantly accounts for two complementary aspects of sentence structure—token ordering and dependency syntax. Our inference formulation permits rich, linguistically-motivated features that factor over the tokens, n-grams and dependencies of the output. Structural integrity is ma"
W13-3508,J10-3005,0,0.0107696,"by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti and Wolsey, 1994) between all pairs of tokens, inspired by recent work in dependency parsing (Martins et al., 2009). Linear token ordering is maintained by defining realadj valued adjacency commodity flow variabl"
W13-3508,W09-1801,0,0.71992,"constraints is described by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti and Wolsey, 1994) between all pairs of tokens, inspired by recent work in dependency parsing (Martins et al., 2009). Linear token ordering is maintained by defining realadj valued adjacen"
W13-3508,P09-1039,0,0.324957,"les are restricted to integer values. A number of highly optimized general-purpose solvers exist for solving ILPs thereby making them tractable for sentencelevel natural language problems in which the number of variables and constraints is described by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to e"
W13-3508,P02-1057,0,0.65009,"Missing"
W13-3508,P08-2007,0,0.016993,"number of highly optimized general-purpose solvers exist for solving ILPs thereby making them tractable for sentencelevel natural language problems in which the number of variables and constraints is described by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti"
W13-3508,P05-1012,0,0.0988603,"Missing"
W13-3508,E06-1038,0,0.925216,"ket and pipe, by his own later admission stunned a party attended by his friends and future Python colleagues by coming out as a homosexual. Compressed: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual. Compression can therefore be viewed as analogous to text summarization1 defined at the sentence level. Unsurprisingly, independent selection of tokens for an output sentence does not lead to fluent or meaningful compressions; thus, compression systems often assemble output text from units that are larger than single tokens such as n-grams (McDonald, 2006; Clarke and Lapata, 2008) or edges in a dependency structure (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010). These systems implicitly rely on a structural representation of text—as a sequence of tokens or as a dependency tree respectively—to to underpin the generation of an output sentence. In this work, we present structured transduction: a novel supervised framework for sentence compression which employs a joint inference strategy to simultaneously recover sentence compressions under both these structural representations of text—a token sequence as well as a tree of syntact"
W13-3508,W08-1105,0,0.420026,"nds and future Python colleagues by coming out as a homosexual. Compressed: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual. Compression can therefore be viewed as analogous to text summarization1 defined at the sentence level. Unsurprisingly, independent selection of tokens for an output sentence does not lead to fluent or meaningful compressions; thus, compression systems often assemble output text from units that are larger than single tokens such as n-grams (McDonald, 2006; Clarke and Lapata, 2008) or edges in a dependency structure (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010). These systems implicitly rely on a structural representation of text—as a sequence of tokens or as a dependency tree respectively—to to underpin the generation of an output sentence. In this work, we present structured transduction: a novel supervised framework for sentence compression which employs a joint inference strategy to simultaneously recover sentence compressions under both these structural representations of text—a token sequence as well as a tree of syntactic dependencies. Sentence generation is treated as a discriminative structured prediction"
W13-3508,W11-1610,0,0.0536604,"Missing"
W13-3508,N10-1131,0,0.611848,"gues by coming out as a homosexual. Compressed: In 1967 Chapman, who had cultivated a conventional image, stunned a party by coming out as a homosexual. Compression can therefore be viewed as analogous to text summarization1 defined at the sentence level. Unsurprisingly, independent selection of tokens for an output sentence does not lead to fluent or meaningful compressions; thus, compression systems often assemble output text from units that are larger than single tokens such as n-grams (McDonald, 2006; Clarke and Lapata, 2008) or edges in a dependency structure (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010). These systems implicitly rely on a structural representation of text—as a sequence of tokens or as a dependency tree respectively—to to underpin the generation of an output sentence. In this work, we present structured transduction: a novel supervised framework for sentence compression which employs a joint inference strategy to simultaneously recover sentence compressions under both these structural representations of text—a token sequence as well as a tree of syntactic dependencies. Sentence generation is treated as a discriminative structured prediction task in which rich linguistically-m"
W13-3508,W11-1611,0,0.244065,"Missing"
W13-3508,W06-1616,0,0.00969871,"ll of the decision variables are restricted to integer values. A number of highly optimized general-purpose solvers exist for solving ILPs thereby making them tractable for sentencelevel natural language problems in which the number of variables and constraints is described by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce ad"
W13-3508,N03-1026,0,0.0682373,"models of language (Rush and Collins, 2011). We are currently exploring similar ideas for compression and other text-to-text generation problems. Related Work An early notion of compression was proposed by Dras (1997) as reluctant sentence paraphrasing under length constraints. Jing and McKeown (2000) analyzed human-generated summaries and identified a heavy reliance on sentence reduction (Jing, 2000). The extraction by Knight and Marcu (2000) of a dataset of natural compression instances from the Ziff-Davis corpus spurred interest in supervised approaches to the task (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Unno et al., 2006; Galley and McKeown, 2007; Nomoto, 2007). In particular, McDonald (2006) expanded on Knight & Marcu’s (2002) transitionbased model by using dynamic programming to recover optimal transition sequences, and Clarke and Lapata (2006a) used ILP to replace pairwise transitions with trigrams. Other recent work (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010) has used dependency trees directly as sentence representations for compression. Another line of research has attempted to broaden the notion of compression beyond mere"
W13-3508,P11-1008,0,0.0308424,"Missing"
W13-3508,C12-2120,1,0.841474,"solvers exist for solving ILPs thereby making them tractable for sentencelevel natural language problems in which the number of variables and constraints is described by a low-order polynomial over the size of the input. ti ∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti and Wolsey, 1994) between all pairs of toke"
W13-3508,P05-1036,0,0.292267,"sh and Collins, 2011). We are currently exploring similar ideas for compression and other text-to-text generation problems. Related Work An early notion of compression was proposed by Dras (1997) as reluctant sentence paraphrasing under length constraints. Jing and McKeown (2000) analyzed human-generated summaries and identified a heavy reliance on sentence reduction (Jing, 2000). The extraction by Knight and Marcu (2000) of a dataset of natural compression instances from the Ziff-Davis corpus spurred interest in supervised approaches to the task (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Unno et al., 2006; Galley and McKeown, 2007; Nomoto, 2007). In particular, McDonald (2006) expanded on Knight & Marcu’s (2002) transitionbased model by using dynamic programming to recover optimal transition sequences, and Clarke and Lapata (2006a) used ILP to replace pairwise transitions with trigrams. Other recent work (Filippova and Strube, 2008; Galanis and Androutsopoulos, 2010) has used dependency trees directly as sentence representations for compression. Another line of research has attempted to broaden the notion of compression beyond mere word deletion (Cohn and Lap"
W13-3508,P06-2109,0,0.419823,"epochs. 3 Experiments In order to evaluate the performance of the structured transduction framework, we ran compression experiments over the newswire (NW) and broadcast news transcription (BN) corpora collected by Clarke and Lapata (2008). Sentences in these datasets are accompanied by gold compressions—one per sentence for NW and three for BN—produced by trained human annotators who were restricted to using word deletion, so paraphrasing and word reordering do not play a role. For this reason, we chose to evaluate the systems using n-gram precision and recall (among other metrics), following Unno et al. (2006) and standard MT evaluations. We filtered the corpora to eliminate instances with less than 2 and more than 110 tokens and used 3.1 Results Table 1 summarizes the results from compression experiments in which the target compression rate is set to the average gold compression rate for each instance. We observe a significant gain for the joint structured transduction system over the Clarke and Lapata (2008) approach for n-gram F1 . Since n-gram metrics do not distinguish between content words and function words, we also include an evaluation metric that observes the precision, recall and F-measu"
W13-3508,D12-1022,0,0.128995,"∈T + X hti ,tj ,tk i∈U + X hti ,tj i∈V Constrained ILP Formulation yijk · θngr (i, j, k) zij · θdep (i, j) = arg max x&gt; θ tok + y&gt; θ ngr + z&gt; θ dep (2) x,y,z 67 2.2.2 Recent years have seen ILP applied to many structured NLP applications including dependency parsing (Riedel and Clarke, 2006; Martins et al., 2009), text alignment (DeNero and Klein, 2008; Chang et al., 2010; Thadani et al., 2012) and many previous approaches to sentence and document compression (Clarke and Lapata, 2008; Filippova and Strube, 2008; Martins and Smith, 2009; Clarke and Lapata, 2010; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012). A key challenge for structured transduction models lies in ensuring that output token sequences and dependency trees are well formed. This requires that output structures are fully connected and that cycles are avoided. In order to accomplish this, we introduce additional variables to establish singlecommodity flow (Magnanti and Wolsey, 1994) between all pairs of tokens, inspired by recent work in dependency parsing (Martins et al., 2009). Linear token ordering is maintained by defining realadj valued adjacency commodity flow variables γij which must be non-zero whenever tj directly follows"
W13-3508,W02-1001,0,\N,Missing
W15-4612,P14-1048,0,0.0519694,"sequence x. The weights θk are estimated using gradient descent to maximize the likelihood of the input. In our formulation, each x is a PDTB document, consisting of a sequence of sentences (for the intra-sentence relation tagger) or a sequence of sentence pairs (for the adjacent sentence relation tagger). y consists of all type-level discourse relation categories. In our experiments, we used a maximum likelihood prior and limited the gradient descent to a maximum of 200 epochs instead of waiting for it to converge. While CRFs have been used in the past for subtasks of RST discourse parsing (Feng and Hirst, 2014) and for finding the arguments of explicit relations in PDTB (Ghosh et al., 2011), no sequential approaches have ever been used in a way that models the sequential dependency between PDTB relations. Previous work (Pitler et al., 2009; Zhou Features 4.1 Intra-sentence tagger The intra-sentence tagger deals only with explicit relations, and as such focuses on features related to discourse connectives. We use Pitler and Nenkova (2009)’s connective classifier to identify discourse connectives within the sentence, and for each connective generate the following binary features: • Connective • Previo"
W15-4612,I11-1120,0,0.0412543,"kelihood of the input. In our formulation, each x is a PDTB document, consisting of a sequence of sentences (for the intra-sentence relation tagger) or a sequence of sentence pairs (for the adjacent sentence relation tagger). y consists of all type-level discourse relation categories. In our experiments, we used a maximum likelihood prior and limited the gradient descent to a maximum of 200 epochs instead of waiting for it to converge. While CRFs have been used in the past for subtasks of RST discourse parsing (Feng and Hirst, 2014) and for finding the arguments of explicit relations in PDTB (Ghosh et al., 2011), no sequential approaches have ever been used in a way that models the sequential dependency between PDTB relations. Previous work (Pitler et al., 2009; Zhou Features 4.1 Intra-sentence tagger The intra-sentence tagger deals only with explicit relations, and as such focuses on features related to discourse connectives. We use Pitler and Nenkova (2009)’s connective classifier to identify discourse connectives within the sentence, and for each connective generate the following binary features: • Connective • Previous word + connective • Connective + next word • Connective’s syntactic category •"
W15-4612,P03-1054,0,0.0306984,"ity. Centrality in document is the cosine similarity of the sentence pair to the document as a whole. The intuition is that certain relations (e.g., argumentative relations such as causality and concession) would tend to be more common around the main topic of the document. Finally, we include features for words that are shared by both sentences called expanded shared words - expanded because we use WordNet (Fellbaum, 1998) to expand the usual list of words in Connective features Syntactic features Syntactic features are derived from the parse tree of the sentence. We use the Stanford Parser (Klein and Manning, 2003) to derive the trees. Unlike much previous work, we do not use the gold parse trees of the PTB. Lin et al. (2009) introduced the production rule features, which are some of the strongest for implicit relation disambiguation. Production rules are all parent-children relations in the constituent parse of a sentence, e.g. [VP → NP PP NP]. The binary feature formulation includes the existence of each rule in arg1, in arg2, and in both. Li and Nenkova (2014b) hypothesized that production rules are too sparse, and found that using their production stick features achieved higher performance. Unlike a"
W15-4612,W14-4320,0,0.472579,"s in text, and 96 Proceedings of the SIGDIAL 2015 Conference, pages 96–104, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics ments of a relation. Identifying and disambiguating implicit relations has been the hardest task to achieve good performance at, and is an active area of research. Pitler et al. (2009) were the first to identify implicit relations in the PDTB in a realistic setting, and later work has improved on their methods as well as introducing new ideas (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Li and Nenkova, 2014a). Most recently, Lin et al. (2014) have introduced and evaluated the first system which provides end-to-end discourse parsing over PDTB (the Lin parser). In their work, they have combined much of the earlier work on specific subtasks, utilizing a connective disambiguation component and an explicit relation disambiguation component inspired by Pitler and Nenkova (2009)’s method, as well as an implicit relation disambiguation component descending from their own previous work (Lin et al., 2009). Their approach is to decipher the document in a structured way, in four steps: first, identify expli"
W15-4612,P13-2013,1,0.91914,"not marked by connectives in text, and 96 Proceedings of the SIGDIAL 2015 Conference, pages 96–104, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics ments of a relation. Identifying and disambiguating implicit relations has been the hardest task to achieve good performance at, and is an active area of research. Pitler et al. (2009) were the first to identify implicit relations in the PDTB in a realistic setting, and later work has improved on their methods as well as introducing new ideas (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Li and Nenkova, 2014a). Most recently, Lin et al. (2014) have introduced and evaluated the first system which provides end-to-end discourse parsing over PDTB (the Lin parser). In their work, they have combined much of the earlier work on specific subtasks, utilizing a connective disambiguation component and an explicit relation disambiguation component inspired by Pitler and Nenkova (2009)’s method, as well as an implicit relation disambiguation component descending from their own previous work (Lin et al., 2009). Their approach is to decipher the document in a structured way, in four steps:"
W15-4612,W14-4327,0,0.449615,"s in text, and 96 Proceedings of the SIGDIAL 2015 Conference, pages 96–104, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics ments of a relation. Identifying and disambiguating implicit relations has been the hardest task to achieve good performance at, and is an active area of research. Pitler et al. (2009) were the first to identify implicit relations in the PDTB in a realistic setting, and later work has improved on their methods as well as introducing new ideas (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Li and Nenkova, 2014a). Most recently, Lin et al. (2014) have introduced and evaluated the first system which provides end-to-end discourse parsing over PDTB (the Lin parser). In their work, they have combined much of the earlier work on specific subtasks, utilizing a connective disambiguation component and an explicit relation disambiguation component inspired by Pitler and Nenkova (2009)’s method, as well as an implicit relation disambiguation component descending from their own previous work (Lin et al., 2009). Their approach is to decipher the document in a structured way, in four steps: first, identify expli"
W15-4612,W01-1605,0,0.13327,"n answering) focus on sentences or clauses anyway and would find this formulation natural. In this paper, we present a simple yet powerful sequential approach to PDTB discourse parsing, utilizing two CRFs and features that are designed to discriminate both explicit and implicit relations. We surpass state-of-the-art performance with a simpler structure, less hand-crafted rules for special scenarios and with an approach that makes adding new features extremely easy. 2 Related Work Early data-driven work on discourse has focused on frameworks such as RST, using the small RST Discourse Treebank (Carlson et al., 2001). Marcu (1997) and later Soricut and Marcu (2003) developed methods for parsing documents into the RST discourse representation. There has also been more recent work on end-to-end RST-style parsing (LeThanh et al., 2004; duVerle and Prendinger, 2009). Recently, there has been more focus on the PDTB (Prasad et al., 2008), the largest annotated discourse corpus currently in existence. Most work so far has focused on solving specific subtasks of the overall parsing task. Pitler and Nenkova (2009) focused on explicit relations and found that they are relatively easy to disambiguate using syntactic"
W15-4612,D09-1036,0,0.210793,"Missing"
W15-4612,P09-1075,0,0.0505948,"criminate both explicit and implicit relations. We surpass state-of-the-art performance with a simpler structure, less hand-crafted rules for special scenarios and with an approach that makes adding new features extremely easy. 2 Related Work Early data-driven work on discourse has focused on frameworks such as RST, using the small RST Discourse Treebank (Carlson et al., 2001). Marcu (1997) and later Soricut and Marcu (2003) developed methods for parsing documents into the RST discourse representation. There has also been more recent work on end-to-end RST-style parsing (LeThanh et al., 2004; duVerle and Prendinger, 2009). Recently, there has been more focus on the PDTB (Prasad et al., 2008), the largest annotated discourse corpus currently in existence. Most work so far has focused on solving specific subtasks of the overall parsing task. Pitler and Nenkova (2009) focused on explicit relations and found that they are relatively easy to disambiguate using syntactic features. Wellner (2009) used both lexical and syntactic features to identify the argu1 There is also a fifth step, identifying spans that attribute a statement to a source, e.g. “B.P. explains that ...”. Attribution span detection is a secondary ta"
W15-4612,C10-2172,0,0.0807959,"se at all. In addition, many relations are not marked by connectives in text, and 96 Proceedings of the SIGDIAL 2015 Conference, pages 96–104, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics ments of a relation. Identifying and disambiguating implicit relations has been the hardest task to achieve good performance at, and is an active area of research. Pitler et al. (2009) were the first to identify implicit relations in the PDTB in a realistic setting, and later work has improved on their methods as well as introducing new ideas (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Li and Nenkova, 2014a). Most recently, Lin et al. (2014) have introduced and evaluated the first system which provides end-to-end discourse parsing over PDTB (the Lin parser). In their work, they have combined much of the earlier work on specific subtasks, utilizing a connective disambiguation component and an explicit relation disambiguation component inspired by Pitler and Nenkova (2009)’s method, as well as an implicit relation disambiguation component descending from their own previous work (Lin et al., 2009). Their approach is to decipher"
W15-4612,P02-1047,0,0.113214,"r results show that treating the task as sequential is useful. One interesting direction for continuing this research is to transform the two tagging tasks into two joint prediction tasks, and perhaps eventually into one joint prediction task. While we build on previous work in defining our features, we also introduced some novel variations. We have defined the production angles family of features, which are related to the production rules of Lin et al. (2009) and the production sticks of Li and Nenkova (2014b). We also contribute to the word pair features line of research, which started with Marcu and Echihabi (2002) and has been part of most work on implicit relation disambiguation since, with our variations on the dense word pair similarity features introduced by Biran and McKeown (2013). Our expanded shared words features are also novel. Our main aim in this paper was to show that experiments with discourse parsing can be done fairly easily using one of the many freely available sequential models. We hope that this method will make the task more accessible to researchers and help in moving towards a fully statistical and holistic approach to discourse parsing. The parser described in this paper is publ"
W15-4612,J93-2004,0,0.0487761,"meworks allow more for certain relations) do not necessarily correspond to sentences or clauses, and may not even be contiguous under some theories. Over the years, multiple theories of discourse have been proposed. Most recently, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has been introduced, featuring hierarchical relation categories which generalize over other theories such as Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003), as well as a relatively large annotated corpus aligned with the WSJ section of the Penn Treebank (PTB) (Marcus et al., 1993). While the relation categories in PDTB are hierarchical, unlike RST and other frameworks, the discourse structure of a PDTB document is not fully hierarchical so that documents in general do not have a tree-like discourse structure. This is a crucial detail which allows our proposed method to work on PDTB documents. While there has been much work recently on disambiguating discourse relations in the PDTB, most have not been full parsing systems. That is, they operate in an experimental environment where some information is given (for example, some systems disambiguate only implicit relations,"
W15-4612,W03-0430,0,0.0656285,"ntence, and with our approach we can at most tag one correctly. Because of these two limitations, we have an upper bound on Fmeasure performance of 89.4 in the PDTB corpus. Since current state-of-the-art performance is far below this level, we do not view this as an urgent problem. At any rate, additional specialized approaches can be added to correctly handle those rare cases. In this paper, we use Conditional Random Fields (CRFs) for both taggers. CRFs were first introduced by Lafferty et al. (2001) and have been successfully used for many NLP tagging tasks such as named entity recognition (McCallum and Li, 2003) and shallow parsing (Sha and Pereira, 2003). We use simple linear-chain CRFs for both taggers. In the linear-chain CRF model, the posterior probabilities for an ordered sequence input x = {x1 , . . . , x|x |} of tag labels y = {y1 , . . . , y|x |} are defined as P (y|x) ∝ |x| Y i=1 exp K X Contingency Expansion Temporal Type (Level 2) Concession Contrast Pragmatic Concession Pragmatic Contrast Cause Condition Pragmatic Cause Pragmatic Condition Alternative Conjunction Exception Instantiation List Restatement Asynchronous Synchrony Table 1: The PTDB relation category hierarchy, with level 1 cl"
W15-4612,W12-1614,0,0.746901,"rarchical so that documents in general do not have a tree-like discourse structure. This is a crucial detail which allows our proposed method to work on PDTB documents. While there has been much work recently on disambiguating discourse relations in the PDTB, most have not been full parsing systems. That is, they operate in an experimental environment where some information is given (for example, some systems disambiguate only implicit relations, where it is assumed that the arguments of the relation have been identified and that the relation is known to be implicit (Pitler and Nenkova, 2009; Park and Cardie, 2012)). Full systems, in contrast, operate on unannotated text documents producing the full discourse structure of the text, including both implicit and explicit relations, and so can be realistically used in NLP applications. Although not strictly parsing in the case of PTDB, such systems perform what has been called the end-to-end Full discourse parsing in the PDTB framework is a task that has only recently been attempted. We present the Two Taggers approach, which reformulates the discourse parsing task as two simpler tagging tasks: identifying the relation within each sentence, and identifying"
W15-4612,P09-2004,0,0.346799,"document is not fully hierarchical so that documents in general do not have a tree-like discourse structure. This is a crucial detail which allows our proposed method to work on PDTB documents. While there has been much work recently on disambiguating discourse relations in the PDTB, most have not been full parsing systems. That is, they operate in an experimental environment where some information is given (for example, some systems disambiguate only implicit relations, where it is assumed that the arguments of the relation have been identified and that the relation is known to be implicit (Pitler and Nenkova, 2009; Park and Cardie, 2012)). Full systems, in contrast, operate on unannotated text documents producing the full discourse structure of the text, including both implicit and explicit relations, and so can be realistically used in NLP applications. Although not strictly parsing in the case of PTDB, such systems perform what has been called the end-to-end Full discourse parsing in the PDTB framework is a task that has only recently been attempted. We present the Two Taggers approach, which reformulates the discourse parsing task as two simpler tagging tasks: identifying the relation within each se"
W15-4612,P09-1077,0,0.586178,"urse connectives or markers, such as “because” and “but”, these are often ambiguous: they may apply to more than one relation category, or they may be used in a way that has nothing to do with discourse at all. In addition, many relations are not marked by connectives in text, and 96 Proceedings of the SIGDIAL 2015 Conference, pages 96–104, c Prague, Czech Republic, 2-4 September 2015. 2015 Association for Computational Linguistics ments of a relation. Identifying and disambiguating implicit relations has been the hardest task to achieve good performance at, and is an active area of research. Pitler et al. (2009) were the first to identify implicit relations in the PDTB in a realistic setting, and later work has improved on their methods as well as introducing new ideas (Lin et al., 2009; Zhou et al., 2010; Park and Cardie, 2012; Biran and McKeown, 2013; Li and Nenkova, 2014a). Most recently, Lin et al. (2014) have introduced and evaluated the first system which provides end-to-end discourse parsing over PDTB (the Lin parser). In their work, they have combined much of the earlier work on specific subtasks, utilizing a connective disambiguation component and an explicit relation disambiguation componen"
W15-4612,prasad-etal-2008-penn,0,0.8126,"Approach Or Biran Columbia University orb@cs.columbia.edu Kathleen McKeown Columbia University kathy@cs.columbia.edu Abstract disambiguating these implicit relations is difficult even when it is known a relation exists. Adding to the difficulty is the fact that the arguments of the relation (there are usually two, although some frameworks allow more for certain relations) do not necessarily correspond to sentences or clauses, and may not even be contiguous under some theories. Over the years, multiple theories of discourse have been proposed. Most recently, the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has been introduced, featuring hierarchical relation categories which generalize over other theories such as Rhetorical Structure Theory (RST) (Mann and Thompson, 1987) and SDRT (Asher and Lascarides, 2003), as well as a relatively large annotated corpus aligned with the WSJ section of the Penn Treebank (PTB) (Marcus et al., 1993). While the relation categories in PDTB are hierarchical, unlike RST and other frameworks, the discourse structure of a PDTB document is not fully hierarchical so that documents in general do not have a tree-like discourse structure. This is a crucial detail which al"
W15-4612,N03-1028,0,0.156376,"tag one correctly. Because of these two limitations, we have an upper bound on Fmeasure performance of 89.4 in the PDTB corpus. Since current state-of-the-art performance is far below this level, we do not view this as an urgent problem. At any rate, additional specialized approaches can be added to correctly handle those rare cases. In this paper, we use Conditional Random Fields (CRFs) for both taggers. CRFs were first introduced by Lafferty et al. (2001) and have been successfully used for many NLP tagging tasks such as named entity recognition (McCallum and Li, 2003) and shallow parsing (Sha and Pereira, 2003). We use simple linear-chain CRFs for both taggers. In the linear-chain CRF model, the posterior probabilities for an ordered sequence input x = {x1 , . . . , x|x |} of tag labels y = {y1 , . . . , y|x |} are defined as P (y|x) ∝ |x| Y i=1 exp K X Contingency Expansion Temporal Type (Level 2) Concession Contrast Pragmatic Concession Pragmatic Contrast Cause Condition Pragmatic Cause Pragmatic Condition Alternative Conjunction Exception Instantiation List Restatement Asynchronous Synchrony Table 1: The PTDB relation category hierarchy, with level 1 classes and level 2 types. The level 3 subtype"
W15-4612,N03-1030,0,0.141537,"way and would find this formulation natural. In this paper, we present a simple yet powerful sequential approach to PDTB discourse parsing, utilizing two CRFs and features that are designed to discriminate both explicit and implicit relations. We surpass state-of-the-art performance with a simpler structure, less hand-crafted rules for special scenarios and with an approach that makes adding new features extremely easy. 2 Related Work Early data-driven work on discourse has focused on frameworks such as RST, using the small RST Discourse Treebank (Carlson et al., 2001). Marcu (1997) and later Soricut and Marcu (2003) developed methods for parsing documents into the RST discourse representation. There has also been more recent work on end-to-end RST-style parsing (LeThanh et al., 2004; duVerle and Prendinger, 2009). Recently, there has been more focus on the PDTB (Prasad et al., 2008), the largest annotated discourse corpus currently in existence. Most work so far has focused on solving specific subtasks of the overall parsing task. Pitler and Nenkova (2009) focused on explicit relations and found that they are relatively easy to disambiguate using syntactic features. Wellner (2009) used both lexical and s"
W15-4612,C04-1048,0,\N,Missing
W19-8672,P16-2008,0,0.323474,"Missing"
W19-8672,W18-6556,0,0.0829952,"simple encoder-decoder models with greedy decoding are capable of generating semantically correct utterances that are as good as state-of-the-art outputs in both automatic and human evaluations of quality. 1 Training Reference Utterance Near The Six Bells is a venue that is children friendly named The Golden Curry. Figure 1: Example MR for the Inform DA with example human reference utterance. must be capable of correctly generating utterances for novel MRs at test time. In practice, even with delexicalization (Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018), copy and coverage mechanisms (Elder et al., 2018), and overgeneration plus reranking (Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018), DNN generators still produce errors (Duˇsek et al., 2019). In this work, rather than develop more sophisticated DNN architectures or ensembles, we explore the use of simpler DNNs with self-training. We train a bare-bones unidirectional neural encoderdecoder with attention (Bahdanau et al., 2014) as our base model from which we sample novel utterances for MRs not seen in the original training data. We obtain a diverse collection of samples using noise injection sampling (Cho, 2016). Using an MR parser, we"
W19-8672,N18-1014,0,0.21702,"markably, after training on the augmented data, even simple encoder-decoder models with greedy decoding are capable of generating semantically correct utterances that are as good as state-of-the-art outputs in both automatic and human evaluations of quality. 1 Training Reference Utterance Near The Six Bells is a venue that is children friendly named The Golden Curry. Figure 1: Example MR for the Inform DA with example human reference utterance. must be capable of correctly generating utterances for novel MRs at test time. In practice, even with delexicalization (Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018), copy and coverage mechanisms (Elder et al., 2018), and overgeneration plus reranking (Duˇsek and Jurˇc´ıcˇ ek, 2016; Juraska et al., 2018), DNN generators still produce errors (Duˇsek et al., 2019). In this work, rather than develop more sophisticated DNN architectures or ensembles, we explore the use of simpler DNNs with self-training. We train a bare-bones unidirectional neural encoderdecoder with attention (Bahdanau et al., 2014) as our base model from which we sample novel utterances for MRs not seen in the original training data. We obtain a diverse collection of samples using noise inj"
W19-8672,D14-1181,0,0.00336905,"dding the survivors to A. Classifier-based parser qφ It is perhaps too optimistic to believe we can construct reasonable rules in all cases. Rule creation quickly becomes tedious and for more complex MRs this would become a bottleneck. To address these concerns, we also study the feasibility of using learned classifiers to predict the presence and value of the attributes. For each attribute in the E2E dataset, we trained a separate convolutional neural network (CNN) classifier to predict the correct attribute value (or n/a if the attribute is not present). The CNN architecture follows that of Kim (2014) and is trained with gradient descent on the original training data. See Appendix C for full architecture and training details. The average E2E validation F-score is 0.94. 5 3. Train a new generator p1 on the combined dataset D ∪ A. Steps 1 and 3 are identical, the generators p0 and p1 have the same architecture and training setup, ony the dataset, D vs. D ∪ A, is different. We now discuss step 2 in detail. Step 2: E2E Dataset To sample a novel MR with S attributes, we sample a combination of S − 1 attributes uniformly at random (always appending the name attribute since every MR contains it)."
W19-8672,W18-6557,0,0.0120796,"We use a batch size of 128, with a learning rate of 0.25, weight decay penalty of 0.0001, and a dropout probability of 0.25. We select the best model iteration using validation set BLEU score6 . Using the self-training method outlined in section 5, we create augmented datasets using either qR or qφ , which we refer to as AqR and Aqφ respectively (qφ is only in the delexicalized setting). For both D∪AqR and D∪Aqφ we train new generators p1 using the same training setting as above (although we terminate training after 50 epochs because the models converge much faster with the additional data). (Puzikov and Gurevych, 2018), as determined during the shared task evaluation (Duˇsek et al., 2019). Surprisingly, p0 using greedy decoding surpases all of the baseline systems. This is quite shocking as the Slug model ensembles three different sequence-to-sequence models producing 10 outputs each using beam search and reranking based on slot alignment to select the final generation output. The p1 /qR model remains competitive with Slug, again even using greedy decoding. The p1 /qφ starts underperforming Slug on BLEU score but remains competitive on ROUGE-L and METEOR again when using greedy decoding. Overall the augment"
W19-8672,N19-1410,0,0.0351551,"ing, or other specialized attention mechanisms or loss functions to obtain low error rates. Instead we use data-augmentation to obtain a more reliable but simpler model. Data augmentation has also been used by prior neural generation models. Juraska et al. (2018) breaks multi-sentence utterances into separate training instances. They also try training on more complex sentences alone but this model was less reliably able to realize all attributes correctly. They also do not generate new utterance/MR pairs for training as we do. Our method is in some ways similar to the reconstructor setting of Shen et al. (2019), where a base speaker model S0 produces utterances and a listener model L reconstructs the input MR. In the 1.77 1.76 1.69 Table 8: E2E augmented dataset statistics: total utterances, words per utterance, sentences per utterance, and mean D-Level score. Table 8 shows the statistics for the E2E test set outputs. In the lexicalized setting, the mean D-level results support our hypothesis; syntactic complexity of test set outputs decreases from p0 to p1 . In the delexicalized setting this is somewhat true; three of the p1 models have lower mean D-level scores than p0 with greedy decoding. Curiou"
W19-8672,N16-1015,0,0.0610786,"Missing"
W19-8672,D15-1199,0,0.12077,"Missing"
W19-8672,N16-1086,0,\N,Missing
W97-0210,J93-2001,0,0.0299717,"Missing"
W97-0210,J92-4003,0,0.0087093,"Missing"
W97-0210,C94-1042,0,0.2653,"ourse structure (Morris and Hirst, 2 [] m Exploiting domain-independent syntactic clues A given word may have n distinct senses and appear within m different syntactic contexts, but typically, not all n x m combinations are valid. The syntactic context can partly disambiguate the semantic content. For example, when the verb question has a that-clause complement, it cannot have the sense of &quot;ask&quot;, but rather must have the sense of &quot;challenge&quot;. To identify such interacting syntactic and semantic constraints at the lexical level, we utilize three knowledge bases for verbs: * The COMLEX database (Grishman et al., 1994; Macleod and Grishman, 1995), which includes detailed subcategorization information for each verb, and some adjectives and nouns. 58 m m m m m • Levin's classification of verbs in terms of their allowed alternations (Levin, 1993). Alternations include syntactic transformations such as thereinsertion (e.g., A ship appeared on the horizon ---, There appeared a ship on the horizon) and locative-inversion (e.g., --* On the horizon there appeared a ship). Much in the same way as subcategorization frames, alternations are constrained by the sense of the word; for example, the verb appear allows the"
W97-0210,P93-1023,1,0.822265,"irect route, it allows the automatic identification of the predominant sense of a word in a given text or subject topic. It is indirect because the actual result is groups of word forms, but we presume each group to represent a relatively homogeneous semantic class. Thus we presume that the relevant sense of a given word form in a group is in the same lexical field as the senses of the other word forms in the same group. The process is highly domain-dependent, i.e., the same set of words will be partitioned in different ways when the domain changes. For example, when our word grouping system (Hatzivassiloglou and McKeown, 1993) classified about 280 frequent adjectives in stock market reports, it formed, among others, the cluster {common, preferred}. This cluster would look odd were not the domain considered. ~ This information on predominant senses for each word form in a given corpus can be computed automatically, but remains implicit. To map the results onto word sense associations, and thus explicitly identify the predominant senses, we utilize the links between senses provided by WordNet. We note that while words like question and ask are ultimately connected in WordNet, the actual connections are only between s"
W97-0210,P94-1002,0,0.0217563,"Missing"
W97-0210,P96-1038,0,0.0135642,"ld exclude &quot;much of the English language&quot; by narrowing the lexicon, verb tense and aspect, and syntactic complexity. Such observations inform the increasing trend towards analysis of homogeneous corpora to identify linguistic constraints for use in systems intended to understand or generate coherent discourse. Recent work in this vein includes identification of lexical constraints from textual tutorial dialogue (Moser and Moore, 1995), constraints on illocutionary act type from spoken task-oriented dialogue (Allen et al., 1995), prosodic constraints from spoken information-seeking monologues (Hirschberg and Nakatani, 1996), and constraints on referring expressions from spoken narrative monologue (Passonneau, 1996). Related work suggests that constraints of different types are interdependent (Biber, 1993; Passonneau and Litman, forthcoming), hence should be investigated together. Our ultimate goal is to develop methods to tag lexical semantic features in discourse corpora in order to enhance extraction of constraints of the sort just listed. Two types of investigations that would undoubtedly be enhanced are explorations of the interrelation of lexical cohesion and global discourse structure (Morris and Hirst, 2"
W97-0210,C96-1080,0,0.0148988,"ed specifications of the syntactic frames for each verb (92 distinct subcategorization types). The allowed alternations (which we encoded in machine-readable form from the detailed rules supplied in (Levin, 1993)) provide additional constraints. Mapping the more precise syntactic information in COMLEX to the verb frames of WordNet allows the construction of a more detailed syntactic entry for each word sense, and enables the association of alternation constraints with the senses in WordNet. In the future, it will also allow us to use corpora tagged with COMLEX subcategorization frames, e.g., (Macleod et al., 1996). We have manually constructed a table that maps WordNet syntactic constraints to the ones used in COMLEX (and vice versa) and another that maps allowed alternations from (Levin, 1993) to COMLEX or WordNet syntactic frames. A program consuits the three databases and the mapping tables and, for each word occurrence constructs a list of the senses that are compatible with the syntactic constraints. During this process, a detailed entry for the word is formed, containing both syntactic and semantic information. The resulting entries comprise a rich lexical resource that we plan to use for text ge"
W97-0210,J96-2003,0,0.0182327,"we find the following links between question and the verbs ask, inquire, chal. tion, which lenge, and dispute: (question1, asks), (questiou~, asks), (questions, asks), (questions, inquire~), and (question1, challenge~). Thus, if question is placed in the same semantic group with ask and inquire, the three senses {1, 2, 3} survive out of the five senses of question, with a preference for sense 3. If, on the other hand, question is classified with challenge and dispute, only sense 1 survives. We performed an experiment analyzing a specific verb group produced by one semantic clustering program (McMahon and Smith, 1996). This group contains 19 verbs, all but one of them ambiguous, including ask, call, charge, regard, say, and wish. We measured for each sense of the 19 words how many of the other words have at least one sense linked with that sense in WordNet (in the same toplevel verb sense tree). The results, part of which is shown in Table 2, indicate that some senses are much more strongly connected with the other words in the group, and so probably predominate in the corpus that was used to induce the group. For example, one of the senses of ask, &quot;require&quot; (as in This job asks (for) long hours) is not li"
W97-0210,H93-1061,0,0.0774281,"es with pruned word sense tags using the Brown corpus. The first step of the method is to identify the subcategorization pattern for a specific verb token. Here we rely on heuristics to identify the major constituents to the left and right of a verb token, as described in (Jing et al., 1997). After hypothesizing the subcategorization pattern for a specific verb token, we use our sense restriction matrices (as in Table 1) to tag the verb token with a pruned set of senses. W e evaluate the resulting sense tag against the version of the Brown corpus that has been hand-tagged with WordNet senses (Miller et al., 1993). For appear, which we use as an example throughout this paper, we find 100 tokens in the Brown corpus. Of these, 46 are intransitive or have a locative prepositional phrase complement. Our method tags each of these tokens with two or three possible senses, and in all but one case, the sense tag includes the valid sense. Another 31 tokens are followed by to and a subject-controlled infinitive. In all these cases, our method makes a single, correct prediction out of the eight possible senses. For all 100 uses of appear in the corpus, the average number of possible senses predicted by our method"
W97-0210,J91-1002,0,0.0133805,"Missing"
W97-0210,P93-1024,0,0.0309831,"Missing"
W97-0210,J93-2005,0,0.0286451,"f the large number of alternatives and the likely closeness in meaning among them. Selecting a subset of almost synonymous verb senses is significantly harder than, for example, disambiguating bank between the &quot;edge of river&quot; and '~financial institution&quot; senses. z~ool = l[] ,ooo. . u ,~ m 3 SO0. lhlm----. 1'0 ~) 30 Using domain-dependent semantic classifications to identify predomi n a n t senses The process outlined above has two significant advantages: first, it can be automatically applied, assuming a robust method for parsing the relevant verb phrase context (the experiments presented in (Pustejovsky et al., 1993) depend on the same type of information). Second, it reduces the ambiguity of a given word without sacrificing accuracy, insofar as the three input knowledge sources are accurate. To further restrict the size of the set of valid senses produced, we are currently exploring domaindependent, automatically constructed semantic classifications. Semantic classification programs (Brown et al., 1992; Hatzivassiloglou and McKeown, 1993; Pereira et al., 1993) use statistical information based on cooccurrence with appropriate marker words to partition a set of words into semantic groups or classes. 41 Nu"
W97-0210,H93-1052,0,0.0676495,"Missing"
W97-0210,P95-1018,0,\N,Missing
W97-0210,H92-1045,0,\N,Missing
W97-0903,P88-1000,0,0.0679493,"th domain-specific knowledge that is not explicitly present in the input. [n FLOWDoC and ZEDDoc, semantic enrichment is done at various stages by consulting external ontologies. • D i s c o u r s e Organizer: The discourse organizer performs all the remaining functions prior to lexicalization and surface generation2. Three sub-modules apply general discourse coherence constraints at the levels of discourse, sentence, and sentence constituent. The first module performs aggregation and text linearization operations using an ontology of rhetorical predicates derived from Hobbs (1985) and Polanyi (1988). Linear order and prominence of the subconstituents are then determined, followed by constraints on subconstituents that affect lexical choice (e.g., centering and informational constraints, as in (Passonneau, 1996)). 2|n previous work we referred to this module as the Sentence Planner (Passonneau et al., 1996). A Common Representation All three systems employ a consistent, standardized attribute-value data format that persists from each module to the next. Examples of this internal data format were shown in Figures 1-3. This fbrmat is used for representing and processing conceptualsemantic,"
W97-0903,W94-0319,0,0.0301262,"of P L A N D o c &apos; s architecture as possible, often adapting and generalizing modules that were originally written with only the P L A N D o c system in mind. All three systems employ a modular pipeline architecture. A pipeline architecture is one that separates the functions involved in text generation, such as content planning, discourse organization, lexicalization, and syntactic realization, into distinct modules that operate in sequence. Modular pipeline architectures have a long history of use in text gen16 eration systems (Kukich, 1983a; McKeown, 1985; McDonald and Pustejovsky, 1986; Reiter, 1994), although recent work argues for the need for interaction between modules (Danlos, 1987; Rubinoff, 1992; McKeown et al., 1993). The most powerful argument for using pipeline architectures is the potential benefit of re-using individual modules for subsequent applications. However, with the exception of surface realization modules such as F U F / S U R G E (Elhadad, 1992; Robin, 1994), actual code re-use has been minimal due to the lack of agreement about the order and grouping of subprocesses into modules. In P L A N D o c , FLowDoc, and ZEDDoc, we utilize the following main modules, in the o"
W97-0903,P93-1031,1,0.461759,"th only the P L A N D o c system in mind. All three systems employ a modular pipeline architecture. A pipeline architecture is one that separates the functions involved in text generation, such as content planning, discourse organization, lexicalization, and syntactic realization, into distinct modules that operate in sequence. Modular pipeline architectures have a long history of use in text gen16 eration systems (Kukich, 1983a; McKeown, 1985; McDonald and Pustejovsky, 1986; Reiter, 1994), although recent work argues for the need for interaction between modules (Danlos, 1987; Rubinoff, 1992; McKeown et al., 1993). The most powerful argument for using pipeline architectures is the potential benefit of re-using individual modules for subsequent applications. However, with the exception of surface realization modules such as F U F / S U R G E (Elhadad, 1992; Robin, 1994), actual code re-use has been minimal due to the lack of agreement about the order and grouping of subprocesses into modules. In P L A N D o c , FLowDoc, and ZEDDoc, we utilize the following main modules, in the order listed below: • M e s s a g e G e n e r a t o r : The message generator transcribes the raw data from LEIS-PLAN execution"
W97-0903,A94-1002,1,0.829776,"t generation applications. At the same time, to take full advantage of these opportunities, text generation systems must be easily adaptable to new domains, changing data formats, and distinct underlying ontologies. One crucial factor contributing to the generalization and subsequent practical and commercial viability of text generation systems is the adaptation and re-use of text generation modules and the development of re-usable tools and techniques. In this paper, we focus on the lessons learned during the successive development of three text generation systems at Bellcore: P L A N D o c (McKeown et al., 1994) summarizes execution traces of an expert system for telephone network capacity expansion analysis; FLOwDoc (Passonneau et al., 1996) provides summaries of the most important events in flow diagrams constructed during business reengineering; and Z E D D o c (Passonnean et al., 1997) produces summaries of activity for a user-specified set of advertisements within a user-specified time period from logs of WWW page hits. We built FLowDoc and Z E D D o c by adapting components of the P L A N D o c system. The transfer of the original P L A N D o c modules to new domains led to the replacement of s"
W97-0903,P95-1053,0,0.465208,"icantly adapted our P L A N DOC architecture for use in FLOwDOC, but we were able to re-use the F L o w D o c architecture and much of its code in Z E D D o c . Figure 4 contrasts the architecture of P L A N D o c with those of F L O w D o c and Z E D D o c . In fact, the functions of the Lexicalization and Surface Generation modules remained constant across all three systems. But the functions of the first three modules shifted significantly from P L A N D o c to FLOwDOC. In particular, the function of message aggregation lay exclusively in the Discourse Organization module in P L A N D o c (Shaw, 1995), whereas aggregation functions are executed in both the Message Generation and Discourse Organization modules in FLOWDOC. Because the development of domain-independent, plug-and-play ontology modules is one of the major features that affected these shifts in function, and because such modules greatly increase the portability of the system, we devote the next section to a more detailed description of the function of ontological generalization. 6 (a) Overall architecture for P L A N D o c . Ontological Generalization Ontological generalization refers to the problem of composing, with the help o"
W97-1204,C86-1141,0,0.0353483,"S system based on the new architecture. Currently, only limited semantic, syntactic and prosodic features are covered i n o u r prototype system. 2 1 Related Work Recently, people have become more interested in developing CTS algorithms to improve the quality of synthesized speech. In (Prevost, 1995) and (Steedman, 1996), theme, rheme and contrast are used as important knowledge sources in determining accentual patterns. In (Davis and Hirschberg, 1988), given/new and topic structure are used to control intonational variation. Other CTS related research includes (Young and Fallside, 1979) and (Danlos et al., 1986). Most of the CTS systems developed to date have a closely integrated architecture. Because of this, CTS algorithms which map information from NLG to TTS parameters are system dependent. There is some related research in developing markup languages for TTS and speech transcription. The Speech Synthesis Markup Language( SSML) (Isard, 1995) is used as an interface for TTS. The motivation behind SSML is to overcome the difficulty that different TTS systems require different input format. No additional information is provided as input to TTS, but SSML provides a straightforward representation of e"
W97-1204,P88-1023,0,0.71594,"Missing"
W97-1204,J90-3003,0,0.557336,"Missing"
W98-1123,P94-1002,0,0.398315,"t, tended to have their own prior segmentation markings consisting of headers or bullets, so these were excluded. We thus concentrated our work on a corpus of shorter articles, averaging roughly 800-1500 words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER:L i n e a r S e g m e n t a t i o n For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. [. &quot; - &quot; ~ - ~ &quot; - ~ W e i g h :'t [Terms I ITerrn I I [Links ~--~Score I ISegment I:egnts ] [Boundarie] Figure 1. SEGMENTERArchitecture 1,1 E x t r a c"
W98-1123,J97-1003,0,0.395035,"boundary heavily, but instead place the emphasis on the rear. Front: a paragraph in which a link begins. During: a paragraph in which a link occurs, but is not a front paragraph. Rear: a paragraph in which a link just stopped occurring the paragraph before. No link: any remaining paragraphs. paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : Ixxl ix21 t y p e :n f d r n f d Figure 2zt A term &quot;wine&quot;, and its occurrences and type. We also tried to semantically cluster terms by using Miller et al. (1990)'s WordNet 1.5 with edge counting to determine relatedness, as suggested by Hearst (1997). However, results showed only minor improvement in precision and over a tenfold increase in execution time. 199 1.2.3. Zero Sum Normalization When we iterate the weighting process described above over each term, and total the scores assigned, we come up with a numerical score for an indication of which paragraphs are more likely to beh a topical boundary. The higher the numerical score, the higher the likelihood that the paragraph is a beginning of a new topical segment. The question then is what should the threshold be? paras 1 2 3 4 5 7 8 sents 12345678901234567890123456789012345 wine : ixx"
W98-1123,P93-1041,0,0.756536,"Missing"
W98-1123,P97-1013,0,0.0278967,"words in length: 15 from the Wall Street Journal in the Linguistic Data Consortium's 1988 collection, and 5 from the on-line The Economist from 1997. We constructed an evaluation standard from human segmentation judgments to test our output. 1 SEGMENTER:L i n e a r S e g m e n t a t i o n For the purposes of discourse structure identification, we follow a formulation of the problem similar to Hearst (1994), in which zero or more segment boundaries are found at various paragraph separations, which identify one or more topical text segments. Our segmentation is linear, rather than hierarchical (Marcu 1997 and Yaari 1997), i.e. the input article is divided into a linear sequence of adjacent segments. Our segmentation methodology has three distinct phases (Figure 1), which are executed sequentially. We will describe each of these phases in detail. [. &quot; - &quot; ~ - ~ &quot; - ~ W e i g h :'t [Terms I ITerrn I I [Links ~--~Score I ISegment I:egnts ] [Boundarie] Figure 1. SEGMENTERArchitecture 1,1 E x t r a c t i n g Useful T o k e n s The task of determining segmentation b r e ~ s depends fundamentally on extracting useful topic information from the text. We extract three categories of information, which r"
W98-1123,C94-2121,0,0.1839,"Missing"
W98-1123,P93-1020,0,0.0629062,"Missing"
W98-1123,P94-1050,0,0.488043,"Missing"
W98-1123,W97-0304,0,\N,Missing
W98-1123,W97-0703,0,\N,Missing
W98-1123,C94-1042,0,\N,Missing
W98-1123,J96-2004,0,\N,Missing
W99-0619,P98-2165,1,0.39555,"tween content words and function words. Although this approach is simple, it tends to assign more pitch accents than necessary. We also tried the content/function word model on our corpus and as expected, we found it to be less powerful than the partof-speech model. More advanced pitch accent models make use of other information, such as part-of-speech, given/new distinctions and contrast information (Hirschberg, 1993). Semantic information is also employed in predicting accent patterns for complex nominal phrases (Sproat, 1994). Other comprehensive pitch accent models have been suggested in (Pan and McKeown, 1998) in the framework of Concept-to-Speech generation where the output of a natural lan155 guage generation system is used to predict pitch accent. 6 Discussion Since IC is not a perfect measurement of informativeness, it can cause problems in accent prediction. Moreover, even if a perfect measurement of informativeness is available, more features may be needed in order to build a satisfactory pitch accent model. In this section, we discuss each of these issues. IC does not directly measure the informativeness of a word. It measures the rarity of a word in a corpus. That a word is rare doesn't nec"
W99-0619,H93-1054,0,0.0175997,"prediction, since they measure two different kinds of informativeness, it is possible that a TF*IDF+IC model can 154 ! perform better than the IC model. Similarly, if TF*IDF is incorporated in the POS÷IC model, the overall performance may increase for the P O S + I C + T F * I D F model. However, our experiment shows no improvements when TF*IDF is incorporated in the IC and POS+IC model. Our experiments show that IC is always the dominant predictor when both IC and TF*IDF are presented. 5 Related Work Information based approaches were applied in some natural languages applications before. In (Resnik, 1993; Resnik, 1995), IC was used to measure semantic similarity between words and it is shown to be more effective than traditional measurements of semantic distance within the WordNet hierarchy. A similar log-based informationlike measurement was also employed in (Leacock and Chodorow, 1998) to measure semantic similarity. TF*IDF scores are mainly used in keyword-based information retrieval tasks. For example, TF*IDF has been used in (Salton, :1989; Salton, 1991) to index the words ini a document and is also implemented in SMART (Buckley, 1985) which is a general;-purpose information retrieval pa"
W99-0619,C98-2160,1,\N,Missing
