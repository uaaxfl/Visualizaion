2021.privatenlp-1.4,An Investigation towards Differentially Private Sequence Tagging in a Federated Framework,2021,-1,-1,2,1,2564,abhik jana,Proceedings of the Third Workshop on Privacy in Natural Language Processing,0,"To build machine learning-based applications for sensitive domains like medical, legal, etc. where the digitized text contains private information, anonymization of text is required for preserving privacy. Sequence tagging, e.g. as done in Named Entity Recognition (NER) can help to detect private information. However, to train sequence tagging models, a sufficient amount of labeled data are required but for privacy-sensitive domains, such labeled data also can not be shared directly. In this paper, we investigate the applicability of a privacy-preserving framework for sequence tagging tasks, specifically NER. Hence, we analyze a framework for the NER task, which incorporates two levels of privacy protection. Firstly, we deploy a federated learning (FL) framework where the labeled data are not shared with the centralized server as well as the peer clients. Secondly, we apply differential privacy (DP) while the models are being trained in each client instance. While both privacy measures are suitable for privacy-aware models, their combination results in unstable models. To our knowledge, this is the first study of its kind on privacy-aware sequence tagging models."
2021.nodalida-main.43,Error Analysis of using {BART} for Multi-Document Summarization: A Study for {E}nglish and {G}erman Language,2021,-1,-1,3,0,2709,timo johner,Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa),0,"Recent research using pre-trained language models for multi-document summarization task lacks deep investigation of potential erroneous cases and their possible application on other languages. In this work, we apply a pre-trained language model (BART) for multi-document summarization (MDS) task using both fine-tuning and without fine-tuning. We use two English datasets and one German dataset for this study. First, we reproduce the multi-document summaries for English language by following one of the recent studies. Next, we show the applicability of the model to German language by achieving state-of-the-art performance on German MDS. We perform an in-depth error analysis of the followed approach for both languages, which leads us to identifying most notable errors, from made-up facts and topic delimitation, and quantifying the amount of extractiveness."
2021.naacl-srw.5,Towards Layered Events and Schema Representations in Long Documents,2021,-1,-1,2,0,3177,hans hatzel,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"In this thesis proposal, we explore the application of event extraction to literary texts. Considering the lengths of literary documents modeling events in different granularities may be more adequate to extract meaningful information, as individual elements contribute little to the overall semantics. We adapt the concept of schemas as sequences of events all describing a single process, connected through shared participants extending it to for multiple schemas in a document. Segmentation of event sequences into schemas is approached by modeling event sequences, on such task as the narrative cloze task, the prediction of missing events in sequences. We propose building on sequences of event embeddings to form schema embeddings, thereby summarizing sections of documents using a single representation. This approach will allow for the comparisons of different sections of documents and entire literary works. Literature is a challenging domain based on its variety of genres, yet the representation of literary content has received relatively little attention."
2021.naacl-srw.21,Towards Multi-Modal Text-Image Retrieval to improve Human Reading,2021,-1,-1,4,0,3214,florian schneider,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop,0,"In primary school, children{'}s books, as well as in modern language learning apps, multi-modal learning strategies like illustrations of terms and phrases are used to support reading comprehension. Also, several studies in educational psychology suggest that integrating cross-modal information will improve reading comprehension. We claim that state-of- he-art multi-modal transformers, which could be used in a language learner context to improve human reading, will perform poorly because of the short and relatively simple textual data those models are trained with. To prove our hypotheses, we collected a new multi-modal image-retrieval dataset based on data from Wikipedia. In an in-depth data analysis, we highlight the differences between our dataset and other popular datasets. Additionally, we evaluate several state-of-the-art multi-modal transformers on text-image retrieval on our dataset and analyze their meager results, which verify our claims."
2021.naacl-main.351,Word Complexity is in the Eye of the Beholder,2021,-1,-1,4,0,4297,sian gooding,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a {''}one-size-fits-all{''} approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and non-native speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds."
2021.naacl-demos.12,{A}ctive{A}nno: General-Purpose Document-Level Annotation Tool with Active Learning Integration,2021,-1,-1,3,0,4869,max wiechmann,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations,0,"ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects, conducting annotations, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other software systems, including an API for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results."
2021.konvens-1.4,How Hateful are Movies? A Study and Prediction on Movie Subtitles,2021,-1,-1,5,0,5541,niklas boguszewski,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.konvens-1.15,Neural End-to-end Coreference Resolution for {G}erman in Different Domains,2021,-1,-1,3,1,5567,fynn schroder,Proceedings of the 17th Conference on Natural Language Processing (KONVENS 2021),0,None
2021.findings-emnlp.218,Probing Pre-trained Language Models for Semantic Attributes and their Values,2021,-1,-1,2,0,6968,meriem beloucif,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Pretrained language models (PTLMs) yield state-of-the-art performance on many natural language processing tasks, including syntax, semantics and commonsense. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g., the correlation between rich and high net worth. We use PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLMs encode semantic attributes along with their values. Such inferences based on semantics are intuitive for humans as part of our language understanding. Since PTLMs are trained on large amount of Wikipedia data we would assume that they can generate similar predictions, yet our findings reveal that PTLMs are still much worse than humans on this task. We show evidence and analysis explaining how to exploit our methodology to integrate better context and semantics into PTLMs using knowledge bases."
2021.eacl-demos.8,Forum 4.0: An Open-Source User Comment Analysis Framework,2021,-1,-1,3,0,11019,marlo haering,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"With the increasing number of user comments in diverse domains, including comments on online journalism and e-commerce websites, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces Forum 4.0, an open-source framework to semi-automatically analyze, aggregate, and visualize user comments based on labels defined by domain experts. We demonstrate the applicability of Forum 4.0 with comments analytics scenarios within the domains of online journalism and app stores. We outline the underlying container architecture, including the web-based user interface, the machine learning component, and the task manager for time-consuming tasks. We finally conduct machine learning experiments with simulated annotations and different sampling strategies on existing datasets from both domains to evaluate Forum 4.0{'}s performance. Forum 4.0 achieves promising classification results (ROC-AUC {\mbox{$\geq$}} 0.9 with 100 annotated samples), utilizing transformer-based embeddings with a lightweight logistic regression model. We explain how Forum 4.0{'}s architecture is applicable for millions of user comments in real-time, yet at feasible training and classification costs."
2021.eacl-demos.23,{SC}o{T}: Sense Clustering over Time: a tool for the analysis of lexical change,2021,-1,-1,5,0,11069,christian haase,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"We present Sense Clustering over Time (SCoT), a novel network-based tool for analysing lexical change. SCoT represents the meanings of a word as clusters of similar words. It visualises their formation, change, and demise. There are two main approaches to the exploration of dynamic networks: the discrete one compares a series of clustered graphs from separate points in time. The continuous one analyses the changes of one dynamic network over a time-span. SCoT offers a new hybrid solution. First, it aggregates time-stamped documents into intervals and calculates one sense graph per discrete interval. Then, it merges the static graphs to a new type of dynamic semantic neighbourhood graph over time. The resulting sense clusters offer uniquely detailed insights into lexical change over continuous intervals with model transparency and provenance. SCoT has been successfully used in a European study on the changing meaning of {`}crisis{'}."
2021.eacl-demos.36,Which is Better for Deep Learning: Python or {MATLAB}? Answering Comparative Questions in Natural Language,2021,-1,-1,3,0,11131,viktoriia chekalina,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,0,"We present a system for answering comparative questions (Is X better than Y with respect to Z?) in natural language. Answering such questions is important for assisting humans in making informed decisions. The key component of our system is a natural language interface for comparative QA that can be used in personal assistants, chatbots, and similar NLP devices. Comparative QA is a challenging NLP task, since it requires collecting support evidence from many different sources, and direct comparisons of rare objects may be not available even on the entire Web. We take the first step towards a solution for such a task offering a testbed for comparative QA in natural language by probing several methods, making the three best ones available as an online demo."
2020.semeval-1.213,{UHH}-{LT} at {S}em{E}val-2020 Task 12: Fine-Tuning of Pre-Trained Transformer Networks for Offensive Language Detection,2020,16,0,3,1,10216,gregor wiedemann,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Fine-tuning of pre-trained transformer networks such as BERT yield state-of-the-art results for text classification tasks. Typically, fine-tuning is performed on task-specific training datasets in a supervised manner. One can also fine-tune in unsupervised manner beforehand by further pre-training the masked language modeling (MLM) task. Hereby, in-domain data for unsupervised MLM resembling the actual classification target dataset allows for domain adaptation of the model. In this paper, we compare current pre-trained transformer networks with and without MLM fine-tuning on their performance for offensive language detection. Our MLM fine-tuned RoBERTa-based classifier officially ranks 1st in the SemEval 2020 Shared Task 12 for the English language. Further experiments with the ALBERT model even surpass this result."
2020.peoples-1.8,Social Media Unrest Prediction during the {COVID}-19 Pandemic: Neural Implicit Motive Pattern Recognition as Psychometric Signs of Severe Crises,2020,-1,-1,2,0,15737,dirk johannssen,"Proceedings of the Third Workshop on Computational Modeling of People's Opinions, Personality, and Emotion's in Social Media",0,"The COVID-19 pandemic has caused international social tension and unrest. Besides the crisis itself, there are growing signs of rising conflict potential of societies around the world. Indicators of global mood changes are hard to detect and direct questionnaires suffer from social desirability biases. However, so-called implicit methods can reveal humans intrinsic desires from e.g. social media texts. We present psychologically validated social unrest predictors and replicate scalable and automated predictions, setting a new state of the art on a recent German shared task dataset. We employ this model to investigate a change of language towards social unrest during the COVID-19 pandemic by comparing established psychological predictors on samples of tweets from spring 2019 with spring 2020. The results show a significant increase of the conflict indicating psychometrics. With this work, we demonstrate the applicability of automated NLP-based approaches to quantitative psychological research."
2020.pam-1.13,Generating Lexical Representations of Frames using Lexical Substitution,2020,-1,-1,4,1,11070,saba anwar,Proceedings of the Probability and Meaning Conference (PaM 2020),0,"Semantic frames are formal linguistic structures describing situations/actions/events, e.g. Commercial transfer of goods. Each frame provides a set of roles corresponding to the situation participants, e.g. Buyer and Goods, and lexical units (LUs) {--} words and phrases that can evoke this particular frame in texts, e.g. Sell. The scarcity of annotated resources hinders wider adoption of frame semantics across languages and domains. We investigate a simple yet effective method, lexical substitution with word representation models, to automatically expand a small set of frame-annotated sentences with new words for their respective roles and LUs. We evaluate the expansion quality using FrameNet. Contextualized models demonstrate overall superior performance compared to the non-contextualized ones on roles. However, the latter show comparable performance on the task of LU expansion."
2020.lrec-1.722,Automatic Compilation of Resources for Academic Writing and Evaluating with Informal Word Identification and Paraphrasing System,2020,38,0,4,1,282,seid yimam,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present the first approach to automatically building resources for academic writing. The aim is to build a writing aid system that automatically edits a text so that it better adheres to the academic style of writing. On top of existing academic resources, such as the Corpus of Contemporary American English (COCA) academic Word List, the New Academic Word List, and the Academic Collocation List, we also explore how to dynamically build such resources that would be used to automatically identify informal or non-academic words or phrases. The resources are compiled using different generic approaches that can be extended for different domains and languages. We describe the evaluation of resources with a system implementation. The system consists of an informal word identification (IWI), academic candidate paraphrase generation, and paraphrase ranking components. To generate candidates and rank them in context, we have used the PPDB and WordNet paraphrase resources. We use the Concepts in Context (CoInCO) {``}All-Words{''} lexical substitution dataset both for the informal word identification and paraphrase generation experiments. Our informal word identification component achieves an F-1 score of 82{\%}, significantly outperforming a stratified classifier baseline. The main contribution of this work is a domain-independent methodology to build targeted resources for writing aids."
2020.lrec-1.728,Word Sense Disambiguation for 158 Languages using Word Embeddings Only,2020,37,0,8,0,1966,varvara logacheva,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Disambiguation of word senses in context is easy for humans, but is a major challenge for automatic approaches. Sophisticated supervised and knowledge-based models were developed to solve this task. However, (i) the inherent Zipfian distribution of supervised training instances for a given word and/or (ii) the quality of linguistic knowledge representations motivate the development of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al., (2018), enabling WSD in these languages. Models and system are available online."
2020.coling-main.91,Exploring {A}mharic Sentiment Analysis from Social Media Texts: Building Annotation Tools and Classification Models,2020,-1,-1,4,1,282,seid yimam,Proceedings of the 28th International Conference on Computational Linguistics,0,"This paper presents the study of sentiment analysis for Amharic social media texts. As the number of social media users is ever-increasing, social media platforms would like to understand the latent meaning and sentiments of a text to enhance decision-making procedures. However, low-resource languages such as Amharic have received less attention due to several reasons such as lack of well-annotated datasets, unavailability of computing resources, and fewer or no expert researchers in the area. This research addresses three main research questions. We first explore the suitability of existing tools for the sentiment analysis task. Annotation tools are scarce to support large-scale annotation tasks in Amharic. Also, the existing crowdsourcing platforms do not support Amharic text annotation. Hence, we build a social-network-friendly annotation tool called {`}ASAB{'} using the Telegram bot. We collect 9.4k tweets, where each tweet is annotated by three Telegram users. Moreover, we explore the suitability of machine learning approaches for Amharic sentiment analysis. The FLAIR deep learning text classifier, based on network embeddings that are computed from a distributional thesaurus, outperforms other supervised classifiers. We further investigate the challenges in building a sentiment analysis system for Amharic and we found that the widespread usage of sarcasm and figurative speech are the main issues in dealing with the problem. To advance the sentiment analysis research in Amharic and other related low-resource languages, we release the dataset, the annotation tool, source code, and models publicly under a permissive."
2020.cogalex-1.1,Individual corpora predict fast memory retrieval during reading,2020,-1,-1,5,0,21783,markus hofmann,Proceedings of the Workshop on the Cognitive Aspects of the Lexicon,0,"The corpus, from which a predictive language model is trained, can be considered the experience of a semantic system. We recorded everyday reading of two participants for two months on a tablet, generating individual corpus samples of 300/500K tokens. Then we trained word2vec models from individual corpora and a 70 million-sentence newspaper corpus to obtain individual and norm-based long-term memory structure. To test whether individual corpora can make better predictions for a cognitive task of long-term memory retrieval, we generated stimulus materials consisting of 134 sentences with uncorrelated individual and norm-based word probabilities. For the subsequent eye tracking study 1-2 months later, our regression analyses revealed that individual, but not norm-corpus-based word probabilities can account for first-fixation duration and first-pass gaze duration. Word length additionally affected gaze duration and total viewing duration. The results suggest that corpora representative for an individual{'}s long-term memory structure can better explain reading performance than a norm corpus, and that recently acquired information is lexically accessed rapidly."
2020.acl-main.268,Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks,2020,-1,-1,2,1,5567,fynn schroder,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation."
W19-4516,Categorizing Comparative Sentences,2019,0,0,5,1,1663,alexander panchenko,Proceedings of the 6th Workshop on Argument Mining,0,"We tackle the tasks of automatically identifying comparative sentences and categorizing the intended preference (e.g., {``}Python has better NLP libraries than MATLAB{''} â Python, better, MATLAB). To this end, we manually annotate 7,199 sentences for 217 distinct target item pairs from several domains (27{\%} of the sentences contain an oriented comparison in the sense of {``}better{''} or {``}worse{''}). A gradient boosting model based on pre-trained sentence embeddings reaches an F1 score of 85{\%} in our experimental evaluation. The model can be used to extract comparative sentences for pro/con argumentation in comparative / argument search engines or debating technologies."
W19-3014,Reviving a psychometric measure: Classification and prediction of the Operant Motive Test,2019,-1,-1,2,0,15737,dirk johannssen,Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology,0,"Implicit motives allow for the characterization of behavior, subsequent success and long-term development. While this has been operationalized in the operant motive test, research on motives has declined mainly due to labor-intensive and costly human annotation. In this study, we analyze over 200,000 labeled data items from 40,000 participants and utilize them for engineering features for training a logistic model tree machine learning model. It captures manually assigned motives well with an F-score of 80{\%}, coming close to the pairwise annotator intraclass correlation coefficient of r = .85. In addition, we found a significant correlation of r = .2 between subsequent academic success and data automatically labeled with our model in an extrinsic evaluation."
W19-0413,Language-Agnostic Model for Aspect-Based Sentiment Analysis,2019,0,0,4,0.31746,7352,md akhtar,Proceedings of the 13th International Conference on Computational Semantics - Long Papers,0,"In this paper, we propose a language-agnostic deep neural network architecture for aspect-based sentiment analysis. The proposed approach is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network, which is further assisted with extra hand-crafted features. We define three different architectures for the successful combination of word embeddings and hand-crafted features. We evaluate the proposed approach for six languages (i.e. English, Spanish, French, Dutch, German and Hindi) and two problems (i.e. aspect term extraction and aspect sentiment classification). Experiments show that the proposed model attains state-of-the-art performance in most of the settings."
S19-2018,{HHMM} at {S}em{E}val-2019 Task 2: Unsupervised Frame Induction using Contextualized Word Embeddings,2019,5,1,5,1,11070,saba anwar,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (Qasem-iZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages."
S19-2137,{UHH}-{LT} at {S}em{E}val-2019 Task 6: Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection,2019,0,1,3,1,10216,gregor wiedemann,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present a neural network based approach of transfer learning for offensive language detection. For our system, we compare two types of knowledge transfer: supervised and unsupervised pre-training. Supervised pre-training of our bidirectional GRU-3-CNN architecture is performed as multi-task learning of parallel training of five different tasks. The selected tasks are supervised classification problems from public NLP resources with some overlap to offensive language such as sentiment detection, emoji classification, and aggressive language classification. Unsupervised transfer learning is performed with a thematic clustering of 40M unlabeled tweets via LDA. Based on this dataset, pre-training is performed by predicting the main topic of a tweet. Results indicate that unsupervised transfer from large datasets performs slightly better than supervised training on small {`}near target category{'} datasets. In the SemEval Task, our system ranks 14 out of 103 participants."
S19-1014,Learning Graph Embeddings from {W}ord{N}et-based Similarity Measures,2019,0,0,4,0.449179,2619,andrey kutuzov,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"We present path2vec, a new approach for learning graph embeddings that relies on structural measures of pairwise node similarities. The model learns representations for nodes in a dense space that approximate a given user-defined graph distance measure, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. Evaluation of the proposed model on semantic similarity and word sense disambiguation tasks, using various WordNet-based similarity measures, show that our approach yields competitive results, outperforming strong graph embedding baselines. The model is computationally efficient, being orders of magnitude faster than the direct computation of graph-based distances."
P19-3031,{TARGER}: Neural Argument Mining at Your Fingertips,2019,0,1,6,0,5979,artem chernodub,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present TARGER, an open source neural argument mining framework for tagging arguments in free input texts and for keyword-based retrieval of arguments from an argument-tagged web-scale corpus. The currently available models are pre-trained on three recent argument mining datasets and enable the use of neural argument mining without any reproducibility effort on the user{'}s side. The open source code ensures portability to other domains and use cases."
P19-2044,Improving Neural Entity Disambiguation with Graph Embeddings,2019,0,0,3,0,25522,ozge sevgili,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Entity Disambiguation (ED) is the task of linking an ambiguous entity mention to a corresponding entry in a knowledge base. Current methods have mostly focused on unstructured text data to learn representations of entities, however, there is structured information in the knowledge base itself that should be useful to disambiguate entities. In this work, we propose a method that uses graph embeddings for integrating structured information from the knowledge base with unstructured information from text-based representations. Our experiments confirm that graph embeddings trained on a graph of hyperlinks between Wikipedia articles improve the performances of simple feed-forward neural ED model and a state-of-the-art neural ED system."
P19-2045,Hierarchical Multi-label Classification of Text with Capsule Networks,2019,0,6,3,0,635,rami aly,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Capsule networks have been shown to demonstrate good performance on structured data in the area of visual inference. In this paper we apply and compare simple shallow capsule networks for hierarchical multi-label text classification and show that they can perform superior to other neural networks, such as CNNs and LSTMs, and non-neural network architectures such as SVMs. For our experiments, we use the established Web of Science (WOS) dataset and introduce a new real-world scenario dataset, the BlurbGenreCollection (BGC). Our results confirm the hypothesis that capsule networks are especially advantageous for rare events and structurally diverse categories, which we attribute to their ability to combine latent encoded information."
P19-1316,On the Compositionality Prediction of Noun Phrases using Poincar{\\'e} Embeddings,2019,43,0,5,1,2564,abhik jana,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The compositionality degree of multiword expressions indicates to what extent the meaning of a phrase can be derived from the meaning of its constituents and their grammatical relations. Prediction of (non)-compositionality is a task that has been frequently addressed with distributional semantic models. We introduce a novel technique to blend hierarchical information with distributional information for predicting compositionality. In particular, we use hypernymy information of the multiword and its constituents encoded in the form of the recently introduced Poincar{\'e} embeddings in addition to the distributional information to detect compositionality for noun phrases. Using a weighted average of the distributional similarity and a Poincar{\'e} similarity function, we obtain consistent and substantial, statistically significant improvement across three gold standard datasets over state-of-the-art models based on distributional information only. Unlike traditional approaches that solely use an unsupervised setting, we have also framed the problem as a supervised task, obtaining comparable improvements. Further, we publicly release our Poincar{\'e} embeddings, which are trained on the output of handcrafted lexical-syntactic patterns on a large corpus."
P19-1325,Making Fast Graph-based Algorithms with Graph Metric Embeddings,2019,32,0,4,0.449179,2619,andrey kutuzov,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Graph measures, such as node distances, are inefficient to compute. We explore dense vector representations as an effective way to approximate the same information. We introduce a simple yet efficient and effective approach for learning graph embeddings. Instead of directly operating on the graph structure, our method takes structural measures of pairwise node similarities into account and learns dense node representations reflecting user-defined graph distance measures, such as e.g. the shortest path distance or distance measures that take information beyond the graph structure into account. We demonstrate a speed-up of several orders of magnitude when predicting word similarity by vector operations on our embeddings as opposed to directly computing the respective path-based measures, while outperforming various other graph embeddings on semantic similarity and word sense disambiguation tasks."
P19-1474,Every Child Should Have Parents: A Taxonomy Refinement Algorithm Based on Hyperbolic Term Embeddings,2019,29,0,5,0,635,rami aly,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We introduce the use of Poincar{\'e} embeddings to improve existing state-of-the-art approaches to domain-specific taxonomy induction from text as a signal for both relocating wrong hyponym terms within a (pre-induced) taxonomy as well as for attaching disconnected terms in a taxonomy. This method substantially improves previous state-of-the-art results on the SemEval-2016 Task 13 on taxonomy extraction. We demonstrate the superiority of Poincar{\'e} embeddings over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space."
P19-1584,Adversarial Learning of Privacy-Preserving Text Representations for De-Identification of Medical Records,2019,0,0,4,0,25891,max friedrich,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"De-identification is the task of detecting protected health information (PHI) in medical text. It is a critical step in sanitizing electronic health records (EHR) to be shared for research. Automatic de-identification classifiers can significantly speed up the sanitization process. However, obtaining a large and diverse dataset to train such a classifier that works well across many types of medical text poses a challenge as privacy laws prohibit the sharing of raw medical records. We introduce a method to create privacy-preserving shareable representations of medical text (i.e. they contain no PHI) that does not require expensive manual pseudonymization. These representations can be shared between organizations to create unified datasets for training de-identification models. Our representation allows training a simple LSTM-CRF de-identification model to an F1 score of 97.4{\%}, which is comparable to a strong baseline that exposes private information in its representation. A robust, widely available de-identification classifier based on our representation could potentially enable studies for which de-identification would otherwise be too costly."
N19-4017,{LT} Expertfinder: An Evaluation Framework for Expert Finding Methods,2019,0,1,3,0,25969,tim fischer,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),0,"Expert finding is the task of ranking persons for a predefined topic or search query. Finding experts for a specified area is an important task and has attracted much attention in the information retrieval community. Most approaches for this task are evaluated in a supervised fashion, which depend on predefined topics of interest as well as gold standard expert rankings. Famous representatives of such datasets are enriched versions of DBLP provided by the ArnetMiner projet or the W3C Corpus of TREC. However, manually ranking experts can be considered highly subjective and detailed rankings are hardly distinguishable. Evaluating these datasets does not necessarily guarantee a good or bad performance of the system. Particularly for dynamic systems, where topics are not predefined but formulated as a search query, we believe a more informative approach is to perform user studies for directly comparing different methods in the same view. In order to accomplish this in a user-friendly way, we present the LT Expert Finder web-application, which is equipped with various query-based expert finding methods that can be easily extended, a detailed expert profile view, detailed evidence in form of relevant documents and statistics, and an evaluation component that allows the qualitative comparison between different rankings."
J19-3002,Watset: Local-Global Graph Clustering with Applications in Sense and Frame Induction,2019,145,2,3,1,756,dmitry ustalov,Computational Linguistics,0,"We present a detailed theoretical and computational analysis of the Watset meta-algorithm for fuzzy graph clustering, which has been found to be widely applicable in a variety of domains. This algorithm creates an intermediate representation of the input graph, which reflects the {``}ambiguity{''} of its nodes. Then, it uses hard clustering to discover clusters in this {``}disambiguated{''} intermediate graph. After outlining the approach and analyzing its computational complexity, we demonstrate that Watset shows competitive results in three applications: unsupervised synset induction from a synonymy graph, unsupervised semantic frame induction from dependency triples, and unsupervised semantic class induction from a distributional thesaurus. Our algorithm is generic and can also be applied to other networks of linguistic data."
W18-0507,A Report on the Complex Word Identification Shared Task 2018,2018,16,0,2,1,282,seid yimam,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We report the findings of the second Complex Word Identification (CWI) shared task organized as part of the BEA workshop co-located with NAACL-HLT{'}2018. The second CWI shared task featured multilingual and multi-genre datasets divided into four tracks: English monolingual, German monolingual, Spanish monolingual, and a multilingual track with a French test set, and two tasks: binary classification and probabilistic classification. A total of 12 teams submitted their results in different task/track combinations and 11 of them wrote system description papers that are referred to in this report and appear in the BEA workshop proceedings."
S18-1163,"{B}om{J}i at {S}em{E}val-2018 Task 10: Combining Vector-, Pattern- and Graph-based Information to Identify Discriminative Attributes",2018,0,0,2,0,181,enrico santus,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes BomJi, a supervised system for capturing discriminative attributes in word pairs (e.g. yellow as discriminative for banana over watermelon). The system relies on an XGB classifier trained on carefully engineered graph-, pattern- and word embedding-based features. It participated in the SemEval-2018 Task 10 on Capturing Discriminative Attributes, achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems."
P18-2010,Unsupervised Semantic Frame Induction using Triclustering,2018,20,3,4,1,756,dmitry ustalov,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We use dependency triples automatically extracted from a Web-scale corpus to perform unsupervised semantic frame induction. We cast the frame induction problem as a triclustering problem that is a generalization of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task."
N18-3027,Document-based Recommender System for Job Postings using Dense Representations,2018,0,2,3,0,29338,ahmed elsafty,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",0,"Job boards and professional social networks heavily use recommender systems in order to better support users in exploring job advertisements. Detecting the similarity between job advertisements is important for job recommendation systems as it allows, for example, the application of item-to-item based recommendations. In this work, we research the usage of dense vector representations to enhance a large-scale job recommendation system and to rank German job advertisements regarding their similarity. We follow a two-folded evaluation scheme: (1) we exploit historic user interactions to automatically create a dataset of similar jobs that enables an offline evaluation. (2) In addition, we conduct an online A/B test and evaluate the best performing method on our platform reaching more than 1 million users. We achieve the best results by combining job titles with full-text job descriptions. In particular, this method builds dense document representation using words of the titles to weigh the importance of words of the full-text description. In the online evaluation, this approach allows us to increase the click-through rate on job recommendations for active users by 8.0{\%}."
L18-1093,Enriching Frame Representations with Distributionally Induced Senses,2018,17,0,3,1,17226,stefano faralli,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce a new lexical resource that enriches the Framester knowledge graph, which links Framnet, WordNet, VerbNet and other resources, with semantic features from text corpora. These features are extracted from distributionally induced sense inventories and subsequently linked to the manually-constructed frame representations to boost the performance of frame disambiguation in context. Since Framester is a frame-based knowledge graph, which enables full-fledged OWL querying and reasoning, our resource paves the way for the development of novel, deeper semantic-aware applications that could benefit from the combination of knowledge from text and complex symbolic representations of events and participants. Together with the resource we also provide the software we developed for the evaluation in the task of Word Frame Disambiguation (WFD)."
L18-1164,An Unsupervised Word Sense Disambiguation System for Under-Resourced Languages,2018,13,1,5,1,756,dmitry ustalov,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this paper, we present Watasense, an unsupervised system for word sense disambiguation. Given a sentence, the system chooses the most relevant sense of each input word with respect to the semantic similarity between the given sentence and the synset constituting the sense of the target word. Watasense has two modes of operation. The sparse mode uses the traditional vector space model to estimate the most similar word sense corresponding to its context. The dense mode, instead, uses synset embeddings to cope with the sparsity problem. We describe the architecture of the present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index."
L18-1167,Retrofitting Word Representations for Unsupervised Sense Aware Word Similarities,2018,0,3,2,1,18079,steffen remus,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1244,Improving Hypernymy Extraction with Distributional Semantic Classes,2018,0,1,5,1,1663,alexander panchenko,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this paper, we show for the first time how distributionally-induced semantic classes can be helpful for extraction of hypernyms. We present a method for (1) inducing sense-aware semantic classes using distributional semantics and (2) using these induced semantic classes for filtering noisy hypernymy relations. Denoising of hypernyms is performed by labeling each semantic class with its hypernyms. On one hand, this allows us to filter out wrong extractions using the global structure of the distributionally similar senses. On the other hand, we infer missing hypernyms via label propagation to cluster terms. We conduct a large-scale crowdsourcing study showing that processing of automatically extracted hypernyms using our approach improves the quality of the hypernymy extraction both in terms of precision and recall. Furthermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a benchmarking dataset."
L18-1286,Building a Web-Scale Dependency-Parsed Corpus from {C}ommon{C}rawl,2018,-1,-1,5,1,1663,alexander panchenko,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
J18-3005,Using Semantics for Granularities of Tokenization,2018,36,1,2,1,24663,martin riedl,Computational Linguistics,0,"Depending on downstream applications, it is advisable to extend the notion of tokenization from low-level character-based token boundary detection to identification of meaningful and useful language units. This entails both identifying units composed of several single words that form a several single words that form a, as well as splitting single-word compounds into their meaningful parts. In this article, we introduce unsupervised and knowledge-free methods for these two tasks. The main novelty of our research is based on the fact that methods are primarily based on distributional similarity, of which we use two flavors: a sparse count-based and a dense neural-based distributional semantic model. First, we introduce DRUID, which is a method for detecting MWEs. The evaluation on MWE-annotated data sets in two languages and newly extracted evaluation data sets for 32 languages shows that DRUID compares favorably over previous methods not utilizing distributional information. Second, we present SECOS, an algorithm for decompounding close compounds. In an evaluation of four dedicated decompounding data sets across four languages and on data sets extracted from Wiktionary for 14 languages, we demonstrate the superiority of our approach over unsupervised baselines, sometimes even matching the performance of previous language-specific and supervised methods. In a final experiment, we show how both decompounding and MWE information can be used in information retrieval. Here, we obtain the best results when combining word information with MWEs and the compound parts in a bag-of-words retrieval set-up. Overall, our methodology paves the way to automatic detection of lexical units beyond standard tokenization techniques without language-specific preprocessing steps such as POS tagging."
D18-2009,Demonstrating {P}ar4{S}em - A Semantic Writing Aid with Adaptive Paraphrasing,2018,0,0,2,1,282,seid yimam,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"In this paper, we present Par4Sem, a semantic writing aid tool based on adaptive paraphrasing. Unlike many annotation tools that are primarily used to collect training examples, Par4Sem is integrated into a real word application, in this case a writing aid tool, in order to collect training examples from usage data. Par4Sem is a tool, which supports an adaptive, iterative, and interactive process where the underlying machine learning models are updated for each iteration using new training examples from usage data. After motivating the use of ever-learning tools in NLP applications, we evaluate Par4Sem by adopting it to a text simplification task through mere usage."
D18-2014,A Multilingual Information Extraction Pipeline for Investigative Journalism,2018,10,0,3,1,10216,gregor wiedemann,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We introduce an advanced information extraction pipeline to automatically process very large collections of unstructured textual data for the purpose of investigative journalism. The pipeline serves as a new input processor for the upcoming major release of our New/s/leak 2.0 software, which we develop in cooperation with a large German news organization. The use case is that journalists receive a large collection of files up to several Gigabytes containing unknown contents. Collections may originate either from official disclosures of documents, e.g. Freedom of Information Act requests, or unofficial data leaks."
C18-1028,{P}ar4{S}im {--} Adaptive Paraphrasing for Text Simplification,2018,0,0,2,1,282,seid yimam,Proceedings of the 27th International Conference on Computational Linguistics,0,"Learning from a real-world data stream and continuously updating the model without explicit supervision is a new challenge for NLP applications with machine learning components. In this work, we have developed an adaptive learning system for text simplification, which improves the underlying learning-to-rank model from usage data, i.e. how users have employed the system for the task of simplification. Our experimental result shows that, over a period of time, the performance of the embedded paraphrase ranking model increases steadily improving from a score of 62.88{\%} up to 75.70{\%} based on the NDCG@10 evaluation metrics. To our knowledge, this is the first study where an NLP component is adaptively improved through usage."
yimam-etal-2017-entity,Entity-Centric Information Access with Human in the Loop for the Biomedical Domain,2017,14,2,5,1,282,seid yimam,Proceedings of the Biomedical {NLP} Workshop associated with {RANLP} 2017,0,"In this paper, we describe the concept of entity-centric information access for the biomedical domain. With entity recognition technologies approaching acceptable levels of accuracy, we put forward a paradigm of document browsing and searching where the entities of the domain and their relations are explicitly modeled to provide users the possibility of collecting exhaustive information on relations of interest. We describe three working prototypes along these lines: NEW/S/LEAK, which was developed for investigative journalists who need a quick overview of large leaked document collections; STORYFINDER, which is a personalized organizer for information found in web pages that allows adding entities as well as relations, and is capable of personalized information management; and adaptive annotation capabilities of WEBANNO, which is a general-purpose linguistic annotation tool. We will discuss future steps towards the adaptation of these tools to biomedical data, which is subject to a recently started project on biomedical knowledge acquisition. A key difference to other approaches is the centering around the user in a Human-in-the-Loop machine learning approach, where users define and extend categories and enable the system to improve via feedback and interaction."
W17-6933,There{'}s no {`}Count or Predict{'} but task-based selection for distributional models,2017,26,1,2,1,24663,martin riedl,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-1909,Using Linked Disambiguated Distributional Networks for Word Sense Disambiguation,2017,28,5,4,1,1663,alexander panchenko,"Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications",0,"We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks reduces the sparsity of sense representations used for WSD. We evaluate these enriched representations within two lexical sample sense disambiguation benchmarks. Our results indicate that (1) features extracted from the corpus-based resource help to significantly outperform a model based solely on the lexical resource; (2) our method achieves results comparable or better to four state-of-the-art unsupervised knowledge-based WSD systems including three hybrid systems that also rely on text corpora. In contrast to these hybrid methods, our approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems."
W17-0202,Replacing {OOV} Words For Dependency Parsing With Distributional Semantics,2017,18,2,3,0,21982,prasanth kolachina,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
W17-0213,Using Pseudowords for Algorithm Comparison: An Evaluation Framework for Graph-based Word Sense Induction,2017,25,2,2,0,16575,flavio cecchini,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
S17-2009,{IIT}-{UHH} at {S}em{E}val-2017 Task 3: Exploring Multiple Features for Community Question Answering and Implicit Dialogue Identification,2017,0,6,2,0,32227,titas nandi,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"In this paper we present the system for Answer Selection and Ranking in Community Question Answering, which we build as part of our participation in SemEval-2017 Task 3. We develop a Support Vector Machine (SVM) based system that makes use of textual, domain-specific, word-embedding and topic-modeling features. In addition, we propose a novel method for dialogue chain identification in comment threads. Our primary submission won subtask C, outperforming other systems in all the primary evaluation metrics. We performed well in other English subtasks, ranking third in subtask A and eighth in subtask B. We also developed open source toolkits for all the three English subtasks by the name cQARank [\url{https://github.com/TitasNandi/cQARank}]."
S17-2025,{STS}-{UHH} at {S}em{E}val-2017 Task 1: Scoring Semantic Textual Similarity Using Supervised and Unsupervised Ensemble,2017,9,0,3,1,32228,sarah kohail,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper reports the STS-UHH participation in the SemEval 2017 shared Task 1 of Semantic Textual Similarity (STS). Overall, we submitted 3 runs covering monolingual and cross-lingual STS tracks. Our participation involves two approaches: unsupervised approach, which estimates a word alignment-based similarity score, and supervised approach, which combines dependency graph similarity and coverage features with lexical similarity measures using regression methods. We also present a way on ensembling both models. Out of 84 submitted runs, our team best multi-lingual run has been ranked 12th in overall performance with correlation of 0.61, 7th among 31 participating teams."
S17-2153,{IITPB} at {S}em{E}val-2017 Task 5: Sentiment Prediction in Financial Text,2017,0,5,5,0.833333,24889,abhishek kumar,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper reports team IITPB{'}s participation in the SemEval 2017 Task 5 on {`}Fine-grained sentiment analysis on financial microblogs and news{'}. We developed 2 systems for the two tracks. One system was based on an ensemble of Support Vector Classifier and Logistic Regression. This system relied on Distributional Thesaurus (DT), word embeddings and lexicon features to predict a floating sentiment value between -1 and +1. The other system was based on Support Vector Regression using word embeddings, lexicon features, and PMI scores as features. The system was ranked 5th in track 1 and 8th in track 2."
yimam-etal-2017-multilingual,Multilingual and Cross-Lingual Complex Word Identification,2017,6,3,4,1,282,seid yimam,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Complex Word Identification (CWI) is an important task in lexical simplification and text accessibility. Due to the lack of CWI datasets, previous works largely depend on Simple English Wikipedia and edit histories for obtaining {`}gold standard{'} annotations, which are of doubtable quality, and limited only to English. We collect complex words/phrases (CP) for English, German and Spanish, annotated by both native and non-native speakers, and propose language independent features that can be used to train multilingual and cross-lingual CWI models. We show that the performance of cross-lingual CWI systems (using a model trained on one language and applying it on the other languages) is comparable to the performance of monolingual CWI systems."
P17-1145,{W}atset: Automatic Induction of Synsets from a Graph of Synonyms,2017,20,0,3,1,756,dmitry ustalov,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. First, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Second, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform a fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results, outperforming five competitive state-of-the-art methods in terms of F-score on three gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources."
I17-2068,{CWIG}3{G}2 - Complex Word Identification Task across Three Text Genres and Two User Groups,2017,0,8,4,1,282,seid yimam,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI dataset (CWIG3G2) covering three text genres News, WikiNews, and Wikipedia) annotated by both native and non-native English speakers. Unlike previous datasets, we cover single words, as well as complex phrases, and present them for judgment in a paragraph context. We present the first study on cross-genre and cross-group CWI, showing measurable influences in native language and genre types."
E17-2087,Negative Sampling Improves Hypernymy Extraction Based on Projection Learning,2017,33,7,3,1,756,dmitry ustalov,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We present a new approach to extraction of hypernyms based on projection learning and word embeddings. In contrast to classification-based approaches, projection-based methods require no candidate hyponym-hypernym pairs. While it is natural to use both positive and negative training examples in supervised relation extraction, the impact of positive examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for regularization of the model significantly improve performance compared to the state-of-the-art approach of Fu et al. (2014) on three datasets from different languages."
E17-1009,Unsupervised Does Not Mean Uninterpretable: The Case for Word Sense Induction and Disambiguation,2017,56,16,5,1,1663,alexander panchenko,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"The current trend in NLP is the use of highly opaque models, e.g. neural networks and word embeddings. While these models yield state-of-the-art results on a range of tasks, their drawback is poor interpretability. On the example of word sense induction and disambiguation (WSID), we show that it is possible to develop an interpretable model that matches the state-of-the-art models in accuracy. Namely, we present an unsupervised, knowledge-free WSID approach, which is interpretable at three levels: word sense inventory, sense feature representations, and disambiguation procedure. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other unsupervised systems while offering the possibility to justify its decisions in human-readable form."
E17-1056,The {C}ontrast{M}edium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just A Few Links,2017,40,6,3,1,17226,stefano faralli,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manually selected input root and leaf concepts. This is achieved by leveraging structural information from a companion reference taxonomy, to which the input knowledge graph is linked (either automatically or manually). When used in conjunction with methods for hypernym acquisition and knowledge base linking, our methodology provides a complete solution for end-to-end taxonomy induction. We conduct experiments using automatically acquired knowledge graphs, as well as a SemEval benchmark, and show that our method is able to achieve high performance on the task of taxonomy induction."
D17-2016,"Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation",2017,12,10,7,1,1663,alexander panchenko,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the gap between these two so far disconnected groups of methods. Namely, our system, providing access to several state-of-the-art WSD models, aims to be interpretable as a knowledge-based system while it remains completely unsupervised and knowledge-free. The presented tool features a Web interface for all-word disambiguation of texts that makes the sense predictions human readable by providing interpretable word sense inventories, sense representations, and disambiguation results. We provide a public API, enabling seamless integration."
W16-5301,Vectors or Graphs? On Differences of Representations for Distributional Semantic Models,2016,34,2,1,1,2565,chris biemann,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"Distributional Semantic Models (DSMs) have recently received increased attention, together with the rise of neural architectures for scalable training of dense vector embeddings. While some of the literature even includes terms like {`}vectors{'} and {`}dimensionality{'} in the definition of DSMs, there are some good reasons why we should consider alternative formulations of distributional models. As an instance, I present a scalable graph-based solution to distributional semantics. The model belongs to the family of {`}count-based{'} DSMs, keeps its representation sparse and explicit, and thus fully interpretable. I will highlight some important differences between sparse graph-based and dense vector approaches to DSMs: while dense vector-based models are computationally easier to handle and provide a nice uniform representation that can be compared and combined in many ways, they lack interpretability, provenance and robustness. On the other hand, graph-based sparse models have a more straightforward interpretation, handle sense distinctions more naturally and can straightforwardly be linked to knowledge bases, while lacking the ability to compare arbitrary lexical units and a compositionality operation. Since both representations have their merits, I opt for exploring their combination in the outlook."
W16-5308,Towards a resource based on users{'} knowledge to overcome the Tip of the Tongue problem.,2016,34,1,2,0,33495,michael zock,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"Language production is largely a matter of words which, in the case of access problems, can be searched for in an external resource (lexicon, thesaurus). In this kind of dialogue the user provides the momentarily available knowledge concerning the target and the system responds with the best guess(es) it can make given this input. As tip-of-the-tongue (ToT)-studies have shown, people always have some knowledge concerning the target (meaning fragments, number of syllables, ...) even if its complete form is eluding them. We will show here how to tap on this knowledge to build a resource likely to help authors (speakers/writers) to overcome the ToT-problem. Yet, before doing so we need a better understanding of the various kinds of knowledge people have when looking for a word. To this end, we asked crowdworkers to provide some cues to describe a given target and to specify then how each one of them relates to the target, in the hope that this could help others to find the elusive word. Next, we checked how well a given search strategy worked when being applied to differently built lexical networks. The results showed quite dramatic differences, which is not really surprising. After all, different networks are built for different purposes; hence each one of them is more or less suited for a given task. What was more surprising though is the fact that the relational information given by the users did not allow us to find the elusive word in WordNet better than without it."
W16-4011,A Web-based Tool for the Integrated Annotation of Semantic and Syntactic Structures,2016,11,21,7,0.493827,11279,richard castilho,Proceedings of the Workshop on Language Technology Resources and Tools for Digital Humanities ({LT}4{DH}),0,"We introduce the third major release of WebAnno, a generic web-based annotation tool for distributed teams. New features in this release focus on semantic annotation tasks (e.g. semantic role labelling or event annotation) and allow the tight integration of semantic annotations with syntactic annotations. In particular, we introduce the concept of slot features, a novel constraint mechanism that allows modelling the interaction between semantic and syntactic annotations, as well as a new annotation user interface. The new features were developed and used in an annotation project for semantic roles on German texts. The paper briefly introduces this project and reports on experiences performing annotations with the new tool. On a comparative evaluation, our tool reaches significant speedups over WebAnno 2 for a semantic annotation task."
W16-2613,{E}mpiri{ST}: {AIPHES} - Robust Tokenization and {POS}-Tagging for Different Genres,2016,10,4,3,1,18079,steffen remus,Proceedings of the 10th Web as Corpus Workshop,0,None
W16-1801,Learning Paraphrasing for Multiword Expressions,2016,31,1,4,1,282,seid yimam,Proceedings of the 12th Workshop on Multiword Expressions,0,"In this paper, we investigate the impact of context for the paraphrase ranking task, comparing and quantifying results forn multi-word expressions and single words. We focus on systematic integration of existing paraphrase resources to producen paraphrase candidates and later ask human annotators to judge paraphrasability in context.n We first conduct a paraphrase-scoring annotation task with and without context for targets that are i) single- and multi-wordn expressions ii) verbs and nouns. We quantify how differently annotators score paraphrases when context information is provided.n Furthermore, we report on experiments with automatic paraphrase ranking. If we regard the problem as a binary classificationn task, we obtain an F1xe2x80x93score of 81.56% and 79.87% for multi-word expressions and single words resp. using kNN classifier. Approaching the problem as a learning-to-rank task, we attain MAP scores up to 87.14% and 91.58% for multiword expressions and single words resp. using LambdaMART, thus yielding highquality contextualized paraphrased selection. Further, we provide the first dataset with paraphrase judgments for multi-word targets in context."
W16-1816,Impact of {MWE} Resources on Multiword Recognition,2016,13,4,2,1,24663,martin riedl,Proceedings of the 12th Workshop on Multiword Expressions,0,"In this paper, we demonstrate the impact of Multiword Expression (MWE) resources in the task of MWE recognition in text. We present results based on the Wiki50 corpus for MWE resources, generated using unsupervised methods from raw text and resources that are extracted using manual text markup and lexical resources. We show that resources acquired from manual annotation yield the best MWE tagging performance. However, a more finegrained analysis that differentiates MWEs according to their part of speech (POS) reveals that automatically acquired MWE lists outperform the resources generated from human knowledge for three out of four classes."
W16-1620,Making Sense of Word Embeddings,2016,41,39,3,0,33974,maria pelevina,Proceedings of the 1st Workshop on Representation Learning for {NLP},0,"We present a simple yet effective approach for learning word sense embeddings. In contrast to existing techniques, which either directly learn sense representations from corpora or rely on sense inventories from lexical resources, our approach can induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An integrated WSD mechanism enables labeling of words in context with learned sense vectors, which gives rise to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems."
S16-1174,{IIT}-{TUDA} at {S}em{E}val-2016 Task 5: Beyond Sentiment Lexicon: Combining Domain Dependency and Distributional Semantics Features for Aspect Based Sentiment Analysis,2016,15,24,5,0,12110,ayush kumar,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
S16-1206,"{TAXI} at {S}em{E}val-2016 Task 13: a Taxonomy Induction Method based on Lexico-Syntactic Patterns, Substrings and Focused Crawling",2016,32,19,8,1,1663,alexander panchenko,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"We present a system for taxonomy construction that reached the first place in all subtasks of the SemEval 2016 challenge on Taxonomy Extraction Evaluation. Our simple yet effective approach harvests hypernyms with substring inclusion and Hearst-style lexicosyntactic patterns from domain-specific texts obtained via language model based focused crawling. Extracted taxonomies are evaluated on English, Dutch, French and Italian for three domains each (Food, Environment and Science). Evaluations against a gold standard and by human judgment show that our method outperforms more complex and knowledge-rich approaches on most domains and languages. Furthermore, to adapt the method to a new domain or language, only a small amount of manual labour is needed."
P16-4028,new/s/leak {--} Information Extraction and Visualization for Investigative Data Journalists,2016,9,6,9,1,282,seid yimam,Proceedings of {ACL}-2016 System Demonstrations,0,None
P16-1012,Language Transfer Learning for Supervised Lexical Substitution,2016,31,2,2,0,4459,gerold hintz,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a framework for lexical substitution that is able to perform transfer learning across languages. Datasets for this task are available in at least three languages (English, Italian, and German). Previous work has addressed each of these tasks in isolation. In contrast, we regard the union of three shared tasks as a combined multilingual dataset. We show that a supervised system can be trained effectively, even if training and evaluation data are from different languages. Successful transfer learning between languages suggests that the learned model is in fact independent of the underlying language. We combine state-of-the-art unsupervised features obtained from syntactic word embeddings and distributional thesauri in a supervised delexicalized ranking system. Our system improves over state of the art in the full lexical substitution task in all three languages."
N16-1075,Unsupervised Compound Splitting With Distributional Semantics Rivals Supervised Methods,2016,13,12,2,1,24663,martin riedl,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1572,Domain-Specific Corpus Expansion with Focused Webcrawling,2016,0,3,2,1,18079,steffen remus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This work presents a straightforward method for extending or creating in-domain web corpora by focused webcrawling. The focused webcrawler uses statistical N-gram language models to estimate the relatedness of documents and weblinks and needs as input only N-grams or plain texts of a predefined domain and seed URLs as starting points. Two experiments demonstrate that our focused crawler is able to stay focused in domain and language. The first experiment shows that the crawler stays in a focused domain, the second experiment demonstrates that language models trained on focused crawls obtain better perplexity scores on in-domain corpora. We distribute the focused crawler as open source software."
L16-1656,{S}em{R}el{D}ata â Multilingual Contextual Annotation of Semantic Relations between Nominals: Dataset and Guidelines,2016,15,2,2,0.740741,32428,darina benikova,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Semantic relations play an important role in linguistic knowledge representation. Although their role is relevant in the context of written text, there is no approach or dataset that makes use of contextuality of classic semantic relations beyond the boundary of one sentence. We present the SemRelData dataset that contains annotations of semantic relations between nominals in the context of one paragraph. To be able to analyse the universality of this context notion, the annotation was performed on a multi-lingual and multi-genre corpus. To evaluate the dataset, it is compared to large, manually created knowledge resources in the respective languages. The comparison shows that knowledge bases not only have coverage gaps; they also do not account for semantic relations that are manifested in particular contexts only, yet still play an important role for text cohesion."
C16-2049,Demonstrating Ambient Search: Implicit Document Retrieval for Speech Streams,2016,13,0,5,0,5570,benjamin milde,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"In this demonstration paper we describe Ambient Search, a system that displays and retrieves documents in real time based on speech input. The system operates continuously in ambient mode, i.e. it generates speech transcriptions and identifies main keywords and keyphrases, while also querying its index to display relevant documents without explicit query. Without user intervention, the results are dynamically updated; users can choose to interact with the system at any time, employing a conversation protocol that is enriched with the ambient information gathered continuously. Our evaluation shows that Ambient Search outperforms another implicit speech-based information retrieval system. Ambient search is available as open source software."
C16-1196,Ambient Search: A Document Retrieval System for Speech Streams,2016,18,2,5,0,5570,benjamin milde,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We present Ambient Search, an open source system for displaying and retrieving relevant documents in real time for speech input. The system works ambiently, that is, it unobstructively listens to speech streams in the background, identifies keywords and keyphrases for query construction and continuously serves relevant documents from its index. Query terms are ranked with Word2Vec and TF-IDF and are continuously updated to allow for ongoing querying of a document collection. The retrieved documents, in our case Wikipedia articles, are visualized in real time in a browser interface. Our evaluation shows that Ambient Search compares favorably to another implicit information retrieval system on speech streams. Furthermore, we extrinsically evaluate multiword keyphrase generation, showing positive impact for manual transcriptions."
R15-1027,Distributional Semantics for Resolving Bridging Mentions,2015,32,3,3,0,37352,tim feuerbach,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"We explore the impact of adding distributional knowledge to a state-of-the-art coreference resolution system. By integrating features based on word and context expansions from a distributional thesaurus (DT), automatically mined IS-A relationships and shallow syntactical clues into the Berkeley system (Durrett and Klein, 2013), we are able to increase its F1 score on bridging mentions, i.e. coreferent mentions with non-identical heads, by 8.29 points. Our semantic features improve over the Web-based features of Bansal and Klein (2012). Since bridging mentions are a hard but infrequent class of coreference, this leads to merely small improvements in the overall system."
P15-4018,{J}o{B}im{V}iz: A Web-based Visualization for Graph-based Distributional Semantic Models,2015,22,9,4,1,17254,eugen ruppert,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"This paper introduces a web-based visualization framework for graph-based distributional semantic models. The visualization supports a wide range of data structures, including term similarities, similarities of contexts, support of multiword expressions, sense clusters for terms and sense labels. In contrast to other browsers of semantic resources, our visualization accepts input sentences, which are subsequently processed with languageindependent or language-dependent ways to compute term-context representations. Our web demonstrator currently contains models for multiple languages, based on different preprocessing such as dependency parsing and n-gram context representations. These models can be accessed from a database, the web interface and via a RESTful API. The latter facilitates the quick integration of such models in research prototypes."
N15-1098,Do Supervised Distributional Methods Really Learn Lexical Inference Relations?,2015,27,100,3,0,3267,omer levy,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Distributional representations of words have been recently used in supervised settings for recognizing lexical inference relations between word pairs, such as hypernymy and entailment. We investigate a collection of these state-of-the-art methods, and show that they do not actually learn a relation between two words. Instead, they learn an independent property of a single word in the pair: whether that word is a xe2x80x9cprototypical hypernymxe2x80x9d."
J15-2006,"Book Reviews: Ontology-Based Interpretation of Natural Language by Philipp Cimiano, Christina Unger and John {M}c{C}rae",2015,-1,-1,1,1,2565,chris biemann,Computational Linguistics,0,None
D15-1290,A Single Word is not Enough: Ranking Multiword Expressions Using Distributional Semantics,2015,25,15,2,1,24663,martin riedl,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a new unsupervised mechanism, which ranks word n-grams according to their multiwordness. It heavily relies on a new uniqueness measure that computes, based on a distributional thesaurus, how often an n-gram could be replaced in context by a single-worded term. In addition with a downweighting mechanism for incomplete terms this forms a new measure called DRUID. Results show large improvements on two small test sets over competitive baselines. We demonstrate the scalability of the method to large corpora, and the independence of the measure of shallow syntactic filtering."
W14-5117,Multiobjective Optimization and Unsupervised Lexical Acquisition for Named Entity Recognition and Classification,2014,0,2,3,0,38305,govind,Proceedings of the 11th International Conference on Natural Language Processing,0,None
P14-5016,Automatic Annotation Suggestions and Custom Annotation Layers in {W}eb{A}nno,2014,14,21,2,1,282,seid yimam,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"In this paper, we present a flexible approach to the efficient and exhaustive manual annotation of text documents. For this purpose, we extend WebAnno (Yimam et al., 2013) an open-source web-based annotation tool. 1 While it was previously limited to specific annotation layers, our extension allows adding and configuring an arbitrary number of layers through a web-based UI. These layers can be annotated separately or simultaneously, and support most types of linguistic annotations such as spans, semantic classes, dependency relations, lexical chains, and morphology. Further, we tightly integrate a generic machine learning component for automatic annotation suggestions of span annotations. In two case studies, we show that automatic annotation suggestions, combined with our split-pane UI concept, significantly reduces annotation time."
P14-1096,That{'}s sick dude!: Automatic identification of word sense change across different timescales,2014,28,36,4,0,39200,sunny mitra,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we propose an unsupervised method to identify noun sense changes based on rigorous analysis of time-varying text data available in the form of millions of digitized books. We construct distributional thesauri based networks from data at different time points and cluster each of them separately to obtain word-centric sense clusters corresponding to the different time points. Subsequently, we compare these sense clusters of two different time points to find if (i) there is birth of a new sense or (ii) if an older sense has got split into more than one sense or (iii) if a newer sense has been formed from the joining of older senses or (iv) if a particular sense has died. We conduct a thorough evaluation of the proposed methodology both manually as well as through comparison with WordNet. Manual evaluation indicates that the algorithm could correctly identify 60.4% birth cases from a set of 48 randomly picked samples and 57% split/join cases from a set of 21 randomly picked samples. Remarkably, in 44% cases the birth of a novel sense is attested by WordNet, while in 46% cases and 43% cases split and join are respectively confirmed by WordNet. Our approach can be applied for lexicography, as well as for applications like word sense disambiguation or semantic search."
riedl-etal-2014-distributed,Distributed Distributional Similarities of {G}oogle {B}ooks Over the Centuries,2014,14,6,3,1,24663,martin riedl,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper introduces a distributional thesaurus and sense clusters computed on the complete Google Syntactic N-grams, which is extracted from Google Books, a very large corpus of digitized books published between 1520 and 2008. We show that a thesaurus computed on such a large text basis leads to much better results than using smaller corpora like Wikipedia. We also provide distributional thesauri for equal-sized time slices of the corpus. While distributional thesauri can be used as lexical resources in NLP tasks, comparing word similarities over time can unveil sense change of terms across different decades or centuries, and can serve as a resource for diachronic lexicography. Thesauri and clusters are available for download."
benikova-etal-2014-nosta,{N}o{S}ta-{D} Named Entity Annotation for {G}erman: Guidelines and Dataset,2014,16,26,2,0.740741,32428,darina benikova,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We describe the annotation of a new dataset for German Named Entity Recognition (NER). The need for this dataset is motivated by licensing issues and consistency issues of existing datasets. We describe our approach to creating annotation guidelines based on linguistic and semantic considerations, and how we iteratively refined and tested them in the early stages of annotation in order to arrive at the largest publicly available dataset for German NER, consisting of over 31,000 manually annotated sentences (over 591,000 tokens) from German Wikipedia and German online news. We provide a number of statistics on the dataset, which indicate its high quality, and discuss legal aspects of distributing the data as a compilation of citations. The data is released under the permissive CC-BY license, and will be fully available for download in September 2014 after it has been used for the GermEval 2014 shared task on NER. We further provide the full annotation guidelines and links to the annotation tool used for the creation of this resource."
cholakov-etal-2014-lexical,Lexical Substitution Dataset for {G}erman,2014,10,4,2,0,33554,kostadin cholakov,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article describes a lexical substitution dataset for German. The whole dataset contains 2,040 sentences from the German Wikipedia, with one target word in each sentence. There are 51 target nouns, 51 adjectives, and 51 verbs randomly selected from 3 frequency groups based on the lemma frequency list of the German WaCKy corpus. 200 sentences have been annotated by 4 professional annotators and the remaining sentences by 1 professional annotator and 5 additional annotators who have been recruited via crowdsourcing. The resulting dataset can be used to evaluate not only lexical substitution systems, but also different sense inventories and word sense disambiguation systems."
C14-1136,Combining Supervised and Unsupervised Parsing for Distributional Similarity,2014,34,2,3,1,24663,martin riedl,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we address the role of syntactic parsing for distributional similarity. On the one hand, we are exploring distributional similarities as an extrinsic test bed for unsupervised parsers. On the other hand, we explore whether single unsupervised parsers, or their combination, can contribute to better distributional similarities, or even replace supervised parsing as a preprocessing step for word similarity. We evaluate distributional thesauri against manually created taxonomies both for English and German for five unsupervised parsers. While for English, a supervised parser is the best single parser in this evaluation, we find an unsupervised parser to work best for German. For both languages, we show significant improvements in word similarity when combining features from supervised and unsupervised parsers. To our knowledge, this is the first work where unsupervised parsers are systematically evaluated extrinsically in a semantic task, and the first work to show that unsupervised parsing can complement and even replace supervised parsing, when used as a pre-processing feature."
W13-5002,{J}o{B}im{T}ext Visualizer: A Graph-based Approach to Contextualizing Distributional Similarity,2013,27,6,1,1,2565,chris biemann,Proceedings of {T}ext{G}raphs-8 Graph-based Methods for Natural Language Processing,0,"We introduce an interactive visualization component for the JoBimText project. JoBimText is an open source platform for large-scale distributional semantics based on graph representations. First we describe the underlying technology for computing a distributional thesaurus on words using bipartite graphs of words and context features, and contextualizing the list of semantically similar words towards a given sentential context using graphbased ranking. Then we demonstrate the capabilities of this contextualized text expansion technology in an interactive visualization. The visualization can be used as a semantic parser providing contextualized expansions of words in text as well as disambiguation to word senses induced by graph clustering, and is provided as an open source tool."
W13-5006,From Global to Local Similarities: A Graph-Based Contextualization Method using Distributional Thesauri,2013,24,1,2,1,24663,martin riedl,Proceedings of {T}ext{G}raphs-8 Graph-based Methods for Natural Language Processing,0,"After recasting the computation of a distributional thesaurus in a graph-based framework for term similarity, we introduce a new contextualization method that generates, for each term occurrence in a text, a ranked list of terms that are semantically similar and compatible with the given context. The framework is instantiated by the definition of term and context, which we derive from dependency parses in this work. Evaluating our approach on a standard data set for lexical substitution, we show substantial improvements over a strong non-contextualized baseline across all parts of speech. In contrast to comparable approaches, our framework defines an unsupervised generative method for similarity in context and does not rely on the existence of lexical resources as a source for candidate expansions."
W13-1409,Exploring Cities in Crime: Significant Concordance and Co-occurrence in Quantitative Literary Analysis,2013,38,3,4,0,41087,janneke rauscher,Proceedings of the Workshop on Computational Linguistics for Literature,0,"We present CoocViewer, a graphical analysis tool for the purpose of quantitative literary analysis, and demonstrate its use on a corpus of crime novels. The tool displays words, their significant co-occurrences, and contains a new visualization for significant concordances. Contexts of words and co-occurrences can be displayed. After reviewing previous research and current challenges in the newly emerging field of quantitative literary research, we demonstrate how CoocViewer allows comparative research on literary corpora in a project-specific study, and how we can confirm or enhance our hypotheses through quantitative literary analysis."
S13-2007,{S}em{E}val-2013 Task 5: Evaluating Phrasal Semantics,2013,17,19,4,0,39671,ioannis korkontzelos,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"This paper describes the SemEval-2013 Task 5: xe2x80x9cEvaluating Phrasal Semanticsxe2x80x9d. Its first subtask is about computing the semantic similarity of words and compositional phrases of minimal length. The second one addresses deciding the compositionality of phrases in a given context. The paper discusses the importance and background of these subtasks and their structure. In succession, it introduces the systems that participated and discusses evaluation results."
P13-4001,"{W}eb{A}nno: A Flexible, Web-based and Visually Supported System for Distributed Annotations",2013,11,61,4,1,282,seid yimam,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present WebAnno, a general purpose web-based annotation tool for a wide range of linguistic annotations. WebAnno offers annotation project management, freely configurable tagsets and the management of users in different roles. WebAnno uses modern web technology for visualizing and editing annotations in a web browser. It supports arbitrarily large documents, pluggable import/export filters, the curation of annotations across various users, and an interface to farming out annotations to a crowdsourcing platform. Currently WebAnno allows part-ofspeech, named entity, dependency parsing and co-reference chain annotations. The architecture design allows adding additional modes of visualization and editing, when new kinds of annotations are to be supported."
N13-1119,Three Knowledge-Free Methods for Automatic Lexical Chain Extraction,2013,37,10,2,1,18079,steffen remus,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present three approaches to lexical chaining based on the LDA topic model and evaluate them intrinsically on a manually annotated set of German documents. After motivating the choice of statistical methods for lexical chaining with their adaptability to different languages and subject domains, we describe our new two-level chain annotation scheme, which rooted in the concept of cohesive harmony. Also, we propose a new measure for direct evaluation of lexical chains. Our three LDA-based approaches outperform two knowledge-based state-of-the art methods to lexical chaining by a large margin, which can be attributed to lacking coverage of the knowledge resource. Subsequent analysis shows that the three methods yield a different chaining behavior, which could be utilized in tasks that use lexical chaining as a component within NLP applications."
N13-1133,Supervised All-Words Lexical Substitution using Delexicalized Features,2013,33,21,2,0,20401,gyorgy szarvas,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary. Instead of learning word-specific substitution patterns, a global model for lexical substitution is trained on delexicalized (i.e., non lexical) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. This way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. Using features from lexical resources, as well as a variety of features computed from large corpora (n-gram counts, distributional similarity) and a ranking method based on the posterior probabilities obtained from a Maximum Entropy classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets."
D13-1089,Scaling to Large{\\mbox{$^3$}} Data: An Efficient and Effective Method to Compute Distributional Thesauri,2013,28,14,2,1,24663,martin riedl,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We introduce a new highly scalable approach for computing Distributional Thesauri (DTs). By employing pruning techniques and a distributed framework, we make the computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora."
W12-3307,{T}opic{T}iling: A Text Segmentation Algorithm based on {LDA},2012,18,51,2,1,24663,martin riedl,Proceedings of {ACL} 2012 Student Research Workshop,0,"This work presents a Text Segmentation algorithm called TopicTiling. This algorithm is based on the well-known TextTiling algorithm, and segments documents using the Latent Dirichlet Allocation (LDA) topic model. We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics. We show significant improvements over state of the art segmentation algorithms on two standard datasets. As an additional benefit, TopicTiling performs the segmentation in linear time and thus is computationally less expensive than other LDA-based segmentation methods."
W12-0703,Sweeping through the Topic Space: Bad luck? Roll again!,2012,18,5,2,1,24663,martin riedl,Proceedings of the Joint Workshop on Unsupervised and Semi-Supervised Learning in {NLP},0,"Topic Models (TM) such as Latent Dirichlet Allocation (LDA) are increasingly used in Natural Language Processing applications. At this, the model parameters and the influence of randomized sampling and inference are rarely examined --- usually, the recommendations from the original papers are adopted. In this paper, we examine the parameter space of LDA topic models with respect to the application of Text Segmentation (TS), specifically targeting error rates and their variance across different runs. We find that the recommended settings result in error rates far from optimal for our application. We show substantial variance in the results for different runs of model estimation and inference, and give recommendations for increasing the robustness and stability of topic models. Running the inference step several times and selecting the last topic ID assigned per token, shows considerable improvements. Similar improvements are achieved with the mode method: We store all assigned topic IDs during each inference iteration step and select the most frequent topic ID assigned to each word. These recommendations do not only apply to TS, but are generic enough to transfer to other applications."
S12-1059,{UKP}: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures,2012,36,150,2,0,41371,daniel bar,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We present the UKP system which performed best in the Semantic Textual Similarity (STS) task at SemEval-2012 in two out of three metrics. It uses a simple log-linear regression model, trained on the training data, to combine multiple text similarity measures of varying complexity. These range from simple character and word n-grams and common subsequences to complex features such as Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300 features implemented."
N12-1064,How Text Segmentation Algorithms Gain from Topic Models,2012,15,18,2,1,24663,martin riedl,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper introduces a general method to incorporate the LDA Topic Model into text segmentation algorithms. We show that semantic information added by Topic Models significantly improves the performance of two word-based algorithms, namely TextTiling and C99. Additionally, we introduce the new TopicTiling algorithm that is designed to take better advantage of topic information. We show consistent improvements over word-based methods and achieve state-of-the art performance on a standard dataset."
biemann-2012-turk,Turk Bootstrap Word Sense Inventory 2.0: A Large-Scale Resource for Lexical Substitution,2012,12,16,1,1,2565,chris biemann,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents the Turk Bootstrap Word Sense Inventory (TWSI) 2.0. This lexical resource, created by a crowdsourcing process using Amazon Mechanical Turk (http://www.mturk.com), encompasses a sense inventory for lexical substitution for 1,012 highly frequent English common nouns. Along with each sense, a large number of sense-annotated occurrences in context are given, as well as a weighted list of substitutions. Sense distinctions are not motivated by lexicographic considerations, but driven by substitutability: two usages belong to the same sense if their substitutions overlap considerably. After laying out the need for such a resource, the data is characterized in terms of organization and quantity. Then, we briefly describe how this data was used to create a system for lexical substitutions. Training a supervised lexical substitution system on a smaller version of the resource resulted in well over 90{\%} acceptability for lexical substitutions provided by the system. Thus, this resource can be used to set up reliable, enabling technologies for semantic natural language processing (NLP), some of which we discuss briefly."
J12-1007,Book Review: Graph-Based Natural Language Processing and Information Retrieval by Rada Mihalcea and Dragomir Radev,2012,8,0,1,1,2565,chris biemann,Computational Linguistics,0,None
C12-1017,Quantifying Semantics using Complex Network Analysis,2012,32,27,1,1,2565,chris biemann,Proceedings of {COLING} 2012,0,"Though it is generally accepted that language models do not capture all aspects of real language, no adequate measures to quantify their shortcomings have been proposed until now. We will use n-gram models as workhorses to demonstrate that the differences between natural and generated language are indeed quantifiable. More specifically, for two algorithmic approaches, we demonstrate that each of them can be used to distinguish real text from generated text accurately and to quantify the difference. Therefore, we obtain a coherent indication how far a language model is from naturalness. Both methods are based on the analysis of co-occurrence networks: a specific graph cluster measure, the transitivity, and a specific kind of motif analysis, where the frequencies of selected motifs are compared. In our study, artificial texts are generated by n-gram models, for n = 2, 3, 4. We found that, the larger n is chosen, the narrower the distance between generated and natural text is. However, even for n = 4, the distance is still large enough to allow an accurate distinction. The motif approach even allows a deeper insight into those semantic properties of natural language that evidently cause these differences: polysemy and synonymy. To complete the picture, we show that another motif-based approach by Milo et al. (2004) does not allow such a distinction. Using our method, it becomes possible for the first time to measure generative language models deficiencies with regard to semantics of natural language."
C12-1109,Using Distributional Similarity for Lexical Expansion in Knowledge-based Word Sense Disambiguation,2012,34,42,2,0,1740,tristan miller,Proceedings of {COLING} 2012,0,"We explore the contribution of distributional information for purely knowledge-based word sense disambiguation. Specifically, we use a distributional thesaurus, computed from a large parsed corpus, for lexical expansion of context and sense information. This bridges the lexical gap that is seen as the major obstacle for word overlapxe2x80x90based approaches. We apply this mechanism to two traditional knowledge-based methods and show that distributional information significantly improves disambiguation results across several data sets. This improvement exceeds the state of the art for disambiguation without sense frequency informationxe2x80x94a situation which is especially encountered with new domains or languages for which no sense-annotated corpus is available."
W11-1304,Distributional Semantics and Compositionality 2011: Shared Task Description and Results,2011,5,35,1,1,2565,chris biemann,Proceedings of the Workshop on Distributional Semantics and Compositionality,0,"This paper gives an overview of the shared task at the ACL-HLT 2011 DiSCo (Distributional Semantics and Compositionality) workshop. We describe in detail the motivation for the shared task, the acquisition of datasets, the evaluation methodology and the results of participating systems. The task of assigning a numerical score for a phrase according to its compositionality showed to be hard. Many groups reported features that intuitively should work, yet showed no correlation with the training data. The evaluation reveals that most systems outperform simple baselines, yet have difficulties in reliably assigning a compositionality score that closely matches the gold standard. Overall, approaches based on word space models performed slightly better than methods relying solely on statistical association measures."
W10-2309,Co-Occurrence Cluster Features for Lexical Substitutions in Context,2010,10,13,1,1,2565,chris biemann,Proceedings of {T}ext{G}raphs-5 - 2010 Workshop on Graph-based Methods for Natural Language Processing,0,"This paper examines the influence of features based on clusters of co-occurrences for supervised Word Sense Disambiguation and Lexical Substitution. Co-occurrence cluster features are derived from clustering the local neighborhood of a target word in a co-occurrence graph based on a corpus in a completely unsupervised fashion. Clusters can be assigned in context and are used as features in a supervised WSD system. Experiments fitting a strong baseline system with these additional features are conducted on two datasets, showing improvements. Co-occurrence features are a simple way to mimic Topic Signatures (Martinez et al., 2008) without needing to construct resources manually. Further, a system is described that produces lexical substitutions in context with very high precision."
P09-2062,Syntax is from {M}ars while Semantics from {V}enus! Insights from Spectral Analysis of Distributional Similarity Networks,2009,10,6,1,1,2565,chris biemann,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"We study the global topology of the syntactic and semantic distributional similarity networks for English through the technique of spectral analysis. We observe that while the syntactic network has a hierarchical structure with strong communities and their mixtures, the semantic network has several tightly knit communities along with a large core without any such well-defined community structure."
biemann-etal-2008-asv,{ASV} Toolbox: a Modular Collection of Language Exploration Tools,2008,27,22,1,1,2565,chris biemann,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"ASV Toolbox is a modular collection of tools for the exploration of written language data both for scientific and educational purposes. It includes modules that operate on word lists or texts and allow to perform various linguistic annotation, classification and clustering tasks, including language detection, POS-tagging, base form reduction, named entity recognition, and terminology extraction. On a more abstract level, the algorithms deal with various kinds of word similarity, using pattern-based and statistical approaches. The collection can be used to work on large real-world data sets as well as for studying the underlying algorithms. Each module of the ASV Toolbox is designed to work either on a plain text files or with a connection to a MySQL database. While it is especially designed to work with corpora of the Leipzig Corpora Collection, it can easily be adapted to other sources."
W07-2425,Combining Contexts in Lexicon Learning for Semantic Parsing,2007,15,0,2,0,3343,richard socher,Proceedings of the 16th Nordic Conference of Computational Linguistics ({NODALIDA} 2007),0,"We introduce a method for the automatic construction of noun entries in a semantic lexicon. Using the entries already present in the lexicon, semantic features are inherited from known to yet unknown words along similar contexts. As contexts, we use three specific syntactic-semantic relations: modifying adjective, verb-deep-subject and verbdeep-object. The combination of evidences from different contexts yields very high precision for most semantic features, giving rise to the fully automatic incorporation into the lexicon."
W07-2445,{\\'I}slenskur Or{\\dh}asj{\\'o}{\\dh}ur {--} Building a Large {I}celandic Corpus,2007,-1,-1,3,0,39766,erla hallsteinsdottir,Proceedings of the 16th Nordic Conference of Computational Linguistics ({NODALIDA} 2007),0,None
N07-3010,Unsupervised Natural Language Processing Using Graph Models,2007,15,1,1,1,2565,chris biemann,Proceedings of the {NAACL}-{HLT} 2007 Doctoral Consortium,0,"In the past, NLP has always been based on the explicit or implicit use of linguistic knowledge. In classical computer linguistic applications explicit rule based approaches prevail, while machine learning algorithms use implicit knowledge for generating linguistic knowledge. The question behind this work is: how far can we go in NLP without assuming explicit or implicit linguistic knowledge? How much efforts in annotation and resource building are needed for what level of sophistication in text processing? This work tries to answer the question by experimenting with algorithms that do not presume any linguistic knowledge in the system. The claim is that the knowledge needed can largely be acquired by knowledge-free and unsupervised methods. Here, graph models are employed for representing language data. A new graph clustering method finds related lexical units, which form word sets on various levels of homogeneity. This is exemplified and evaluated on language separation and unsupervised part-of-speech tagging, further applications are discussed."
N07-1014,A Random Text Model for the Generation of Statistical Language Invariants,2007,18,7,1,1,2565,chris biemann,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Abstract A novel random text generation model is introduced. Unlike in previous random text models, that mainly aim at producing a Zipfian distribution of word frequencies, our model also takes the properties of neighboring co-occurrence into account and introduces the notion of sentences in random text. After pointing out the defi-ciencies of related models, we provide a generation process that takes neither the Zipfian distribution on word frequencies nor the small-world structure of the neighboring co-occurrence graph as a constraint. Nevertheless, these distribu-tions emerge in the process. The distribu-tions obtained with the random generation model are compared to a sample of natu-ral language data, showing high agree-ment also on word length and sentence length. This work proposes a plausible model for the emergence of large-scale characteristics of language without as-suming a grammar or semantics. 1 Introduction G. K. Zipf (1949) discovered that if all words in a sample of natural language are arranged in de-creasing order of frequency, then the relation be-tween a wordxe2x80x99s frequency and its rank in the list follows a power-law. Since then, a significant amount of research in the area of quantitative lin-guistics has been devoted to the question how this property emerges and what kind of processes gen-erate such Zipfian distributions. The relation between the frequency of a word at rank r and its rank is given by f(r) xc2xb5 r"
W06-3812,{C}hinese Whispers - an Efficient Graph Clustering Algorithm and its Application to Natural Language Processing Problems,2006,16,216,1,1,2565,chris biemann,Proceedings of {T}ext{G}raphs: the First Workshop on Graph Based Methods for Natural Language Processing,0,"We introduce Chinese Whispers, a randomized graph-clustering algorithm, which is time-linear in the number of edges. After a detailed definition of the algorithm and a discussion of its strengths and weaknesses, the performance of Chinese Whispers is measured on Natural Language Processing (NLP) problems as diverse as language separation, acquisition of syntactic word classes and word sense disambiguation. At this, the fact is employed that the small-world property holds for many graphs in NLP."
P06-3002,Unsupervised Part-of-Speech Tagging Employing Efficient Graph Clustering,2006,7,161,1,1,2565,chris biemann,Proceedings of the {COLING}/{ACL} 2006 Student Research Workshop,0,"An unsupervised part-of-speech (POS) tagging system that relies on graph clustering methods is described. Unlike in current state-of-the-art approaches, the kind and number of different tags is generated by the method itself. We compute and merge two partitionings of word graphs: one based on context similarity of high frequency words, another on log-likelihood statistics for words of lower frequencies. Using the resulting word clusters as a lexicon, a Viterbi POS tagger is trained, which is refined by a morphological component. The approach is evaluated on three different languages by measuring agreement with existing taggers."
W05-1704,Dictionary acquisition using parallel text and co-occurrence statistics,2006,-1,-1,1,1,2565,chris biemann,Proceedings of the 15th Nordic Conference of Computational Linguistics ({NODALIDA} 2005),0,None
W05-1729,Rigorous dimensionality reduction through linguistically motivated feature selection for text categorization,2006,12,6,2,0,49073,hans witschel,Proceedings of the 15th Nordic Conference of Computational Linguistics ({NODALIDA} 2005),0,None
biemann-etal-2004-automatic,Automatic Acquisition of Paradigmatic Relations Using Iterated Co-occurrences,2004,24,28,1,1,2565,chris biemann,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We introduce the notion of iterated co-occurrences, which can be obtained by performing the calculation of statistically significant cooccurrences not on sentence level, but on co-occurrence sets of previous calculations. The underlying mechanisms are explained in detail and we give reasons, why this iteration results in sets of semantically homogeneous words. These can be used for the automatic acquisition of paradigmatic relations in order to semi-automatically extend lexical-semantic word nets or thesauri, widening the acquisition bottleneck. A small evaluation for synset expansion for German language and some discussion conclude the work."
C04-1178,Semiautomatic Extension of {C}ore{N}et using a Bootstrapping Mechanism on Corpus-based Co-occurrences,2004,8,6,1,1,2565,chris biemann,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"The paper describes a language-independent approach for semiautomatic extension of lexical-semantic word nets and evaluates the method on CoreNet, the Korean version of word net. In a bootstrapping fashion, the so-called 'Pendulum Algorithm' operates on word sets obtained by co-occurrence statistics on a large un-annotated corpus and keeps error propagation low by a verification step. Results are not sufficient for automatic extension, but provide a good candidate set. Further improvements are discussed."
