2021.semeval-1.41,{S}em{E}val-2021 Task 12: Learning with Disagreements,2021,-1,-1,8,0,1738,alexandra uma,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"Disagreement between coders is ubiquitous in virtually all datasets annotated with human judgements in both natural language processing and computer vision. However, most supervised machine learning methods assume that a single preferred interpretation exists for each item, which is at best an idealization. The aim of the SemEval-2021 shared task on learning with disagreements (Le-Wi-Di) was to provide a unified testing framework for methods for learning from data containing multiple and possibly contradictory annotations covering the best-known datasets containing information about disagreements for interpreting language and classifying images. In this paper we describe the shared task and its results."
2021.naacl-main.204,Beyond Black {\\&} White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning,2021,-1,-1,6,0.799087,477,tommaso fornaciari,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Supervised learning assumes that a ground truth label exists. However, the reliability of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training models. We propose a novel method to incorporate this disagreement as information: in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the divergence between the predictions and the target soft-labels with several loss-functions and evaluate the models on various NLP tasks. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates overfitting. It significantly improves performance across tasks, beyond the standard approach and prior work."
2021.naacl-main.329,Stay Together: A System for Single and Split-antecedent Anaphora Resolution,2021,-1,-1,4,1,4243,juntao yu,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in {``}Time-Warner is considering a legal challenge to Telecommunications Inc{'}s plan to buy half of Showtime Networks Inc{--}a move that could lead to all-out war between the two powerful companies{''}. Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics."
2021.findings-emnlp.226,Patterns of Polysemy and Homonymy in Contextualised Language Models,2021,-1,-1,2,1,6979,janosch haber,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"One of the central aspects of contextualised language models is that they should be able to distinguish the meaning of lexically ambiguous words by their contexts. In this paper we investigate the extent to which the contextualised embeddings of word forms that display multiplicity of sense reflect traditional distinctions of polysemy and homonymy. To this end, we introduce an extended, human-annotated dataset of graded word sense similarity and co-predication acceptability, and evaluate how well the similarity of embeddings predicts similarity in meaning. Both types of human judgements indicate that the similarity of polysemic interpretations falls in a continuum between identity of meaning and homonymy. However, we also observe significant differences within the similarity ratings of polysemes, forming consistent patterns for different types of polysemic sense alternation. Our dataset thus appears to capture a substantial part of the complexity of lexical ambiguity, and can provide a realistic test bed for contextualised embeddings. Among the tested models, BERT Large shows the strongest correlation with the collected word sense similarity ratings, but struggles to consistently replicate the observed similarity patterns. When clustering ambiguous word forms based on their embeddings, the model displays high confidence in discerning homonyms and some types of polysemic alternations, but consistently fails for others."
2021.eacl-main.232,{BERT}ective: Language Models and Contextual Information for Deception Detection,2021,-1,-1,3,0.799087,477,tommaso fornaciari,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Spotting a lie is challenging but has an enormous potential impact on security as well as private and public safety. Several NLP methods have been proposed to classify texts as truthful or deceptive. In most cases, however, the target texts{'} preceding context is not considered. This is a severe limitation, as any communication takes place in context, not in a vacuum, and context can help to detect deception. We study a corpus of Italian dialogues containing deceptive statements and implement deep neural models that incorporate various linguistic contexts. We establish a new state-of-the-art identifying deception and find that not all context is equally useful to the task. Only the texts closest to the target, if from the same speaker (rather than questions by an interlocutor), boost performance. We also find that the semantic information in language models such as BERT contributes to the performance. However, BERT alone does not capture the implicit knowledge of deception cues: its contribution is conditional on the concurrent use of attention to learn cues from BERT{'}s representations."
2021.crac-1.2,Coreference Resolution for the Biomedical Domain: A Survey,2021,-1,-1,2,0,11304,pengcheng lu,"Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference",0,"Issues with coreference resolution are one of the most frequently mentioned challenges for information extraction from the biomedical literature. Thus, the biomedical genre has long been the second most researched genre for coreference resolution after the news domain, and the subject of a great deal of research for NLP in general. In recent years this interest has grown enormously leading to the development of a number of substantial datasets, of domain-specific contextual language models, and of several architectures. In this paper we review the state of-the-art of coreference in the biomedical domain with a particular attention on these most recent developments."
2021.crac-1.9,Data Augmentation Methods for Anaphoric Zero Pronouns,2021,-1,-1,2,1,11317,abdulrahman aloraini,"Proceedings of the Fourth Workshop on Computational Models of Reference, Anaphora and Coreference",0,"In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and many others, unrealized (null) arguments in certain syntactic positions can refer to a previously introduced entity, and are thus called anaphoric zero pronouns. The existing resources for studying anaphoric zero pronoun interpretation are however still limited. In this paper, we use five data augmentation methods to generate and detect anaphoric zero pronouns automatically. We use the augmented data as additional training materials for two anaphoric zero pronoun systems for Arabic. Our experimental results show that data augmentation improves the performance of the two systems, surpassing the state-of-the-art results."
2021.codi-sharedtask.1,"The {CODI}-{CRAC} 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue",2021,-1,-1,5,0,2579,sopan khosla,"Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue",0,"In this paper, we provide an overview of the CODI-CRAC 2021 Shared-Task: Anaphora Resolution in Dialogue. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple subtasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these subtasks, and provide a brief summary of the participating systems and the results obtained across ?? runs from 5 teams, with most submissions achieving significantly better results than our baseline methods."
2021.bppf-1.3,We Need to Consider Disagreement in Evaluation,2021,-1,-1,7,0,7,valerio basile,"Proceedings of the 1st Workshop on Benchmarking: Past, Present and Future",0,"Evaluation is of paramount importance in data-driven research fields such as Natural Language Processing (NLP) and Computer Vision (CV). Current evaluation practice largely hinges on the existence of a single {``}ground truth{''} against which we can meaningfully compare the prediction of a model. However, this comparison is flawed for two reasons. 1) In many cases, more than one answer is correct. 2) Even where there is a single answer, disagreement among annotators is ubiquitous, making it difficult to decide on a gold standard. We argue that the current methods of adjudication, agreement, and evaluation need serious reconsideration. Some researchers now propose to minimize disagreement and to fix datasets. We argue that this is a gross oversimplification, and likely to conceal the underlying complexity. Instead, we suggest that we need to better capture the sources of disagreement to improve today{'}s evaluation practice. We discuss three sources of disagreement: from the annotator, the data, and the context, and show how this affects even seemingly objective tasks. Datasets with multiple annotations are becoming more common, as are methods to integrate disagreement into modeling. The logical next step is to extend this to evaluation."
2020.wanlp-1.31,The {QMUL}/{HRBDT} contribution to the {NADI} {A}rabic Dialect Identification Shared Task,2020,-1,-1,2,1,11317,abdulrahman aloraini,Proceedings of the Fifth Arabic Natural Language Processing Workshop,0,"We present the Arabic dialect identification system that we used for the country-level subtask of the NADI challenge. Our model consists of three components: BiLSTM-CNN, character-level TF-IDF, and topic modeling features. We represent each tweet using these features and feed them into a deep neural network. We then add an effective heuristic that improves the overall performance. We achieved an F1-Macro score of 20.77{\%} and an accuracy of 34.32{\%} on the test set. The model was also evaluated on the Arabic Online Commentary dataset, achieving results better than the state-of-the-art."
2020.starsem-1.12,Assessing Polyseme Sense Similarity through Co-predication Acceptability and Contextualised Embedding Distance,2020,-1,-1,2,1,6979,janosch haber,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"Co-predication is one of the most frequently used linguistic tests to tell apart shifts in polysemic sense from changes in homonymic meaning. It is increasingly coming under criticism as evidence is accumulating that it tends to mis-classify specific cases of polysemic sense alteration as homonymy. In this paper, we collect empirical data to investigate these accusations. We asses how co-predication acceptability relates to explicit ratings of polyseme word sense similarity, and how well either measure can be predicted through the distance between target words{'} contextualised word embeddings. We find that sense similarity appears to be a major contributor in determining co-predication acceptability, but that co-predication judgements tend to rate especially less similar sense interpretations equally as unacceptable as homonym pairs, effectively mis-classifying these instances. The tested contextualised word embeddings fail to predict word sense similarity consistently, but the similarities between BERT embeddings show a significant correlation with co-predication ratings. We take this finding as evidence that BERT embeddings might be better representations of context than encodings of word meaning."
2020.pam-1.17,Word Sense Distance in Human Similarity Judgements and Contextualised Word Embeddings,2020,-1,-1,2,1,6979,janosch haber,Proceedings of the Probability and Meaning Conference (PaM 2020),0,"Homonymy is often used to showcase one of the advantages of context-sensitive word embedding techniques such as ELMo and BERT. In this paper we want to shift the focus to the related but less exhaustively explored phenomenon of polysemy, where a word expresses various distinct but related senses in different contexts. Specifically, we aim to i) investigate a recent model of polyseme sense clustering proposed by Ortega-Andres {\&} Vicente (2019) through analysing empirical evidence of word sense grouping in human similarity judgements, ii) extend the evaluation of context-sensitive word embedding systems by examining whether they encode differences in word sense similarity and iii) compare the word sense similarities of both methods to assess their correlation and gain some intuition as to how well contextualised word embeddings could be used as surrogate word sense similarity judgements in linguistic experiments."
2020.nlp4call-1.3,Polygloss - A conversational agent for language practice,2020,-1,-1,2,0,16413,etiene dalcol,Proceedings of the 9th Workshop on NLP for Computer Assisted Language Learning,0,None
2020.lrec-1.1,Neural Mention Detection,2020,-1,-1,3,1,4243,juntao yu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Mention detection is an important preprocessing step for annotation and interpretation in applications such as NER and coreference resolution, but few stand-alone neural models have been proposed able to handle the full range of mentions. In this work, we propose and compare three neural network-based approaches to mention detection. The first approach is based on the mention detection part of a state of the art coreference resolution system; the second uses ELMO embeddings together with a bidirectional LSTM and a biaffine classifier; the third approach uses the recently introduced BERT model. Our best model (using a biaffine classifier) achieves gains of up to 1.8 percentage points on mention recall when compared with a strong baseline in a HIGH RECALL coreference annotation setting. The same model achieves improvements of up to 5.3 and 6.2 p.p. when compared with the best-reported mention detection F1 on the CONLL and CRAC coreference data sets respectively in a HIGH F1 annotation setting. We then evaluate our models for coreference resolution by using mentions predicted by our best model in start-of-the-art coreference systems. The enhanced model achieved absolute improvements of up to 1.7 and 0.7 p.p. when compared with our strong baseline systems (pipeline system and end-to-end system) respectively. For nested NER, the evaluation of our model on the GENIA corpora shows that our model matches or outperforms state-of-the-art models despite not being specifically designed for this task."
2020.lrec-1.2,A Cluster Ranking Model for Full Anaphora Resolution,2020,-1,-1,3,1,4243,juntao yu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Anaphora resolution (coreference) systems designed for the CONLL 2012 dataset typically cannot handle key aspects of the full anaphora resolution task such as the identification of singletons and of certain types of non-referring expressions (e.g., expletives), as these aspects are not annotated in that corpus. However, the recently released dataset for the CRAC 2018 Shared Task can now be used for that purpose. In this paper, we introduce an architecture to simultaneously identify non-referring expressions (including expletives, predicative s, and other types) and build coreference chains, including singletons. Our cluster-ranking system uses an attention mechanism to determine the relative importance of the mentions in the same cluster. Additional classifiers are used to identify singletons and non-referring markables. Our contributions are as follows. First all, we report the first result on the CRAC data using system mentions; our result is 5.8{\%} better than the shared task baseline system, which used gold mentions. Second, we demonstrate that the availability of singleton clusters and non-referring expressions can lead to substantially improved performance on non-singleton clusters as well. Third, we show that despite our model not being designed specifically for the CONLL data, it achieves a score equivalent to that of the state-of-the-art system by Kantor and Globerson (2019) on that dataset."
2020.lrec-1.11,Cross-lingual Zero Pronoun Resolution,2020,-1,-1,2,1,11317,abdulrahman aloraini,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In languages like Arabic, Chinese, Italian, Japanese, Korean, Portuguese, Spanish, and many others, predicate arguments in certain syntactic positions are not realized instead of being realized as overt pronouns, and are thus called zero- or null-pronouns. Identifying and resolving such omitted arguments is crucial to machine translation, information extraction and other NLP tasks, but depends heavily on semantic coherence and lexical relationships. We propose a BERT-based cross-lingual model for zero pronoun resolution, and evaluate it on the Arabic and Chinese portions of OntoNotes 5.0. As far as we know, ours is the first neural model of zero-pronoun resolution for Arabic; and our model also outperforms the state-of-the-art for Chinese. In the paper we also evaluate BERT feature extraction and fine-tune models on the task, and compare them with our model. We also report on an investigation of BERT layers indicating which layer encodes the most suitable representation for the task."
2020.gamnlp-1.11,Aggregation Driven Progression System for {GWAP}s,2020,-1,-1,5,0,19286,osman kicikoglu,Workshop on Games and Natural Language Processing,0,"As the uses of Games-With-A-Purpose (GWAPs) broadens, the systems that incorporate its usages have expanded in complexity. The types of annotations required within the NLP paradigm set such an example, where tasks can involve varying complexity of annotations. Assigning more complex tasks to more skilled players through a progression mechanism can achieve higher accuracy in the collected data while acting as a motivating factor that rewards the more skilled players. In this paper, we present the progression technique implemented in Wormingo , an NLP GWAP that currently includes two layers of task complexity. For the experiment, we have implemented four different progression scenarios on 192 players and compared the accuracy and engagement achieved with each scenario."
2020.crac-1.3,Anaphoric Zero Pronoun Identification: A Multilingual Approach,2020,-1,-1,2,1,11317,abdulrahman aloraini,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference",0,"Pro-drop languages such as Arabic, Chinese, Italian or Japanese allow morphologically null but referential arguments in certain syntactic positions, called anaphoric zero-pronouns. Much NLP work on anaphoric zero-pronouns (AZP) is based on gold mentions, but models for their identification are a fundamental prerequisite for their resolution in real-life applications. Such identification requires complex language understanding and knowledge of real-world entities. Transfer learning models, such as BERT, have recently shown to learn surface, syntactic, and semantic information,which can be very useful in recognizing AZPs. We propose a BERT-based multilingual model for AZP identification from predicted zero pronoun positions, and evaluate it on the Arabic and Chinese portions of OntoNotes 5.0. As far as we know, this is the first neural network model of AZP identification for Arabic; and our approach outperforms the stateof-the-art for Chinese. Experiment results suggest that BERT implicitly encode information about AZPs through their surrounding context."
2020.crac-1.11,Neural Coreference Resolution for {A}rabic,2020,-1,-1,3,1,11317,abdulrahman aloraini,"Proceedings of the Third Workshop on Computational Models of Reference, Anaphora and Coreference",0,"No neural coreference resolver for Arabic exists, in fact we are not aware of any learning-based coreference resolver for Arabic since (Bj{\""o}rkelund and Kuhn, 2014). In this paper, we introduce a coreference resolution system for Arabic based on Lee et al{'}s end-to-end architecture combined with the Arabic version of bert and an external mention detector. As far as we know, this is the first neural coreference resolution system aimed specifically to Arabic, and it substantially outperforms the existing state-of-the-art on OntoNotes 5.0 with a gain of 15.2 points conll F1. We also discuss the current limitations of the task for Arabic and possible approaches that can tackle these challenges."
2020.coling-main.315,Multitask Learning-Based Neural Bridging Reference Resolution,2020,46,0,2,1,4243,juntao yu,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a multi task learning-based neural model for resolving bridging references tackling two key challenges. The first challenge is the lack of large corpora annotated with bridging references. To address this, we use multi-task learning to help bridging reference resolution with coreference resolution. We show that substantial improvements of up to 8 p.p. can be achieved on full bridging resolution with this architecture. The second challenge is the different definitions of bridging used in different corpora, meaning that hand-coded systems or systems using special features designed for one corpus do not work well with other corpora. Our neural model only uses a small number of corpus independent features, thus can be applied to different corpora. Evaluations with very different bridging corpora (ARRAU, ISNOTES, BASHI and SCICORP) suggest that our architecture works equally well on all corpora, and achieves the SoTA results on full bridging resolution for all corpora, outperforming the best reported results by up to 36.3 p.p.."
2020.coling-main.538,Free the Plural: Unrestricted Split-Antecedent Anaphora Resolution,2020,-1,-1,4,1,4243,juntao yu,Proceedings of the 28th International Conference on Computational Linguistics,0,"Now that the performance of coreference resolvers on the simpler forms of anaphoric reference has greatly improved, more attention is devoted to more complex aspects of anaphora. One limitation of virtually all coreference resolution models is the focus on single-antecedent anaphors. Plural anaphors with multiple antecedents-so-called split-antecedent anaphors (as in John met Mary. They went to the movies) have not been widely studied, because they are not annotated in ONTONOTES and are relatively infrequent in other corpora. In this paper, we introduce the first model for unrestricted resolution of split-antecedent anaphors. We start with a strong baseline enhanced by BERT embeddings, and show that we can substantially improve its performance by addressing the sparsity issue. To do this, we experiment with auxiliary corpora where split-antecedent anaphors were annotated by the crowd, and with transfer learning models using element-of bridging references and single-antecedent coreference as auxiliary tasks. Evaluation on the gold annotated ARRAU corpus shows that the out best model uses a combination of three auxiliary corpora achieved F1 scores of 70{\%} and 43.6{\%} when evaluated in a lenient and strict setting, respectively, i.e., 11 and 21 percentage points gain when compared with our baseline."
2020.cllrd-1.4,Speaking Outside the Box: Exploring the Benefits of Unconstrained Input in Crowdsourcing and Citizen Science Platforms,2020,-1,-1,3,1,1741,jon chamberlain,Proceedings of the LREC 2020 Workshop on ``Citizen Linguistics in Language Resource Development'',0,"Crowdsourcing approaches provide a difficult design challenge for developers. There is a trade-off between the efficiency of the task to be done and the reward given to the user for participating, whether it be altruism, social enhancement, entertainment or money. This paper explores how crowdsourcing and citizen science systems collect data and complete tasks, illustrated by a case study from the online language game-with-a-purpose Phrase Detectives. The game was originally developed to be a constrained interface to prevent player collusion, but subsequently benefited from posthoc analysis of over 76k unconstrained inputs from users. Understanding the interface design and task deconstruction are critical for enabling users to participate in such systems and the paper concludes with a discussion of the idea that social networks can be viewed as form of citizen science platform with both constrained and unconstrained inputs making for a highly complex dataset."
2020.acl-main.577,Named Entity Recognition as Dependency Parsing,2020,29,0,3,1,4243,juntao yu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Named Entity Recognition (NER) is a fundamental task in Natural Language Processing, concerned with identifying spans of text expressing references to entities. NER research is often focused on flat entities only (flat NER), ignoring the fact that entity references can be nested, as in [Bank of [China]] (Finkel and Manning, 2009). In this paper, we use ideas from graph-based dependency parsing to provide our model a global view on the input via a biaffine model (Dozat and Manning, 2017). The biaffine model scores pairs of start and end tokens in a sentence which we use to explore all spans, so that the model is able to predict named entities accurately. We show that the model works well for both nested and flat NER through evaluation on 8 corpora and achieving SoTA performance on all of them, with accuracy gains of up to 2.2 percentage points."
P19-1077,Crowdsourcing and Aggregating Nested Markable Annotations,2019,0,0,6,0,25588,chris madge,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"One of the key steps in language resource creation is the identification of the text segments to be annotated, or markables, which depending on the task may vary from nominal chunks for named entity resolution to (potentially nested) noun phrases in coreference resolution (or mentions) to larger text segments in text segmentation. Markable identification is typically carried out semi-automatically, by running a markable identifier and correcting its output by hand{--}which is increasingly done via annotators recruited through crowdsourcing and aggregating their responses. In this paper, we present a method for identifying markables for coreference annotation that combines high-performance automatic markable detectors with checking with a Game-With-A-Purpose (GWAP) and aggregation using a Bayesian annotation model. The method was evaluated both on news data and data from a variety of other genres and results in an improvement on F1 of mention boundaries of over seven percentage points when compared with a state-of-the-art, domain-independent automatic mention detector, and almost three points over an in-domain mention detector. One of the key contributions of our proposal is its applicability to the case in which markables are nested, as is the case with coreference markables; but the GWAP and several of the proposed markable detectors are task and language-independent and are thus applicable to a variety of other annotation scenarios."
P19-1408,Using Automatically Extracted Minimum Spans to Disentangle Coreference Evaluation from Boundary Detection,2019,18,0,3,0.714286,4244,nafise moosavi,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The common practice in coreference resolution is to identify and evaluate the maximum span of mentions. The use of maximum spans tangles coreference evaluation with the challenges of mention boundary detection like prepositional phrase attachment. To address this problem, minimum spans are manually annotated in smaller corpora. However, this additional annotation is costly and therefore, this solution does not scale to large corpora. In this paper, we propose the MINA algorithm for automatically extracting minimum spans to benefit from minimum span evaluation in all corpora. We show that the extracted minimum spans by MINA are consistent with those that are manually annotated by experts. Our experiments show that using minimum spans is in particular important in cross-dataset coreference evaluation, in which detected mention boundaries are noisier due to domain shift. We have integrated MINA into https://github.com/ns-moosavi/coval for reporting standard coreference scores based on both maximum and automatically detected minimum spans."
N19-1176,A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric Interpretation,2019,0,4,1,1,1743,massimo poesio,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We present a corpus of anaphoric information (coreference) crowdsourced through a game-with-a-purpose. The corpus, containing annotations for about 108,000 markables, is one of the largest corpora for coreference for English, and one of the largest crowdsourced NLP corpora, but its main feature is the large number of judgments per markable: 20 on average, and over 2.2M in total. This characteristic makes the corpus a unique resource for the study of disagreements on anaphoric interpretation. A second distinctive feature is its rich annotation scheme, covering singletons, expletives, and split-antecedent plurals. Finally, the corpus also comes with labels inferred using a recently proposed probabilistic model of annotation for coreference. The labels are of high quality and make it possible to successfully train a state of the art coreference resolver, including training on singletons and non-referring expressions. The annotation model can also result in more than one label, or no label, being proposed for a markable, thus serving as a baseline method for automatically identifying ambiguous markables. A preliminary analysis of the results is presented."
W18-0702,Anaphora Resolution with the {ARRAU} Corpus,2018,0,11,1,1,1743,massimo poesio,"Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference",0,"The ARRAU corpus is an anaphorically annotated corpus of English providing rich linguistic information about anaphora resolution. The most distinctive feature of the corpus is the annotation of a wide range of anaphoric relations, including bridging references and discourse deixis in addition to identity (coreference). Other distinctive features include treating all NPs as markables, including non-referring NPs; and the annotation of a variety of morphosyntactic and semantic mention and entity attributes, including the genericity status of the entities referred to by markables. The corpus however has not been extensively used for anaphora resolution research so far. In this paper, we discuss three datasets extracted from the ARRAU corpus to support the three subtasks of the CRAC 2018 Shared Task{--}identity anaphora resolution over ARRAU-style markables, bridging references resolution, and discourse deixis; the evaluation scripts assessing system performance on those datasets; and preliminary results on these three tasks that may serve as baseline for subsequent research in these phenomena."
Q18-1040,Comparing {B}ayesian Models of Annotation,2018,2,7,6,1,3884,silviu paun,Transactions of the Association for Computational Linguistics,0,"The analysis of crowdsourced annotations in natural language processing is concerned with identifying (1) gold standard labels, (2) annotator accuracies and biases, and (3) item difficulties and error patterns. Traditionally, majority voting was used for 1, and coefficients of agreement for 2 and 3. Lately, model-based analysis of corpus annotations have proven better at all three tasks. But there has been relatively little work comparing them on the same datasets. This paper aims to fill this gap by analyzing six models of annotation, covering different approaches to annotator ability, item difficulty, and parameter pooling (tying) across annotators and items. We evaluate these models along four aspects: comparison to gold labels, predictive accuracy for new annotations, annotator characterization, and item difficulty, using four datasets with varying degrees of noise in the form of random (spammy) annotators. We conclude with guidelines for model selection, application, and implementation."
D18-1218,A Probabilistic Annotation Model for Crowdsourcing Coreference,2018,0,1,5,1,3884,silviu paun,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"The availability of large scale annotated corpora for coreference is essential to the development of the field. However, creating resources at the required scale via expert annotation would be too expensive. Crowdsourcing has been proposed as an alternative; but this approach has not been widely used for coreference. This paper addresses one crucial hurdle on the way to make this possible, by introducing a new model of annotation for aggregating crowdsourced anaphoric annotations. The model is evaluated along three dimensions: the accuracy of the inferred mention pairs, the quality of the post-hoc constructed silver chains, and the viability of using the silver chains as an alternative to the expert-annotated chains in training a state of the art coreference system. The results suggest that our model can extract from crowdsourced annotations coreference chains of comparable quality to those obtained with expert annotation."
W17-4210,Incongruent Headlines: Yet Another Way to Mislead Your Readers,2017,20,9,3,0,31705,sophie chesney,Proceedings of the 2017 {EMNLP} Workshop: Natural Language Processing meets Journalism,0,"This paper discusses the problem of incongruent headlines: those which do not accurately represent the information contained in the article with which they occur. We emphasise that this phenomenon should be considered separately from recognised problematic headline types such as clickbait and sensationalism, arguing that existing natural language processing (NLP) methods applied to these related concepts are not appropriate for the automatic detection of headline incongruence, as an analysis beyond stylistic traits is necessary. We therefore suggest a number of alternative methodologies that may be appropriate to the task at hand as a foundation for future work in this area. In addition, we provide an analysis of existing data sets which are related to this work, and motivate the need for a novel data set in this domain."
Q17-1002,Visually Grounded and Textual Semantic Models Differentially Decode Brain Activity Associated with Concrete and Abstract Nouns,2017,34,18,4,1,32488,andrew anderson,Transactions of the Association for Computational Linguistics,0,"Important advances have recently been made using computational semantic models to decode brain activity patterns associated with concepts; however, this work has almost exclusively focused on concrete nouns. How well these models extend to decoding abstract nouns is largely unknown. We address this question by applying state-of-the-art computational models to decode functional Magnetic Resonance Imaging (fMRI) activity patterns, elicited by participants reading and imagining a diverse set of both concrete and abstract nouns. One of the models we use is linguistic, exploiting the recent word2vec skipgram approach trained on Wikipedia. The second is visually grounded, using deep convolutional neural networks trained on Google Images. Dual coding theory considers concrete concepts to be encoded in the brain both linguistically and visually, and abstract concepts only linguistically. Splitting the fMRI data according to human concreteness ratings, we indeed observe that both models significantly decode the most concrete nouns; however, accuracy is significantly greater using the text-based models for the most abstract nouns. More generally this confirms that current computational models are sufficiently advanced to assist in investigating the representational structure of abstract concepts in the brain."
W16-4312,Predicting {B}rexit: Classifying Agreement is Better than Sentiment and Pollsters,2016,10,8,3,0,33595,fabio celli,"Proceedings of the Workshop on Computational Modeling of People{'}s Opinions, Personality, and Emotions in Social Media ({PEOPLES})",0,"On June 23rd 2016, UK held the referendum which ratified the exit from the EU. While most of the traditional pollsters failed to forecast the final vote, there were online systems that hit the result with high accuracy using opinion mining techniques and big data. Starting one month before, we collected and monitored millions of posts about the referendum from social media conversations, and exploited Natural Language Processing techniques to predict the referendum outcome. In this paper we discuss the methods used by traditional pollsters and compare it to the predictions based on different opinion mining techniques. We find that opinion mining based on agreement/disagreement classification works better than opinion mining based on polarity classification in the forecast of the referendum outcome."
W16-0710,Coreference Resolution for the {B}asque Language with {BART},2016,19,1,6,0,14240,ander soraluze,Proceedings of the Workshop on Coreference Resolution Beyond {O}nto{N}otes ({CORBON} 2016),0,"In this paper we present our work on Coreference Resolution in Basque, a unique language which poses interesting challenges for the problem of coreference. We explain how we extend the coreference resolution toolkit, BART, in order to enable it to process Basque. Then we run four different experiments showing both a significant improvement by extending a baseline feature set and the effect of calculating performance of hand-parsed mentions vs. automatically parsed mentions. Finally, we discuss some key characteristics of Basque which make it particularly challenging for coreference and draw a road map for future work."
L16-1131,The {O}n{F}orum{S} corpus from the Shared Task on Online Forum Summarisation at {M}ulti{L}ing 2015,2016,0,1,3,0.64685,34068,mijail kabadjov,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we present the OnForumS corpus developed for the shared task of the same name on Online Forum Summarisation (OnForumS at MultiLing{'}15). The corpus consists of a set of news articles with associated readers{'} comments from The Guardian (English) and La Repubblica (Italian). It comes with four levels of annotation: argument structure, comment-article linking, sentiment and coreference. The former three were produced through crowdsourcing, whereas the latter, by an experienced annotator using a mature annotation scheme. Given its annotation breadth, we believe the corpus will prove a useful resource in stimulating and furthering research in the areas of Argumentation Mining, Summarisation, Sentiment, Coreference and the interlinks therein."
L16-1323,Phrase Detectives Corpus 1.0 Crowdsourced Anaphoric Coreference.,2016,0,5,2,1,1741,jon chamberlain,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Natural Language Engineering tasks require large and complex annotated datasets to build more advanced models of language. Corpora are typically annotated by several experts to create a gold standard; however, there are now compelling reasons to use a non-expert crowd to annotate text, driven by cost, speed and scalability. Phrase Detectives Corpus 1.0 is an anaphorically-annotated corpus of encyclopedic and narrative text that contains a gold standard created by multiple experts, as well as a set of annotations created by a large non-expert crowd. Analysis shows very good inter-expert agreement (kappa=.88-.93) but a more variable baseline crowd agreement (kappa=.52-.96). Encyclopedic texts show less agreement (and by implication are harder to annotate) than narrative texts. The release of this corpus is intended to encourage research into the use of crowds for text annotation and the development of more advanced, probabilistic language models, in particular for anaphoric coreference."
L16-1326,{ARRAU}: Linguistically-Motivated Annotation of Anaphoric Descriptions,2016,0,5,6,0.622253,28598,olga uryupina,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents a second release of the ARRAU dataset: a multi-domain corpus with thorough linguistically motivated annotation of anaphora and related phenomena. Building upon the first release almost a decade ago, a considerable effort had been invested in improving the data both quantitatively and qualitatively. Thus, we have doubled the corpus size, expanded the selection of covered phenomena to include referentiality and genericity and designed and implemented a methodology for enforcing the consistency of the manual annotation. We believe that the new release of ARRAU provides a valuable material for ongoing research in complex cases of coreference as well as for a variety of related tasks. The corpus is publicly available through LDC."
W15-4638,"{M}ulti{L}ing 2015: Multilingual Summarization of Single and Multi-Documents, On-line Fora, and Call-center Conversations",2015,16,22,8,0,17116,george giannakopoulos,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"In this paper we present an overview of MultiLing 2015, a special session at SIGdial 2015. MultiLing is a communitydriven initiative that pushes the state-ofthe-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems. There were in total 23 participants this year submitting their system outputs to one or more of the four tasks of MultiLing: MSS, MMS, OnForumS and CCCS. We provide a brief overview of each task and its participation and evaluation."
Q15-1018,Combining Minimally-supervised Methods for {A}rabic Named Entity Recognition,2015,47,8,3,1,559,maha althobaiti,Transactions of the Association for Computational Linguistics,0,"Supervised methods can achieve high performance on NLP tasks, such as Named Entity Recognition (NER), but new annotations are required for every new domain and/or genre change. This has motivated research in minimally supervised methods such as semi-supervised learning and distant learning, but neither technique has yet achieved performance levels comparable to those of supervised methods. Semi-supervised methods tend to have very high precision but comparatively low recall, whereas distant learning tends to achieve higher recall but lower precision. This complementarity suggests that better results may be obtained by combining the two types of minimally supervised methods. In this paper we present a novel approach to Arabic NER using a combination of semi-supervised and distant learning techniques. We trained a semi-supervised NER classifier and another one using distant learning techniques, and then combined them using a variety of classifier combination schemes, including the Bayesian Classifier Combination (BCC) procedure recently proposed for sentiment analysis. According to our results, the BCC model leads to an increase in performance of 8 percentage points over the best base classifiers."
althobaiti-etal-2014-aranlp,{A}ra{NLP}: a {J}ava-based Library for the Processing of {A}rabic Text.,2014,17,26,3,1,559,maha althobaiti,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a free, Java-based library named {``}AraNLP{''} that covers various Arabic text preprocessing tools. Although a good number of tools for processing Arabic text already exist, integration and compatibility problems continually occur. AraNLP is an attempt to gather most of the vital Arabic text preprocessing tools into one library that can be accessed easily by integrating or accurately adapting existing tools and by developing new ones when required. The library includes a sentence detector, tokenizer, light stemmer, root stemmer, part-of speech tagger (POS-tagger), word segmenter, normalizer, and a punctuation and diacritic remover."
E14-3012,Automatic Creation of {A}rabic Named Entity Annotated Corpus Using {W}ikipedia,2014,20,9,3,1,559,maha althobaiti,Proceedings of the Student Research Workshop at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we propose a new methodology to exploit Wikipedia features and structure to automatically develop an Arabic NE annotated corpus. Each Wikipedia link is transformed into an NE type of the target article in order to produce the NE annotation. Other Wikipedia features - namely redirects, anchor texts, and inter-language links - are used to tag additional NEs, which appear without links in Wikipedia texts. Furthermore, we have developed a filtering algorithm to eliminate ambiguity when tagging candidate NEs. Herein we also introduce a mechanism based on the high coverage of Wikipedia in order to address two challenges particular to tagging NEs in Arabic text: rich morphology and the absence of capitalisation. The corpus created with our new method (WDC) has been used to train an NE tagger which has been tested on different domains. Judging by the results, an NE tagger trained on WDC can compete with those trained on manually annotated corpora."
E14-1030,Identifying fake {A}mazon reviews as learning from crowds,2014,15,22,2,1,477,tommaso fornaciari,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Customers who buy products such as books online often rely on other customers reviews more than on reviews found on specialist magazines. Unfortunately the confidence in such reviews is often misplaced due to the explosion of so-called sock puppetry-Authors writing glowing reviews of their own books. Identifying such deceptive reviews is not easy. The first contribution of our work is the creation of a collection including a number of genuinely deceptive Amazon book reviews in collaboration with crime writer Jeremy Duns, who has devoted a great deal of effort in unmasking sock puppeting among his colleagues. But there can be no certainty concerning the other reviews in the collection: All we have is a number of cues, also developed in collaboration with Duns, suggesting that a review may be genuine or deceptive. Thus this corpus is an example of a collection where it is not possible to acquire the actual label for all instances, and where clues of deception were treated as annotators who assign them heuristic labels. A number of approaches have been proposed for such cases; we adopt here the 'learning from crowds' approach proposed by Raykar et al. (2010). Thanks to Duns' certainly fake reviews, the second contribution of this work consists in the evaluation of the effectiveness of different methods of annotation, according to the performance of models trained to detect deceptive reviews. xc2xa9 2014 Association for Computational Linguistics."
R13-1005,A Semi-supervised Learning Approach to {A}rabic Named Entity Recognition,2013,24,12,3,1,559,maha althobaiti,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We present ASemiNER, a semisupervised algorithm for identifying Named Entities (NEs) in Arabic text. ASemiNER does not require annotated training data, or gazetteers. It also can be easily adapted to handle more than the three standard NE types (Person, Location, and Organisation). To our knowledge, our algorithm is the first study that intensively investigates the semi-supervised pattern-based learning approach to Arabic Named Entity Recognition (NER). We describe ASemiNER and compare its performance with different supervised systems. We evaluate this algorithm by way of experiments to extract the three standard named-entity types. Ultimately, our algorithm outperforms simple supervised systems and also performs well when we evaluate its performance in order to extract three new, specialised types of NEs (Politicians, Sportspersons, and Artists)."
I13-1099,Adapting a State-of-the-art Anaphora Resolution System for Resource-poor Language,2013,16,7,5,0,28345,utpal sikdar,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper we present our work on adapting a state-of-the-art anaphora resolution system for a resource poor language, namely Bengali. Performance of any anaphoric resolver greatly depends on the quality of a high accurate mention detector. We develop a number of models for mention detection based on heuristics and machine learning. Our experiments show that, a language-dependent system can attain reasonably good performance when re-trained on a new language with a proper subset of features. The system yields the MUC recall, precision and F-measure values of 57.80%, 79.00% and 66.70%, respectively. Our experiments with other available scorers show the F-measure values of 59.47%, 49.83%, 31.81% and 70.82% for BCUB, CEAFM, CEAFE and BLANC, respectively."
D13-1202,"Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts",2013,44,23,4,1,32488,andrew anderson,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility."
W12-5103,On discriminating f{MRI} representations of abstract {W}ord{N}et taxonomic categories,2012,28,1,4,1,32488,andrew anderson,Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon,0,"How abstract knowledge is organised is a key question in cognitive science, and has clear repercussions for the design of artifical lexical resources, but is poorly understood. We present fMRI results for an experiment where participants imagined situations associated with abstract words, when cued with a visual word stimulus. We use a multivariate-pattern analysis procedure to demonstrate that 7 WordNet style Taxonomic categories (e.g. 'Attribute', 'Event', 'SocialRole'), can be decoded from neural data at a level better than chance. This demonstrates that category distinctions in artificial lexical resources have some explanatory value for neural organisation. Secondly, we tested for similarity in the interrelationship of the taxonomic categories in our fMRI data and the associated interrelations in popular distributed semantic models (LSA,HAL,COALS). Although distributed models have been successfully applied to predict concrete noun fMRI data (e.g. Mitchell et al., 2008), no evidence of association was found for our abstract concepts. This suggests that development of new models/experimental strategies may be necessary to elucidate the organisation of abstract knowledge."
W12-4515,{BART} goes multilingual: The {U}ni{TN} / {E}ssex submission to the {C}o{NLL}-2012 Shared Task,2012,18,12,3,1,28598,olga uryupina,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"This paper describes the UniTN/Essex submission to the CoNLL-2012 Shared Task on the Multilingual Coreference Resolution. We have extended our CoNLL-2011 submission, based on BART, to cover two additional languages, Arabic and Chinese. This paper focuses on adapting BART to new languages, discussing the problems we have encountered and the solutions adopted. In particular, we propose a novel entity-mention detection algorithm that might help identify nominal mentions in an unknown language. We also discuss the impact of basic linguistic information on the overall performance level of our coreference resolution system."
W12-3618,Annotating Archaeological Texts: An Example of Domain-Specific Annotation in the Humanities,2012,15,0,4,0,10600,francesca bonin,Proceedings of the Sixth Linguistic Annotation Workshop,0,"Developing content extraction methods for Humanities domains raises a number of challenges, from the abundance of non-standard entity types to their complexity to the scarcity of data. Close collaboration with Humanities scholars is essential to address these challenges. We discuss an annotation schema for Archaeological texts developed in collaboration with domain experts. Its development required a number of iterations to make sure all the most important entity types were included, as well as addressing challenges including a domain-specific handling of temporal expressions, and the existence of many systematic types of ambiguity."
W12-0406,On the Use of Homogenous Sets of Subjects in Deceptive Language Analysis,2012,21,7,2,1,477,tommaso fornaciari,Proceedings of the Workshop on Computational Approaches to Deception Detection,0,"Recent studies on deceptive language suggest that machine learning algorithms can be employed with good results for classification of texts as truthful or untruthful. However, the models presented so far do not attempt to take advantage of the differences between subjects. In this paper, models have been trained in order to classify statements issued in Court as false or not-false, not only taking into consideration the whole corpus, but also by identifying more homogenous subsets of producers of deceptive language. The results suggest that the models are effective in recognizing false statements, and their performance can be improved if subsets of homogeneous data are provided."
fornaciari-poesio-2012-decour,{D}e{C}our: a corpus of {DE}ceptive statements in {I}talian {COUR}ts,2012,14,4,2,1,477,tommaso fornaciari,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In criminal proceedings, sometimes it is not easy to evaluate the sincerity of oral testimonies. DECOUR - DEception in COURt corpus - has been built with the aim of training models suitable to discriminate, from a stylometric point of view, between sincere and deceptive statements. DECOUR is a collection of hearings held in four Italian Courts, in which the speakers lie in front of the judge. These hearings become the object of a specific criminal proceeding for calumny or false testimony, in which the deceptiveness of the statements of the defendant is ascertained. Thanks to the final Court judgment, that points out which lies are told, each utterance of the corpus has been annotated as true, uncertain or false, according to its degree of truthfulness. Since the judgment of deceptiveness follows a judicial inquiry, the annotation has been realized with a greater degree of confidence than ever before. Moreover, in Italy this is the first corpus of deceptive texts not relying on Âmock' lies created in laboratory conditions, but which has been collected in a natural environment."
uryupina-poesio-2012-domain,Domain-specific vs. Uniform Modeling for Coreference Resolution,2012,14,5,2,1,28598,olga uryupina,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Several corpora annotated for coreference have been made available in the past decade. These resources differ with respect to their size and the underlying structure: the number of domains and their similarity. Our study compares domain-specific models, learned from small heterogeneous subsets of the investigated corpora, against uniform models, that utilize all the available data. We show that for knowledge-poor baseline systems, domain-specific and uniform modeling yield same results. Systems, relying on large amounts of linguistic knowledge, however, exhibit differences in their performance: with all the designed features in use, domain-specific models suffer from over-fitting, whereas with pre-selected feature sets they tend to outperform union models."
C12-2086,Relational Structures and Models for Coreference Resolution,2012,19,2,2,0,32511,trucvien nguyen,Proceedings of {COLING} 2012: Posters,0,"Coreference resolution is the task of identifying the sets of mentions referring to the same entity. Although modern machine learning approaches to coreference resolution exploit a variety of semantic information, the literature on the effect of relational information on coreference is still very limited. In this paper, we discuss and compare two methods for incorporating relational information into a coreference resolver. One approach is to use a filtering algorithm to rerank the output of coreference hypotheses. The filter is based on the relational structures between mentions and their corresponding relationships. The second approach is to use a joint model enriched with a set of relational features derived from semantic relations of each mention. Both methods have shown to improve the performance of a learning-based state-of-the-art coreference resolver."
W11-1908,Multi-metric optimization for coreference: The {U}ni{TN} / {IITP} / {E}ssex submission to the 2011 {CONLL} Shared Task,2011,17,10,4,1,28598,olga uryupina,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"Because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multi-objective function Optimization (moo) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously."
W11-1508,Structure-Preserving Pipelines for Digital Libraries,2011,16,5,1,1,1743,massimo poesio,"Proceedings of the 5th {ACL}-{HLT} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"Most existing HLT pipelines assume the input is pure text or, at most, HTML and either ignore (logical) document structure or remove it. We argue that identifying the structure of documents is essential in digital library and other types of applications, and show that it is relatively straightforward to extend existing pipelines to achieve ones in which the structure of a document is preserved."
P11-1081,A Cross-Lingual {ILP} Solution to Zero Anaphora Resolution,2011,25,45,2,0,12930,ryu iida,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We present an ILP-based model of zero anaphora detection and resolution that builds on the joint determination of anaphoricity and coreference model proposed by Denis and Baldridge (2007), but revises it and extends it into a three-way ILP problem also incorporating subject detection. We show that this new model outperforms several baselines and competing models, as well as a direct translation of the Denis/Baldridge model, for both Italian and Japanese zero anaphora. We incorporate our model in complete anaphoric resolvers for both Italian and Japanese, showing that our approach leads to improved performance also when not used in isolation, provided that separate classifiers are used for zeros and for explicitly realized anaphors."
I11-1011,Single and multi-objective optimization for feature selection in anaphora resolution,2011,22,9,4,1,4603,sriparna saha,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"There is no generally accepted metric for measuring the performance of anaphora resolution systems, and the existing metricsxe2x80x94MUC, B3, CEAF, Blanc, among othersxe2x80x94tend to reward significantly different behaviors. Systems optimized according to one metric tend to perform poorly with respect to other ones, making it very difficult to compare anaphora resolution systems, as clearly shown by the results of the SEMEVAL 2010 Multilingual Coreference task. One solution would be to find a single completely satisfactory metric, but itxe2x80x99s not clear whether this is possible and at any rate it is not going to happen any time soon. An alternative is to optimize models according to multiple metrics simultaneously. In this paper, we show, first of all, that this is possible to develop such models using Multi-objective Optimization (MOO) techniques based on Genetic Algorithms. Secondly, we show that optimizing according to multiple metrics simultaneously may result in better results with respect to each individual metric than optimizing according to that metric only."
W10-0605,Detecting Semantic Category in Simultaneous {EEG}/{MEG} Recordings,2010,20,7,2,1,18472,brian murphy,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Computational Neurolinguistics,0,"Electroencephalography (EEG) and magnetoencephalography (MEG) are closely related neuroimaging technologies that both measure summed electrical activity of synchronous sources of neural activity. However they differ in the portions of the brain to which they are more sensitive, in the frequency bands they can detect, and to the amount of noise to which they are subject. Since semantic representations are thought to be widely distributed in the brain, this preliminary study considered if the broader coverage offered by simultaneous EEG/MEG recordings would increase sensitivity to these cognitive states. The results showed that MEG data allowed stimuli in two semantic categories (mammals and tools) to be distinguished more accurately, despite some experimental settings that were optimised for EEG. The addition of EEG data did not prove informative, indicating that it may be redundant relative to MEG, even when using dimensionality reduction techniques to combat overfitting."
S10-1001,{S}em{E}val-2010 Task 1: Coreference Resolution in Multiple Languages,2010,24,99,7,0,28963,marta recasens,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper presents the SemEval-2010 task on Coreference Resolution in Multiple Languages. The goal was to evaluate and compare automatic coreference resolution systems for six different languages (Catalan, Dutch, English, German, Italian, and Spanish) in four evaluation settings and using four different metrics. Such a rich scenario had the potential to provide insight into key issues concerning coreference resolution: (i) the portability of systems across languages, (ii) the relevance of different levels of linguistic information, and (iii) the behavior of scoring metrics."
S10-1021,{BART}: A Multilingual Anaphora Resolution System,2010,10,29,2,0,10004,samuel broscheit,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"BART (Versley et al., 2008) is a highly modular toolkit for coreference resolution that supports state-of-the-art statistical approaches and enables efficient feature engineering. For the SemEval task 1 on Coreference Resolution, BART runs have been submitted for German, English, and Italian.n n BART relies on a maximum entropy-based classifier for pairs of mentions. A novel entity-mention approach based on Semantic Trees is at the moment only supported for English."
rodriguez-etal-2010-anaphoric,Anaphoric Annotation of {W}ikipedia and Blogs in the Live Memories Corpus,2010,14,27,5,1,35059,kepa rodriguez,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The Live Memories corpus is an Italian corpus annotated for anaphoric relations. This annotation effort aims to contribute to two significant issues for the CL research: the lack of annotated anaphoric resources for Italian and the increasing interest for the social Web. The Live Memories Corpus contains texts from the Italian Wikipedia about the region Trentino/S{\""u}d Tirol and from blog sites with users' comments. It is planned to add a set of articles of local news papers. The corpus includes manual annotated information about morphosyntactic agreement, anaphoricity, and semantic class of the NPs. The anaphoric annotation includes discourse deixis, bridging relations and markes cases of ambiguity with the annotation of alternative interpretations. For the annotation of the anaphoric links the corpus takes into account specific phenomena of the Italian language like incorporated clitics and phonetically non realized pronouns. Reliability studies for the annotation of the mentioned phenomena and for annotation of anaphoric links in general offer satisfactory results. The Wikipedia and blogs dataset will be distributed under Creative Commons Attributions licence."
poesio-etal-2010-babyexp,{B}aby{E}xp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do,2010,15,1,1,1,1743,massimo poesio,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions."
broscheit-etal-2010-extending,Extending {BART} to Provide a Coreference Resolution System for {G}erman,2010,26,10,4,0,10004,samuel broscheit,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a flexible toolkit-based approach to automatic coreference resolution on German text. We start with our previous work aimed at reimplementing the system from Soon et al. (2001) for English, and extend it to duplicate a version of the state-of-the-art proposal from Klenner and Ailloud (2009). Evaluation performed on a benchmarking dataset, namely the TueBa-D/Z corpus (Hinrichs et al., 2005b), shows that machine learning based coreference resolution can be robustly performed in a language other than English."
poesio-etal-2010-creating,Creating a Coreference Resolution System for {I}talian,2010,10,12,1,1,1743,massimo poesio,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper summarizes our work on creating a full-scale coreference resolution (CR) system for Italian, using BART â an open-source modular CR toolkit initially developed for English corpora. We discuss our experiments on language-specific issues of the task. As our evaluation experiments show, a language-agnostic system (designed primarily for English) can achieve a performance level in high forties (MUC F-score) when re-trained and tested on a new language, at least on gold mention boundaries. Compared to this level, we can improve our F-score by around 10{\%} introducing a small number of language-specific changes. This shows that, with a modular coreference resolution platform, such as BART, one can straightforwardly develop a family of robust and reliable systems for various languages. We hope that our experiments will encourage researchers working on coreference in other languages to create their own full-scale coreference resolution systems â as we have mentioned above, at the moment such modules exist only for very few languages other than English."
W09-3912,Interactive Gesture in Dialogue: a {PTT} Model,2009,11,6,2,0,40746,hannes rieser,Proceedings of the {SIGDIAL} 2009 Conference,0,"Gestures are usually looked at in isolation or from an intra-propositional perspective essentially tied to one speaker. The Bielefeld multi-modal Speech-And-Gesture-Alignment (SAGA) corpus has many interactive gestures relevant for the structure of dialogue (Rieser 2008, 2009). To describe them, a dialogue theory is needed which can serve as a speech-gesture interface. PTT (Poesio and Traum 1997, Poesio and Rieser submitted a) can do this job in principle, how this can be achieved is the main topic of this paper. As a precondition, the empirical research procedure from systematic corpus annotation via gesture typology to a partial ontology for gestures is described. It is then explained how PTT is extended to provide an incremental modelling of speech plus gesture in an assertion-acknowledgement adjacency pair where grounding between dialogue participants is obtained through gesture."
W09-3703,Play your way to an annotated corpus: Games with a purpose and anaphoric annotation,2009,0,0,1,1,1743,massimo poesio,Proceedings of the Eight International Conference on Computational Semantics,0,"The lack of large-scale corpora annotated with semantic information has been a serious bottleneck for computational semantics, slowing down not only the development of more advanced statistical methods, but also our empirical understanding of the phenomena. The creation of the Ontonotes corpus will finally bring computational semantics to the point where computational syntax was in 1993 - but in the meantime, we have come to appreciate the limitations of that methodology both theoretically and as a way of gathering judgments. In this talk, I will discuss an ongoing effort to use the 'Games with a Purpose' methodology to create a large-scale anaphorically annotated corpus in which multiple judgments are maintained about the interpretation of each anaphoric expression - and in particular, the Phrase Detectives game: http://www.phrasedetectives.org"
W09-3309,Constructing an Anaphorically Annotated Corpus with Non-Experts: Assessing the Quality of Collaborative Annotations,2009,19,21,3,1,1741,jon chamberlain,Proceedings of the 2009 Workshop on The People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources (People{'}s Web),0,"This paper reports on the ongoing work of Phrase Detectives, an attempt to create a very large anaphorically annotated text corpus. Annotated corpora of the size needed for modern computational linguistics research cannot be created by small groups of hand-annotators however the ESP game and similar games with a purpose have demonstrated how it might be possible to do this through Web collaboration. We show that this approach could be used to create large, high-quality natural language resources."
R09-1006,Unsupervised Knowledge Extraction for Taxonomies of Concepts from {W}ikipedia,2009,8,1,2,0,25310,eduard barbu,Proceedings of the International Conference {RANLP}-2009,0,A novel method for unsupervised acquisition of knowledge for taxonomies of concepts from raw Wikipedia text is presented. We assume that the concepts classified under the same node in a taxonomy are described in a comparable way in Wikipedia. The concepts in 6 taxonomies extracted from WordNet are mapped onto Wikipedia pages and the lexico-syntactic patterns describing semantic structures expressing relevant knowledge for the concepts are automatically learnt.
P09-5006,State-of-the-art {NLP} Approaches to Coreference Resolution: Theory and Practical Recipes,2009,10,12,2,1,9871,simone ponzetto,Tutorial Abstracts of {ACL}-{IJCNLP} 2009,0,The identification of different nominal phrases in a discourse as used to refer to the same (discourse) entity is essential for achieving robust natural language understanding (NLU). The importance of this task is directly amplified by the field of Natural Language Processing (NLP) currently moving towards high-level linguistic tasks requiring NLU capabilities such as e.g. recognizing textual entailment. This tutorial aims at providing the NLP community with a gentle introduction to the task of coreference resolution from both a theoretical and an application-oriented perspective. Its main purposes are: (1) to introduce a general audience of NLP researchers to the core ideas underlying state-of-the-art computational models of coreference; (2) to provide that same audience with an overview of NLP applications which can benefit from coreference information.
J09-4002,{O}bituaries: Janet Hitzeman,2009,-1,-1,1,1,1743,massimo poesio,Computational Linguistics,0,None
J09-1003,Evaluating Centering for Information Ordering Using Corpora,2009,43,36,3,0.487805,46630,nikiforos karamanis,Computational Linguistics,0,In this article we discuss several metrics of coherence defined using centering theory and investigate the usefulness of such metrics for information ordering in automatic text generation. We estimate empirically which is the most promising metric and how useful this metric is using a general methodology applied on several corpora. Our main result is that the simplest metric (which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformed by other metrics which make use of additional centering-based features. This baseline can be used for the development of both text-to-text and concept-to-text generation systems.
D09-1065,{EEG} responds to conceptual stimuli and corpus semantics,2009,21,29,3,1,18472,brian murphy,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Mitchell et al. (2008) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fMRI. This could be a very powerful technique for evaluating conceptual models extracted from corpora; however, fMRI is expensive and imposes strong constraints on data collection. Following on experiments that demonstrated that EEG activation patterns encode enough information to discriminate broad conceptual categories, we show that corpus-based semantic representations can predict EEG activation patterns with significant accuracy, and we evaluate the relative performance of different corpus-models on this task."
W08-2230,Addressing the Resource Bottleneck to Create Large-Scale Annotated Texts,2008,10,13,2,1,1741,jon chamberlain,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Large-scale linguistically annotated resources have become available in recent years. This is partly due to sophisticated automatic and semiautomatic approaches that work well on specific tasks such as part-of-speech tagging. For more complex linguistic phenomena like anaphora resolution there are no tools that result in high-quality annotations without massive user intervention. Annotated corpora of the size needed for modern computational linguistics research cannot however be created by small groups of hand annotators. The ANAWIKI project strikes a balance between collecting high-quality annotations from experts and applying a game-like approach to collecting linguistic annotation from the general Web population. More generally, ANAWIKI is a project that explores to what extend expert annotations can be substituted by a critical mass of non-expert judgements."
P08-4003,{BART}: A Modular Toolkit for Coreference Resolution,2008,26,17,3,1,4696,yannick versley,Proceedings of the {ACL}-08: {HLT} Demo Session,0,"Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort, yet there is very limited availability of off-the shelf tools for researchers whose interests are not in coreference, or for researchers who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of the Soon et al. (2001) proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers."
day-etal-2008-corpus,A Corpus for Cross-Document Co-reference,2008,6,11,5,0,47358,david day,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes a newly created text corpus of news articles that has been annotated for cross-document co-reference. Being able to robustly resolve references to entities across document boundaries will provide a useful capability for a variety of tasks, ranging from practical information retrieval applications to challenging research in information extraction and natural language understanding. This annotated corpus is intended to encourage the development of systems that can more accurately address this problem. A manual annotation tool was developed that allowed the complete corpus to be searched for likely co-referring entity mentions. This corpus of 257K words links mentions of co-referent people, locations and organizations (subject to some additional constraints). Each of the documents had already been annotated for within-document co-reference by the LDC as part of the ACE series of evaluations. The annotation process was bootstrapped with a string-matching-based linking procedure, and we report on some of initial experimentation with the data. The cross-document linking information will be made publicly available."
poesio-artstein-2008-anaphoric,Anaphoric Annotation in the {ARRAU} Corpus,2008,20,67,1,1,1743,massimo poesio,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Arrau is a new corpus annotated for anaphoric relations, with information about agreement and explicit representation of multiple antecedents for ambiguous anaphoric expressions and discourse antecedents for expressions which refer to abstract entities such as events, actions and plans. The corpus contains texts from different genres: task-oriented dialogues from the Trains-91 and Trains-93 corpus, narratives from the English Pear Stories corpus, newspaper articles from the Wall Street Journal portion of the Penn Treebank, and mixed text from the Gnome corpus."
versley-etal-2008-bart-modular,{BART}: A modular toolkit for coreference resolution,2008,26,17,3,1,4696,yannick versley,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Developing a full coreference system able to run all the way from raw text to semantic interpretation is a considerable engineering effort. Accordingly, there is very limited availability of off-the shelf tools for researchers whose interests are not primarily in coreference or others who want to concentrate on a specific aspect of the problem. We present BART, a highly modular toolkit for developing coreference applications. In the Johns Hopkins workshop on using lexical and encyclopedic knowledge for entity disambiguation, the toolkit was used to extend a reimplementation of Soon et al.Âs proposal with a variety of additional syntactic and knowledge-based features, and experiment with alternative resolution processes, preprocessing tools, and classifiers. BART has been released as open source software and is available from http://www.sfs.uni-tuebingen.de/{\textasciitilde}versley/BART"
poesio-etal-2008-anawiki,{ANAWIKI}: Creating Anaphorically Annotated Resources through Web Cooperation,2008,10,19,1,1,1743,massimo poesio,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The ability to make progress in Computational Linguistics depends on the availability of large annotated corpora, but creating such corpora by hand annotation is very expensive and time consuming; in practice, it is unfeasible to think of annotating more than one million words. However, the success of Wikipedia and other projects shows that another approach might be possible: take advantage of the willingness of Web users to contribute to collaborative resource creation. AnaWiki is a recently started project that will develop tools to allow and encourage large numbers of volunteers over the Web to collaborate in the creation of semantically annotated corpora (in the first instance, of a corpus annotated with information about anaphora)."
J08-4004,Survey Article: Inter-Coder Agreement for Computational Linguistics,2008,130,709,2,0.25,16785,ron artstein,Computational Linguistics,0,"This article is a survey of methods for measuring agreement among corpus annotators. It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff's alpha as well as Scott's pi and Cohen's kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-like measures in computational linguistics, may be more appropriate for many corpus annotation tasks---but that their use makes the interpretation of the value of the coefficient even harder."
C08-1121,Coreference Systems Based on Kernels Methods,2008,32,29,3,1,4696,yannick versley,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Various types of structural information - e.g., about the type of constructions in which binding constraints apply, or about the structure of names - play a central role in coreference resolution, often in combination with lexical information (as in expletive detection). Kernel functions appear to be a promising candidate to capture structure-sensitive similarities and complex feature combinations, but care is required to ensure they are exploited in the best possible fashion. In this paper we propose kernel functions for three subtasks of coreference resolution - binding constraint detection, expletive identification, and aliasing - together with an architecture to integrate them within the standard framework for coreference resolution."
W07-1524,Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus,2007,20,16,4,1,35059,kepa rodriguez,Proceedings of the Linguistic Annotation Workshop,0,"The LUNA corpus is a multi-lingual, multi-domain spoken dialogue corpus currently under development that will be used to develop a robust natural spoken language understanding toolkit for multilingual dialogue services. The LUNA corpus will be annotated at multiple levels to include annotations of syntactic, semantic, and discourse information; specialized annotation tools will be used for the annotation at each of these levels. In order to synchronize these multiple layers of annotation, the PAULA standoff exchange format will be used. In this paper, we present the corpus and its PAULA-based architecture."
W07-1029,Discovering contradicting protein-protein interactions in text,2007,3,2,2,0,49026,olivia sanchez,"Biological, translational, and clinical language processing",0,"In biomedical texts, contradictions about protein-protein interactions (PPIs) occur when an author reports observing a given PPI whereas another author argues that very same interaction does not take place: e.g., when author X argues that protein A interacts with protein B whereas author Y claims that protein A does not interact with B. Of course, merely discovering a potential contradiction does not mean the argument is closed as other factors may have caused the proteins to behave in different ways. We present preliminary work towards the automatic detection of potential contradictions between PPIs from text and an agreement experimental evaluation of our method."
W05-1003,Identifying Concept Attributes Using a Classifier,2005,22,39,1,1,1743,massimo poesio,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,We developed a novel classification of concept attributes and two supervised classifiers using this classification to identify concept attributes from candidate attributes extracted from the Web. Our binary (attribute / non-attribute) classifier achieves an accuracy of 81.82% whereas our 5-way classifier achieves 80.35%.
W05-0302,"Merging {P}rop{B}ank, {N}om{B}ank, {T}ime{B}ank, {P}enn {D}iscourse {T}reebank and Coreference",2005,21,21,4,0,993,james pustejovsky,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"Many recent annotation efforts for English have focused on pieces of the larger problem of semantic annotation, rather than initially producing a single unified representation. This paper discusses the issues involved in merging four of these efforts into a unified linguistic structure: PropBank, NomBank, the Discourse Treebank and Coreference Annotation undertaken at the University of Essex. We discuss resolving overlapping and conflicting annotation as well as how the various annotation schemes can reinforce each other to produce a representation that is greater than the sum of its parts."
W05-0311,"The Reliability of Anaphoric Annotation, Reconsidered: Taking Ambiguity into Account",2005,21,53,1,1,1743,massimo poesio,Proceedings of the Workshop on Frontiers in Corpus Annotations {II}: Pie in the Sky,0,"We report the results of a study of the reliability of anaphoric annotation which (i) involved a substantial number of naive subjects, (ii) used Krippendorff's xcexb1 instead of K to measure agreement, as recently proposed by Passonneau, and (iii) allowed annotators to mark anaphoric expressions as ambiguous."
H05-1001,Improving {LSA}-based Summarization with Anaphora Resolution,2005,23,31,3,0,12063,josef steinberger,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We propose an approach to summarization exploiting both lexical information and the output of an automatic anaphoric resolver, and using Singular Value Decomposition (SVD) to identify the main terms. We demonstrate that adding anaphoric information results in significant performance improvements over a previously developed system, in which only lexical terms are used as the input to SVD. However, we also show that how anaphoric information is used is crucial: whereas using this information to add new terms does result in improved performance, simple substitution makes the performance worse."
W04-3221,Attribute-Based and Value-Based Clustering: An Evaluation,2004,22,89,2,0,41080,abdulrahman almuhareb,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"In most research on concept acquisition from corpora, concepts are modeled as vectors of relations extracted from syntactic structures. In the case of modifiers, these relations often specify values of attributes, as in (attr red); this is unlike what typically proposed in theories of knowledge representation, where concepts are typically defined in terms of their attributes (e.g., color). We compared models of concepts based on values with models based on attributes, using lexical clustering as the basis for comparison. We find that attribute-based models work better than value-based ones, and result in shorter descriptions; but that mixed models including both the best attributes and the best values work best of all."
W04-3232,Identifying Broken Plurals in Unvowelised {A}rabic Tex,2004,-1,-1,2,0,51336,abduelbaset goweder,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-2327,"The {MATE}/{GNOME} Proposals for Anaphoric Annotation, Revisited",2004,22,69,1,1,1743,massimo poesio,Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004,0,"In the five years since it was proposed, the MATE scheme for anaphoric annotation has been used in a variety of annotation projects, and the resulting corpora have been used to study both anaphora resolution and NL generation. Annotation tools inspired by the proposals have been used in some of these projects. In this paper we discuss these first experiences with the scheme, some lessons that have been learned, and suggest a few modifications."
W04-0707,Discourse-New Detectors for Definite Description Resolution: A Survey and a Preliminary Proposal,2004,17,26,1,1,1743,massimo poesio,Proceedings of the Conference on Reference Resolution and Its Applications,0,None
W04-0210,Discourse Annotation and Semantic Annotation in the {GNOME} corpus,2004,33,53,1,1,1743,massimo poesio,Proceedings of the Workshop on Discourse Annotation,0,"The GNOME corpus was created to study the discourse and semantic properties of discourse entities that affect their realization and interpretation, and particularly salience. We discuss what information was annotated and the methods we followed."
P04-1019,Learning to Resolve Bridging References,2004,25,77,1,1,1743,massimo poesio,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references. We find that using first mention, utterance distance, and lexical distance computed using either Google or WordNet results in an accuracy significantly higher than obtained in previous experiments."
P04-1050,Evaluating Centering-Based Metrics of Coherence,2004,20,36,2,0.487805,46630,nikiforos karamanis,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We use a reliably annotated corpus to compare metrics of coherence based on Centering Theory with respect to their potential usefulness for text structuring in natural language generation. Previous corpus-based evaluations of the coherence of text according to Centering did not compare the coherence of the chosen text structure with that of the possible alternatives. A corpus-based methodology is presented which distinguishes between Centering-based metrics taking these alternatives into account, and represents therefore a more appropriate way to evaluate Centering from a text structuring perspective."
sanchez-graillet-poesio-2004-acquiring,Acquiring {B}ayesian Networks from Text,2004,14,24,2,0,51094,olivia sanchezgraillet,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Causal inference is one of the most fundamental reasoning processes and one that is essential for question-answering as well as more general AI applications such as decision-making and diagnosis. Bayesian Networks are a popular formalism for encoding (probabilistic) causal knowledge that allows for inference. We developed a system for acquiring causal knowledge from text. Our system identifies sentences that specify causal relations and extracts from them causal patterns, taking into account connectives such as conjunction, disjunction and negation, and recognising causes and effects by analysing terms. The dependencies among the causes and effects found in text can be encoded as Bayesian networks. We evaluated our work by comparing the network structures obtained by our system with the ones created by a human evaluator. Introduction and Motivations Causal inference is one of the most fundamental reasoning processes (Glymour, 2003; Pazzani, 1991; Trabassoxe2x80x99s paper in Goldman et al, 1999) and one which is essential for question-answering as well as more general AI applications such as decision-making and diagnosis. Methods for acquiring knowledge about causal rules are a prerequisite for the development of systems capable of causal inference in these applications, especially in complex domains (Girju, 2003; Kontos et al, 2002). Bayesian Networks (Pearl, 1998) are a popular formalism for encoding probabilistic causal knowledge and for causal inference. Such networks are typically acquired from data (Mani & Cooper, 2001), but text is a rich source of information about causal relations that can be exploited, even though there are a number of problems to take into account (Hearst, 1999). In this paper we discuss domain-independent methods for acquiring from text causal knowledge encoded as Bayesian networks. Background Bayesian Networks A Bayesian network (Pearl, 2000) is a directed acyclic graph whose arcs denote a direct causal influence between parent nodes (causes) and children nodes (effects). The nodes can be used to encode any random variable. For example, a person can be ill or well; the car engine can be working normally or having problems, etc. Such graph is associated with a probability distribution that satisfies the Markov Assumption. By using Bayesian networks it is possible to handle incomplete knowledge as well as to make predictions by using the conditional probability distribution tables (CPT). There is one table for each node, which describes the conditional probability of that node given the different values of its parents (Friedman & Goldszmidt, 1996). A disadvantage of these tables is that they can be huge because the size of the table is locally exponential to the number of parents of the node. The complete joint probability distribution for the network is expressed by the CPTs for all the variables together with the conditional independences described by the network (Mitchell, 1997). Identifying Causal Relations Acquiring causal knowledge from text requires, first of all, identifying portions of text that specify a causal relation (henceforth causal patterns) between causes and effects (henceforth events) such as: xe2x80x9cCorruption and insecurity cause social problemsxe2x80x9d, xe2x80x9cDisease provokes pain or deathxe2x80x9d, xe2x80x9cEarthquake generates victimsxe2x80x9d (Girju & Moldovan, 2002; Wolff et al, 2002); and second, analysing these causal patterns (a) taking into account the possible presence of connectives such as conjunction, disjunction and negation and (b) identifying causes and effects by analysing terms. These analysis steps are seldom discussed in the literature and have been the focus of our research. We consider each step in turn. Finding causal patterns Causal patterns can be expressed by cues such as connectives, as in xe2x80x9cthe manager fired John because he was lazyxe2x80x9d; verbs, as in xe2x80x9csmoking causes cancerxe2x80x9d; or NPs, as in xe2x80x9cViruses are the cause of neurological diseasesxe2x80x9c. After a preliminary analysis, we decided to concentrate in this first stage on causal patterns in which both events are expressed as noun phrases (ignoring cases such as in xe2x80x9cthe manager fired John because he was lazyxe2x80x9d). We also decided to restrict the number of cues to the cause words in Roget Thesaurus found to be the most frequent in texts using Google, together with the causal verbs proposed by Girju and Moldovan (2002). Girju and Moldovan focused on explicit intra-sentential syntactic patterns of the forms and . In the latter they use WordNet (Fellbaum, 1998) causal relations to find noun concepts of the verbs with nominalizations. They developed a method for automatic detection of causation patterns and semi-automatic validation of ambiguous lexico-syntactic patterns that refer to causal relationships. In this work we used the causal verbs that"
poesio-kabadjov-2004-general,"A General-Purpose, Off-the-shelf Anaphora Resolution Module: Implementation and Preliminary Evaluation",2004,16,52,1,1,1743,massimo poesio,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,GuiTAR is an anaphora resolution system designed to be modular and usable as an off-the-shelf component of a NL processing pipeline. We discuss the systemxe2x80x99s design and a preliminary evaluation of the two algorithms implemented in the current version of the system xe2x80x93 for definite descriptions and for pronoun resolution.
J04-3003,{C}entering: A Parametric Theory and Its Instantiations,2004,112,141,1,1,1743,massimo poesio,Computational Linguistics,0,"Centering theory is the best-known framework for theorizing about local coherence and salience; however, its claims are articulated in terms of notions which are only partially specified, such as utterance, realization, or ranking. A great deal of research has attempted to arrive at more detailed specifications of these parameters of the theory; as a result, the claims of centering can be instantiated in many different ways. We investigated in a systematic fashion the effect on the theory's claims of these different ways of setting the parameters. Doing this required, first of all, clarifying what the theory's claims are (one of our conclusions being that what has become known as Constraint 1 is actually a central claim of the theory). Secondly, we had to clearly identify these parametric aspects: For example, we argue that the notion of pronoun used in Rule 1 should be considered a parameter. Thirdly, we had to find appropriate methods for evaluating these claims. We found that while the theory's main claim about salience and pronominalization, Rule 1xe2x80x94a preference for pronominalizing the backward-looking center (CB)xe2x80x94is verified with most instantiations, Constraint 1xe2x80x93a claim about (entity) coherence and CB uniquenessxe2x80x94is much more instantiation-dependent: It is not verified if the parameters are instantiated according to very mainstream views (vanilla instantiation), it holds only if indirect realization is allowed, and is violated by between 20% and 25% of utterances in our corpus even with the most favorable instantiations. We also found a trade-off between Rule 1, on the one hand, and Constraint 1 and Rule 2, on the other: Setting the parameters to minimize the violations of local coherence leads to increased violations of salience, and vice versa. Our results suggest that entity coherencexe2x80x94continuous reference to the same entitiesxe2x80x94must be supplemented at least by an account of relational coherence."
W03-2605,Associative Descriptions and Salience: A Preliminary Investigation,2003,28,12,1,1,1743,massimo poesio,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,None
poesio-etal-2002-acquiring,Acquiring Lexical Knowledge for Anaphora Resolution,2002,26,79,1,1,1743,massimo poesio,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The lack of adequate bases of commonsense or even lexical knowledge is perhaps the main obstacle to the development of highperformance, robust tools for semantic interpretation. It is also generally accepted that, notwithstanding the increasing availability in recent years of substantial hand-coded lexical resources such as WordNet and EuroWordNet, addressing the commonsense knowledge bottleneck will eventually require the development of effective techniques for acquiring such information automatically, e.g., from corpora. We discuss research aimed at improving the performance of anaphora resolution systems by acquiring the commonsense knowledge require to resolve the more complex cases of anaphora, such as bridging references. We focus in particular on the problem of acquiring information about part-of relations."
N01-1002,Corpus-based {NP} Modifier Generation,2001,18,18,2,0.555556,9674,hua cheng,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper describes how we annotated and analysed the NP modifiers in a corpus of museum descriptions to discover rules for the selection and realisation of such modifiers, in particular non-referring ones. We implemented the regularities into an extension of the ILEX system to generate complex NPs capable of serving multiple communicative goals."
W00-1705,Semantic Annotation for Generation: Issues in Annotating a Corpus to Develop and Evaluate Discourse Entity Realization Algorithms,2000,17,1,1,1,1743,massimo poesio,Proceedings of the {COLING}-2000 Workshop on Semantic Annotation and Intelligent Content,0,"We are annotating a corpus with information relevant to discourse entity realization, and especially the information needed to decide which type of NP to use. The corpus is being used to study correlations between NP type and certain semantic or discourse features, to evaluate hand-coded algorithms, and to train statistical models. We report on the development of our annotation scheme, the problems we have encountered, and the results obtained so far."
poesio-2000-annotating,Annotating a Corpus to Develop and Evaluate Discourse Entity Realization Algorithms: Issues and Preliminary Results,2000,24,31,1,1,1743,massimo poesio,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We are annotating a corpus with information relevant to discourse entity realization, and especially the information needed to decide which type of NP to use. The corpus is being used to study correlations between NP type and certain semantic or discourse features, to evaluate hand-coded algorithms, and to train statistical models. We report on the development of our annotation scheme, the problems we have encountered, and the results obtained so far."
J00-4003,An Empirically-based System for Processing Definite Descriptions,2000,75,171,2,0,6070,renata vieira,Computational Linguistics,0,"We present an implemented system for processing definite descriptions in arbitrary domains. The design of the system is based on the results of a corpus analysis previously reported, which highlighted the prevalence of discourse-new descriptions in newspaper corpora. The annotated corpus was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions."
C00-2130,Corpus-based Development and Evaluation of a System for Processing Definite Descriptions,2000,20,6,2,0,6070,renata vieira,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We present an implemented system for processing definite descriptions. The system is based on the results of a corpus analysis previously reported, which showed how common discourse-new descriptions are in newspaper corpora, and identified several problems to be dealt with when developing computational methods for interpreting bridging descriptions. The annotated corpus produced in this earlier work was used to extensively evaluate the proposed techniques for matching definite descriptions with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and suggesting anchors for bridging descriptions."
C00-1045,Pronominalization revisited,2000,19,22,3,0,53165,renate henschel,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"Pronominalization has been related to the idea of a local focus - a set of discourse entities in the speaker's centre of attention, for example in Gundel et al. (1993)'s givenness hierarchy or in centering theory. Both accounts say that the determination of the focus depends on syntactic as well as pragmatic factors, but have not been able to pin those factors down. In this paper, we uncover the major factors which determine the focus set in descriptive texts. This new focus definition has been evaluated with respect to two corpora: museum exhibit labels, and newspaper articles. It provides an operationalizable basis for pronoun production, and has been implemented as the reusable module gnome-np. The algorithm behind gnome-np is compared with the most recent pronoun generation algorithm of McCoy and Strube (1999)."
A00-2001,Modelling Grounding and Discourse Obligations Using Update Rules,2000,13,94,2,0,45119,colin matheson,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper describes an implementation of some key aspects of a theory of dialogue processing whose main concerns are to provide models of GROUNDING and of the role of DISCOURSE OBLIGATIONS in an agent's deliberation processes. Our system uses the TrindiKit dialogue move engine toolkit, which assumes a model of dialogue in which a participant's knowledge is characterised in terms of INFORMATION STATES which are subject to various kinds of updating mechanisms."
P98-1090,Long Distance Pronominalisation and Global Focus,1998,23,19,2,1,47991,janet hitzeman,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"Our corpus of descriptive text contains a significant number of long-distance pronominal references (8.4% of the total). In order to account for how these pronouns are intepreted, we re-examine Grosz and Sidner's theory of the attentional state, and in particular the use of the global focus to supplement centering theory. Our corpus evidence concerning these long-distance pronominal references, as well as studies of the use of descriptions, proper names and ambiguous uses of pronouns, lead us to conclude that a discourse focus stack mechanism of the type proposed by Sidner is essential to account for the use of these referring expressions. We suggest revising the Grosz & Sidner framework by allowing for the possibility that an entity in a focus space may have special status."
J98-2001,A Corpus-based Investigation of Definite Description Use,1998,36,224,1,1,1743,massimo poesio,Computational Linguistics,0,"We present the results of a study of the use of definite descriptions in written texts aimed at assessing the feasibility of annotating corpora with information about definite description interpretation. We ran two experiments, in which subjects were asked to classify the uses of definite descriptions in a corpus of 33 newspaper articles, containing a total of 1,412 definite descriptions. We measured the agreement among annotators about the classes assigned to definite descriptions, as well as the agreement about the antecedent assigned to those definites that the annotators classified as being related to an antecedent in the text. The most interesting result of this study from a corpus annotation perspective was the rather low agreement (K = 0.63) that we obtained using versions of Hawkins's and Prince's classification schemes; better results (K = 0.76) were obtained using the simplified scheme proposed by Fraurud that includes only two classes, first-mention and subsequent-mention. The agreement about antecedents was also not complete. These findings raise questions concerning the starategy of evaluating systems for definite description interpretation by comparing their results with a standardized annotation. From a linguistic point of view, the most interesting observations were the great number of discourse-new definites in our corpus (in one of our experiments, about 50% of the definites in the collection were classified as discourse-new, 30% as anaphoric, and 18% as associative/bridging) and the presence of definites that did not seem to require a complete disambiguation."
C98-1087,Long Distance Pronominalisation and Global Focus,1998,23,19,2,1,47991,janet hitzeman,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"Our corpus of descriptive text contains a significant number of long-distance pronominal references (8.4% of the total). In order to account for how these pronouns are intepreted, we re-examine Grosz and Sidner's theory of the attentional state, and in particular the use of the global focus to supplement centering theory. Our corpus evidence concerning these long-distance pronominal references, as well as studies of the use of descriptions, proper names and ambiguous uses of pronouns, lead us to conclude that a discourse focus stack mechanism of the type proposed by Sidner is essential to account for the use of these referring expressions. We suggest revising the Grosz & Sidner framework by allowing for the possibility that an entity in a focus space may have special status."
W97-1301,Resolving bridging references in unrestricted text,1997,13,63,1,1,1743,massimo poesio,"Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts",0,"Our goal is to develop a system capable of treating the largest possible subset of definite descriptions in unrestricted written texts. A previous prototype resolved anaphoric uses of definite descriptions and identified some types of first-mention uses, achieving a recall of 56%. In this paper we present the latest version of our system, which handles some types of bridging references, uses WordNet as a source of lexical knowledge, and achieves a recall of 65%."
J96-1007,Book Reviews: Logic and Lexicon,1996,-1,-1,1,1,1743,massimo poesio,Computational Linguistics,0,None
P93-1010,Temporal Centering,1993,24,46,3,0,55082,megumi kameyama,31st Annual Meeting of the Association for Computational Linguistics,1,"We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge. A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators --- discourse reference intervals and event intervals. This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals. Our temporal property-sharing principle is a defeasible inference rule on the logical form. Along with lexical and causal reasoning, it plays a role in incrementally resolving underspecified aspects of the event structure representation of an utterance against the current context."
P93-1011,Assigning a Semantic Scope to Operators,1993,25,3,1,1,1743,massimo poesio,31st Annual Meeting of the Association for Computational Linguistics,1,"I propose that the characteristics of the scope disambiguation process observed in the literature can be explained in terms of the way in which the model of the situation described by a sentence is built. The model construction procedure I present builds an event structure by identifying the situations associated with the operators in the sentence and their mutual dependency relations, as well as the relations between these situations and other situations in the context. The procedure takes into account lexical semantics and the result of various discourse interpretation procedures such as definite description interpretation, and does not require a complete disambiguation to take place."
