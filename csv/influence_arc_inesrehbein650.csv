2020.acl-main.379,2020.lrec-1.641,0,0.0512648,"Missing"
2020.acl-main.379,C10-1011,0,0.0177086,"uding punctuation. 4.1 Dev UAS w/o punct. Dev Test 92.45 92.35 73.30 97.34 91.45 92.09 92.05 91.45 79.56 95.33 92.09 80.21 95.99 92.05 80.19 95.82 91.45 70.72 96.15 92.09 71.26 96.65 92.05 71.51 96.55 Table 1: Accuracy for k-best list from PTB. Top: accuracies reported in Zhu et al. (2015). Bottom: our k-best lists extracted with Huang and Sagae (2010)’s model using the parse forests. We follow the original train/dev/test splits and use the predicted POS and morphological tags provided by the shared task organizers. The top k parses are produced using the graph-based parser in the MATE tools (Bohnet, 2010),2 a non-neural model that employs second order, approximate nonprojective parsing (McDonald and Pereira, 2006). The algorithm first finds the highest scored projective tree with exact inference, then rearranges the edges one at a time as long as the overall score improves and the parse tree does not violate the tree constraint. This algorithm also creates a list of kbest trees through its search process. We also tried to generate the k-best lists with a transition-based parser by adding a beam search decoder, but the beam failed to improve the parsing upper bound. Czech We use the Czech Unive"
2020.acl-main.379,Q17-1010,0,0.0152028,"Missing"
2020.acl-main.379,P05-1022,0,0.192839,"utational Linguistics, pages 4123–4133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2 Related Work Reranking is a popular technique to improve parsing performance of the output of a base parser. First, the top k candidate trees are generated by the base parser, then these trees are reranked using additional features not accessible to the base parser. This adds a more global and complete view of the trees, in contrast to the local and incomplete features used by the parser. Discriminative rerankers have been a success story in constituency parsing (Collins and Koo, 2005; Charniak and Johnson, 2005). A disadvantage of the traditional feature-rich rerankers is that the large number of potentially sparse features makes them prone to overfitting, and also reduces the efficiency of the systems. Neural rerankers offer a solution to that problem by learning dense, low-dimensional feature representations that are better at generalization, and so reduce the risk of overfitting. Neural reranking The first neural reranker has been presented by Socher et al. (2013) for constituency parsing, based on a recursive neural network which processes the nodes in the parse tree bottom-up and learns dense fe"
2020.acl-main.379,J05-1003,0,0.205235,"he Association for Computational Linguistics, pages 4123–4133 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2 Related Work Reranking is a popular technique to improve parsing performance of the output of a base parser. First, the top k candidate trees are generated by the base parser, then these trees are reranked using additional features not accessible to the base parser. This adds a more global and complete view of the trees, in contrast to the local and incomplete features used by the parser. Discriminative rerankers have been a success story in constituency parsing (Collins and Koo, 2005; Charniak and Johnson, 2005). A disadvantage of the traditional feature-rich rerankers is that the large number of potentially sparse features makes them prone to overfitting, and also reduces the efficiency of the systems. Neural rerankers offer a solution to that problem by learning dense, low-dimensional feature representations that are better at generalization, and so reduce the risk of overfitting. Neural reranking The first neural reranker has been presented by Socher et al. (2013) for constituency parsing, based on a recursive neural network which processes the nodes in the parse tree"
2020.acl-main.379,P07-1050,0,0.758166,"Missing"
2020.acl-main.379,Q13-1012,0,0.020869,"s in the parse trees (see §3 for a more detailed description of the two models). k-best vs. forest reranking There exist two different approaches to reranking for parsing: k-best reranking and forest reranking. In k-best reranking, the complete parse tree is encoded and presented to the reranker. A disadvantage of k-best reranking is the limited scope of the k-best list which provides an upper bound for reranking performance. In contrast, a packed parse forest is a compact representation of exponentially many trees of which each node represents a deductive step. Forest reranking (Huang, 2008; Hayashi et al., 2013) approximately decodes the highest scored tree with both local and non-local features in a parse forest with cube pruning (Huang and Chiang, 2005). that are both non-projective and produce packed parse forests as output. 3 Neural Reranking Models for Dependency Parsing In this section, we look into reranking for dependency parsing and compare two different types of models: the generative inside-outside recursive neural network (IORNN) reranker (Le and Zuidema, 2014) and the discriminative reranker based on recurrent convolutional neural networks (RCNNs) (Zhu et al., 2015). In addition, we prop"
2020.acl-main.379,P08-1067,0,0.0678741,"ds and phrases in the parse trees (see §3 for a more detailed description of the two models). k-best vs. forest reranking There exist two different approaches to reranking for parsing: k-best reranking and forest reranking. In k-best reranking, the complete parse tree is encoded and presented to the reranker. A disadvantage of k-best reranking is the limited scope of the k-best list which provides an upper bound for reranking performance. In contrast, a packed parse forest is a compact representation of exponentially many trees of which each node represents a deductive step. Forest reranking (Huang, 2008; Hayashi et al., 2013) approximately decodes the highest scored tree with both local and non-local features in a parse forest with cube pruning (Huang and Chiang, 2005). that are both non-projective and produce packed parse forests as output. 3 Neural Reranking Models for Dependency Parsing In this section, we look into reranking for dependency parsing and compare two different types of models: the generative inside-outside recursive neural network (IORNN) reranker (Le and Zuidema, 2014) and the discriminative reranker based on recurrent convolutional neural networks (RCNNs) (Zhu et al., 2015"
2020.acl-main.379,W05-1506,0,0.128794,"s to reranking for parsing: k-best reranking and forest reranking. In k-best reranking, the complete parse tree is encoded and presented to the reranker. A disadvantage of k-best reranking is the limited scope of the k-best list which provides an upper bound for reranking performance. In contrast, a packed parse forest is a compact representation of exponentially many trees of which each node represents a deductive step. Forest reranking (Huang, 2008; Hayashi et al., 2013) approximately decodes the highest scored tree with both local and non-local features in a parse forest with cube pruning (Huang and Chiang, 2005). that are both non-projective and produce packed parse forests as output. 3 Neural Reranking Models for Dependency Parsing In this section, we look into reranking for dependency parsing and compare two different types of models: the generative inside-outside recursive neural network (IORNN) reranker (Le and Zuidema, 2014) and the discriminative reranker based on recurrent convolutional neural networks (RCNNs) (Zhu et al., 2015). In addition, we propose a new reranking model for dependency parsing that employs graph convolutional networks (GCNs) to encode the trees. 3.1 Generative models A gen"
2020.acl-main.379,P10-1110,0,0.14475,"of the models above does consider the scores from the base parser when ranking trees. Therefore, it seems plausible to try combining the advantages from both models, base parser and reranker, to produce a better final model. The most common way to do so is to consider the base parser and the reranker as a mixture model. The score of any reranking model sr can be combined with the score of the base parser sb using a linear combination: s(x, y) = αsr (x, y, Θ) + (1 − α)sb (x, y) where α ∈ [0, 1] is a parameter. 4125 (6) 4 Evaluating Neural Rerankers for Dependency Parsing UAS w/ punct. Dataset Huang and Sagae (2010) Baseline 91.34 k = 10, forest Top tree 91.34 Oracle worst 79.68 Oracle best 95.31 k = 64, forest Top tree 91.34 Oracle worst 70.62 Oracle best 96.06 Data English Following Zhu et al. (2015), we use the Penn Treebank (PTB) with standard splits: sections 2-21 for training, section 22 for development and section 23 for testing. Their reranking models are applied to unlabeled trees. The authors used the linear incremental parser from Huang and Sagae (2010) to produce k-best lists and achieved slight improvements due to differences in optimization. In contrast, we obtained the data and pretrained"
2020.acl-main.379,D14-1081,0,0.255806,"sparse features makes them prone to overfitting, and also reduces the efficiency of the systems. Neural rerankers offer a solution to that problem by learning dense, low-dimensional feature representations that are better at generalization, and so reduce the risk of overfitting. Neural reranking The first neural reranker has been presented by Socher et al. (2013) for constituency parsing, based on a recursive neural network which processes the nodes in the parse tree bottom-up and learns dense feature presentations for the whole tree. This approach was adapted for dependency parsing by Le and Zuidema (2014). Zhu et al. (2015) improve on previous work by proposing a recursive convolutional neural network (RCNN) architecture for reranking which can capture syntactic and semantic properties of words and phrases in the parse trees (see §3 for a more detailed description of the two models). k-best vs. forest reranking There exist two different approaches to reranking for parsing: k-best reranking and forest reranking. In k-best reranking, the complete parse tree is encoded and presented to the reranker. A disadvantage of k-best reranking is the limited scope of the k-best list which provides an upper"
2020.acl-main.379,P14-2050,0,0.0118966,"t Oracle best Czech Baseline k = 50 Top tree Oracle worst Oracle best Test LAS UAS LAS 92.91 91.04 90.19 87.90 91.75 81.20 96.40 90.04 79.48 95.08 88.36 79.04 93.51 86.28 77.12 91.71 92.22 89.30 91.87 88.85 91.02 82.24 95.04 88.28 79.68 92.71 90.74 81.98 94.70 87.93 79.32 92.29 Table 2: k-best list accuracies for the German SPMRL and Czech UD datasets. Pre-trained word embeddings In all experiments on English, we use the 50-dimensional GloVe word embeddings (Pennington et al., 2014) trained on Wikipedia 2014 and Gigaword 5. For German, we train 100-dimensional dependencybased word embeddings (Levy and Goldberg, 2014) on the SdeWaC corpus (Faaß and Eckart, 2013) with a cutoff frequency of 20 for both words and contexts and set the number of negative samples to 15. In experiments on Czech, we reduce the number of dimensions of the word vectors from fastText (Bojanowski et al., 2017) to 100 using PCA (Raunak et al., 2019). 4.2 Reproducing reranking results for PTB This section is dedicated to the reproduction of the published results for the IORNN and RCNN rerankers on the English PTB. All results are from one run since we observe little variation between different runs4 (and even between different settings"
2020.acl-main.379,D17-1159,0,0.0134962,"so if the edge is closer to the lower level, since the difference spreads to the upper level. Thus, we believe that a discriminative reranker can benefit from a model that considers nodes in a tree more equally, as done in our GCN model below. GCN GCNs have been used to encode nodes in a graph with (syntactic) information from their neighbors. By stacking several layers of GCNs, the learned representation can capture information about directly connected nodes (with only one layer), or nodes that are K hops away (with K layers). We adapt the syntactic gated GCNs for semantic role labeling from Marcheggiani and Titov (2017) to encode the parse trees in our experiments. To our best knowledge, this is the first time GCNs are used for reranking in dependency parsing. Let the hidden representation of node v after K (K) GCN layers be hv . The plausibility score of each tree is the sum of the scores of all nodes in the tree: s(x, y, Θ) = X v · h(K) v (4) v∈y Training Given an input sentence x, the input to the reranker is the corresponding correct parse tree y and a list of trees generated by a base parser gen(x). As in conventional ranking systems, all discriminative rerankers can be trained with a margin-based hinge"
2020.acl-main.379,E06-1011,0,0.0457916,"5 79.56 95.33 92.09 80.21 95.99 92.05 80.19 95.82 91.45 70.72 96.15 92.09 71.26 96.65 92.05 71.51 96.55 Table 1: Accuracy for k-best list from PTB. Top: accuracies reported in Zhu et al. (2015). Bottom: our k-best lists extracted with Huang and Sagae (2010)’s model using the parse forests. We follow the original train/dev/test splits and use the predicted POS and morphological tags provided by the shared task organizers. The top k parses are produced using the graph-based parser in the MATE tools (Bohnet, 2010),2 a non-neural model that employs second order, approximate nonprojective parsing (McDonald and Pereira, 2006). The algorithm first finds the highest scored projective tree with exact inference, then rearranges the edges one at a time as long as the overall score improves and the parse tree does not violate the tree constraint. This algorithm also creates a list of kbest trees through its search process. We also tried to generate the k-best lists with a transition-based parser by adding a beam search decoder, but the beam failed to improve the parsing upper bound. Czech We use the Czech Universal Dependencies (UD) Treebank,3 based on the Prague Dependency Treebank 3.0 (Bejˇcek et al., 2013). We use th"
2020.acl-main.379,D13-1032,0,0.0281828,"Missing"
2020.acl-main.379,D14-1162,0,0.0809104,"4126 https://code.google.com/p/mate-tools https://universaldependencies.org/ Dataset Dev UAS German Baseline k = 50 Top tree Oracle worst Oracle best Czech Baseline k = 50 Top tree Oracle worst Oracle best Test LAS UAS LAS 92.91 91.04 90.19 87.90 91.75 81.20 96.40 90.04 79.48 95.08 88.36 79.04 93.51 86.28 77.12 91.71 92.22 89.30 91.87 88.85 91.02 82.24 95.04 88.28 79.68 92.71 90.74 81.98 94.70 87.93 79.32 92.29 Table 2: k-best list accuracies for the German SPMRL and Czech UD datasets. Pre-trained word embeddings In all experiments on English, we use the 50-dimensional GloVe word embeddings (Pennington et al., 2014) trained on Wikipedia 2014 and Gigaword 5. For German, we train 100-dimensional dependencybased word embeddings (Levy and Goldberg, 2014) on the SdeWaC corpus (Faaß and Eckart, 2013) with a cutoff frequency of 20 for both words and contexts and set the number of negative samples to 15. In experiments on Czech, we reduce the number of dimensions of the word vectors from fastText (Bojanowski et al., 2017) to 100 using PCA (Raunak et al., 2019). 4.2 Reproducing reranking results for PTB This section is dedicated to the reproduction of the published results for the IORNN and RCNN rerankers on the"
2020.acl-main.379,W19-4328,0,0.0202343,"Missing"
2020.acl-main.379,W14-6111,0,0.0530457,"Missing"
2020.acl-main.379,P13-1045,0,0.00878922,"features used by the parser. Discriminative rerankers have been a success story in constituency parsing (Collins and Koo, 2005; Charniak and Johnson, 2005). A disadvantage of the traditional feature-rich rerankers is that the large number of potentially sparse features makes them prone to overfitting, and also reduces the efficiency of the systems. Neural rerankers offer a solution to that problem by learning dense, low-dimensional feature representations that are better at generalization, and so reduce the risk of overfitting. Neural reranking The first neural reranker has been presented by Socher et al. (2013) for constituency parsing, based on a recursive neural network which processes the nodes in the parse tree bottom-up and learns dense feature presentations for the whole tree. This approach was adapted for dependency parsing by Le and Zuidema (2014). Zhu et al. (2015) improve on previous work by proposing a recursive convolutional neural network (RCNN) architecture for reranking which can capture syntactic and semantic properties of words and phrases in the parse trees (see §3 for a more detailed description of the two models). k-best vs. forest reranking There exist two different approaches t"
2020.acl-main.379,P16-1132,0,0.844748,"Missing"
2020.acl-main.379,P15-1112,0,0.123209,"makes them prone to overfitting, and also reduces the efficiency of the systems. Neural rerankers offer a solution to that problem by learning dense, low-dimensional feature representations that are better at generalization, and so reduce the risk of overfitting. Neural reranking The first neural reranker has been presented by Socher et al. (2013) for constituency parsing, based on a recursive neural network which processes the nodes in the parse tree bottom-up and learns dense feature presentations for the whole tree. This approach was adapted for dependency parsing by Le and Zuidema (2014). Zhu et al. (2015) improve on previous work by proposing a recursive convolutional neural network (RCNN) architecture for reranking which can capture syntactic and semantic properties of words and phrases in the parse trees (see §3 for a more detailed description of the two models). k-best vs. forest reranking There exist two different approaches to reranking for parsing: k-best reranking and forest reranking. In k-best reranking, the complete parse tree is encoded and presented to the reranker. A disadvantage of k-best reranking is the limited scope of the k-best list which provides an upper bound for rerankin"
2020.argmining-1.4,W15-1622,0,0.0280961,"greement for MF annotation casts doubt on the validity of the findings. While we expect that more extensive training and more detailed guidelines will increase IAA for human annotation at least slightly, we still think that due to the fuzziness of the concept of morality, high agreement scores are not very probable. Thus, we would like to propose a different approach to the annotation of moral foundations where we ground the annotations in lexical semantics. This approach has already been shown to improve IAA for a similarly difficult annotation task, namely the annotation of causal language (Dunietz et al., 2015). The authors created a lexical resource for terms that can trigger causality in text and instructed annotators to disambiguate instances of those terms in context, showing that their modularized, dictionary-based approach yields substantially increased IAA scores. 37 Inspired by their work, we propose to anchor MF annotations in lexical semantics, using an expanded version of the MFD as seed terms. The annotators will then be presented with instances of these terms and instructed to disambiguate them in context, and also to annotate a small, predefined set of semantic roles, such as Betrayer,"
2020.argmining-1.4,L16-1591,0,0.0607593,"Missing"
2020.argmining-1.4,D16-1129,0,0.0408923,"Missing"
2020.argmining-1.4,P18-1067,0,0.0224223,"opolitical, social, and cultural variables. Garten et al. (2016) address the coverage problem of dictionary-based approaches by replacing the terms in the MF dictionary with their averaged vector representations in distributional space. They show that predicting moral foundations based on the cosine similarity of the words in a text to the distributional representations outperforms a naive method that predicts MF based on word counts. Machine learning-based approaches Recent work has applied the framework of MFT to research questions in the social and political sciences (Fulgoni et al., 2016; Johnson and Goldwasser, 2018; Rezapour et al., 2019; Xie et al., 2019), replacing dictionary-based counts with more sophisticated methods. Johnson and Goldwasser (2018) model moral framing in politicians’ tweets, using probabilistic soft logic (Bach et al., 2013). Lin et al. (2018) improve the prediction of moral foundations by acquiring additional background knowledge from Wikipedia, using information extraction techniques such as entity linking and cross-document knowledge propagation. Xie et al. (2019) study the change of moral sentiment in longitudinal data, presenting a parameter-free model that predicts moral senti"
2020.argmining-1.4,D14-1162,0,0.0833565,"n siamese networks. The modified SBERT encodes sentence similarity in a human interpretable way where similar sentence pairs can be retrieved efficiently, based on cosine similarity. While previous work has computed BERT-based embeddings for text sequences (i) by averaging (or summing) over all word embeddings for this particular text or (ii) by using the network output at the position of the [CLS] token, Reimers and Gurevych (2019) show that those representations are not well suited to encode sentence similarity and often yield inferior results as compared to using averaged Glove embeddings (Pennington et al., 2014). 7 GeneralMorality includes terms related to moral concepts that do not fit into one of the five MFs (e.g. ethic, good, evil). http://wordnet-rdf.princeton.edu/json/pwn30/... 9 We are aware that this treatment is not optimal. A better solution would link those terms to a synset that captures their offensive usage, similar to the one for Kraut: offensive term for a person of German descent. 8 33 SBERT can also be applied to tasks where an anchor text is compared to a positive and a negative text sample, thus learning to maximize a score based on the similarity of the anchor text to the first s"
2020.argmining-1.4,D19-1410,0,0.0646209,"ually. For instance, a variety of offensive terms have been removed in WN 3.1, and thus, we had to link terms like darky or tom to black (noun.person) manually.9 As each of the synsets is assigned a fixed-size score vector in our lexicon, any function to aggregate these vectors is conceivable. To obtain vectors that do not depend on the input text’s length, we decided to take their mean. The result is a vector consisting of 6 entries, where each entry represents a MF, including GeneralMorality. II. Contextualized MF sequence embeddings (SBERT-Wiki) Our second method uses SentenceBERT (SBERT) (Reimers and Gurevych, 2019) to obtain text representations that encode moral sentiment. SBERT is a modification of the original transformer model, based on siamese networks. The modified SBERT encodes sentence similarity in a human interpretable way where similar sentence pairs can be retrieved efficiently, based on cosine similarity. While previous work has computed BERT-based embeddings for text sequences (i) by averaging (or summing) over all word embeddings for this particular text or (ii) by using the network output at the position of the [CLS] token, Reimers and Gurevych (2019) show that those representations are"
2020.argmining-1.4,W19-1305,0,0.0699916,", protect*, compassion* for Carevirtue and suffer*, crush*, killer* for Carevice . The MFD includes, on average, 32 words per moral subcategory. Frimer et al. (2019) presents a new version of the MFD with more entries per MF subcategory, selected according to prototypicality estimates for each MF, based on cosine similarity for Word2Vec embeddings for each item in the dictionary. While the authors admit that the construct validity of the MFD 2.0 is not better than for the original MFD, they recommend the use of the MFD 2.0 due to its improved coverage. Other work on expanding the MFD includes Rezapour et al. (2019) who increase the original size of the dictionary to over 4,600 lexical items, using a quality controlled, human in the loop process.4 The MF dictionary has been used in several studies in the political and social sciences, psychology, and related fields (Takikawa and Sakamoto, 2017; Matsuo et al., 2018; Lewis, 2019). Dictionary-based approaches to measuring moral values in text, however, have severe shortcomings. They can neither 3 Another foundation currently under discussion is Liberty-Opression. Arguably, a comparison of size is not meaningful, given that the original MFD includes regular"
2020.argmining-1.4,W17-5203,0,0.0562437,"Missing"
2020.argmining-1.4,E17-1017,0,0.0257716,"umentative text, we investigate whether we can find correlations between moral values and different aspects of argumentation, such as argument quality, stance, or audience approval. We are interested in the following research questions: RQ1: Do debaters that produce high-quality arguments make more or less frequent use of moral framing? RQ2: Is moral framing more strongly related to a positive or negative stance? RQ3: Can we find a positive correlation between the audiences’ approval and the use of moral frames? Our main contributions are the following: (i) We augment the ArgQuality Corpus of Wachsmuth et al. (2017) with annotations for moral values, as a first test set for the evaluation of moral sentiment in argumentation; (ii) We evaluate two methods for the prediction of moral sentiment on the new dataset; (iii) We present a correlation study investigating the relation between moral framing and argument quality, stance, and audience reactions. The paper is structured as follows. We first review work on quantifying moral sentiment in text (§2). In §3, we describe the annotation of our test set and present different approaches to the automatic detection of moral sentiment in debates. Then we discuss ou"
2020.argmining-1.4,L18-1192,0,0.04126,"Missing"
2020.argmining-1.4,D19-1472,0,0.0191778,"al. (2016) address the coverage problem of dictionary-based approaches by replacing the terms in the MF dictionary with their averaged vector representations in distributional space. They show that predicting moral foundations based on the cosine similarity of the words in a text to the distributional representations outperforms a naive method that predicts MF based on word counts. Machine learning-based approaches Recent work has applied the framework of MFT to research questions in the social and political sciences (Fulgoni et al., 2016; Johnson and Goldwasser, 2018; Rezapour et al., 2019; Xie et al., 2019), replacing dictionary-based counts with more sophisticated methods. Johnson and Goldwasser (2018) model moral framing in politicians’ tweets, using probabilistic soft logic (Bach et al., 2013). Lin et al. (2018) improve the prediction of moral foundations by acquiring additional background knowledge from Wikipedia, using information extraction techniques such as entity linking and cross-document knowledge propagation. Xie et al. (2019) study the change of moral sentiment in longitudinal data, presenting a parameter-free model that predicts moral sentiment on three different levels: (i) moral"
2020.coling-main.185,P08-1037,0,0.142521,"ized by Atterer and Sch¨utze (2007). The authors argue that using gold information for candidate extraction is highly unrealistic as it will always include the correct solution in the candidate set, while in real applications the correct solution might not be available. Therefore, PP attachment disambiguation systems should be used to refine the preliminary syntactic analysis of a sentence provided by a parser, or to reattach PP attachments. Their experiments with three PP reattachment systems show that none of the systems was able to obtain a significant improvement over the baseline parser. Agirre et al. (2008) later follow their evaluation setup and report a small but significant improvement in parsing accuracy using word sense information. 2050 Up to now, the PP attachment disambiguation problem has been studied extensively, with the latest models incorporating novel techniques like neural networks and word embeddings. Most proposed approaches revolve around improving lexical coverage, either by utilizing large, external corpora or by integrating semantic information. Despite the fact that some systems report better results than a baseline parser, results for PP attachment still fall behind those"
2020.coling-main.185,J07-4002,0,0.109886,"Missing"
2020.coling-main.185,P98-1013,0,0.0713563,"lated are likely to occur in similar contexts, and thus will show similar selectional preferences regarding PP attachment. The semantic word class can be determined based on external resources like WordNet (Fellbaum, 1998) (Brill and Resnik, 1994; Belinkov et al., 2014; Dasigi et al., 2017), or automatically via mutual information clustering (Ratnaparkhi et al., 1994). Alternatively, lexical sparsity can also be tackled by extending the lexicon with semantically similar words from syntactic collocation databases and thesauri (Pantel and Lin, 2000). Semantic information of verbs from FrameNet (Baker et al., 1998) or VerbNet (Schuler, 2005) have also been used as features in PP attachment disambiguation systems (Belinkov et al., 2014; Schuler, 2005). Syntactic features have been used to recover missing context information in the extracted system input. Examples are the distance between the candidate head and the preposition (Olteanu and Moldovan, 2005; Belinkov et al., 2014; de Kok et al., 2017b), verb subcategorization (Olteanu and Moldovan, 2005) or topological fields (de Kok et al., 2017b). Pre-trained word embeddings that capture both syntax and semantics are frequently used as input to PP attachme"
2020.coling-main.185,Q14-1043,0,0.0927501,"possible attachment sites, as well as other potential verbs or nouns in the same sentence that might also be attachment candidates. For example, Brill and Resnik (1994) and Ratnaparkhi et al. (1994) formally define the task input as a quadruple (v, n1 , p, n2 ) where v and n1 are a verbal and a nominal head candidate, p is the preposition and n2 is the head of the object of p. This setup has been criticized in recent work as highly artificial and unrealistic and has been extended by considering a larger set of possible attachment sites in the sentence, e.g., all words within a certain window (Belinkov et al., 2014), or by selecting head candidates based on linguistic criteria such as topological fields (de Kok et al., 2017a). Features A diverse set of features has been proposed in the literature. Lexical association scores computed on external, large corpora can help to compensate for limited training data in supervised approaches. The unannotated data are parsed to extract tuples of the form (v, n1 , p, n2 ), and their counts are used as features for PP attachment disambiguation (Hindle and Rooth, 1993; Ratnaparkhi, 1998; Pantel and Lin, 2000; de Kok et al., 2017b). Alternatively, lexical associations"
2020.coling-main.185,C10-1011,0,0.0618326,"Missing"
2020.coling-main.185,C94-2195,0,0.766179,"ommons.org/licenses/by/4.0/. License details: http:// 2049 Proceedings of the 28th International Conference on Computational Linguistics, pages 2049–2061 Barcelona, Spain (Online), December 8-13, 2020 PP attachment disambiguation system and evaluate it in a realistic scenario, comparing its performance to that of a strong neural parser (§3). In §4 we propose a new approach based on contextualized word embeddings that overcomes limitations of previous work, and we summarize our results in §5. 2 Related Work Problem formulation Early work on PP attachment disambiguation (Hindle and Rooth, 1993; Brill and Resnik, 1994; Ratnaparkhi et al., 1994) traditionally formulated the task as a binary choice between a given verbal and a nominal head candidate while ignoring other parts of speech as possible attachment sites, as well as other potential verbs or nouns in the same sentence that might also be attachment candidates. For example, Brill and Resnik (1994) and Ratnaparkhi et al. (1994) formally define the task input as a quadruple (v, n1 , p, n2 ) where v and n1 are a verbal and a nominal head candidate, p is the preposition and n2 is the head of the object of p. This setup has been criticized in recent work a"
2020.coling-main.185,W17-0404,0,0.0529951,"Missing"
2020.coling-main.185,P17-1191,0,0.0117135,"00; de Kok et al., 2017b). Alternatively, lexical associations can be estimated based on word co-occurrences in a very large corpus like the World Wide Web (WWW), queried by a search engine (Volk, 2001; Olteanu and Moldovan, 2005). Semantic information can also help to overcome data sparsity since words that are semantically related are likely to occur in similar contexts, and thus will show similar selectional preferences regarding PP attachment. The semantic word class can be determined based on external resources like WordNet (Fellbaum, 1998) (Brill and Resnik, 1994; Belinkov et al., 2014; Dasigi et al., 2017), or automatically via mutual information clustering (Ratnaparkhi et al., 1994). Alternatively, lexical sparsity can also be tackled by extending the lexicon with semantically similar words from syntactic collocation databases and thesauri (Pantel and Lin, 2000). Semantic information of verbs from FrameNet (Baker et al., 1998) or VerbNet (Schuler, 2005) have also been used as features in PP attachment disambiguation systems (Belinkov et al., 2014; Schuler, 2005). Syntactic features have been used to recover missing context information in the extracted system input. Examples are the distance be"
2020.coling-main.185,P16-2001,0,0.0671163,"Missing"
2020.coling-main.185,E17-2050,0,0.0306999,"Missing"
2020.coling-main.185,N19-1423,0,0.0175653,"Missing"
2020.coling-main.185,K17-3002,0,0.0682041,"Missing"
2020.coling-main.185,P06-2029,0,0.070052,"orization (Olteanu and Moldovan, 2005) or topological fields (de Kok et al., 2017b). Pre-trained word embeddings that capture both syntax and semantics are frequently used as input to PP attachment disambiguation systems with a neural network architecture (Belinkov et al., 2014; Dasigi et al., 2017; de Kok et al., 2017b) to improve lexical coverage and enrich the lexical information. Comparison with syntactic parsing Early work focuses on evaluating PP attachment disambiguation as an independent task rather than comparing results to parser output or integrating it with parsing. Exceptions are Foth and Menzel (2006) and Roh et al. (2011) who integrate lexical preferences in rulebased parsers. Disambiguation systems that rely on an oracle (the mechanism that extracts the two candidate attachment sites, based on gold standard data) have been criticized by Atterer and Sch¨utze (2007). The authors argue that using gold information for candidate extraction is highly unrealistic as it will always include the correct solution in the candidate set, while in real applications the correct solution might not be available. Therefore, PP attachment disambiguation systems should be used to refine the preliminary synta"
2020.coling-main.185,J93-1005,0,0.635601,"ional License. creativecommons.org/licenses/by/4.0/. License details: http:// 2049 Proceedings of the 28th International Conference on Computational Linguistics, pages 2049–2061 Barcelona, Spain (Online), December 8-13, 2020 PP attachment disambiguation system and evaluate it in a realistic scenario, comparing its performance to that of a strong neural parser (§3). In §4 we propose a new approach based on contextualized word embeddings that overcomes limitations of previous work, and we summarize our results in §5. 2 Related Work Problem formulation Early work on PP attachment disambiguation (Hindle and Rooth, 1993; Brill and Resnik, 1994; Ratnaparkhi et al., 1994) traditionally formulated the task as a binary choice between a given verbal and a nominal head candidate while ignoring other parts of speech as possible attachment sites, as well as other potential verbs or nouns in the same sentence that might also be attachment candidates. For example, Brill and Resnik (1994) and Ratnaparkhi et al. (1994) formally define the task input as a quadruple (v, n1 , p, n2 ) where v and n1 are a verbal and a nominal head candidate, p is the preposition and n2 is the head of the object of p. This setup has been cri"
2020.coling-main.185,D12-1096,0,0.0483176,"Missing"
2020.coling-main.185,P14-2050,0,0.0772785,"Missing"
2020.coling-main.185,D13-1032,0,0.0530322,"Missing"
2020.coling-main.185,H05-1035,0,0.0783738,"features has been proposed in the literature. Lexical association scores computed on external, large corpora can help to compensate for limited training data in supervised approaches. The unannotated data are parsed to extract tuples of the form (v, n1 , p, n2 ), and their counts are used as features for PP attachment disambiguation (Hindle and Rooth, 1993; Ratnaparkhi, 1998; Pantel and Lin, 2000; de Kok et al., 2017b). Alternatively, lexical associations can be estimated based on word co-occurrences in a very large corpus like the World Wide Web (WWW), queried by a search engine (Volk, 2001; Olteanu and Moldovan, 2005). Semantic information can also help to overcome data sparsity since words that are semantically related are likely to occur in similar contexts, and thus will show similar selectional preferences regarding PP attachment. The semantic word class can be determined based on external resources like WordNet (Fellbaum, 1998) (Brill and Resnik, 1994; Belinkov et al., 2014; Dasigi et al., 2017), or automatically via mutual information clustering (Ratnaparkhi et al., 1994). Alternatively, lexical sparsity can also be tackled by extending the lexicon with semantically similar words from syntactic collo"
2020.coling-main.185,P00-1014,0,0.172925,"in the sentence, e.g., all words within a certain window (Belinkov et al., 2014), or by selecting head candidates based on linguistic criteria such as topological fields (de Kok et al., 2017a). Features A diverse set of features has been proposed in the literature. Lexical association scores computed on external, large corpora can help to compensate for limited training data in supervised approaches. The unannotated data are parsed to extract tuples of the form (v, n1 , p, n2 ), and their counts are used as features for PP attachment disambiguation (Hindle and Rooth, 1993; Ratnaparkhi, 1998; Pantel and Lin, 2000; de Kok et al., 2017b). Alternatively, lexical associations can be estimated based on word co-occurrences in a very large corpus like the World Wide Web (WWW), queried by a search engine (Volk, 2001; Olteanu and Moldovan, 2005). Semantic information can also help to overcome data sparsity since words that are semantically related are likely to occur in similar contexts, and thus will show similar selectional preferences regarding PP attachment. The semantic word class can be determined based on external resources like WordNet (Fellbaum, 1998) (Brill and Resnik, 1994; Belinkov et al., 2014; Da"
2020.coling-main.185,H94-1048,0,0.83439,".0/. License details: http:// 2049 Proceedings of the 28th International Conference on Computational Linguistics, pages 2049–2061 Barcelona, Spain (Online), December 8-13, 2020 PP attachment disambiguation system and evaluate it in a realistic scenario, comparing its performance to that of a strong neural parser (§3). In §4 we propose a new approach based on contextualized word embeddings that overcomes limitations of previous work, and we summarize our results in §5. 2 Related Work Problem formulation Early work on PP attachment disambiguation (Hindle and Rooth, 1993; Brill and Resnik, 1994; Ratnaparkhi et al., 1994) traditionally formulated the task as a binary choice between a given verbal and a nominal head candidate while ignoring other parts of speech as possible attachment sites, as well as other potential verbs or nouns in the same sentence that might also be attachment candidates. For example, Brill and Resnik (1994) and Ratnaparkhi et al. (1994) formally define the task input as a quadruple (v, n1 , p, n2 ) where v and n1 are a verbal and a nominal head candidate, p is the preposition and n2 is the head of the object of p. This setup has been criticized in recent work as highly artificial and unr"
2020.coling-main.185,P98-2177,0,0.46254,"le attachment sites in the sentence, e.g., all words within a certain window (Belinkov et al., 2014), or by selecting head candidates based on linguistic criteria such as topological fields (de Kok et al., 2017a). Features A diverse set of features has been proposed in the literature. Lexical association scores computed on external, large corpora can help to compensate for limited training data in supervised approaches. The unannotated data are parsed to extract tuples of the form (v, n1 , p, n2 ), and their counts are used as features for PP attachment disambiguation (Hindle and Rooth, 1993; Ratnaparkhi, 1998; Pantel and Lin, 2000; de Kok et al., 2017b). Alternatively, lexical associations can be estimated based on word co-occurrences in a very large corpus like the World Wide Web (WWW), queried by a search engine (Volk, 2001; Olteanu and Moldovan, 2005). Semantic information can also help to overcome data sparsity since words that are semantically related are likely to occur in similar contexts, and thus will show similar selectional preferences regarding PP attachment. The semantic word class can be determined based on external resources like WordNet (Fellbaum, 1998) (Brill and Resnik, 1994; Bel"
2020.coling-main.185,Y11-1060,0,0.0272169,"ovan, 2005) or topological fields (de Kok et al., 2017b). Pre-trained word embeddings that capture both syntax and semantics are frequently used as input to PP attachment disambiguation systems with a neural network architecture (Belinkov et al., 2014; Dasigi et al., 2017; de Kok et al., 2017b) to improve lexical coverage and enrich the lexical information. Comparison with syntactic parsing Early work focuses on evaluating PP attachment disambiguation as an independent task rather than comparing results to parser output or integrating it with parsing. Exceptions are Foth and Menzel (2006) and Roh et al. (2011) who integrate lexical preferences in rulebased parsers. Disambiguation systems that rely on an oracle (the mechanism that extracts the two candidate attachment sites, based on gold standard data) have been criticized by Atterer and Sch¨utze (2007). The authors argue that using gold information for candidate extraction is highly unrealistic as it will always include the correct solution in the candidate set, while in real applications the correct solution might not be available. Therefore, PP attachment disambiguation systems should be used to refine the preliminary syntactic analysis of a sen"
2020.coling-main.185,W14-6111,0,0.0648905,"Missing"
2020.coling-main.185,P19-1230,0,0.0462292,"Missing"
2020.lrec-1.566,C18-1139,0,0.0135655,"on on the GermEval dataset mostly adds labels for temporal categories (DATE, DUR, TIME) and numeric categories (ORDINAL, CARDINAL,MON, QUANT, PERC). 5. Experiments To establish baseline scores for tagging performance using our scheme, we experimented with two systems that model the task of NER as a sequence labeling problem. The one for which we report results here is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Our second system uses the character-based contextual string embeddings, provided by the the flair library (Akbik et al., 2018; Akbik et al., 2019). Since the flair tagger was consistently outperformed by the BERT-based system, we do not report results for it here for lack of space. Transformers, of which BERT is an example, have recently pushed the state of the art for many NLP applications by ID 1 2 3 4 5 avg. Acc. (all) 98.19 98.23 98.16 98.20 98.17 98.19 Acc. 86.18 86.94 86.22 86.57 86.10 86.40 Prec. Rec. (non-O) 85.88 83.25 85.58 83.63 85.13 83.06 85.24 82.79 85.99 83.17 85.56 83.18 F1 84.55 84.60 84.08 83.99 84.56 84.36 Table 4: Results for NER sequence tagging with BERT on the German CoNLL-2003 data. TAG LOC M"
2020.lrec-1.566,N19-4010,0,0.0149201,"ataset mostly adds labels for temporal categories (DATE, DUR, TIME) and numeric categories (ORDINAL, CARDINAL,MON, QUANT, PERC). 5. Experiments To establish baseline scores for tagging performance using our scheme, we experimented with two systems that model the task of NER as a sequence labeling problem. The one for which we report results here is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Our second system uses the character-based contextual string embeddings, provided by the the flair library (Akbik et al., 2018; Akbik et al., 2019). Since the flair tagger was consistently outperformed by the BERT-based system, we do not report results for it here for lack of space. Transformers, of which BERT is an example, have recently pushed the state of the art for many NLP applications by ID 1 2 3 4 5 avg. Acc. (all) 98.19 98.23 98.16 98.20 98.17 98.19 Acc. 86.18 86.94 86.22 86.57 86.10 86.40 Prec. Rec. (non-O) 85.88 83.25 85.58 83.63 85.13 83.06 85.24 82.79 85.99 83.17 85.56 83.18 F1 84.55 84.60 84.08 83.99 84.56 84.36 Table 4: Results for NER sequence tagging with BERT on the German CoNLL-2003 data. TAG LOC MISC ORG PER Prec. 84."
2020.lrec-1.566,benikova-etal-2014-nosta,0,0.0440065,"Missing"
2020.lrec-1.566,N19-1423,0,0.064446,"t ORGpart] [Christian Wulff PER] Fine: [VW-Aufsichtrat TITLE] [Christian Wulff PER] Finally, we see that the fine-grained annotation on the GermEval dataset mostly adds labels for temporal categories (DATE, DUR, TIME) and numeric categories (ORDINAL, CARDINAL,MON, QUANT, PERC). 5. Experiments To establish baseline scores for tagging performance using our scheme, we experimented with two systems that model the task of NER as a sequence labeling problem. The one for which we report results here is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Our second system uses the character-based contextual string embeddings, provided by the the flair library (Akbik et al., 2018; Akbik et al., 2019). Since the flair tagger was consistently outperformed by the BERT-based system, we do not report results for it here for lack of space. Transformers, of which BERT is an example, have recently pushed the state of the art for many NLP applications by ID 1 2 3 4 5 avg. Acc. (all) 98.19 98.23 98.16 98.20 98.17 98.19 Acc. 86.18 86.94 86.22 86.57 86.10 86.40 Prec. Rec. (non-O) 85.88 83.25 85.58 83.63 85.13 83.06 85.24 82.79 85.99 83.17 85.56 83.18 F1"
2020.lrec-1.566,E03-1068,0,0.2808,"Missing"
2020.lrec-1.566,gravier-etal-2012-etape,0,0.0328382,"Missing"
2020.lrec-1.566,C96-1079,0,0.344869,"Hierarchy (ENEH) with 200 categories in three layers.2 The ENEH intends to be domaingeneral and includes, for instance, paths from the Name-root such as Name→Facility→Line→{Railroad, Road, Canal, Water Route, Tunnel, Bridge} as well as Name→Product→Rule→{Rule Other, Treaty, Law}. Besides the Name root, the inventory has roots for Time (e.g. Timex other Spring semester) and Numex (numerical) expressions (e.g. Frequency twice, five times), which are not NEs but which are of interest for downstream applications. 2 https://nlp.cs.nyu.edu/ene/ In fact, the first Named Entity tag set introduced by Grishman and Sundheim (1996) already included categories for percentages, time and monetary expressions. By contrast, the work of Leitner et al. (2019) is more narrowly focused. They annotate German data from the legal domain with 19 fine-grained classes that can be mapped to 7 coarse classes.10 of the 19 fine categories are lawrelated and 9 are domain independent. Similarly, in work on entity recognition in traffic-related events, Schiersch et al. (2018) expand the classical 4-category NE inventory with domain general subtypes (e.g. ORG-COM(mercial) for businesses), domain-specific subtypes (e.g. LocationStop for public"
2020.lrec-1.566,W11-0411,0,0.0242263,"uld never occur in our data. On the other hand, we want more detail on some of the classic four categories as well as add some custom ones. Against this background, we take the categories used by the OntoNotes project (Weischedel et al., 2013) as a starting point. It recognizes 10 types of named entities and 7 types of what it calls values, such as MONEY and PERCENT. We add some categories and adjust and expand the definitions so they suit our overall inventory and account for linguistic facts of German as needed. The most comparable scheme to ours is the QUAERO scheme of Rosset et al. (2011; Grouin et al. (2011) that was used by the ETAPE evaluation campaign for French (Gravier et al., 2012) for NER annotation on a corpus of TV broadcast speech. It is hierarchical with 32 subcategories under 7 supercategories, whereas our scheme with 31 labels is flat. The schemes differ somewhat in where they add detail but they have many similar or overlapping categories. 3. 3.1. Data Israel corpus The main dataset we work with is the “Israel-Korpus: Wiener in Jerusalem”, or ISW corpus for short. It consists of transcriptions of biographic interviews of Israeli citizens who emigrated from Vienna, Austria, during th"
2020.lrec-1.566,W03-0419,0,0.0651763,"ddings with positional information and selfattention. The representations are trained in two different task setups, i.e. by predicting masked words based on their left and right context and by classifying two sentences based on how probable it is that the second one immediately succeeds the first one in a text document. As a result, the learned embeddings encode information about the left and right context for each word which makes them superior to most previous representations. Devlin et al. (2019) have proposed a BERT architecture for sequence tagging on the CoNLL-2003 NER shared task data (Sang and Meulder, 2003). The model uses the pretrained BERT embeddings for initialization and then finetunes the representations by adding a simple classification layer on top of the pre-trained BERT model and jointly optimizing the model parameters on the downstream task. Each BERT model provides its own tokenization which splits longer words into sub-tokens. The sequence tagger uses only the first sub-token as the input to the classifier, which then predicts a label for each token. In our experiments, we use the HuggingFace transformers library (Wolf et al., 2019) that provides pre-trained transformer models for d"
2020.lrec-1.566,L18-1703,0,0.0292337,"imes), which are not NEs but which are of interest for downstream applications. 2 https://nlp.cs.nyu.edu/ene/ In fact, the first Named Entity tag set introduced by Grishman and Sundheim (1996) already included categories for percentages, time and monetary expressions. By contrast, the work of Leitner et al. (2019) is more narrowly focused. They annotate German data from the legal domain with 19 fine-grained classes that can be mapped to 7 coarse classes.10 of the 19 fine categories are lawrelated and 9 are domain independent. Similarly, in work on entity recognition in traffic-related events, Schiersch et al. (2018) expand the classical 4-category NE inventory with domain general subtypes (e.g. ORG-COM(mercial) for businesses), domain-specific subtypes (e.g. LocationStop for public transit stops) and new domain-specific toplevel types (e.g. for Distance expressions). 2.3. NER on speech As our work is focused on biographic narratives, we do not require a large and deep hierarchy of NEs, many of which would never occur in our data. On the other hand, we want more detail on some of the classic four categories as well as add some custom ones. Against this background, we take the categories used by the OntoNo"
2020.lrec-1.566,schmidt-2014-database,0,0.0182056,"re domain differences. Keywords: Named Entity Recognition, spoken language, German, oral history corpora 1. Introduction While Named Entity Recognition (NER) is typically envisioned in service of NLP tasks such as information extraction, question answering, automatic translation, etc (Jurafsky, 2000), we are interested in it also from a corpus linguistic and digital humanities perspective. The Datenbank f¨ur Gesprochenes Deutsch (DGD; ‘Database for Spoken German’) that is hosted by the Leibniz Institute for the German Language is a repository of, and a platform for research on, spoken German (Schmidt, 2014). It contains a large, continuously growing collection of currently 34 variational and conversational corpora, totalling more than 4.000 hours of audiovisual material, which are used to address a wide variety of research questions. As a first step in adding a layer of shallow semantic analysis to these spoken corpora, we want to provide NER tags. Out of all the corpora in the database, we chose to begin with the ISW corpus, which contains transcripts of German-language biographic narrative interviews with Austrian-born emigrants to Israel. The ISW corpus is part of a series of three interrelat"
2020.lrec-1.566,sekine-nobata-2004-definition,0,0.278706,"Missing"
2020.lrec-1.645,N18-1090,0,0.154878,"ices, taking into account a number of issues whose classification was partially inspired by the list of topics from the Special Track on the Syntac5 http://www.spmrl.org/sancl-posters2014. html 6 This is not to say that there is no conventional, well-punctuated data on social media. For instance, many corporations and institutions employ social media managers who adhere to common editing standards. Conversely, some sentence boundaries in canonical written language are also ambiguous, e.g. in headings, tables 5242 Name ATDT (UD) Hi-En-CS TwitterAAE (TAAE) Reference (Albogamy and Ramsay, 2017) (Bhat et al., 2018) (Blodgett et al., 2018) Source Twitter Twitter Twitter TWITTIRÒ-UD (TWRO) DWT W2.0 Foreebank (Frb) Tweebank (Twb) Tweebank2 (Twb2) TDT xUGC ITU tweeDe Postwita-UD (Pst) FSMB EWT SDT CWT GUM (Cignarella et al., 2019) (Daiber and Van Der Goot, 2016) (Foster et al., 2011) (Kaljahi et al., 2015) (Kong et al., 2014) (Liu et al., 2018) (Luotolahti et al., 2015) (Martínez Alonso et al., 2016) (Pamay et al., 2015) (Rehbein et al., 2019) (Sanguinetti et al., 2018) (Seddah et al., 2012) (Silveira et al., 2014) (Wang et al., 2017) (Wang et al., 2014) (Zeldes, 2017) Twitter Twitter Twitter, sport forums"
2020.lrec-1.645,P04-3031,0,0.214172,"l constructs occurring in social media. For instance, (sequences of) hashtags and URLs are separated out into ‘sentences’ of their own whenever they occur at the beginning or at the end of a tweet and do not have any syntactic function. The above segmentation policies notwithstanding, tweeDe still features the use of parataxis for juxtaposed clauses that are not separated by punctuation. A third option besides not segmenting and segmenting manually is, of course, to segment automatically. In the spirit of maintaining a real-world scenario, Frb split their forum data into sentences using NLTK (Bird and Loper, 2004), with no post-corrections. Accordingly, the resource contains instances where multiple sentences are merged into one sentence due to punctuation errors such as a comma being used instead of a full stop, as in Example 1. Conversely, there are cases where a single sentence is split over multiple lines, resulting in multiple sentences (Example 2) that are not rejoined. (1) Combofix will start, When it is scanning don’t move the mouse cursor inside the box, can cause freezing. (2) I’m sure the devs. can give you more details on this Tokenization Tokenization problems in informal text include case"
2020.lrec-1.645,P18-1131,0,0.0314012,"Missing"
2020.lrec-1.645,W19-7803,0,0.0239954,"n ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding difficult by means of the"
2020.lrec-1.645,W16-1714,1,0.893259,"Missing"
2020.lrec-1.645,L16-1667,1,0.88063,"Missing"
2020.lrec-1.645,L16-1248,0,0.0199681,"246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the"
2020.lrec-1.645,N13-1037,0,0.108812,"generated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013), and that continue to make its 1 https://noisy-text.github.io/ This phrase is used here in a broader sense to indicate all those orthographic, lexical as well as structural choices adopted by a user, often for expressive purposes. 2 treatment a difficult task. The availability of ad hoc training resources remaining an essential factor for the analysis of these texts, in the last decade, numerous resources of this type have been developed. A good proportion of these have been annotated according to the UD scheme (Nivre et al., 2016), a dependency-based format which has achieved great popularit"
2020.lrec-1.645,N10-1060,0,0.0742547,"e popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some"
2020.lrec-1.645,P11-2008,0,0.520555,"Missing"
2020.lrec-1.645,D14-1108,0,0.30934,"e last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UG"
2020.lrec-1.645,lacheret-etal-2014-rhapsodie,0,0.0291573,"inustria’s education’ 5246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions"
2020.lrec-1.645,W16-5403,0,0.0247332,"nn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described"
2020.lrec-1.645,N18-1088,0,0.414107,"ade it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foste"
2020.lrec-1.645,W15-4301,1,0.715109,"l media, treebanks, Universal Dependencies, annotation guidelines, UGC 1. Introduction The immense popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as we"
2020.lrec-1.645,J93-2004,0,0.07288,"the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding difficult by means of the current UD scheme. Elliptical structures and missing elements In constituency-based treebanks of canonical texts such as the Penn Treebank (Marcus et al., 1993) the annotation of empty elements results from the need to keep traces of movement and long-distance dependencies, usually marked with traces and co-indexations at the lexical level in addition to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject"
2020.lrec-1.645,W16-3905,1,0.923501,"resources comprise texts from discussion forums of any kind. Only two treebanks consist of texts from different sub-domains, i.e. blogs, reviews, emails, newsgroups and question answers (EWT), and Wikinews, Wikivoyage, wikiHow, Wikipedia, interviews, Creative Commons fiction and Reddit (GUM), and one is made up of generic data automatically crawled from the web (TDT). Syntactic frameworks As regards the formalism adopted to represent the syntactic structure, dependencies are by far the most used paradigm, especially among the treebanks created from 2014 onward. As also pointed out by Martínez Alonso et al. (2016), a dependency-based annotation lends 3 A more complete table with additional information on the surveyed treebanks can be found here: http://di.unito.i t/webtreebanks. 4 https://developer.twitter.com/en/develop er-terms/agreement-and-policy#c-respect-use rs-control-and-privacy 5241 Phenomenon Lang Attested example Standard form Ergographic phenomena (encoding simplification) GA Leigh aris! Léigh arís! ˙ TR Istanbuldaki agaclar Istanbul’daki a˘gaçlar EN ppl people TR slm selam EN Happy Birthday 2 me Happy Birthday to me TR n zmn ne zaman FR je sé je sais GA gura míle go raibh míle FR tous mes"
2020.lrec-1.645,N13-1039,0,0.155761,"Missing"
2020.lrec-1.645,L18-1106,0,0.0219631,"1. Introduction The immense popularity gained by social media in the last decade has made it an eligible source of data for a large number of research fields and applications, especially for sentiment analysis and opinion mining. In order to successfully process the data available from such sources, linguistic analysis is often helpful, which in turn prompts the use of NLP tools to that end. Despite the ever increasing number of contributions, especially on Part-of-Speech tagging (Gimpel et al., 2011; Owoputi et al., 2013; Lynn et al., 2015; Bosco et al., 2016; Çetino˘glu and Çöltekin, 2016; Proisl, 2018) and parsing (Foster, 2010; Petrov and McDonald, 2012; Kong et al., 2014; Liu et al., 2018), automatic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall"
2020.lrec-1.645,W15-1302,1,0.787281,"accurate cross-lingual studies, all switched tokens should be (consistently) lemmatized if the language is known to annotators. Otherwise the surface form should be used, allowing for more comprehensive lemmatization at a later date. Disfluencies Similar to spoken language, UGC often contains disfluencies such as repetitions, fillers or aborted sentences. This might be surprising, given that UGC does not pose the same pressure on cognitive processing that online spoken language production does. In UGC, however, what seems to be a performance error has in fact a completely different function (Rehbein, 2015). Here, repetitions, self-repair and hesitation markers are often used with humorous intent (Example 12) (12) “Le proposte per l’education di Confindustria” ‘The proposals for the Confinustria’s education’ 5246 Du hast den Apple Wahnsinn... äh, Spirit einfach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity res"
2020.lrec-1.645,L16-1376,0,0.0124556,"on to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject and object of the right hand-side conjunct are annotated with the orphan or remnant13 relations (Schuster et al., 2017). Even though the Enhanced UD scheme proposes to include a ghost-token (Schuster and Manning, 2016) which will be the actual governor of the right hand-side conjuncts , nothing is prescribed regarding treatment of ellipsis without 13 det/nmod Figure 1: Pathological example with two contesting structures from two different readings of the token “-2” surrounded by at least 2 elided elements. (Adapted to UD2.5 from (Martínez Alonso et al., 2016)) Du Hengst! äh, hängst. You stallion! uh, hang2.P s.Sg . “You stallion! uh, you’re stalled.” 5. fixed det This treatment, however, loses information whenever the reparandum does not have the same grammatical function as the repair, which is sometimes t"
2020.lrec-1.645,W17-0416,0,0.0155036,"nce dependencies, usually marked with traces and co-indexations at the lexical level in addition to actual nodes dominating such empty elements. The dependency syntax framework usually does not use such devices as this syntactic phenomena can be represented with crossing branches resulting in non-projective trees. In the specific case of gapping coordination, which can be analyzed as the results of the deletion of a verbal predicate (e.g. John lovesi Mary and Paul (ei ) Virginia), both the subject and object of the right hand-side conjunct are annotated with the orphan or remnant13 relations (Schuster et al., 2017). Even though the Enhanced UD scheme proposes to include a ghost-token (Schuster and Manning, 2016) which will be the actual governor of the right hand-side conjuncts , nothing is prescribed regarding treatment of ellipsis without 13 det/nmod Figure 1: Pathological example with two contesting structures from two different readings of the token “-2” surrounded by at least 2 elided elements. (Adapted to UD2.5 from (Martínez Alonso et al., 2016)) Du Hengst! äh, hängst. You stallion! uh, hang2.P s.Sg . “You stallion! uh, you’re stalled.” 5. fixed det This treatment, however, loses information when"
2020.lrec-1.645,C12-1149,1,0.52101,"ic processing of usergenerated content (UGC) still represents a challenging task, as is shown by the workshop series on noisy user-generated text (W-NUT)1 . UGC is a continuum of text sub-genres that may considerably vary according to the specific conventions and limitations posed by the medium used (blog, discussion forum, online chat, microblog, etc.), its degree of ""canonicalness"" with respect to a more standard language, as well as the linguistic devices2 adopted to convey a message. Overall, however, there are some well-recognized phenomena that characterize UGC as a whole (Foster, 2010; Seddah et al., 2012; Eisenstein, 2013), and that continue to make its 1 https://noisy-text.github.io/ This phrase is used here in a broader sense to indicate all those orthographic, lexical as well as structural choices adopted by a user, often for expressive purposes. 2 treatment a difficult task. The availability of ad hoc training resources remaining an essential factor for the analysis of these texts, in the last decade, numerous resources of this type have been developed. A good proportion of these have been annotated according to the UD scheme (Nivre et al., 2016), a dependency-based format which has achie"
2020.lrec-1.645,D08-1110,0,0.0232123,"d apply to INTER CS (universaldependencies.org, 2019a). In the cases of INTRA CS that are compositional and the grammar of the switched text is known to annotators, the dependency labels should represent the syntactic role each switched token plays. • Markup symbols (e.g. < >) have the UPOS SYM similar to e.g., math operators in the UD guidelines, and they are attached to the root with dep. Code switching While capturing code-switching (CS) in tweets is also a motivation for a tweet-based unit of analysis (Çetino˘glu, 2016; Lynn and Scannell, 2019), it is an emerging topic of interest in NLP (Solorio and Liu, 2008; Solorio et al., 2014; Bhat et al., 2018) and thus should be captured in treebank data. CS (switching between languages) can occur on a number of levels. CS that occurs at the sentence or clause level is referred to as inter-sentential switching (INTER) as shown between English and Irish in Example 9: (9) “Má tá AON Gaeilge agat, úsáid í! It’s Irish Language Week.” If you have ANY Irish, use it! It’s Irish Language Week. INTER switching can also be used to describe bilingual tweets where the switched text represents a translation of the previous segment: “Happy St Patrick’s Day! La Fhéile Pád"
2020.lrec-1.645,W14-3907,0,0.0224716,"iversaldependencies.org, 2019a). In the cases of INTRA CS that are compositional and the grammar of the switched text is known to annotators, the dependency labels should represent the syntactic role each switched token plays. • Markup symbols (e.g. < >) have the UPOS SYM similar to e.g., math operators in the UD guidelines, and they are attached to the root with dep. Code switching While capturing code-switching (CS) in tweets is also a motivation for a tweet-based unit of analysis (Çetino˘glu, 2016; Lynn and Scannell, 2019), it is an emerging topic of interest in NLP (Solorio and Liu, 2008; Solorio et al., 2014; Bhat et al., 2018) and thus should be captured in treebank data. CS (switching between languages) can occur on a number of levels. CS that occurs at the sentence or clause level is referred to as inter-sentential switching (INTER) as shown between English and Irish in Example 9: (9) “Má tá AON Gaeilge agat, úsáid í! It’s Irish Language Week.” If you have ANY Irish, use it! It’s Irish Language Week. INTER switching can also be used to describe bilingual tweets where the switched text represents a translation of the previous segment: “Happy St Patrick’s Day! La Fhéile Pádraig sona daoibh!” Thi"
2020.lrec-1.645,L16-1250,0,0.0251562,"fach noch nicht verstanden ;) ‘You haven’t yet understood the Apple madness... uh spirit ;)’ conj Disfluencies pose a major challenge for syntactic analysis as they often result in an incomplete structure or in a tree where duplicate lexical fillers compete for the same functional slot. An additional problem is caused by the high ambiguity resulting from fragmented texts where the context needed for determining the grammatical function of each argument is missing. For UD, some treebanks with spoken language material exist (Lacheret et al., 2014; Dobrovoljc and Nivre, 2016; Leung et al., 2016; Øvrelid and Hohle, 2016; Caron et al., 2019) and the UD guidelines propose the following analysis for disfluency repairs (universaldependencies.org, 2019b). root root cc nmod case Go to the case couˆ -2 P4 suddently minus two P4 less of det rightto the left Other open questions concern the use of hesitation markers in UGC. We propose to consider them as multi-functional discourse structuring devices and annotate them as discourse markers, attached to the root. Discussion In this last section, we propose a brief discussion on some open questions in which the nature of the phenomena described makes their encoding diff"
2020.lrec-1.645,W19-7723,1,0.892639,"Missing"
2020.lrec-1.645,L16-1102,0,0.0250592,"Missing"
2020.lrec-1.645,I11-1100,1,0.739484,"Missing"
2020.lrec-1.645,D15-1157,1,0.88306,"Missing"
2020.lrec-1.645,L16-1262,0,0.0724564,"Missing"
2020.lrec-1.645,W15-1610,0,0.0503228,"Missing"
2020.lrec-1.645,W19-7811,1,0.885346,"Missing"
2020.lrec-1.645,L18-1279,1,0.892617,"Missing"
2020.lrec-1.645,silveira-etal-2014-gold,0,0.0780259,"Missing"
2020.lrec-1.645,D14-1122,0,0.0259874,"Missing"
2020.lrec-1.645,P17-1159,0,0.0585556,"Missing"
2020.lrec-1.731,W14-0703,0,0.019091,"r a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. Other studies on German have also been focussed on discourse connectives. Stede et al. (1998; 2002) created a lexicon for German discourse markers, augmented with semantic relations (Scheffler and Stede, 2016). Gastel et al. (2011) present annotations for discourse connectives in the TüBa-D/Z (Telljohann et al., 2004), including a small number of causal connectives. A rule-based system for detecting a set of 8 causal German discourse connectives in spoken discourse has been presented by Bögel et al. (2014). Their system predicts whether or not a connective is causal and they also try to predict the causality type, i.e. Reason or Result. Automatic prediction of causal relations in text Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in the BeCause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more"
2020.lrec-1.731,W11-0906,0,0.0146999,"ext Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in the BeCause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more complex multi-word triggers with non-contiguous argument spans. Their new system is a transition-based parser, extending the transition system of the Propbank semantic parser of Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identify the causal triggers in the text, and then it labels the argument spans, i.e. cause, effect and means. The new system not only makes the time-consuming featureengineering of earlier work superfluous, it also outperforms the previous system by a large margin. Another neural approach for causal language detection is presented by Dasgupta et al. (2018)"
2020.lrec-1.731,W18-5035,0,0.0191701,"Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identify the causal triggers in the text, and then it labels the argument spans, i.e. cause, effect and means. The new system not only makes the time-consuming featureengineering of earlier work superfluous, it also outperforms the previous system by a large margin. Another neural approach for causal language detection is presented by Dasgupta et al. (2018) who extract causeeffect relations from text. They combine a bidirectional LSTM with linguistic features and use word and phrase embeddings to model the similarity between different causal arguments of the same type, e.g. the similarity between the two events ’engine failure’ and ’engine breakdown’. 3. Annotation Schema Our annotaiton scheme is adapted from Dunietz et al. (2015), but with an extended set of arguments. While Dunietz et al. (2015) annotate exactly two arguments, namely C AUSE and E FFECT, we also consider the ACTOR and the A FFECTED party of the causal event. The motivation behi"
2020.lrec-1.731,N19-1423,0,0.0161212,"in Table 5.6 train dev test Tokens 86,797 3,899 35,803 Sent. 2,915 151 1,336 Trigger 2,937 151 1,377 causal 1,787 78 873 non-causal 1,150 73 504 5.2. (8) Table 5: German causal annotation dataset split into training/development/test sets. Model: A BERT-based causal sequence tagger We model the task of causal language prediction as a sequence labelling problem, following related work on local semantic role labelling employing syntax-agnostic neural methods (Collobert et al., 2011). Our system is a neural sequence tagger based on Bi-directional Encoder Representations from Transformers (BERT) (Devlin et al., 2019). Recently, transformers have pushed the state of the art for many NLP applications by learning context-sensitive embeddings with different optimisation strategies and then fine-tuning the pre-trained embeddings in a task-specific setup. BERT embeddings are usually trained on large amounts of data, incorporating word embeddings with positional information and self-attention. The representations are trained in two different task setups, i.e. by predicting masked words based on their left and right context and by classifying two sentences based on how probable it is that the second one immediate"
2020.lrec-1.731,W15-1622,0,0.290724,"es of causality such as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the paper is structured as follows. First, we review related work on annotating causal language (section 2.). In section 3., we present the anno"
2020.lrec-1.731,Q17-1009,0,0.161826,"s “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the paper is structured as follows. First, we review related work on annotating causal language (section 2.). In section 3., we present the annotation scheme we use i"
2020.lrec-1.731,W17-0812,0,0.208882,"s “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the paper is structured as follows. First, we review related work on annotating causal language (section 2.). In section 3., we present the annotation scheme we use i"
2020.lrec-1.731,D18-1196,0,0.0165722,"number of causal connectives. A rule-based system for detecting a set of 8 causal German discourse connectives in spoken discourse has been presented by Bögel et al. (2014). Their system predicts whether or not a connective is causal and they also try to predict the causality type, i.e. Reason or Result. Automatic prediction of causal relations in text Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in the BeCause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more complex multi-word triggers with non-contiguous argument spans. Their new system is a transition-based parser, extending the transition system of the Propbank semantic parser of Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identif"
2020.lrec-1.731,P15-1033,0,0.0204527,"ause corpus. Their system uses rich syntactic and lexical information and outperforms a naive baseline. In follow-up work, Dunietz et al. (2018) model the prediction of causal relations as a surface construction labelling task which can be seen as an extension of shallow semantic parsing to more complex multi-word triggers with non-contiguous argument spans. Their new system is a transition-based parser, extending the transition system of the Propbank semantic parser of Choi and Palmer (2011) for the prediction of causal constructions. The transition system is integrated in the LSTM parser of Dyer et al. (2015) which is used to compute the features for the transition system. The system operates in two steps. First, it tries to identify the causal triggers in the text, and then it labels the argument spans, i.e. cause, effect and means. The new system not only makes the time-consuming featureengineering of earlier work superfluous, it also outperforms the previous system by a large margin. Another neural approach for causal language detection is presented by Dasgupta et al. (2018) who extract causeeffect relations from text. They combine a bidirectional LSTM with linguistic features and use word and"
2020.lrec-1.731,W03-1210,0,0.180402,"se of implicit discourse relations where the missing trigger is inserted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003) or on parallel or comparable corpora (Versley, 2010; Hidey and McKeown, 2016; Rehbein and Ruppenhofer, 2017). Girju (2003) has tried to detect instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN  causes bonynessN  . After identifying noun pairs that might express a causal relation, she uses the extracted pairs to search for verbs in a large corpus that might link the nouns and express the causal relation. She then collects these verbs and obtains a list of ambiguous verbs that might express causality. To disambiguate them, Girju extracts sentences from a lar"
2020.lrec-1.731,grivaz-2010-human,0,0.02253,"Dynamic Force Model which provides a framework that tries to distinguish weak and strong causal forces, and captures different types of causality such as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. This problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011), or abstain from reporting inter-annotator agreement at all. A notable exception is the work of Dunietz et al. (2015; Dunietz et al. (2017b) who, inspired by the theory of construction grammar (Goldberg, 1995), aim at building a constructicon for English causal language. When annotating these pre-defined contructions in text, Dunietz et al. (2015) obtain high agreement scores for human annotation. In the paper, we adapt their approach and present a new dataset for German causal language, with annotations in context for verbs, nouns and adpositions. The remainder of the p"
2020.lrec-1.731,P16-1135,0,0.13391,"serted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003) or on parallel or comparable corpora (Versley, 2010; Hidey and McKeown, 2016; Rehbein and Ruppenhofer, 2017). Girju (2003) has tried to detect instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN  causes bonynessN  . After identifying noun pairs that might express a causal relation, she uses the extracted pairs to search for verbs in a large corpus that might link the nouns and express the causal relation. She then collects these verbs and obtains a list of ambiguous verbs that might express causality. To disambiguate them, Girju extracts sentences from a large text corpus and manually annotates them, according to whether or not they"
2020.lrec-1.731,2005.mtsummit-papers.11,0,0.183731,"all disagreements were resolved by the two expert annotators. Table 2 shows inter-annotator agreement (IAA) scores for the different subsets of our data. We reAnnotating German Causal Language This section presents the data and annotation setup as well as inter-annotator agreement scores for the annotation of German causal language. 4.1. Confusion matrix for causal types (verbs) A NNOT 1 A NNOT 2 C ONSEQ . M OTIV. P URPOSE N ONE Data The data we annotate comes from two sources, (i) newspaper text from the TiGer corpus (Dipper et al., 2001) and (ii) political speeches from the Europarl corpus (Koehn, 2005). We chose those two sources as we wanted to include medially written and spoken data, and we selected corpora that allow us to make the annotated data available to the research community. 4.2. A NNOT 1 A NNOT 2 C ONSEQ . M OTIV. P URPOSE N ONE The annotation was done by three annotators, two expert annotators and one advanced student of Computation Linguistics. Each instance included only one causal trigger to be annotated. The triggers were marked and the annotators had been instructed to ignore other potentially causal expressions in the same sentence. For annotation, we used the POS verb n"
2020.lrec-1.731,C16-1007,0,0.0254721,"mporal relations, as a causal relation requires the temporal order of the two events involved, and many studies have looked at both phenomena together. Mirza et al. (2014) have annotated causal relations in the TempEval-3 corpus (UzZaman et al., 2013), with an annotation scheme inspired by TimeML (Pustejovsky et al., 2010). Based on Talmy (1988) and Wolff et al. (2005), they also distinguish whether the first event causes, enables or prevents the second event. Their annotations cover different parts of speech such as verbs, adpositions, adverbials and discourse connectives. In follow-up work (Mirza and Tonelli, 2016) they present a sieve-based system that jointly predicts temporal and causal relations in the TempEval-3 data and the TimeBank corpus (Pustejovsky et al., 2003). Their system makes use of a rich feature set, including morpho-syntactic information, syntactic dependencies, event order, WordNet similarity as well as the annotations that exist in the TimeBank corpus such as T IMEX 3 attribute types or temporal signals. Implicit vs. explicit causality It is well known that the description of causal events is not always expressed by 5968 means of an explicit causal trigger in the text, and humans ha"
2020.lrec-1.731,prasad-etal-2008-penn,0,0.481203,"own that the description of causal events is not always expressed by 5968 means of an explicit causal trigger in the text, and humans have no problem interpreting even implicit causal relations. This is exemplified in the Causality-by-default hypothesis (Sanders, 2005) that has shown that humans, when presented with two consecutive sentences expressing a relation that is ambiguous between a causal and an additive reading, tend to interpret the relation as causal, as in (1). (1) She went to the pub last night. This morning, she was late for work. The annotations in the Penn Discourse treebank (Prasad et al., 2008; Prasad et al., 2018) accomodate this phenomenon by the use of implicit discourse relations where the missing trigger is inserted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on"
2020.lrec-1.731,W18-4710,0,0.0385694,"ion of causal events is not always expressed by 5968 means of an explicit causal trigger in the text, and humans have no problem interpreting even implicit causal relations. This is exemplified in the Causality-by-default hypothesis (Sanders, 2005) that has shown that humans, when presented with two consecutive sentences expressing a relation that is ambiguous between a causal and an additive reading, tend to interpret the relation as causal, as in (1). (1) She went to the pub last night. This morning, she was late for work. The annotations in the Penn Discourse treebank (Prasad et al., 2008; Prasad et al., 2018) accomodate this phenomenon by the use of implicit discourse relations where the missing trigger is inserted by the annotators. Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge ba"
2020.lrec-1.731,pustejovsky-etal-2010-iso,0,0.0445824,"n our new dataset in Section 5. and end with conclusions and suggestions for future work. 2. Related Work In this section, we give an overview over previous work on annotating causal relations. Temporal and causal relations It has often been noted that the concept of causality is closely linked to temporal relations, as a causal relation requires the temporal order of the two events involved, and many studies have looked at both phenomena together. Mirza et al. (2014) have annotated causal relations in the TempEval-3 corpus (UzZaman et al., 2013), with an annotation scheme inspired by TimeML (Pustejovsky et al., 2010). Based on Talmy (1988) and Wolff et al. (2005), they also distinguish whether the first event causes, enables or prevents the second event. Their annotations cover different parts of speech such as verbs, adpositions, adverbials and discourse connectives. In follow-up work (Mirza and Tonelli, 2016) they present a sieve-based system that jointly predicts temporal and causal relations in the TempEval-3 data and the TimeBank corpus (Pustejovsky et al., 2003). Their system makes use of a rich feature set, including morpho-syntactic information, syntactic dependencies, event order, WordNet similar"
2020.lrec-1.731,W17-0813,1,0.806488,"Other work chooses to restrict themselves to annotating causal language, i.e. to those relations that are explicitly expressed in the text (Dunietz et al., 2015; Mirza et al., 2014). We follow the latter approach and only consider causal events that are grounded in lexical expressions in the text, ignoring implicit causal relations such as in (1) above. Bootstrapping causal relations Many studies have tried to bootstrap causal relations, based on external knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003) or on parallel or comparable corpora (Versley, 2010; Hidey and McKeown, 2016; Rehbein and Ruppenhofer, 2017). Girju (2003) has tried to detect instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN  causes bonynessN  . After identifying noun pairs that might express a causal relation, she uses the extracted pairs to search for verbs in a large corpus that might link the nouns and express the causal relation. She then collects these verbs and obtains a list of ambiguous verbs that might express causality. To disambiguate them, Girju extracts sentences from a large text corpus and manually annotates them, according to whether or not they have a causal meaning. The annot"
2020.lrec-1.731,W03-0419,0,0.177092,"dings with positional information and self-attention. The representations are trained in two different task setups, i.e. by predicting masked words based on their left and right context and by classifying two sentences based on how probable it is that the second one immediately succeeds the first one in a text document. As a result, the learned embeddings encode information about the left and right context for each word which makes them superior to most previous representations. Devlin et al. (2019) have proposed a BERT architecture for sequence tagging on the CoNLL-2003 NER shared task data (Sang and Meulder, 2003). The model uses the pretrained BERT embeddings for initialisation and then finetunes the representations by adding a simple classification layer on top of the pre-trained BERT model and jointly finetuning the model parameters on the downstream task. Each BERT model provides its own tokenisation which splits longer words into sub-tokens. The sequence tagger uses only the first sub-token as the input to the classifier, which then predicts a label for each token. 6 The mismatch between the number of sentences and triggers is caused by German particle verbs where the particle can be split from th"
2020.lrec-1.731,L16-1160,0,0.0149944,"it discourse relations, Versley (2010) presents a multilingual approach for data projection. He classifies German explicit discourse relations without German training data, solely based on the English annotations projected to German via word-aligned parallel text. He also presents a bootstrapping approach for a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. Other studies on German have also been focussed on discourse connectives. Stede et al. (1998; 2002) created a lexicon for German discourse markers, augmented with semantic relations (Scheffler and Stede, 2016). Gastel et al. (2011) present annotations for discourse connectives in the TüBa-D/Z (Telljohann et al., 2004), including a small number of causal connectives. A rule-based system for detecting a set of 8 causal German discourse connectives in spoken discourse has been presented by Bögel et al. (2014). Their system predicts whether or not a connective is causal and they also try to predict the causality type, i.e. Reason or Result. Automatic prediction of causal relations in text Dunietz et al. (2017a) present a classical feature-based system for causal tagging, trained on the annotations in t"
2020.lrec-1.731,P98-2202,0,0.0606705,"Missing"
2020.lrec-1.731,telljohann-etal-2004-tuba,0,0.212236,"Missing"
2020.lrec-1.731,S13-2001,0,0.0380343,"ng causal language. We present baseline results for a causal tagger on our new dataset in Section 5. and end with conclusions and suggestions for future work. 2. Related Work In this section, we give an overview over previous work on annotating causal relations. Temporal and causal relations It has often been noted that the concept of causality is closely linked to temporal relations, as a causal relation requires the temporal order of the two events involved, and many studies have looked at both phenomena together. Mirza et al. (2014) have annotated causal relations in the TempEval-3 corpus (UzZaman et al., 2013), with an annotation scheme inspired by TimeML (Pustejovsky et al., 2010). Based on Talmy (1988) and Wolff et al. (2005), they also distinguish whether the first event causes, enables or prevents the second event. Their annotations cover different parts of speech such as verbs, adpositions, adverbials and discourse connectives. In follow-up work (Mirza and Tonelli, 2016) they present a sieve-based system that jointly predicts temporal and causal relations in the TempEval-3 data and the TimeBank corpus (Pustejovsky et al., 2003). Their system makes use of a rich feature set, including morpho-sy"
2020.lrec-1.731,L16-1603,0,0.0315098,"Missing"
2020.lrec-1.731,P13-4001,0,0.0212306,"Support C ONSEQUENCE diesCause bedenkliche Folgen. 3 5970 by For the polar distinction, they report perfect agreement. For more details on the annotation scheme, we refer the reader to Dunietz et al. (2015; 2018). Figure 1: Causal annotations for the verb bereiten, visualised in the annotation tool Webanno. While the first version of the BeCause corpus included the label I NFERENCE for epistemic uses of causality, this label was given up in version 2.0 of the corpus (Dunietz et al. (2017a). We decided to follow their decision and only consider three types of causality. 4. online tool WebAnno (Yimam and Gurevych, 2013) (Figure 1). Each instance in the dataset was annotated by at least two annotators, and after the annotation process was completed, all disagreements were resolved by the two expert annotators. Table 2 shows inter-annotator agreement (IAA) scores for the different subsets of our data. We reAnnotating German Causal Language This section presents the data and annotation setup as well as inter-annotator agreement scores for the annotation of German causal language. 4.1. Confusion matrix for causal types (verbs) A NNOT 1 A NNOT 2 C ONSEQ . M OTIV. P URPOSE N ONE Data The data we annotate comes fro"
2020.lrec-1.878,C18-1139,0,0.256533,"utterances can predict the syntactic boundaries annotated in the SegCor corpus. They showed that while there is a correlation between gap length and surface syntax, gap length on its own is not sufficient for a reliable prediction of SLU boundaries. Our work builds on previous work on automatic boundary detection in German spoken language transcripts (Ruppenhofer and Rehbein, 2019) and tries to further improve the accuracy for SLU boundary detection. Ruppenhofer and Rehbein (2019) modelled the problem as a sequence tagging task and showed that neural models with contextual string embeddings (Akbik et al., 2018), based on the Flair library of Akbik et al. (2019), outperform a classical feature-based CRF classifier. This paper presents new experiments where we (i) test different neural architectures and task setups for SLU boundary detection, and (ii) investigate the potential benefits of additional training data from a different source of spoken language. This paper proceeds as follows. We discuss related work in Section 2. and present our dataset in Section 3. Our experiments and their results are described in sections 4. and 5. While we show that training data expansion for this task is not straigh"
2020.lrec-1.878,N19-4010,0,0.13254,"notated in the SegCor corpus. They showed that while there is a correlation between gap length and surface syntax, gap length on its own is not sufficient for a reliable prediction of SLU boundaries. Our work builds on previous work on automatic boundary detection in German spoken language transcripts (Ruppenhofer and Rehbein, 2019) and tries to further improve the accuracy for SLU boundary detection. Ruppenhofer and Rehbein (2019) modelled the problem as a sequence tagging task and showed that neural models with contextual string embeddings (Akbik et al., 2018), based on the Flair library of Akbik et al. (2019), outperform a classical feature-based CRF classifier. This paper presents new experiments where we (i) test different neural architectures and task setups for SLU boundary detection, and (ii) investigate the potential benefits of additional training data from a different source of spoken language. This paper proceeds as follows. We discuss related work in Section 2. and present our dataset in Section 3. Our experiments and their results are described in sections 4. and 5. While we show that training data expansion for this task is not straightforward even with data from the same domain (Secti"
2020.lrec-1.878,N19-1423,0,0.483044,"ion on speaker turns considerably improves results. We experimented with a feature-rich classification setup based on Conditional Random Fields (CRF) that allowed us to easily include additional information, such as PoS tags or lemmas. However, we also showed that the CRF classifier can be outperformed by a simpler neural model that incorporates contextualised string embeddings (Akbik et al., 2018; Akbik et al., 2019). Given the success of the neural model, we would like to test whether further improvements can be obtained with transfer learning based on BERT’s contextualised word embeddings (Devlin et al., 2019) (Section 5.). 3. Data This section presents our gold standard for the segmentation of German oral corpora, created in the SegCor (“Segmentation of Oral Corpora”) project,1 as well as the additional spoken language data we use in our training data expansion experiments. 3.1. SegCor The SegCor data has some features that distinguish it from most previous work. Our data represents conversational speech with two or more speakers that was recorded in nonlaboratory settings. Since tools based on the automatic processing of the audio signal do not work all that well on our data, we instead work with"
2020.lrec-1.878,L16-1147,0,0.0174895,"cture are shown in Table 4. When comparing our results to our previous work on the same dataset (Table 4, lower part), we see that the BERT model outperforms all previous models (CRF, Flair-FastText, Flair-FastText+Flair embeddings) except for the Flair model that was trained with our own customised embeddings (Flair-FastText+Flair+custom), in addition to the FastText and Flair embeddings provided by the Flair library (Akbik et al., 2019). These custom embeddings are character-based Flair embeddings that have been trained on ca. 11 million ‘sentences’ extracted from the open subtitles corpus (Lison and Tiedemann, 2016) and an in-house twitter dataset. All sentences with a length &gt; 60 characters have been removed, as have sentences that contained more than one comma and one period, question mark or exclamation mark. The punc8 7107 Taken from Devlin et al. (2019) and adapted to our setup. tuation marks were removed before training and the data was lowercased. The motivation for this setup was to train embeddings on text that is more similar to our spoken language transcripts. As shown above, our BERT sequence tagging model achieves results in the same range as the model stacked with the original FastText, Fla"
2020.lrec-1.878,P05-1056,0,0.0818523,"Missing"
2020.lrec-1.878,C12-2096,0,0.077911,"Missing"
2020.lrec-1.878,rehbein-etal-2014-kiezdeutsch,1,0.814425,"e-like units is not always one to one. Common deviations are as follows. First, a contribution may correspond to several SLUs as illustrated by (1). (1) 1 contribution : n SLUs a. &lt; c &gt;h ich weiß net ich glaub eher nich h h&lt; /c &gt; b. &lt; SLU &gt;h ich weiß net&lt; /SLU &gt; &lt; SLU &gt; ich glaub eher nich h h&lt; /SLU &gt; c. ‘I don’t know. I rather think not.’ Second, several contributions may jointly correspond to one SLU. (2) 3.2. KiDKo In addition to the rather small SegCor dataset we also have access to a much larger corpus of informal spoken youth language, the KiezDeutsch-Korpus (KiDKo) (Wiese et al., 2012; Rehbein et al., 2014). KiDKo contains spontaneous peergroup dialogues of adolescents from multiethnic Berlin-Kreuzberg (around 266,000 tokens) and a supplementary corpus with adolescent speakers from monoethnic Berlin-Hellersdorf (around 111,000 tokens, excluding punctuation). On the normalisation layer where punctuation is included, the token counts add up to around 359,000 tokens (main corpus) and 149,000 tokens (supplementary corpus). On the normalisation layer, the data includes punctuation and is segmented into sentence-like units. On top of the normalisation, the corpus comprises additional annotation layers"
2020.lrec-1.878,W11-3808,1,0.781091,"m as we have done before, but this time replacing the character-based contextual string embeddings of Akbik et al. (2018; Akbik et al. (2019) by the BERT embeddings. 5.1. SLU detection as sequence tagging In our experiments, we use the HuggingFace transformers library (Wolf et al., 2019) that provides pre-trained transformer models for different languages and tasks. As our input transcripts are lower-cased, we use the pre-trained German uncased BERT model (bert-base-german-dbmdzuncased).5 3 A similar method was used successfully in Søgaard (2011) for cross-lingual unsupervised parsing, and in Rehbein (2011) for self-training of monolingual parsers. 4 These transcripts, however, had only been annotated by one annotator so it was not clear whether the results might reflect a lower quality in the annotations. 5 The model has been trained by the MDZ Digital Library team (dbmdz) at the Bavarian State Library on 2,350,234,427 tokens of raw text, including Wikipedia, the EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. For details see https://github.com/dbmdz/german-bert. 7106 O G B E was what was bei in bei muttersprachlern untersucht native speakers investivated mutter ##spr"
2020.lrec-1.878,W15-5938,0,0.0693302,"Missing"
2020.lrec-1.878,schmidt-2014-research,1,0.804737,"“Segmentation of Oral Corpora”) project,1 as well as the additional spoken language data we use in our training data expansion experiments. 3.1. SegCor The SegCor data has some features that distinguish it from most previous work. Our data represents conversational speech with two or more speakers that was recorded in nonlaboratory settings. Since tools based on the automatic processing of the audio signal do not work all that well on our data, we instead work with the transcripts only. Our dataset consists of 33 documents with more than 54,000 lexical tokens originating from the FOLK corpus (Schmidt, 2014) that were divided into sentence-like units by the SegCor project. This data set was doubly annotated and disagreements were adjudicated (Westpfahl and Gorisch, 2018). Note that to avoid confusion, we reserve the term segment and related forms for the division of speech into chunks by the transcribers that was guided by silences in the speech signal. For the division of the material into sentence-like units we will use the term “SLU boundary detection”. 1 https://www1.ids-mannheim.de/prag/ muendlichekorpora/segcor.html 7103 The raw FOLK transcripts, which we take as our input and which lack SL"
2020.lrec-1.878,P11-2120,0,0.0292205,"ur task is to model SLU detection as a sequence tagging problem as we have done before, but this time replacing the character-based contextual string embeddings of Akbik et al. (2018; Akbik et al. (2019) by the BERT embeddings. 5.1. SLU detection as sequence tagging In our experiments, we use the HuggingFace transformers library (Wolf et al., 2019) that provides pre-trained transformer models for different languages and tasks. As our input transcripts are lower-cased, we use the pre-trained German uncased BERT model (bert-base-german-dbmdzuncased).5 3 A similar method was used successfully in Søgaard (2011) for cross-lingual unsupervised parsing, and in Rehbein (2011) for self-training of monolingual parsers. 4 These transcripts, however, had only been annotated by one annotator so it was not clear whether the results might reflect a lower quality in the annotations. 5 The model has been trained by the MDZ Digital Library team (dbmdz) at the Bavarian State Library on 2,350,234,427 tokens of raw text, including Wikipedia, the EU Bookshop corpus, Open Subtitles, CommonCrawl, ParaCrawl and News Crawl. For details see https://github.com/dbmdz/german-bert. 7106 O G B E was what was bei in bei mutters"
2020.lrec-1.878,A00-1012,0,0.475982,"Missing"
2020.lrec-1.878,W18-4913,0,0.0939406,"e application of Natural Language Processing tools (e.g. PoStaggers, syntactic parsers) to transcripts of spoken language. While various proposals have been made for how to divide spoken language in corpora into smaller units, typically these divisions were not guided by syntactic considerations. Instead, division into inter-pausal units is common (e.g. Hamaker et al. (1998) for the Switchboard corpus (John J. Godfrey, Edward Holliman, 1993)). For German, the SegCor project presented a proposal and guidelines for dividing transcribed speech into sentencelike units based on Topological Fields (Westpfahl and Gorisch, 2018; Westpfahl et al., 2019). The Topological Fields Model (Drach, 1937; H¨ohle, 1986) is a descriptive grammar formalism that captures regularities in German word order by dividing sentences in different verbal and non-verbal fields and describing their position with regard to the main verb. In a corpus-based study, Schmidt and Westpfahl (2018) then investigated how well the length of gaps between utterances can predict the syntactic boundaries annotated in the SegCor corpus. They showed that while there is a correlation between gap length and surface syntax, gap length on its own is not suffici"
2020.lrec-1.878,L16-1237,1,0.850272,"n (2019). Once we have extracted our input sequences from SegCor, we train the sequence tagger provided by the HuggingFace library with a batch size of 32 and a learning rate of 5e5 for three iterations on our data.7 The sequence tagging model was originally intended for NER and similar tasks. In our setup, however, we input an unsegmented sequence from our spoken transcripts and let the model predict for each token whether or not this token is followed by an SLU 6 The disambiguation between discourse markers and other adverbial forms was done based on automatically predicted finegrained PoS (Westpfahl and Schmidt, 2016). 7 We also experimented with other learning rates which gave inferior results on the development set. Macro Acc Macro F1 ID F1 B F1 O Embeddings/ Features 95.3 95.1 95.2 95.0 95.1 90.3 89.6 89.7 89.5 89.6 83.4 82.0 82.2 81.8 82.0 97.3 97.2 97.2 97.1 97.1 avg. 95.1 89.7 82.3 97.2 CRF BERT sequence tagging 1 2 3 4 5 94.7 92.3 95.1 94.8 95.4 88.3 83.4 89.6 89.3 90.2 79.7 71.3 82.0 81.6 83.1 96.9 95.5 97.1 97.0 97.4 Flair To accomodate BERT’s length restrictions, we extract sequences with a maximum length of 80 tokens as follows. We iterate over each token in the input data, looking for probable"
2020.udw-1.16,W19-8006,0,0.0202011,"Missing"
2020.udw-1.16,W17-0404,0,0.0650363,"Missing"
2020.udw-1.16,de-marneffe-etal-2014-universal,0,0.0772979,"Missing"
2020.udw-1.16,W10-0803,0,0.0406196,"ly in the basic annotation since these cases would involve an orphan relationship. Further, the apparent conjunction, where present, would need to be changed both in terms of POS and syntactic dependency. We might, for instance, treat und somewhat arbitrarily as an ADV related by advmod to the head of the right conjunct. 2.3 Presentational relative clause construction (PRC) The presentational relative construction (PRC) involves the combination of a semantically weak main clause and a relative clause which, rather than the main clause, contains the assertion of the utterance (Lambrecht, 1988; Duffield et al., 2010). The PRC is, therefore, normally more aptly paraphrased by a single sentence (29a) than by a sequence of sentences (29b). (29) You have some folks who deny she LOST when she suspended her campaign. a. Some folks deny she LOST when she suspended her campaign. b. #You have some folks. They deny she LOST when she suspended her campaign. 144 Figure 2: NFPK construction root root punct conj nsubj cc advmod punct Ich und lügen ? I and lie ? Ich und lügen ? I and lie ? nsubj (a) Subject and predicate, using enhanced UD (b) Subject and predicate, basic UD only Formally, PRCs mostly feature subject ga"
2020.udw-1.16,lacheret-etal-2014-rhapsodie,0,0.0299834,"s. Contrast the effect of the lie-test in response to a cleft (38) with that in response to the PRC (36) (38) A: It’s colleagues of mine who have done this. B: That’s a lie! 2 Lambrecht (1988) assumes PRCs are restricted to involve subject gaps in the relative clause but Duffield et al. (2010) report instances with object gaps in the relative clause. 145 a. It’s not them (but Pat who did it). b. #Nobody has done this. The Italian PoSTWITA treebank (Sanguinetti et al., 2018) contains an instance of what we call PRC that is simply annotated as a relative clause.3 The treebank for spoken French (Lacheret et al., 2014) handles PRC instances the same way. But given their special properties, presentational relative clauses should be treated differently from restrictive and appositive relative clauses. A simple option involving only basic UD would be to only subtype the acl-relation further and introduce acl:presrel. Figure 3: Presentational Relative Construction root advcl obj nsubj det acl:presrel nsubj ccomp mark nsubj nsubj obj det You have some folks who deny she LOST when she suspended her campaign . The annotation strategy for the English PRC can also be carried over to Italian, French and German. Note"
2020.udw-1.16,L18-1279,0,0.0238837,". a. He doesn’t want a car. b. # He can’t build it. Presentational relative clauses are also not the same as the relative clauses in cleft-sentences. Contrast the effect of the lie-test in response to a cleft (38) with that in response to the PRC (36) (38) A: It’s colleagues of mine who have done this. B: That’s a lie! 2 Lambrecht (1988) assumes PRCs are restricted to involve subject gaps in the relative clause but Duffield et al. (2010) report instances with object gaps in the relative clause. 145 a. It’s not them (but Pat who did it). b. #Nobody has done this. The Italian PoSTWITA treebank (Sanguinetti et al., 2018) contains an instance of what we call PRC that is simply annotated as a relative clause.3 The treebank for spoken French (Lacheret et al., 2014) handles PRC instances the same way. But given their special properties, presentational relative clauses should be treated differently from restrictive and appositive relative clauses. A simple option involving only basic UD would be to only subtype the acl-relation further and introduce acl:presrel. Figure 3: Presentational Relative Construction root advcl obj nsubj det acl:presrel nsubj ccomp mark nsubj nsubj obj det You have some folks who deny she"
2020.udw-1.16,L16-1376,0,0.0229223,"to support multilingual NLP applications on the one hand and to facilitate the linguistic study of similarities and differences from a typological perspective. While the UD framework also covers the annotation of parts of speech and of morphological features, the annotation of syntactic dependencies is at its core. UD currently uses 37 labels for broadly attested grammatical relations. In addition to these basic UD dependencies, UD allows for an enhanced representation that “aims to make implicit relations between content words more explicit by adding relations and augmenting relation names” (Schuster and Manning, 2016). The most prominent examples of added relations in enhanced representations are links that help propagate relations over conjunctions. While the basic dependency structure is assumed to form a (possibly non-projective) tree, enhanced UD representations often no longer are trees. The subtyping of relations serves to capture more fine-grained language-specific constructions (de Marneffe et al., 2014). For instance, in English, subjects of passive clauses bear the relation nsubj:pass. In this paper, we discuss several constructions that are found in spoken language and data from social media, wh"
2020.udw-1.16,silveira-etal-2014-gold,0,0.0429109,"Missing"
2020.udw-1.16,W16-1709,0,0.0123888,"nstructions with similar challenges will be encountered. To allow for consistent and expressive analyses, we think the UD community would benefit from discussing which mechanisms to use for which kinds of constructions. We have explored the use of enhanced UD annotations, keeping the relation inventory the same, and as an alternative introducing new dependency relations and relation subtypes, which may lead to sparsely attested relation types. One option we have not explored but which the GUM corpus uses is a kind of constructional annotation (specifically, of sentence types) in the metadata (Zeldes and Simonson, 2016). This would, however, not localize which words are part of the construction. Other ways to track constructions may be conceivable. Whatever the annotation mechanisms used, we think that adding linguistic analyses for such constructions to the UD guidelines might help to improve annotation consistency across languages, and thus the quality of the treebanks. Existing UD treebanks already feature some well-represented constructions that are treated inconsistently between treebanks and/or languages along the lines we discussed. For instance, the two clauses of the paratactic correlative construct"
2021.emnlp-main.615,P18-1017,0,0.0137701,"nd parties) and coalitions into the model. As for NER, we use a sequence tagging setup and create our training data as follows. Based on a number of manually created dictionaries, we augment the German input data with BIO labels on the token level (Figure 1). We then use this data in a MTL setting, in combination with C-type prediction for Ireland, Germany and Austria. Valence classification. To improve results for polarity classification, we experiment with a highly related task from the area of sentiment analysis (Zhang et al., 2018). Specifically, we use the multilingual NRC VAD lexicon of Mohammad (2018) Named Entity Recognition. Directing the mod- which provides valence scores for over 20,000 pivot els’ attention towards named entities such as per- terms, created via Best-Worst Scaling. We extract sons, organisations etc. might provide useful infor- the scores for the English words and their German mation for C-type prediction. We use the English translations and, for each sentence in our German Ontonotes dataset (Weischedel et al., 2013) and the and Irish input data, compute a valence score by German Twitter news headlines data (Ruppenhofer simply adding up the scores for each lemma in et a"
2021.emnlp-main.615,2020.emnlp-demos.7,0,0.0337264,"Missing"
2021.emnlp-main.615,2020.lrec-1.566,1,0.780076,"Missing"
2021.emnlp-main.615,2020.coling-main.426,0,0.0228252,"the capabilities of computational methods for text analysis have drastically evolved and the usage of Natural Language Processing (NLP) methods have increasingly expanded in scope, enabling a wide range of applications for the automatised analysis of political texts (Glavaš et al., 2019). Much work in NLP in the past has focused on making sense of how legislative processes are codified in text – e.g., by analysing debate moIn this paper, we present the task of coalition sigtions (Abercrombie et al., 2019; Abercrombie and nal detection as a new, challenging NLU task. We Batista-Navarro, 2020; Sawhney et al., 2020, inter propose to approach this task as a machine learning alia). However, when analysing electoral dynamics problem and decompose it into two subtasks, (i) the it seems necessary to complement the application identification of text sequences that contain a signal of NLP methods on domain-specific texts such as and (ii) the prediction of the polarity of each potenelectoral manifestos with a broader analysis of sig- tial signal, that is, whether the speaker promotes or nals from news texts (Blokker et al., 2020). opposes the signalled coalition – cf. examples 1.1 Specifically in the case of go"
2021.emnlp-main.615,2020.emnlp-demos.6,0,0.024649,"Missing"
2021.konvens-1.13,N13-1132,0,0.0292588,"ers or information from external knowledge bases. While these labelling functions are expected to have low coverage and might also introduce a certain amount of noise, Snorkel addresses this problem by learning an unsupervised generative model over the output of the labelling functions, based on the (dis)agreements between the predicted labels. This approach is similar in spirit to previous work on 7 See https://spacy.io/api/ dependencymatcher. To generate the trees, we use the German de_core_news_sm model also provided by spaCy. quality estimation for annotations obtained from crowdsourcing (Hovy et al., 2013). The output of Snorkel is a set of probabilistic labels that can be used as input to any supervised ML classifier. Table 5 shows the number of patterns used for each class and the number of hits, i.e., instances extracted by each pattern from the unlabelled training data. Please note that the number of patterns is not very informative on its own, as patterns can make use of regular expressions, lemma lists and syntactic patterns over dependency trees, thus allowing us to extract a larger variety of diverse training examples than could be obtained based on simple string matches. As an example,"
2021.konvens-1.13,C18-2002,0,0.0263387,"Missing"
C10-1107,D09-1031,1,0.849439,"cisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where InterAnnotator Agreement (IAA) is low and thus the consistency of th"
C10-1107,burchardt-etal-2006-salsa,0,0.0832951,"Missing"
C10-1107,P07-1007,0,0.0721121,"instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be ap"
C10-1107,N06-1016,0,0.323805,"onment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign sem"
C10-1107,N04-1012,0,0.202379,"ier is re-trained on the new data set. The newly trained classifier now picks the next instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the"
C10-1107,rehbein-ruppenhofer-2010-theres,1,0.836151,"f sentences selected by the classifier. For our human annotators, however, due to different annotation decisions the resulting set of sentences is expected to differ. Sampling method Uncertainty sampling is a standard sampling method for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. During early stages of the learning process when the classifier is trained on a very small seed data set, it is not beneficial to add the instances with the lowest classifier confidence. Instead, we use a dynamic version of uncertainty sampling (Rehbein and Ruppenhofer, 2010), based on the confidence of a maximum entropy classifier2 , taking into account how much the classifier has learned so far. In each iteration one new instance is selected from the pool and presented to the oracle. After annotation the classifier is retrained on the new data set. The modified uncertainty sampling results in a more robust classifier performance during early stages of the learning process. 952 2 http://maxent.sourceforge.net A1 A2 A3 A4 A5 A6 ø sl Anlass R U 8.6 9.6 4.4 5.7 9.9 9.2 5.8 4.9 3.0 3.5 5.4 6.3 6.2 6.5 25.8 27.8 Motiv R U 5.9 6.6 4.8 5.9 6.8 6.7 3.6 3.6 3.0 2.6 5.3 4."
C10-1107,P08-1098,0,0.0208374,"ions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign semantic frames to predicates, following Erk (200"
C10-1107,W07-1516,0,0.17367,"signs the correct label. The newly-annotated instances are added to the seed data and the classifier is re-trained on the new data set. The newly trained classifier now picks the next instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL see"
C10-1107,D07-1051,0,0.0387629,"grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign semantic frames to predic"
C10-1107,D07-1082,0,0.198912,"its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grai"
C10-1107,C08-1143,0,0.124091,"ge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation sche"
C10-1107,J04-3001,0,0.391882,"istic environment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for lea"
C10-1107,C08-1059,0,0.314897,"Missing"
C10-1107,P08-1000,0,\N,Missing
C18-1010,W15-2210,0,0.06211,"Missing"
C18-1010,W11-2929,0,0.456228,"Missing"
C18-1010,N13-1132,0,0.21226,"automatically predicted parse trees. We evaluate our approach in two different scenarios, i) in an active learning (AL) setup where we try to detect and manually correct errors in existing treebanks, and ii) in a domain adaptation setting where we automatically improve parsing performance without manual correction. We make an implementation of our approach publically available.1 2 Model Our error detection model is an adaptation and substantial extension of M ACE -AL (Rehbein and Ruppenhofer, 2017) which combines a generative, unsupervised method for estimating annotator reliability (M ACE) (Hovy et al., 2013) with active learning for the task of error detection in automatically annotated data. M ACE -AL can be applied to any classification task and has been tested in two applications, POS tagging and Named Entity Recognition (NER) (Rehbein and Ruppenhofer, 2017). The model is, however, not applicable to tasks with structured output such as trees or graphs. In contrast to POS tagging or NER where we try to detect annotation errors in the predicted labels for individual tokens, when looking for errors in parse trees we have to deal with directed, labelled relations between nodes, and changing the re"
C18-1010,Q16-1023,0,0.0944823,"iational inference model (M ACE -AL-T REE) or on the output from the Chu-Liu-Edmonds algorithm (M ACE -AL-T REE -C LE). A key advantage of our model is that, while making use of the feedback from active learning, we do not have to retrain the parsers after each iteration, which would be infeasible due to the time requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive bias"
C18-1010,D12-1096,0,0.0276049,"This not only includes PPs but also adverbal and adjectival modification. The next frequent error type are incorrectly attached punctuations. This, however, is not an error type we are concerned with and it might make sense to exclude punctuation from the error correction.13 More interesting attachment errors include coordination (CJ, CD), parenthesis (PAR), apposition (APP) and again the core arguments (OA, AG, DA, SB, PD). This shows that our model is not biased toward a small set of specific error types but is able to detect frequent parsing errors that are well-known from the literature (Kummerfeld et al., 2012). 3.5 Domain adaptation with self-training In the final set of experiments we want to test whether we can use our method to automatically correct parser output for self-training, to improve parsing accuracy for out-of-domain data. As before, we use the English web treebank, split into different genres as described above. Again, we pretend that no annotated training data for the different genres are available. This time, however, we make use of the supplementary raw text data for the five web genres (Table 1) that were provided for the SANCL shared task (Petrov and McDonald, 2012). The data is"
C18-1010,P14-1130,0,0.130379,"output from the Chu-Liu-Edmonds algorithm (M ACE -AL-T REE -C LE). A key advantage of our model is that, while making use of the feedback from active learning, we do not have to retrain the parsers after each iteration, which would be infeasible due to the time requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can com"
C18-1010,D07-1013,0,0.0621733,"as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can complement each other (McDonald and Nivre, 2007). All parsers use default settings as reported in the literature. For the BISTGraph parser, we selected the model with the highest LAS on the development set after 10 training iterations.5 We run the Malt parser with the settings arc-eager, liblinear without any further optimisation. For all experiments, we remove the language-specific extended labels and only train the parsers on the universal dependency relations. 3 Other update strategies such as always updating the predictions of the annotator with the lowest performance, or updating the predictions of all annotators did not yield the same"
C18-1010,nilsson-nivre-2008-malteval,0,0.0676691,"Missing"
C18-1010,nivre-etal-2006-maltparser,0,0.0845134,"rning, we do not have to retrain the parsers after each iteration, which would be infeasible due to the time requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can complement each other (McDonald and Nivre, 2007). All parsers use default settings as reported in the literature. For the BISTGraph parser, we selected the mod"
C18-1010,D08-1093,0,0.247649,"Missing"
C18-1010,P17-1107,1,0.891838,"nd when dealing with low-resource languages. We present such a method, aimed at the detection of parser errors in manually and automatically predicted parse trees. We evaluate our approach in two different scenarios, i) in an active learning (AL) setup where we try to detect and manually correct errors in existing treebanks, and ii) in a domain adaptation setting where we automatically improve parsing performance without manual correction. We make an implementation of our approach publically available.1 2 Model Our error detection model is an adaptation and substantial extension of M ACE -AL (Rehbein and Ruppenhofer, 2017) which combines a generative, unsupervised method for estimating annotator reliability (M ACE) (Hovy et al., 2013) with active learning for the task of error detection in automatically annotated data. M ACE -AL can be applied to any classification task and has been tested in two applications, POS tagging and Named Entity Recognition (NER) (Rehbein and Ruppenhofer, 2017). The model is, however, not applicable to tasks with structured output such as trees or graphs. In contrast to POS tagging or NER where we try to detect annotation errors in the predicted labels for individual tokens, when look"
C18-1010,P07-1078,0,0.0345818,"etting, however, clearly outperforms our baselines and shows a significant increase in parsing accuracy. Nonetheless, when comparing the results for M ACE -AL-T REE with self-training to the results without self-training we see that our self-training approach fails to beat the results obtained on the original data on three web genres. We only observe improved results on the emails and on the reviews. These two genres are also the ones that have the largest amount of unknown words which might explain why self-training is more promising for emails and reviews than for the other three genres, as Reichart and Rappoport (2007) have shown that the number of unknown words can be a good indicator for the potential benefit from self-training. 4 Related work Most studies on error detection in treebanks have focussed on finding errors in manual annotations (Dickinson and Meurers, 2003; Ule and Simov, 2004; van Noord, 2004; Dickinson and Meurers, 2005; Agrawal et al., 2013). Dickinson and Meurers (2003; 2005) proposed the use of variation n-grams to detect inconsistencies in manually annotated constituency treebanks and Boyd et al. (2008) extend this line of work to dependency trees. This approach, however, only works for"
C18-1010,W14-6111,0,0.045865,"Missing"
C18-1010,K17-3001,0,0.241419,"ference on Computational Linguistics, pages 107–118 Santa Fe, New Mexico, USA, August 20-26, 2018. obl nsubj dobj nmod case She eats spaghetti with chopsticks nsubj dobj obl case She eats spaghetti with chopsticks nsubj dobj case She eats spaghetti with chopsticks Figure 1: Correct tree (left), tree with label error (center) and tree with attachment error (right). A high-precision method for error detection in automatically predicted trees, however, would be of tremendous use for many treebanking projects. Below, we first describe the approach of Hovy et al. (2013) and Rehbein and Ruppenhofer (2017) for unstructured data (§2.1) and then our extension of the model for the purpose of error detection in tree structures (§2.2). 2.1 MACE-AL: Error detection in unstructured data M ACE -AL combines variational inference (VI) with active learning (AL) to model the reliability of automatically predicted annotations. As input, it takes the output of a committee of classifiers (e.g. the output of N POS taggers) and uses Bayesian inference to learn which taggers’ predictions are more trustworthy than others. The method is unsupervised, meaning that the distribution of the true labels Y = {y1 , y2 ,"
C18-1010,N10-1091,0,0.0374489,"he settings arc-eager, liblinear without any further optimisation. For all experiments, we remove the language-specific extended labels and only train the parsers on the universal dependency relations. 3 Other update strategies such as always updating the predictions of the annotator with the lowest performance, or updating the predictions of all annotators did not yield the same increase in results. 4 It has been shown that the most important success criterion for ensemble parsing is the diversity of the base parsers rather than model complexity or a high accuracy for the individual parsers (Surdeanu and Manning, 2010). 5 We started with 30 iterations but noticed no further increase in LAS after the first 10 iterations. 110 genre answers email newsgroups rewiews weblogs # trees 3,488 4,900 2,391 3,813 2,030 dev 1,000 1,000 1,000 1,000 1,000 test 2,488 3,900 1,391 2,813 1,030 train 13,134 11,722 14,231 12,809 14,592 # unlabelled sent. 27,274 1,194,173 1,000,000 1,965,350 524,834 Table 1: Distribution in the UD-En web treebank (training set does not include data from the respective genre but contains all the remaining treebank sentences excluding the ones for this particular genre). 3.1 Error detection with a"
C18-1010,ule-simov-2004-unexpected,0,0.0924351,"Missing"
C18-1010,P04-1057,0,0.118926,"Missing"
C18-1010,P11-2060,0,0.0628969,"Missing"
C18-1010,E17-1063,0,0.103258,"requirements. Instead, we only train the parsers once, for offline preprocessing, before active learning starts. 3 Experiments In our experiments, we use five different dependency parsers for preprocessing to predict the parse trees. We employ the neural BiLSTM parser (BISTGraph ) of Kiperwasser and Goldberg (2016) and the RBG parser (Lei et al., 2014), both graph-based, as well as the transition-based IMSTrans parser (Bj¨orkelund and Nivre, 2015) and the slightly outdated Malt parser (Nivre et al., 2006).4 As the fifth model, we use our in-house implementation of the head-selection parser of Zhang et al. (2017) which also employs bidirectional LSTMs (Hochreiter and Schmidhuber, 1997) for learning the feature representations. We chose the parsers so that they cover a range of different parsing algorithms and approaches, as it has been shown before that the parsers’ predictions, due to the inductive biases of the algorithms involved, can complement each other (McDonald and Nivre, 2007). All parsers use default settings as reported in the literature. For the BISTGraph parser, we selected the model with the highest LAS on the development set after 10 training iterations.5 We run the Malt parser with the"
D07-1066,H91-1060,0,0.715134,"Missing"
D07-1066,brants-hansen-2002-developments,0,0.0464714,"Missing"
D07-1066,P03-1013,0,0.154719,"metric as well as a dependency-based evaluation, and present novel approaches measuring the effect of controlled error insertion on treebank trees and parser output. We also provide extensive past-parsing crosstreebank conversion. The results of the experiments show that, contrary to K¨ubler et al. (2006), the question whether or not German is harder to parse than English remains undecided. 1 Introduction A long-standing and unresolved issue in the parsing literature is whether parsing less-configurational languages is harder than e.g. parsing English. German is a case in point. Results from Dubey and Keller (2003) suggest that state-of-the-art parsing scores for German are generally lower than those obtained for English, while recent results from K¨ubler et al. (2006) raise the possibility that this might Despite being the standard metric for measuring PCFG parser performance, PARSEVAL has been criticised for not representing ’real’ parser quality (Carroll et al., 1998; Brisco et al., 2002; Sampson and Babarbczy, 2003). PARSEVAL checks label and wordspan identity in parser output compared to the original treebank trees. It neither weights results, differentiating between linguistically more or less sev"
D07-1066,W06-1614,0,0.531017,"Missing"
D07-1066,P06-3004,0,0.13696,"TIGER Test Set Table 6 shows that all error types inserted into Sentence 9 in our test set result in the same evaluation score for the PARSEVAL metric, while the LA metric provides a more discriminative treatment of PP attachment errors, label errors and span errors for the same sentence (Table 6). However, the differences in the LA results are only indirectly caused by the different error types. They actually reflect the number of terminal nodes affected by the error insertion. For Label I and II the LA results vary considerably, because the substitution of the PP for 634 K¨ubler (2005) and Maier (2006) assess the impact of the different treebank annotation schemes on PCFG parsing by conducting a number of modifications converting the T¨uBa-D/Z into a format more similar to the NEGRA (and hence TIGER) treebank. After each modification they extract a PCFG from the modified treebank and measure the effect of the changes on parsing results. They show that with each modification transforming the T¨uBa-D/Z into a more NEGRA-like format the parsing results also become more similar to the results of the NEGRA treebank, i.e. the results get worse. Maier takes this as evidence that the T¨uBa-D/Z is m"
D07-1066,H94-1020,0,0.0325769,"e substantial worse than T¨uBa-D/Zbased parser output trees. We have shown that different treebank annotation schemes have a strong impact on parsing results for similar input data with similar (simulated) parser errors. Therefore the question whether a particular language is harder to parse than another language or not, can not be answered by comparing parsing results for parsers trained on treebanks with different annotation schemes. Comparing PARSEVALbased parsing results for a parser trained on the T¨uBa-D/Z or TIGER to results achieved by a parser trained on the English Penn-II treebank (Marcus et al., 1994) does not provide conclusive evidence about the parsability of a particular language, because the results show a bias introduced by the combined effect of annotation scheme and evaluation metric. This means that the question whether German is harder to parse than English, is still undecided. A possible way forward is perhaps a dependency-based evaluation of TIGER/T¨uBa-D/Z with Penn-II trained grammars for ’similar’ test and training sets and cross-treebank and -language controlled error insertion experiments. Even this is not entirely straightforward as it is not completely clear what constit"
D07-1066,C04-1024,0,0.035224,"lds or unary nodes on PCFG parsing, we convert the trees in the parser output of a parser trained on the original unconverted treebank resources. This allows us to preserve the basic syntactic structure and also the errors present in the output trees resulting from a potential bias in the original treebank training resources. The results for the original parser output evaluated against the unmodified gold trees should not be crucially different from the results for the modified parser output evaluated against the modified gold trees. 5.1 Experimental Setup For Experiment II we trained BitPar (Schmid, 2004), a parser for highly ambiguous PCFG grammars, on the two treebanks. The T¨uBa-D/Z training data consists of the 21067 treebank trees not included in the T¨uBa-D/Z test set. Because of the different size of the two treebanks we selected 21067 sentences from the TIGER treebank, starting from sentence 10000 (and excluding the sentences in the TIGER test set). Before extracting the grammars we resolved the crossing branches in the TIGER treebank as described in Section 3. After this preprocessing step we extracted an unlexicalised PCFG from each of our training sets. Our TIGER grammar has a total"
D07-1066,A97-1014,0,\N,Missing
K19-1044,P19-1620,0,0.0145013,"od. We observe that models trained on zˆ perform better than those trained on z, presumably because the test instances are represented the same way. To evaluate if zˆ is still an informative point and not just positioned randomly in feature space, we train a model on actual randomly sampled points. The sampled point z ∼ N (0, I) is decoded to query sequence x, labeled and subsequently re7 Discussion Related work in the context of semi-supervised learning has focused on developing methods to generate synthetic training instances for different tasks (Sennrich et al., 2016; Hayashi et al., 2018; Alberti et al., 2019; Winata et al., 2019), in order to accelerate the learning process. Sennrich et al. (2016) create artificial training instances for machine translation, using monolingual data paired with automatic back-translations. Their work obtains substantial improvements for several languages and has triggered many follow-up studies that apply the idea of back-translation to different tasks. For example, Hayashi et al. (2018) augment the training data for attention-based end-to-end automatic speech recognition with synthetic instances, and Winata et al. (2019) generate artificial training examples to im"
K19-1044,K16-1002,0,0.191899,"z− ) is formed by centroid c+ and centroid c− of positive and negative labeled instances respectively. The mid point zs is queried and, depending on the annotated label l, replaces the corresponding zl . This step is repeated b times, reducing the distance between the z = µ + σ  where  ∼ N (0, I) (1) and is the element-wise product. To prevent the model from pushing σ close to 0 and thus falling back to a deterministic autoencoder, the objective is extended by the Kullback-Leibler (KL) diver473 gence between prior p(z) and q(z|x): L(θ; x) = −KL(qθ (z|x)||p(z)) +Eqθ (z|x) [logpθ (x|z)]. (2) Bowman et al. (2016) apply this idea for sentence generation using an RNN as encoder and decoder. They observe that a strong auto-regressive language modeling ability in the decoder reduces the information stored in the latent variable, right up to a complete collapse of the KL term. They explore different techniques to weaken the decoder, like word dropout or KL term weight annealing, as possible solutions. This guarantees a semantically rich latent variable and good sentence generation ability. Below, we describe how to combine both techniques in order to generate meaningful queries for Membership Query Synthes"
K19-1044,D17-1070,0,0.026275,"uijser and van Gemert, 2017). In contrast to im472 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 472–481 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics age processing, discrete domains like natural language do not exhibit a direct mapping from feature to instance space. Strategies that circumvent this problem include the search for nearest (observed) neighbors in feature space (Wang et al., 2015) or crafting queries by switching words (Awasthi and Kanade, 2012). Sentence representation learning (Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Wang et al., 2019) in combination with new methods for semi-supervised learning (Kingma et al., 2014; Hu et al., 2017; Xu et al., 2017; Odena, 2016; Radford et al., 2017) have shown to improve classification tasks by leveraging unlabeled text. Methods based on deep generative models like GANs or VAEs are able to generate sentences from any point in representation space. Mehrjou et al. (2018) use VAEs to learn structural information from unlabeled data and use it as an additional criterion in conventional active learning to make it more robust against outliers and no"
K19-1044,W19-3504,0,0.0130248,"nguage Inference (NLI) or QA. This, however, requires modifications to the structure of the autoencoder and exceeds the scope of this work. Membership Query Synthesis might also be an interesting approach for tasks where the automatic extraction of large amounts of unlabelled data is not straight-forward. One example that comes to mind is the detection of offensive language or ’hate speech’, where we have to deal with highly unbalanced training sets with only a small number of positive instances, and attempts to increase this number have been shown to result in systematically biased datasets (Davidson et al., 2019; Wiegand et al., 2019). Table 2 suggests that the generator produces instances with a more balanced class ratio (1.7 and 1.2) than the pool data (2.6) it was trained on. It might be worthwhile to explore whether the generation of synthetic training instances can help to mitigate the problem to select instances from both classes in an highly imbalanced data pool. 8 Acknowledgments Part of this research has been conducted within the Leibniz Science Campus “Empirical Linguistics and Computational Modeling”, funded by the Leibniz Association under grant no. SAS-2015IDS-LWC and by the Ministry of"
K19-1044,P16-1009,0,0.0119335,"z and zˆ generated with the gen uniform method. We observe that models trained on zˆ perform better than those trained on z, presumably because the test instances are represented the same way. To evaluate if zˆ is still an informative point and not just positioned randomly in feature space, we train a model on actual randomly sampled points. The sampled point z ∼ N (0, I) is decoded to query sequence x, labeled and subsequently re7 Discussion Related work in the context of semi-supervised learning has focused on developing methods to generate synthetic training instances for different tasks (Sennrich et al., 2016; Hayashi et al., 2018; Alberti et al., 2019; Winata et al., 2019), in order to accelerate the learning process. Sennrich et al. (2016) create artificial training instances for machine translation, using monolingual data paired with automatic back-translations. Their work obtains substantial improvements for several languages and has triggered many follow-up studies that apply the idea of back-translation to different tasks. For example, Hayashi et al. (2018) augment the training data for attention-based end-to-end automatic speech recognition with synthetic instances, and Winata et al. (2019)"
K19-1044,D13-1170,0,0.00342764,"he SST2 training set. Table 1: Training parameters for the Variational Autoencoder. tator can skip neutral or uninterpretable instances. These skip actions also count towards the annotation budget. 5.1 Learner The Learner is an SVM1 with linear kernel. Each instance is represented as the latent variable z learned by the autoencoder. The latent variable is a vector with 50 dimensions and the SVM is trained on this representation. We calculate classification performance on the reduced SST2 test set and report F1-scores. Data The data used in our experiments comes from two sources, (i) the SST2 (Socher et al., 2013) and (ii) SAR14 (Nguyen et al., 2014). We limit sentence length to a maximum of 15 words. This is motivated by lower training times and the tendency of vanilla VAEs not to perform well on longer sentences (Shen et al., 2019). Generator The generator is the decoder of the VAE described above. Once a point z in feature space is selected, it is used as the input of the decoder x = dec(z) which generates the human readable sentence x in an autoregressive way. Sentiment task SST2 (Socher et al., 2013) is a binary sentiment classification dataset compiled from rottentomatoes.com. As we only consider"
K19-1044,N19-1060,0,0.0164752,"or QA. This, however, requires modifications to the structure of the autoencoder and exceeds the scope of this work. Membership Query Synthesis might also be an interesting approach for tasks where the automatic extraction of large amounts of unlabelled data is not straight-forward. One example that comes to mind is the detection of offensive language or ’hate speech’, where we have to deal with highly unbalanced training sets with only a small number of positive instances, and attempts to increase this number have been shown to result in systematically biased datasets (Davidson et al., 2019; Wiegand et al., 2019). Table 2 suggests that the generator produces instances with a more balanced class ratio (1.7 and 1.2) than the pool data (2.6) it was trained on. It might be worthwhile to explore whether the generation of synthetic training instances can help to mitigate the problem to select instances from both classes in an highly imbalanced data pool. 8 Acknowledgments Part of this research has been conducted within the Leibniz Science Campus “Empirical Linguistics and Computational Modeling”, funded by the Leibniz Association under grant no. SAS-2015IDS-LWC and by the Ministry of Science, Research, and"
K19-1044,K19-1026,0,0.0151074,"dels trained on zˆ perform better than those trained on z, presumably because the test instances are represented the same way. To evaluate if zˆ is still an informative point and not just positioned randomly in feature space, we train a model on actual randomly sampled points. The sampled point z ∼ N (0, I) is decoded to query sequence x, labeled and subsequently re7 Discussion Related work in the context of semi-supervised learning has focused on developing methods to generate synthetic training instances for different tasks (Sennrich et al., 2016; Hayashi et al., 2018; Alberti et al., 2019; Winata et al., 2019), in order to accelerate the learning process. Sennrich et al. (2016) create artificial training instances for machine translation, using monolingual data paired with automatic back-translations. Their work obtains substantial improvements for several languages and has triggered many follow-up studies that apply the idea of back-translation to different tasks. For example, Hayashi et al. (2018) augment the training data for attention-based end-to-end automatic speech recognition with synthetic instances, and Winata et al. (2019) generate artificial training examples to improve automatic speech"
kubler-etal-2008-compare,daum-etal-2004-automatic,0,\N,Missing
kubler-etal-2008-compare,brants-hansen-2002-developments,0,\N,Missing
kubler-etal-2008-compare,A97-1014,0,\N,Missing
kubler-etal-2008-compare,C04-1024,0,\N,Missing
kubler-etal-2008-compare,W06-1614,1,\N,Missing
kubler-etal-2008-compare,W07-1506,0,\N,Missing
kubler-etal-2008-compare,P03-1054,0,\N,Missing
kubler-etal-2008-compare,D07-1066,1,\N,Missing
kubler-etal-2008-compare,P06-3004,1,\N,Missing
kubler-etal-2008-compare,C04-1056,0,\N,Missing
kubler-etal-2008-compare,emms-2008-tree,0,\N,Missing
L16-1165,afantenos-etal-2012-empirical,0,0.138689,"automatic discourse relation labellers. Keywords: Annotation of discourse relations (DRs), interoperability of annotation schemes, DRs in spoken and written genres 1. Introduction Over the last decade, research in NLP has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed differently (Cuenca, 2015): Spoken communication is characterised by a high degree of interactivity that requires turn-taking devices for discourse management; sentence len"
L16-1165,S15-1016,0,0.243898,"last decade, research in NLP has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed differently (Cuenca, 2015): Spoken communication is characterised by a high degree of interactivity that requires turn-taking devices for discourse management; sentence length on average is shorter, and the pressure of rapid online processing often leads to disfluent structures. In contrast to written communication, the speaker and the hearer have access to additi"
L16-1165,chiarcos-2014-towards,0,0.106958,"has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed differently (Cuenca, 2015): Spoken communication is characterised by a high degree of interactivity that requires turn-taking devices for discourse management; sentence length on average is shorter, and the pressure of rapid online processing often leads to disfluent structures. In contrast to written communication, the speaker and the hearer have access to additional channels of"
L16-1165,W11-0401,0,0.0241548,"his endeavour. Chiarcos observes that in order to fully map the annotations from the RST and PDTB corpora, structural transformations are necessary, e.g. a conversion of the annotations to dependency DAGs. Benamara and Taboada (2015) propose a taxonomy for mapping discourse relations across two different frameworks, Rhetorical Structure Theory (RST) and Segmented Discourse Representation Theory (SDRT), and use it to map the annotations in three resources (the RST-DT English corpus (Carlson et al., 2002), the SDRT Annodis French corpus (Afantenos et al., 2012), and the RST Spanish Treebank (da Cunha et al., 2011)). The authors claim that their taxonomy is robust across theoretical frameworks and can be applied to corpora from different languages. However, they identify some problems for the mapping, e.g. SDRT does not distinguish between causal and epistemic uses of causal relations, and the C AUSE - RESULT and C ONSEQUENCE relations in the RST-DT corpus are similar and can correspond to either R EASON or R ESULT. Sanders et al. (In preparation) propose to use unifying dimensions to be able to ‘translate’ annotations from one framework to another. In their proposal, they extend the dimensions distingu"
L16-1165,W14-4916,0,0.183831,"Missing"
L16-1165,W15-0210,0,0.0159121,"is the second event. 3. Two different strands of research are relevant to our work. The first one focusses on the unification of different annotation schemes for the annotation of discourse relations, the second is on developing or adapting discourse annotation schemes for spoken language. 3.1. CCR The annotation framework of CCR is based on a cognitive theory of coherence relations, proposed by Sanders et al. (1992). The theory has since been used to annotate various types of discourse, including spoken text, chat fragRelated Work Mapping annotations across frameworks Bunt et al. (2012) and Prasad and Bunt (2015) describe efforts to create an international standard for the annotation of discourse with semantic relations, based on different frameworks for discourse annotation. They define a set of 20 core 1040 discourse relations (Prasad and Bunt, 2015) that they consider to be indispensible for the annotation of DRs, together with clear definitions and examples for each relation. In future work, they plan to provide mappings from their core set to the annotation labels in different DR frameworks such as the PDTB, RST, SDRT and CCR. Such an ambitious enterprise raises questions about the inter-operabil"
L16-1165,prasad-etal-2008-penn,0,0.820417,"her extensions. The new corpus has roughly the size of the CoNLL 2015 Shared Task test set, and we hence hope that it will be a valuable resource for the evaluation of automatic discourse relation labellers. Keywords: Annotation of discourse relations (DRs), interoperability of annotation schemes, DRs in spoken and written genres 1. Introduction Over the last decade, research in NLP has widened its scope, moving beyond the sentence level to analysing the discourse structure of a text. This has resulted in the creation of discourse-annotated corpora, such as the Penn Discourse Treebank (PDTB) (Prasad et al., 2008), the Rhetorical Structure Theory (RST) Treebank (Carlson et al., 2002), and the Annodis corpus (Segmented Discourse Representation Theory, SDRT) (Afantenos et al., 2012), to name a few. However, as of yet, the different frameworks are not inter-operable, nor is there a unified scheme for discourse annotation (but see the proposals by Benamara and Taboada (2015), Chiarcos (2014), Bunt et al. (2012), and Sanders et al. (In preparation)). Most discourse-annotated corpora are based on written rather than spoken text. This point is crucial, as spoken and written texts are produced and processed di"
L16-1165,tonelli-etal-2010-annotation,0,0.534873,"pping, that was created based on theoretical assumptions, can be considered as verified if a substantial part of the annotations in the two frameworks can be translated into each other, using the mapping. In Section 4., we present such a validation experiment, using the CCR and PDTB 3.0 frameworks, and analyse the results with respect to the predictions made by our mapping. 3.2. Annotating DRs in spoken language While most annotation projects have focussed on annotating discourse relations in written text, there are some studies on annotating discourse-relational devices in the spoken domain. Tonelli et al. (2010) adapt the PDTB 2.0 annotation scheme to make it more suitable for annotating spoken data from the LUNA Corpus, which is a language resource with helpdesk dialogues on the topic of software/hardware troubleshooting. The special properties of spoken conversation caused them to change the PDTB sense hierarchy and include new relations. The most important change is the addition of speechact relations in the sense of Sweetser (1990). Other changes include the G OAL relation, likely motivated by the peculiarities of the task-oriented help-desk dialogues. They also discarded the L IST relation, as i"
L16-1165,P09-1076,0,0.346103,"e PDTB framework to annotate spoken data are Demirs¸ahin and Zeyrek (2014) and Stoyanchev and Bangalore (2015). However, they have not made any further changes to the PDTB framework. 4. Data & Method The data we use in our annotation experiment comes from the SPICE-Ireland corpus (Kallen and Kirk, 2012), a corpus of spoken Irish English containing a variety of different genres, from which we selected broadcast interviews and telephone conversations for discourse relation annotation. The PDTB corpus also includes texts from different genres: essays, summaries, news and letters, as described in Webber (2009). This will allow us to investigate the differences in the use of DRs in different genres from the spoken and written domain. SPICE-Ireland corpus The corpus includes the spoken part of the ICE-Ireland corpus (Kallen and Kirk, 2008), with texts ranging over 15 different discourse settings (e.g. broadcast discussions, broadcast news, classroom discussions, private telephone conversations, or parliamentary debates, amongst others). SPICE-Ireland comprises pragmatic annotations on top of the transcriptions from the ICEIreland corpus, including mark-up for speech-act functions and discourse marker"
P10-1111,W97-0307,0,0.140606,"Missing"
P10-1111,W02-1503,0,0.0974823,"Missing"
P10-1111,P04-1041,1,0.830825,"Missing"
P10-1111,J08-1003,1,0.872782,"Missing"
P10-1111,P06-2018,1,0.937482,"Missing"
P10-1111,W04-1905,0,0.0139116,"). Due to the constraints imposed on the classification, the function labeller can no longer assign two subjects to the same S node. Faced with two nodes whose most probable label is SB, it has to decide on one of them taking the next best label for the other. This way, it outputs the optimal solution with respect to the set of constraints. Note that this requires the feature model not only to rank the correct label highest but also to provide a reasonable ranking of the other labels as well. 4 Evaluation We conducted a number of experiments using 1,866 sentences of the TiGer Dependency Bank (Forst et al., 2004) as our test set. The TiGerDB is a part of the TiGer Treebank semi-automatically converted into a dependency representation. We use the manually labelled TiGer trees corresponding to the sentences in the TiGerDB for assessing the labelling quality in the intrinsic evaluation, and precision f-score 83.60 83.20 recall tagging acc. 82.81 97.97 Table 1: evalb unlabelled parsing scores on test set for Berkeley Parser trained on 48,000 sentences (sentence length ≤ 40) 4.1 Intrinsic Evaluation In the intrinsic evaluation, we measured the quality of the labelling itself. We used the node span evaluati"
P10-1111,W07-1203,0,0.18101,"Missing"
P10-1111,P03-1054,0,0.0158506,"e functions constitute the core meaning of a sentence (as in: who did what to whom), it is important to get them right. We present a system that adds grammatical function labels to constituent parser output for German in a postprocessing step. We combine a statistical classifier with an integer linear program (ILP) to model non-violable global linguistic constraints, restricting the solution space of the classifier to those labellings that comply with our set of global constraints. There are, of course, many other ways of including functional information into the output of a syntactic parser. Klein and Manning (2003) show that merging some linguistically motivated function labels with specific syntactic categories can improve the performance of a PCFG model on Penn-II En1 Coordinate subjects/objects form a constituent that functions as a joint subject/object. 1087 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087–1097, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics glish data.2 Tsarfaty and Sim’aan (2008) present a statistical model (Relational-Realizational Parsing) that alternates between functional and configurational"
P10-1111,P07-2051,0,0.192396,"be labelled OG or AG9 . 7 Note that some of these constraints are language specific in that they represent linguistic facts about German and do not necessarily hold for other languages. Furthermore, the constraints are treebank specific to a certain degree in that they use a TiGer-specific set of labels and are conditioned on TiGer-specific configurations and categories. 8 SB = subject, OA = accusative object, OA2 = second accusative object, DA = dative, OG = genitive object, OP = prepositional object, PD = predicate, OC = clausal object, EP = expletive es 9 AG = genitive adjunct 1090 Unlike Klenner (2007), we do not use predefined subcategorization frames, instead letting the statistical model choose arguments. In TiGer, sentences whose main verbs are formed from auxiliary-participle combinations, are annotated by embedding the participle under an extra VP node and non-subject arguments are sisters to the participle. Therefore we add an extension of the constraint in (6) to the constraint set in order to also include the daughters of an embedded VP node in such a case. Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punct"
P10-1111,H05-1064,0,0.0761706,"Missing"
P10-1111,P95-1037,0,0.0370794,"number of daughters the number of terminals covered the lemma of the left/right corner terminal the category of the left/right corner terminal the category of the mother node the category of the mother’s head node the lemma of the mother’s head node the category of the grandmother node the category of the grandmother’s head node the lemma of the grandmother’s head node the case features for noun phrases the category for PP objects the lemma for PP objects (if terminal node) These features are also computed for the head of the phrase, determined using a set of headfinding rules in the style of Magerman (1995) adapted to TiGer. For lemmatisation, we use TreeTagger (Schmid, 1994) and case features of noun 1089 phrases are obtained from a full German morphological analyser based on (Schiller, 1994). If a noun phrase consists of a single word (e. g. pronouns, but also bare common nouns and proper nouns), all case values output by the analyser are used to reflect the case syncretism. For multi-word noun phrases, the case feature is computed by taking the intersection of all case-bearing words inside the noun phrase, i. e. determiners, pronouns, adjectives, common nouns and proper nouns. If, for some re"
P10-1111,P09-1039,0,0.0685521,"Missing"
P10-1111,E06-1011,0,0.201487,"Missing"
P10-1111,P06-1055,0,0.069168,"tension of the constraint in (6) to the constraint set in order to also include the daughters of an embedded VP node in such a case. Because of the particulars of the annotation scheme of TiGer, we can decide some labels in advance. As mentioned before, punctuation does not get a label in TiGer. We set the label for those nodes to −− (no label). Other examples are: the dependencies from TiGerDB for assessing the quality and coverage of the automatically acquired LFG resources in the extrinsic evaluation. In order to test on real parser output, the test set was parsed with the Berkeley Parser (Petrov et al., 2006) trained on 48k sentences of the TiGer corpus (Table 1), excluding the test set. Since the Berkeley Parser assumes projective structures, the training data and test data were made projective by raising non-projective nodes in the tree (K¨ubler, 2005). • If a node’s category is PTKVZ (separated verb particle), it is labeled SVP (separable verb particle). The maximum entropy classifier of the function labeller was trained on 46,473 sentences of the TiGer Treebank (excluding the test set) which yields about 1.2 million nodes as training samples. For training the Maximum Entropy Model, we used the"
P10-1111,C04-1197,0,0.0154566,"of German and does not allow for unary branching. The annotations use non-projective trees modelling long distance dependencies directly by crossing branches. Words are lemmatised and part-of-speech tagged with the Stuttgart-T¨ubingen Tag Set (STTS) (Schiller et al., 1999) and contain morphological annotations (Release 2). TiGer uses 25 syntactic categories and a set of 42 function labels to annotate the grammatical function of a phrase. The function labeller consists of two main components, a maximum entropy classifier and an integer linear program. This basic architecture was introduced by Punyakanok et al. (2004) for the task of semantic role labelling and since then has been applied to different NLP tasks without significant changes. In our case, its input is a bare tree 4 Although the classifier may, of course, still identify the wrong phrase as subject or object. structure (as obtained by a standard phrase structure parser) and it outputs a tree structure where every node is labelled with the grammatical relation it bears to its mother node. For each possible label and for each node, the classifier assigns a probability that this node is labelled by this label. This results in a complete probabilit"
P10-1111,W04-2401,0,0.0408733,"Missing"
P10-1111,C08-1112,0,0.0700124,"Missing"
P10-1111,A00-2031,0,\N,Missing
P10-1111,J08-2005,0,\N,Missing
P10-1111,J96-1002,0,\N,Missing
P11-1005,J09-4005,0,0.0942503,"of annotation noise in the context of supervised classification. Section 3 describes the experimental setup of our simulation study and presents results. In Section 4 we present our filtering approach and show its impact on AL performance. Section 5 concludes and outlines future work. ing, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The authors argue that even for those data points where the annotators agreed on one particular class, a proportion of the agreement might be merely due to chance. Following this assumption, the authors propose a measure to estimate the amount of annotation noise in the data after removing all hard cases. Klebanov et al. (2008; 2009) show that, according to their model, high inter-annotator agreement (κ) ac"
P11-1005,W08-1202,0,0.0312749,"n recent research on the impact of annotation noise in the context of supervised classification. Section 3 describes the experimental setup of our simulation study and presents results. In Section 4 we present our filtering approach and show its impact on AL performance. Section 5 concludes and outlines future work. ing, while systematic errors (as caused by biased annotators) can seriously impair the performance of a supervised classifier even if the observed accuracy of the classifier on a test set coming from the same population as the training data is as high as 0.8. Related work (Beigman Klebanov et al., 2008; Beigman Klebanov and Beigman, 2009) has been studying annotation noise in a multi-annotator setting, distinguishing between hard cases (unreliably annotated due to genuine ambiguity) and easy cases (reliably annotated data). The authors argue that even for those data points where the annotators agreed on one particular class, a proportion of the agreement might be merely due to chance. Following this assumption, the authors propose a measure to estimate the amount of annotation noise in the data after removing all hard cases. Klebanov et al. (2008; 2009) show that, according to their model,"
P11-1005,P07-1007,0,0.0225273,"(e.g. sentiment analysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learn2.2 44 Annotation Noise and Active Learning For AL to be succesful, we need to remove"
P11-1005,N06-1016,0,0.0259507,"for many NLP tasks (e.g. sentiment analysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learn2.2 44 Annotation Noise and Active Learning For AL to be succesful,"
P11-1005,P09-1021,0,0.0207875,"ics Saarland University josefr@coli.uni-sb.de Ines Rehbein Computational Linguistics Saarland University rehbein@coli.uni-sb.de Abstract Active learning has been applied to several NLP tasks like part-of-speech tagging (Ringger et al., 2007), chunking (Ngai and Yarowsky, 2000), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Named Entity Recognition (Shen et al., 2004; Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), Word Sense Disambiguation (Chen et al., 2006; Zhu and Hovy, 2007; Chan and Ng, 2007), text classification (Tong and Koller, 1998) or statistical machine translation (Haffari and Sarkar, 2009), and has been shown to reduce the amount of annotated data needed to achieve a certain classifier performance, sometimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense"
P11-1005,J04-3001,0,0.074961,"Missing"
P11-1005,C08-1059,0,0.0426874,"Missing"
P11-1005,P00-1016,0,0.0724716,"Missing"
P11-1005,N04-1012,0,0.0727825,"Missing"
P11-1005,C10-1107,1,0.797444,"metimes by as much as half. Most of these studies, however, have only simulated the active learning process using goldstandard data. This setting is crucially different from a real world scenario where we have to deal with erroneous data and inconsistent annotation decisions made by the human annotators. While simulations are an indispensable instrument to test different parameters and settings, it has been shown that when applying AL to highly ambiguous tasks like e.g. Word Sense Disambiguation (WSD) with fine-grained sense distinctions, AL can actually harm the learning process (Dang, 2004; Rehbein et al., 2010). Dang suggests that the lack of a positive effect of AL might be due to inconsistencies in the human annotations and that AL cannot efficiently be applied to tasks which need double blind annotation with adjudication to insure a sufficient data quality. Even if we take a more optimistic view and assume that AL might still be useful even for tasks featuring a high degree of ambiguity, it remains crucial to address the problem of annotation noise and its impact on AL. Active Learning (AL) has been proposed as a technique to reduce the amount of annotated data needed in the context of supervised"
P11-1005,J08-3001,0,0.36679,"ssumes that a) all instances where annotators disagreed are in fact hard cases, and b) that for the hard cases the annotators decisions are obtained by coin-flips. In our experience, some amount of disagreement can also be observed for easy cases, caused by attention slips or by a deviant interpretation of some class(es) by one of the annotators, and the annotation decision of an individual annotator cannot so much be described as random choice (coin-flip) but as systematically biased selection, causing the types of errors which have been shown to be problematic for supervised classification (Reidsma and Carletta, 2008). Further problems arise in the AL scenario where the instances to be annotated are selected as a function of the sampling method and the annotation judgements made before. Therefore, Beigman and Klebanov Beigman (2009)’s approach of identifying unreliably annotated instances by disagreement is not applicable to AL, as most instances are annotated only once. 2 Related Work We are interested in the question whether or not AL can be successfully applied to a supervised classification task where we have to deal with a considerable amount of inconsistencies and noise in the data, which is the case"
P11-1005,W07-1516,0,0.0526379,"Missing"
P11-1005,P04-1075,0,0.0690551,"Missing"
P11-1005,D07-1082,0,0.028536,"alysis, the detection of metaphors, WSD with fine-grained word senses, to name but a few). Therefore we do not consider part-of-speech tagging or syntactic parsing, where coders are expected to agree on most annotation decisions. Instead, we focus on work on AL for WSD, where intercoder agreement (at least for fine-grained annotation schemes) usually is much lower than for the former tasks. 2.1 Annotation Noise Studies on active learning for WSD have been limited to running simulations of AL using gold standard data and a coarse-grained annotation scheme (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007). Two exceptions are Dang (2004) and Rehbein et al. (2010) who both were not able to replicate the positive findings obtained for AL for WSD on coarse-grained sense distinctions. A possible reason for this failure is the amount of annotation noise in the training data which might mislead the classifier during the AL process. Recent work on the impact of annotation noise on a machine learning task (Reidsma and Carletta, 2008) has shown that random noise can be tolerated in supervised learn2.2 44 Annotation Noise and Active Learning For AL to be succesful, we need to remove systematic noise in t"
P11-1005,C08-1143,0,0.0190414,"s noise-free. For classification we use a maximum entropy classifier.3 Our sampling method is uncertainty sampling (Lewis and Gale, 1994), a standard sampling heuristic for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. As a measure of uncertainty we use Shannon entropy (1) (Zhang and Chen, 2002) and the margin metric (2) (Schein and Ungar, 2007). The first measure considers the model’s predictions q for each class c and selects those instances from the pool where the Shannon entropy is highest. assigned by the human coders. Zhu et al. (2008) present a method for detecting outliers in the pool of unannotated data to prevent these instances from becoming part of the training data. This approach is different from ours, where we focus on detecting annotation noise in the manually labelled training data produced by the human coders. Schein and Ungar (2007) provide a systematic investigation of 8 different sampling methods for AL and their ability to handle different types of noise in the data. The types of noise investigated are a) prediction residual error (the portion of squared error that is independent of training set size), and b"
P17-1107,E03-1068,0,0.127447,"entify errors in automatically labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have proposed selective relabelling"
P17-1107,W11-3405,0,0.0226244,"with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have proposed selective relabelling strategies when working with non-exper"
P17-1107,D09-1015,0,0.0113734,"a development set during training, we also extract the dev data from sections 00-18 of the PTB. 1163 hours. This is a problem for the typical AL setting where it is crucial not to keep the human annotators waiting for the next instance while the system retrains. A major advantage of our setup is that we do not need to retrain the baseline classifiers as we only use them once, for preprocessing, before the actual error detection starts. For the NER experiment, we use tools for which pretrained models for German are available, namely GermaNER (Benikova et al., 2015), and the StanfordNER system (Finkel and Manning, 2009) with models trained on the HGC and the DeWaC corpus (Baroni et al., 2009; Faruqui and Pad´o, 2010).6 4.2 EVAL: tagger acc. Preprocessing Annotation matrix: ... cn ... DT N NE ... N V V ... V ... ... ... ... AL for N iterations Select instances Evaluation measures QBC entropy To increase the number of annotators we use an older version of the StanfordNER (2009-01-16) and a newer version (2015-12-09), with both the DeWaC and HGC models, resulting in a total of 5 annotators for the NER task. 7 Please note that the success of our method relies on the variation in the ensemble predictions, and thu"
P17-1107,P09-1032,0,0.053732,"Missing"
P17-1107,gimenez-marquez-2004-svmtool,0,0.0309355,"Missing"
P17-1107,P07-2053,0,0.0441676,"Missing"
P17-1107,N13-1132,0,0.574409,"rrection of automatically prelabelled text is not feasible. Given the importance of identifying noisy annotations in automatically annotated data, it is all the more surprising that up to now this area of research has been severely understudied. This paper addresses this gap and presents a method for error detection in automatically labelled text. As test cases, we use POS tagging and Named Entity Recognition, both standard preprocessing steps for many NLP applications. However, our approach is general and can also be applied to other classification tasks. Our approach is based on the work of Hovy et al. (2013) who develop a generative model for estimating the reliability of multiple annotators in a crowdsourcing setting. We adapt the generative model to the task of finding errors in automatically labelled data by integrating it in an active learning (AL) framework. We first show that the approach of Hovy et al. (2013) on its own is not able to beat a strong baseline. We then present our integrated model, in which we impose human supervision on the generative model through AL, and show that we are able to achieve substantial improvements in two different tasks and for two languages. Our contribution"
P17-1107,D07-1031,0,0.0204342,"al behaviour of annotator j in the case that the annotator is not trying to predict the correct label. The model parameters are learned by maximizing the marginal likelihood of the observed data, using Expectation Maximization (EM) (Dempster et al., 1977) and Bayesian variational inference. Bayesian inference is used to provide the model with priors on the annotators’ behaviour and yields improved correlations over EM between the model estimates and the annotators’ proficiency while keeping accuracy high. For details on the implementation and parameter settings refer to Hovy et al. (2013) and Johnson (2007). We adapt the model of Hovy et al. (2013) and apply it to the task of error detection in automatically labelled text. To that end, we integrate the variational model in an active learning (AL) setting, with the goal of identifying as many errors as possible while keeping the number of instances to be checked as small as possible. The tasks we chose in our experiments are POS tagging and NER, but our approach is general and can easily be applied to other classification tasks. 3.2 Active learning Active learning (Cohn et al., 1996) is a semisupervised framework where a machine learner is traine"
P17-1107,C02-1021,0,0.0703545,"ction that is able to identify errors in automatically labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have pr"
P17-1107,P10-1052,0,0.0404155,"Missing"
P17-1107,E09-1060,0,0.244572,"y labelled text with high precision and high recall. To the best of our knowledge, our method is the first that addresses this task in an AL framework. We show how AL can be used to guide an unsupervised generative model, and we will make our code available to the research community.1 Our approach works particularly well in out-of-domain settings where no annotated training data is yet available. 2 Related work Quite a bit of work has been devoted to the identifcation of errors in manually annotated corpora (Eskin, 2000; van Halteren, 2000; Kveton and Oliva, 2002; Dickinson and Meurers, 2003; Loftsson, 2009; Ambati et al., 2011). 1 Our code is available at http://www.cl. uni-heidelberg.de/˜rehbein/resources. 1160 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1160–1170 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1107 Several studies have tried to identify trustworthy annotators in crowdsourcing settings (Snow et al., 2008; Bian et al., 2009), amongst them the work of Hovy et al. (2013) described in Section 3. Others have proposed selective relabelling strategies when"
P17-1107,P14-1014,0,0.0701229,"Missing"
P17-1107,P16-2067,0,0.0416643,"Missing"
P17-1107,W14-4903,1,0.896919,"Missing"
P17-1107,J08-3001,0,0.0222801,"Missing"
P17-1107,N03-1033,0,0.0311353,"Missing"
P17-1107,W00-1907,0,0.529233,"Missing"
P17-1107,D08-1027,0,0.296059,"Missing"
rehbein-etal-2014-kiezdeutsch,I11-1041,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,W99-0608,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,schmidt-2012-exmaralda,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,E06-1034,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,E03-1068,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,C02-1021,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,E09-1060,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,W00-1907,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,P10-2038,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,P98-1029,0,\N,Missing
rehbein-etal-2014-kiezdeutsch,C98-1029,0,\N,Missing
rehbein-ruppenhofer-2010-theres,N07-1051,0,\N,Missing
rehbein-ruppenhofer-2010-theres,nivre-etal-2006-maltparser,0,\N,Missing
rehbein-ruppenhofer-2010-theres,burchardt-etal-2006-salsa,0,\N,Missing
rehbein-ruppenhofer-2010-theres,N06-1016,0,\N,Missing
rehbein-ruppenhofer-2010-theres,D07-1082,0,\N,Missing
rehbein-ruppenhofer-2010-theres,N03-1033,0,\N,Missing
rehbein-ruppenhofer-2010-theres,W08-1008,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P03-1054,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P95-1026,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P98-1013,0,\N,Missing
rehbein-ruppenhofer-2010-theres,C98-1013,0,\N,Missing
rehbein-ruppenhofer-2010-theres,P07-1007,0,\N,Missing
rehbein-ruppenhofer-2010-theres,I05-1081,0,\N,Missing
rehbein-ruppenhofer-2010-theres,kunze-lemnitzer-2002-germanet,0,\N,Missing
rehbein-ruppenhofer-2010-theres,W09-1107,0,\N,Missing
W07-2460,brants-hansen-2002-developments,0,0.0749195,"Missing"
W07-2460,W06-1614,0,0.083109,"Missing"
W07-2460,P06-3004,0,0.039055,"Missing"
W07-2460,C04-1024,0,0.0423635,"al., 2004) than for a parser trained on the NEGRA treebank (Skut et al., 1997). Maier (2006) takes that as evidence that the NEGRA annotation scheme is less adequate for PCFG parsing, while a parser trained on the T¨uBa-D/Z yields PARSEVAL results in the same range as a parser trained on the English Penn-II treebank (K¨ubler et al., 2006). These results are based on the assumption that PARSEVAL is an appropriate measure for comparing parser performance of a PCFG parser trained on treebanks with different annotation schemes. This paper presents parsing experiments with the PCFG parser BitPar (Schmid, 2004) trained on two German treebanks. The treebanks contain text from the same domain, namely two German daily newspapers, but differ considerably with regard to their annotation schemes. We score parsing results using three different evaluation measures and show that the PARSEVAL results do not correlate with the results of the other metrics. An analysis of specific error types shows the differences between the three measures. Our results indicate that dependencybased evaluation is most appropriate to compare parser output for parsers trained on different treebank annotation schemes. Section 2 de"
W07-2460,A97-1014,0,0.754871,"Missing"
W09-3003,W04-3202,0,0.070424,"Missing"
W09-3003,brants-plaehn-2000-interactive,0,0.0682926,"Missing"
W09-3003,burchardt-etal-2006-salto,0,0.0276165,"Missing"
W09-3003,H01-1026,0,0.0727584,"Missing"
W09-3003,W06-0602,0,0.0520171,"Missing"
W09-3003,W07-1509,0,0.0260804,"Missing"
W09-3003,J93-2004,0,0.0329044,"ion and for Shalmaneser error types are made by human annotators throughout all three annotation trials, and that these errors are different from the ones made by the ASRL. Indicated by f-score, the most difficult frames in our data set are Scrutiny, Fluidic motion, Seeking, Make noise and Communication noise. This shows that automatic pre-annotation, even if noisy and of low quality, does not corrupt human annotators on a grand scale. Furthermore, if the preannotation is good it can even improve the overall annotation quality. This is in line with previous studies for other annotation tasks (Marcus et al., 1993). 4.3 pre-annotation decreases annotation quality. Most interestingly, the two annotators who showed a decrease in f-score on the text segments pre-annotated by Shalmaneser (compared to the text segments with no pre-annotation provided) had been assigned to the same group (Group I). Both had first annotated the enhanced, high-quality pre-annotation, in the second trial the sentences pre-annotated by Shalmaneser, and finally the texts with no pre-annotation. It might be possible that they benefitted from the ongoing training, resulting in a higher f-score for the third text segment (no pre-anno"
W09-3003,P02-1045,0,0.0339238,"cquisition bottleneck is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly 19 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP frame typically also requires fewer physical operations from the annotator than correcting a number of wrongly assigned frame elements. We aim to answer three research questions in our study: First, we explore whether pre-annotation of frame labels can indeed speed up the annotation process. This question is important because frame as"
W09-3003,W03-1015,0,0.0167118,"is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly 19 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP frame typically also requires fewer physical operations from the annotator than correcting a number of wrongly assigned frame elements. We aim to answer three research questions in our study: First, we explore whether pre-annotation of frame labels can indeed speed up the annotation process. This question is important because frame assignment, in terms of"
W09-3003,C02-1145,0,0.0741674,"Missing"
W09-3003,P98-1013,0,0.00732122,"number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem from the data creation side. Specifically we explore whether a semi-automatic annotation set-up in which a human expert corrects the output of an automatic system can help to speed up the annotation process without sacrificing annotation quality. For our study, we explore the task of framesemantic argument structure annotation (Baker et al., 1998). We chose this particular task because it is a rather complex – and therefore time-consuming – undertaking, and it involves making a number of different but interdependent annotation decisions for each instance to be labeled (e.g. frame assignment and labeling of frame elements, see Section 3.1). Semi-automatic support would thus be of real benefit. More specifically, we explore the usefulness of automatic pre-annotation for the first step in the annotation process, namely frame assignment (word sense disambiguation). Since the available inventory of frame elements is dependent on the chosen"
W09-3003,C98-1013,0,\N,Missing
W09-3820,E06-2001,0,0.0433967,"Missing"
W09-3820,P05-1022,0,0.173263,"ment set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only local features reaps most of the benefits of the global model and performs at a similar level than earlier reranking-based approaches, pointing to the fact that local ambiguities often result in the n-best list not containing the correct parse. The model we propose here extracts a pruned parse forest from a simple unlexicalized parser and then uses a factored discriminative model to apply a rich set of features using the lexicalized parse"
W09-3820,P05-1039,0,0.537671,"Ines Rehbein Dep. of Computational Linguistics Universit¨at des Saarlandes rehbein@coli.uni-sb.de Introduction To capture the semantic relations inherent in a text, parsing has to recover both structural information and grammatical functions, which commonly coincide in English, but not in freer word order languages such as German. Instead one has to make use of morphological features in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject<)dative<accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees 2 Parsing German with Morphology and Valence Information As a base parser, we use BitPar (Schmid, 2004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, c Paris, October 2009"
W09-3820,P08-1109,0,0.0477397,"kov[lex] markovGF[lex] markovGF[+pp] no GFs 77.40 72.09 74.66 73.94 75.00 77.68 77.55 78.43 with GFs NA 60.48 62.47 61.63 63.58 66.05 66.69 67.90 Table 1: Evaluation results: PARSEVAL F1 on PaGe development set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only local features reaps most of the benefits of the global model and performs at a similar level than earlier reranking-based approaches, pointing to the fact that local ambiguities often result in the n-best list not containing the cor"
W09-3820,P06-1041,0,0.0446749,"iner categorization to restore the grammatical function labels automatically: Using the most frequent function label sequence associated with a rule yields good results even in the presence of markovization, where some of the surrounding context is lost. Furthermore, this approach allows us to use the grammatical function label assignments in the subsequent discriminative model, thus yielding typed dependencies rather than the unlabeled dependencies that are used in the lexicalization model of the Stanford parser. Verb Valence We use information from the lexicon of the WCDG parser for German (Foth and Menzel, 2006) to mark verbs according to the arguments that they can take. While the WCDG lexicon contains more information, we only encode the possibility of accusative and dative complements, ignoring entries for genitive or clausal complements. Markovization with Argument Marking It has been noted consistently (Klein and Manning, 2003; Schiehlen, 2004) that using markovization - replacing the original treebank rules by an approximation that only considers a limited context window of one or two siblings - improves results at least for a constituency-based evaluation. However, in some cases this simple ma"
W09-3820,P02-1036,0,0.0228928,"ed to approaches that reattach dependents in the output of the parser rather than integrating them in the parsing process. 135 Settings Rafferty and Manning (2008) —, training with GFs markov[unlex] markov+parent[unlex] markovGF[unlex] markov[lex] markovGF[lex] markovGF[+pp] no GFs 77.40 72.09 74.66 73.94 75.00 77.68 77.55 78.43 with GFs NA 60.48 62.47 61.63 63.58 66.05 66.69 67.90 Table 1: Evaluation results: PARSEVAL F1 on PaGe development set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only"
W09-3820,P08-1067,0,0.0891415,"lation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only local features reaps most of the benefits of the global model and performs at a similar level than earlier reranking-based approaches, pointing to the fact that local ambiguities often result in the n-best list not containing the correct parse. The model we propose here extracts a pruned parse forest from a simple unlexicalized parser and then uses a factored discriminative model to apply a rich set of features using the lexicalized parse tree and its typed dependencies. fW-w-pos, CW-w-pos word form, cluster f-sp , fS-sp -size node"
W09-3820,P03-1054,0,0.0259084,"abel assignments in the subsequent discriminative model, thus yielding typed dependencies rather than the unlabeled dependencies that are used in the lexicalization model of the Stanford parser. Verb Valence We use information from the lexicon of the WCDG parser for German (Foth and Menzel, 2006) to mark verbs according to the arguments that they can take. While the WCDG lexicon contains more information, we only encode the possibility of accusative and dative complements, ignoring entries for genitive or clausal complements. Markovization with Argument Marking It has been noted consistently (Klein and Manning, 2003; Schiehlen, 2004) that using markovization - replacing the original treebank rules by an approximation that only considers a limited context window of one or two siblings - improves results at least for a constituency-based evaluation. However, in some cases this simple markovization scheme leads to undesirable results including sentences with multiple subjects, as predicative arguments also have nominative case. To avoid this, we additionally mark which arguments have already been seen, yielding node labels such as S fin<VVFIN a<RNP a<sa in the case of a partial constituent for a finite sent"
W09-3820,E09-1047,0,0.0193481,"Missing"
W09-3820,W08-1005,0,0.110669,"Missing"
W09-3820,W08-1006,0,0.155962,"ding a “CC” ending to the node label. Finally, auxiliaries are split according to their verb lemma into sein (be), haben (have), werden (become). To aid the identification of noun phrase case, we add information related to case/number/gender syncretism to the preterminal labels of determiners, nouns, and adjectives (for details, see Versley, 2005) that allows to accurately determine the set of possible cases while keeping the size of the tagset relatively small . Restoring Grammatical Functions Adding edge labels to the nodes in PCFG parsing easily creates sparse data problems, as reported by Rafferty and Manning (2008), who witness a drop in constituent F-measure (excluding grammatical function labels) when they include function labels in the symbols of their PCFG. On the other hand, the informativity of grammatical function labels for the contents of the node does not always justify their cost in terms of data sparseness. Thus, we chose an approach where we include linguistically relevant information in the node labels (see above), and use the finer categorization to restore the grammatical function labels automatically: Using the most frequent function label sequence associated with a rule yields good res"
W09-3820,C04-1056,0,0.77237,"nto the parser. 1 Ines Rehbein Dep. of Computational Linguistics Universit¨at des Saarlandes rehbein@coli.uni-sb.de Introduction To capture the semantic relations inherent in a text, parsing has to recover both structural information and grammatical functions, which commonly coincide in English, but not in freer word order languages such as German. Instead one has to make use of morphological features in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject<)dative<accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees 2 Parsing German with Morphology and Valence Information As a base parser, we use BitPar (Schmid, 2004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, c Paris,"
W09-3820,C04-1024,0,0.142505,"to recover both structural information and grammatical functions, which commonly coincide in English, but not in freer word order languages such as German. Instead one has to make use of morphological features in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject<)dative<accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees 2 Parsing German with Morphology and Valence Information As a base parser, we use BitPar (Schmid, 2004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, c Paris, October 2009. 2009 Association for Computational Linguistics Unknown Words For the base PCFG parse, we use a decision tree with 43 regular expressions as features, five of which are tailored t"
W09-3820,schmid-etal-2004-smor,0,0.0236103,"for Computational Linguistics Unknown Words For the base PCFG parse, we use a decision tree with 43 regular expressions as features, five of which are tailored towards recognizing the past and zu-infinitive form of separable prefix verbs (abarbeiten ⇒ abgearbeitet, abzuarbeiten), which cannot be recognized by considering suffixes only. The extended part of speech tags for verbs (which contain valency information) are interpolated between the distribution at the concrete leaf of the decision tree and the global valency distribution for the (coarse) part-ofspeech tag. Additionally, we use SMOR (Schmid et al., 2004) in conjunction with the verb lexicon and a gazetteer list containing person and location names to determine possible fine-grained part-ofspeech tags for unknown words. rived from the Tiger treebank and largely inspired by earlier work on annotated treebank grammars for German (Schiehlen, 2004; Dubey, 2005; Versley, 2005). Subcategorization With respect to the treebank grammar, we refine the node labels with linguistically important information that is only implicit in the treebank but would be tedious (and pointless) to annotate by hand: Firstly, we annotate NPs by case; clause nodes (S and V"
W09-3820,W08-1008,0,\N,Missing
W10-1401,P05-1038,0,0.0195671,"arily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 on N EGRA. They showed that usi"
W10-1401,W10-1408,1,0.785091,"Missing"
W10-1401,W10-1404,0,0.236375,"s is substantial lexical data sparseness due to high morphological variation in surface forms. The question is therefore, given our finite, and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of stat"
W10-1401,H91-1060,0,0.0333452,"Missing"
W10-1401,E03-1005,0,0.0521711,"Missing"
W10-1401,W06-2920,0,0.219462,"red MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. While dependency parsers are adaptable to many languages, as reflected in the multiplicity of the languages covered,1 the analysis by Nivre et al. (2007b) shows that the best result was obtained for English, followed by Catalan, and that the most difficult languages to parse were Arabic, Basque, and Greek. Nivre et al. (2007a) drew a somewhat typological conclusion, that languages with rich morphology and free word order are the hardest to parse. This was shown to be the case for both MaltParser (Nivre e"
W10-1401,W10-1409,1,0.834713,"and often fairly small, annotated sets of data, how can we guess the morphological analyses, including the PoS tag assignment and various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of"
W10-1401,W08-2102,0,0.0218025,"Missing"
W10-1401,A00-2018,0,0.0303149,"Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologic"
W10-1401,P00-1058,0,0.0168692,"Missing"
W10-1401,W10-1406,0,0.0567829,"Missing"
W10-1401,P99-1065,0,0.261618,"Missing"
W10-1401,P97-1003,0,0.183573,"r) are reflected in the form of words, morphological information is often secondary to other syntactic factors, such as the position of words and their arrangement into phrases. German, an Indo-European language closely related to English, already exhibits some of the properties that make parsing MRLs problematic. The Semitic languages Arabic and Hebrew show an even more extreme case in terms of the richness of their morphological forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lex"
W10-1401,H05-1100,0,0.0111384,"ful in parsing English are necessarily appropriate for parsing MRLs – but associated with this question are important questions concerning the annotation scheme of the related treebanks. Obviously, when annotating structures for languages with characteristics different than English one has to face different annotation decisions, and it comes as no surprise that the annotated structures for MRLs often differ from those employed in the PTB. 1 The shared tasks involved 18 languages, including many MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish. For Spanish and French, it was shown by Cowan and Collins (2005) and in (Arun and Keller, 2005; Schluter and van Genabith, 2007), that restructuring the treebanks’ native annotation scheme to match the PTB annotation style led to a significant gain in parsing performance of Head-Driven models of the kind proposed in (Collins, 1997). For German, a language with four different treebanks and two substantially different annotation schemes, it has been shown that a PCFG parser is sensitive to the kind of representation employed in the treebank. Dubey and Keller (2003), for example, showed that a simple PCFG parser outperformed an emulation of Collins’ model 1 o"
W10-1401,2008.jeptalnrecital-long.17,1,0.829354,"Missing"
W10-1401,P03-1013,0,0.0556766,"rted to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s"
W10-1401,P05-1039,0,0.0235903,"cal forms and the flexibility in their syntactic ordering. 2.2 Parsing MRLs Pushing the envelope of constituency parsing: The Head-Driven models of the type proposed by Collins (1997) have been ported to parsing many MRLs, often via the implementation of Bikel (2002). For Czech, the adaptation by Collins et al. (1999) culminated in an 80 F1 -score. German has become almost an archetype of the problems caused by MRLs; even though German has a moderately rich morphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not"
W10-1401,P08-1109,0,0.0528707,"Missing"
W10-1401,W10-1412,1,0.240586,"various features, of an OOV word? How can we learn the probabilities of such assignments? In a more general setup, this problem is akin to handling out-of-vocabulary or rare words for robust statistical parsing, and techniques for domain adaptation via lexicon enhance5 Constituency-Based Dependency-Based (Marton et al., 2010)† (Bengoetxea and Gojenola, 2010) - German Hebrew Hindi (Attia et al., 2010) (Attia et al., 2010) (Attia et al., 2010) (Seddah et al., 2010) (Candito and Seddah, 2010)† (Maier, 2010) (Tsarfaty and Sima’an, 2010) - Korean (Chung et al., 2010) Arabic Basque English French (Goldberg and Elhadad, 2010)† (Ambati et al., 2010a)† (Ambati et al., 2010b) - Table 1: An overview of SPMRL contributions. († report results also for non-gold standard input) ment (also explored for English and other morphologically impoverished languages). So, in fact, incorporating morphological information inside the syntactic model for the purpose of statistical parsing is anything but trivial. In the next section we review the various approaches taken in the individual contributions of the SPMRL workshop for addressing such challenges. 4 Parsing MRLs: Recurring Trends The first workshop on parsing MRLs features 11"
W10-1401,E09-1038,1,0.7929,"isambiguate the morphological analyses of input forms? Should we do that prior to parsing or perhaps jointly with it?2 Representation and Modeling: Assuming that the input to our system reflects morphological information, one way or another, which types of morpho2 Most studies on parsing MRLs nowadays assume the gold standard segmentation and disambiguated morphological information as input. This is the case, for instance, for the Arabic parsing at CoNLL 2007 (Nivre et al., 2007a). This practice deludes the community as to the validity of the parsing results reported for MRLs in shared tasks. Goldberg et al. (2009), for instance, show a gap of up to 6pt F1 -score between performance on gold standard segmentation vs. raw text. One way to overcome this is to devise joint morphological and syntactic disambiguation frameworks (cf. (Goldberg and Tsarfaty, 2008)). logical information should we include in the parsing model? Inflectional and/or derivational? Case information and/or agreement features? How can valency requirements reflected in derivational morphology affect the overall syntactic structure? In tandem with the decision concerning the morphological information to include, we face genuine challenges"
W10-1401,W05-0303,0,0.043074,"Missing"
W10-1401,P08-1067,0,0.0516751,"Missing"
W10-1401,P03-1054,0,0.00472369,"rphology and a moderately free word order, parsing results are far from those for English (see (K¨ubler, 2008) and references therein). Dubey (2005) showed that, for German parsing, adding case and morphology information together with smoothed markovization and an adequate unknown-word model is more important than lexicalization (Dubey and Keller, 2003). For Modern Hebrew, Tsarfaty and Sima’an (2007) show that a simple treebank PCFG augmented with parent annotation and morphological information as state-splits significantly outperforms Head-Driven markovized models of the kind made popular by Klein and Manning (2003). Results for parsing Modern Standard Arabic using Bikel’s implementation on gold-standard tagging and segmentation have not improved substantially since the initial release of the treebank (Maamouri et al., 2004; Kulick et al., 2006; Maamouri et al., 2008). For Italian, Corazza et al. (2004) used the Stanford parser and Bikel’s parser emulation of Collins’ model 2 (Collins, 1997) on the ISST treebank, and obtained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (200"
W10-1401,W06-1614,0,0.154599,"Missing"
W10-1401,P95-1037,0,0.0299787,"Missing"
W10-1401,W10-1407,0,0.0148488,"elements into account, and thus learn the different distributions associated with morphologically marked elements in constituency structures, to improve performance. In addition to free word order, MRLs show higher degree of freedom in extraposition. Both of these phenomena can result in discontinuous structures. In constituency-based treebanks, this is either annotated as additional information which has to be recovered somehow (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that th"
W10-1401,J93-2004,0,0.0355629,". We synthesize the contributions of researchers working on parsing Arabic, Basque, French, German, Hebrew, Hindi and Korean to point out shared solutions across languages. The overarching analysis suggests itself as a source of directions for future investigations. 1 Introduction The availability of large syntactically annotated corpora led to an explosion of interest in automatically inducing models for syntactic analysis and disambiguation called statistical parsers. The development of successful statistical parsing models for English focused on the Wall Street Journal Penn Treebank (PTB, (Marcus et al., 1993)) as the primary, and sometimes only, resource. Since the initial release of the Penn Treebank (PTB Marcus et Among the arguments that have been proposed to explain this performance gap are the impact of small data sets, differences in treebanks’ annotation schemes, and inadequacy of the widely used PARS E VAL evaluation metrics. None of these aspects in isolation can account for the systematic performance deterioration, but observed from a wider, crosslinguistic perspective, a picture begins to emerge – that the morphologically rich nature of some of the languages makes them inherently more s"
W10-1401,W10-1402,0,0.0399505,"Missing"
W10-1401,E06-1011,0,0.0266021,"labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the"
W10-1401,P05-1012,0,0.11798,"Missing"
W10-1401,P05-1013,0,0.0312259,"how (traces in the case of the PTB, complex edge labels in the German T¨uBa-D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with medium"
W10-1401,P09-1040,0,0.0260548,"D/Z), or as discontinuous phrase structures, which cannot be handled with current PCFG models. Maier (2010) suggests the use of Linear Context-Free Rewriting Systems (LCFRSs) in order to make discontinuous structure transparent to the parsing process and yet preserve familiar notions from constituency. Dependency representation uses non-projective dependencies to reflect discontinuities, which is problematic to parse with models that assume projectivity. Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. Moreover, they show that in combination with other transformations, it improves the utility of these other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare"
W10-1401,N07-1051,0,0.0143897,"markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representation format does not rely crucially on the position of words and the internal grouping of surface chunks (Mel’ˇcuk, 1988). It is an entirely different question, however, whether dependency parsers are in fact better suited for parsing such languages. The CoNLL shared tasks on"
W10-1401,P06-1055,0,0.0941097,"tained significantly lower results compared to English. It is notable that these models were applied without adding morphological signatures, using gold lemmas instead. Corazza et al. (2004) further tried different refinements including parent annotation and horizontal markovization, but none of them obtained the desired improvement. For French, Crabb´e and Candito (2008) and Seddah et al. (2010) show that, given a corpus comparable in size and properties (i.e. the number of tokens and grammar size), the performance level, both for Charniak’s parser (Charniak, 2000) and the Berke3 ley parser (Petrov et al., 2006) was higher for parsing the PTB than it was for French. The split-mergesmooth implementation of (Petrov et al., 2006) consistently outperform various lexicalized and unlexicalized models for French (Seddah et al., 2009) and for many other languages (Petrov and Klein, 2007). In this respect, (Petrov et al., 2006) is considered MRL-friendly, due to its language agnostic design. The rise of dependency parsing: It is commonly assumed that dependency structures are better suited for representing the syntactic structures of free word order, morphologically rich, languages, because this representatio"
W10-1401,D07-1066,1,0.534932,"Missing"
W10-1401,P81-1022,0,0.830446,"Missing"
W10-1401,W07-2219,1,0.909816,"Missing"
W10-1401,C08-1112,1,0.709478,"Missing"
W10-1401,W10-1405,1,0.846235,"Missing"
W10-1401,W09-3820,1,0.856092,"e other ones, too. 4.4 Estimation and Smoothing: Coping with Lexical Sparsity Morphological word form variation augments the vocabulary size and thus worsens the problem of lexical data sparseness. Words occurring with mediumfrequency receive less reliable estimates, and the number of rare/unknown words is increased. One way to cope with the one of both aspects of this problem is through clustering, that is, providing an abstract representation over word forms that reflects their shared morphological and morphosyntactic aspects. This was done, for instance, in previous work on parsing German. Versley and Rehbein (2009) cluster words according to linear context features. These clusters include valency information added to verbs and morphological features such as case and number added to pre-terminal nodes. The clusters are then integrated as features in a discriminative parsing model to cope with unknown words. Their discriminative model thus obtains state-of-the-art results on parsing German. 8 Several contribution address similar challenges. For constituency-based generative parsers, the simple technique of replacing word forms with more abstract symbols is investigated by (Seddah et al., 2010; Candito and"
W10-1401,W10-1411,0,\N,Missing
W10-1401,W10-1403,0,\N,Missing
W10-1401,W10-1410,1,\N,Missing
W10-1401,W08-1008,0,\N,Missing
W10-1401,P05-1022,0,\N,Missing
W10-1401,P08-1043,1,\N,Missing
W10-1401,D07-1096,0,\N,Missing
W11-3808,P05-1022,0,0.0229741,"n related work. Section 3 describes the setup of our experiments and reports preliminary results. In Section 4 we conclude and outline future work. 2 Related work The question whether or not self-training can be employed to improve parsing accuracy and to overcome sparse data problems has gained a lot of attention in recent years. While training a generative parsing model on its own output (Charniak, 1997; Steedman et al., 2003) does not seem to work well, McClosky et al. (2006a; 2006b) showed promising results when combining the self-training approach with a two-stage reranking parser model (Charniak and Johnson, 2005). This triggered a number of follow-up studies especially in the area of domain adaptation (Bacchiani et al., 2006; Foster et al., 2007; McClosky et al., 2010), where self-training is used to adapt the parser to a target domain for which no (or only a small amount of) annotated training data is available. (Reichart and Rappoport, 2007) are the first to report successful self-training using a generative parsing model only. They claim that the crucial difference to earlier studies is the size of the seed data and the number of parser output trees added to the training data. In their experiments"
W11-3808,W07-2204,0,0.029883,"Missing"
W11-3808,D09-1087,0,0.0303788,"6.75 67.86 67.30 72.54 72.11 72.32 72.79 72.41 72.60 73.06 72.71 72.88 73.59*** 73.10*** 73.34 73.44 . 72.74 73.09 74.08*** 73.55** 73.82 72.23 71.82 72.03 73.20 72.55 72.88 73.21 72.71 72.96 Table 1: Parsing results (PARSEVAL) for the different self-training settings, including GF in the evaluation (asterisks indicate significant differences between self-training and the baseline: p=0.001***, p=0.005**, p=0.01*, p=0.05 .) including erroneous parser output trees. So far, it is not clear to us why the lexicalised parser performs poorly in the self-training setting. This result is in line with (Huang and Harper, 2009), who observed that the PCFG-LA parser used in their experiments benefitted more from self-training as compared to a lexicalised generative parser. However, our results are not necessarily an effect of lexicalisation, but might be due to the overall lower accuracy of the Stanford parser on German (see K¨ubler (2008)). A quantiative and qualitative error analysis might give us some interesting insight into the underlying reasons and into the question when and why self-training will work for parsing. 4 Conclusions and future work We presented preliminary results on self-training experiments for"
W11-3808,P03-1054,0,0.00388017,"ded them to the TiGer training data (subsets 1-8). We re-trained the parser and evaluated against the TiGer test set, comparing the results against the perforquotes but does not distinguish between opening and closing double quotes. 3 The language models were produced and calculated using the CMU/Cambridge toolkit (http://mi.eng.cam.ac. uk/prc14/toolkit.html). mance of the parser when trained on the original subset from the TiGer treebank. 3.4 Parsing experiments For our experiments we use the unlexicalised Berkeley parser (Petrov et al., 2006) and the lexicalised form of the Stanford parser (Klein and Manning, 2003). The Berkeley parser is an unlexicalised latent variable PCFG parser which uses a split-andmerge technique to automatically refine the training data. The splits result in more and more finegrained subcategories, which are merged again if not proven useful. We train a PCFG from each of the 8 training subsets by carrying out six cycles of the split-and-merge process. The model is languageagnostic. The Stanford parser provides a factored probabilistic model combining a PCFG with a dependency model. We use the Stanford parser in its lexicalised, markovised form.4 Both parsers were trained on the"
W11-3808,W08-1008,0,0.109495,"Missing"
W11-3808,N06-1020,0,0.114342,"further improve results while avoiding the downside of expensive human annotation. The paper is structured as follows. Section 2 reports on related work. Section 3 describes the setup of our experiments and reports preliminary results. In Section 4 we conclude and outline future work. 2 Related work The question whether or not self-training can be employed to improve parsing accuracy and to overcome sparse data problems has gained a lot of attention in recent years. While training a generative parsing model on its own output (Charniak, 1997; Steedman et al., 2003) does not seem to work well, McClosky et al. (2006a; 2006b) showed promising results when combining the self-training approach with a two-stage reranking parser model (Charniak and Johnson, 2005). This triggered a number of follow-up studies especially in the area of domain adaptation (Bacchiani et al., 2006; Foster et al., 2007; McClosky et al., 2010), where self-training is used to adapt the parser to a target domain for which no (or only a small amount of) annotated training data is available. (Reichart and Rappoport, 2007) are the first to report successful self-training using a generative parsing model only. They claim that the crucial d"
W11-3808,P06-1043,0,0.0361057,"further improve results while avoiding the downside of expensive human annotation. The paper is structured as follows. Section 2 reports on related work. Section 3 describes the setup of our experiments and reports preliminary results. In Section 4 we conclude and outline future work. 2 Related work The question whether or not self-training can be employed to improve parsing accuracy and to overcome sparse data problems has gained a lot of attention in recent years. While training a generative parsing model on its own output (Charniak, 1997; Steedman et al., 2003) does not seem to work well, McClosky et al. (2006a; 2006b) showed promising results when combining the self-training approach with a two-stage reranking parser model (Charniak and Johnson, 2005). This triggered a number of follow-up studies especially in the area of domain adaptation (Bacchiani et al., 2006; Foster et al., 2007; McClosky et al., 2010), where self-training is used to adapt the parser to a target domain for which no (or only a small amount of) annotated training data is available. (Reichart and Rappoport, 2007) are the first to report successful self-training using a generative parsing model only. They claim that the crucial d"
W11-3808,C08-1071,0,0.107001,"Missing"
W11-3808,N10-1004,0,0.0122533,"he question whether or not self-training can be employed to improve parsing accuracy and to overcome sparse data problems has gained a lot of attention in recent years. While training a generative parsing model on its own output (Charniak, 1997; Steedman et al., 2003) does not seem to work well, McClosky et al. (2006a; 2006b) showed promising results when combining the self-training approach with a two-stage reranking parser model (Charniak and Johnson, 2005). This triggered a number of follow-up studies especially in the area of domain adaptation (Bacchiani et al., 2006; Foster et al., 2007; McClosky et al., 2010), where self-training is used to adapt the parser to a target domain for which no (or only a small amount of) annotated training data is available. (Reichart and Rappoport, 2007) are the first to report successful self-training using a generative parsing model only. They claim that the crucial difference to earlier studies is the size of the seed data and the number of parser output trees added to the training data. In their experiments they train a reimplementation of Collins’ parsing model 2 on a small seed set of trees (100-2000 trees) from the WSJ and add automatically parsed analyses for"
W11-3808,P06-1055,0,0.0168606,"ve TiGer training subset. Then we parsed the selected sentences and added them to the TiGer training data (subsets 1-8). We re-trained the parser and evaluated against the TiGer test set, comparing the results against the perforquotes but does not distinguish between opening and closing double quotes. 3 The language models were produced and calculated using the CMU/Cambridge toolkit (http://mi.eng.cam.ac. uk/prc14/toolkit.html). mance of the parser when trained on the original subset from the TiGer treebank. 3.4 Parsing experiments For our experiments we use the unlexicalised Berkeley parser (Petrov et al., 2006) and the lexicalised form of the Stanford parser (Klein and Manning, 2003). The Berkeley parser is an unlexicalised latent variable PCFG parser which uses a split-andmerge technique to automatically refine the training data. The splits result in more and more finegrained subcategories, which are merged again if not proven useful. We train a PCFG from each of the 8 training subsets by carrying out six cycles of the split-and-merge process. The model is languageagnostic. The Stanford parser provides a factored probabilistic model combining a PCFG with a dependency model. We use the Stanford pars"
W11-3808,P07-1078,0,0.0816571,"ile training a generative parsing model on its own output (Charniak, 1997; Steedman et al., 2003) does not seem to work well, McClosky et al. (2006a; 2006b) showed promising results when combining the self-training approach with a two-stage reranking parser model (Charniak and Johnson, 2005). This triggered a number of follow-up studies especially in the area of domain adaptation (Bacchiani et al., 2006; Foster et al., 2007; McClosky et al., 2010), where self-training is used to adapt the parser to a target domain for which no (or only a small amount of) annotated training data is available. (Reichart and Rappoport, 2007) are the first to report successful self-training using a generative parsing model only. They claim that the crucial difference to earlier studies is the size of the seed data and the number of parser output trees added to the training data. In their experiments they train a reimplementation of Collins’ parsing model 2 on a small seed set of trees (100-2000 trees) from the WSJ and add automatically parsed analyses for WSJ sections 2-21. Then they test their models on section 23 of the WSJ and report a substantial improvement for the in-domain self-training setting. 63 Discussion has focussed o"
W11-3808,C08-1098,0,0.0159935,"in the training set in subset 1, the first 10000 trees in subset 2, and so on, up to 40000 trees (subset 8). We resolved the crossing branches in the TiGer trees by attaching the nonhead child nodes higher up in the tree, following (K¨ubler, 2005). 3.3 Data point selection In the next step we created language models for each of the 8 TiGer training subsets on the basis of the part-of-speech trigrams3 and computed the perplexity for each sentence in the T¨uBa-D/Z treebank based on its part-of-speech trigrams. The T¨uBaD/Z POS tags used in our experiments have been assigned using the RFTagger (Schmid and Laws, 2008). For TiGer, we used the gold POS tags. Perplexity (Equation 1) is an informationtheoretic measure and can be used to assess the homogeneity of a corpus. It can be unpacked as the inverse of the corpus probability, normalised by corpus size. The perplexity of a sentence from the T¨uBa-D/Z tells us how similar this sentence is to the TiGer training data. s P P (W ) = N 1 P (w1 w2 ...wN ) (1) For each of the 8 subsets we selected the 25000 sentences from the T¨uBa-D/Z with the lowest perplexity, thus the T¨uBa-D/Z sentences most similar in structure to the respective TiGer training subset. Then"
W11-3808,P10-1111,1,0.860976,"Missing"
W11-3808,P11-2120,0,0.103414,"Missing"
W11-3808,E03-1008,0,0.305198,"y to the training data is a good strategy which can further improve results while avoiding the downside of expensive human annotation. The paper is structured as follows. Section 2 reports on related work. Section 3 describes the setup of our experiments and reports preliminary results. In Section 4 we conclude and outline future work. 2 Related work The question whether or not self-training can be employed to improve parsing accuracy and to overcome sparse data problems has gained a lot of attention in recent years. While training a generative parsing model on its own output (Charniak, 1997; Steedman et al., 2003) does not seem to work well, McClosky et al. (2006a; 2006b) showed promising results when combining the self-training approach with a two-stage reranking parser model (Charniak and Johnson, 2005). This triggered a number of follow-up studies especially in the area of domain adaptation (Bacchiani et al., 2006; Foster et al., 2007; McClosky et al., 2010), where self-training is used to adapt the parser to a target domain for which no (or only a small amount of) annotated training data is available. (Reichart and Rappoport, 2007) are the first to report successful self-training using a generative"
W11-3808,I08-3008,0,0.0391029,"Missing"
W12-3716,P98-1013,0,0.396438,"d Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1998) offering a novel representation for sentiment analysis based on frame semantics. 2 Shallow and pragmatic approaches Current approaches to sentiment analysis are mainly pragmatically oriented, without giving equal weight to semantics. One aspect concerns the identification of sentiment-bearing expressions. The annotations in the MPQA corpus (Wiebe et al., 2005), for instance, were created without limiting what annotators can annotate in terms of syntax or lexicon. While this serves the spirit of discovering the variety of opinion expressions in actual contexts, it makes it difficult to match o"
W12-3716,W11-1707,0,0.0253401,"ion for sentiment analysis that is tailored to these aims. 1 Introduction Sentiment analysis has made a lot of progress on more coarse-grained analysis levels using shallow techniques. However, recent years have seen a trend towards more fine-grained and ambitious analyses requiring more linguistic knowledge and more complex statistical models. Recent work has tried to produce relatively detailed summaries of opinions expressed in news texts (Stoyanov and Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1998) offering a novel representation for sentiment analysis based on frame semantics. 2 Shallow and pragmatic approaches Current approaches to senti"
W12-3716,R11-1060,0,0.0156682,"t, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims. 1 Introduction Sentiment analysis has made a lot of progress on more coarse-grained analysis levels using shallow techniques. However, recent years have seen a trend towards more fine-grained and ambitious analyses requiring more linguistic knowledge and more complex statistical models. Recent work has tried to produce relatively detailed summaries of opinions expressed in news texts (Stoyanov and Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1998) offering a novel representation for sentiment analysis based on frame semantics. 2 Shallow"
W12-3716,W06-0301,0,0.105245,"ons are tied to one expression and where presuppositions and temporal structure come into play. An example is the verb despoil: there is a positive opinion by the reporter about the despoiled entity in its former state, a negative opinion about its present state, and (inferrable) negative sentiment towards the despoiler. In most resources, the positive opinion will not be represented. The most common approach to the task is an information extraction-like pipeline. Expressions of opinion, sources and targets are often dealt with separately, possibly using separate resources. Some work such as (Kim and Hovy, 2006) has explored 105 the connection to role labeling. One reason not to pursue this is that “in many practical situations, the annotation beyond opinion holder labeling is too expensive” (Wiegand, 2010, p.121). (Shaikh et al., 2007) use semantic dependencies and composition rules for sentence-level sentiment scoring but do not deal with source and target extraction. The focus on robust partial solutions, however, prevents the creation of an integrated high-quality resource. 3 The extended frame-semantic approach We now sketch a view of sentiment analysis on the basis of an appropriately extended"
W12-3716,W11-1702,0,0.0123826,"as described above enables us to readily represent multiple opinions. For instance, the verb brag in the modified Bragging frame has two opinion frames. The first one has positive polarity and represents the frame-internal point of view. The S PEAKER is the Source relative to the T OPIC as the Target. The second opinion frame has negative polarity, representing the reporter’s point of view. The S PEAKER is the Target but the Source is unspecified, indicating that it needs to be resolved to an embedded source. For a similar representation of multiple opinions in a Dutch lexical resource, see (Maks and Vossen, 2011). Event structure and presuppositions A complete representation of subjectivity needs to include event and presuppositional structure. This is necessary, for instance, for predicates like come around (on) in (1), which involve changes of opinion relative to the same target by the same source. Without the possibility of distinguishing between attitudes held at different times, the sentiment associated with these predicates cannot be modeled adequately. (1) Newsom is still against extending weekday metering to evenings, but has COME AROUND on Sunday enforcement. For come around (on), we want to"
W12-3716,ruppenhofer-etal-2008-finding,1,0.879942,"igh-quality resource. 3 The extended frame-semantic approach We now sketch a view of sentiment analysis on the basis of an appropriately extended model of frame semantic representation.1 Link to semantic frames and roles Since the possible sources and targets of opinion are usually identical to a predicate’s semantic roles, we add opinion frames with slots for Source, Target, Polarity and Intensity to the FrameNet database. We map the Source and Target opinion roles to semantic roles as appropriate, which enables us to use semantic role labeling systems in the identification of opinion roles (Ruppenhofer et al., 2008). In SentiFrameNet all lexical units (LUs) that are inherently evaluative are associated with opinion frames. The language of polar facts is not associated with opinion frames. However, we show in the longer version of this paper (cf. footnote 1) how we support certain types of inferred sentiment. With regard to targets, our representation selects as targets of opinion the target spans of (Stoyanov and Cardie, 2008) rather than their opinion topics (see Section 2). For us, opinion topics that do not coincide with target spans are inferential opinion targets. Formal diversity of opinion express"
W12-3716,C08-1103,0,0.119217,"the subjectivity clues extracted are inherently evaluative or Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 104–109, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics merely associated with statements of polar fact. Pragmatic considerations also lead to certain expressions of sentiment or opinion being excluded from analysis. (Seki, 2007), for instance, annotated sentences as “not opinionated” if they contain indirect hearsay evidence or widely held opinions. In the case of targets, the work by (Stoyanov and Cardie, 2008) exhibits a pragmatic focus as well. These authors distinguish between (a) the topic of a fine-grained opinion, defined as the real-world object, event or abstract entity that is the subject of the opinion as intended by the opinion holder; (b) the topic span associated with an opinion expression is the closest, minimal span of text that mentions the topic; and (c) the target span defined as the span of text that covers the syntactic surface form comprising the contents of the opinion. As the definitions show, (Stoyanov and Cardie, 2008) focus on text-level, pragmatic relevance by paying atten"
W12-3716,R11-1028,0,0.0607944,"mbining ressources to create synergies with related work in NLP. In the paper, we propose SentiFrameNet, an extension to FrameNet, as a novel representation for sentiment analysis that is tailored to these aims. 1 Introduction Sentiment analysis has made a lot of progress on more coarse-grained analysis levels using shallow techniques. However, recent years have seen a trend towards more fine-grained and ambitious analyses requiring more linguistic knowledge and more complex statistical models. Recent work has tried to produce relatively detailed summaries of opinions expressed in news texts (Stoyanov and Cardie, 2011); to assess the impact of quotations from business leaders on stock prices (Drury et al., 2011); to detect implicit sentiment (Balahur et al., 2011); etc. Accordingly, we can expect that greater demands will be made on the amount of linguistic knowledge, its representation, and the evaluation of systems. Against this background, we argue that it is worthwhile to complement the existing shallow and pragmatic approaches with a deep, lexicalsemantics based one in order to enable deeper analysis. We report on ongoing work in constructing Sen104 tiFrameNet, an extension of FrameNet (Baker et al., 1"
W12-3716,P10-1059,0,0.0732426,"treated differently. A similar challenge lies in distinguishing so-called polar facts from inherently sentiment-bearing expressions. For example, out of context, one would not associate any of the words in the sentence Wages are high in Switzerland with a particular evaluative meaning. In specific contexts, however, we may take the sentence as reason to either think positively or negatively of Switzerland: employees receiving wages may be drawn to Switzerland, while employers paying wages may view this state of affairs negatively. As shown by the inter-annotator agreement results reported by (Toprak et al., 2010), agreement on distinguishing polar facts from inherently evaluative language is low. Unsurprisingly, many efforts at automatically building up sentiment lexica simply harvest expressions that frequently occur as part of polar facts without resolving whether the subjectivity clues extracted are inherently evaluative or Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 104–109, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics merely associated with statements of polar fact. Pragmatic considerations al"
W12-3716,P06-1134,0,0.0607214,"rdie, 2008) focus on text-level, pragmatic relevance by paying attention to what the author intends, rather than concentrating on the explicit syntactic dependent (their target span) as the topic. This pragmatic focus is also in evidence in (Wilson, 2008)’s work on contextual polarity classification, which uses features in the classification that are syntactically independent of the opinion expression such as the number of subjectivity clues in adjoining sentences. Among lexicon-driven approaches, we find that despite arguments that word sense distinctions are important to sentiment analysis (Wiebe and Mihalcea, 2006), often-used resources do not take them into account and new resources are still being created which operate on the more shallow lemma-level (e.g. (Neviarouskaya et al., 2009)). Further, most lexical resources do not adequately represent cases where multiple opinions are tied to one expression and where presuppositions and temporal structure come into play. An example is the verb despoil: there is a positive opinion by the reporter about the despoiled entity in its former state, a negative opinion about its present state, and (inferrable) negative sentiment towards the despoiler. In most resou"
W12-3716,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W12-3716,C98-1013,0,\N,Missing
W14-4903,H92-1022,0,0.542398,"Missing"
W14-4903,J92-4003,0,0.34764,"Missing"
W14-4903,E03-1068,0,0.459196,"Missing"
W14-4903,E06-1034,0,0.055307,"Missing"
W14-4903,W11-0408,0,0.0472373,"Missing"
W14-4903,C02-1021,0,0.782384,"Missing"
W14-4903,E09-1060,0,0.252436,"Missing"
W14-4903,petrov-etal-2012-universal,0,0.060714,"Missing"
W14-4903,rehbein-etal-2014-kiezdeutsch,1,0.77785,"d recall for our system. Precision is computed as the number of correctly identified error candidates, divided by the number of all (correctly and incorrectly identified) error candidates (number of true positives / (number of true positives + false positives)), and recall by dividing the number of identified errors by the total number of errors in the data (true positives / (true positives + false negatives)). 4 Experimental Setup The data we use in our experiments comes from two sources, i) from a corpus of informal, spoken German youth language (The KiezDeutsch Korpus (KiDKo) Release 1.0) (Rehbein et al., 2014), and ii) from the TIGER corpus (Brants et al., 2002), a German newspaper corpus. 4.1 Kiezdeutsch – Informal youth language KiDKo is a new language resource including informal, spontaneous dialogues from peer-to-peer communication of adolescents. The current version of the corpus includes the audio signals aligned with transcriptions, as well as a normalisation layer and POS annotations. Additional annotation layers (Chunking, Topological Fields) are in progress. The transcription scheme has an orthographic basis but, in order to enable investigations of prosodic characteristics of the data, i"
W14-4903,W00-1308,0,0.331656,"Missing"
W14-4903,W00-1907,0,0.68645,"Missing"
W14-4903,W10-1838,0,\N,Missing
W15-1302,N01-1016,0,0.0609442,"3; Goffman, 1981; Levelt, 1983; Clark, 1996; (1) (2) (3) (4) (5) (6) I will uh I will come tomorrow. I will leave on Sat uh on Sunday. I think I uh have you seen my wallet? I have met Sarah and Peter and uhm Lara. Sarah is Michael’s sister. Uh? Really? A: He cheated on her. B: Ugh! That’s bad! The role of fillers in spoken language has been discussed in the literature (for an overview, see Corley and Stewart (2008)). Despite this, work on processing disfluencies in NLP has mostly considered them as mere performance phenomena and focused on disfluency detection to improve automatic processing (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Qian and Liu, 2013; Rasooli and 12 Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics (ExProM 2015), pages 12–21, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Tetreault, 2013; Rasooli and Tetreault, 2014). Far fewer studies have focused on the information that disfluencies contribute to the overall meaning of the utterance. An exception are Womack et al. (2012) who consider disfluencies as extra-propositional indicators of cognitive processing. In this paper, we take a similar"
W15-1302,P04-1005,0,0.0766804,"83; Clark, 1996; (1) (2) (3) (4) (5) (6) I will uh I will come tomorrow. I will leave on Sat uh on Sunday. I think I uh have you seen my wallet? I have met Sarah and Peter and uhm Lara. Sarah is Michael’s sister. Uh? Really? A: He cheated on her. B: Ugh! That’s bad! The role of fillers in spoken language has been discussed in the literature (for an overview, see Corley and Stewart (2008)). Despite this, work on processing disfluencies in NLP has mostly considered them as mere performance phenomena and focused on disfluency detection to improve automatic processing (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Qian and Liu, 2013; Rasooli and 12 Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics (ExProM 2015), pages 12–21, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Tetreault, 2013; Rasooli and Tetreault, 2014). Far fewer studies have focused on the information that disfluencies contribute to the overall meaning of the utterance. An exception are Womack et al. (2012) who consider disfluencies as extra-propositional indicators of cognitive processing. In this paper, we take a similar stand and present a study t"
W15-1302,N13-1102,0,0.0122766,"(4) (5) (6) I will uh I will come tomorrow. I will leave on Sat uh on Sunday. I think I uh have you seen my wallet? I have met Sarah and Peter and uhm Lara. Sarah is Michael’s sister. Uh? Really? A: He cheated on her. B: Ugh! That’s bad! The role of fillers in spoken language has been discussed in the literature (for an overview, see Corley and Stewart (2008)). Despite this, work on processing disfluencies in NLP has mostly considered them as mere performance phenomena and focused on disfluency detection to improve automatic processing (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Qian and Liu, 2013; Rasooli and 12 Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics (ExProM 2015), pages 12–21, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Tetreault, 2013; Rasooli and Tetreault, 2014). Far fewer studies have focused on the information that disfluencies contribute to the overall meaning of the utterance. An exception are Womack et al. (2012) who consider disfluencies as extra-propositional indicators of cognitive processing. In this paper, we take a similar stand and present a study that investigates the"
W15-1302,D13-1013,0,0.0261998,"Missing"
W15-1302,E14-4010,0,0.0139393,"ken language has been discussed in the literature (for an overview, see Corley and Stewart (2008)). Despite this, work on processing disfluencies in NLP has mostly considered them as mere performance phenomena and focused on disfluency detection to improve automatic processing (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Qian and Liu, 2013; Rasooli and 12 Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics (ExProM 2015), pages 12–21, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Tetreault, 2013; Rasooli and Tetreault, 2014). Far fewer studies have focused on the information that disfluencies contribute to the overall meaning of the utterance. An exception are Womack et al. (2012) who consider disfluencies as extra-propositional indicators of cognitive processing. In this paper, we take a similar stand and present a study that investigates the use of filled pauses in informal spoken German youth language and in written, but conceptually oral text from social media, namely Twitter microblogs.1 We compare the use of FP in computer-mediated communication (CMC) to that in spoken language, and present quantitative and"
W15-1302,rehbein-etal-2014-kiezdeutsch,1,0.828093,"s of self-recordings of every-day conversations between adolescents from urban areas. All informants are native speakers of German. The corpus contains spontaneous, highly informal peer group dialogues of adolescents from multiethnic BerlinKreuzberg (around 266,000 tokens excluding punctuation) and a supplementary corpus with adolescent speakers from monoethnic Berlin-Hellersdorf (around 111,000 tokens). On the normalisation layer where punctuation is included, the token counts add up to around 359,000 tokens (main corpus) and 149,000 tokens (supplementary corpus). The first release of KiDKo (Rehbein et al., 2014) includes the transcriptions (aligned with the audio files), a normalisation layer, and a layer with partof-speech (POS) annotations as well as non-verbal descriptions and the translation of Turkish codeswitching. The data was transcribed using an adapted version of the transcription inventory GAT 2 (Selting et al., 1998), also called GAT minimal transcript, which uses uppercased letters to encode the primary accent and hyphens in round brackets to mark silent pauses of varying length. The microblogging data consists of Germanlanguage Twitter messages from different regions 4 The maximum lengt"
W15-1302,E12-2021,0,0.0292864,"Missing"
W15-1302,W12-3801,0,0.143464,"sidered them as mere performance phenomena and focused on disfluency detection to improve automatic processing (Charniak and Johnson, 2001; Johnson and Charniak, 2004; Qian and Liu, 2013; Rasooli and 12 Proceedings of the Second Workshop on Extra-Propositional Aspects of Meaning in Computational Semantics (ExProM 2015), pages 12–21, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics Tetreault, 2013; Rasooli and Tetreault, 2014). Far fewer studies have focused on the information that disfluencies contribute to the overall meaning of the utterance. An exception are Womack et al. (2012) who consider disfluencies as extra-propositional indicators of cognitive processing. In this paper, we take a similar stand and present a study that investigates the use of filled pauses in informal spoken German youth language and in written, but conceptually oral text from social media, namely Twitter microblogs.1 We compare the use of FP in computer-mediated communication (CMC) to that in spoken language, and present quantitative and qualitative results from a corpus study showing similarities as well as differences between FP in both the spoken and written register. Based on our findings,"
W17-0813,W14-0703,0,0.291055,"Missing"
W17-0813,P07-1003,0,0.0095194,"e world. We, instead, are also interested in relations that are interpreted as causal by humans, even if they are not strictly expressed as causal by a lexical marker, such as temporal relations or speech-act causality. (7) nsubj Gentrification N1: NOUN Gentrifizierung SB social problems VERB ADP ADJ N2: NOUN ¨ fuhrt zu sozialen Problemen MO NK Figure 1: Parallel tree with English cause and aligned German noun pair And if you want to say no, say noEffect ’Cause there’s a million ways to goCause M OTIVATION and the RBG parser (Lei et al., 2014) for German. We then applied the Berkeley Aligner (DeNero and Klein, 2007) to obtain word alignments for all aligned sentences. This allows us to map the dependency trees onto each other and to project (most of) the tokens from English to German and vice versa.3 4.2 Method Step 1 First, we select all sentences in the corpus that contain a form of the English verb cause. We then restrict our set of candidates to instances of cause where both the subject and the direct object are realised as nouns, as illustrated in example (8). Knowledge-lean extraction of causal relations and their participants (8) Alcoholnsubj causes 17 000 needless deathsdobj on the road a year. S"
W17-0813,W15-1622,0,0.271817,"h as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. The problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011). Some annotation efforts abstain altogether from reporting inter-annotator agreement at all. A notable exception is Dunietz et al. (2015) who take a lexical approach and aim at building a constructicon for English causal language. By constructicon they mean “a list of English constructions that conventionally express causality” (Dunietz et al., 2015). They show that their approach dramatically increases agreement between the annotators and thus the quality of the annotations (for details see section 2). We adapt their approach of framing the annotation task as a lexicon creation process and present first steps towards buildIn this paper, we present a simple, yet effective method for the automatic identification and extraction o"
W17-0813,W03-1210,0,0.258305,", and b) annotation studies that discuss the description and disambiguation of causal phenomena in natural language. As we are still in the process of building our resource and collecting training data, we will for now set aside work on automatic classification of causality such as (Mirza and Tonelli, 2014; Dunietz et al., In press) as well as the rich literature on shallow discourse parsing, and focus on annotation and identification of causal phenomena. Early work on identification and extraction of causal relations from text heavily relied on knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003). Girju (2003) identifies instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN1 causes bonynessN2 . Like Versley (2010), most work on identifying causal language for German has been focusing on discourse connectives. Stede et al. (1998; 2002) have developed a lexicon of German discourse markers that has been augmented with semantic relations (Scheffler and Stede, 2016). Another resource for German is the T¨uBa-D/Z that includes annotations for selected discourse connectives, with a small number of causal connectives (Gastel et al., 2011). B¨ogel et al. (2014) pr"
W17-0813,grivaz-2010-human,0,0.223503,"s Dynamic Force Model which provides a framework that tries to distinguish weak and strong causal forces, and captures different types of causality such as “letting”, “hindering”, “helping” or “intending”. While each of these theories manages to explain some aspects of causality, none of them seems to provide a completely satisfying account of the phenomenon under consideration. The problem of capturing and specifying the concept of causality is also reflected in linguistic annotation efforts. Human annotators often show only a moderate or even poor agreement when annotating causal phenomena (Grivaz, 2010; Gastel et al., 2011). Some annotation efforts abstain altogether from reporting inter-annotator agreement at all. A notable exception is Dunietz et al. (2015) who take a lexical approach and aim at building a constructicon for English causal language. By constructicon they mean “a list of English constructions that conventionally express causality” (Dunietz et al., 2015). They show that their approach dramatically increases agreement between the annotators and thus the quality of the annotations (for details see section 2). We adapt their approach of framing the annotation task as a lexicon"
W17-0813,P16-1135,0,0.112993,"English verb cause as a seed to identify transitive causal verbs. In contrast to Girju’s WordNet-based approach, we use parallel data and project the English tokens to their German counterparts. The contributions of this paper are as follows. 1. We present a bootstrapping method to identify and extract causal relations and their participants from text, based on parallel corpora. 2. We present the first version of a German causal constructicon, containing 100 entries for causal verbal expressions. Ours is not the first work that exploits parallel or comparable corpora for causality detection. Hidey and McKeown (2016) work with monolingual comparable corpora, English Wikipedia and simple Wikipedia. They use explicit discourse connectives from the PDTB (Prasad et al., 2008) as seed data and identify alternative lexicalizations for causal discourse relations. Versley (2010) classifies German explicit discourse relations without German training data, solely based on the English annotations projected to German via word-aligned parallel text. He also presents a bootstrapping approach for a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. 3. We provide over"
W17-0813,L16-1160,0,0.0617345,"rsing, and focus on annotation and identification of causal phenomena. Early work on identification and extraction of causal relations from text heavily relied on knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003). Girju (2003) identifies instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN1 causes bonynessN2 . Like Versley (2010), most work on identifying causal language for German has been focusing on discourse connectives. Stede et al. (1998; 2002) have developed a lexicon of German discourse markers that has been augmented with semantic relations (Scheffler and Stede, 2016). Another resource for German is the T¨uBa-D/Z that includes annotations for selected discourse connectives, with a small number of causal connectives (Gastel et al., 2011). B¨ogel et al. (2014) present a rule-based system for identifying eight causal German connectors in spoken multilogs, and the causal relations R EASON , R ESULT expressed by them. To the best of our knowledge, ours is the first effort to describe causality in German on a broader scale, not limited to discourse connectives. 106 3 Annotation Scheme (1) Elektromagnetische FelderCause Electromagnetic fields k¨onnen KrebsEffect"
W17-0813,P98-2202,0,0.415512,"Missing"
W17-0813,2005.mtsummit-papers.11,0,0.114294,"t incorporate the result (e.g. death) or means (e.g. talk) of causation as part of their meaning. Again, we follow Dunietz et al. and also exclude such cases from our lexicon. In this work, we focus on verbal triggers of causality. Due to our extraction method (section 4), we are mostly dealing with verbal triggers that are instances of the type C ONSEQUENCE. Therefore we cannot say much about the applicability of the different annotation types at this point but will leave this to future work. 4 amod Data The data we use in our experiments come from the English-German part of Europarl corpus (Koehn, 2005). The corpus is aligned on the sentence-level and contains more than 1,9 mio. English-German parallel sentences. We tokenised and parsed the text to obtain dependency trees, using the Stanford parser (Chen and Manning, 2014) for English 3 Some tokens did not receive an alignment and are thus ignored in our experiments. 108 Data: Europarl (En-Ge) Input: seed word: cause (En) Output: list of causal triggers (Ge) S TEP 1: if seed in sentence then if cause linked to subj, dobj (En) then if subj, dobj == noun then if subj, dobj aligned with nouns (Ge) then extract noun pair (Ge); end end end end S"
W17-0813,P14-1130,0,0.0195583,"(2015) who only deal with causal language, not with causality in the world. We, instead, are also interested in relations that are interpreted as causal by humans, even if they are not strictly expressed as causal by a lexical marker, such as temporal relations or speech-act causality. (7) nsubj Gentrification N1: NOUN Gentrifizierung SB social problems VERB ADP ADJ N2: NOUN ¨ fuhrt zu sozialen Problemen MO NK Figure 1: Parallel tree with English cause and aligned German noun pair And if you want to say no, say noEffect ’Cause there’s a million ways to goCause M OTIVATION and the RBG parser (Lei et al., 2014) for German. We then applied the Berkeley Aligner (DeNero and Klein, 2007) to obtain word alignments for all aligned sentences. This allows us to map the dependency trees onto each other and to project (most of) the tokens from English to German and vice versa.3 4.2 Method Step 1 First, we select all sentences in the corpus that contain a form of the English verb cause. We then restrict our set of candidates to instances of cause where both the subject and the direct object are realised as nouns, as illustrated in example (8). Knowledge-lean extraction of causal relations and their participant"
W17-0813,C14-1198,0,0.0225917,"ion 3, we describe our annotation scheme and the data we use in our experiments. Sections 4, 5 and 6 present our approach and the results, and we conclude and outline future work in section 7. 2 Related Work Two strands of research are relevant to our work, a) work on automatic detection of causal relations in text, and b) annotation studies that discuss the description and disambiguation of causal phenomena in natural language. As we are still in the process of building our resource and collecting training data, we will for now set aside work on automatic classification of causality such as (Mirza and Tonelli, 2014; Dunietz et al., In press) as well as the rich literature on shallow discourse parsing, and focus on annotation and identification of causal phenomena. Early work on identification and extraction of causal relations from text heavily relied on knowledge bases (Kaplan and Berry-Rogghe, 1991; Girju, 2003). Girju (2003) identifies instances of noun-verb-noun causal relations in WordNet glosses, such as starvationN1 causes bonynessN2 . Like Versley (2010), most work on identifying causal language for German has been focusing on discourse connectives. Stede et al. (1998; 2002) have developed a lex"
W17-0813,prasad-etal-2008-penn,0,0.118477,"ens to their German counterparts. The contributions of this paper are as follows. 1. We present a bootstrapping method to identify and extract causal relations and their participants from text, based on parallel corpora. 2. We present the first version of a German causal constructicon, containing 100 entries for causal verbal expressions. Ours is not the first work that exploits parallel or comparable corpora for causality detection. Hidey and McKeown (2016) work with monolingual comparable corpora, English Wikipedia and simple Wikipedia. They use explicit discourse connectives from the PDTB (Prasad et al., 2008) as seed data and identify alternative lexicalizations for causal discourse relations. Versley (2010) classifies German explicit discourse relations without German training data, solely based on the English annotations projected to German via word-aligned parallel text. He also presents a bootstrapping approach for a connective dictionary that relies on distribution-based heuristics on word-aligned German-English text. 3. We provide over 1,000 annotated causal instances (and growing) for the lexical triggers, augmented by a set of negative instances to be used as training data. The remainder o"
W17-0813,L16-1603,0,0.103989,"Missing"
W17-0813,C98-2197,0,\N,Missing
W17-0813,D14-1082,0,\N,Missing
W17-4117,W13-3520,0,0.0316136,"e not per se a problem for parsing, as long as we are able to learn something about their morphological properties. Table 5: Perplexity for different language models on German texts from Wikipedia. In our experiment, we use the framework1 and setup described in Vania and Lopez (2017) to build a language model for German texts. The framework includes implementations for word and subword-based (morpheme, character or character n-gram) embeddings and uses either bidirectional LSTMs or addition as the combination function of subwords. The German data sets are from the preprocessed Wikipedia data (Al-Rfou et al., 2013). Hyperlinks have been removed and the input texts have been lower-cased before learning the word- and compound-based embeddings. For the characterbased embeddings, the upper-cased letters have been preserved. We split the data into training, development and test sets, with approximately 1.2M, 150K and 150K tokens, respectively. For training and evaluation we closely follow Vania and Lopez (2017). We report results for three language models. The word model and the character model (using a bidirectional LSTM as composition function)2 are already implemented in the framework. For the compound em"
W17-4117,D15-1041,0,0.0439121,"Missing"
W17-4117,N16-1155,0,0.0270466,"Missing"
W17-4117,W16-1603,0,0.0378133,"Missing"
W17-4117,W16-2512,0,0.0400817,"Missing"
W17-4117,D14-1082,0,0.0287546,"Missing"
W17-4117,D15-1176,0,0.052651,"Missing"
W17-4117,N15-1140,0,0.0461573,"Missing"
W17-4117,D15-1188,0,0.0694282,"Missing"
W17-4117,W13-3512,0,0.11745,"Missing"
W17-4117,W15-0122,0,0.0694294,"Missing"
W17-4117,J93-2004,0,0.0604779,"Missing"
W17-4117,P16-2067,0,0.0230562,"Missing"
W17-4117,W14-6111,0,0.0249988,"Missing"
W17-4117,W13-3204,0,0.0610145,"Missing"
W17-4117,P17-1184,0,0.0147119,"ord but, crucially, morphological information. This was confirmed by the improved results for using character-based embeddings instead of the compound-based ones, where we were able to make up for the decrease in LAS that resulted from removing POS information from the input. Our results are important, as they show that unknown words are not per se a problem for parsing, as long as we are able to learn something about their morphological properties. Table 5: Perplexity for different language models on German texts from Wikipedia. In our experiment, we use the framework1 and setup described in Vania and Lopez (2017) to build a language model for German texts. The framework includes implementations for word and subword-based (morpheme, character or character n-gram) embeddings and uses either bidirectional LSTMs or addition as the combination function of subwords. The German data sets are from the preprocessed Wikipedia data (Al-Rfou et al., 2013). Hyperlinks have been removed and the input texts have been lower-cased before learning the word- and compound-based embeddings. For the characterbased embeddings, the upper-cased letters have been preserved. We split the data into training, development and test"
W17-4117,weller-heid-2012-analyzing,0,0.0208002,"he weight and bias vectors. We now outline our compositional model for compound embeddings. We assume that most compounds have a transparent meaning that can be inferred from the meaning of its components and hypothesize that providing the parser with subword embeddings that combine the representations of the individual components will help the model to handle unseen compounds. To that end, we first split each compound into lexemes and then combine the sequence of lexemes as we did for the character-based embeddings, using a bidirectional LSTM. For compound splitting, we use the IMS splitter (Weller and Heid, 2012) which adopts a frequency-based approach with additional linguistic features. The input information for the splitter (frequencies, POS and lemmas) was extracted from SdeWac (Faaß and Eckart, 2013), a cleanedup version of the deWac corpus (Baroni et al., 2009) with automatic POS tags and lemmas. 4 Experiments 4.1 Parsing Model Our parsing model is an extension of the headselection parser of Zhang et al. (2017) (figure 1). Given the sentence S = (w0 , w1 , ..., wN ) and xi as the input representation of word wi , the model 118 animals comrades comrades ROOT hROOT hAll hanimals hare hcomrades biL"
W17-4117,E17-1063,0,0.0346326,"ompound into lexemes and then combine the sequence of lexemes as we did for the character-based embeddings, using a bidirectional LSTM. For compound splitting, we use the IMS splitter (Weller and Heid, 2012) which adopts a frequency-based approach with additional linguistic features. The input information for the splitter (frequencies, POS and lemmas) was extracted from SdeWac (Faaß and Eckart, 2013), a cleanedup version of the deWac corpus (Baroni et al., 2009) with automatic POS tags and lemmas. 4 Experiments 4.1 Parsing Model Our parsing model is an extension of the headselection parser of Zhang et al. (2017) (figure 1). Given the sentence S = (w0 , w1 , ..., wN ) and xi as the input representation of word wi , the model 118 animals comrades comrades ROOT hROOT hAll hanimals hare hcomrades biLSTM biLSTM biLSTM biLSTM biLSTM Module Word emb. POS emb. Character-based emb. Compound emb. BiLSTM Regularization Optimization xROOT xAll xanimals xare xcomrades Others Figure 1: The parsing as head selection model (4) B hB i = LSTMB (xi , hi+1 ) (5) hi = [hFi ; hB i ] 4.2 (6) Word Embeddings (+word) Each word w in the lexicon is represented as a vector ew in the lookup table. We do not use any pre-trained e"
W17-4907,bird-etal-2008-acl,0,0.0321287,"o that of Stamatatos (2017). We also obscure words that occur below a certain frequency threshold. In contrast to Stamatatos, however, we use a CNN to classify the texts. We test our approach in a more realistic setting where the author has to be chosen from a much larger set of candidates (&gt;800). To disentangle the influence of topic and genre from author style, we test our method on a highly homogeneous set of scientific articles from the areas of computational linguistics and NLP. 3 Datasets and Tools In our experiments, we used single-author papers from the ACL Anthology Reference Corpus (Bird et al., 2008). The corpus contains scientific papers published in the proceedings of various conferences and workshops in the areas of computational linguistics and natural language processing. The earliest data is from 1965, the latest data is from 2007. We designated all papers published in the year 2006 as development data and all papers published in 2007 as test data, with the remaining data used for training. New authors without publications before this date were not treated any differently from those which were represented in the training data. We only retained publications from authors with at least"
W17-4907,E17-1107,0,0.122039,"related tasks, e.g. for Native Language Identification (NLI). As shown by Brooke and Hirst (2011), the topic of a document can often bias classification results in an NLI task, even when abstracting away from the context words by using character ngrams. Golcher and Reznicek (2011) reported a similar effect, showing how topic works as a confounding variable when investigating L1 influences in learner language. To assess the real potential of authorship attribution techniques, we need methods that are able to generalize to unseen data, and that are robust against the impact of topic and genre. Stamatatos (2017) addresses the problem of topic-sensitivity using text distortion. Before extracting token or character ngram features, he masks all tokens that occur below a certain frequency threshold by replacing either the whole token or each character in the token by an asterisk. He tests his approach in an authorship attribution task on texts from different topics and genres (&lt;15 authors), and in an author verification task on data from the PAN 2014 evaluation campaign (Stamatatos et al., 2014). Stamatatos shows that SVMs trained on the features extracted from the distorted texts outperform previous mod"
W17-4907,N03-1033,0,0.0302853,"f POS-tags. We used 100 convolution filters of length 1, 2 and 3 words each and a batch size of 20 sentences. Like that of Kim (2014), our fully connected layer was trained with dropout. The dropout rate was set to 0.5 during training. The network scans the entire input text of a segment using a sliding-window approach before applying max-pooling over time and making a prediction of authorship based on the prediction of the softmax layer. We tested the following frequencycutoff settings: Table 1: Corpus statistics for the ACL Anthology dataset. For POS-tagging, we used the Stanford POStagger (Toutanova et al., 2003).4 In addition to POS-tags, we use the pre-trained word embeddings available from Google5 trained using the skip-gram objective (Mikolov et al., 2013) as input features for our convolutional neural network. Word frequencies were computed on the News Commentary and News Discussions English datasets provided by the WMT15 workshop.6 4 Experiments 1. Retain only the 1,000 most frequent words in our large, out-of-domain corpus of English, use their word embeddings as input features alongside a one-hot encoding of their POStags as predicted by the Stanford POS-tagger. Replace all other words with an"
W17-4907,D14-1181,0,0.0178294,"rmed on the level of these segments. Table 1 gives an overview of corpus statistics. Training Development Test Publications Segments 1583 210 117 5360 620 323 padded concatenation of word embedding vectors and POS-tag one-hot encodings. The network then applies a single layer of convolving filters with varying window sizes, and a max-overtime pooling layer which retains only the maximum value. The resulting features are passed to a fully-connected softmax layer to obtain a probability distribution over labels. Figure 1 gives an overview of the model architecture. We used the implementation of Kim (2014),7 which we modified in a number of ways. We used static channels only and did not modify the pre-trained word embeddings. Our input feature map contained not only the 300-dimensional word embeddings, but also a one-hot representation of POS-tags. We used 100 convolution filters of length 1, 2 and 3 words each and a batch size of 20 sentences. Like that of Kim (2014), our fully connected layer was trained with dropout. The dropout rate was set to 0.5 during training. The network scans the entire input text of a segment using a sliding-window approach before applying max-pooling over time and m"
W17-4907,E17-2106,0,0.0742317,"tures extracted from the distorted texts outperform previous models in a cross-topic scenario. For topic-specific settings, however, where each author is strongly correlated with a specific topic, his approach yields results below the baseline.2 So far, only few studies have employed deep neural networks (NN) for authorship attribution. Ge et al. (2016) used a feed-forward NN language model to classify short transcripts from 18 coursera lectures that are controlled for topic. Rhodes (2015) trained a convolutional neural network (CNN) on word representations to classify medium-sized texts, and Shrestha et al. (2017) applied a CNN to identify the authors of tweets, based on character ngrams. Bagnall (2015) used a multi-headed recurrent neural network (RNN) language model to estimate character probabilities for each author in the PAN 2015 authorship identification task and outperformed all other models. Their results show the promise of deep NN for improving authorship attribution. Our approach is similar in spirit to that of Stamatatos (2017). We also obscure words that occur below a certain frequency threshold. In contrast to Stamatatos, however, we use a CNN to classify the texts. We test our approach i"
W17-6318,W14-6110,0,0.0642389,"Missing"
W17-6318,A00-2031,0,0.0523701,"man, we augment the labelling component of a neural dependency parser with a decision history. We present different ways to encode the history, using different LSTM architectures, and show that our models yield significant improvements, resulting in a LAS for German that is close to the best result from the SPMRL 2014 shared task (without the reranker). 1 2 Related Work Grammatical function labelling is commonly integrated into syntactic parsing. Few studies have adressed the issue as a separate classification task. While most of them assign grammatical functions on top of constituency trees (Blaheta and Charniak, 2000; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006; Klenner, 2007; Seeker et al., 2010), less work has tried to predict GF labels for unlabelled dependency trees. One of them is McDonald et al. (2006) who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task. Another approach has been proposed by Zhang et al. (2017) who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input. Then, in a post-pro"
W17-6318,W06-2920,0,0.0149582,"context, we follow Zhang et al. (2017) and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German. For English, we use the Penn Treebank (PTB) (Marcus et al., 1993) with standard training/dev/test splits. The POS tags are assigned using the Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De Marneffe et al., 2006). The German and Czech data come from the CoNLL-X shared task (Buchholz and Marsi, 2006) and our data split follows Zhang et al. (2017). As the CoNLL-X testsets are rather small (∼ 360 sentences), we also 130 Model UAS Baseline BI LSTM( L ) BI LSTM( B ) TREE LSTM D EN S E en 93.35 91.58 91.92* 91.91* 91.92* 91.90 cs 89.70 83.42 84.08* 83.80 83.82 81.72 deCoN LL 93.09 90.22 90.87* 90.97* 90.89* 89.60 deSP M RL 91.29 88.15 88.73* 88.74* 88.74* - deSP M RL baseline BI LSTM( L ) BI LSTM( B ) treeLSTM deSP M RL Table 1: Results for different labellers applied to the unlabelled parser output. The first row reports UAS for the input to the labellers. The last row (D EN S E) shows the re"
W17-6318,P06-2018,0,0.0802546,"Missing"
W17-6318,de-marneffe-etal-2006-generating,0,0.0167756,"Missing"
W17-6318,Q16-1025,0,0.0264315,"Missing"
W17-6318,P04-1040,0,0.132313,"Missing"
W17-6318,J93-2004,0,0.0574203,"two hidden layers (baseline), two bidirectional LSTM models (BI LSTM( L ) and BI LSTM( B )), and one tree LSTM model (TREE LSTM) (§3). The hidden layer dimension in all LSTM models was set to 200. The models were trained for 10 epochs, and were optimized using Adam Our interest is focussed on German, but to put our work in context, we follow Zhang et al. (2017) and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German. For English, we use the Penn Treebank (PTB) (Marcus et al., 1993) with standard training/dev/test splits. The POS tags are assigned using the Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De Marneffe et al., 2006). The German and Czech data come from the CoNLL-X shared task (Buchholz and Marsi, 2006) and our data split follows Zhang et al. (2017). As the CoNLL-X testsets are rather small (∼ 360 sentences), we also 130 Model UAS Baseline BI LSTM( L ) BI LSTM( B ) TREE LSTM D EN S E en 93.35 91.58 91.92* 91.91* 91.92* 91.90 cs 89.70 83.42 84.08* 83.80 83.82 81.72 deC"
W17-6318,W06-2932,0,0.297092,"ificant improvements, resulting in a LAS for German that is close to the best result from the SPMRL 2014 shared task (without the reranker). 1 2 Related Work Grammatical function labelling is commonly integrated into syntactic parsing. Few studies have adressed the issue as a separate classification task. While most of them assign grammatical functions on top of constituency trees (Blaheta and Charniak, 2000; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006; Klenner, 2007; Seeker et al., 2010), less work has tried to predict GF labels for unlabelled dependency trees. One of them is McDonald et al. (2006) who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task. Another approach has been proposed by Zhang et al. (2017) who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input. Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network. Introduction For languages with a non-configurational word order and rich(er) morphology, such as German, gramma"
W17-6318,W14-6111,0,0.0292127,"Missing"
W17-6318,P10-1111,1,0.891186,"Missing"
W17-6318,N03-1033,0,0.123881,"TREE LSTM) (§3). The hidden layer dimension in all LSTM models was set to 200. The models were trained for 10 epochs, and were optimized using Adam Our interest is focussed on German, but to put our work in context, we follow Zhang et al. (2017) and report results also for English, which has a configurational word order, and for Czech, which has a free word order, rich morphology, and less ambiguity in the case paradigm than German. For English, we use the Penn Treebank (PTB) (Marcus et al., 1993) with standard training/dev/test splits. The POS tags are assigned using the Stanford POS tagger (Toutanova et al., 2003) with ten-way jackknifing, and constituency trees are converted to Stanford basic dependencies (De Marneffe et al., 2006). The German and Czech data come from the CoNLL-X shared task (Buchholz and Marsi, 2006) and our data split follows Zhang et al. (2017). As the CoNLL-X testsets are rather small (∼ 360 sentences), we also 130 Model UAS Baseline BI LSTM( L ) BI LSTM( B ) TREE LSTM D EN S E en 93.35 91.58 91.92* 91.91* 91.92* 91.90 cs 89.70 83.42 84.08* 83.80 83.82 81.72 deCoN LL 93.09 90.22 90.87* 90.97* 90.89* 89.60 deSP M RL 91.29 88.15 88.73* 88.74* 88.74* - deSP M RL baseline BI LSTM( L )"
W17-6318,E17-1063,0,0.114052,"d into syntactic parsing. Few studies have adressed the issue as a separate classification task. While most of them assign grammatical functions on top of constituency trees (Blaheta and Charniak, 2000; Jijkoun and de Rijke, 2004; Chrupała and van Genabith, 2006; Klenner, 2007; Seeker et al., 2010), less work has tried to predict GF labels for unlabelled dependency trees. One of them is McDonald et al. (2006) who first generate the unlabelled trees using a graph-based parser, and then model the assignment of dependency labels as a sequence labelling task. Another approach has been proposed by Zhang et al. (2017) who present a simple, yet efficient and accurate parsing model that generates unlabelled trees by identifying the most probable head for each token in the input. Then, in a post-processing step, they assign labels to each head-dependent pair, using a two-layer rectifier network. Introduction For languages with a non-configurational word order and rich(er) morphology, such as German, grammatical function (GF) labels are essential for interpreting the meaning of a sentence. Case syncretism in the German case paradigm makes GF labelling a challenging task. See (1) for an example where the nouns"
W17-6318,N16-1035,0,0.0177696,"ROOT of, future chamber, music music, of ?, future ROOT COP NSUBJ DET PREP PUNCT POBJ NN biLSTM(b) biLSTM(b) biLSTM(b) biLSTM(b) biLSTM(b) biLSTM(b) biLSTM(b) biLSTM(b) this, future the, future of, future ?, future music, of chamber, music future, ROOT is, future Figure 2: The processing order of the sentence in figure 1 a) in the BI LSTM( L ) model (top) and b) in the BI LSTM( B ) model (bottom). ROOT Top-down tree LSTM Intuitively, it seems more natural to present the input as a tree structure when trying to predict the dependency labels. We do that by adopting the top-down tree LSTM model (Zhang et al., 2016) that processes nodes linked through dependency paths in a top-down manner. To make it comparable to the previous LSTM models, we only use one LSTM instead of four, and do not stack LSTMs. The hidden state is computed as follow: (lbl) hi = treeLSTM(bi , hi−1 ) treeLSTM COP DET PREP PUNCT future treeLSTM treeLSTM treeLSTM treeLSTM Is this the of treeLSTM POBJ ? treeLSTM NN music treeLSTM chamber Figure 3: The processing order of the sentence in figure 1 in the TREE LSTM model. train and test on the much larger German SPMRL 2014 shared task data (Seddah et al., 2014) (5,000 test sentences). For"
W17-6318,P07-2051,0,\N,Missing
W17-6525,L16-1262,0,0.222686,"on that the observed results of their experiments are “unconvincing and not very promising” (Mareˇcek et al., 2013). Versley and Kirilin (2015) look at the influence of languages and annotation schemes in universal dependency parsing, comparing 5 different parsers on 5 languages using two variants of UD schemes. They state that encoding content words as head has a negative impact on parsing results and that PP attachment errors account for a large portion of 219 the differences in accuracy between the different parsers and between treebanks of varying sizes. Recent work by Gulordava and Merlo (2016) has looked at word order variation and its impact on dependency parsing of 12 languages. They focus on word order freedom and dependency length as two properties of word order that systematically vary between different languages. To assess their impact on parsing accuracy, they modify the original treebanks by minimising the dependency lengths and the entropy of the headdirection (whether the head of dependent dep can be positioned to the left, to the right, or either way), thus creating artificial treebanks with systematically different word order properties. Parsing results on the modified"
W17-6525,petrov-etal-2012-universal,0,0.0226163,"efined feature templates but selects the most probable head for each token based on word representations learned by a bidirectional long-short memory model (LSTM) (Hochreiter and Schmidhuber, 1997). Despite its simplicity and the lack of global optimisation, Zhang et al. (2017) report competetive results for English, Czech, and German. For the first two parsers, we use default settings and the provided feature templates (for the RBG parser we use the standard setting without pretrained word embeddings), with no languagespecific parameter optimisation.8 We use the coarse-grained universal POS (Petrov et al., 2012) for all languages. The RBG and IMSTrans parser 8 Please note that our goal is not to improve, or compare, results for individual languages but to assess the impact of different encoding decisions on the parsing accuracy for one language. 222 are trained on gold POS and morphological features provided by the UD project, the headselection model is trained without morphological information, using word and POS embeddings only. We choose the head-selection model to test whether a potential positive impact of the conversion might simply be a bias introduced by the feature templates, which might fav"
W17-6525,P13-1051,0,0.0514112,"Missing"
W17-6525,D07-1013,0,\N,Missing
W17-6525,de-marneffe-etal-2014-universal,0,\N,Missing
W17-6525,W10-1401,1,\N,Missing
W17-6525,C12-1147,0,\N,Missing
W17-6525,W15-2210,0,\N,Missing
W17-6525,W15-2131,0,\N,Missing
W17-6525,W15-2134,0,\N,Missing
W17-6525,P14-1130,0,\N,Missing
W17-6525,W15-2112,0,\N,Missing
W17-6525,Q16-1025,0,\N,Missing
W17-6525,W17-0411,0,\N,Missing
W17-6525,E17-2001,0,\N,Missing
W17-6525,E17-1063,0,\N,Missing
W17-6525,W10-2927,0,\N,Missing
W17-7614,R13-1008,0,0.0187248,"en measuring domain and genre similarity but use both evenhandedly. To our best knowledge, there are no studies on parser adaptation that try to separate domain from genre effects. 2.2 Adapting parsers to new genres and domains Many parsing studies have addressed the problem of parser adaptation to new genres or domains, often focussed on adapting a Penn treebank-trained parser to biomedical text or to web data.3 Different techniques have been tested for parser adaptation, such as transformations applied to the target data (Foster, 2010), ensemble parsing (Dredze et al., 2007) or co-training (Baucom et al., 2013). Other studies have tried to distinguish between features specific to the source data and general features that also occur in the target data (Dredze et al., 2007), or to create domain- or genre-specific parsing models and select the model combination that most probably will maximise parsing scores on the target data (McClosky et al., 2010; Plank and Sima’an, 2008). Plank and van Noord (2011) and Mukherjee et al. (2017) create new training sets that reflect the distribution in the target data by identifying the source data most similar to the target, based on measures that assess structural o"
W17-7614,W15-2210,0,0.0539888,"Missing"
W17-7614,W06-1615,0,0.102227,"even more surprising as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are different from the training data (Sekine, 1997; Gildea, 2001; McClosky et al., 2006). This might be due to either domain or genre differences, however, in NLP we usually refer to both as out-of-domain effects. While many studies have successfully shown how we can adapt tools to new domains (or genres) (Blitzer et al., 2006; Titov, 2011; Mitchell and Steedman, 2015), less is known about the underlying properties that are responsible for the decrease in performance. Out-of-domain (including out-of-genre) effects might be due to a large amount of unknown words introduced by topic shifts but might also be caused by a higher structural complexity in the data, by longer dependencies or a higher amount of non-projectivity. Intuitively, we assume that domain differences can be captured by content-related features (e.g. from topic modelling) while we expect that functional differences between genres are reflected in str"
W17-7614,N10-1060,0,0.0189117,"es do not distinguish between content-based and structural features when measuring domain and genre similarity but use both evenhandedly. To our best knowledge, there are no studies on parser adaptation that try to separate domain from genre effects. 2.2 Adapting parsers to new genres and domains Many parsing studies have addressed the problem of parser adaptation to new genres or domains, often focussed on adapting a Penn treebank-trained parser to biomedical text or to web data.3 Different techniques have been tested for parser adaptation, such as transformations applied to the target data (Foster, 2010), ensemble parsing (Dredze et al., 2007) or co-training (Baucom et al., 2013). Other studies have tried to distinguish between features specific to the source data and general features that also occur in the target data (Dredze et al., 2007), or to create domain- or genre-specific parsing models and select the model combination that most probably will maximise parsing scores on the target data (McClosky et al., 2010; Plank and Sima’an, 2008). Plank and van Noord (2011) and Mukherjee et al. (2017) create new training sets that reflect the distribution in the target data by identifying the sourc"
W17-7614,W01-0521,0,0.163946,"or register for a particular text. However, despite the amount of work dedicated to genre prediction, the theoretical concept of genre remains vague and no agreement has been reached within the NLP (and linguistics) community on how to define it.1 This is even more surprising as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are different from the training data (Sekine, 1997; Gildea, 2001; McClosky et al., 2006). This might be due to either domain or genre differences, however, in NLP we usually refer to both as out-of-domain effects. While many studies have successfully shown how we can adapt tools to new domains (or genres) (Blitzer et al., 2006; Titov, 2011; Mitchell and Steedman, 2015), less is known about the underlying properties that are responsible for the decrease in performance. Out-of-domain (including out-of-genre) effects might be due to a large amount of unknown words introduced by topic shifts but might also be caused by a higher structural complexity in the dat"
W17-7614,Q16-1025,0,0.0230546,"genre data is levelled out. As before, we note substantial differences between the parsing scores for the different genres. This shows that the gap in results is not due to missing in-domain (or in-genre) training data but that certain genres are in fact harder to parse than others. To find out what it is that makes agency texts so much easier to parse than the commentaries and letters, we compare linguistic properties of the texts in the different genres that have been associated with syntactic complexity and parsing difficulty in the literature (Roark et al., 2007; McDonald and Nivre, 2007; Gulordava and Merlo, 2016). size (token) agency commentary documentary interview letter portrait 10,000 10,000 random in-domain 85.65 86.30 78.47 79.01 79.20 79.13 78.64 78.83 77.61 79.53 80.06 80.52 50,000 50,000 random in-domain 89.87 90.68 83.31 83.80 84.61 84.82 83.95 84.47 83.08 n.a. 85.66 n.a. Table 3: LAS for random training sets from reports and in-domain training sets (avg. LAS over 5 runs). Table 4 shows the average sentence length, the number of finite verbs per sentence (as an approximation of the complexity of the sentence structure), the number of unknown words, the average dependency length, the average"
W17-7614,P97-1005,0,0.675929,"ate how differences between articles in a newspaper corpus relate to the concepts of genre and domain and how they influence parsing performance of a transition-based dependency parser. We do this by applying various similarity measures for data point selection and testing their adequacy for creating genre-aware parsing models. 1 Introduction The work of Biber (1988; 1995) and Biber & Conrad (2009) on language variation has brought valuable insights into the concepts of genre and register and the linguistic features that define them. It has also triggered many studies on genre classification (Kessler et al., 1997; Feldman et al., 2009; Passonneau et al., 2014), trying to automatically predict the genre or register for a particular text. However, despite the amount of work dedicated to genre prediction, the theoretical concept of genre remains vague and no agreement has been reached within the NLP (and linguistics) community on how to define it.1 This is even more surprising as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type o"
W17-7614,H94-1020,0,0.0860862,"rams. In addition, we compare the potential of content and structural features to measure domain and genre similarity with linguistically defined features, inspired by the work of Biber (1988; 1995) on register variation. 3 Experiments In our experiments, we use the TüBa-D/Z treebank (Telljohann et al., 2004), a corpus of German newspaper text from the taz, a German daily newspaper, that includes more than 95,000 sentences annotated with constituency trees and grammatical function labels. The data has been automatically converted to dependencies. Webber (2009) has shown for the Penn treebank (Marcus et al., 1994) that even newspaper corpora should not be considered as homogeneous objects but typically also consist of multiple genres. Similar to the Penn treebank, the TüBa-D/Z (v10) includes articles from a variety of genres. The genre labels in the TüBa-D/Z have been assigned by the editors of the taz and are: reports, commentaries, documentaries, letters to the editor, interviews, portraits and messages from news agencies. It is, however, not clear to what extend these labels correspond to linguistically well-defined categories, i. e., whether documents within a specific genre category share “linguis"
W17-7614,P06-1043,0,0.0506597,"or a particular text. However, despite the amount of work dedicated to genre prediction, the theoretical concept of genre remains vague and no agreement has been reached within the NLP (and linguistics) community on how to define it.1 This is even more surprising as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are different from the training data (Sekine, 1997; Gildea, 2001; McClosky et al., 2006). This might be due to either domain or genre differences, however, in NLP we usually refer to both as out-of-domain effects. While many studies have successfully shown how we can adapt tools to new domains (or genres) (Blitzer et al., 2006; Titov, 2011; Mitchell and Steedman, 2015), less is known about the underlying properties that are responsible for the decrease in performance. Out-of-domain (including out-of-genre) effects might be due to a large amount of unknown words introduced by topic shifts but might also be caused by a higher structural complexity in the data, by longer dependencie"
W17-7614,D07-1013,0,0.0506701,"e effect of training on ingenre data is levelled out. As before, we note substantial differences between the parsing scores for the different genres. This shows that the gap in results is not due to missing in-domain (or in-genre) training data but that certain genres are in fact harder to parse than others. To find out what it is that makes agency texts so much easier to parse than the commentaries and letters, we compare linguistic properties of the texts in the different genres that have been associated with syntactic complexity and parsing difficulty in the literature (Roark et al., 2007; McDonald and Nivre, 2007; Gulordava and Merlo, 2016). size (token) agency commentary documentary interview letter portrait 10,000 10,000 random in-domain 85.65 86.30 78.47 79.01 79.20 79.13 78.64 78.83 77.61 79.53 80.06 80.52 50,000 50,000 random in-domain 89.87 90.68 83.31 83.80 84.61 84.82 83.95 84.47 83.08 n.a. 85.66 n.a. Table 3: LAS for random training sets from reports and in-domain training sets (avg. LAS over 5 runs). Table 4 shows the average sentence length, the number of finite verbs per sentence (as an approximation of the complexity of the sentence structure), the number of unknown words, the average dep"
W17-7614,W15-2610,0,0.029705,"ike genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are different from the training data (Sekine, 1997; Gildea, 2001; McClosky et al., 2006). This might be due to either domain or genre differences, however, in NLP we usually refer to both as out-of-domain effects. While many studies have successfully shown how we can adapt tools to new domains (or genres) (Blitzer et al., 2006; Titov, 2011; Mitchell and Steedman, 2015), less is known about the underlying properties that are responsible for the decrease in performance. Out-of-domain (including out-of-genre) effects might be due to a large amount of unknown words introduced by topic shifts but might also be caused by a higher structural complexity in the data, by longer dependencies or a higher amount of non-projectivity. Intuitively, we assume that domain differences can be captured by content-related features (e.g. from topic modelling) while we expect that functional differences between genres are reflected in structural features such as part-of-speech n-g"
W17-7614,D13-1032,0,0.0201516,"Missing"
W17-7614,E17-1033,0,0.206246,"erent techniques have been tested for parser adaptation, such as transformations applied to the target data (Foster, 2010), ensemble parsing (Dredze et al., 2007) or co-training (Baucom et al., 2013). Other studies have tried to distinguish between features specific to the source data and general features that also occur in the target data (Dredze et al., 2007), or to create domain- or genre-specific parsing models and select the model combination that most probably will maximise parsing scores on the target data (McClosky et al., 2010; Plank and Sima’an, 2008). Plank and van Noord (2011) and Mukherjee et al. (2017) create new training sets that reflect the distribution in the target data by identifying the source data most similar to the target, based on measures that assess structural or topic similarity between both. Features used in these experiments (McClosky et al., 2010; Plank and van Noord, 2011; Mukherjee et al., 2017) include known and unknown words, character n-grams and LDA topics but do not (or only implicitly) capture structural similarity. The authors show that content and surface features are successful in selecting appropriate training data for the new domain and also work better than us"
W17-7614,C14-1054,0,0.0236337,"wspaper corpus relate to the concepts of genre and domain and how they influence parsing performance of a transition-based dependency parser. We do this by applying various similarity measures for data point selection and testing their adequacy for creating genre-aware parsing models. 1 Introduction The work of Biber (1988; 1995) and Biber & Conrad (2009) on language variation has brought valuable insights into the concepts of genre and register and the linguistic features that define them. It has also triggered many studies on genre classification (Kessler et al., 1997; Feldman et al., 2009; Passonneau et al., 2014), trying to automatically predict the genre or register for a particular text. However, despite the amount of work dedicated to genre prediction, the theoretical concept of genre remains vague and no agreement has been reached within the NLP (and linguistics) community on how to define it.1 This is even more surprising as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are diffe"
W17-7614,J11-2004,0,0.174445,"based on pos n-grams Distance from topic distribution Text statistics, morpho-syntactic features no. of features 33 n.a. 100 40 Table 5: Overview of the settings and features used for data point selection. 3.4 Data point selection for genre-aware parsing We now explore whether we can train genre-aware parsing models on larger data by selecting out-ofgenre data points that are similar to the target genre. To that end, we test the adequacy of different feature types for measuring similarity. Kessler et al. (1997) (KNS) have obtained consistently good results for genre prediction across topics (Petrenz and Webber, 2011), based only on surface features.8 Plank and van Noord (2011) and Mukherjee et al. (2017) have trained domain-specific parsing models based on content (LDA topics) and surface features (word frequencies and character n-gram frequencies). In our experiments, we would like to compare the adequacy of content and surface features with data selection based on structural similarity (where similarity is operationalised as the perplexity of a language model (LM) based on POS n-grams) and features that take into account the linguistic properties of a text, relying on Biber-style features. Table 5 gives"
W17-7614,plank-simaan-2008-subdomain,0,0.0626309,"Missing"
W17-7614,P11-1157,0,0.0508904,"Missing"
W17-7614,W11-3808,1,0.764814,"osky et al., 2010; Plank and van Noord, 2011; Mukherjee et al., 2017) include known and unknown words, character n-grams and LDA topics but do not (or only implicitly) capture structural similarity. The authors show that content and surface features are successful in selecting appropriate training data for the new domain and also work better than using genre labels assigned by humans (Plank and van Noord, 2011). Søgaard (2011), however, has shown that data point selection based on structural similarity can improve parsing accuracy significantly in a cross-lingual parser adaptation setting and Rehbein (2011) shows a similar effect for in-domain self-training. Based 2A full survey of work on register, genre or domain variation is beyond the scope of this paper. We refer to Biber (1988) and especially Lee (2001) for a review of how these terms have been used in various theoretical frameworks. 3See, e. g., the CoNLL 2007 Shared Task on Domain Adaptation (Nivre et al., 2007) and the SANCL 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012). 96 portrait letter documentary agency interview commentary taz report # articles # sent # token avg. sent length # sent train pool # sent testset 42 1"
W17-7614,W07-1001,0,0.0273674,"figure 1) so that the effect of training on ingenre data is levelled out. As before, we note substantial differences between the parsing scores for the different genres. This shows that the gap in results is not due to missing in-domain (or in-genre) training data but that certain genres are in fact harder to parse than others. To find out what it is that makes agency texts so much easier to parse than the commentaries and letters, we compare linguistic properties of the texts in the different genres that have been associated with syntactic complexity and parsing difficulty in the literature (Roark et al., 2007; McDonald and Nivre, 2007; Gulordava and Merlo, 2016). size (token) agency commentary documentary interview letter portrait 10,000 10,000 random in-domain 85.65 86.30 78.47 79.01 79.20 79.13 78.64 78.83 77.61 79.53 80.06 80.52 50,000 50,000 random in-domain 89.87 90.68 83.31 83.80 84.61 84.82 83.95 84.47 83.08 n.a. 85.66 n.a. Table 3: LAS for random training sets from reports and in-domain training sets (avg. LAS over 5 runs). Table 4 shows the average sentence length, the number of finite verbs per sentence (as an approximation of the complexity of the sentence structure), the number of unkn"
W17-7614,A97-1015,0,0.275416,"dict the genre or register for a particular text. However, despite the amount of work dedicated to genre prediction, the theoretical concept of genre remains vague and no agreement has been reached within the NLP (and linguistics) community on how to define it.1 This is even more surprising as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are different from the training data (Sekine, 1997; Gildea, 2001; McClosky et al., 2006). This might be due to either domain or genre differences, however, in NLP we usually refer to both as out-of-domain effects. While many studies have successfully shown how we can adapt tools to new domains (or genres) (Blitzer et al., 2006; Titov, 2011; Mitchell and Steedman, 2015), less is known about the underlying properties that are responsible for the decrease in performance. Out-of-domain (including out-of-genre) effects might be due to a large amount of unknown words introduced by topic shifts but might also be caused by a higher structural complex"
W17-7614,P11-2120,0,0.0258625,"rget data by identifying the source data most similar to the target, based on measures that assess structural or topic similarity between both. Features used in these experiments (McClosky et al., 2010; Plank and van Noord, 2011; Mukherjee et al., 2017) include known and unknown words, character n-grams and LDA topics but do not (or only implicitly) capture structural similarity. The authors show that content and surface features are successful in selecting appropriate training data for the new domain and also work better than using genre labels assigned by humans (Plank and van Noord, 2011). Søgaard (2011), however, has shown that data point selection based on structural similarity can improve parsing accuracy significantly in a cross-lingual parser adaptation setting and Rehbein (2011) shows a similar effect for in-domain self-training. Based 2A full survey of work on register, genre or domain variation is beyond the scope of this paper. We refer to Biber (1988) and especially Lee (2001) for a review of how these terms have been used in various theoretical frameworks. 3See, e. g., the CoNLL 2007 Shared Task on Domain Adaptation (Nivre et al., 2007) and the SANCL 2012 Shared Task on Parsing the"
W17-7614,telljohann-etal-2004-tuba,0,0.0456175,"e different feature types capture similar properties in the data. We consider contentrelated features to be characteristic for certain domains while we expect that functional differences between genres are reflected in structural differences between texts and can be captured by features such as part-of-speech n-grams. In addition, we compare the potential of content and structural features to measure domain and genre similarity with linguistically defined features, inspired by the work of Biber (1988; 1995) on register variation. 3 Experiments In our experiments, we use the TüBa-D/Z treebank (Telljohann et al., 2004), a corpus of German newspaper text from the taz, a German daily newspaper, that includes more than 95,000 sentences annotated with constituency trees and grammatical function labels. The data has been automatically converted to dependencies. Webber (2009) has shown for the Penn treebank (Marcus et al., 1994) that even newspaper corpora should not be considered as homogeneous objects but typically also consist of multiple genres. Similar to the Penn treebank, the TüBa-D/Z (v10) includes articles from a variety of genres. The genre labels in the TüBa-D/Z have been assigned by the editors of the"
W17-7614,P11-1007,0,0.020101,"as concepts like genre and domain seem to be of crucial importance to our field and it is well known that the accuracy of NLP tools trained on one type of text will decrease noticeably when applying the same tools to another type of text with underlying properties that are different from the training data (Sekine, 1997; Gildea, 2001; McClosky et al., 2006). This might be due to either domain or genre differences, however, in NLP we usually refer to both as out-of-domain effects. While many studies have successfully shown how we can adapt tools to new domains (or genres) (Blitzer et al., 2006; Titov, 2011; Mitchell and Steedman, 2015), less is known about the underlying properties that are responsible for the decrease in performance. Out-of-domain (including out-of-genre) effects might be due to a large amount of unknown words introduced by topic shifts but might also be caused by a higher structural complexity in the data, by longer dependencies or a higher amount of non-projectivity. Intuitively, we assume that domain differences can be captured by content-related features (e.g. from topic modelling) while we expect that functional differences between genres are reflected in structural featu"
W17-7614,P09-1076,0,0.0328947,"captured by features such as part-of-speech n-grams. In addition, we compare the potential of content and structural features to measure domain and genre similarity with linguistically defined features, inspired by the work of Biber (1988; 1995) on register variation. 3 Experiments In our experiments, we use the TüBa-D/Z treebank (Telljohann et al., 2004), a corpus of German newspaper text from the taz, a German daily newspaper, that includes more than 95,000 sentences annotated with constituency trees and grammatical function labels. The data has been automatically converted to dependencies. Webber (2009) has shown for the Penn treebank (Marcus et al., 1994) that even newspaper corpora should not be considered as homogeneous objects but typically also consist of multiple genres. Similar to the Penn treebank, the TüBa-D/Z (v10) includes articles from a variety of genres. The genre labels in the TüBa-D/Z have been assigned by the editors of the taz and are: reports, commentaries, documentaries, letters to the editor, interviews, portraits and messages from news agencies. It is, however, not clear to what extend these labels correspond to linguistically well-defined categories, i. e., whether doc"
W19-2505,D17-2008,0,0.0204194,"126 15 3 Unfortunately, this is not always an option. de Marneffe et al. (2009) show that the automatic resolution of multi-word alignments to the right target term is a hard problem and requires automatic recognition of multi-word expressions. For more complex projection tasks, we will thus need a more sophisticated alignment method, based on graph optimisation or machine learning. Previous work in the context of semantic role labelling has followed this approach, with promising results (Padó and Lapata, 2005, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013; Akbik et al., 2015; Akbik and Vollgraf, 2017; Aminian et al., 2017). We would like to explore this further in future work. PP 528 229 7 1 Table 3: N-gram statistics for mention words (raw frequencies) in the corpus. A recurring pattern in our data is the incorrect Figure 8: Transfer error caused by translation divergence (incorrect 1:1 sentence alignment). Figure 9: Transfer error caused by incorrect coalignment. 42 6 Conclusions and future work Zhu. 2015. Generating high quality proposition banks for multilingual Semantic Role Labeling. In The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Internationa"
W19-2505,E14-1005,0,0.0240454,"information extraction from the news domain (Pouliquen et al., 2007; Krestel et al., 2008; Pareti et al., 2013; Pareti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test the projection of coreference annotations, a task related to speaker attribution, using multiple source languages. Muzny et al. (2017) improved on previous work on quote and speaker attribution by providing a cleaned-up dataset, the QuoteLi3 corpus, which includes more annotations than the previous datasets. They also present a two-step deterministic sieve model for speaker attribution on the entity level and report a high precision for their approach1 . This means that"
W19-2505,I17-2003,0,0.0202796,"his is not always an option. de Marneffe et al. (2009) show that the automatic resolution of multi-word alignments to the right target term is a hard problem and requires automatic recognition of multi-word expressions. For more complex projection tasks, we will thus need a more sophisticated alignment method, based on graph optimisation or machine learning. Previous work in the context of semantic role labelling has followed this approach, with promising results (Padó and Lapata, 2005, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013; Akbik et al., 2015; Akbik and Vollgraf, 2017; Aminian et al., 2017). We would like to explore this further in future work. PP 528 229 7 1 Table 3: N-gram statistics for mention words (raw frequencies) in the corpus. A recurring pattern in our data is the incorrect Figure 8: Transfer error caused by translation divergence (incorrect 1:1 sentence alignment). Figure 9: Transfer error caused by incorrect coalignment. 42 6 Conclusions and future work Zhu. 2015. Generating high quality proposition banks for multilingual Semantic Role Labeling. In The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on N"
W19-2505,H05-1045,0,0.0607333,"next section, we present our approach to annotation transfer of quotes and speaker mentions based on an automatically created parallel corpus, with the aim of creating annotated resources for quote detection and speaker attribution for German literature. Related work Quote detection has been an active field of research, mostly for information extraction from the news domain (Pouliquen et al., 2007; Krestel et al., 2008; Pareti et al., 2013; Pareti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test the projection of coreference annotations, a task related to speaker attribution, using multiple source languages. Muzny et al. (2017) im"
W19-2505,J94-4004,0,0.374271,"et al., 2013). While the method has recently been outperformed by neural approaches (Legrand et al., 2016), its fast and efficient implementation and decent results make it well-suited for integration in our pipeline. 3.4 Emma 742 399 49 4.1 Impact of the literary translation For many novels, not just one but a number of translations are available. We are thus confronted with the problem of having to choose one translation from a set of available texts, and it is not clear how to determine the most adequate translation for the task at hand. Translation divergences are a known problem for MT (Dorr, 1994; Dorr et al., 2004). In parallel corpora of literary prose, however, divergences are even more prominent than in many other genres. A high-quality literary translation not only needs to transfer the semantic meaning of the source text into the target language but also has to consider stilistic devices such as metaphor, alliteration, hyperbole, oxymoron, simile and more that are difficult to translate. Therefore, the translator often has Annotation transfer The final step in our pipeline is the transfer of annotations from the source to the target side. For the task at hand, we directly transf"
W19-2505,N13-1073,0,0.0313366,"uoteLi3 corpus (Muzny et al., 2017) provides manual annotations of speakers and quotes in three novels (Emma and Pride and Prejudice by Jane Austen and The Steppe by Anton Chekhov).4 Since no publically available digital translation for the Chekhov novel was found, our evaluation will focus on the two Austen novels which include more than 2,300 annotations for quotes and more than 1,100 mentions for 81 speakers (table 1). Word alignment Once we have aligned the sentences in our parallel corpus, the next step is the alignment of words between the source and target sentences. We use fast_align (Dyer et al., 2013), a log-linear reparameterisation of IBM Model 2, the second of a set of well-known SMT alignment models developed by IBM in the late 1980s. Fast_align is unsupervised and thus applicable to any language for which training data is available. It outperforms the Giza++ implementation of the IBM Models 1-5 (Och and Ney, 2003) with regard to speed, translation quality (measured in BLEU score) and alignment error rate (Dyer et al., 2013). While the method has recently been outperformed by neural approaches (Legrand et al., 2016), its fast and efficient implementation and decent results make it well"
W19-2505,W12-2513,0,0.020594,"f quote and speaker mention annotations from English to German. We evaluate the different components of the pipeline and discuss challenges specific to literary texts. Our experiments show that after applying a reasonable amount of semi-automatic postprocessing we can obtain high-quality aligned and annotated resources for a new language. 1 Introduction Recent years have seen an increasing interest in using computational and mixed method approaches for literary studies. A case in point is the analysis of literary characters using social network analysis (Elson et al., 2010; Rydberg-Cox, 2011; Agarwal et al., 2012; Kydros and Anastasiadis, 2014). While the first networks have been created manually, follow-up studies have tried to automatically extract the information needed to fill the network with life. The manual construction of such networks can yield high quality analyses, however, the amount of time needed for manually extracting the information is huge. The second approach based on automatic information extraction is more adequate for large scale investigations of literary texts. However, due to the difficulty of the task the quality of the resulting network is often seriously hampered. In some s"
W19-2505,P10-1015,0,0.0453018,"teinbach|rehbein}@cl.uni-heidelberg.de Abstract A more meaningful analysis requires the identification of character entities and their mentions in the text, as well as the attribution of quotes to their respective speakers. Unfortunately, this is not an easy task. Characters in novels are mostly referred to by anaphoric mentions, such as personal pronouns or nominal descriptors (e.g. “the old women” or “the hard-headed lawyer”), and these have to be traced back to the respective entity to whom they refer, i.e. the speaker. For English, automatic approaches based on machine learning (Elson and McKeown, 2010; He et al., 2013) or rule-based systems (Muzny et al., 2017) have been developed for this task, and a limited amount of annotated resources already exists. For most other languages, however, such resources are not yet available. To make progress towards the fully automatic identification of speakers and quotes in literary texts, we need more training data. As the fully manual annotation of such resources is time-consuming and costly, we present a method for the automatic transfer of annotations from English to other languages where resources for speaker attribution and quote detection are spa"
W19-2505,W09-2501,0,0.0917589,"Missing"
W19-2505,J93-1004,0,0.520703,"ngs with 500 dimensions and a hidden layer size of 1024). quotes mentions entities Aligning MT and human translation The Bleualign algorithm is composed of two steps. In the first step, the algorithm tries to find a set of anchor points, using BLEU as a similarity score between the machine-translated source text and the human-translated target text. These anchor points are a set of 1:1 alignments considered reliable based on BLEU scores and sentence order. In a second step, the sentences between these anchor points are either aligned using BLEUbased heuristics or the length-based algorithm of Gale and Church (1993b). The latter algorithm is applied to the target and translated source sentences and functions as a fallback for all gaps with a symmetrical size of unaligned sentences. Sentences that cannot be aligned are discarded. We use default parameters for Bleualign (a maximum of 3 alternative BLEU-aligned sentences in the first run, a BLEU-scoring restriction on bigrams and second pass gap-filling by means of BLEU and the Gale and Church algorithm). 3.3 P&P 1,575 765 32 total 2,317 1,164 81 Table 1: Annotations of quotes, speaker mentions and entities in the QuoteLi3 corpus (Emma and Pride and Prejud"
W19-2505,W17-1506,0,0.0214497,"eti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test the projection of coreference annotations, a task related to speaker attribution, using multiple source languages. Muzny et al. (2017) improved on previous work on quote and speaker attribution by providing a cleaned-up dataset, the QuoteLi3 corpus, which includes more annotations than the previous datasets. They also present a two-step deterministic sieve model for speaker attribution on the entity level and report a high precision for their approach1 . This means that we can apply the rule-based sieve model to new text in order to generate more training data for the task at hand. The m"
W19-2505,E17-1044,0,0.257224,"meaningful analysis requires the identification of character entities and their mentions in the text, as well as the attribution of quotes to their respective speakers. Unfortunately, this is not an easy task. Characters in novels are mostly referred to by anaphoric mentions, such as personal pronouns or nominal descriptors (e.g. “the old women” or “the hard-headed lawyer”), and these have to be traced back to the respective entity to whom they refer, i.e. the speaker. For English, automatic approaches based on machine learning (Elson and McKeown, 2010; He et al., 2013) or rule-based systems (Muzny et al., 2017) have been developed for this task, and a limited amount of annotated resources already exists. For most other languages, however, such resources are not yet available. To make progress towards the fully automatic identification of speakers and quotes in literary texts, we need more training data. As the fully manual annotation of such resources is time-consuming and costly, we present a method for the automatic transfer of annotations from English to other languages where resources for speaker attribution and quote detection are sparse. We test our approach for German, making use of publicall"
W19-2505,P13-1129,0,0.141744,"n}@cl.uni-heidelberg.de Abstract A more meaningful analysis requires the identification of character entities and their mentions in the text, as well as the attribution of quotes to their respective speakers. Unfortunately, this is not an easy task. Characters in novels are mostly referred to by anaphoric mentions, such as personal pronouns or nominal descriptors (e.g. “the old women” or “the hard-headed lawyer”), and these have to be traced back to the respective entity to whom they refer, i.e. the speaker. For English, automatic approaches based on machine learning (Elson and McKeown, 2010; He et al., 2013) or rule-based systems (Muzny et al., 2017) have been developed for this task, and a limited amount of annotated resources already exists. For most other languages, however, such resources are not yet available. To make progress towards the fully automatic identification of speakers and quotes in literary texts, we need more training data. As the fully manual annotation of such resources is time-consuming and costly, we present a method for the automatic transfer of annotations from English to other languages where resources for speaker attribution and quote detection are sparse. We test our a"
W19-2505,J03-1002,0,0.0109311,"nclude more than 2,300 annotations for quotes and more than 1,100 mentions for 81 speakers (table 1). Word alignment Once we have aligned the sentences in our parallel corpus, the next step is the alignment of words between the source and target sentences. We use fast_align (Dyer et al., 2013), a log-linear reparameterisation of IBM Model 2, the second of a set of well-known SMT alignment models developed by IBM in the late 1980s. Fast_align is unsupervised and thus applicable to any language for which training data is available. It outperforms the Giza++ implementation of the IBM Models 1-5 (Och and Ney, 2003) with regard to speed, translation quality (measured in BLEU score) and alignment error rate (Dyer et al., 2013). While the method has recently been outperformed by neural approaches (Legrand et al., 2016), its fast and efficient implementation and decent results make it well-suited for integration in our pipeline. 3.4 Emma 742 399 49 4.1 Impact of the literary translation For many novels, not just one but a number of translations are available. We are thus confronted with the problem of having to choose one translation from a set of available texts, and it is not clear how to determine the mo"
W19-2505,J13-3002,0,0.0119094,"otation transfer of quotes and speaker mentions based on an automatically created parallel corpus, with the aim of creating annotated resources for quote detection and speaker attribution for German literature. Related work Quote detection has been an active field of research, mostly for information extraction from the news domain (Pouliquen et al., 2007; Krestel et al., 2008; Pareti et al., 2013; Pareti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test the projection of coreference annotations, a task related to speaker attribution, using multiple source languages. Muzny et al. (2017) improved on previous work on quote and speaker attribution b"
W19-2505,P13-1117,0,0.0213922,"3). n-gram unigram bigram trigram 4-gram Emma 254 126 15 3 Unfortunately, this is not always an option. de Marneffe et al. (2009) show that the automatic resolution of multi-word alignments to the right target term is a hard problem and requires automatic recognition of multi-word expressions. For more complex projection tasks, we will thus need a more sophisticated alignment method, based on graph optimisation or machine learning. Previous work in the context of semantic role labelling has followed this approach, with promising results (Padó and Lapata, 2005, 2009; van der Plas et al., 2011; Kozhevnikov and Titov, 2013; Akbik et al., 2015; Akbik and Vollgraf, 2017; Aminian et al., 2017). We would like to explore this further in future work. PP 528 229 7 1 Table 3: N-gram statistics for mention words (raw frequencies) in the corpus. A recurring pattern in our data is the incorrect Figure 8: Transfer error caused by translation divergence (incorrect 1:1 sentence alignment). Figure 9: Transfer error caused by incorrect coalignment. 42 6 Conclusions and future work Zhu. 2015. Generating high quality proposition banks for multilingual Semantic Role Labeling. In The 53rd Annual Meeting of the Association for Comp"
W19-2505,krestel-etal-2008-minding,0,0.110956,"Missing"
W19-2505,P02-1040,0,0.103781,"Missing"
W19-2505,D13-1101,0,0.0640087,"Missing"
W19-2505,P11-2052,0,0.0696558,"Missing"
W19-2505,W16-2207,0,0.0164438,"s domain (Pouliquen et al., 2007; Krestel et al., 2008; Pareti et al., 2013; Pareti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test the projection of coreference annotations, a task related to speaker attribution, using multiple source languages. Muzny et al. (2017) improved on previous work on quote and speaker attribution by providing a cleaned-up dataset, the QuoteLi3 corpus, which includes more annotations than the previous datasets. They also present a two-step deterministic sieve model for speaker attribution on the entity level and report a high precision for their approach1 . This means that we can apply the rul"
W19-2505,P14-5010,0,0.00241538,"and more sophisticated post-processing and transfer methods. Sub-task specific outputs are flushed to file after each step in the pipeline. Thereby, the user is given the opportunity to modify the output at any stage of the process. 3.1 Sentence segmentation Sentence segmentation is by no means a solved problem (see, e.g., Read et al. (2012) for a thorough evaluation of different segmentation tools). This is especially true when working with literary prose where embedded sentences inside of quotes pose a challenge for sentence boundary detection. In our pipeline, we use the Stanford CoreNLP (Manning et al., 2014) which offers out-of-the-box tokenisation and sentence splitting. We selected CoreNLP because it offers support for many languages and is robust and easy to integrate. Once the input text is segmented into individual sentences, we need to align each source sentence to one or more sentences in the target text. 3.2 Sentence alignment Sentence alignment is an active field of research in statistical machine translation (SMT). The task can be described as follows. Given a set of source language sentences and a set of target language sentences, assign corresponding sentences from both sets, where ea"
W19-2505,C12-2096,0,0.0341323,"ented as classes and integrated into the main class as sub-module imports. The modular architecture facilitates the integration of additional classes or class-methods inside the main class, the replacement of individual components as well as the integration of new languages and more sophisticated post-processing and transfer methods. Sub-task specific outputs are flushed to file after each step in the pipeline. Thereby, the user is given the opportunity to modify the output at any stage of the process. 3.1 Sentence segmentation Sentence segmentation is by no means a solved problem (see, e.g., Read et al. (2012) for a thorough evaluation of different segmentation tools). This is especially true when working with literary prose where embedded sentences inside of quotes pose a challenge for sentence boundary detection. In our pipeline, we use the Stanford CoreNLP (Manning et al., 2014) which offers out-of-the-box tokenisation and sentence splitting. We selected CoreNLP because it offers support for many languages and is robust and easy to integrate. Once the input text is segmented into individual sentences, we need to align each source sentence to one or more sentences in the target text. 3.2 Sentence"
W19-2505,P16-1164,0,0.0128222,"nguistics and present a method for determining the best translation for this task. 2 pipeline can be easily adapted to new languages. In the next section, we present our approach to annotation transfer of quotes and speaker mentions based on an automatically created parallel corpus, with the aim of creating annotated resources for quote detection and speaker attribution for German literature. Related work Quote detection has been an active field of research, mostly for information extraction from the news domain (Pouliquen et al., 2007; Krestel et al., 2008; Pareti et al., 2013; Pareti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test th"
W19-2505,E17-3017,0,0.0513711,"Missing"
W19-2505,2010.amta-papers.14,0,0.0725542,"Missing"
W19-2505,E12-1033,0,0.0232275,"resent our approach to annotation transfer of quotes and speaker mentions based on an automatically created parallel corpus, with the aim of creating annotated resources for quote detection and speaker attribution for German literature. Related work Quote detection has been an active field of research, mostly for information extraction from the news domain (Pouliquen et al., 2007; Krestel et al., 2008; Pareti et al., 2013; Pareti, 2015; Scheible et al., 2016). Related work in the context of opinion mining has tried to identify the holders (speakers) and targets of opinions (Choi et al., 2005; Wiegand and Klakow, 2012; Johansson and Moschitti, 2013). Elson and McKeown (2010) were among the first to propose a supervised machine learning model for quote attribution in literary text. He et al. (2013) extended their supervised approach by including contextual knowledge from unsupervised actor-topic models. Almeida et al. (2014) and Fertmann (2016) combined the task of speaker identification with coreference resolution. Grishina and Stede (2017) test the projection of coreference annotations, a task related to speaker attribution, using multiple source languages. Muzny et al. (2017) improved on previous work on"
W19-2505,2015.lilt-12.6,0,0.0768502,"Missing"
W19-4017,P15-2072,0,0.0255526,"hey also try to identify argument structure based on discourse connectives and report negative results for this approach, probably due to data sparseness. Felder (2015) shows how concessive and contrastive discourse connectives can be used to identify the central points of conflict in an argument, based on the selection or foregrounding of specific subtopics that are used to frame the discourse. A certain topic can, for example, be discussed against the background of moral or economic arguments, and thus appeal to different groups of people with differing political views (also see the work of Card et al. (2015, 2016) on media frames). In his work, Felder uses discourse connectives as signals for identifying conflicting framing strategies but does not investigate their strategic function in the discourse. Kamalski et al. (2006) present two psycholinguistic experiments showing that the use of discourse connectives not only has a positive effect on the hearer’s comprehension facility but also leads to a more positive evaluation of the text. This observation suggests that discourse connectives might be used as strategic devices in persuasive text. Background 2.1 Discourse connectives in argumentative a"
W19-4017,C16-2026,0,0.0358169,"Missing"
W19-4017,D16-1148,0,0.044825,"Missing"
W19-4017,P13-1025,0,0.0646164,"Missing"
W19-4017,D15-1267,0,0.105748,"ium (spoken vs. written), the discourse situation (monologic vs. dialogic, formal vs. informal), the purpose of communication (informative vs. persuasive), and more. As we are most interested in investigating different strategies of persuasion, we focus on explicit markers of concessive and contrastive discourse relations, used by the speaker to provide a convincing argument that might persuade the hearer. Work on discourse analysis and argumentation mining has highlighted the important role of discourse connectives for analysing argumentation structure (Felder, 2015; Stab and Gurevych, 2014; Eckle-Kohler et al., 2015). In addition, psycholinguistic studies have shown that explicit coherence marking not only improves sentence comprehension but also results in a more positive evalua144 Proceedings of the 13th Linguistic Annotation Workshop, pages 144–154 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics of discourse relations in the different situational settings (§5). Finally, we discuss whether the different distribution of discourse relations in each setting reflects different strategies used to pursue a communicative purpose (§6), and how these might relate to audience des"
W19-4017,C14-1054,0,0.0210782,"s in the use of discourse connectives might reflect functional differences. In our study, we try to eliminate this factor. We control for speaker and –in the smaller subcorpus– also for topic. All texts have a clear persuasive Given that the texts have the same persuasive function, we assume that the differences we observed reflect differences along two dimensions of variation that are correlated with the situational settings of text production. In a more detailed analysis with more features than just the counts for different connective forms, similar to Biber (1995); Biber and Conrad (2009); Passonneau et al. (2014), we would expect to find the two dimensions displayed in figure 2. The first dimension distinguishes highly edited texts from less edited ones. Here the articles are positioned on the left end of the dimension, the talks can be located somewhere in the middle and the interviews as the least edited of the three text types are positioned at the right end. The second dimension concerns the interactional dimension of communication and sets monological texts apart from dialogical ones. Here, the articles can be placed at the monological end of the scale while the interviews are clearly dialogical"
W19-4017,N12-1057,0,0.0293623,"different discourse settings. Litt (2012) extended the model for what he calls the “imagined audience”, accounting for style shifts in situations where the real audience is not known to the speaker and thus the speaker adapts her style to a mental model of a hearer. This is relevant for many broadcasting media, for example for social media platforms such as forums, blogs or microtext messengers. Many studies have described and quantified effects of audience design in social media, looking at power relations, politeness and other variables of style shifts (Bramsen et al., 2011; Gilbert, 2012; Prabhakaran et al., 2012; niculescu mizil et al., 2012; DanescuNiculescu-Mizil et al., 2013; Pavalanathan and Eisenstein, 2015). Applying the audience design model of Bell (1984) to our data, we have to account for different types of audiences. In the interview situation, the speaker is talking to one or more adressees who are known to the speaker and who are able to interact with her. In oral talks, the audience is visible to the speaker and can be directly addressed by her. However, the speaker usually has much less information about the hearers, and the audience has very limited means to actively take part in the"
W19-4017,prasad-etal-2008-penn,0,0.0968753,"stead of discourse connective forms. As there is no one-to-one correspondence between discourse connectives and discourse relations and automatic tools are not yet reliable enough, we need to manually disambiguate the relations in the data. 5 Annotation study • Concession: Although she was qualified, she didn’t get the job. We present an annotation study where we annotate all instances of discourse connectives that can express a causal, contrastive or concessive discourse relation. We follow the framework of PDTB3 (Webber et al., 2016), a revised version of the Penn Discourse Treebank scheme (Prasad et al., 2008). The question we would like to answer is: Does the difference in distribution of discourse connectives in the texts shown above reflect differences in the distribution of discourse relations, or are the same relations expressed by different devices that are more adequate for a given discourse situation? 5.1 • Contrast: Mary likes to read while John loves cooking. • Cause: The street is wet because it rained last night. • Cause + Belief (+β): She must be home because the light is on. • Cause + Speechact (+ζ): He’s in Denver because he just called me an hour ago. 5.2 Discourse connectives versu"
W19-4017,L16-1165,1,0.914006,"suasive text. In particular, we investigate how different dimensions of variation impact a speaker’s linguistic behaviour during the production of argumentative texts. The approach we take is a comparative study of discourse connectives in persuasive texts produced by the same speaker, but in different discourse situations. Our investigation takes the following two observations as its starting point. First, it has been shown that the use of discourse connectives and the distribution of explicit and implicit discourse relations is genre-dependent (see, e.g., Webber (2009) for written genres or Rehbein et al. (2016) for spoken texts).1 Second, Eckle-Kohler et al. (2015) show that certain discourse connectives are highly predictive features for distinguishing claims and premises in argumentative texts.2 This suggests that discourse connectives play a crucial role as strategic devices for persuasion. We follow O’Keefe (1990) and define persuasion as “a successful intentional effort at influencing another’s mental state through communication in a circumstance in which the persuadee has some measure of freedom”(O’Keefe, 1990, p.5). We distinguish persuasive from merely argumentative texts that rely on the ne"
W19-4017,D14-1006,0,0.0270253,"ons, depending on the medium (spoken vs. written), the discourse situation (monologic vs. dialogic, formal vs. informal), the purpose of communication (informative vs. persuasive), and more. As we are most interested in investigating different strategies of persuasion, we focus on explicit markers of concessive and contrastive discourse relations, used by the speaker to provide a convincing argument that might persuade the hearer. Work on discourse analysis and argumentation mining has highlighted the important role of discourse connectives for analysing argumentation structure (Felder, 2015; Stab and Gurevych, 2014; Eckle-Kohler et al., 2015). In addition, psycholinguistic studies have shown that explicit coherence marking not only improves sentence comprehension but also results in a more positive evalua144 Proceedings of the 13th Linguistic Annotation Workshop, pages 144–154 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics of discourse relations in the different situational settings (§5). Finally, we discuss whether the different distribution of discourse relations in each setting reflects different strategies used to pursue a communicative purpose (§6), and how these"
W19-4017,P09-1076,0,0.215575,"ly marked discourse relations in persuasive text. In particular, we investigate how different dimensions of variation impact a speaker’s linguistic behaviour during the production of argumentative texts. The approach we take is a comparative study of discourse connectives in persuasive texts produced by the same speaker, but in different discourse situations. Our investigation takes the following two observations as its starting point. First, it has been shown that the use of discourse connectives and the distribution of explicit and implicit discourse relations is genre-dependent (see, e.g., Webber (2009) for written genres or Rehbein et al. (2016) for spoken texts).1 Second, Eckle-Kohler et al. (2015) show that certain discourse connectives are highly predictive features for distinguishing claims and premises in argumentative texts.2 This suggests that discourse connectives play a crucial role as strategic devices for persuasion. We follow O’Keefe (1990) and define persuasion as “a successful intentional effort at influencing another’s mental state through communication in a circumstance in which the persuadee has some measure of freedom”(O’Keefe, 1990, p.5). We distinguish persuasive from me"
W19-4017,W16-1704,0,0.0580718,"e). dialogical Figure 2: Dimensions of variation for different discourse settings. are determined by position in inter-sentential relations as well as in intra-sentential paratactic structures (e.g. coordinations). In intra-sentential subordinated structures, the subordinated clause is always labelled Arg2, regardless of its position. The new relation hierarchy is shown in table 1. Additionally, some relations can be marked as either pragmatic (epistemic) (β; for implicit beliefs) or as speech acts (ζ).6 These features should be understood as properties of the arguments, not of the relations (Webber et al., 2016). The examples below illustrate the different relation types relevant to our study. We follow the PDTB conventions and mark the first argument in italics and the second argument in boldface. The discourse connective that signals the relation is underlined. some interaction with the audience. These two dimensions of variation might explain the variance in the distribution of discourse connectives in the data, as the same discourse relations can be expressed via different connectives (or can also be left implicit). We thus hypothesize that the differences we observe will disappear when we look a"
W19-7811,W17-1312,0,0.0352609,"Missing"
W19-7811,P18-1131,0,0.243557,"Missing"
W19-7811,W17-0404,0,0.0202856,"ower: OOV for lower-cased word forms). Inter-Annotator Agreement We computed IAA on a subset of the data with 1,630 tokens. For labelled attachments, the agreement between the two annotators was 0.83 κ, for unlabelled attachments the score increased to 0.89 κ. 3 Annotation decisions Below we discuss decisions we made during the annotation process that deviate from other existing German UD treebanks, i.e. the UD-GSD and the UD-TüBa-D/Z. UD-GSD has been converted from an earlier version of Stanford-style dependencies (McDonald et al., 2013) and contains mostly web reviews while the UD-TüBa-D/Z (Çöltekin et al., 2017) is a conversion of the TüBa-D/Z (Telljohann et al., 2004) and includes articles from a German daily newspaper. Placeholder sentences In the UD-GSD treebank, finite subordinate placeholder sentences with dass or ob (that, whether) are mostly analysed as ccomp while infinite correlates are annotated as acl and attached to the placeholder, usually a pronominal adverb. In contrast, the TüBa-D/Z attaches both finite and infinite placeholder clauses as adverbial clause to the verb of the matrix clause. We decided to annotate finite and infinite placeholder sentences as acl and attach both to their"
W19-7811,E03-1068,0,0.0901857,"Missing"
W19-7811,K17-3002,0,0.0171392,"al. 2014) EN (Liu et al. 2018) EN-AAE (Blodgett et al. 2018) EN-MS (Blodgett et al. 2018) IT (Sanguinetti et al. 2018) # token n.a. 12,149 55,607 3,072 3,524 124,410 # tweets 519* 840 3,550 250 250 6,712 LAS 67.3 – 77.7 56.1 67.7 81.5 (parser) Malt2006 D&M2017 D&M2017 D&M2017 D&M2017 Table 3: Statistics for manually annotated treebanks (*Foster et al. only report # sentences, not # tweets. We expect the no. of tweets to be slightly lower than 500). The data of Blodgett et al. includes AAE and main-stream (MS) English tweets. The last two columns report results for the Dozat & Manning parser (Dozat et al., 2017) (w/o domain adaptation) or the Malt parser from the literature. 6 Related work Twitter treebanks exist not only for English (Kong et al., 2014; Liu et al., 2018; Blodgett et al., 2018) but also for Italian (Sanguinetti et al., 2018) and Arabic (Albogamy et al., 2017). Foster et al. (2011) were among the first to provide syntactic analyses for Twitter microtext. They created a testset with over 500 sentences extracted from tweets. The data was automatically parsed with a constituency parser and the trees were manually corrected by one annotator. Inter-annotator agreement (IAA) for labelled bra"
W19-7811,foth-etal-2014-size,0,0.0547846,"Missing"
W19-7811,W17-0407,0,0.0129448,"istent with the one for conditional clauses that are similar in meaning (e.g.: If I scroll down further, I can see more), where the subordinate if-clause is also an adverbial clausal modifier of the matrix clause. the more_constant the market_shares declined , the more_regular became reformed “The more consistently market shares declined, the more regularly reforms were carried out.” Figure 6: Comparative clause with je-desto in the TüBa-D/Z-UD. 4 Comparison to other German UD treebanks We now compare tweeDe to three other German treebanks, i) UD-GSD, ii) TüBa-D/Z and iii) UD-HDT. The UD-HDT (Hennig and Köhn, 2017) is a conversion of the Hamburg Dependency Treebank (Foth et 3 While this is the canonical order, it is also possible to switch the order of the matrix and subordinate clauses. Constructions without verbal predicates are also possible: Je mehr, desto lustiger. (The more, the merrier). 4 While these are less frequent than the canonical form with je-desto/umso, it is easy to find instances in a large corpus such as the DeWac (Baroni et al., 2009), as well as instances that include only the je without a second particle where the matrix clause then needs to be in V1 word order. Figure 7: Distribut"
W19-7811,D14-1108,0,0.0239314,"Missing"
W19-7811,N18-1088,0,0.0967366,"Missing"
W19-7811,P13-2017,0,0.07675,"ite (OOV: number of out-of-vocabulary words with regard to the training set; lower: OOV for lower-cased word forms). Inter-Annotator Agreement We computed IAA on a subset of the data with 1,630 tokens. For labelled attachments, the agreement between the two annotators was 0.83 κ, for unlabelled attachments the score increased to 0.89 κ. 3 Annotation decisions Below we discuss decisions we made during the annotation process that deviate from other existing German UD treebanks, i.e. the UD-GSD and the UD-TüBa-D/Z. UD-GSD has been converted from an earlier version of Stanford-style dependencies (McDonald et al., 2013) and contains mostly web reviews while the UD-TüBa-D/Z (Çöltekin et al., 2017) is a conversion of the TüBa-D/Z (Telljohann et al., 2004) and includes articles from a German daily newspaper. Placeholder sentences In the UD-GSD treebank, finite subordinate placeholder sentences with dass or ob (that, whether) are mostly analysed as ccomp while infinite correlates are annotated as acl and attached to the placeholder, usually a pronominal adverb. In contrast, the TüBa-D/Z attaches both finite and infinite placeholder clauses as adverbial clause to the verb of the matrix clause. We decided to annot"
W19-7811,nivre-etal-2006-maltparser,0,0.229204,"Missing"
W19-7811,petrov-etal-2012-universal,0,0.0416535,"Missing"
W19-7811,L18-1279,0,0.0752658,"Missing"
W19-7811,K17-3001,0,0.0557305,"Missing"
W19-7811,telljohann-etal-2004-tuba,0,0.0162673,"greement We computed IAA on a subset of the data with 1,630 tokens. For labelled attachments, the agreement between the two annotators was 0.83 κ, for unlabelled attachments the score increased to 0.89 κ. 3 Annotation decisions Below we discuss decisions we made during the annotation process that deviate from other existing German UD treebanks, i.e. the UD-GSD and the UD-TüBa-D/Z. UD-GSD has been converted from an earlier version of Stanford-style dependencies (McDonald et al., 2013) and contains mostly web reviews while the UD-TüBa-D/Z (Çöltekin et al., 2017) is a conversion of the TüBa-D/Z (Telljohann et al., 2004) and includes articles from a German daily newspaper. Placeholder sentences In the UD-GSD treebank, finite subordinate placeholder sentences with dass or ob (that, whether) are mostly analysed as ccomp while infinite correlates are annotated as acl and attached to the placeholder, usually a pronominal adverb. In contrast, the TüBa-D/Z attaches both finite and infinite placeholder clauses as adverbial clause to the verb of the matrix clause. We decided to annotate finite and infinite placeholder sentences as acl and attach both to their respective placeholder (figure 3). there belongs also a re"
