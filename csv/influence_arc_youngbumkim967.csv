2021.acl-long.95,2021.naacl-main.426,0,0.0340868,"2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) proposed a self-training method 1 Since SGD accounts for a large portion of the existing MR-to-Text data that SC-GPT utilized in training, we could not apply AUG NLG to SC-GPT for the F EW S HOT SGD task. Few-shot Transfer Learning for NLG The goal of NLG is to translate an MR A into  its natural language response x = x1 , . . . , xT , where xi is the ith token in the sequence x and T is the sequence length. A is defined as the combination of intent I and slot-value pairs {(si , vi )}Pi=1 : A = {I, (s1 , v1 ), . . . , (sP , vP )}, (1) where the intent stands for the illocutionary type of th"
2021.acl-long.95,P16-2008,0,0.0518135,"Missing"
2021.acl-long.95,N19-1409,0,0.0281745,"n-like, model-based NLG approaches, in particular neural models, have recently been gaining an increasing traction (Gao et al., 2018; Wen et al., 2015). However, neither the template-based approaches nor the model-based approaches are sufficiently scalable for large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots. With the rise of neural transfer learning for NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019). In particular, Peng et al. (2020) proposed F EW S HOT WOZ, the first NLG benchmark test in few-shot learning settings, and achieved a SOTA performance by leveraging existing MR-to-Text data sets via task-specific continued pre-training. Despite the improved result, their approach leaves little room for further improvements as MR-to-Text data are expensive to obtain for new domains, practically circling back to the same scalability problem after exhausting the existing data. In order to go beyond this restriction, this paper proposes AUG NLG, a novel data augmentation approach, that automatic"
2021.acl-long.95,D18-1045,0,0.030578,"020) and associated variants (Tran and Le Nguyen, 2017; Tran et al., 2017). Despite the improved flexibility and naturalness over template-based methods, neural approaches require large amounts of annotated data to reach good performance. Data Augmentation Data augmentation has been widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) proposed a self-training method 1 Since SGD accounts for a large portion of the existing MR-to-Text data that"
2021.acl-long.95,P18-5002,0,0.0260981,"ems provide a natural interface to achieve various daily-life tasks. Natural Language Generation (NLG) is a key component in such a system to convert the structured meaning representation (MR) to the natural language, as shown in Figure 1. In task-oriented dialogue systems, NLG is typically accomplished by filling out a basic set of developer-provided templates, leading to a conversational system generating unnatural, robotic responses. In order to make the system sound more human-like, model-based NLG approaches, in particular neural models, have recently been gaining an increasing traction (Gao et al., 2018; Wen et al., 2015). However, neither the template-based approaches nor the model-based approaches are sufficiently scalable for large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots. With the rise of neural transfer learning for NLP using pretrained LMs, recently, neural NLGs started to leverage transfer learning and showed some promising results (Radford et al., 2019; Brown et al., 2020; Dai et al., 2019; Edunov et al., 2019). In particular, Peng et al. (2020) proposed F EW S HOT WOZ, the first NLG benchmark test in few-shot learning s"
2021.acl-long.95,2020.acl-main.60,0,0.0225058,"l approaches require large amounts of annotated data to reach good performance. Data Augmentation Data augmentation has been widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) proposed a self-training method 1 Since SGD accounts for a large portion of the existing MR-to-Text data that SC-GPT utilized in training, we could not apply AUG NLG to SC-GPT for the F EW S HOT SGD task. Few-shot Transfer Learning for NLG The goal of NLG is to tran"
2021.acl-long.95,2020.findings-emnlp.372,0,0.0261917,"n, 2017; Tran et al., 2017). Despite the improved flexibility and naturalness over template-based methods, neural approaches require large amounts of annotated data to reach good performance. Data Augmentation Data augmentation has been widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) proposed a self-training method 1 Since SGD accounts for a large portion of the existing MR-to-Text data that SC-GPT utilized in training, we could not app"
2021.acl-long.95,K17-1044,0,0.0435774,"Missing"
2021.acl-long.95,W17-5528,0,0.0376934,"Missing"
2021.acl-long.95,N16-1015,0,0.0603802,"Missing"
2021.acl-long.95,D15-1199,0,0.0689846,"Missing"
2021.acl-long.95,D19-1375,0,0.0283275,"cˇ ek, 2016), SC-LSTM (Wen et al., 2016), T2G2 (Kale and Rastogi, 2020), AdapterCL (Madotto et al., 2020) and associated variants (Tran and Le Nguyen, 2017; Tran et al., 2017). Despite the improved flexibility and naturalness over template-based methods, neural approaches require large amounts of annotated data to reach good performance. Data Augmentation Data augmentation has been widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) propose"
2021.acl-long.95,P16-1009,0,0.0208601,"erCL (Madotto et al., 2020) and associated variants (Tran and Le Nguyen, 2017; Tran et al., 2017). Despite the improved flexibility and naturalness over template-based methods, neural approaches require large amounts of annotated data to reach good performance. Data Augmentation Data augmentation has been widely applied to a variety of NLP tasks, including sentence classification (Xie et al., 2020), natural language inference (Hu et al., 2019) and spoken language understanding (Li et al., 2019; Quan and Xiong, 2019; Zhao et al., 2019). Prior approaches for text data utilized back-translation (Sennrich et al., 2016; Edunov et al., 2018), c-BERT word replacement (Jiao et al., 2020), mixed labels and representations (Guo et al., 2019; Chen et al., 2020) and paraphrase data (Gao et al., 2020). However, the range of augmented data will be inherently limited, particularly in few-shot learning settings due to the nature of prior approaches, which only leverages in-domain data. In contrast, we take a rarely explored approach, tapping into a wealth of opendomain text that covers almost all topics. Recently, Du et al. (2021) proposed a self-training method 1 Since SGD accounts for a large portion of the existing"
2021.emnlp-main.489,P18-5002,0,0.0178069,"evious request (frephrase in Section 3). There are many ways to build these two mechanisms, either rule-based or model-based. Due to space limitation, we leave more details of the two mechanisms outside the scope of this paper. For completeness and better context to the reader however, we briefly describe various ways to build them, which would be straight-forward to adapt and implement. The NLU component typically has three main types of underlying models - domain classifiers, intent classifiers, and slot taggers (El-Kahky et al., 2014). The three modeling tasks can be treated independently (Gao et al., 2018) or as a joint optimization task (Liu and Lane, 2016; Hakkani-Tür et al., 2.1 User Dissatisfaction Detection 2016), and some systems have a model to rank Unless we specifically solicit users’ feedback on across all domains, intents and slots on a certain satisfaction after an experience, user feedback is unit of semantic interpretation (Su et al., 2018). mostly implicit. There are many implicit user beLeveraging implicit feedback from the users has been widely studied in the context of recommen- havior signals that can help with detecting user disdation systems (Hu et al., 2008; He et al., 201"
2021.emnlp-main.489,P17-1089,0,0.0225352,"Ponnusamy et al., 2019), improving the error-handling system responses (""I don’t know Natural Language Generation component (Zhang that one.""), the templates executed for generatet al., 2018), and using user engagement signals ing natural language error-handling responses (the for improving the entity labeling task specifically focused on Music domain (Muralidharan et al., song entity is not found for playing music), and the absence of a response (Beaver and Mueen, 2020; 2019). We note that compared to explicit feedback Sarikaya, 2017). There are also component-level (Petrushkov et al., 2018; Iyer et al., 2017), using signals such as latency or low confidence scores implicit feedback is more scalable and does not for the underlying models within each component introduce friction in user experience. But it comes such as ASR or NLU. with a challenge of the feedback being noisy, and leveraging the feedback is more difficult when there For more advanced approaches, we can combine is no sufficient data such as for improving tail cases the signals from the user behavior and the system (Wang et al., 2021a,b). together, try to model user interaction patterns, and In this paper, we specifically focus on two"
2021.emnlp-main.489,2020.findings-emnlp.347,1,0.711683,"ience. But it comes such as ASR or NLU. with a challenge of the feedback being noisy, and leveraging the feedback is more difficult when there For more advanced approaches, we can combine is no sufficient data such as for improving tail cases the signals from the user behavior and the system (Wang et al., 2021a,b). together, try to model user interaction patterns, and In this paper, we specifically focus on two types use additional context from past interaction history of implicit user feedback - dissatisfaction of expe- beyond immediate turns (Jiang et al., 2015; Ultes 6055 and Minker, 2014; Bodigutla et al., 2020). Furthermore, user satisfaction can depend on usage scenarios (Kiseleva et al., 2016), and for specific experiences like listening to music, we can adapt related concepts such as dwell time in the search and information retrieval fields to further fine-tune. 2.2 User Rephrase Detection There are many lines of work in the literature that are closely related to this task under the topics of text/sentence semantic similarity detection and paraphrase detection. The approaches generally fall into lexical matching methods (Manning and Schutze, 1999), leveraging word meaning or concepts with a knowl"
2021.emnlp-main.489,P18-2052,0,0.0201937,"include generic ponent (Ponnusamy et al., 2019), improving the error-handling system responses (""I don’t know Natural Language Generation component (Zhang that one.""), the templates executed for generatet al., 2018), and using user engagement signals ing natural language error-handling responses (the for improving the entity labeling task specifically focused on Music domain (Muralidharan et al., song entity is not found for playing music), and the absence of a response (Beaver and Mueen, 2020; 2019). We note that compared to explicit feedback Sarikaya, 2017). There are also component-level (Petrushkov et al., 2018; Iyer et al., 2017), using signals such as latency or low confidence scores implicit feedback is more scalable and does not for the underlying models within each component introduce friction in user experience. But it comes such as ASR or NLU. with a challenge of the feedback being noisy, and leveraging the feedback is more difficult when there For more advanced approaches, we can combine is no sufficient data such as for improving tail cases the signals from the user behavior and the system (Wang et al., 2021a,b). together, try to model user interaction patterns, and In this paper, we specif"
2021.emnlp-main.489,D19-1410,0,0.0133277,"d information retrieval fields to further fine-tune. 2.2 User Rephrase Detection There are many lines of work in the literature that are closely related to this task under the topics of text/sentence semantic similarity detection and paraphrase detection. The approaches generally fall into lexical matching methods (Manning and Schutze, 1999), leveraging word meaning or concepts with a knowledge base such as WordNet (Mihalcea et al., 2006), latent semantic analysis methods (Landauer et al., 1998), and those based on word embeddings (Camacho-Collados and Pilehvar, 2018) and sentence embeddings (Reimers and Gurevych, 2019). In terms of modeling architecture, Siamese network is common and has been applied with CNN (Hu et al., 2014), LSTM (Mueller and Thyagarajan, 2016), and BERT (Reimers and Gurevych, 2019). The task is also related to the problems in community question-answering systems for finding semantically similar questions and answers (Srba and Bielikova, 2016). 2.3 Problem Definition consecutive turns by the same user. Denote mt to be the NLU component at timestamp t. We collect the interaction session data Slive = {s1 , s2 , . . . , sn } from live traffic for a certain period of time ∆ (e.g., one week)"
2021.emnlp-main.489,W14-4328,0,0.0203326,"Missing"
2021.emnlp-main.489,2021.findings-acl.133,1,0.717951,"ompared to explicit feedback Sarikaya, 2017). There are also component-level (Petrushkov et al., 2018; Iyer et al., 2017), using signals such as latency or low confidence scores implicit feedback is more scalable and does not for the underlying models within each component introduce friction in user experience. But it comes such as ASR or NLU. with a challenge of the feedback being noisy, and leveraging the feedback is more difficult when there For more advanced approaches, we can combine is no sufficient data such as for improving tail cases the signals from the user behavior and the system (Wang et al., 2021a,b). together, try to model user interaction patterns, and In this paper, we specifically focus on two types use additional context from past interaction history of implicit user feedback - dissatisfaction of expe- beyond immediate turns (Jiang et al., 2015; Ultes 6055 and Minker, 2014; Bodigutla et al., 2020). Furthermore, user satisfaction can depend on usage scenarios (Kiseleva et al., 2016), and for specific experiences like listening to music, we can adapt related concepts such as dwell time in the search and information retrieval fields to further fine-tune. 2.2 User Rephrase Detection"
2021.findings-acl.133,N19-1423,0,0.195543,"The slice distributions are computed in deterministic (weighted sum of slices) or stochastic (sampling) way in re-weighting r and x. In this paper, we extend SBL with a mixture of attentions (MoA) mechanism. Two different attention mechanisms are learned to jointly attend to the defined slices from different representations in different latent subspaces. The first attention is based on slice membership likelihood and/or experts confidence as in SBL (Chen et al., 2019), which we call membership attention. The second one is dot-product attention that is based on the backbone model (e.g., BERT (Devlin et al., 2019)) extracted representations. The MoA approach is akin to multi-head attention (Vaswani et al., 2017) but with different attention types that receive different inputs. As presented in Figure 1, the two attentions in MoA can work jointly to attend to (1) the expert rep1530 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1530–1536 August 1–6, 2021. ©2021 Association for Computational Linguistics def SF_Length(utterance,k=10): return len(utterance) &lt; k def SF_Long(sentence,k=10): n = len(sentence .split(’ ’)) return n &gt; k def SF_Time(utterance): return &quot;time&quot; in u"
2021.findings-acl.133,S19-1026,0,0.0448107,"Missing"
2021.findings-acl.133,N18-3003,1,0.884126,"Missing"
2021.findings-acl.133,P18-1206,1,0.818896,"l utterances are encoded with BERT which captures the similarity between the sample and the utterances in the “Email” slice. 5 SBL has been recently used in many applications. Penha et al. (Penha and Hauff, 2020) proposed to adapt SBL to improve ranking performance and capture the failures of the ranker model. Wang et al. (Wang et al., 2021) recently implemented SBL in a commercial conversational AI system in order to handle the long-tail problem of imbalanced distribution in customer queries and further improved the performance of the conversational skill routing components (Li et al., 2021; Kim et al., 2018b,a). Our proposed mixture of attention (MoA) is an instance of multi-head attention (Vaswani et al., 2017) but with different attention types. MoA can also be extended to include other attention types. We have shown the effectiveness of this mechanism in determining the slice distributions. 6 Conclusion This paper extends SBL with MoA (SBL-MoA) to improve model performance on particular data slices. We empirically show that SBL-MoA yields better slice level performance lift to baseline and vanilla SBL with two NLU tasks: linguistic acceptability and intent detection. Related Work SBL (Chen et"
2021.findings-acl.133,D18-1317,0,0.0378396,"Missing"
2021.findings-acl.133,P07-1125,0,0.119548,"Missing"
2021.findings-acl.133,2020.scai-1.1,0,0.0383908,"slice distributions given some random samples. We denote p1 and p2 for membership and dot-product attention respectively. The experiments show that p1 and p2 reach an agreement on predicting the correct slices. Interestingly, the sample in (d) — “write sms to our friends”, in principle, should be sliced as “base”, but both attentions exhibit high confidence to s3=“Email”. We conjecture the reason is that all utterances are encoded with BERT which captures the similarity between the sample and the utterances in the “Email” slice. 5 SBL has been recently used in many applications. Penha et al. (Penha and Hauff, 2020) proposed to adapt SBL to improve ranking performance and capture the failures of the ranker model. Wang et al. (Wang et al., 2021) recently implemented SBL in a commercial conversational AI system in order to handle the long-tail problem of imbalanced distribution in customer queries and further improved the performance of the conversational skill routing components (Li et al., 2021; Kim et al., 2018b,a). Our proposed mixture of attention (MoA) is an instance of multi-head attention (Vaswani et al., 2017) but with different attention types. MoA can also be extended to include other attention"
2021.findings-acl.133,2021.tacl-1.4,0,0.0635549,"Missing"
2021.naacl-main.319,D16-1230,0,0.0227888,"ew-shot transfer learning. • We conduct extensive experiments using data from a large-scale commercial conversational system, demonstrating significant improvements to label efficiency and generalization. 2 Related Work User Satisfaction in Conversational Systems interact with a human user. For instance, using generic metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) one can measure how the agent’s responses are consistent with a set of provided ground-truth answers. However, these approaches not only suffer from shortcomings such as inconsistency with the human understanding (Liu et al., 2016; Novikova et al., 2017) but also are not practical for a real-world conversation system due to their dependence on ground-truth responses. A more recent approach is to use human annotations specifically tailored for the user satisfaction task as a source of supervision to train end-to-end prediction models (Bodigutla et al., 2019). Jiang et al. (2015) suggested training individual models for 6 general skills and devised engineered features to link user actions to the user satisfaction for each studied skill. Park et al. (2020) proposed a hybrid method to learn from human annotation and user f"
2021.naacl-main.319,2021.ccl-1.108,0,0.0823378,"Missing"
2021.naacl-main.319,W17-5520,0,0.0170967,"ure inner loop optimization process and is scalto select the most satisfying response among a set able to very large datasets and complex modof candidates and hence guiding the conversation. els. Based on our experiments using real data from a large-scale commercial system, the sugThe problem of user satisfaction modeling has regested approach is able to significantly reduce cently attracted significant research attention (Jiang the required number of annotations, while imet al., 2015; Bodigutla et al., 2019; Park et al., proving the generalization on unseen skills. 2020; Pragst et al., 2017; Rach et al., 2017). These methods either rely on annotated datasets providing 1 Introduction ground-truth labels to train and evaluate (Bodigutla Nowadays automated conversational agents such et al., 2019) or rely on ad hoc or human-engineered as Alexa, Siri, Google Assistant, Cortana, etc. are metrics that do not necessarily model the true user widespread and play an important role in many dif- satisfaction (Jiang et al., 2015). Access to reliferent aspects of our lives. Their applications vary able annotations to be used in building satisfaction from storytelling and education for children to as- models has b"
2021.naacl-main.319,D17-1238,0,0.0128731,"earning. • We conduct extensive experiments using data from a large-scale commercial conversational system, demonstrating significant improvements to label efficiency and generalization. 2 Related Work User Satisfaction in Conversational Systems interact with a human user. For instance, using generic metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) one can measure how the agent’s responses are consistent with a set of provided ground-truth answers. However, these approaches not only suffer from shortcomings such as inconsistency with the human understanding (Liu et al., 2016; Novikova et al., 2017) but also are not practical for a real-world conversation system due to their dependence on ground-truth responses. A more recent approach is to use human annotations specifically tailored for the user satisfaction task as a source of supervision to train end-to-end prediction models (Bodigutla et al., 2019). Jiang et al. (2015) suggested training individual models for 6 general skills and devised engineered features to link user actions to the user satisfaction for each studied skill. Park et al. (2020) proposed a hybrid method to learn from human annotation and user feedback data that is sca"
2021.naacl-main.319,P02-1040,0,0.117488,"e-of-the-art deep language models and the acquired knowledge is transferable to the user satisfaction prediction. • We suggest a novel and scalable few-shot transfer learning approach that is able to improve the label efficiency even further in the case of few-shot transfer learning. • We conduct extensive experiments using data from a large-scale commercial conversational system, demonstrating significant improvements to label efficiency and generalization. 2 Related Work User Satisfaction in Conversational Systems interact with a human user. For instance, using generic metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) one can measure how the agent’s responses are consistent with a set of provided ground-truth answers. However, these approaches not only suffer from shortcomings such as inconsistency with the human understanding (Liu et al., 2016; Novikova et al., 2017) but also are not practical for a real-world conversation system due to their dependence on ground-truth responses. A more recent approach is to use human annotations specifically tailored for the user satisfaction task as a source of supervision to train end-to-end prediction models (Bodigutla et al., 2019). Jiang et al."
C16-1038,J92-4003,0,0.527356,"27 97.27 92.30 Union 89.24 82.09 81.79 89.91 58.74 84.54 77.76 79.21 79.87 63.21 89.48 85.71 87.46 84.25 74.45 79.22 91.27 81.07 Daum´e III (2009) 97.36 94.4 96.6 92.61 89.34 92.76 92.3 94.43 89.95 90.86 96.44 95.78 94.13 94.02 94.43 94.75 98.44 94.04 Table 2: F1 scores across seventeen personal assistant domains for Noadapt, Union and Daum´e III (2009) in a setting with sparse binary-valued features. 6.2 Results For non-neural models, we use conditional random fields (CRFs) (Lafferty et al., 2001) with a rich set of binary-valued features such as lexical features, gazetteers, Brown clusters (Brown et al., 1992) and context words. For parameter estimation, we used L-BFGS (Liu and Nocedal, 1989) with 100 as the maximum iteration count and 1.0 for the L2 regularization parameter. Their results are shown in Table 2. A few observations: the model without domain adaptation (Noadapt) is already very competitive because we have sufficient training data. However, simply training a single model with aggregated queries across all domains significantly degrades performance (Union). This is because in many cases the same query is labeled differently depending on the domain and the 393 context. This is also becau"
C16-1038,J81-4005,0,0.696695,"Missing"
C16-1038,P15-1033,0,0.00953814,"tially defines a feedforward network in which the same set of parameters are used multiple times and is well-suited for sequential data. But a simple RNN suffers from the vanishing gradient problem and does not model long-term dependency very well (Pascanu et al., 2013). An LSTM addresses this issue by introducing a memory cell in RNN architecture that can control how much past information to retain or to forget. This modification has been shown to be crucial in practice. Many recent works in NLP have achieved state-of-the-art results using variants of LSTM, for example in dependency parsing (Dyer et al., 2015) and machine translation (Bahdanau et al., 2014). 389 4 A Neural Extension of the Feature Augmentation Method We now describe a neural extension of the feature augmentation method for domain adaptation (Daum´e III, 2009). Our model consists of K + 1 LSTMs: one LSTM θ is used on all domains, and the remaining K LSTMs θ1 . . . θK are used only for the corresponding domains. More specifically, we predict one of L labels at the t-th time step in a given user query in domain k ∈ {1 . . . K} as follows. The common LSTM θ produces an output vector ht ∈ Rd and the domain-specific LSTM θk produces an o"
C16-1038,N15-1009,1,0.716868,"d across all domains and independent LSTMs used within individual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation an"
C16-1038,P15-2132,1,0.899567,"d across all domains and independent LSTMs used within individual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation an"
C16-1038,P15-2032,1,0.908968,"d across all domains and independent LSTMs used within individual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation an"
C16-1038,D16-1222,1,0.815342,"dividual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation and sequence modeling. First, we review the feature augment"
C16-1038,P16-2002,1,0.842343,"dividual domains, and then combining their outputs in the top layer. Second, we propose using the framework for learning predictive structures by Ando and Zhang (2005) for domain adaptation which has not previously been considered for this task (the original work only considers multi-tasking in the context of semi-supervised learning): we likewise consider a neural extension of this framework. We perform slot tagging experiments on 17 different personal digital assistant domains that Cortana handles (Tur, 2006; Anastasakos et al., 2014; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2016a; Kim et al., 2016b). Our methods give clear performance improvement over naive baselines such as training K independent models on individual domains or training one model on the union of all domains. Our methods also significantly outperform the feature augmentation method of Daum´e III (2009) with standard sparse binary features implemented with conditional random fields (CRFs). The rest of the paper is organized as follows. In Section 2, we discuss related works. In Section 3, we provide background information on domain adaptation and sequence modeling. First, we review the feature augment"
C16-1038,D14-1162,0,0.106812,"cument Edit and create note Set up a phone Order food using app Find location and direction Remind appointment and to-do list Make a restaurant reservations Find and book a cab Ask weather Table 1: Data sets used in the experiments. For each domain, the number of unique labels, the number of quires in the training, development, and test sets, input vocabulary size of the training set, and short description about domain. the development set in preliminary experiments. To initialize word embedding, we used word embedding trained on 6 billion tokens (6B-200d) from Wikipedia 2014 plus Gigaword 5 (Pennington et al., 2014). These configurations were selected by observing the models performance on held-out development set. We compare the following methods for the slot tagging tasks: • NoAdapt: train a feature-rich CRF only on target training data. • Union: train a feature-rich CRF on the union of source and target training data. • Daume: train a feature-rich CRF with the discrete feature duplication method of Daum´e III (2009). • 1D&E: train a domain specific LSTM with a generic embedding on all domain training data, shown on the right in Figure 2. • 1D&L: train a domain specific LSTM with a generic LSTM on all"
C16-1193,W06-1615,0,0.254209,"n attempt to improve performance on any particular domain. There is a rich body of work in domain adaptation for natural language processing. A notable example is the feature augmentation method of Daum´e III (2009), who propose partitioning the model parameters to those that handle common patterns and those that handle domain-specific patterns. This way, the model is forced to learn from all domains yet preserve domain-specific knowledge. Another domain adaptation technique used in natural language processing utilizes unlabeled data in source and target distributions to find shared patterns (Blitzer et al., 2006; Blitzer et al., 2011). This is achieved by finding a shared subspace between the two domains through singular value decomposition (SVD). Unlike the feature augmentation method of Daum´e III (2009), however, it does not leverage labeled data in the target domain. This work is rather different from the conventional works in domain adaptation in that we remove the need to distinguish domains: we have a single model that can handle arbitrary (including unknown) domains. Among other benefits, this approach removes the error propagation due to domain misclassification. Most domain adaptation metho"
C16-1193,J92-4003,0,0.551064,"umbers of training, test and development queries across domains are 2964K, 217K and 153K, respectively. Note that we keep domain-specific slots such as alarm state, but there are enough shared labels across domains. To be specific, we have shared 62 labels among 131 labels. In ALARM domain, there are 6 shared slots among 8 slots. 4.2 Results In all our experiments, we follow same setting as in (Kim et al., 2015b; Kim et al., 2015c). We trained Conditional Random Fields (CRFs)(Lafferty et al., 2001) and used n-gram features up to n = 3, regular expression, lexicon features, and Brown Clusters (Brown et al., 1992). With these features, we compare the following methods for slot tagging1 : • In-domain: Train a domain-specific model using the domain-specific data covering the slots supported in that domain. • Binary: Train a binary classifier for each slot type, assuming prediction for each slot type is independent of one another. Then combine the classification result with the slots needed for a given schema. For each binary slot tagger targeting a specific slot type, the labeled data is programatically 1 For parameter estimation, we used L-BFGS (Liu and Nocedal, 1989) with 100 as the maximum number of i"
C16-1193,P15-1033,0,0.0374903,"Missing"
C16-1193,N15-1009,1,0.690831,"f slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., qu"
C16-1193,P15-2132,1,0.872887,"f slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., qu"
C16-1193,P15-2032,1,0.830449,"f slot sequences allowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., qu"
C16-1193,D16-1222,1,0.817385,"lowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., query topics), compos"
C16-1193,P16-2002,1,0.807908,"lowed under the schema. The proposed model achieves competitive and often superior performance over the conventional model trained separately per domain. 1 Introduction Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015). Apple’s Siri, Google Now, Microsoft’s Cortana, and Amazon’s Alexa are some examples of personal digital assistants. Spoken language understanding is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014; Kim et al., 2015a; Kim et al., 2016b). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them. The number of domains supported by these systems constantly increases, and whether there is a method that allows us to easily scale to a larger number of domains is an unsolved problem (Kim et al., 2015d; Kim et al., 2016a). The main reason behind the need for additional domains is that we require a new set of schema (i.e., query topics), compos"
C16-1193,D14-1162,0,0.0813171,"existing slots and intents. In cases where the new domain needs an a new intent or slot, the underlying generic models have to updated with the updated schema. 2052 3.1 Schema Prediction The first stage produces a set of label types that serve as constraints in the second stage. To this end, we use Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) (Figure 1). The LSTM processes the given query to produce a fixed-size vector where the input at each time step is the word embedding corresponding to the word used at the time. We initialize these word embeddings with GloVe vectors (Pennington et al., 2014). Then the network maps the query vector to a distribution over schema types. In more detail, we first map each word of the utterance into d-dimensional vector space using an embedding matrix of size V by d (which is trained along with other parameters in the network), where V is the size of the vocabulary. Then we map the sequence of the word vectors, {x1 , . . . xT }, to LSTM outputs {h1 , . . . hT } where we take the last output to be a d-dimensional summary vector of the utterance s = hT . We then use parameters W ∈ Rk×d and b ∈ Rk where k is the number of slot types and compute yˆ = softm"
C16-1193,Q13-1001,0,0.069055,"Missing"
C18-1210,I05-2034,0,0.135444,"Missing"
C18-1210,E99-1023,0,0.176274,"inguistic knowledge, lattice tree lookup (Park et al., 2010), application of regular and irregular inflection rules (Kang and Kim, 1992), morphosyntactic rule sets, and by using a pre-computed dictionary (Shim and Yang, 2004). However, we investigate whether morphological analysis of Korean is feasible without the use of any of these techniques and without a dictionary by making the assumption that common transformations and their underlying grapheme modifications can be easily recognized and learned with a Bi-LSTM model. Bi-LSTM-CRFs have been used for sequential tagging with BIO annotation (Sang and Veenstra, 1999). Huang, et al (2015) show their effectiveness for POS tagging, chunking, named entity recognition (NER). These models show state-of-the-art accuracy at several tasks. Similar models have also been proposed in universal morphological analysis. Heigold, et al (2016b) show how a nested LSTM architecture can be applied to word-level morphological tagging for a wide variety of languages. At the lower level, an LSTM network is used for character-level embedding to reduce OOV errors. However, this work does not investigate how such a model would operate for the most widely used Korean Sejong Corpus."
C18-1273,E09-1087,0,0.0374769,"Missing"
C18-1273,D16-1223,0,0.0670097,"Missing"
C18-1273,N16-1030,0,0.686004,"f-the-art results in English. In Ma and Hovy (2016), a Bi-LSTM-CRF model incorporated with a character-level CNN is trained in an end-to-end fashion. They evaluated this approach on English POS tagging and NER, achieving state-of-the-art performance on both tasks. However, feature vectors generated by CNN are position-independent due to the max-over-time pooling layer, and are more sensitive to model weight initialization compared to the method proposed in this paper. Another effective way of generating feature vectors from a variable length sequence of characters is to use RNN. For instance, Lample et al. (2016) extracted character-level features using a bi-directional LSTM and used them with pre-trained word embeddings as word representations for another Bi-LSTMCRF model. Evaluating this model for NER, they obtained state-of-the-art results for Dutch, German, and Spanish, and close to state-of-the-art results for English. Intuitively, character-level feature generation via RNN should be more effective than CNN, since RNN processes each character sequentially and thus should form a better model of character ordering. However, Reimers and Gurevych (2017) empirically showed that these two methods have"
C18-1273,P09-1116,0,0.0719401,"Missing"
C18-1273,D15-1104,0,0.0355852,"Missing"
C18-1273,P16-1101,0,0.339829,"extracting character-level features from words is crucial in many Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and Slot tagging. Thus, most state-of-the-art methods for these tasks exploit some kind of character-level features (Huang et al., 2015; dos Santos and Zadrozny, 2014). Recently, generating character-level features with neural architectures such as Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN) has drawn much attention, mainly because it doesn’t require human labor and shows superior performance (Ma and Hovy, 2016; dos Santos and Zadrozny, 2014). However, CNN struggles at distinguishing anagrams, and RNN is inherently slow due to its sequential nature. In this paper, we propose an effective and efficient way of extracting character-level features using a densely connected network. The key benefits of the proposed method can be summarized as follows. First, it does not require any hand-crafted features or data preprocessing. Each word is processed based on n-gram statistics of the training data, and vectorized using bag-of-characters. Additional features are based on hexadecimal values of the character-"
C18-1273,J93-2004,0,0.0635891,"Missing"
C18-1273,D15-1151,0,0.025816,"Missing"
C18-1273,W14-1609,0,0.0268036,"Missing"
C18-1273,D14-1162,0,0.0766034,"Missing"
C18-1273,W09-1119,0,0.157669,"Missing"
C18-1273,D17-1035,0,0.105261,"th sequence of characters is to use RNN. For instance, Lample et al. (2016) extracted character-level features using a bi-directional LSTM and used them with pre-trained word embeddings as word representations for another Bi-LSTMCRF model. Evaluating this model for NER, they obtained state-of-the-art results for Dutch, German, and Spanish, and close to state-of-the-art results for English. Intuitively, character-level feature generation via RNN should be more effective than CNN, since RNN processes each character sequentially and thus should form a better model of character ordering. However, Reimers and Gurevych (2017) empirically showed that these two methods have no statistically significant difference in terms of performance. Furthermore, RNN has a higher time-complexity caused by its sequential nature, which makes it less favorable. 3 Proposed Method The proposed method is built on bag-of-characters (BOC) representation. However, BOC is prone to anagrams and thus is susceptible to word collisions, i.e. different words having the same vector representation. The main focus of the proposed method is to minimize word collision while maintaining the key benefits described above. To achieve this goal, we spli"
C18-1273,P07-1096,0,0.0307004,"Missing"
C18-1273,P11-2009,0,0.0386908,"Missing"
C18-1273,W03-0419,0,0.150539,"Missing"
C18-1273,N03-1033,0,0.218595,"Missing"
C18-1273,D14-1101,0,0.0332051,"Missing"
D11-1030,P06-1084,0,0.0675846,"Missing"
D11-1030,W09-0106,0,0.0129859,"phological analysis for each unlabeled language which lies as close as possible in the feature space to a training language. We apply our model to eight inflecting languages, and induce nominal morphology with substantially higher accuracy than a traditional, MDLbased approach. Our analysis indicates that accuracy continues to improve substantially as the number of training languages increases. 1 Introduction Over the past several decades, researchers in the natural language processing community have focused most of their efforts on developing text processing tools and techniques for English (Bender, 2009), a morphologically simple language. Recently, increasing attention has been paid to the wide variety of other languages of the world. Most of these languages still pose severe difficulties, due to (i) their 322 lack of annotated textual data, and (ii) the fact that they exhibit linguistic structure not found in English, and are thus not immediately susceptible to many traditional NLP techniques. Consider the example of nominal part-of-speech analysis. The Penn Treebank defines only four English noun tags (Marcus et al., 1994), and as a result, it is easy to treat the words bearing these tags"
D11-1030,P10-1131,0,0.0677835,"k and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. x. We thus generalize nearest-neighbor prediction to the structured scenario and propose the following prediction rule: 3 where the index ` ranges over the training languages. In words, we predict the morphological analysis y for our test language which places it as close as possible in the universal feature space to one of the training la"
D11-1030,W10-2906,0,0.0442234,"se cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. x. We thus generalize nearest-neighbor prediction to the structured"
D11-1030,N09-1009,0,0.0670507,"gan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be used as training examples for our test language. x. We thus generalize nearest-neighbor prediction to the structured scenario and propose the following prediction rule: 3 where the index ` ranges over the training languages. In words, we predict the morphological analysis y for our test language which places it as close as possible in the universal feat"
D11-1030,W02-1001,0,0.361232,", and some deletion rule d ∈ D. We then consider the joint segmentation of these words into a new stem s, and their respective suffixes. As before, we choose the segmentation if it brings us closer in feature space to our target training language. Stage 3: Find New Suffixes This stage is exactly analogous to the previous stage, except we now fix the set of stems T and seek to find new suffixes. 3.2 A Monolingual Supervised Model In order to provide a plausible upper bound on performance, we also formulate a supervised monolingual morphological model, using the structured perceptron framework (Collins, 2002). Here we assume that we are given some training sequence of inputs and morphological analyses (all within one language): (x1 , y1 ), (x2 , y2 ), . . . , (xn , yn ). We define each input xi to be a noun w, along with a morphological tag z, which specifies the gender, case, and number of the noun. The goal is to predict the correct segmentation of w into stem, suffix, and phonological deletion rule: yi = (t, f, d).6 To do so, we define a feature function over inputlabel pairs, (x, y), with the following binary feature templates: (1) According to label yi , the stem is t 6 While the assumption o"
D11-1030,P91-1017,0,0.1633,"nd his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith,"
D11-1030,D07-1023,0,0.0371429,"Missing"
D11-1030,erjavec-2004-multext,0,0.0554785,"still pose severe difficulties, due to (i) their 322 lack of annotated textual data, and (ii) the fact that they exhibit linguistic structure not found in English, and are thus not immediately susceptible to many traditional NLP techniques. Consider the example of nominal part-of-speech analysis. The Penn Treebank defines only four English noun tags (Marcus et al., 1994), and as a result, it is easy to treat the words bearing these tags as completely distinct word classes, with no internal morphological structure. In contrast, a comparable tagset for Hungarian includes 154 distinct noun tags (Erjavec, 2004), reflecting Hungarian’s rich inflectional morphology. When dealing with such languages, treating words as atoms leads to severe data sparsity problems. Because annotated resources do not exist for most morphologically rich languages, prior research has focused on unsupervised methods, with a focus on developing appropriate inductive biases. However, inductive biases and declarative knowledge are notoriously difficult to encode in well-founded models. Even putting aside this practical matter, a universally correct inductive bias, if there is one, is unlikely to be be discovered by a priori rea"
D11-1030,J01-2001,0,0.0684778,"l induction, as well as multilingual analysis in NLP. 323 Unsupervised Morphological Induction: Unsupervised morphology remains an active area of research (Schone and Jurafsky, 2001; Goldsmith, 2005; Adler and Elhadad, 2006; Creutz and Lagus, 2005; Dasgupta and Ng, 2007; Creutz and Lagus, 2007; Poon et al., 2009). Many existing algorithms derive morpheme lexicons by identifying recurring patterns in words. The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings, resulting in a minimum description length (MDL) lexicon and corpus (Goldsmith, 2001; Goldsmith, 2005). Later work cast this idea in a probabilistic framework in which the the MDL solution is equivalent to a MAP estimate in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators"
D11-1030,P06-1146,0,0.0533017,"nalysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al.,"
D11-1030,N09-1024,0,0.0369263,"Missing"
D11-1030,W97-0213,0,0.136431,"first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and"
D11-1030,N01-1024,0,0.0459439,"Missing"
D11-1030,P08-1084,1,0.880902,"ction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any"
D11-1030,D08-1109,1,0.894631,"01), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of"
D11-1030,P09-1009,1,0.882357,"chers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be use"
D11-1030,N09-1010,1,0.87877,"chers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009b; Naseem et al., 2009), and grammar induction (Snyder et al., 2009a; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method proposed in this paper does not assume the existence of any parallel text, but does assume that labeled data exists for a wide variety of languages, to be use"
D11-1030,N01-1026,0,0.107393,"). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder and Barzilay, 2008a), part-of-speech induction (Sny"
D11-1030,P00-1027,0,0.0923545,"te in a suitable Bayesian model (Creutz and Lagus, 2005). In all these approaches, a locally optimal segmentation is identified using a task-specific greedy search. Multilingual Analysis: An influential line of prior multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008b; Snyder an"
D11-1030,J93-2004,0,\N,Missing
D11-1030,H01-1035,0,\N,Missing
D12-1031,P10-1131,0,0.0350459,"k and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languages. In this regard, our work most closely resembles recent work which trains a universal morphological analyzer usa c ch e h i j o ph q r th u v w x z phonemes /a/ /5/ /A/ /@/ /2/ &gt; &gt; &gt; /c/ /dZ/ /k/ /s/ /ts/ /tS/ /|/ &gt; /k/ /tS/ /x/ /S/ /e/ /i/ /æ/ /@/ /E/ /-/ /h/ /x/ /ø/ /H/ /i/ /j/ /I/ &gt; &gt; /dZ/ /h/ /j/ /tS/ /x/ /é/ /Z/ /o/ /u/ /6/ /0/ /f/ /ph / /k/ /q/ /!/ /r/ /ó/ /R/ /ö/ /K/ /th / /T/ /u/ /w/ /y/ /1/ /U"
D12-1031,P09-1088,0,0.0137742,"d Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languages. In this regard, our work most closely resembles recent work which trains a universal morphological analyzer"
D12-1031,W10-2906,0,0.0187753,"se cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languages. In this regard, our work most closely resembles recent work which trains a universal morphological analyzer usa c ch e h i j o ph"
D12-1031,N09-1009,0,0.0316525,"gan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languages. In this regard, our work most closely resembles recent work which trains a universal morphological analyzer usa c ch e h i j o ph q r th u v w x z phonemes /a/ /5/ /A/ /@/ /2/ &gt; &gt; &gt; /c/ /dZ/ /k/ /s/ /ts/ /tS/ /|/ &gt; /k/ /tS/ /x/ /S/ /e/ /i/ /æ/ /@/ /E/ /-/ /h/ /x/ /ø/ /H/ /i/ /j/ /I/ &gt; &gt; /dZ/ /h/ /j/ /tS/ /x/ /é/ /Z/ /o/ /u/ /6/ /0/ /f/ /ph / /k/ /q/ /!/ /r/ /ó/ /R/ /"
D12-1031,P91-1017,0,0.130463,"nd his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith,"
D12-1031,P09-1015,0,0.021823,"h prior work has gone into developing methods for accurate grapheme-to-phoneme prediction. The common assumption underlying this research has been that some sort of knowledge, usually in the form of a pronunciation dictionary or phonemically annotated text, is available for the language at hand. The focus has been on developing techniques for dealing with the phonemic ambiguity present both in annotated and unseen words. For example, Jiampojamarn and Kondrak (Jiampojamarn and Kondrak, 2010) develop a method for aligning pairs of written and phonemically transcribed strings; Dwyer and Kondrak (Dwyer and Kondrak, 2009) develop a method for accurate letter-to-phoneme conversion while minimizing the number of training examples; Reddy and Goldsmith (Reddy and Goldsmith, 2010) develop an MDL-based approach to finding subword units that align well to phonemes. A related line of work has grown around the task of machine transliteration. In this task, the goal is to automatically transliterate a name in one language into the written form of another language. Often this involves some level of phonetic analysis in one or both languages. Notable recent work in this vein includes research by Sproat et al (Sproat et al"
D12-1031,P10-1080,0,0.0141446,"rns, and as our experiments show, can be explicitly modeled to achieve high accuracy in our task. 2.2 Grapheme-to-Phoneme Prediction Much prior work has gone into developing methods for accurate grapheme-to-phoneme prediction. The common assumption underlying this research has been that some sort of knowledge, usually in the form of a pronunciation dictionary or phonemically annotated text, is available for the language at hand. The focus has been on developing techniques for dealing with the phonemic ambiguity present both in annotated and unseen words. For example, Jiampojamarn and Kondrak (Jiampojamarn and Kondrak, 2010) develop a method for aligning pairs of written and phonemically transcribed strings; Dwyer and Kondrak (Dwyer and Kondrak, 2009) develop a method for accurate letter-to-phoneme conversion while minimizing the number of training examples; Reddy and Goldsmith (Reddy and Goldsmith, 2010) develop an MDL-based approach to finding subword units that align well to phonemes. A related line of work has grown around the task of machine transliteration. In this task, the goal is to automatically transliterate a name in one language into the written form of another language. Often this involves some leve"
D12-1031,D11-1030,1,0.549701,"/ /|/ &gt; /k/ /tS/ /x/ /S/ /e/ /i/ /æ/ /@/ /E/ /-/ /h/ /x/ /ø/ /H/ /i/ /j/ /I/ &gt; &gt; /dZ/ /h/ /j/ /tS/ /x/ /é/ /Z/ /o/ /u/ /6/ /0/ /f/ /ph / /k/ /q/ /!/ /r/ /ó/ /R/ /ö/ /K/ /th / /T/ /u/ /w/ /y/ /1/ /U/ /Y/ /b/ /f/ /v/ /w/ /B/ /u/ /v/ /w/ &gt; /ks/ /x/ /{/ /S/ &gt; &gt; /dz/ /s/ /ts/ /z/ /T/ #lang 106 62 39 106 85 106 79 103 15 32 95 15 104 70 74 44 72 ent 1.25 2.33 1.35 1.82 1.24 0.92 2.05 1.47 0.64 1.04 1.50 0.64 0.96 1.18 0.89 1.31 0.93 Table 1: Ambiguous graphemes and the set of phonemes that they may represent among our set of 107 languages. ing a structured nearest neighbor approach for 8 languages (Kim et al., 2011). Our work extends this idea to a new task and also considers a much larger set of languages. As our results will indicate, we found that a nearest neighbor approach was not as effective as our proposed model-based approach. 3 Data and Features In this section we discuss the data and features used in our experiments. 3.1 Data The data for our experiments comes from three sources: (i) grapheme-phoneme mappings from an online encyclopedia, (ii) translations of the Universal Declaration of Human Rights (UDHR)2 , and (iii) entries from the World Atlas of Language Structures (WALS) (Haspelmath and"
D12-1031,P06-1146,0,0.0123854,"sis An influential thread of previous multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al.,"
D12-1031,N06-2030,0,0.0208606,"hat (i) we assume no knowledge for our target language beyond a small unannotated text (and possibly some region or language family information), and (ii) our goal is to construct the inventory of mappings between the language’s letters and its phonemes (the latter of which we do not know ahead of time). When a grapheme maps to more than one phoneme, we do not attempt to disambiguate particular instances of that grapheme in words. 335 A final thread of related work is the task of quantitatively categorizing writing systems according to their levels of phonography and logography (Sproat, 2000; Penn and Choma, 2006). As our data-set consists entirely of Latin-based writing systems, our work can be viewed as a more fine-grained computational exploration of the space of writing systems, with a focus on phonographic systems with the Latin pedigree. 2.3 Multilingual Analysis An influential thread of previous multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems"
D12-1031,N09-1005,0,0.0265613,"mples; Reddy and Goldsmith (Reddy and Goldsmith, 2010) develop an MDL-based approach to finding subword units that align well to phonemes. A related line of work has grown around the task of machine transliteration. In this task, the goal is to automatically transliterate a name in one language into the written form of another language. Often this involves some level of phonetic analysis in one or both languages. Notable recent work in this vein includes research by Sproat et al (Sproat et al., 2006) on transliteration between Chinese and English using comparable corpora, and Ravi and Knight (Ravi and Knight, 2009) who take a decipherment approach to this problem. Our work differs from all previous work on grapheme-to-phoneme prediction in that (i) we assume no knowledge for our target language beyond a small unannotated text (and possibly some region or language family information), and (ii) our goal is to construct the inventory of mappings between the language’s letters and its phonemes (the latter of which we do not know ahead of time). When a grapheme maps to more than one phoneme, we do not attempt to disambiguate particular instances of that grapheme in words. 335 A final thread of related work i"
D12-1031,N10-1107,0,0.0210225,"e sort of knowledge, usually in the form of a pronunciation dictionary or phonemically annotated text, is available for the language at hand. The focus has been on developing techniques for dealing with the phonemic ambiguity present both in annotated and unseen words. For example, Jiampojamarn and Kondrak (Jiampojamarn and Kondrak, 2010) develop a method for aligning pairs of written and phonemically transcribed strings; Dwyer and Kondrak (Dwyer and Kondrak, 2009) develop a method for accurate letter-to-phoneme conversion while minimizing the number of training examples; Reddy and Goldsmith (Reddy and Goldsmith, 2010) develop an MDL-based approach to finding subword units that align well to phonemes. A related line of work has grown around the task of machine transliteration. In this task, the goal is to automatically transliterate a name in one language into the written form of another language. Often this involves some level of phonetic analysis in one or both languages. Notable recent work in this vein includes research by Sproat et al (Sproat et al., 2006) on transliteration between Chinese and English using comparable corpora, and Ravi and Knight (Ravi and Knight, 2009) who take a decipherment approac"
D12-1031,W97-0213,0,0.0155802,"first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and"
D12-1031,P08-1084,1,0.843538,"ction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel tex"
D12-1031,D08-1109,1,0.842773,"01), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languag"
D12-1031,N09-1010,1,0.8426,"chers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languages. In this regard, o"
D12-1031,P09-1009,1,0.850778,"chers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Snyder et al., 2008; Snyder et al., 2009a; Naseem et al., 2009), and grammar induction (Snyder et al., 2009b; Blunsom et al., 2009; Burkett et al., 2010). An even more recent line of work does away with the assumption of parallel texts and performs joint unsupervised induction for various languages through the use of coupled priors in the context of grammar induction (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010). In contrast to these previous approaches, the method we propose does not assume the existence of any parallel text, but instead assumes that labeled data exists for a wide variety of languages. In this regard, o"
D12-1031,P06-1010,0,0.024743,"ndrak, 2009) develop a method for accurate letter-to-phoneme conversion while minimizing the number of training examples; Reddy and Goldsmith (Reddy and Goldsmith, 2010) develop an MDL-based approach to finding subword units that align well to phonemes. A related line of work has grown around the task of machine transliteration. In this task, the goal is to automatically transliterate a name in one language into the written form of another language. Often this involves some level of phonetic analysis in one or both languages. Notable recent work in this vein includes research by Sproat et al (Sproat et al., 2006) on transliteration between Chinese and English using comparable corpora, and Ravi and Knight (Ravi and Knight, 2009) who take a decipherment approach to this problem. Our work differs from all previous work on grapheme-to-phoneme prediction in that (i) we assume no knowledge for our target language beyond a small unannotated text (and possibly some region or language family information), and (ii) our goal is to construct the inventory of mappings between the language’s letters and its phonemes (the latter of which we do not know ahead of time). When a grapheme maps to more than one phoneme, w"
D12-1031,N01-1026,0,0.0534685,"ration of the space of writing systems, with a focus on phonographic systems with the Latin pedigree. 2.3 Multilingual Analysis An influential thread of previous multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder and Barzilay, 2008b), part-of-speech induction (Sny"
D12-1031,P00-1027,0,0.0389699,"an be viewed as a more fine-grained computational exploration of the space of writing systems, with a focus on phonographic systems with the Latin pedigree. 2.3 Multilingual Analysis An influential thread of previous multilingual work starts with the observation that rich linguistic resources exist for some languages but not others. The idea then is to project linguistic information from one language onto others via parallel data. Yarowsky and his collaborators first developed this idea and applied it to the problems of part-of-speech tagging, noun-phrase bracketing, and morphology induction (Yarowsky and Wicentowski, 2000; Yarowsky et al., 2000; Yarowsky and Ngai, 2001), and other researchers have applied the idea to syntactic and semantic analysis (Hwa et al., 2005; Padó and Lapata, 2006) In these cases, the existence of a bilingual parallel text along with highly accurate predictions for one of the languages was assumed. Another line of work assumes the existence of bilingual parallel texts without the use of any supervision (Dagan et al., 1991; Resnik and Yarowsky, 1997). This idea has been developed and applied to a wide variety tasks, including morphological analysis (Snyder and Barzilay, 2008a; Snyder an"
D12-1031,H01-1035,0,\N,Missing
D15-1150,P11-1061,0,0.0328946,"the vertical axis gives the number of languages in each token range. 2 2.1 Related Work Multilingual Projection The idea of projecting annotated resources across languages using parallel data was first proposed by Yarowsky et al. (2001). This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filtering methods to mitigate the effects of cross- lingual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token basis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich lang"
D15-1150,P11-1043,0,0.0197435,"lignments themselves. Instead of simply aligning each source language to the target language in isolation, we will instead use a confidence model to synthesize information from multiple sources. While there are not many well-known papers that have explored word alignment on a multilingual scale1 , there have been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1 Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is pro"
D15-1150,D12-1001,0,0.0552649,"s the number of languages in each token range. 2 2.1 Related Work Multilingual Projection The idea of projecting annotated resources across languages using parallel data was first proposed by Yarowsky et al. (2001). This early work recognized the noisy nature of automatic word alignments and engineered smoothing and filtering methods to mitigate the effects of cross- lingual variation and alignment errors. More recent work in this vein has dealt with this by instead transferring information at the word type or model structure level, rather than on a token-by-token basis (Das and Petrov, 2011; Durrett et al., 2012). Current state-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate align"
D15-1150,I05-1075,0,0.0253841,"ate-of-the-art results for indirectly supervised POS performance use a combination of token constraints as well as type constraints mined from Wiktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise. Fossum and Abney (2005) found that using two source languages project-sources gave better results than simply using more data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsuper"
D15-1150,D13-1205,0,0.062882,"t language and a set of resource-rich languages. Fortunately, such parallel data exists for just about every written language, in the form of Bible translations. Around 2,500 languages have at least partial Bible translations, and somewhere between 500 and 1,000 languages have complete translations. We have collected such electronic Bible translations for 650 languages. Figure 1 breaks down the number of languages in our collection according to their token count. The majority of our languages have at least 200,000 tokens of Bible translations. While previous studies (T¨ackstr¨om et al., 2013; Ganchev and Das, 2013) have addressed this general setting, they have typically assumed the existence of a partial tag dictionary as well as large quantities of non-parallel data in the target language. These assumptions are quite reasonable for the dozen most popular languages in the world, but are inadequate for the creation of a truly worldwide repository of NLP tools and linguistic data. In fact, we argue that such ancillary sources of information are not really necessary once we take into account the vastly multilingual nature of our parallel data. Annotations projected from individual resource-rich languages"
D15-1150,D12-1031,1,0.856126,"data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on automatic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve projection accuracy, the question of word alignment performance becomes crucial. Our hypo"
D15-1150,N13-1139,1,0.80208,"e. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on automatic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve projection accuracy, the question of word alignment performance becomes crucial. Our hypothesis is that multipl"
D15-1150,P13-1150,1,0.846594,"e. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Kim and Snyder, 2013b). We have showed that cross-lingual supervised learning leads to significant performance gains over monolingual models. We point out that the previous tasks have considered as word-level structural analyses and our present case as a sentence-level analysis. 2.2 Word Alignment Most of the papers surveyed above rely on automatic word alignments to guide the cross-lingual transfer of information. Given our desire to use highly multilingual information to improve projection accuracy, the question of word alignment performance becomes crucial. Our hypothesis is that multipl"
D15-1150,D11-1030,1,0.90127,"Missing"
D15-1150,P14-2104,1,0.729771,"ensionality (Kakade and Foster, 2007). This seems especially relevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-ri"
D15-1150,N15-1009,1,0.917698,"elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201"
D15-1150,P15-2132,1,0.895137,"elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201"
D15-1150,P15-2032,1,0.918413,"elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201"
D15-1150,P15-1046,1,0.917735,"elevant to natural language processing scenarios, where the ambient dimension is extremely large and sparse, but reductions to dense lower-dimensional spaces may preserve nearly all the relevant semantic and syntactic information. In fact, CCA has recently been adapted to learning latent word representations in an interesting way: by dividing each word position into a token view (which only sees surrounding context) and a type view (which only sees the word itself) and performing a CCA between these two views (Dhillon et al., 2012; Kim et al., 2014; Stratos et al., 2014; Stratos et al., 2015; Kim et al., 2015c). CCA is also used to induce label representations (Kim et al., 2015d) and lexicon representations (Kim et al., 2015b). Our technique will extend this idea by additionally considering a third projected tag view. Crucially, it is this view which pushes the latent representations into coherent part-of-speech categories, allowing us to simply apply multi-class SVM for unseen words in our test set. 3 Tag projection from resource-rich languages In this section, we describe two methods for incorporating transferred tags from resource-rich languages: sequence-based learning (T¨ackstr¨om et al., 201"
D15-1150,D12-1127,0,0.0606808,"Missing"
D15-1150,N06-1014,0,0.0597904,"e language projections are beneficial not only in weeding out random errors and idiosyncratic variations, but also in improving the linguistic consistency of the alignments themselves. Instead of simply aligning each source language to the target language in isolation, we will instead use a confidence model to synthesize information from multiple sources. While there are not many well-known papers that have explored word alignment on a multilingual scale1 , there have been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1 Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear"
D15-1150,Q13-1001,0,0.129648,"Missing"
D15-1150,D14-1187,0,0.271695,"Missing"
D15-1150,H01-1035,0,0.245423,"Missing"
D15-1150,W12-0209,0,0.0298582,"e been related efforts to symmetrize bilingual alignment models, using a variety of techniques ranging from modifications of EM (Liang et al., 2006), posterior-regularized objective function (Ganchev et al., 2010), and by considering relaxations of the hard combinatorial assignment problem (DeNero and Macherey, 2011). 2.3 Canonical Correlation Analysis (CCA) Our method for generalizing the projections to unseen words and contexts is based on Canonical Correlation Analysis (CCA), a dimensionality reduction technique first introduced by Hotelling (1936). The key idea is to consider two groups 1 Mayer and Cysouw (2012) used multilingual word alignment to compare languages of random variables with corresponding observations and to find linear subspaces with highest correlation between the two views. This can be seen as a kind of supervised version of Principal Components Analysis (PCA), where each view is providing supervision for the other. In fact, it can be shown that CCA directly generalizes both multiple linear regression and Fisher’s Latent Discriminative Analysis (LDA) (Glahn, 1968). From a learning theory perspective, CCA is interesting in that it allows us to prove regret-based learning bounds that"
D15-1150,D11-1006,0,0.0520304,"ktionary (Li et al., 2012; T¨ackstr¨om et al., 2013; Ganchev and Das, 2013). As we argued above, the only widely available source of information for most low-resource languages is in fact their Bible translation. Perhaps surprisingly, our experiments show that this data source suffices to achieve state-of-the-art results. Several previous authors have considered the advantage of using more than one resource-rich language to alleviate alignment noise. Fossum and Abney (2005) found that using two source languages project-sources gave better results than simply using more data from one language. McDonald et al. (2011) also found advantages to using multiple language sources for projecting parsing constraints. In more of an unsu1293 pervised context (but using small tag dictionaries), adding more languages to the mix has been shown to improve part-of-speech performance across all component languages (Naseem et al., 2009). In our own previous multilingual work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013a; Ki"
D15-1150,C14-1110,0,0.0662502,"Missing"
D15-1150,petrov-etal-2012-universal,0,0.13137,"Missing"
D15-1150,P15-1124,0,0.0606504,"Missing"
D16-1222,J92-4003,0,0.722706,"have different granularity and diverse semantics. Note that we keep domain-specific slots such as alarm state, but there are enough shared labels across domains. For example, ALARM domain, there are 6 shared slots among 8 slots. There are 62 slots appearing more than one domain. Especially, some basic slots such as time, date,place name,person name,location and product appear in most domains. 4.2 Slot Taggers In all our experiments, we trained Conditional Random Fields (CRFs)(Lafferty et al., 2001) and used n-gram features up to n = 3, regular expression, lexicon features, and Brown Clusters (Brown et al., 1992). With these features, we compare the following methods for slot tagging1 : 1 For parameter estimation, we used L-BFGS (Liu and Nocedal, 1989) with 100 as the maximum iteration count and 1.0 for the L2 regularization parameter. 2073 • In-domain: Train a domain specific model using the domain specific data covering the slots supported in that domain. • Binary: Train a binary classifier for each slot type, combine the slots needed for a given domain schema. • Post: Train a single model with all domain data, take the one-best parse of the tagger and filterout slots outside the domain schema. • Co"
D16-1222,N15-1009,1,0.230508,"ROL domain could be overwhelmed by slots with large training data (e.g. place name in PLACES domain). • Domain-specific schema: In practice, the domains the system handles are not constructed at the same time. They are designed for different application back-ends, requirements and scenarios at different points in time. In other words, the semantic schema for a domain is designed without considering other domains. In 2072 3 Constrained Decoding Slot tagging is considered as a sequence learning problem (Deoras and Sarikaya, 2013; Li et al., 2009; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015b; Kim et al., 2015c; Kim et al., 2015a). In sequence learning, given a sample query x1 . . . xn , the decoding problem is to find the most likely slot sequence among all the possible sequences, y1 . . . yn : f (x1 . . . xn ) = arg max p(x1 . . . xn , y1 . . . yn ) y1 ...yn Here, we assume that output space in training is same as those in test time. However, in our problem, output (slot) space in test time can be different from those in training time. At test time, we may observe different slot sequences than what is observed in the training time. This is not an issue for the Binary approach,"
D16-1222,P15-2132,1,0.793123,"ROL domain could be overwhelmed by slots with large training data (e.g. place name in PLACES domain). • Domain-specific schema: In practice, the domains the system handles are not constructed at the same time. They are designed for different application back-ends, requirements and scenarios at different points in time. In other words, the semantic schema for a domain is designed without considering other domains. In 2072 3 Constrained Decoding Slot tagging is considered as a sequence learning problem (Deoras and Sarikaya, 2013; Li et al., 2009; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015b; Kim et al., 2015c; Kim et al., 2015a). In sequence learning, given a sample query x1 . . . xn , the decoding problem is to find the most likely slot sequence among all the possible sequences, y1 . . . yn : f (x1 . . . xn ) = arg max p(x1 . . . xn , y1 . . . yn ) y1 ...yn Here, we assume that output space in training is same as those in test time. However, in our problem, output (slot) space in test time can be different from those in training time. At test time, we may observe different slot sequences than what is observed in the training time. This is not an issue for the Binary approach,"
D16-1222,P15-2032,1,0.758171,"ROL domain could be overwhelmed by slots with large training data (e.g. place name in PLACES domain). • Domain-specific schema: In practice, the domains the system handles are not constructed at the same time. They are designed for different application back-ends, requirements and scenarios at different points in time. In other words, the semantic schema for a domain is designed without considering other domains. In 2072 3 Constrained Decoding Slot tagging is considered as a sequence learning problem (Deoras and Sarikaya, 2013; Li et al., 2009; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015b; Kim et al., 2015c; Kim et al., 2015a). In sequence learning, given a sample query x1 . . . xn , the decoding problem is to find the most likely slot sequence among all the possible sequences, y1 . . . yn : f (x1 . . . xn ) = arg max p(x1 . . . xn , y1 . . . yn ) y1 ...yn Here, we assume that output space in training is same as those in test time. However, in our problem, output (slot) space in test time can be different from those in training time. At test time, we may observe different slot sequences than what is observed in the training time. This is not an issue for the Binary approach,"
D16-1222,P15-1046,1,0.832935,"ROL domain could be overwhelmed by slots with large training data (e.g. place name in PLACES domain). • Domain-specific schema: In practice, the domains the system handles are not constructed at the same time. They are designed for different application back-ends, requirements and scenarios at different points in time. In other words, the semantic schema for a domain is designed without considering other domains. In 2072 3 Constrained Decoding Slot tagging is considered as a sequence learning problem (Deoras and Sarikaya, 2013; Li et al., 2009; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015b; Kim et al., 2015c; Kim et al., 2015a). In sequence learning, given a sample query x1 . . . xn , the decoding problem is to find the most likely slot sequence among all the possible sequences, y1 . . . yn : f (x1 . . . xn ) = arg max p(x1 . . . xn , y1 . . . yn ) y1 ...yn Here, we assume that output space in training is same as those in test time. However, in our problem, output (slot) space in test time can be different from those in training time. At test time, we may observe different slot sequences than what is observed in the training time. This is not an issue for the Binary approach,"
D16-1222,C16-1193,1,0.854619,"Missing"
D17-1302,C16-1126,0,0.0258994,"Missing"
D17-1302,P13-2112,0,0.0215578,"mbeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can"
D17-1302,W04-3229,0,0.125229,"Missing"
D17-1302,D14-1181,0,0.00177753,"the i-th sentence in the minibatch, and pˆi,j is the predicted tag. In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections. Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016). First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014). The encoder is with three convolution filters whose sizes are 3, 4, and 5. For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector. Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer. The discriminator is implemented 1 We also tried isolated character-level modules but the overall performance was worse. 2833 as a fully-connected neural network w"
D17-1302,D15-1150,1,0.847344,"ter embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for kno"
D17-1302,C16-1038,1,0.73952,"eddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages. Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets. In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings. Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al2832 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Softmax Output Language Discriminator ?1 Gradient Reversal ?2 … ?? Bidirectional Language Model CNN/MaxPool Encoder ?2 ?3 ???? ?1 ???? … ??−1 Common BLSTM ℎ1?? ?? ℎ1 ? ℎ1 ℎ2?? ?? … ℎ2 Private BLSTM ?? ?? ℎ? ℎ1 ℎ? ?? ℎ1 ? ℎ1 ? ℎ2 ℎ? ?? ℎ2 ?? ?? ℎ2 ? ? ℎ2 ?? … ℎ? ?? ℎ? ? ℎ? Wor"
D17-1302,P15-1046,1,0.653527,"ter embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for kno"
D17-1302,D15-1176,0,0.0797098,"Missing"
D17-1302,P16-2067,0,0.365392,"aluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language. 1 Introduction Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic r"
D17-1302,P17-1194,0,0.011091,"passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic. The loss function of the language classifier is formulated as: La = − S X li log lˆi , (2) i=1 where S is the number of sentences, li is the language of the i-th sentence, and lˆi is the softmax output of the tagging. Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic. Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word. For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy &lt;eos&gt;” and the backward LSTM predicts “&lt;bos&gt; I am”. This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017). We adopted the bidirectional languag"
D17-1302,Q13-1001,0,0.0992589,"Missing"
D17-1302,D14-1187,0,0.149168,"Missing"
D17-1302,N16-1156,0,0.0461915,"e shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collo"
D18-1106,D17-1070,0,0.0291219,"BiLSTM) (Graves and Schmidhuber, 2005). Then, an utterance vector is composed by concatenating the last outputs of the forward LSTM and the backward LSTM.2 To represent the domain enablement information, we obtain a weighted sum of domain enablement vector where the weights are calculated by logistic sigmoid function on top of the multiplicative attention (Luong et al., 2015) for the utterance vector and the domain enablement vectors. The attention weight of an enabled domain e is formu2 We have also evaluated word vector summation, CNN (Kim, 2014), BiLSTM mean-pooling, and BiLSTM maxpooling (Conneau et al., 2017) as alternative utterance representation methods, but they did not show better performance on our task. 3 We utilize scaled exponential linear units (SeLU) as the activation function for the hidden layer(Klambauer et al., 2017). 895 and the target model (Hinton et al., 2014). Selfdistillation, which trains a model leveraging the outputs of the source model with the same architecture or capacity, has been shown to improve the target model’s performance with a distillation method (Furlanello et al., 2018). We use a variant of self-distillation methods, where the model outputs at the previous epo"
D18-1106,C16-1291,0,0.0189194,"n n-dimensional confidence score vector from the model, and y is an n-dimensional one-hot vector whose element corresponding to the position of the ground-truth domain is set to 1. 2.1 Supervised Enablement Attention Attention weights are originally intended to be automatically trained in an end-to-end fashion (Bahdanau et al., 2015), but it has been shown that applying proper explicit supervision to the attention improves the downstream tasks such as machine translation given the word alignment and constituent parsing given annotations between surface words and nonterminals (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017). We hypothesize that if the ground-truth domain is one of the enabled domains, the attention weight for the ground-truth domain should be high and vice versa. To apply this hypothesis in the model training as a supervised attention method, we formulate an auxiliary loss function as follows: X La = − ye log ae + (1 − ye ) log (1 − ae ) , e∈E where a˜e is the attention weight of the model showing the dev set performance in the previous epochs. It is formulated as: u · v  e a˜e = σ , T where T is the temperature for sufficient usage of all the attention weights as the"
D18-1106,P84-1044,0,0.269097,"Missing"
D18-1106,D15-1166,0,0.105867,"Missing"
D18-1106,I17-2002,0,0.0223568,"nfidence score vector from the model, and y is an n-dimensional one-hot vector whose element corresponding to the position of the ground-truth domain is set to 1. 2.1 Supervised Enablement Attention Attention weights are originally intended to be automatically trained in an end-to-end fashion (Bahdanau et al., 2015), but it has been shown that applying proper explicit supervision to the attention improves the downstream tasks such as machine translation given the word alignment and constituent parsing given annotations between surface words and nonterminals (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017). We hypothesize that if the ground-truth domain is one of the enabled domains, the attention weight for the ground-truth domain should be high and vice versa. To apply this hypothesis in the model training as a supervised attention method, we formulate an auxiliary loss function as follows: X La = − ye log ae + (1 − ye ) log (1 − ae ) , e∈E where a˜e is the attention weight of the model showing the dev set performance in the previous epochs. It is formulated as: u · v  e a˜e = σ , T where T is the temperature for sufficient usage of all the attention weights as the soft target. In this work"
D18-1106,D16-1249,0,0.0445025,"Missing"
D18-1106,D14-1181,0,0.00340921,"edding followed by bidirectional long shortterm memory (BiLSTM) (Graves and Schmidhuber, 2005). Then, an utterance vector is composed by concatenating the last outputs of the forward LSTM and the backward LSTM.2 To represent the domain enablement information, we obtain a weighted sum of domain enablement vector where the weights are calculated by logistic sigmoid function on top of the multiplicative attention (Luong et al., 2015) for the utterance vector and the domain enablement vectors. The attention weight of an enabled domain e is formu2 We have also evaluated word vector summation, CNN (Kim, 2014), BiLSTM mean-pooling, and BiLSTM maxpooling (Conneau et al., 2017) as alternative utterance representation methods, but they did not show better performance on our task. 3 We utilize scaled exponential linear units (SeLU) as the activation function for the hidden layer(Klambauer et al., 2017). 895 and the target model (Hinton et al., 2014). Selfdistillation, which trains a model leveraging the outputs of the source model with the same architecture or capacity, has been shown to improve the target model’s performance with a distillation method (Furlanello et al., 2018). We use a variant of sel"
D18-1106,D14-1162,0,0.0810746,"Missing"
D18-1106,N18-3003,1,0.845234,"sistant, Microsoft Cortana, and Apple Siri have been widely used as real-life applications of natural language understanding (Sarikaya et al., 2016; Sarikaya, 2017). In natural language understanding, domain classification is a task that finds the most relevant domain given an input utterance (Tur and de Mori, 2011). For example, “make a lion sound” and “find me an apple pie recipe” should be classified as ZooKeeper and AllRecipe, respectively. Recent IPDAs cover more than several thousands of diverse domains by including third-party developed domains such as Alexa Skills (Kumar et al., 2017; Kim et al., 2018a; Kim and Kim, 2018), 1 Enabled domain information specifically refers to preferred or authenticated domains in Amazon Alexa, but it can be extended to other information such as the list of recently used domains. 894 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 894–899 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ?: ground-truth one hot vector ?"" ?# ?+ ?"" ?# ?+ ?9 Supervised attention ?"" ?+ ?, ?9 … Feed-forward Self-distillation ?"" ?&lt;+ ?&lt;, &lt; Utterance vector ? from BiLSTM BiLSTM ?("" ?&apos;"" ?"" ?"
D18-1106,P18-1206,1,0.663071,"sistant, Microsoft Cortana, and Apple Siri have been widely used as real-life applications of natural language understanding (Sarikaya et al., 2016; Sarikaya, 2017). In natural language understanding, domain classification is a task that finds the most relevant domain given an input utterance (Tur and de Mori, 2011). For example, “make a lion sound” and “find me an apple pie recipe” should be classified as ZooKeeper and AllRecipe, respectively. Recent IPDAs cover more than several thousands of diverse domains by including third-party developed domains such as Alexa Skills (Kumar et al., 2017; Kim et al., 2018a; Kim and Kim, 2018), 1 Enabled domain information specifically refers to preferred or authenticated domains in Amazon Alexa, but it can be extended to other information such as the list of recently used domains. 894 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 894–899 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics ?: ground-truth one hot vector ?"" ?# ?+ ?"" ?# ?+ ?9 Supervised attention ?"" ?+ ?, ?9 … Feed-forward Self-distillation ?"" ?&lt;+ ?&lt;, &lt; Utterance vector ? from BiLSTM BiLSTM ?("" ?&apos;"" ?"" ?"
N13-1139,P09-1015,0,0.218886,"l produces state-of-the-art or comparable accuracies across a 1 The model generalizes easily to graphones consisting of more than one grapheme or phoneme, but in both (Bisani and Ney, 2008) and our initial experiments we found that the 01-to01 model always performed best. wide range of languages and data sets.2 We use the publicly available code provided by the authors.3 In all our experiments we set h = 4 (i.e. a 5-gram model), as we found that accuracy tended to be flat for h &gt; 4. Active Learning for G2P Perhaps most closely related to our work are the papers of Kominek and Black (2006) and Dwyer and Kondrak (2009), both of which use active learning to efficiently bootstrap pronunciation dictionaries. In the former, the authors develop an active learning word selection strategy for inducing pronunciation rules. In fact, their greedy n-gram selection strategy shares some of the some intuition as our second data set selection method, but they were unable to achieve any accuracy gains over randomly selected words without active learning. Dwyer and Kondrak use a Query-by-Bagging active learning strategy over decision tree learners. They find that their active learning strategy produces higher accuracy acros"
N13-1139,2005.mtsummit-papers.30,0,0.0300159,"egy produces higher accuracy across 5 of the 6 languages that they explored (English being the exception). They extract further performance gains through various refinements to their model. Even so, we found that the Bisani and Ney grapheme-tophoneme (G2P) model (Bisani and Ney, 2008) always achieved higher accuracy, even when trained on random words. Furthermore, the relative gains that we observe using our optimal data set selection strategies (without any active learning) are much larger than the relative gains of active learning found in their study. Data Set Selection and Active Learning Eck et al (2005) developed a method for training compact Machine Translation systems by selecting a subset of sentences with high n-gram coverage. Their selection criterion essentially corresponds to our feature coverage selection method using coverage function cov2 (see Section 3.2). As our results will show, the use of a geometric feature discount (cov3 ) provided better results in our task. Otherwise, we are not aware of previous work 2 We note that the discriminative model of Jiampojamarn and Kondrak (2010) outperforms the Bisani and Ney model by an average of about 0.75 percentage points across five data"
N13-1139,P10-1080,0,0.0159623,"uch larger than the relative gains of active learning found in their study. Data Set Selection and Active Learning Eck et al (2005) developed a method for training compact Machine Translation systems by selecting a subset of sentences with high n-gram coverage. Their selection criterion essentially corresponds to our feature coverage selection method using coverage function cov2 (see Section 3.2). As our results will show, the use of a geometric feature discount (cov3 ) provided better results in our task. Otherwise, we are not aware of previous work 2 We note that the discriminative model of Jiampojamarn and Kondrak (2010) outperforms the Bisani and Ney model by an average of about 0.75 percentage points across five data sets. 3 http://www-i6.informatik.rwth-aachen. de/web/Software/g2p.html 1198 proposing optimal data set selection as a general research problem. Of course, active learning strategies can be employed for this task by starting with a small random seed of examples and incrementally adding small batches. Unfortunately, this can lead to datasets that are biased to work well for one particular class of models and task, but may otherwise perform worse than a random set of examples (Settles, 2010, Secti"
N13-1139,J94-3001,0,0.131607,"Missing"
N13-1139,N06-1030,0,0.0333866,"zation techniques. Their model produces state-of-the-art or comparable accuracies across a 1 The model generalizes easily to graphones consisting of more than one grapheme or phoneme, but in both (Bisani and Ney, 2008) and our initial experiments we found that the 01-to01 model always performed best. wide range of languages and data sets.2 We use the publicly available code provided by the authors.3 In all our experiments we set h = 4 (i.e. a 5-gram model), as we found that accuracy tended to be flat for h &gt; 4. Active Learning for G2P Perhaps most closely related to our work are the papers of Kominek and Black (2006) and Dwyer and Kondrak (2009), both of which use active learning to efficiently bootstrap pronunciation dictionaries. In the former, the authors develop an active learning word selection strategy for inducing pronunciation rules. In fact, their greedy n-gram selection strategy shares some of the some intuition as our second data set selection method, but they were unable to achieve any accuracy gains over randomly selected words without active learning. Dwyer and Kondrak use a Query-by-Bagging active learning strategy over decision tree learners. They find that their active learning strategy p"
N13-1139,J00-2003,0,0.0183697,"Missing"
N15-1009,P11-1061,0,0.0330234,"ion scheme that leverages unlabeled data, we show that our method gives significant improvement over strong supervised and weakly-supervised baselines. 1 Introduction A key problem in natural language processing (NLP) is to effectively utilize large amounts of unlabeled and partially labeled data in situations where little or no annotations are available for a task of interest. Many recent work tackled this problem mostly in the context of part-of-speech (POS) tagging by transferring POS tags from a supervised language via automatic alignment and/or constructing tag dictionaries from the web (Das and Petrov, 2011; Li et al., 2012; T¨ackstr¨om et al., 2013). In this work, we attack this problem in the context of slot tagging, where the goal is to find correct semantic segmentation of a given query, which is an important task for information extraction and natural language understanding. For instance, answering the question “when is the new bill murray movie release date?” requires recognizing and labeling key phrases: e.g., “bill murray” as actor and “movie” as media type. To remedy this limitation, we propose a weakly supervised framework that utilizes the information available in web click logs. A we"
N15-1009,N06-1041,0,0.024639,"4; Sarikaya et al., 2014; Marin et al., 2014). Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006). T¨ackstr¨om et al. (2013) investigate weakly supervised POS tagging in low-resource languages, combining dictionary constraints and labels projected across languages via parallel corpora and automatic alignment. Our work can be seen as an extension of their approach to the structured-data projection setup presented by Li et al. (2009). A notable component of our extension is that we introduce a training algorithm for learning a hidden unit CRF of Maaten et al. (2011) from partially labeled sequences. This model has a set of binary latent variables that introduce non-linearity by mediating be"
N15-1009,D12-1127,0,0.0437924,"Missing"
N15-1009,P09-1113,0,0.0325752,"red weakly supervised slot tagging using aligned labels from a database as constraints. Wu and Weld (2007) train a CRF on heuristically annotated Wikipedia articles with relations mentioned in their structured infobox data. Li et al. (2009) applied a similar strategy incorporating structured data projected through click-log data as both heuristic labels and additional features. Knowledge graphs and search logs have been also considered as extra resources (Liu et al., 2013; El-Kahky et al., 2014; Anastasakos et al., 2014; Sarikaya et al., 2014; Marin et al., 2014). Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006). T¨ackstr¨om et al. (2013) investigate weakly supervised POS tagging"
N15-1009,D12-1042,0,0.0198089,"aligned labels from a database as constraints. Wu and Weld (2007) train a CRF on heuristically annotated Wikipedia articles with relations mentioned in their structured infobox data. Li et al. (2009) applied a similar strategy incorporating structured data projected through click-log data as both heuristic labels and additional features. Knowledge graphs and search logs have been also considered as extra resources (Liu et al., 2013; El-Kahky et al., 2014; Anastasakos et al., 2014; Sarikaya et al., 2014; Marin et al., 2014). Distant supervision methods (Mintz et al., 2009; Riedel et al., 2010; Surdeanu et al., 2012; Agichtein and Gravano, 2000) learn to extract relations from text using weak supervision from related structured data sources such as Freebase or Wikipedia. These approaches rely on named entity recognition as a pre-processing step to identify text spans corresponding to candidate slot values. In contrast, our approach jointly segments and predicts slots. Works on weakly supervised POS tagging are also closely related to ours (Toutanova and Johnson, 2007; Haghighi and Klein, 2006). T¨ackstr¨om et al. (2013) investigate weakly supervised POS tagging in low-resource languages, combining dictio"
N15-1009,Q13-1001,0,0.0339637,"Missing"
N15-1009,P13-1164,0,0.028454,"e. To remedy this limitation, we propose a weakly supervised framework that utilizes the information available in web click logs. A web click log is a mapping from a user query to URL link. For example, users issuing queries about movies tend to click on links from the IMDB.com or rottentomatoes.com, which provide rich structured data for entities such as title of the movie (“The Matrix”), the director (“The Wachowski Brothers”), and the release date (“1999”). Web click logs present an opportunity to learn semantic tagging models from large-scale and naturally occurring user interaction data (Volkova et al., 2013). While some previous works (Li et al., 2009) have applied a similar strategy to incorporate click logs in slot tagging, they do not employ recent advances in machine learning to effectively leverage the incomplete annotations. In this paper, we pursue and extend learning from partially labeled sequences, in particular the approach of T¨ackstr¨om et al. (2013). 84 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 84–92, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Instead of projecting labels fro"
N16-1032,W15-4319,1,0.894016,"Missing"
N16-1032,J92-4003,0,0.2809,"extend the original gazetteers with two methods: gathering data from knowledge graph and constructing task-specific gazetteer with phrase embeddings. 4.1 Basic Features The model of Ritter et al. (2011) employs the features described in this subsection. They are composed of the following features: (1) n-grams: unigrams and bigrams, (2) capitalization, (3) three character suffix and prefix presence, (4) binary features that indicate presence of hyphen, punctuation mark, single-digit and double-digit, (5) gazetteers (6) topics inferred by LabeledLDA (Ramage et al., 2009), and (7) brown cluster (Brown et al., 1992) produced by Ritter et al. (2011). To alleviate the problem of word sparsity, we also use task-specific latent continuous word representations, induced on 65 million unlabeled tweets with 1.3 billion tokens. We create three sets of word representations: CCA (Dhillon et al., 2012; Kim et al., 2015a) based on matrix factorization, word2vec (Mikolov et al., 2013) and glove (Pennington et al., 2014), which are gradient based. All word representation algorithms produce 50dimensional word vectors for all words occurring at least 40 times in the data. We use left and right word of the target word as"
N16-1032,P13-1150,1,0.803768,"lion tokens. We create three sets of word representations: CCA (Dhillon et al., 2012; Kim et al., 2015a) based on matrix factorization, word2vec (Mikolov et al., 2013) and glove (Pennington et al., 2014), which are gradient based. All word representation algorithms produce 50dimensional word vectors for all words occurring at least 40 times in the data. We use left and right word of the target word as context for learning the word representations. We also use compounding embeddings as an additional feature. Combining multiple sets of features has been proven to be effective (Koo et al., 2008; Kim and Snyder, 2013; Yu et al., 2013). We explore four different ways of combining the word representations: element-wise averaging, element-wise multiplication, concatenation and hierarchical clustering. We empirically determined that the elementwise averaging achieves better performance than single embeddings and other combination methods. We do not describe the results for embedding com284 Gazetteers Expansion from Knowledge Graph To expand gazetteers from knowledge graph, we apply the following processing steps. We first extract the seed words from training data. With seed words, we then collect the relevant"
N16-1032,P14-2104,1,0.844707,"s. Y contains all possible label sequences of x, and Φ maps (x, y) into a feature vector that is a linear combination of local feature vectors: Φ(x, y) = P n j=1 φ(xj , yj−1 , yj ). Given fully observed training data, {(x(i) , y (i) )}N i=1 , the objective of the training is to find θ that maximizes the log likelihood of the training data under the model with l2 -regularization: θ∗ = argmax θ N X i=1 log p(y (i) |x(i) ; θ) λ − ||θ||2 . 2 (1) CRFs have benefited from having a rich set of gazetteers as features in the model (Smith and Osborne, 2006; Liu and Sarikaya, 2014; Hillard et al., 2011; Kim et al., 2014; Kim et al., 2015c; Kim et al., 2015b; Kim et al., 2015d). Smith and Osborne (2006) point out that common gazetteer features fire 1 The original dropout technique is to inactivate features randomly. Here, we consider to decrease the weight of a specific feature. 283 where G is a set of gazetteers and freq(g) counts how many times words appear in gazetteer g from training data. In our experiments, we tuned both penalty weights for local features and for gazetteer features based on a small held-out validation set. The θg is a member of model parameter θ and each gazetteer has its own parameter"
N16-1032,D15-1150,1,0.767334,"015; Baldwin et al., 2015; Bellmore et al., 2015). The traditional machine learned modeling approaches trained with small and clean general text, such as news articles, perform poorly when applied to tweets, because tweets are structurally very different from general text. Thus, it § Both authors contributed equally. Another method is to build the task-specific gazetteers. Task-specific gazetteers make the models more general and increase their coverage for unseen events. They have been proven to be useful on a number of tasks (Smith and Osborne, 2006; Li et al., 2009; Liu and Sarikaya, 2014; Kim et al., 2015b; Kim et al., 2015c). Since gazetteers can improve modeling performance, here we more focus on how to use gazetteer more effectively. To build gazetteers with sufficient coverage for our task, we first expand gazetteers from knowledge graph and phrase embeddings. However, since the expanded gazetteers cover significant proportions of the entities in the training data, the weight of gazetteers features are easily inflated and thus the model tends to rely too much on lexical features extracted from the gazetteers fea282 Proceedings of NAACL-HLT 2016, pages 282–288, c San Diego, California, June"
N16-1032,P15-2132,1,0.78144,"015; Baldwin et al., 2015; Bellmore et al., 2015). The traditional machine learned modeling approaches trained with small and clean general text, such as news articles, perform poorly when applied to tweets, because tweets are structurally very different from general text. Thus, it § Both authors contributed equally. Another method is to build the task-specific gazetteers. Task-specific gazetteers make the models more general and increase their coverage for unseen events. They have been proven to be useful on a number of tasks (Smith and Osborne, 2006; Li et al., 2009; Liu and Sarikaya, 2014; Kim et al., 2015b; Kim et al., 2015c). Since gazetteers can improve modeling performance, here we more focus on how to use gazetteer more effectively. To build gazetteers with sufficient coverage for our task, we first expand gazetteers from knowledge graph and phrase embeddings. However, since the expanded gazetteers cover significant proportions of the entities in the training data, the weight of gazetteers features are easily inflated and thus the model tends to rely too much on lexical features extracted from the gazetteers fea282 Proceedings of NAACL-HLT 2016, pages 282–288, c San Diego, California, June"
N16-1032,P15-2032,1,0.876667,"015; Baldwin et al., 2015; Bellmore et al., 2015). The traditional machine learned modeling approaches trained with small and clean general text, such as news articles, perform poorly when applied to tweets, because tweets are structurally very different from general text. Thus, it § Both authors contributed equally. Another method is to build the task-specific gazetteers. Task-specific gazetteers make the models more general and increase their coverage for unseen events. They have been proven to be useful on a number of tasks (Smith and Osborne, 2006; Li et al., 2009; Liu and Sarikaya, 2014; Kim et al., 2015b; Kim et al., 2015c). Since gazetteers can improve modeling performance, here we more focus on how to use gazetteer more effectively. To build gazetteers with sufficient coverage for our task, we first expand gazetteers from knowledge graph and phrase embeddings. However, since the expanded gazetteers cover significant proportions of the entities in the training data, the weight of gazetteers features are easily inflated and thus the model tends to rely too much on lexical features extracted from the gazetteers fea282 Proceedings of NAACL-HLT 2016, pages 282–288, c San Diego, California, June"
N16-1032,E14-1048,0,0.0465075,"Missing"
N16-1032,D14-1162,0,0.0841169,"resence, (4) binary features that indicate presence of hyphen, punctuation mark, single-digit and double-digit, (5) gazetteers (6) topics inferred by LabeledLDA (Ramage et al., 2009), and (7) brown cluster (Brown et al., 1992) produced by Ritter et al. (2011). To alleviate the problem of word sparsity, we also use task-specific latent continuous word representations, induced on 65 million unlabeled tweets with 1.3 billion tokens. We create three sets of word representations: CCA (Dhillon et al., 2012; Kim et al., 2015a) based on matrix factorization, word2vec (Mikolov et al., 2013) and glove (Pennington et al., 2014), which are gradient based. All word representation algorithms produce 50dimensional word vectors for all words occurring at least 40 times in the data. We use left and right word of the target word as context for learning the word representations. We also use compounding embeddings as an additional feature. Combining multiple sets of features has been proven to be effective (Koo et al., 2008; Kim and Snyder, 2013; Yu et al., 2013). We explore four different ways of combining the word representations: element-wise averaging, element-wise multiplication, concatenation and hierarchical clusterin"
N16-1032,D09-1026,0,0.00738326,"xical coverage. To alleviate the problem, we extend the original gazetteers with two methods: gathering data from knowledge graph and constructing task-specific gazetteer with phrase embeddings. 4.1 Basic Features The model of Ritter et al. (2011) employs the features described in this subsection. They are composed of the following features: (1) n-grams: unigrams and bigrams, (2) capitalization, (3) three character suffix and prefix presence, (4) binary features that indicate presence of hyphen, punctuation mark, single-digit and double-digit, (5) gazetteers (6) topics inferred by LabeledLDA (Ramage et al., 2009), and (7) brown cluster (Brown et al., 1992) produced by Ritter et al. (2011). To alleviate the problem of word sparsity, we also use task-specific latent continuous word representations, induced on 65 million unlabeled tweets with 1.3 billion tokens. We create three sets of word representations: CCA (Dhillon et al., 2012; Kim et al., 2015a) based on matrix factorization, word2vec (Mikolov et al., 2013) and glove (Pennington et al., 2014), which are gradient based. All word representation algorithms produce 50dimensional word vectors for all words occurring at least 40 times in the data. We us"
N16-1032,D11-1141,0,0.632904,"the problem is to develop methods of utilizing a large amount of unlabeled data. One way is to induce word embeddings in a real-valued vector space from a large number of tweets (Kim et al., 2015a; Mikolov et al., 2013; Pennington et al., 2014). It is shown that the task-specific embeddings induced on tweets provide more powerful than those created from out-ofdomain texts (Owoputi et al., 2012; Anastasakos et al., 2014). Introduction Nowadays, people are generating tremendous amount of information on social websites. For example, more than 200 million tweets are generated everyday on Twitter (Ritter et al., 2011). Twitter has become a key news source, in addition to standard news channels. As such, social scientists are starting to pay attention to it in recent years (Bollen et al., 2011; Chung and Mustafaraj, 2011; Xu et al., 2014; Calvin et al., 2015; Baldwin et al., 2015; Bellmore et al., 2015). The traditional machine learned modeling approaches trained with small and clean general text, such as news articles, perform poorly when applied to tweets, because tweets are structurally very different from general text. Thus, it § Both authors contributed equally. Another method is to build the task-spec"
N16-1032,W06-2918,0,0.218196,"011; Chung and Mustafaraj, 2011; Xu et al., 2014; Calvin et al., 2015; Baldwin et al., 2015; Bellmore et al., 2015). The traditional machine learned modeling approaches trained with small and clean general text, such as news articles, perform poorly when applied to tweets, because tweets are structurally very different from general text. Thus, it § Both authors contributed equally. Another method is to build the task-specific gazetteers. Task-specific gazetteers make the models more general and increase their coverage for unseen events. They have been proven to be useful on a number of tasks (Smith and Osborne, 2006; Li et al., 2009; Liu and Sarikaya, 2014; Kim et al., 2015b; Kim et al., 2015c). Since gazetteers can improve modeling performance, here we more focus on how to use gazetteer more effectively. To build gazetteers with sufficient coverage for our task, we first expand gazetteers from knowledge graph and phrase embeddings. However, since the expanded gazetteers cover significant proportions of the entities in the training data, the weight of gazetteers features are easily inflated and thus the model tends to rely too much on lexical features extracted from the gazetteers fea282 Proceedings of N"
N16-1032,P05-1003,0,0.106167,"Missing"
N16-1032,W15-4310,1,0.768174,"that larger gazetteers can mitigate the “unseen words” problem by increasing the coverage of the gazetteers. 285 F1 62.76 64.67 CRFsvanila CRFsLOP CRFsdropout F1 64.67 65.54 69.38 Table 2: Comparison of models with or without dropout. CRFsvanilla is the vanilla CRFs with all features. CRFsLOP is a combination of CRFs with all features except for gazetteers and CRFs with gazetteers only, using logarithmic opinion pool (LOP). CRFsdropout is the dropout CRFs with all features. 5.3 Analysis While previous NER tasks mostly focus on reporting numbers on the original data set (Baldwin et al., 2015; Yang and Kim, 2015; Kim et al., 2015c), we further investigate how the tagging performance may change, if entities are unseen at test time. To enable such analysis, we create additional test set based on the original test set by replacing each word in person and company entities with a special token, XXXXX, indicating unseen words. This new test set represents an extreme case, where none of the words contained in the gazetteers are observed in the training data. Table 3 represents the comparison of vanilla CRF model and dropout model for unseen test. Gazetteer is helpful to resolve “unseen words” problem. Unfor"
N16-1032,N13-1063,0,0.0173128,"three sets of word representations: CCA (Dhillon et al., 2012; Kim et al., 2015a) based on matrix factorization, word2vec (Mikolov et al., 2013) and glove (Pennington et al., 2014), which are gradient based. All word representation algorithms produce 50dimensional word vectors for all words occurring at least 40 times in the data. We use left and right word of the target word as context for learning the word representations. We also use compounding embeddings as an additional feature. Combining multiple sets of features has been proven to be effective (Koo et al., 2008; Kim and Snyder, 2013; Yu et al., 2013). We explore four different ways of combining the word representations: element-wise averaging, element-wise multiplication, concatenation and hierarchical clustering. We empirically determined that the elementwise averaging achieves better performance than single embeddings and other combination methods. We do not describe the results for embedding com284 Gazetteers Expansion from Knowledge Graph To expand gazetteers from knowledge graph, we apply the following processing steps. We first extract the seed words from training data. With seed words, we then collect the relevant lexicons from kno"
N16-1032,P08-1068,0,\N,Missing
N16-3010,D14-1223,1,0.78207,"Missing"
N16-3010,W15-4654,0,0.0361476,"Missing"
N18-3003,W02-2236,0,0.122145,"ulti-turn conversations for future work. Reranking approaches attempt to improve upon an initial ranking by considering additional contextual information. Initial model outputs are trimmed down to a subset of most likely candidates, and each candidate is combined with additional features to form a hypothesis to be re-scored. Reranking has been applied to various natural language processing tasks, including machine translation (Shen et al., 2004), parsing (Collins and Koo, 2005), sentence boundary detection (Roark et al., 2006), named entity recognition (Nguyen et al., 2010), and supertagging (Chen et al., 2002). In the context of NLU or SLU systems, Morbini et al. (2012) showed a reranking approach using k-best lists from multiple automatic speech recognition (ASR) engines to improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-dom"
N18-3003,N15-1009,1,0.833864,"automatic speech recognition (ASR) engines to improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Shortlisting-Reranking Architecture Our shortlisting-reranking models process an incoming utterance as follows. (1) Shortlister performs a naive, fast ranking"
N18-3003,J05-1003,0,0.0975942,"ion to the existing literature in the context of IPDA and NLU systems. In this work, we limit our scope to first-turn utterances and leave multi-turn conversations for future work. Reranking approaches attempt to improve upon an initial ranking by considering additional contextual information. Initial model outputs are trimmed down to a subset of most likely candidates, and each candidate is combined with additional features to form a hypothesis to be re-scored. Reranking has been applied to various natural language processing tasks, including machine translation (Shen et al., 2004), parsing (Collins and Koo, 2005), sentence boundary detection (Roark et al., 2006), named entity recognition (Nguyen et al., 2010), and supertagging (Chen et al., 2002). In the context of NLU or SLU systems, Morbini et al. (2012) showed a reranking approach using k-best lists from multiple automatic speech recognition (ASR) engines to improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sari"
N18-3003,P17-1119,1,0.817956,"o improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Shortlisting-Reranking Architecture Our shortlisting-reranking models process an incoming utterance as follows. (1) Shortlister performs a naive, fast ranking of all domains to find the k-best list using"
N18-3003,P17-1060,1,0.872555,"o improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Shortlisting-Reranking Architecture Our shortlisting-reranking models process an incoming utterance as follows. (1) Shortlister performs a naive, fast ranking of all domains to find the k-best list using"
N18-3003,P15-2032,1,0.85901,"automatic speech recognition (ASR) engines to improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Shortlisting-Reranking Architecture Our shortlisting-reranking models process an incoming utterance as follows. (1) Shortlister performs a naive, fast ranking"
N18-3003,C10-2104,0,0.0306712,"ope to first-turn utterances and leave multi-turn conversations for future work. Reranking approaches attempt to improve upon an initial ranking by considering additional contextual information. Initial model outputs are trimmed down to a subset of most likely candidates, and each candidate is combined with additional features to form a hypothesis to be re-scored. Reranking has been applied to various natural language processing tasks, including machine translation (Shen et al., 2004), parsing (Collins and Koo, 2005), sentence boundary detection (Roark et al., 2006), named entity recognition (Nguyen et al., 2010), and supertagging (Chen et al., 2002). In the context of NLU or SLU systems, Morbini et al. (2012) showed a reranking approach using k-best lists from multiple automatic speech recognition (ASR) engines to improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improv"
N18-3003,C16-1038,1,0.844822,"n-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Shortlisting-Reranking Architecture Our shortlisting-reranking models process an incoming utterance as follows. (1) Shortlister performs a naive, fast ranking of all domains to find the k-best list using only the character and wordlevel information. The goal here is to achieve high domain recall with maximal efficiency and minimal information and latency. (2) For each domain in the k-best list, we prepare a hypothesis per dom"
N18-3003,P15-1046,1,0.839201,"automatic speech recognition (ASR) engines to improve response category classification for virtual museum guides. Basili et al. (2013) showed that reranking multiple ASR candidates by analyzing their syntactic properties can improve spoken command understanding in human-robot interaction, but with more focus on ASR improvement. Xu and Sarikaya (2014) showed that multi-turn contextual information and recurrent neural networks can improve domain classification in a multi-domain and multiturn NLU system. There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (El-Kahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Shortlisting-Reranking Architecture Our shortlisting-reranking models process an incoming utterance as follows. (1) Shortlister performs a naive, fast ranking"
N18-3003,N04-1023,0,0.0971855,"Missing"
N18-3003,D15-1176,0,0.0123668,": Shortlister Shortlister consists of three layers: an orthography-sensitive character and word embedding layer, a BiLSTM layer that makes a vector representation for the words in a given utterance, and an output layer for domain classification. Figure 2 shows the overall shortlister architecture. o = sof tmax (W · h + b) where W and b are parameters for a linear transformation. For training, we use cross-entropy loss, which is formulated as follows: n X La = − li log oi (1) Embedding layer In order to capture characterlevel patterns, we construct an orthographysensitive word embedding layer (Ling et al., 2015; Ballesteros et al., 2015). Let C, W, and ⊕ denote the set of characters, the set of words, and the vector concatenation operator, respectively. We repre0 0 sent an LSTM as a mapping φ : Rd × Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h)1 . The model parameters associated with this layer are: i=1 where l is a n-dimensional one-hot vector whose element corresponding to the position of the ground-truth hypothesis is set to 1. sof tmaxb is used to set the confidence score for each domain to be between 0 and 1. While sof tmaxa tends to highlig"
N19-1379,N18-3003,1,0.469446,"Missing"
N19-1379,P18-1206,1,0.835144,"As have only supported dozens of well-separated domains, where each is defined in terms of a specific application or functionality such as calendar and weather (Sarikaya et al., 2016; Tur and De Mori, 2011; El-Kahky et al., 2014). In order to increase the domain coverage and extend the capabilities of the IPDAs, mainstream IPDAs released tools to allow thirdparty developers to build new domains. Amazons Alexa Skills Kit, Googles Actions and Microsofts Cortana Skills Kit are examples of such tools. To handle the influx of new domains, largescale domain classification methods like S HORTLISTER (Kim et al., 2018b) have been proposed and have achieved good performance. As more new domains are developed rapidly, one of the major challenges in large-scale domain classification is how to quickly accommodate the new domains without losing the learned prediction power on the known ones. A straightforward solution is to simply retraining the whole model whenever new domains are available. However, this is not desirable since retraining is often time consuming. Another approach is to utilize continual learning where we dynamically evolve the model whenever a new domain is available. There is extensive work o"
N19-1379,P17-1060,1,0.820829,"and De Mori, 2011). They were typically limited to a small number of domains which were designed by specialists to be well-separated. To support large-scale domain classification, (Kim et al., 2018b) proposed S HORTLISTER, a neural-based model. (Kim et al., 2018a) extended S HORTLISTER by using additional contextual information to rerank the predictions of S HORTLISTER. However, none of them can continuously accommodate new domains without full model retrains. Continuous Domain Adaptation: To our knowledge, there is little work on the topic of continuous domain adaptation for NLU and IPDAs. (Kim et al., 2017) proposed an attention-based method for continuous domain adaptation, but it Continual Learning: Several techniques have been proposed to mitigate the catastrophic forgetting (Kemker et al., 2018). Regularization methods add constraints to the network to prevent important parameters from changing too much (Kirkpatrick et al., 2017; Zenke et al., 2017). Ensemble methods alleviate catastrophic forgetting by explicitly or implicitly learning multiple classifiers and using them to make the final predictions (Dai et al., 2009; Ren et al., 2017; Fernando et al., 2017). Rehearsal methods use data fro"
N19-1379,D15-1166,0,0.0477224,"ll utterance. The second module is the personalized domain summarization module. For each utterance from an IPDA user, a list of domains have been enabled by the user. These enabled domains can be viewed as user-specific personalized information. It has been shown that the domain classification accuracy can be significantly improved by leveraging information about enabled domains (Kim et al., 2018b). To represent the domain enablement information, first each enabled domain is mapped to a fixed-dimensional embedding, then a summarization vector is generated by taking an attention weighted sum (Luong et al., 2015) over the enabled domain embeddings. Once the utterance representation and the enabled domain summarization are calculated, we concatenate the two vectors as the final representation. Then the third module, a feed-forward network, is used to predict the confidence score with a sigmoid function for each domain. 2.3 Continuous Domain Adaptation As more new domains are developed, a major challenges in large-scale domain classification is quickly accommodating the new domains into the live production domain classification model without having to perform a full retrain. We refer to this problem as"
N19-2002,D14-1162,0,0.0808357,"Missing"
N19-2002,N18-1172,0,0.0263482,"a large number of locales, given the sufficient amount of data in one of the locales of that language, while achieving high domain classification accuracy over all locales. The approach is based on a multi-task learning framework that aims to share available data to learn a joint representation, and we introduce a way to selectively share knowledge across locales while considering locale-specificity in the joint learning. Multi-task learning has been widely used to tackle the problem of low-resource tasks or leveraging data between correlated targets (Liu et al., 2017; Ruder and Plank, 2018; Augenstein et al., 2018; Peters et al., 2017; Kim et al., 2017b), but none of them consider locale-specificity when sharing knowledge to learn a joint representation. We evaluate our proposed approach on the realworld utterance data spoken by customers to an intelligent personal digital assistant across different locales. The experimental results empirically demonstrate that the proposed universal model scales to multiple locales, while achieving higher domain classification accuracy compared to competing locale-unified models as well as per-locale separate models. The proposed model named universal model is able to"
N19-2002,P17-1161,0,0.0241112,"s, given the sufficient amount of data in one of the locales of that language, while achieving high domain classification accuracy over all locales. The approach is based on a multi-task learning framework that aims to share available data to learn a joint representation, and we introduce a way to selectively share knowledge across locales while considering locale-specificity in the joint learning. Multi-task learning has been widely used to tackle the problem of low-resource tasks or leveraging data between correlated targets (Liu et al., 2017; Ruder and Plank, 2018; Augenstein et al., 2018; Peters et al., 2017; Kim et al., 2017b), but none of them consider locale-specificity when sharing knowledge to learn a joint representation. We evaluate our proposed approach on the realworld utterance data spoken by customers to an intelligent personal digital assistant across different locales. The experimental results empirically demonstrate that the proposed universal model scales to multiple locales, while achieving higher domain classification accuracy compared to competing locale-unified models as well as per-locale separate models. The proposed model named universal model is able to successfully predict"
N19-2002,D17-1302,1,0.890714,"Missing"
N19-2002,P18-1096,0,0.0126278,"nguage understanding to a large number of locales, given the sufficient amount of data in one of the locales of that language, while achieving high domain classification accuracy over all locales. The approach is based on a multi-task learning framework that aims to share available data to learn a joint representation, and we introduce a way to selectively share knowledge across locales while considering locale-specificity in the joint learning. Multi-task learning has been widely used to tackle the problem of low-resource tasks or leveraging data between correlated targets (Liu et al., 2017; Ruder and Plank, 2018; Augenstein et al., 2018; Peters et al., 2017; Kim et al., 2017b), but none of them consider locale-specificity when sharing knowledge to learn a joint representation. We evaluate our proposed approach on the realworld utterance data spoken by customers to an intelligent personal digital assistant across different locales. The experimental results empirically demonstrate that the proposed universal model scales to multiple locales, while achieving higher domain classification accuracy compared to competing locale-unified models as well as per-locale separate models. The proposed model named u"
N19-2002,D16-1222,1,0.899728,"Missing"
N19-2002,P17-1119,1,0.928056,"nt amount of data in one of the locales of that language, while achieving high domain classification accuracy over all locales. The approach is based on a multi-task learning framework that aims to share available data to learn a joint representation, and we introduce a way to selectively share knowledge across locales while considering locale-specificity in the joint learning. Multi-task learning has been widely used to tackle the problem of low-resource tasks or leveraging data between correlated targets (Liu et al., 2017; Ruder and Plank, 2018; Augenstein et al., 2018; Peters et al., 2017; Kim et al., 2017b), but none of them consider locale-specificity when sharing knowledge to learn a joint representation. We evaluate our proposed approach on the realworld utterance data spoken by customers to an intelligent personal digital assistant across different locales. The experimental results empirically demonstrate that the proposed universal model scales to multiple locales, while achieving higher domain classification accuracy compared to competing locale-unified models as well as per-locale separate models. The proposed model named universal model is able to successfully predict domains for local"
N19-2002,C16-1193,1,0.860768,"edictions. Since the availability of domains depends on locales, the prediction layers use the locale information associated with the utterance to route the encoded vector to only a subset of prediction layers in which the domain of the utterance is available. Then, the output vector pro12 Locale US GB CA IN Train 173,258 85,539 7,113 4,821 Validation 24,653 10,378 887 637 Test 122,931 53,226 4,487 2,990 No. domains 177 240 51 41 associated with the utterance to route the encoded utterance to only a subset of domains available in the constrained output space for the locale to make prediction (Kim et al., 2016b,a). Table 1: Data statistics Locale Overall US GB CA IN 177 240 51 41 Localespecific 15 16 3 4 Localeindependent 162 224 48 37 Singlelocale 0 82 6 12 Small • universal This is our main contribution model described throughout the paper. 35 100 33 20 • universal + adv An extension of ‘universal’ incorporating the adversarial locale prediction loss as discussed in Section 3.2. Table 2: Test set breakdown US GB CA IN US 177 GB 155 240 CA 44 27 51 IN 26 23 10 41 4.3 To demonstrate the effectiveness of our model architecture especially on domains with insufficient data and/or locale-dependency, we"
N19-2002,C16-1038,1,0.859196,"edictions. Since the availability of domains depends on locales, the prediction layers use the locale information associated with the utterance to route the encoded vector to only a subset of prediction layers in which the domain of the utterance is available. Then, the output vector pro12 Locale US GB CA IN Train 173,258 85,539 7,113 4,821 Validation 24,653 10,378 887 637 Test 122,931 53,226 4,487 2,990 No. domains 177 240 51 41 associated with the utterance to route the encoded utterance to only a subset of domains available in the constrained output space for the locale to make prediction (Kim et al., 2016b,a). Table 1: Data statistics Locale Overall US GB CA IN 177 240 51 41 Localespecific 15 16 3 4 Localeindependent 162 224 48 37 Singlelocale 0 82 6 12 Small • universal This is our main contribution model described throughout the paper. 35 100 33 20 • universal + adv An extension of ‘universal’ incorporating the adversarial locale prediction loss as discussed in Section 3.2. Table 2: Test set breakdown US GB CA IN US 177 GB 155 240 CA 44 27 51 IN 26 23 10 41 4.3 To demonstrate the effectiveness of our model architecture especially on domains with insufficient data and/or locale-dependency, we"
N19-2002,P17-1001,0,0.126351,"scaling natural language understanding to a large number of locales, given the sufficient amount of data in one of the locales of that language, while achieving high domain classification accuracy over all locales. The approach is based on a multi-task learning framework that aims to share available data to learn a joint representation, and we introduce a way to selectively share knowledge across locales while considering locale-specificity in the joint learning. Multi-task learning has been widely used to tackle the problem of low-resource tasks or leveraging data between correlated targets (Liu et al., 2017; Ruder and Plank, 2018; Augenstein et al., 2018; Peters et al., 2017; Kim et al., 2017b), but none of them consider locale-specificity when sharing knowledge to learn a joint representation. We evaluate our proposed approach on the realworld utterance data spoken by customers to an intelligent personal digital assistant across different locales. The experimental results empirically demonstrate that the proposed universal model scales to multiple locales, while achieving higher domain classification accuracy compared to competing locale-unified models as well as per-locale separate models. The"
P13-1150,P10-1131,0,0.0245504,"Ugaritic. While the aim of the present work is more modest (discovering very basic phonetic properties of letters) it is also more widely applicable, as we don’t required detailed analysis of a known related language. Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1 . In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of kno"
P13-1150,D11-1059,0,0.0190698,"that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1 We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 1529 For example, the cluster Poisson parameter over vowel observation types might be λ = 9 (indicating 9 vowel letters on average for the cluster), while the parameter over consonant observation types might be λ = 20 (indicating 20 consonant letters on average)."
P13-1150,N09-1009,0,0.0318263,"l of Snyder et al. (2010), which used knowledge of a related language (Hebrew) in an elaborate Bayesian framework to decipher the ancient language of Ugaritic. While the aim of the present work is more modest (discovering very basic phonetic properties of letters) it is also more widely applicable, as we don’t required detailed analysis of a known related language. Other recent work has employed a similar perspective for tying learning across languages. Naseem et al. (2009) use a non-parametric Bayesian model over parallel text to jointly learn part-of-speech taggers across 8 languages, while Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim a"
P13-1150,D11-1005,0,0.0197574,"of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1 We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 1529 For example, the cluster Poisson parameter over vowel observation types might be λ = 9 (indicating 9 vowel letters on average for the cluster), while the parameter over consonant observation types might be λ = 20 (indicating 20 consonant letters on average). These priors will be distinct for each language cluster and serve to characterize its general linguistic and typological properties. We pause at this point to review the Dirichlet distribution in more detail. A k−dimensional Dirichlet with parameters α1 ... αk defines a distribution over the k − 1 simplex with the following den"
P13-1150,D12-1031,1,0.643642,"2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1 . In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of known languages. However, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption"
P13-1150,D11-1030,1,0.7148,"Cohen and Smith (2009) develop a shared logistic normal prior to couple multilingual learning even in the absence of parallel text. In similar veins, Berg-Kirkpatrick and Klein (2010) develop hierarchically tied grammar priors over languages within the same family, and BouchardCôté et al. (2013) develop a probabilistic model of sound change using data from 637 Austronesian languages. In our own previous work, we have developed the idea that supervised knowledge of some number of languages can help guide the unsupervised induction of linguistic structure, even in the absence of parallel text (Kim et al., 2011; Kim and Snyder, 2012)1 . In the latter work we also tackled the problem of unsupervised phonemic prediction for unknown languages by using textual regularities of known languages. However, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts."
P13-1150,P06-2065,0,0.175108,"Missing"
P13-1150,D10-1083,0,0.0234002,"owever, we assumed that the target language was written in a known (Latin) alphabet, greatly reducing the difficulty of the prediction task. In our present case, we assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1 We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 1529 For example, the cluster Poisson parameter over vowel observation types might be λ = 9 (indicating 9 vowel letters on average for the cluster), while the parameter over consonant observation types might be λ = 20 (indicating"
P13-1150,N10-1082,0,0.0184816,"e assume no knowledge of any relationship between the writing system of the target language and known languages, other than that they are all alphabetic in nature. Finally, we note some similarities of our model to some ideas proposed in other contexts. We make the assumption that each observation type (letter) occurs with only one hidden state (consonant or vowel). Similar constraints have been developed for part-of-speech tagging (Lee et al., 2010; Christodoulopoulos et al., 2011), and the power of type-based sampling has been demonstrated, even in the absence of explicit model constraints (Liang et al., 2010). 3 Model Our generative Bayesian model over the observed vocabularies of hundreds of languages is 1 We note that similar ideas were simultaneously proposed by other researchers (Cohen et al., 2011). 1528 1529 For example, the cluster Poisson parameter over vowel observation types might be λ = 9 (indicating 9 vowel letters on average for the cluster), while the parameter over consonant observation types might be λ = 20 (indicating 20 consonant letters on average). These priors will be distinct for each language cluster and serve to characterize its general linguistic and typological properties"
P13-1150,P10-1107,1,0.882119,"Missing"
P14-2104,kawahara-etal-2002-construction,0,0.026535,"Missing"
P14-2104,W09-1206,0,0.0372098,"Missing"
P14-2104,C04-1100,0,0.0594076,"Missing"
P14-2104,W07-1402,0,0.0440607,"Missing"
P14-2104,H05-1047,0,0.0385336,"Missing"
P14-2104,taule-etal-2008-ancora,0,0.0344296,"Missing"
P14-2104,P10-1040,0,0.172102,"Missing"
P14-2104,J02-3001,0,0.35101,"Missing"
P14-2104,W08-2116,0,0.0276886,"Missing"
P14-2104,W09-1201,0,\N,Missing
P15-1046,W06-1615,0,0.764894,"ions because they assume that the label set is invariant. We propose a solution based on label embeddings induced from canonical correlation analysis (CCA) that reduces the problem to a standard domain adaptation task and allows use of a number of transfer learning techniques. We also introduce a new transfer learning technique based on pretraining of hidden-unit CRFs (HUCRFs). We perform extensive experiments on slot tagging on eight personal digital assistant domains and demonstrate that the proposed methods are superior to strong baselines. 1 One might attempt to apply existing techniques (Blitzer et al., 2006; Daum´e III, 2007) in domain adaption to this problem, but a straightforward application is not possible because these techniques assume that the label set is invariant. In this work, we provide a simple and effective solution to this problem by abstracting the label types using the canonical correlation analysis (CCA) by Hotelling (Hotelling, 1936) a powerful and flexible statistical technique for dimensionality reduction. We derive a low dimensional representation for each label type that is maximally correlated to the average context of that label via CCA. These shared label representation"
P15-1046,N09-1068,0,0.0197997,"ion objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning"
P15-1046,D10-1044,0,0.0119326,"ly but output spaces (slot tags) might. Multi-task learning differs from our task. In general multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models"
P15-1046,W10-2604,0,0.00808131,"and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. 2 Jeong and Lee (2009) pointed out that if the domain is"
P15-1046,N10-1004,0,0.0166861,"rvice request utterances) do not change drastically but output spaces (slot tags) might. Multi-task learning differs from our task. In general multi-task learning aims to improve performance across all domains while our domain adaptation objective is to optimize the performance of semantic slot tagger on the target domain. Below, we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic meth"
P15-1046,P07-1034,0,0.0762092,"we review related work in domain adaption and natural language understanding (NLU). 2.1 Related Work Domain adaptation has been widely used in many natural language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. 2 Jeong and"
P15-1046,D12-1031,1,0.858892,"Missing"
P15-1046,P13-1150,1,0.853578,"Missing"
P15-1046,Q14-1002,0,0.0311458,"Missing"
P15-1046,D11-1030,1,0.907365,"Missing"
P15-1046,H05-1094,0,0.299522,"language processing (NLP) applications including part-of-speech tagging (Schnabel and Sch¨utze, 2014), parsing (McClosky et al., 2010), and machine translation (Foster et al., 2010). Most of the work can be classified either supervised domain adaptation (Chelba and Acero, 2006; Blitzer et al., 2006; Daume III and Marcu, 2006; Daum´e III, 2007; Finkel and Manning, 2009; Chen et al., 2011) or semi-supervised adaptation (Ando and Zhang, 2005; Jiang and Zhai, 2007; Kumar et al., 2010; Huang and Yates, 2010). Our problem setting falls into the former. Multi-task learning has become popular in NLP. Sutton and McCallum (2005) showed that joint 3 Sequence Modeling Technique The proposed techniques in Section 4 and 5 are generic methodologies and not tied to any particular models such as any sequence models and instanced based models. However, because of superior performance over CRF, we use a hidden unit CRF (HUCRF) of Maaten et al. (2011). 1 Multilingual learning (Kim et al., 2011; Kim and Snyder, 2012; Kim and Snyder, 2013) has same setting. 2 Jeong and Lee (2009) pointed out that if the domain is given, their method is the same as that of Daum´e III (2007). 474 As in restricted Boltzmann machines (Larochelle and"
P15-1046,N15-1009,1,0.310738,"s. First, we aim to optimize performance on the target domain by minimizing the gap between source and target domain while multi-task learning jointly learns the shared tasks. Second, in our problem the domains are different, but they are closely related. On the other hand, prior work focuses on multiple subtasks of the same data. Despite the increasing interest in NLU (De Mori et al., 2008; Xu and Sarikaya, 2013; Sarikaya et al., 2014; Xu and Sarikaya, 2014; Anastasakos et al., 2014; El-Kahky et al., 2014; Liu and Sarikaya, 2014; Marin et al., 2014; Celikyilmaz et al., 2015; Ma et al., 2015; Kim et al., 2015), transfer learning in the context of NLU has not been much explored. The most relevant previous work is Tur (2006) and Li et al. (2011), which described both the effectiveness of multi-task learning in the context of NLU. For multi-task learning, they used shared slots by associating each slot type with aggregate active feature weight vector based on an existing domain specific slot tagger. Our empirical results shows that these vector representation might be helpful to find shared slots across domain, but cannot find bijective mapping between domains. Also, Jeong and Lee (2009) presented a t"
P15-1046,P07-1033,0,\N,Missing
P15-2032,P14-2104,1,0.612968,"Missing"
P15-2032,P13-1090,1,0.878428,"Missing"
P15-2032,N15-1009,1,0.34724,"Missing"
P15-2032,W02-1001,0,0.122276,"hnique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Fi"
P15-2032,P15-2132,1,0.543027,"Missing"
P15-2032,P15-1046,1,0.471341,"Missing"
P15-2032,P14-1129,0,0.077615,"Missing"
P15-2032,W03-0430,0,0.0482054,"duction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical repre"
P15-2032,N04-1043,0,0.0547072,"bles are maximally correlated. 0 Let x(1) . . . x(n) ∈ Rd and y (1) . . . y (n) ∈ Rd be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k: Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in unlabeled text. Since it learns to associate the words to their clusters, the quality of clusters becomes important. A straightforward approach would be to perform Brown clustering (Brown et al., 1992), which has been very effective in a variety of NLP tasks (Miller et al., 2004; Koo et al., 2008). However, Brown clustering has some undesirable aspects for our purpose. First, it assigns a single cluster to each word type. Thus a word that can be used very differently depending on its context (e.g., “bank”) is treated the same across the corpus. Second, the Brown model uses only unigram and bigram statistics; this can be an issue if we wish to capture semantics in larger contexts. Finally, the algorithm is rather slow in practice for large vocabulary size. To mitigate these limitations, we propose multisense clustering via canonical correlation analysis (CCA). While t"
P15-2032,P12-1092,0,0.0375882,"ialization as well, we choose to only use θ1 since the label space is taskspecific. This process is illustrated in Figure 2. In summary, the first step is used to find generic parameters between observations and hidden states; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 n X (l) [[yi = 1]] i=1 In the second step, we train a final model on the labeled data {(x(i) , y (i) )}N i=1 using θ1 as an initialization point: θ,γ: vi = Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014), our proposed method is simpler and is shown to perform better in experiments. 3.1 Review of CCA CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated. 0 Let x(1) . . . x(n) ∈ Rd and y (1) . . . y (n) ∈ Rd be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k: Multi-Sense Clustering via CCA The proposed pre-training method requires assigning"
P15-2032,D14-1113,0,0.0141475,"we choose to only use θ1 since the label space is taskspecific. This process is illustrated in Figure 2. In summary, the first step is used to find generic parameters between observations and hidden states; the second step is used to specialize the parameters to a particular task. Note that the first step also generates additional feature types absent in the labeled data which can be useful at test time. 3 n X (l) [[yi = 1]] i=1 In the second step, we train a final model on the labeled data {(x(i) , y (i) )}N i=1 using θ1 as an initialization point: θ,γ: vi = Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2014), our proposed method is simpler and is shown to perform better in experiments. 3.1 Review of CCA CCA is a general technique that operates on a pair of multi-dimensional variables. CCA finds k dimensions (k is a parameter to be specified) in which these variables are maximally correlated. 0 Let x(1) . . . x(n) ∈ Rd and y (1) . . . y (n) ∈ Rd be n samples of the two variables. For simplicity, assume that these variables have zero mean. Then CCA computes the following for i = 1 . . . k: Multi-Sense Clustering via CCA The proposed pre-training method requires assigning a cluster to each word in u"
P15-2032,D12-1031,1,0.776688,"niques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical representation of hidden unit CRFs. pθ,γ (y, z|x) = data. The intuition"
P15-2032,N10-1013,0,0.0969604,"Missing"
P15-2032,N03-1028,0,0.0324813,"ent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical representation of hidden uni"
P15-2032,P10-1040,0,0.0629196,"he deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 192–198, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 2: Illustration of a pre-training scheme for HUCRFs. 0 θ ∈ Rd and γ ∈ Rd and defines a joint probability of y and z conditioned on x as follows: Figure 1: Graphical representation of hidden unit CRFs. pθ,γ (y, z|x)"
P15-2032,I13-1183,0,0.0231971,"her improve the accuracy within the proposed framework. We report consistent gains over standard conditional random fields (CRFs) and HUCRFs without pre-training in semantic tagging, named entity recognition (NER), and part-of-speech (POS) tagging tasks, which could indicate the task independent nature of the proposed technique. 1 Introduction Despite the recent accuracy gains of the deep learning techniques for sequence tagging problems (Collobert and Weston, 2008; Collobert et al., 2011; Mohamed et al., 2010; Deoras et al., 2012; Xu and Sarikaya, 2013; Yao et al., 2013; Mesnil et al., 2013; Wang and Manning, 2013; Devlin et al., 2014), conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) still have been widely used in many research and production systems for the problems due to the effectiveness and simplicity of training, which does not involve task specific parameter tuning (Collins, 2002; McCallum and Li, 2003; Sha and Pereira, 2003; Turian et al., 2010; Kim and Snyder, 2012; Celikyilmaz et al., 2013; Sarikaya et al., 2014; Anastasakos et al., 2014; 192 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Jo"
P15-2032,J92-4003,0,\N,Missing
P15-2032,P08-1068,0,\N,Missing
P15-2132,P15-1046,1,0.609734,"t solution to this problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowle"
P15-2132,J92-4003,0,0.26351,"onaries are mined from the web and search logs automatically using basic pattern matching approaches (e.g. entities sharing the same or similar context in queries or documents) and consequently contain significant amount of noise. As the table indicates, the number of elements in total across all the gazetteers (#total gazet elements) in each domain are too large for models to consume. In all our experiments, we trained conditional random fields (CRFs) (Lafferty et al., 2001) with the following features: (1) n-gram features up to n = 3, (2) regular expression features, and (3) Brown clusters (Brown et al., 1992) induced from search logs. With these features, we compare the following methods to demonstrate the importance of adding appropriate gazetteers: The age of adaline imdb.com en.wikipedia.org youtube.com rottentomatoes.com movieinsider.com Table 1: Top clicked URLs of two movies. One issue with using only click logs is that some entities may not be covered in the query logs since logs are extracted from a limited time frame (e.g. six months). Even the big search engines employ a moving time window for processing and storing search logs. Consequently, click logs are not necessarily good evidence."
P15-2132,P13-1090,1,0.764586,"Missing"
P15-2132,D09-1055,0,0.0188485,"problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowledge graph. 4 Experime"
P15-2132,N13-1139,1,0.525734,"Missing"
P15-2132,N15-1009,1,0.309831,"t solution to this problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowle"
P15-2132,P15-2032,1,0.517976,"t solution to this problem (Hotelling, 1936). The resulting solution 0 u1 . . . uk ∈ Rd and v1 . . . vk ∈ Rd can be used to project the variables from the original d- and d0 -dimensional spaces to a k-dimensional space: Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been 808 Subject Jason Statham Jason Statham Jason Statham Romeo & Juliet Romeo & Juliet used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 imdb.com en.wikipedia.org furious7.com rottentomatoes.com www.msn.com Relation type type type type type Object film.actor tv.actor film.producer film.film music.album Table 2: Entities & relation in the knowle"
P16-2002,N13-1139,1,0.845447,"chines, to build distributed memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . ."
P16-2002,P15-2132,1,0.866296,"ibuted memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . . . xn . A recent re"
P16-2002,P15-2032,1,0.498603,"ibuted memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . . . xn . A recent re"
P16-2002,P15-1046,1,0.929507,"ibuted memory systems achieving large memory capacity. However, building and maintaining these industry grade clusters is not trivial and thus not accessible to everyone. It is critical to have techniques that can process large data within a limited memory budget available in most typical enterprise servers. One solution is to approximate the matrix with some Y ∈ Rl×d where l  n. Many matrix approximation techniques have been proposed, such as random projection (Papadimitriou et al., 1998; Vempala, 2005), sampling (Drineas and Kannan, 2003; Rudelson and Vershynin, 2007; Kim and Snyder, 2013; Kim et al., 2015b), and hashing (Weinberger et al., 2009). Most of these techniques involve randomness, which can be undesirable in certain situations (e.g., when experiments need to be exactly reproducible). Moreover, many are not designed directly for the objective that we care about: namely, ensuring that the covariance matrices X > X and Y > Y remain “similar”. Σj,j = r   2 max Σ2j,j − σbl/2c ,0 iii. Set Y = ΣV > . Output:Y ∈ Rl×d s.t. X > X − Y > Y 2 ≤ 2 ||X||2F /l Figure 1: Matrix sketching algorithm by Liberty (2013). In the output, X ∈ Rn×d denotes the data matrix with rows x1 . . . xn . A recent re"
P16-2002,D14-1162,0,0.0846512,"Missing"
P16-2002,W15-1511,1,0.843538,"ation. We do so by learning a PCA projection matrix Π from the unlabeled data and applying it on both training and test sentences. The matrix sketching algorithm in Figure 1 enables us to compute Π on arbitrarily large data. There are many design considerations for using the sketching algorithm for our task. feature vector of a bag-of-words sentence representation. Specifically, if x is the original bag-ofwords sentence vector, the new representation is given by 3.1 where ⊕ is the vector concatenation operation. This representational scheme is shown to be effective in previous work (e.g., see Stratos and Collins (2015)). xnew = Original sentence representations We use a bag-of-words vector to represent a sentence. Specifically, each sentence is a ddimensional vector x ∈ Rd where d is the size of the vocabulary and xi is the count of an n-gram i in the sentence (we use up to n = 3 in experiments); we denote this representation by SENT. In experiments, we also use a modification of this representation, denoted by SENT+, in which we explicitly define features over the first two words in a query and also use intent predictions made by a supervised model. 3.2 3.5 Random hashing Parallelization Experiment X ∈ R17"
P16-2002,N15-1009,1,\N,Missing
P17-1060,P07-1056,0,0.289109,"Missing"
P17-1060,W06-1615,0,0.442319,"g problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), adversary domain training (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There is a venerable history of research on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared acro"
P17-1060,T75-2026,0,0.460678,"Missing"
P17-1060,N15-1009,1,0.739317,"es difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a c"
P17-1060,D16-1222,1,0.91248,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,P17-1119,1,0.268559,"aining approach of Kim et al. (2016c), a neural analog of Daum´e III (2009). 2 2.1 Related Work Domain Adaptation Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), adversary domain training (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There is a venerable history of research on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from a"
P17-1060,P15-2132,1,0.857862,"es difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a c"
P17-1060,P15-2032,1,0.835715,"es difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredients shown in Figure 1: character and word embedding, a bidirectional LSTM (BiLSTM) at a c"
P17-1060,C16-1193,1,0.943688,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,C16-1038,1,0.647854,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,P16-2002,1,0.914095,"arch on domain adaptation (Daume III and Marcu, 2006; Daum´e III, 2009; Blitzer et al., 2006, 2007; Pan et al., 2011) which is concerned with the shift in data distribution from one domain to another. In the context of NLP, a particularly successful approach is the feature augmentation method of Daum´e III (2009) whose key insight is that if we partition the model parameters to those that handle common patterns and those that handle domainspecific patterns, the model is forced to learn from all domains yet preserve domain-specific knowledge. The method is generalized to the neural paradigm by Kim et al. (2016c) who jointly use a domain-specific LSTM and also a global LSTM shared across all domains. In the context of SLU, Jaech et al. (2016) proposed K domain-specific feedforward layers with a shared word-level LSTM layer across domains; Kim et al. (2016c) instead employed K + 1 LSTMs. Hakkani-T¨ur et al. (2016) proposed to employ a sequence-to-sequence model by introducing a fictitious symbol at the end of an utterance of which tag represents the corresponding domain and intent. All these methods require one to re-train a model from scratch to make it learn the correlation and invariance between d"
P17-1060,N16-1030,0,0.0318111,"? Utterance Figure 1: The overall network architecture of the individual model. 3.1 for each i = 1 . . . n.2 Next, the model computes  W vi , fi−1 ∀i = 1 . . . n fiW = φW f  W bW vi , bW ∀i = n . . . 1 i = φb i+1 Individual Model Architecture Let C denote the set of character types and W the set of word types. Let ⊕ denote the vector concatenation operation. A wildly successful architecture for encoding a sentence (w1 . . . wn ) ∈ W n is given by bidirectional LSTMs (BiLSTMs) (Schuster and Paliwal, 1997; Graves, 2012). Our model first constructs a network over an utterance closely following Lample et al. (2016). The model parameters Θ associated with this BiLSTM layer are and induces a character- and context-sensitive word representation hi ∈ R200 as hi = fiW ⊕ bW i (1) for each i = 1 . . . n. These vectors can be used to perform intent classification or slot tagging on the utterance. • Character embedding ec ∈ R25 for each c ∈ C Intent Classification We can predict the intent of the utterance using (h1 . . . hn ) ∈ R200 in (1) as follows. Let I denote the set of intent types. We introduce a single-layer feedforward network g i : R200 → R|I |whose parameters are denoted by Θi . We compute a |I|-dime"
P17-1060,W16-3603,0,0.00624568,"from scratch to make it learn the correlation and invariance between domains. This becomes difficult to scale when there is a new domain coming in at high frequency. We address this problem by proposing a method that only calls K trained domain experts; we do not have to re-train these domain experts. This gives a clear computational advantage over the feature augmentation method. 2.2 There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). 3 Method We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhuber (1997) for a detailed description. At a high level, the individual model consists of builds on several ingredient"
P17-1119,T75-2026,0,0.648677,"Missing"
P17-1119,N15-1009,1,0.687347,"Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang,"
P17-1119,D16-1222,1,0.931685,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,P17-1060,1,0.268354,"ainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). All th"
P17-1119,P15-2132,1,0.868211,"Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang,"
P17-1119,P15-2032,1,0.816611,"Spoken language understanding (SLU) is an important component of these examples that allows natural communication between the user and the agent (Tur, 2006; El-Kahky et al., 2014). PDAs support a number of scenarios including creating reminders, setting up alarms, note taking, scheduling meetings, finding and consuming entertainment (i.e. movie, music, games), finding places of interest and getting driving directions to them (Kim et al., 2016a). Naturally, there has been an extensive line of prior studies for domain scaling problems to easily scale to a larger number of domains: pretraining (Kim et al., 2015c), transfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang,"
P17-1119,C16-1193,1,0.943775,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,C16-1038,1,0.566274,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,P16-2002,1,0.91564,"rvised DA. They partition the model parameters into two parts: one inducing domainspecific (or private) features and the other domaininvariant (or shared) features. The domaininvariant parameters are adversarially trained using a gradient reversal layer to be poor at domain classification; as a consequence, they produce representations that are domain agnostic. This approach is motivated by a rich literature on the theory of DA pioneered by Ben-David et al. (2007). We describe our use of adversarial training in Section 3.2.3. A special case of Ganin et al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al."
P17-1119,Q17-1036,0,0.0815866,"al. (2016) is developed independently by Kim et al. (2016c) who motivate the method as a generalization of the feature augmentation method of Daum´e III (2009). Bousmalis et al. (2016) extend the framework of Ganin et al. (2016) by additionally encouraging the private and shared features to be mutually exclusive. This is achieved by minimizing the dot product between the two sets of parameters and simultaneously reconstructing the input (for all domains) from the features induced by these parameters. Both Ganin et al. (2016) and Bousmalis et al. (2016) discuss applications in computer vision. Zhang et al. (2017) apply the method of Bousmalis et al. (2016) to tackle transfer learning in NLP. They focus on transfer learning between classification tasks over the same domain (“aspect transfer”). They assume a set of keywords associated with each aspect and use these keywords to inform the learner of the relevance of each sentence for that aspect. 2.2 Spoken Language Understanding Recently, there has been much investment on the personal digital assistant (PDA) technology in industry (Sarikaya, 2015; Sarikaya et al., 2016). Apples Siri, Google Now, Microsofts Cortana, and Amazons Alexa are some examples of"
P17-1119,W16-3603,0,0.0115873,"nsfer learning (Kim et al., 2015d), constrained decoding with a single model (Kim et al., 2016a), multi-task learning (Jaech et al., 2016), neural domain adaptation (Kim et al., 2016c), domainless adaptation (Kim et al., 2016b), a sequence-to-sequence model (Hakkani-T¨ur et al., 2016), domain attention (Kim et al., 2017) and zero-shot learning(Chen et al., 2016; Ferreira et al., 2015). There are also a line of prior works on enhancing model capability and features: jointly modeling intent and slot predictions (Jeong and Lee, 2008; Xu and Sarikaya, 2013; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016a,b), modeling SLU models with web search click logs (Li et al., 2009; Kim et al., 2015a) and enhancing features, including representations (Anastasakos et al., 2014; Sarikaya et al., 2014; Celikyilmaz et al., 2016, 2010; Kim et al., 2016d) and lexicon (Liu and Sarikaya, 2014; Kim et al., 2015b). All the above works assume that there are no any data shift issues which our work try to solve. 3 3.1 Method BiLSTM Encoder We use an LSTM simply as a mapping φ : Rd × 0 0 Rd → Rd that takes an input vector x and a state vector h to output a new state vector h0 = φ(x, h). See Hochreiter and Schmidhube"
P18-1206,C90-2036,0,0.330743,"nd noise suppression, however it remains important for our model to be noise robust. 3.1 Data Programming The key insight of the Data Programming approach is that O(1) simple labeling functions can be used to approximate O(n) human annotated data points with much less effort. We adopt the formalism used by (Ratner et al., 2016) to treat each of instance data generation rule as a rich generative model, defined by a labeling function λ and describe different families of labeling functions. Our data programming pipeline is analogous to the noisy channel model proposed for spelling correction by (Kernighan et al., 1990), and consists of a set of candidate generation and noise detection functions. arg max P (µ|si ) = arg max P (si |µ). P (µ) µ µ where µ and si represent utterances and the ith skill respectively. P (si |µ) the probability of a skill being valid for an utterance is approximated by simple functions that act as candidate data generators λg ∈ Λg based on recognitions produced by a family of query patterns λq ∈ Λq . P (µ) is represented by a family of simple functions that act as noise detectors λn ∈ Λn , which mark utterances as likely being noise. We apply the technique to the query logs of a pop"
P18-1206,N15-1009,1,0.844478,"xpansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Weakly Supervised Training Data Generation Our model addresses the domain classification task in SLU systems. In traditional IPDA systems, these domains are han"
P18-1206,W17-2607,0,0.045273,"ir hierarchical softmax-based output formulation is unsuitable for incremental model updates. Work on zero-shot domain classifier expansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Weakly Supervised Training Da"
P18-1206,P17-1119,1,0.823103,"odels have been explored for the Named Entity Recognition task by Lample et al. (2016). Character-informed sequence models have also been explored for simple text classification with small sets of classes by Xiao and Cho (2016). Joulin et al. (2016) explored highly scalable text classification using a shared hierarchical encoder, but their hierarchical softmax-based output formulation is unsuitable for incremental model updates. Work on zero-shot domain classifier expansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b)"
P18-1206,P17-1060,1,0.86349,"odels have been explored for the Named Entity Recognition task by Lample et al. (2016). Character-informed sequence models have also been explored for simple text classification with small sets of classes by Xiao and Cho (2016). Joulin et al. (2016) explored highly scalable text classification using a shared hierarchical encoder, but their hierarchical softmax-based output formulation is unsuitable for incremental model updates. Work on zero-shot domain classifier expansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b)"
P18-1206,P11-1055,0,0.0972761,"Missing"
P18-1206,P15-2032,1,0.861677,"xpansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Weakly Supervised Training Data Generation Our model addresses the domain classification task in SLU systems. In traditional IPDA systems, these domains are han"
P18-1206,C16-1038,1,0.795614,"xpensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Weakly Supervised Training Data Generation Our model addresses the domain classification task in SLU systems. In traditional IPDA systems, these domains are hand-crafted by experts to be well separable and can easily be annotated by humans because they are small in number. The emergence of self-service SLU results in a large number of potentially mutually overlapping SLU domains. This means that eliciting large volumes of hig"
P18-1206,P15-1046,1,0.817075,"xpansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive. Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017). There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a). 3 Weakly Supervised Training Data Generation Our model addresses the domain classification task in SLU systems. In traditional IPDA systems, these domains are han"
P18-1206,N16-1030,0,0.0497143,"imple linear models such as Multinomial Logistic Regression or Support Vector Machines in a one-versusall setting for multi-class prediction. The models typically use word n-gram features and also those based on static lexicon match, and there have been several recent studies applying deep learning techniques (Xu and Sarikaya, 2014). There is also a line of prior work on enhancing sequential text classification or tagging. Hierarchical character-to-word level LSTM (Hochreiter and Schmidhuber, 1997) architectures similar to our models have been explored for the Named Entity Recognition task by Lample et al. (2016). Character-informed sequence models have also been explored for simple text classification with small sets of classes by Xiao and Cho (2016). Joulin et al. (2016) explored highly scalable text classification using a shared hierarchical encoder, but their hierarchical softmax-based output formulation is unsuitable for incremental model updates. Work on zero-shot domain classifier expansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains. The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping a"
W15-4319,W15-4308,0,0.196632,"et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supra"
W15-4319,P06-2005,0,0.224961,"Missing"
W15-4319,W15-4312,0,0.040755,"d using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the basis of n-grams tatistics. BEKLI (Beckley, 2015) A substitution dictionary is constructed in which keys are non-standard words and values are lists of potential normalizations. Frequent morphology errors are captured by hand-crafted rules. Finally, the Viterbi algorithm is applied to bigram sequences to decode the normalized sentence with maximum probability. LYSGROUP (Mosquera et al., 2015) A system originally developed for Spanish text normalization was adapted to English text normalization. The method consists of a cascaded pipeline of several data adaptors and processors, such as a Twitter POS tagger and a spell checker. 3 Named Entity"
W15-4319,W15-4318,0,0.105136,"Missing"
W15-4319,N15-1075,0,0.0319326,"Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Twitter NER (Ritter et al., 2011), which disti"
W15-4319,W15-4307,0,0.270391,"tain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-competition analysis of the effect of training on development sets is presented in the NRC system description paper (Cherry et al., 2015). 131 Figure 2: Annotation interface. POS Orthographic Gazetteers Brown clustering – X X X – – – X X X – X X – X – X X X – X – – X X X X – X – X – X X – X BASELINE Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Word embedding ML – correlation analysis – – word2vec word2vec & GloVe word2vec X – CRFsuite CRFsuite CRF++ CRF wapiti FFNN CRF++ semi-Markov MIRA entity linking CRF L-BFGS Table 7: Features and machine learning approach taken by each team. Precision Recall F ousia NLANGP nrc multimedialab USFD iitp Hallym lattice 57.66 63.62 53.24 49.52 45.72 60.68 39.59 55.17 55.22 43.12 38.5"
W15-4319,P14-2111,0,0.0445576,"r data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods,"
W15-4319,W09-2010,0,0.0437694,"guistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-"
W15-4319,W15-4306,0,0.105059,"hallenge is concept drift (Dredze et al., 2010; Fromreide et al., 2014); the distribution of language and topics on Twitter is constantly shifting leading to degraded performance of NLP tools over time. To evaluate the effect of drift in a realistic scenario, the current evaluation uses a test set from a separate time period, which was not announced to participants until the (unannotated) test data was released at the beginning of the evaluation period. To address these challenges, there has been an increasing body of work on adapting named entity recognition tools to noisy social media text (Derczynski et al., 2015b; Plank et al., 2014a; Cherry and Guo, 2015; Ritter et al., 2011; Plank et al., 2014b), however different research groups have made use of different evaluation setups (e.g. training / test splits) making it challenging to perform direct comparisons across systems. By organizing a shared evaluation we hope to help establish a common evaluation methodology (for at least one dataset) and also promote research and development of NLP tools for user-generated social media text genres. 3.1 Training and Development Data The training and development data for our task was taken from previous work on Tw"
W15-4319,W15-4322,0,0.0251441,"Missing"
W15-4319,P11-1038,1,0.912118,"Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can b"
W15-4319,D12-1039,1,0.757294,"zation examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words tha"
W15-4319,P13-1155,0,0.0178022,"rds can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task"
W15-4319,W15-4313,0,0.0798376,"shown in Tables 3 and 4. Overall, common approaches were lexicon-based methods, CRFs, and neural network-based approaches. Among the constrained systems, neural networks achieved strong results, even without off-the-shelf tools. In contrast, CRF- and lexicon-based approaches were shown to be effective in the unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et"
W15-4319,D14-1108,0,0.0271234,"new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited t"
W15-4319,W15-4323,0,0.0427639,"ING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015)"
W15-4319,D10-1057,0,0.157913,"Missing"
W15-4319,D13-1008,0,0.00982812,"level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on Engli"
W15-4319,P11-2013,0,0.247801,"dard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined paralle"
W15-4319,W15-4321,0,0.0536682,"s first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as activation function in combination with dropout to prevent overfitting. A context window of 5 words was used As input (2 words left and right). The output is a single tag of the middle word. Afterwards, a rule-based post-processing step was executed to ensure every I-tag has a B-tag in front of it and that all tags within a single span are of the same type. Train and dev were used as training data and used dev 2015 as validation set. NLANGP (Toh et al., 2015) The NLANGP team modeled the problem as a sequential labeling task and used Conditional Random Fields. Several post-processing steps (e.g. rulebased matching) were applied to refine the system output. Besides Brown clusters, Kmeans clusters were also used; the K-means clusters were generated based on word embeddings. nrc (Cherry et al., 2015) NRC applied a MIRAtrained semi-Markov tagger with Gazetteer, Brown cluster and Word Embedding features. The Word Embeddings were built over phrases using Word2Vec’s phrase finder tool, and were modified using an auto-encoder to be predictive of Gazetteer"
W15-4319,P12-1109,0,0.0482015,"es. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical no"
W15-4319,P10-1040,0,0.00514115,"d resources. There were 6 official submissions in the constrained category, and 5 official submissions in the unconstrained category. Overall, deep learning methods and methods based on lexicon-augmented conditional random fields (CRFs) achieved the best results. The winning team achieved a precision of 0.9061 precision, recall of 0.7865, and F1 of 0.8421. The named entity recognition task attracted 8 participants. The majority of teams built their systems using linear-chain conditional random fields (Lafferty et al., 2001), and many teams also used brown clusters and word embedding features (Turian et al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al."
W15-4319,P12-3005,1,0.690086,"e their system categories: • Constrained: participants could not use any data other than the provided training data to perform the text normalization task. They were allowed to use pre-trained tools (e.g., Twitter POS taggers), but no normalization lexicons or extra tweet data. • Unconstrained: participants could use any publicly accessible data or tools to perform the text normalization task. Evaluation was based on token-level precision, recall and F-score. 2.2.1 Preprocessing We first collected tweets using the Twitter Streaming API over the period 23–29 May, 2014, and then used langid.py (Lui and Baldwin, 2012)1 to remove all non-English tweets. Tokenization was performed with CMU-ARK tokeniser.2 To ensure that tweets had a high likelihood of requiring lexical normalization, we filtered out tweets with less than 2 non-standard words (i.e. words not occurring in our dictionary — see Section 2.2.3). While this biases the sample of tweets, the decision was made at a pragmatic level to ensure a reasonable level of lexical normalization and “annotation density”. This was based on a pilot study over a random sample of English tweets, in which we found that many non-standard words were actually unknown nam"
W15-4319,W15-4314,0,0.0181512,"Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al., 2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction mo"
W15-4319,W15-4317,0,0.0288445,"unconstrained category. Surprisingly, the best overall result was achieved by a constrained system, suggesting that the relative advantage in accessing additional datasets or resources has less impact than the quality of the underlying model that is used to model the task. NCSU SAS NING (Jin, 2015) Normalization candidates were generated based on the training data, and scored based on Jaccard index over character n-gram[ s]. Candidates were evaluated using random forest classifiers to offset parameter sensitivity, using features including normalization statistics, string similarity and POS. (Min et al., 2015) Word-level edits are predicted based on long-short term memory (LSTM) recurrent neural networks (RNN), using character sequences and POS tags as features. The LSTM is further complemented with a normalization lexicon induced from the training data. NCSU SAS WOOKHEE (Leeman-Munk et al., 2015) Two forward feed neural networks are used to predict: (1) the normalized token given an input token; and (2) whether a word should be normalized or left intact. Normalized tokens are further edited by a “conformer” which down-weights rare words as normalization candidates. NCSU SAS SAM IITP (Akhtar et al."
W15-4319,W15-4315,0,0.0571066,"Missing"
W15-4319,C14-1168,0,0.0359301,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,E14-1078,0,0.0294125,"t al., 2010). Notable new techniques for named entity recognition in Twitter include a semi-Markov MIRA trained tagger (nrc), Text Normalization Shared Task In this section, we outline the Twitter Text Normalization Shared Task, describing the data and annotation process, and outlining the approaches adopted by participants. 2.1 Background Non-standard words are present in many text genres, including advertisements, professional forums, and SMS messages. They can be the cause of reading and understanding problems for humans, and degrade the accuracy of text processing tools (Han et al., 2013; Plank et al., 2014a; Kong et al., 2014). Text normalization aims to transform non-standard words to their canonical forms (Sproat et al., 2001; Han and Baldwin, 2011) as shown in Figure 1. Common examples of non-standard words include abbreviations (e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have bee"
W15-4319,D11-1141,1,0.828298,"Missing"
W15-4319,N13-1050,0,0.00844789,"(e.g., u “you”), and non-standard spellings (e.g., cuming “coming” or 2mr “tomorrow”). The prevalence of non-standard words in social media text results in markedly higher out-of-vocabulary (OOV) rates; normalizing the text brings OOV rates down to more conventional levels and makes the text more amenable to automatic processing with off-theshelf tools which have been trained on edited text. Text normalization over Twitter data has been addressed at different granularities. For instance, non-standard words can be considered as spelling errors at the character (Liu et al., 2011) or word level (Wang and Ng, 2013). Text normalization can also be approached as a machine 126 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 126–135, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics Figure 1: Normalization examples translation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One ma"
W15-4319,W15-4320,0,0.0994733,"all 56.64 57.52 57.07 =1 Table 5: Precision and recall comparing one annotator against the other. Cohen’s kappa between the annotators was 0.607. Disagreements between the annotators resolved by a 3rd adjudicator for the final datasets. Team ID Affiliation Hallym iitp lattice multimedialab NLANGP nrc ousia USFD Hallym University Indian Institute of Technology Patna University Paris 3 UGent - iMinds Institute for Infocomm Research National Research Council Canada Studio Ousia University of Sheffield Table 6: Team ID and affiliation of the named entity recognition shared task participants. sia (Yamada et al., 2015). All the other teams used CRFs. On top of a CRF, the iitp team used a differential evolution based technique to obtain an optimal feature set. Most systems used the training data as well as both dev sets provided to train their system, except multimedialab which did not use dev2015 as training data and NRC which only used train. 9 Tables 8 and 9 report the results obtained by each team for segmentation and classification of the 10 named entity types and for segmentation only, respectively. 3.4 System Descriptions Following is a brief description of the approach taken by each team: 9 A post-co"
W15-4319,D13-1007,0,0.160415,"anslation task, whereby non-standard words are mapped to more canonical expressions (Aw et al., 2006). Other approaches have involved deep learning (Chrupała, 2014), cognitively-inspired approaches (Liu et al., 2012), random walks (Hassan and Menezes, 2013), and supervision using automatically-mined parallel data (Ling et al., 2013). One major challenge in text normalization research has been the lack of annotated data for training and evaluating methods. As a result, most Twitter text normalization methods have been unsupervised or semi-supervised (Cook and Stevenson, 2009; Han et al., 2012; Yang and Eisenstein, 2013), and evaluated over small-scale handannotated datasets. This has hampered analysis of the strengths and weaknesses of individual methods, and was our motivation in organizing the lexical normalization shared task. 2.2 Shared Task Design This lexical normalization shared task is focused exclusively on English, and was designed with three primary desiderata in mind: (1) to construct a much larger dataset than existing resources; (2) to allow all of 1:1, 1:N and N :1 word n-gramm appings; and (3) to cover not just OOV non-standard words but also non-standard words that happen to coincide in spel"
W15-4319,W15-4311,0,0.0298261,"2015b) A CRF model is trained over the training data, with features including word sequences, POS tags and morphology features. Post-processing heuristics are used to post-edit the output of the CRF. 5 One team (GIGO) didn’t submit a description paper. DCU - ADAPT (Wagner and Foster, 2015) A generalized perceptron method is used generate word edit operations, with features including character n-gram[ s], character classes, and RNN language model hidden layer activation features. The final normalization word is selected based on the noisy channel model with a character language model. IHD RD (Supranovich and Patsepnia, 2015) non-standard words are identified using a CRF tagger, using features such as token-level features, contextual tokens, dictionary lookup, and edit distance. Multiple lexicons are combined to generate normalization candidates. A query misspelling correction module (i.e., DidYouMean) is used to post-process the output. USZEGED (Berend and Tasn´ adi, 2015) A CRF model is used to identify tokens requiring normalization, and determine the type of normalization required. Normalization candidates are then proposed based on revised edit distance. The final normalization candidate is selected on the ba"
W15-4319,W15-4309,0,0.0208179,".14 54.31 56.28 55.22 54.61 51.44 48.5 25.72 70.63 60.29 59.81 58.82 58.13 56.81 53.01 35.71 BASELINE 53.86 46.44 49.88 =1 Table 8: Results segmenting and categorizing entities into 10 types. Hallym (Yang and Kim, 2015) The Hallym team used an approach based on CRFs using both Brown clusters and word embeddings trained using Canonical Correlation Analysis as features. iitp (Akhtar et al., 2015a) The iitp team pro=1 Table 9: Results on segmentation only (no types). posed a multi-objective differential evolution based technique for feature selection in twitter named entity recognition. lattice (Tian, 2015) Lattice employed a CRF model using Wapiti. The feature templates consisted of standard features used in stateof-the-art. They trained first a model with 132 dev 2015 and evaluated this model on train and dev. multimedialab (Godin et al., 2015) The goal of the multimedia lab system was to only use neural networks and word embeddings to show the power of automatic feature learning and semi-supervised methods. A FeedForward Neural Network was first trained, that used only word2vec word embeddings as input. Word embeddings were trained on 400 million unlabeled tweets. Leaky ReLUs were used as act"
W15-4319,W03-0419,0,0.581711,"Missing"
W15-4319,fromreide-etal-2014-crowdsourcing,0,\N,Missing
W15-4319,W15-4316,0,\N,Missing
