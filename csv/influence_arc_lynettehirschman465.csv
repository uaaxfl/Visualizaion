A88-1007,J81-4005,0,0.074611,"Missing"
A88-1007,P84-1102,0,0.0446775,"Missing"
A88-1007,J86-3002,1,\N,Missing
A88-1007,P86-1004,1,\N,Missing
A88-1007,H86-1011,1,\N,Missing
A88-1007,H86-1012,0,\N,Missing
A88-1007,P86-1005,0,\N,Missing
A88-1007,P87-1005,0,\N,Missing
A88-1007,A83-1003,0,\N,Missing
A97-1051,H92-1022,0,0.00539768,"stagging entities throughout a document might actually lead to an increase in effort required to accurately fix or remove tags in the document. the The development of the Alembic Workbench environment came about as a result of MYrRE's efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the ''Named Entity Task"".) The Alembic text processing system applies Eric Brill's notion of ru/e sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. A more powerful approach is to allow patterns, or rules, to form the basis for this pre-tagging. The Alembic phrase-rule interpreter provides the basis for developing rule-based pre-tagging heuristics in the Workbench. In the current version of the Workbench, the user is free to compose these ""phraser"" rules and group them into specialized rule sets. Figure 2 shows an example sequence of rules that could be composed for pre-tagging a c"
A97-1051,P96-1042,0,0.0076112,"ions to the user interface that we have already begun building include part-of-speech tagging (and ""dense"" markup more generally), and full parse syntactic tagging (where we believe reliable training data can be obtained much more quickly than heretofore). In these and other instances the tagging process can be accelerated by applying partial knowledge early on, transforming the task once again into that of editing and correcting. Most of these tagging tasks would be improved by making use of methods that preferentially select ambiguous data for manual annotation--for example, as described in [4]. There are a number of psychological and human factors issues that arise when one considers how the preannotated data in a mixed-initiative system may affect 354 the human editing or post-processing. If the pretagging process has a relatively high recall, then we hypothesize that the human will tend increasingly to trust the pre-annotations, and thereby forget to read the texts carefully to discover any phrases that escaped being annotated. A similar effect seems possible for relatively high precision systems, though proper interface design (to highlight the type assigned to a particular phra"
A97-1051,C96-1079,0,0.0591659,"e might want to simply pre-tag every occurrence of ""President Clinton"" with Person.. ~ Of course, these actions should be taken with some care, since mistagging entities throughout a document might actually lead to an increase in effort required to accurately fix or remove tags in the document. the The development of the Alembic Workbench environment came about as a result of MYrRE's efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the ''Named Entity Task"".) The Alembic text processing system applies Eric Brill's notion of ru/e sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. A more powerful approach is to allow patterns, or rules, to form the basis for this pre-tagging. The Alembic phrase-rule interpreter provides the basis for developing rule-based pre-tagging heuristics in the Workbench. In the current version of the Workbench, the user is free to compo"
A97-1051,C96-1047,1,0.684012,"are provided in Section 7. of interface. For example, in annotating journalistic document collections with ""Named Entity"" tags, one might want to simply pre-tag every occurrence of ""President Clinton"" with Person.. ~ Of course, these actions should be taken with some care, since mistagging entities throughout a document might actually lead to an increase in effort required to accurately fix or remove tags in the document. the The development of the Alembic Workbench environment came about as a result of MYrRE's efforts at refining and modifying our natural language processing system, Alembic [1,7], to new tasks: the Message Understanding Conferences (MUC5 and MUC6), and the TIPSTER Multi-lingual Entity Task (MET1). (See [6] for an overview and history of MUC6 and the ''Named Entity Task"".) The Alembic text processing system applies Eric Brill's notion of ru/e sequences [2,3] at almost every one of its processing stages, from part-ofspeech tagging to phrase tagging, and even to some portions of semantic interpretation and inference. A more powerful approach is to allow patterns, or rules, to form the basis for this pre-tagging. The Alembic phrase-rule interpreter provides the basis for"
A97-1051,H93-1029,1,0.691024,"rmation extraction systems in multiple languages. 7. Implementation The Alembic Workbench interface has been written in Tci/Tk. Some of the analysis and reporting utilities (available from within the interface as well as Unix command-line utilities) are written in Perl, C or Lisp. The separate Alembic NLP system consists of C preprocessing taggers (for dates, word and sentence tokenizafion and part-of-speech assignments) and a Lisp image that incorporates the rest of Alembic: the phraserule interpreter, the phrase rule learner, and a number of discourse-level inference mechanisms described in [8]. This code currently runs on Sun workstations running Sun OS 4.1.3 and Solaris 2.4 (Sun OS 5.4) and greater; we have begun porting the system to Windows NT/Windows 95. We anticipate providing an API for integrating other NLP systems in the near future. The Workbench reads and saves its work in the form of SGML-encoded files, though the original document need not contain any SGML mark-up at all. These files me parsed with the help of an SGML normalizer.4 During the course of the annotation process the Workbench uses a ""Parallel Tag File"" (PTF) format, which separates out the embedded annotatio"
breck-etal-2000-evaluate,P99-1042,1,\N,Missing
C82-1014,C82-2029,1,0.713995,"re of the same or closely related classes. In ""Patient had stiff neck and fever"" there are two readings. The reading in which ""stiff"" is the left adjunct of both ""neck"" a n d ""fever"" is eliminated because ""neck"" and ""fever"" have different subclasses: ""fever"" is a SIGN-SYMPTOM word whereas ""neck"" is a BODY-PART word. However the phrase ""stiff neck"" has a SIGN-SYMPTOM ""computed attribute"" and is in the same class as ""fever""~ therefore we do get the analysis where ""fever"" is conjoined to ""stiff neck"". A more detailed description of constraints on noun phrase conjunction is described by Hirschman [8]. FORMATTING The format itself can be viewed as a derivative of the DIS, obtained by merging several predicate-argument relations into a single larger relation. Because the formats, like the predicate-argument relations, are based on the semantic classes of the DIS, the mapping from decomposition trees into formats can be driven by a table of the correspondences between semantic classes and format columns. QUESTION-ANSWERING The predicate names used in the predicate calculus representation within the question-answering system correspond to the predicate-argument patterns of semantic classes in"
C98-1031,M95-1012,1,0.821391,"Missing"
C98-1031,A97-1029,0,0.0289473,"Missing"
C98-1031,J93-2004,0,0.02632,"Missing"
C98-1031,X96-1049,0,0.0444212,"Missing"
C98-1031,M93-1007,0,\N,Missing
E99-1011,J97-1002,0,0.100238,"Missing"
H01-1028,A97-1051,1,0.804588,"Missing"
H01-1028,P98-1122,0,0.080907,"part <fill_depart_time leg=""1""&gt;about ten PM</fill_depart_time&gt; on <fill_date leg=""1""&gt;september twenty fifth</fill_date&gt;. U21: User said: destination <fill_arrive_city leg=""1""&gt;tokyo japan</fill_arrive_city&gt; S22: System said: did you say you want to fly to <prompt_arrive_city leg=""1""&gt;san diego</prompt_arrive_city&gt;? and traced back to their point of origin. This is quite similar to our baseline manual annotation described in section 3. There have been other approaches to detecting and characterizing errors in HC dialogues. Danieli [2] used expectations to model future user ut terances, and Levow [6][7] used utterance and pause duration, as well as pitch variability to characterize errors and corrections. Dybkjær, Bernsen & Dybkjær [4] developed a set of principles of cooperative HC dialogue, as well as a taxonomy of errors typed according to which of the principles are violated. Finally, Walker et. al. [11][12] have trained an automatic classifier that identifies and predicts problems in HC dialogues. 7. DISCUSSION It is clear that our algorithm and semantic tagset, as they stand now, need improvements to reduce the number of false errors detected. However, even now the automatic method"
H01-1028,polifroni-seneff-2000-galaxy,0,0.0192967,"-filled semantic frames (one for the user’s utterances, and one for the user’s view of the system state), we can annotate the accumulation and revision of information in the paired frames. We hypothesized that, with such a representation, it would be straightforward to detect when the two views of the dialogue differ (a misunderstanding), where the difference originated (source of error), and when the two views reconverge (correction). This would be beneficial because semantic annotation often is used for independent reasons, such as measurements of concepts per turn [8], information bit rate [9], and currently active concepts [10]. Given this, if our hypothesis is correct, then by viewing semantic annotation as a representation of filling slots in user and system frames, it should be possible to detect errors automatically with little or no additional annotation. 2. SEMANTIC TAGGING We tagged 27 dialogues from 4 different systems that participated in a data collection conducted by the DARPA Communicator program in the summer of 2000. These are dialogues between paid subjects and spoken language dialogue systems operating in the air travel domain. Each dialogue was labeled with semant"
H01-1028,A00-2028,0,0.0129409,"? and traced back to their point of origin. This is quite similar to our baseline manual annotation described in section 3. There have been other approaches to detecting and characterizing errors in HC dialogues. Danieli [2] used expectations to model future user ut terances, and Levow [6][7] used utterance and pause duration, as well as pitch variability to characterize errors and corrections. Dybkjær, Bernsen & Dybkjær [4] developed a set of principles of cooperative HC dialogue, as well as a taxonomy of errors typed according to which of the principles are violated. Finally, Walker et. al. [11][12] have trained an automatic classifier that identifies and predicts problems in HC dialogues. 7. DISCUSSION It is clear that our algorithm and semantic tagset, as they stand now, need improvements to reduce the number of false errors detected. However, even now the automatic method offers some advantages over tagging errors manually, the most important of which is that many researchers already annotate their dialogues with semantic tags for other purposes and thus many errors can be detected with no additional annotation. Also, the automatic method associates errors with particular slots, e"
H01-1028,C98-1117,0,\N,Missing
H01-1038,P00-1010,1,0.854482,"Missing"
H01-1038,W00-0503,1,\N,Missing
H86-1002,P86-1004,1,0.856118,"Missing"
H86-1011,H86-1012,1,\N,Missing
H86-1011,P81-1029,1,\N,Missing
H89-1051,P88-1002,1,0.838525,"Missing"
H89-2009,H89-1049,1,0.863876,"Missing"
H89-2009,H86-1012,1,0.907655,"Missing"
H89-2009,P86-1004,1,0.853576,"Missing"
H89-2009,H86-1011,1,\N,Missing
H90-1023,H89-2019,0,0.17242,"in addition, we can evaluate entire (wellformed) dialogues, not just isolated query/answer pairs. Unanswerable Queries For unanswerable queries, we propose t h a t the system recognize that the query is unanswerable and generate (for evaluation purposes) a canonical answer such as UNANSWERABLE_QUERY. This would be scored correct in exactly those cases where the query is in fact unanswerable. The use of a canonical message side-steps the tricky issue of exactly what kind of error message to issue to the user. This solution is proposed in the general spirit of the Canonical Answer Specification [1] which requires only a minimal answer, in order to impose the fewest constraints on the exact nature of the system's answer to the user. This must be distinguished from the use of N O _ A N S W E R , which flags cases where the system does not a t t e m p t to formulate a query. The NO.ANSWER response allows the system to admit that it doesn't understand something. By contrast, the UNANSWERABLE_QUERY answer actually diagnoses the cases where the system understands the query and determines that the query cannot be answered by the database. ###01 Utterance: What are the flights from Atlanta to D"
H90-1023,H90-1044,1,0.816097,"that several sites already have the ability to process context-dependent material ([4], [6], [3]), this should enable contractors to report significantly better overall coverage of the corpus. S u b j e c t i v e Evaluation Criteria In addition to these fully automated evaluation criteria, we also propose that we include some subjective evaluation criteria, specifically: • User Satisfaction • Task Completion Quality and Time At the previous meeting, the MIT group reported on results using outside evaluators to assess system performance ([5]). We report on a similar experiment at this meeting([2]), in which three evaluators showed good reliability in scoring correct system answers. This indicates that subjective black box evaluation is a feasible approach to system evaluation. Out suggestion is that subjective evaluation techniques be used to supplement and complement the various automated techniques under development. Conclusion This proposal does not address several important issues. For example, clearly a useful system would move towards an expert system, and not remain restricted to a DB interface. We agree that this is an important direction, but have not addressed it here. We al"
H90-1023,H90-1030,1,0.79123,"the query (producing the usual CAS output). It would then reset its context to the state before query processing and add the ""canonical context"" from the canonical query and from the canonical display, leaving the system with the appropriate context to handle the next query. This is illustrated in Figure 5. This methodology allows the processing of an entire dialogue, even when the context may not be from the directly preceding query, but from a few queries back. At Unisys, we have already demonstrated the feasibility of substituting an ""external"" DB answer for the internally generated answer [3]. We currently treat the display (that is, the set ofDB tuples returned) as an entity available for reference, in order to capture answer/question dependencies, as illustrated in Figure 3. 4There is still the possibility t h a t the s y s t e m mlslnterprets the query a n d t h e n needs to use the query as context for a subsequent query. I n thls case, providing the a n s w e r m a y not help, unless there is s o m e r e d u n d a n c y b e t w e e n the query a n d the answer. USER: S h o w m e all f l i g h t s f r o m A t l a n t a SYSTEM ANSWER to Denver leaving before 11. (Wrong): NO INF"
H90-1023,H90-1027,0,0.0468426,"Missing"
H90-1023,H89-2022,0,0.0516951,"Missing"
H90-1023,H90-1028,0,\N,Missing
H90-1030,H89-2009,1,0.834755,"ansparent support for the actual interaction with an Ingres DBMS. The server supports the interaction with a logic-based query generator (for PUNDIT, qTIP; for the MIT system, TINA [5]). It provides i n p u t / o u t p u t conversion between Ingres and Prolog or Lisp (the languages of choice for language understanding systems), commands for selecting databases, informative PUNDIT Dialog ~ USER Architecture INGRES System Level Architecture Our system architecture (see Figure 1) is based on a Dialog Manager, and is taken from a previous application for navigating around Cambridge, Massachusetts [1]. The major difference is that the module providing answers for direction assistance was an expert system, while here it is a database. The Dialog Manager, upon receiving an input from the user, calls PUNDIT f o r a n interpretation ZThis work was s u p p o r t e d by D A R P A contract N000014-89C0171, administered b y the Office of Naval Research. ATI$DB Figure 1: Overall System Architecture 141 IDI Server Architecture ili!ii~i!ii~iiiiiilill i ii i!i!ii!i! !i!iiiiii[iiiiiiiii!iiiiii!iiiiii!! !:i!:!i i:!:i:i:i:i:i:i:i:i:i:i:i:i:i:i:i:i:!:i:i:i:i:i:i:i: IDIL Query o M a n a g e s c o n n e o t"
H90-1030,H90-1044,1,0.810428,"erms of grammar rules and in terms of lexicon. Starting with the direction-assistance application, we developed techniques for quantifying the growth of the system as ~This data was collected from the TI debriefing questionaires. W e thank Charles Hemphill of TI for making these questionaires available to u s . i i i I i i 0 200 400 600 800 1000 1200 Sentences in Corpus Figure 4: Incremental growth of lexicon in a function of training data. We recorded the rate of growth in terms of g r a m m a r rules and lexical items as a measure of convergence for both A T I S and directionassistance ([1],[2]) versions of PUNDIT. Our expectation is that the rate of growth should level off as more and more training is seen. To the extent that it does not, significant gaps in coverage can be expected. Figure 3 shows the incremental growth of the g r a m m a r for both domains and Figure 4 shows the incremental growth of the lexicon. It is interesting to note that after 600 sentences from the direction-assistance domain the rate of growth in both g r a m m a r and vocabulary is quite slow, indicating that this amount of training data is enough to provide a good sample of the kinds of constructions us"
H90-1030,H89-1026,0,0.163368,"ommunication with Ingres is done via an Intelligent Database Server [4], developed on another DARPA contract, which we describe in the next section. Intelligent Database Server The ATIS Intelligent Database Server (see Figure 2) consists of the Intelligent Database Interface (IDI) and a server supporting the interaction beteen QTIP and a relational database. The IDI provides a logic-based language for queries and transparent support for the actual interaction with an Ingres DBMS. The server supports the interaction with a logic-based query generator (for PUNDIT, qTIP; for the MIT system, TINA [5]). It provides i n p u t / o u t p u t conversion between Ingres and Prolog or Lisp (the languages of choice for language understanding systems), commands for selecting databases, informative PUNDIT Dialog ~ USER Architecture INGRES System Level Architecture Our system architecture (see Figure 1) is based on a Dialog Manager, and is taken from a previous application for navigating around Cambridge, Massachusetts [1]. The major difference is that the module providing answers for direction assistance was an expert system, while here it is a database. The Dialog Manager, upon receiving an input f"
H90-1030,H89-1027,0,\N,Missing
H90-1044,H89-2009,1,0.347516,"([18]) so we will only briefly describe it here. VOYAQER is a spoken language system for finding directions in Cambridge, Massachusetts. For example the user can ask questions about the locations of objects such as restaurants, universities, and hotels and distances between them. It provides output in the form of a map display as well as natural language. We have used the SUMMIT speech recognition system as well as the directionfinding expert system from VOYACER in the system we are reporting on. The architecture of the system which we report on here has also been largely described elsewhere ([1]), with the exception of the N-best processing, and so will only be summarized here. There are five major components of the system, the speech recognition system ( S U M M I T ) , the dialog manager (VFE), the PUNDIT natural language processing system, the module which formats P U N D I T ' s output for the direction finder (QTIP), and the direction finder itself. VFE takes SUMMIT'S N-best output (computed using a word-pair g r a m m a r of perplexity 60), and sends it to PUNDIT for syntactic and semantic analysis. The first candidate which PUNDIT accepts is sent to qTIP, where it is formatted"
H90-1044,P87-1022,0,0.0642094,"Missing"
H90-1044,T87-1035,0,0.068101,"Missing"
H90-1044,P83-1007,0,0.0655041,"Missing"
H90-1044,H90-1030,1,0.860639,"Missing"
H90-1044,H89-1026,0,0.0831135,"any development had taken place. Development coverage was the coverage after the system was developed for that batch of sentences, and final coverage was the increased coverage that was achieved after development on later batches of sentences. Grammar Pruning Experiments Use of tight syntactic and semantic constraints is an important source of constraint in a spoken language understanding system. There are two approaches to constructing a tight grammar for a given corpus of training material. One approach is to build the grammar incrementally, based on the observed training data, as in TINA ([14]). This approach has the disadvantage of constructing a basic g r a m m a r of English over again for each domain. The other approach is to prune a general g r a m m a r of English to cover only those constructions seen in the training data. This approach has the advantage of making available a 'library' of constructions (that is, the full grammar) which can easily be added to the system when additional data indicates a need for them. 213 In both cases, the coverage of the g r a m m a r will directly reflect the amount of training d a t a seen. We have developed a technique for pruning our gen"
H90-1044,H89-2018,0,0.0315954,"ed in this paper. We then compared the coverage and performance of the minimal grammar and lexicon on a test set, and found that a two-fold decrease in parse time was achieved with only a small loss of coverage. Our second major focus was on evaluation of specific algorithms, using the natural language system as a testbed. In particular, we compared the performance of two algorithms for reference resolution processing. Finally, our third major focus has been to evaluate the overall coverage and accuracy of the entire spoken language system. We did this using two test corpora collected at MIT ([17]), containing a total of 1015 utterances. The system was evaluated on the basis of the first utterance of the N-best output of SUMMIT accepted by PUNDIT, or the first candidate of the N-best if no utterance was accepted by PUNDIT. This paper reports results for word accuracy, sentence accuracy, application accuracy (generating an answer judged reasonable by naive evaluators), and finally false alarm rate (incorrect, incoherent or incomplete answers). S y s t e m Overview The VOYAGER system has been described in detail elsewhere, ([18]) so we will only briefly describe it here. VOYAQER is a spo"
H90-1044,H89-1027,0,0.0535747,"ue queries, which require a clarification dialog to elicit 214 The system was evaluated at the level of word, sentence and application accuracy. Word and sentence level accuracy were measured using the NIST evaluation software. In order to evaluate the system at the application level, we designed a black box evaluation task, using human evaluators to score each interchange of a dialog between the system and a user. The evaluators were five students at the University of Pennsylvania and one Unisys employee who was not a system developer. This evaluation task was similar to one reported by MIT ([19]) in that the evaluators were asked to categorize both the queries and the responses. The queries were categorized as to whether they were appropriate or inappropriate, given the capabilities of the application. They were also categorized on a three point scale of clarity, where 1 represents a clear and fluent query, 2 a partially garbled or badly stated query, and 3 represents a query that is partially or entirely uninterpretable. The responses were categorized first as to whether they were answers or error messages. The answers were then subdivided into 'correct', 'partially correct', and 'i"
H90-1044,H86-1012,1,\N,Missing
H90-1044,H89-2022,0,\N,Missing
H90-1044,H89-2008,0,\N,Missing
H91-1014,H91-1072,1,0.875379,"Missing"
H91-1014,H91-1010,0,0.118809,"Missing"
H91-1014,H91-1071,1,0.837088,", particularly the back-end component that transforms the parse tree into a representation that can be used to maintain discourse, generate confirmation messages, and produce SQL queries for accessing the OAG database. We have also connected the SUMMIT speech recognizer to our ATIS system, so that it can now accept verbal input. Recognition Component The speech recognition configuration is similar to the one used in the VOYAGERsystem and is based on the SUMMIT system [6]. For the ATIS task, we used 76 context-independent phone models trained on speaker-independent data collected at TI and MIT [3]. There were 1284 TI sentences (read and spontaneous versions of 642 sentences) and 1146 spontaneous sentences taken from the MIT training corpus. The lexicon was derived from the vocabulary used by the ATIS natural language component and consisted of 577 words. In order to provide some conservative natural language constraints, the speech recognition component used a generalized word-pair grammar derived from the speech training data augmented with a large number of additional sentences pooled from all available sources of nTIS related text material. The wordpair grammar was generated by pars"
H91-1014,H91-1070,1,0.897645,"e M I T ATIS S y s t e m 1 Stephanie Seneff, James Glass, David Goddeau, David Goodine, Lynette Hirschman, Hong Leung, Michael Phillips, Joseph Polifroni, and Victor Zue Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology Cambridge, Massachusetts 02139 ABSTRACT have collected over the past few months [3]. Aspects of the system involving discourse and dialogue are based on similar principles as before, but has been modified to reflect the new semantic representations. A detailed description of our discourse model can be found in a companion paper [4]. This paper represents a status report on the MIT ATIS system. The most significant new achievement is that we now have a speech-input mode. It is based on the MIT SUMMITsystem using context independent phone models, and includes a word-pair grammar with perplexity 92 (on the June-90 test set). In addition, we have completely redesigned the back-end component, in order to emphasize portability and extensibility. The parser now produces an intermediate semantic frame representation, which serves as the focal point for all back-end operations, such as history management, text generation, and SQ"
H91-1014,H90-1028,1,0.809974,"tem that are tied to a particular domain are now entered through a set of tables associated with a small artificial language for decoding them. We have also improved the display of the database table, making it considerably easier for a subject to comprehend the information given. We report here on the results of the official DARPA February-91 evaluation, as well as on results of an evaluation on data collected at MIT, for both speech input and text input. SYSTEM DESCRIPTION In this section we will describe those aspects of the system that have changed significantly since our report last June [5]. The most significant change has been the incorporation of the speech recognition component. We begin by describing the recognizer configuration and the interface mechanism we are currently using. In the natural language component, the parser and grammar remain unchanged, except for augmentations to improve coverage. However, we have completely redesigned the component that translates from a parse tree to executable SQL queries, and the component that generates verbal responses. Both of these areas are described here in more detail. INTRODUCTION Speech In June 1990, we reported on the initial"
H91-1014,H90-1043,1,0.88538,"in the first round of DARPA common evaluation using text input [5]. Since then, a number of changes have been made to our system, particularly the back-end component that transforms the parse tree into a representation that can be used to maintain discourse, generate confirmation messages, and produce SQL queries for accessing the OAG database. We have also connected the SUMMIT speech recognizer to our ATIS system, so that it can now accept verbal input. Recognition Component The speech recognition configuration is similar to the one used in the VOYAGERsystem and is based on the SUMMIT system [6]. For the ATIS task, we used 76 context-independent phone models trained on speaker-independent data collected at TI and MIT [3]. There were 1284 TI sentences (read and spontaneous versions of 642 sentences) and 1146 spontaneous sentences taken from the MIT training corpus. The lexicon was derived from the vocabulary used by the ATIS natural language component and consisted of 577 words. In order to provide some conservative natural language constraints, the speech recognition component used a generalized word-pair grammar derived from the speech training data augmented with a large number of"
H91-1014,H90-1074,1,\N,Missing
H91-1070,H91-1014,1,0.895401,"s the present status of the discourse and dialogue models within the MIT AWlS system. After describing the models, we will illustrate some of the system's capabilities by way of an example. We then describe our preliminary attempts at collecting data in a booking mode, for which we hive included a complete dialogue elicited from one of our subjects. Finally, potential implications for improvemeats in speech recognition are discussed. MODELLING METHOD OLOGY The back-end component of the MIT ATm system has been completely redesigned since last June [5]. The main system is described in detail in [4], and will only be briefly 354 anaphoric reference such as &quot;it&quot; or &quot;these flights.&quot; Instead, individual modifiers are inherited unless new modifiers override their inheritance. History elements are stored in the standard frame format, and inheritance of a modifier usually amounts to simply inserting it into the appropriate frame of the new sentence. Frame format : [name ~ype key1: va3.uel key2:value2 °.,] F r a,me : [veri~y clause ~opic: [ f l i g h t qset number: 22] predicate: [serve predicate theme': [&quot;dinner&quot; reference ref~ype: meal]]] Figure 1: Frame representation resulting from analysis"
H91-1070,H90-1028,1,0.833794,"user expectations too high. This paper describes the present status of the discourse and dialogue models within the MIT AWlS system. After describing the models, we will illustrate some of the system's capabilities by way of an example. We then describe our preliminary attempts at collecting data in a booking mode, for which we hive included a complete dialogue elicited from one of our subjects. Finally, potential implications for improvemeats in speech recognition are discussed. MODELLING METHOD OLOGY The back-end component of the MIT ATm system has been completely redesigned since last June [5]. The main system is described in detail in [4], and will only be briefly 354 anaphoric reference such as &quot;it&quot; or &quot;these flights.&quot; Instead, individual modifiers are inherited unless new modifiers override their inheritance. History elements are stored in the standard frame format, and inheritance of a modifier usually amounts to simply inserting it into the appropriate frame of the new sentence. Frame format : [name ~ype key1: va3.uel key2:value2 °.,] F r a,me : [veri~y clause ~opic: [ f l i g h t qset number: 22] predicate: [serve predicate theme': [&quot;dinner&quot; reference ref~ype: meal]]] Figure"
H91-1070,H91-1071,1,\N,Missing
H91-1070,J86-3001,0,\N,Missing
H91-1072,H90-1053,0,0.0234353,"a probability estimate for the next word given the preceding word sequence. We feel that a next-word probability is much more appropriate than a rule-production probability for incorporating into a tightly coupled system, since it leads to a simple definition of the total score for the next word as the weighted sum of the language model probability and the acoustic probability. While rule-production probabilities can in fact be generated from the probabilities we provide, they will not, in general, agree with the probabilities as determined by a procedure such as the inside/outside algorithm [6,2]. ARCHITECTURE The VOYAGER system consists of the TINA natural language understanding system and the s u MMIT speech recognition system. These components will only be described briefly here, as they are more fully documented in [7,9,10]. TINA combines a general English syntax at the top level with a semantic grammar framework at lower levels, to provide an interleaved syntax/semantics analysis that minimizes perplexity. As a result, most sentences in TINA have only one parse. TINA uses a best-first heuristic search in parsing, storing alternate candidate parse paths while it pursues the most p"
H91-1072,H89-1012,0,0.027674,"e obvious solution is to bring linguistic knowledge to bear. One way is to take the best acoustic candidate and use a flexible, semantically-based phrase-spotting system to assign a meaning to the sequence of words [8]. This provides a robust interface which can ignore many recognition errors and abandons the notion of a linguistically well-formed overall sentence. It almost always produces some interpretation. However, since it adds no real linguistic constraints, it may produce many false positives (misinterpretation of the input ) . A second possiblity which has been explored at some sites [1] is to have the recognizer produce a word lattice, with (acoustic) transition probabilities between words. The language system can then search this lattice for the best candidate. or with a tighter coupling in which active partial theories dynamically prune the set of allowable next-word candidates during the search. TRAINING PARSE PROBABILITIES Another approach, which is the baseline for these experiments, uses an N-best interface between the recognizer and the language understanding system. In this interface, the recognizer produces sentence hypotheses in decreasing order of acoustic score."
H91-1072,H89-1026,1,0.864332,"eads to a simple definition of the total score for the next word as the weighted sum of the language model probability and the acoustic probability. While rule-production probabilities can in fact be generated from the probabilities we provide, they will not, in general, agree with the probabilities as determined by a procedure such as the inside/outside algorithm [6,2]. ARCHITECTURE The VOYAGER system consists of the TINA natural language understanding system and the s u MMIT speech recognition system. These components will only be described briefly here, as they are more fully documented in [7,9,10]. TINA combines a general English syntax at the top level with a semantic grammar framework at lower levels, to provide an interleaved syntax/semantics analysis that minimizes perplexity. As a result, most sentences in TINA have only one parse. TINA uses a best-first heuristic search in parsing, storing alternate candidate parse paths while it pursues the most promising (most probable) path. In addition, the grammar is trainable from instances of parse trees, as described in the next section. The SUMMIT system transforms a speech waveform into a segment lattice. Features are extracted for each"
H91-1072,H89-1027,1,0.837446,"eads to a simple definition of the total score for the next word as the weighted sum of the language model probability and the acoustic probability. While rule-production probabilities can in fact be generated from the probabilities we provide, they will not, in general, agree with the probabilities as determined by a procedure such as the inside/outside algorithm [6,2]. ARCHITECTURE The VOYAGER system consists of the TINA natural language understanding system and the s u MMIT speech recognition system. These components will only be described briefly here, as they are more fully documented in [7,9,10]. TINA combines a general English syntax at the top level with a semantic grammar framework at lower levels, to provide an interleaved syntax/semantics analysis that minimizes perplexity. As a result, most sentences in TINA have only one parse. TINA uses a best-first heuristic search in parsing, storing alternate candidate parse paths while it pursues the most promising (most probable) path. In addition, the grammar is trainable from instances of parse trees, as described in the next section. The SUMMIT system transforms a speech waveform into a segment lattice. Features are extracted for each"
H91-1072,H91-1014,1,0.879605,"Missing"
H91-1072,H90-1027,0,\N,Missing
H92-1003,H90-1022,0,0.134408,"Missing"
H92-1003,H90-1021,0,0.0307182,"Missing"
H92-1003,H91-1034,0,0.0406026,"Missing"
H92-1003,H91-1098,0,0.0452267,"Missing"
H92-1003,H92-1083,0,0.120167,"Missing"
H92-1003,H92-1008,0,0.032193,"Missing"
H92-1003,H90-1020,0,0.0283695,"Missing"
H92-1003,H91-1070,1,0.768159,"Missing"
H92-1003,H92-1009,0,0.272149,"Missing"
H92-1003,H91-1016,0,0.0760481,"Missing"
H92-1003,H92-1119,0,0.0510433,"Missing"
H92-1003,H91-1015,0,\N,Missing
H92-1003,H92-1004,0,\N,Missing
H92-1003,H89-2074,0,\N,Missing
H92-1003,H91-1008,0,\N,Missing
H92-1005,H90-1022,0,0.037099,"tween these systems: there were significant differences in ability to complete the task, number of queries required to complete the task, and score (as computed through a log file evaluation) between the robust and the non-robust modes. INTRODUCTION For the first two years of the DARPA Spoken Language Program, common evaluation in the ATIS d o m a i n has been performed solely with the C o m m o n Answer Specification (CAS) protocol [4], whereby a system&apos;s performance is determined by comparing its output, expressed as a set of database tuples, with one or more predetermined reference answers [1]. The CAS protocol has the advantage that system evaluation can be carried out automatically, once the principles for generating the reference answers have been established and a corpus has been annotated accordingly. Since direct comparison across systems can be performed relatively easily with this procedure, we have been able to achieve cross fertilization of research ideas, leading to rapid research progress. 1This research was s u p p o r t e d by D A R P A u n d e r Contract N000] 4-89-J-1332, monitored t h r o u g h the Office of Naval Research. 28 QUERY 1: PLEASE LIST THE FLIGHT FROM P"
H92-1005,H91-1071,1,0.818696,"arry out end-to-end evaluation, i.e., evaluation of overall task completion effectiveness, we must be able to determine precisely the task being solved, the correct answer(s), and when t h e s u b j e c t is done. Once these factors have been specified, we can then compute some candidate measures and see if any of them are appropriate for characterizing end-to-end system performance. While true measures of system performance will require a (near) real-time spoken language system, we felt that some preliminary experiments could be conducted within the context of our ATIS data collection effort [3,2]. In our data collection paradigm, a typist types in the subject&apos;s queries verbatim, after removing disfluencies. All subsequent processing is done automatically by the system. To collect data for end-to-end evaluation, we modified our standard data collection procedure slightly, by adding a specific scenario which has a unique answer. For this scenario, the subjects were asked to report the answer explicitly. As a preliminary experiment, we used two simple scenarios. In one of them, subjects were asked to determine 29 II Measurements I[ Mean [Std. Dev. I[ Total ~ of Queries Used 4.8 1.6 # of"
H92-1005,H92-1060,1,\N,Missing
H92-1005,H92-1003,1,\N,Missing
H92-1006,H90-1022,0,0.0393686,"Missing"
H92-1006,H90-1029,0,0.0249482,"Missing"
H92-1006,H90-1001,0,0.0726008,"Missing"
H92-1006,H91-1034,0,0.0355788,"Missing"
H92-1006,H92-1003,1,0.87241,"Missing"
H92-1006,H90-1020,0,0.050636,"Missing"
H92-1006,H92-1005,1,\N,Missing
H92-1006,H92-1009,1,\N,Missing
H92-1006,H90-1021,0,\N,Missing
H92-1006,H90-1023,1,\N,Missing
H92-1006,H92-1001,0,\N,Missing
H92-1006,H92-1068,1,\N,Missing
H92-1006,H91-1070,1,\N,Missing
H92-1006,H89-2017,1,\N,Missing
H92-1016,H90-1016,0,0.0449197,"Missing"
H92-1016,H91-1011,1,0.766755,"(labelled as B G + P L R ) shows that further reduction in error rate is possible by incorporating the PLR. P L R is incorporated by using the parse score in place of the bigram score to reorder the 50 Nbest outputs produced by the recognizer. The sentence error rate is reduced more than the word error rate, presumably due to the fact that P L R can deal with some of the long distance constraints better than the bigram. Context-Dependent Modelling At the last DARPA meeting we first described our work towards accounting for contextual effects on the phonetic modelling component of S U M M I T [5]. We proposed using regression tree analysis to find the contex3 T h i s is r o u g h l y equivalent to p a r s i n g t h e word s t r i n g as a seq u e n c e of f r a g m e n t s r a t h e r t h a n as a c o m p l e t e sentence. 86 tual factors that provided the greatest reduction in the distortion of our phonetic models. In an initial experiment, regression tree analysis was used to form a set of context-specific models for each phonetic unit. However, we found that we were able to obtain the best performance by using the regression trees to independently learn a context-normalization fact"
H92-1016,H92-1003,1,0.839497,"Missing"
H92-1016,H91-1071,1,0.869573,"Missing"
H92-1016,H91-1014,1,0.711669,"at MIT. Some 9,711 utterances in this pool were designated as training material, and an additional 1,595 utterances were set aside as a development set for independent evaluation. To facilitate a meaningful comparison, all the experiments described in this section are performed on the October '91 ""dry-run"" test set, containing some 362 utterances collected at BBN, CMU, MIT, and SRI. The experiments that we conducted are summarized in Table 1, and will be described in this section. In order to monitor progress internally, we also ran the same test set through our system as reported a year ago [8]. Our February '91 system had a vocabulary of 577 words. T h a t system constrained the N - b e s t search with the use of a word-pair g r a m m a r with a perplexity of 92. The N - b e s t outputs were subsequently resorted using our natural language component TINA. It was trained on some 2400 utterances collected at T I and MIT. The recognition performance of t h a t system on the October '91 ""dry-run"" test set, with and without the word-pair language model, is shown in the first two rows of Table 1 (labelled as AW and WP, respectively). Lexicon With the availability of a larger amount of tr"
H92-1016,H92-1060,1,\N,Missing
H94-1017,H94-1018,0,0.026025,"Missing"
H94-1017,H94-1019,0,0.0325487,", the first question is what to evaluate? Where do we put probes to inspect the input and output, in order to perform an evaluation? This issue is discussed in the Sparck Jones paper[ 1]. In some cases, we can evaluate the language technology in isolation from any front-end or back-end application, as shown in Figure 1, where probes are inserted on either side of the language interface itself. This gives us the kind of evaluation used for word error rate in speech (speech in, transcription out) or for machine translation, as proposed in the Brew/Thompson paper (source text in, target text out)[2]. This kind of evaluation computes output as a simple function of input to the language system. • Where to go from here: what additional evaluations are needed and what can be developed to support future research? 1. WHY EVALUATE? Evaluation serves a number of purposes: * Cross-system evaluation: This is a mainstay of the periodic ARPA evaluations on competing systems. Multiple sites agree to run their respective systems on a single application, so that results across systems are comparable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spok"
H94-1017,H94-1020,0,0.0453012,"parable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spoken language systems (ATIS)[8], and automated speech recognition (CSR)[8]. Unfortunately, it is not always possible to measure a meaningful output- for example, researchers have struggled long and hard with measurements for u n d e r s t a n d i n g - how can a system demonstrate that it has understood? If we had a general semantic representation, then we could insert a probe on the output side of the semantic component, independent of any specific application. The last three papers ([3, 4, 5]) take various approaches to the issue of predicate-argument , Within-system progress: This is perhaps the most important role because it supports incremental system development, debugging and even hill climbing and automated learning approaches, if fast evaluation methods are available. 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation. By creating a theory-neutral description of a correct p a r s e , the Treebank annotation enabled r e s e a r c h e r s to take the next step in agreeing to u s e the p a r s e annotations (bracketings) as a"
H94-1017,H94-1021,0,0.0578473,"parable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spoken language systems (ATIS)[8], and automated speech recognition (CSR)[8]. Unfortunately, it is not always possible to measure a meaningful output- for example, researchers have struggled long and hard with measurements for u n d e r s t a n d i n g - how can a system demonstrate that it has understood? If we had a general semantic representation, then we could insert a probe on the output side of the semantic component, independent of any specific application. The last three papers ([3, 4, 5]) take various approaches to the issue of predicate-argument , Within-system progress: This is perhaps the most important role because it supports incremental system development, debugging and even hill climbing and automated learning approaches, if fast evaluation methods are available. 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation. By creating a theory-neutral description of a correct p a r s e , the Treebank annotation enabled r e s e a r c h e r s to take the next step in agreeing to u s e the p a r s e annotations (bracketings) as a"
H94-1017,H94-1022,0,0.0604987,"parable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spoken language systems (ATIS)[8], and automated speech recognition (CSR)[8]. Unfortunately, it is not always possible to measure a meaningful output- for example, researchers have struggled long and hard with measurements for u n d e r s t a n d i n g - how can a system demonstrate that it has understood? If we had a general semantic representation, then we could insert a probe on the output side of the semantic component, independent of any specific application. The last three papers ([3, 4, 5]) take various approaches to the issue of predicate-argument , Within-system progress: This is perhaps the most important role because it supports incremental system development, debugging and even hill climbing and automated learning approaches, if fast evaluation methods are available. 1The Penn Treebank parse annotations provide an interesting case where annotation supported evaluation. By creating a theory-neutral description of a correct p a r s e , the Treebank annotation enabled r e s e a r c h e r s to take the next step in agreeing to u s e the p a r s e annotations (bracketings) as a"
H94-1017,H94-1070,0,0.0657799,"t out)[2]. This kind of evaluation computes output as a simple function of input to the language system. • Where to go from here: what additional evaluations are needed and what can be developed to support future research? 1. WHY EVALUATE? Evaluation serves a number of purposes: * Cross-system evaluation: This is a mainstay of the periodic ARPA evaluations on competing systems. Multiple sites agree to run their respective systems on a single application, so that results across systems are comparable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spoken language systems (ATIS)[8], and automated speech recognition (CSR)[8]. Unfortunately, it is not always possible to measure a meaningful output- for example, researchers have struggled long and hard with measurements for u n d e r s t a n d i n g - how can a system demonstrate that it has understood? If we had a general semantic representation, then we could insert a probe on the output side of the semantic component, independent of any specific application. The last three papers ([3, 4, 5]) take various approaches to the issue of predicate-argument , Within-system progress: This is pe"
H94-1017,H93-1003,0,0.0500447,"computes output as a simple function of input to the language system. • Where to go from here: what additional evaluations are needed and what can be developed to support future research? 1. WHY EVALUATE? Evaluation serves a number of purposes: * Cross-system evaluation: This is a mainstay of the periodic ARPA evaluations on competing systems. Multiple sites agree to run their respective systems on a single application, so that results across systems are comparable. This includes evaluations such as message understanding (MUC)[6], information retrieval (TREC)[7], spoken language systems (ATIS)[8], and automated speech recognition (CSR)[8]. Unfortunately, it is not always possible to measure a meaningful output- for example, researchers have struggled long and hard with measurements for u n d e r s t a n d i n g - how can a system demonstrate that it has understood? If we had a general semantic representation, then we could insert a probe on the output side of the semantic component, independent of any specific application. The last three papers ([3, 4, 5]) take various approaches to the issue of predicate-argument , Within-system progress: This is perhaps the most important role becau"
H94-1017,H93-1004,0,0.0519035,"xt? What is the best response to a particular query? For such cases, it is often expedient to rely on human judgements, provided that these judgements (or relative judgements) are reproducible, given a sufficient number of judges. Evaluation of machine translation systems[lO] has used human judges to evaluate systems with differing degrees of interactivity and across different language pairs. The Brew and Thompson paper[2] also describes reliability of human judges in evaluating machine translation systems. Human judges have also been used in end-to-end evaluation of spoken language interfaces[11]. 4. WHERE T O G O FROM HERE? Because evaluation plays such an important role in driving research, we must weigh carefully what and how we evaluate. Evaluation should be theory neutral, to avoid bias against novel approaches; it should also push the frontiers of what we know how to do; and finally, it should support a broad range of research interests because evaluation is expensive. It requires significant community investment in infrastructure, not to mention time devoted to running evaluations and participating in them. For example, we estimate that the ATIS evaluation required several pers"
H94-1017,H94-1024,0,\N,Missing
H94-1017,H93-1011,0,\N,Missing
J86-3002,C82-1014,1,0.874014,"Missing"
J86-3002,A83-1007,0,0.0253866,"Missing"
J86-3002,P84-1023,1,0.814419,"Missing"
J86-3002,A83-1006,0,0.0364551,"our studies of discovery procedures for domain-specific knowledge. 3 3.1 DISCOVERY PROCEDURES EXPERT VS. TEXT-BASED PROCEDURES Two basic approaches have been proposed for mechanizing (or partially mechanizing) the acquisition of domainspecific information for natural language systems. One of these is based on the systematic interviewing of a domain expert, who provides information on the basic semantic classes and relations of the domain and their linguistic forms and properties. Such an approach has been incorporated into some natural language interfaces for database retrieval, such as TEAM (Grosz 1983) and LDC (Ballard, Lusth, and Tinkham 1984). This approach assumes that the domain expert has some model of the relations in the domain, and a knowledge of the different ways in which these relations can be referenced. This is not unreasonable in the database context, since the database schema can serve as a domain model (divining all the ways in which a relationship can be referenced may still be difficult, however). This approach is more difficult, however, in text analysis applications, particularly because the user may not have such a clear model of the domain semantics from which to work."
J86-3002,J83-3003,0,0.0512428,"Missing"
J86-3002,A83-1016,0,\N,Missing
J93-3001,H91-1060,0,0.0178241,"Missing"
J93-3001,M91-1004,1,0.783053,"al match, or a mismatch, according to the official scoring guidelines. The scoring program kept a history of user interactions. It also produced a detailed score report of the template-by-template scores and a summary score report. In addition to being used for the official scoring, the program was used by the participating sites during development as a tool for determining progress and for regression testing. Additional features allowed the program to be adapted whenever the template design changed, allowed the scoring of subsets of slots and templates useful in linguistic phenomena testing (Chinchor 1991b), and allowed the merging of partial credit decisions across sites for more uniform scoring. During interactive scoring, the program kept track of what partial matches were allowed. By pooling these records across sites, the systems could be rescored, giving all systems the benefit of a partial match allowed for one site. Each site scored its answers individually for presentation at the conference, but the official scores were produced by volunteers from two sites working together. 3. Participant Methodologies 3.1 Technical Requirements The nature of the task in MUC-3 imposed a number of req"
J93-3001,M92-1003,1,0.725354,"r the significance level can be calculated or looked up in published tables (Noreen 1989) if approximate randomization is used. The higher the confidence level, the more probable it is that the approximate randomization test gave the significance level that an exact randomization test would have given. 4.3 Application of Approximate Randomization to the MUC-3 Results We use the approximate randomization technique set forth by Noreen (1989) with stratified shuffling to control for categorical variables that are not of primary interest in the hypothesis test. For more details on this method see Chinchor (1992). The test statistic we have chosen is the absolute value of the difference in recall or precision. The data consist of the four-tuples of number possible, actual, correct, and partially correct for each message for each system. The actual test statistic is calculated 431 Computational Linguistics Volume 19, Number 3 for each pair of systems. The desired number of shuffles is set to 9,999 because it was determined that 9,999 shuffles produced slightly higher confidence levels than 999 and were worth the 16-fold increase in computing time. In the algorithm, once the desired number of shuffles i"
J93-3001,M91-1003,1,0.746756,"ce in second paragraph of Figure 4) and passages marked as ""omitted."" Articles were sometimes split into message-length segments of approximately two pages each and were annotated to indicate continuation. The texts contained nonstandard constructions resulting from the translation of Spanish to Engli:~h and also contained unflagged errors, including typographical errors, misspellings, omitted words, grammatical errors, and punctuation errors. In addition, the texts posed other challenges including the frequent use of 4 Additional statistics concerning the MUC-3corpus are presented elsewhere (Hirschman 1991b). 416 Nancy Chinchor et al. Evaluating Message Understanding Systems Spanish names, untranslated words, and Spanish acronyms followed or preceded by the corresponding full phrase in English or Spanish. 1.4.4 Nature of the Template Fills. The MUC-3 template fill task was complex not only because of the richness of the textual data but also because of the variety of information required to fill template slots and because of the interdependencies among the slot fillers. The templates contained 18 slots. The message-id and template-id slots identified the template. If the message did not contain"
J93-3001,M92-1005,1,0.837858,"Missing"
J93-3001,H92-1003,1,0.344717,"Missing"
J93-3001,M92-1006,0,0.0442172,"Missing"
J93-3001,M91-1035,1,0.891483,"i ! ! n n n u 10 20 30 40 50 60 70 80 RECALL Figure 3 Recall vs. overgeneration. 412 | 90 100 Nancy Chinchor et al. Evaluating Message Understanding Systems descriptions and references. For descriptive purposes, we can group the systems into three broad classes: Pattern-Matching Systems These systems were characterized by fairly direct mappings from text to fillers, without the construction of elaborate intermediate structures. The mapping methods used varied widely. Some treated a text as an unordered set of words, as in traditional text categorization techniques from information retrieval (Lewis 1991). Slot fillers were defined in terms of the presence of words, Boolean combinations of words, or weighted combinations of words. Other methods scanned text for patterns specified in some extension of the language of regular expressions and produced corresponding fillers. Some systems also used hierarchies of word-based concepts or patterns defined in terms of other patterns. While many groups used pattern-matching as an aid to producing more structured analyses, five groups used pattern-matching exclusively. For two groups (Hughes and UNL/USL), use of pattern-matching techniques was the focus"
J93-3001,J90-3005,0,0.0182619,"the MUC-3 test set, but the second might be better on some other text stream, or even on a different chronological period from the same text stream. (The latter issue is discussed in Section 7.5 with reference to the MUC-4 results.) Experimen437 Computational Linguistics Volume 19, Number 3 tation with more and larger data sets will be needed before we can have confidence in the robustness of results such as those reported here. 6. Linguistic Phenomena Test Experiment The scores discussed above provide a black-box measure of system effectiveness because only input/output pairs were examined (Palmer and Finin 1990). The subsystem mechanisms used to create those outputs, were not considered as they would have been in a glass-box test because these subsystems differed widely from system to system. In an attempt to gain some insight into the strengths and weaknesses of subsystems, we examined the effect of particular linguistic constructions on the ability of systems to extract data correctly. The general method of testing linguistic phenomena was to find all instances of the chosen phenomenon in the test messages, to determine the slots that could only be filled correctly if a system were able to handle t"
J93-3001,H92-1006,1,0.822495,"Missing"
J93-3001,M92-1004,1,\N,Missing
M95-1012,P89-1004,0,0.0361753,"ts denote events themselves (in this case the event of being a particular number o f years old), as opposed to the individuals participating in the events (the individual and his or her age) . This treatment is similar to the partial Davidsonian analysis of events due to Hobbs [8] . Note that event individuals are by definition only associated with relations, not unary predicates . As a point of clarification, note that the inference system does not encode facts at the predicate calculu s level so much as at the interpretation level made popular in such systems as the SRI core language engine [1, 3] . In other words, the representation is actually a structured attribute-value graph such as the following, whic h encodes the age apposition above . [[head :person] [proxy pers-02] [modifiers [[head has-age] [proxy ha-04] [arguments (pers-02 [[head age ] [proxy age-03]])]]] ] The first two fields correspond to the embedded phrase : the head field is a semantic sort, and the proxy field holds the designator for the semantic individual denoted by the phrase . The interpretation encoding th e 147 overall apposition ends up in the modifiers slot, an approach adopted from the standard linguistic a"
M95-1012,W91-0201,0,0.0205557,"ts denote events themselves (in this case the event of being a particular number o f years old), as opposed to the individuals participating in the events (the individual and his or her age) . This treatment is similar to the partial Davidsonian analysis of events due to Hobbs [8] . Note that event individuals are by definition only associated with relations, not unary predicates . As a point of clarification, note that the inference system does not encode facts at the predicate calculu s level so much as at the interpretation level made popular in such systems as the SRI core language engine [1, 3] . In other words, the representation is actually a structured attribute-value graph such as the following, whic h encodes the age apposition above . [[head :person] [proxy pers-02] [modifiers [[head has-age] [proxy ha-04] [arguments (pers-02 [[head age ] [proxy age-03]])]]] ] The first two fields correspond to the embedded phrase : the head field is a semantic sort, and the proxy field holds the designator for the semantic individual denoted by the phrase . The interpretation encoding th e 147 overall apposition ends up in the modifiers slot, an approach adopted from the standard linguistic a"
M95-1012,M95-1008,0,0.0154588,"Missing"
M95-1012,H92-1022,0,0.0128644,"genesis of this transformation occurred during a dinner conversation at the last Muc conference, MUC-5 . At that time, several of us reluctantly admitted that our major impediment towards improved performance was reliance on then-standard linguistic models of syntax . We knew we would need an alternative to traditional linguistic grammars, even to the somewhat non-traditional categoria l pseudo-parser we had in place at the time. The problem was, which alternative ? The answer came in the form of rule sequences, an approach Eric Brill originally laid out in his work o n part-of-speech tagging [5, 7] . Rule sequences now underlie all the major processing steps in Alembic. part-ofspeech tagging, syntactic analysis, inference, and even some of the set-fill processing in the Template Elemen t task (TE) . We have found this approach to provide almost an embarrassment of advantages, speed an d accuracy being the most externally visible benefits . In addition, most of our rule sequence processors ar e trainable, typically from small samples . The rules acquired in this way also have the characteristic that the y allow one to readily mix hand-crafted and machine-learned elements . We have exploi"
M95-1012,P85-1008,0,0.023459,"antic relationship between a person and that person's age . More precisely, the following facts are added to the inferential database . person(pers-02 ) has-age((pers-02, age-03) ha-04 ) age(age-03 ) What appears to be a spare argument to the has-age predicate above is the event individual for the predicate. Such arguments denote events themselves (in this case the event of being a particular number o f years old), as opposed to the individuals participating in the events (the individual and his or her age) . This treatment is similar to the partial Davidsonian analysis of events due to Hobbs [8] . Note that event individuals are by definition only associated with relations, not unary predicates . As a point of clarification, note that the inference system does not encode facts at the predicate calculu s level so much as at the interpretation level made popular in such systems as the SRI core language engine [1, 3] . In other words, the representation is actually a structured attribute-value graph such as the following, whic h encodes the age apposition above . [[head :person] [proxy pers-02] [modifiers [[head has-age] [proxy ha-04] [arguments (pers-02 [[head age ] [proxy age-03]])]]]"
M95-1012,M93-1023,0,0.0145264,"Missing"
N04-3004,M95-1012,1,0.472103,"Missing"
N04-3004,C96-1047,0,0.0377062,"Missing"
N04-3004,P00-1010,0,\N,Missing
P84-1023,C82-1040,0,\N,Missing
P84-1023,A83-1016,1,\N,Missing
P84-1023,A83-1007,0,\N,Missing
P86-1004,P81-1029,1,0.907271,"Missing"
P86-1004,H86-1012,1,\N,Missing
P88-1002,A88-1007,1,0.884391,"Missing"
P88-1002,J81-2002,0,\N,Missing
P88-1002,J83-3003,0,\N,Missing
P88-1002,J81-4004,0,\N,Missing
P88-1002,J83-3002,0,\N,Missing
P88-1002,P86-1004,1,\N,Missing
P88-1002,H86-1011,1,\N,Missing
P88-1002,P87-1019,1,\N,Missing
P88-1002,P87-1003,1,\N,Missing
P88-1002,C80-1027,0,\N,Missing
P88-1002,J88-2005,1,\N,Missing
P88-1002,A83-1016,0,\N,Missing
P98-1031,J93-2004,0,\N,Missing
P98-1031,M95-1012,1,\N,Missing
P98-1031,X96-1049,0,\N,Missing
P98-1031,M93-1007,0,\N,Missing
P98-1031,A97-1029,0,\N,Missing
P99-1042,J97-2002,0,0.00927666,"e set of words in the sentence. The word sets are considered to have no structure or order and contain unique elements. For example, the representation for (la) is the set in (lb). la (Sentence): By giving it 6,457 of his books, Thomas Jefferson helped get it started. lb (Bag): {6,457 books by get giving helped his it Jefferson of started Thomas} Extraction of information content from text, both in documents and questions, then consists of tokenizing words and determining sentence boundary punctuation. For English written text, both of these tasks are relatively easy although not trivial--see Palmer and Hearst (1997). The search subtask consists of finding the best match between the word set representing the question and the sets representing sentences in the document. Our system measures the match by size of the intersection of the two word sets. For example, the question in (2a) would receive an intersection score of 1 because of the mutual set element books. 2a (Question): Who gave books to the new library? 2b (Bag): {books gave library new the to who} Because match size does not produce a complete ordering on the sentences of the document, we additionally prefer sentences that first match on longer wo"
P99-1042,C96-1047,0,0.016659,"d locational words, respectively. By using name taggers to identify person, location, and temporal information, we can add semantic class symbols to the question word sets marking the type of the question and then add corresponding class symbols to the word sets whose sentences contain phrases denoting the proper type of entity. For example, due to the name Thomas Jefferson, the word set in (lb) would be extended by :PERSON, as would the word set (2b) because it is a who question. This would increase the matching score by one. The system makes use of the Alembic automated named entity system (Vilain and Day 1996) for finding named entities. In a similar vein, we also created a simple common noun classification module using WordNet (Miller 1990). It works by looking up all nouns of the text and adding person or location classes if any of a noun&apos;s senses is subsumed by the appropriate WordNet class. We also created a filtering module that ranks sentences higher if they contain the appropriate class identifier, even though they may have fewer matching words, e.g., if the bag representation of a sentence does not contain :PERSON, it is ranked lower as an answer to a who question than sentences which do co"
P99-1042,M95-1005,1,0.27314,"Missing"
P99-1042,M93-1007,0,0.0133135,"Missing"
W01-1607,H01-1028,1,0.789257,"akes before the information is acknowledged by the system (either correctly or incorrectly.) If a system responds immediately to the unsolicited information, a count of zero turns is recorded. Figure 5 shows the difference among systems in responding to unsolicited information. We graphed both the average total number of system turns as well as the average number of turns minus repetitions. HC B responds almost immediately to 10 This issue may also be related to where in the dialogue errors occur. We are pursuing another line of research which looks at automatic error detection, described in (Aberdeen et al., 2001). We believe we may also be able to detect unsolicited information automatically, as well as to see whether it is likely to trigger errors by the system. Figure 4: Unsolicited Fields vs. Success Rate of Incorporation unsolicited information while HCs A and C take more turns to respond. HC D has trouble understanding the unsolicited information, and either keeps asking for clarification or continues to ignore the human and prompts for some other piece of information multiple times. Figure 5: Variation of System Response to Unsolicited Information Figure 6 shows the different rates at which syst"
W01-1607,P90-1010,0,0.136995,"Jordan and Di Eugenio, 1997; Flammia and Zue, 1997)), despite the fact the terms initiative and mixedExpert:WHAT TIME DO [req-task-info] initiative are widely used. Intuitively, it seems YOU NEED TO DEPART that control rests with the participant who is movUser:AS SOON AS [give-task-info] ing a conversation ahead at a given point, or sePOSSIBLE AFTER FIVE P.M. lecting new topics for conversation. Expert:THE FIRST FLIGHT [give-task-info] After experimenting with several tagging methAFTER FIVE P.M. ON THAT DATE IS ods, we concluded that the approach presented AT FIVE THIRTY FIVE P.M. ARRIVING in Walker and Whittaker (1990) adopted from IN CHICAGO AT SIX OH SIX P.M. (Whittaker and Stenton, 1988) best captured the ON U.S. AIR aspects of the dialogue we were interested in and, User: IS THAT O’HARE [req-task-info] as with the DAs, could be tagged reliably on our data. Table 1: DA tagging in an HH Exchange Each turn is tagged with which participant has Expert: i have an American [give-task-info] control at the end of that turn, based on the utterance type. Again, we did not tag turns composed Airlines flight departing Seattle at entirely of non-speech annotation, and we also extwelve fifty five p.m., arrives Tokyo c"
W01-1607,P88-1015,0,0.37737,"he terms initiative and mixedExpert:WHAT TIME DO [req-task-info] initiative are widely used. Intuitively, it seems YOU NEED TO DEPART that control rests with the participant who is movUser:AS SOON AS [give-task-info] ing a conversation ahead at a given point, or sePOSSIBLE AFTER FIVE P.M. lecting new topics for conversation. Expert:THE FIRST FLIGHT [give-task-info] After experimenting with several tagging methAFTER FIVE P.M. ON THAT DATE IS ods, we concluded that the approach presented AT FIVE THIRTY FIVE P.M. ARRIVING in Walker and Whittaker (1990) adopted from IN CHICAGO AT SIX OH SIX P.M. (Whittaker and Stenton, 1988) best captured the ON U.S. AIR aspects of the dialogue we were interested in and, User: IS THAT O’HARE [req-task-info] as with the DAs, could be tagged reliably on our data. Table 1: DA tagging in an HH Exchange Each turn is tagged with which participant has Expert: i have an American [give-task-info] control at the end of that turn, based on the utterance type. Again, we did not tag turns composed Airlines flight departing Seattle at entirely of non-speech annotation, and we also extwelve fifty five p.m., arrives Tokyo cluded conventional openings and closings, followat three p.m. the next da"
W01-1607,A00-1014,0,\N,Missing
W01-1607,H01-1015,0,\N,Missing
W01-1607,J96-2004,0,\N,Missing
W03-1301,W02-0312,0,0.0187158,"Missing"
W03-1301,C00-1030,0,\N,Missing
W03-1301,W02-0303,0,\N,Missing
W10-1111,C96-1079,0,\N,Missing
walker-etal-2000-evaluation,H92-1009,0,\N,Missing
walker-etal-2000-evaluation,H92-1006,1,\N,Missing
walker-etal-2000-evaluation,H93-1004,0,\N,Missing
walker-etal-2000-evaluation,walker-etal-2000-developing,1,\N,Missing
walker-etal-2000-evaluation,polifroni-seneff-2000-galaxy,0,\N,Missing
X96-1053,M95-1012,1,0.810171,"I T R E Corporation 202 Burlington Rd. Bedford, M A 01730 {aberdeen, john, day, lynette, palmer, parann, mbv}@mitre.org (def-phraeer label I-word-1 label-action Alembic is a comprehensive information extraction system that has been applied to a range of tasks. These include the now-standard components of the formal MOC evaluations: name tagging (NE in MUC-6), name normalization (WE), and template generation (ST). The system has also been exploited to help segment and index broadcast video and was used for early experiments on variants of the co-reference identification task. (For details, see [1].) For MET, we were of course primarily concerned with the foundational name-tagging task; many downstream modules of the system were left unused. The punchline, as we see it, is that Alembic performed exceptionally well at all three of the MET languages despite having no native speakers for any of them among its development team. We were one of only two sites that attempted all three languages, and were the only group that exploited essentially the same body of code for all three tasks. (de&apos;f-phraeer label right-1 right-2 bounds-action NONE lexeme &quot; associaci6n&quot; ... ORGEX) ORGEX lexeme &quot;de&quot; p"
