W17-0807,Consistent Classification of Translation Revisions: A Case Study of {E}nglish-{J}apanese Student Translations,2017,5,0,6,0,5049,atsushi fujita,Proceedings of the 11th Linguistic Annotation Workshop,0,"Consistency is a crucial requirement in text annotation. It is especially important in educational applications, as lack of consistency directly affects learners{'} motivation and learning performance. This paper presents a quality assessment scheme for English-to-Japanese translations produced by learner translators at university. We constructed a revision typology and a decision tree manually through an application of the OntoNotes method, i.e., an iteration of assessing learners{'} translations and hypothesizing the conditions for consistent decision making, as well as re-organizing the typology. Intrinsic evaluation of the created scheme confirmed its potential contribution to the consistent classification of identified erroneous text spans, achieving visibly higher Cohen{'}s kappa values, up to 0.831, than previous work. This paper also describes an application of our scheme to an English-to-Japanese translation exercise course for undergraduate students at a university in Japan."
C16-2008,{M}u{TUAL}: A Controlled Authoring Support System Enabling Contextual Machine Translation,2016,3,0,2,1,10715,rei miyata,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"The paper introduces a web-based authoring support system, MuTUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system."
W15-4945,{MNH}-{TT}: A Platform to Support Collaborative Translator Training,2015,0,0,4,0,127,masao utiyama,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,None
2015.mtsummit-papers.8,{J}apanese controlled language rules to improve machine translatability of municipal documents,2015,-1,-1,2,1,10715,rei miyata,Proceedings of Machine Translation Summit XV: Papers,0,None
2015.eamt-1.46,{MNH}-{TT}: A Platform to Support Collaborative Translator Training,2015,0,0,4,0,127,masao utiyama,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W12-0114,Design of a hybrid high quality machine translation system,2012,37,5,5,0.866667,12054,bogdan babych,Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"This paper gives an overview of the ongoing FP7 project HyghTra (2010--2014). The HyghTra project is conducted in a partnership between academia and industry involving the University of Leeds and Lingenio GmbH (company). It adopts a hybrid and bootstrapping approach to the enhancement of MT quality by applying rule-based analysis and statistical evaluation techniques to both parallel and comparable corpora in order to extract linguistic information and enrich the lexical and syntactic resources of the underlying (rule-based) MT system that is used for analysing the corpora. The project places special emphasis on the extension of systems to new language pairs and corresponding rapid, automated creation of high quality resources. The techniques are fielded and evaluated within an existing commercial MT environment."
2012.tc-1.1,{MNH}-{TT}: a collaborative platform for translator training,2012,-1,-1,2,0.866667,12054,bogdan babych,Proceedings of Translating and the Computer 34,0,None
2012.eamt-1.9,Building Translation Awareness in Occasional Authors: A User Case from {J}apan,2012,-1,-1,2,0,37934,midori tatsumi,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,None
2012.eamt-1.57,Readability and Translatability Judgments for {``}Controlled {J}apanese{''},2012,6,2,1,1,32117,anthony hartley,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Abstract We report on an experiment to test the ef-ficacy of xe2x80x98controlled languagexe2x80x99 authoring of technical documents in Japanese, with respect both to the readability of the Jap-anese source and the quality of the Eng-lish machine-translated output. Using four MT systems, we tested two sets of writing rules designed for two document types written by authors with contrasting professional profiles. We elicited judg-ments from native speakers to establish the positive or negative impact of each rule on readability and translation quality. 1 Introduction It is widely acknowledged that the typological xe2x80x98distancexe2x80x99 between Japanese and English (the most common European target language for MT from Japanese) hampers the achievement of high-quality translation. We seek to address this challenge by investigating the feasibility of de-veloping a xe2x80x98controlled Japanesexe2x80x99 with explicit restrictions on vocabulary, syntax and style ade-quate for authoring technical documentation. Our starting point is sentences extracted from two types of document: consumer user manuals (UM) and company-internal documents articulat-ing the know-how of key employees (KH). UM are produced by professional technical authors, while KH are written as xe2x80x98one-offsxe2x80x99 by the em-ployees themselves, capturing their own know-how. Thus, there is a sharp difference in the ef-"
Y10-1089,Advanced Corpus Solutions for Humanities Researchers,2010,11,5,2,0,45052,james wilson,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"This paper describes the design and implementation of an interface to corpora in 12 languages, stemming from the analysis of the needs of a diverse group of users: language teachers and language students, (non-computational) linguists, researchers in history and translation studies. We identified a set of requirements shared across the disciplines, as well as more specific requirements from the targeted user groups. The interface is designed to handle large-scale corpora of 20-500 million words."
2009.eamt-1.6,Evaluation-Guided Pre-Editing of Source Text: Improving {MT}-Tractability of Light Verb Constructions,2009,6,2,2,0.857143,12054,bogdan babych,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper reports an experiment on evaluating and improving MT quality of light-verb construction (LVCs) xe2x80x93 combinations of a xe2x80x98semantically depletedxe2x80x99 verb and its complement. Our method uses construction-level human evaluation for systematic discovery of mistranslated contexts and creating automatic pre-editing rules, which make the constructions more tractable for Rule-Based Machine Translation (RBMT) systems. For rewritten phrases we achieve about 40% reduction in the number of incomprehensible translations into English from both French and Russian. The proposed method can be used for enhancing automatic pre-editing functionality of state-of-theart MT systems. It will allow MT users to create their own rewriting rules for frequently mistranslated constructions and contexts, going beyond existing systemsxe2x80x99 capabilities offered by user dictionaries and do-not translate lists."
babych-etal-2008-generalising,Generalising Lexical Translation Strategies for {MT} Using Comparable Corpora,2008,15,3,3,0.916667,12054,bogdan babych,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We report on an on-going research project aimed at increasing the range of translation equivalents which can be automatically discovered by MT systems. The methodology is based on semi-supervised learning of indirect translation strategies from large comparable corpora and applying them in run-time to generate novel, previously unseen translation equivalents. This approach is different from methods based on parallel resources, which currently can reuse only individual translation equivalents. Instead it models translation strategies which generalise individual equivalents and can successfully generate an open class of new translation solutions. The task of the project is integration of the developed technology into open-source MT systems."
babych-hartley-2008-sensitivity,Sensitivity of Automated {MT} Evaluation Metrics on Higher Quality {MT} Output: {BLEU} vs Task-Based Evaluation Methods,2008,8,6,2,0.916667,12054,bogdan babych,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We report the results of our experiment on assessing the ability of automated MT evaluation metrics to remain sensitive to variations in MT quality as the average quality of the compared systems goes up. We compare two groups of metrics: those, which measure the proximity of MT output to some reference translation, and those which evaluate the performance of some automated process on degraded MT output. The experiment shows that proximity-based metrics (such as BLEU) loose sensitivity as the scores go up, but performance-based metrics (e.g., Named Entity recognition from MT output) remain sensitive across the scale. We suggest a model for explaining this result, which attributes stable sensitivity of performance-based metrics to measuring cumulative functional effect of different language levels, while proximity-based metrics measure structural matches on a lexical level and therefore miss higher-level errors that are more typical for better MT systems. Development of new automated metrics should take into account possible decline in sensitivity on higher-quality MT, which should be tested as part of meta-evaluation of the metrics."
kurella-etal-2008-corpus,Corpus-Based Tools for Computer-Assisted Acquisition of Reading Abilities in Cognate Languages,2008,10,0,3,0,48474,svitlana kurella,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents an approach to computer-assisted teaching of reading abilities using corpus data. The approach is supported by a set of tools for automatically selecting and classifying texts retrieved from the Internet. The approach is based on a linguistic model of textual cohesion which describes relations between larger textual units that go beyond the sentence level. We show that textual connectors that link such textual units reliably predict different types of texts, such as ÂinformationÂ and ÂopinionÂ: using only textual connectors as features, an SVM classifier achieves an F-score of between 0.85 and 0.93 for predicting these classes. The tools are used in our project on teaching reading skills in a cognate foreign language (L3) which is cognate to a known foreign language (L2)."
P07-1018,Assisting Translators in Indirect Lexical Transfer,2007,9,11,2,1,12054,bogdan babych,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We present the design and evaluation of a translatorxe2x80x99s amenuensis that uses comparable corpora to propose and rank nonliteral solutions to the translation of expressions from the general lexicon. Using distributional similarity and bilingual dictionaries, the method outperforms established techniques for extracting translation equivalents from parallel corpora. The interface to the system is available at: http://corpus.leeds.ac.uk/assist/v05/"
2007.tc-1.3,A dynamic dictionary for discovering indirect translation equivalents,2007,-1,-1,2,1,12054,bogdan babych,Proceedings of Translating and the Computer 29,0,None
2007.mtsummit-papers.5,Translating from under-resourced languages: comparing direct transfer against pivot translation,2007,-1,-1,2,1,12054,bogdan babych,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.mtsummit-papers.31,Assessing human and automated quality judgments in the {F}rench {MT} evaluation campaign {CESTA},2007,9,11,2,0,27330,olivier hamon,Proceedings of Machine Translation Summit XI: Papers,0,"This paper analyzes the results of the French MT Evaluation Campaign, CESTA (2003-2006). The details of the campaign are first briefly described. The paper then focuses on the results of the two runs, which used human metrics, such as fluency or adequacy, as well as automated metrics, mainly based on n-gram comparison and word error rates. The results show that the quality of the systems can be reliably compared using these metrics, and that the adaptability of some systems to a given domain xe2x80x93 which was the focus of CESTA's second run xe2x80x93 is not strictly related to their intrinsic performance."
2007.mtsummit-aptme.7,Sensitivity of automated models for {MT} evaluation: proximity-based vs. performance-based methods,2007,-1,-1,2,1,12054,bogdan babych,Proceedings of the Workshop on Automatic procedures in MT evaluation,0,None
P06-2095,Using Comparable Corpora to Solve Problems Difficult for Human Translators,2006,12,15,3,0.681818,519,serge sharoff,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"In this paper we present a tool that uses comparable corpora to find appropriate translation equivalents for expressions that are considered by translators as difficult. For a phrase in the source language the tool identifies a range of possible expressions used in similar contexts in target language corpora and presents them to the translator as a list of suggestions. In the paper we discuss the method and present results of human evaluation of the performance of the tool, which highlight its usefulness when dictionary solutions are lacking."
sharoff-etal-2006-using,Using collocations from comparable corpora to find translation equivalents,2006,14,4,3,0.681818,519,serge sharoff,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,In this paper we present a tool for finding appropriate translation equivalents for words from the general lexicon using comparable corpora. For a phrase in the source language the tool suggests arange of possible expressions used in similar contexts in target language corpora. In the paper we discuss the method and present results of human evaluation of the performance of the tool.
2005.mtsummit-posters.13,Estimating the predictive Power of N-gram {MT} Evaluation Metrics across Language and Text Types,2005,-1,-1,2,1,12054,bogdan babych,Proceedings of Machine Translation Summit X: Posters,0,"The use of n-gram metrics to evaluate the output of MT systems is widespread. Typically, they are used in system development, where an increase in the score is taken to represent an improvement in the output of the system. However, purchasers of MT systems or services are more concerned to know how well a score predicts the acceptability of the output to a reader-user. Moreover, they usually want to know if these predictions will hold across a range of target languages and text types. We describe an experiment involving human and automated evaluations of four MT systems across two text types and 23 language directions. It establishes that the correlation between human and automated scores is high, but that the predictive power of these scores depends crucially on target language and text type."
babych-etal-2004-calibrating,Calibrating Resource-light Automatic {MT} Evaluation: a Cheap Approach to Ranking {MT} Systems by the Usability of Their Output,2004,3,1,3,1,12054,bogdan babych,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"MT systems are traditionally evaluated with different criteria, such as adequacy and fluency. Automatic evaluation scores are designed to match these quality parameters. In this paper we introduce a novel parameter xe2x80x93 usability (or utility) of output, which was found to integrate both fluency and adequacy. We confronted two automated metrics, BLEU and LTV, with new data for which human evaluation scores were also produced; we then measured the agreement between the automated and human evaluation scores. The resources produced in the experiment are available on the authorsxe2x80x99 website."
babych-hartley-2004-modelling,Modelling Legitimate Translation Variation for Automatic Evaluation of {MT} Quality,2004,3,9,2,1,12054,bogdan babych,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Automatic methods for MT evaluation are often based on the assumption that MT quality is related to some kind of distance between the evaluated text and a professional human translation (e.g., an edit distance or the precision of matched N-grams). However, independently produced human translations are necessarily different, conveying the same content by dissimilar means. Such legitimate translation variation is a serious problem for distance-based evaluation methods, because mismatches do not necessarily mean degradation in MT quality. In this paper we explore the link between legitimate translation variation and statistical measures of a words salience within a given document, such as tf.idf scores. We show that the use of such scores extends the N-gram distance measures in a way that allows us to accurately predict multiple quality parameters of the text, such as translation adequacy and fluency. However legitimate translation variation also reveals fundamental limits on the applicability of distance-based MT evaluation methods and on data-driven architectures for MT."
C04-1016,Extending {MT} evaluation tools with translation complexity metrics,2004,6,6,3,1,12054,bogdan babych,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we report on the results of an experiment in designing resource-light metrics that predict the potential translation complexity of a text or a corpus of homogenous texts for state-of-the-art MT systems. We show that the best prediction of translation complexity is given by the average number of syllables per word (ASW). The translation complexity metrics based on this parameter are used to normalise automated MT evaluation scores such as BLEU, which otherwise are variable across texts of different types. The suggested approach makes a fairer comparison between the MT systems evaluated on different corpora. The translation complexity metric was integrated into two automated MT evaluation packages - BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author's web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html"
2004.eamt-1.3,Disambiguating translation strategies in {MT} using automatic named entity recognition,2004,-1,-1,2,1,12054,bogdan babych,Proceedings of the 9th EAMT Workshop: Broadening horizons of machine translation and its applications,0,None
elliott-etal-2004-fluency,A fluency error categorization scheme to guide automated machine translation evaluation,2004,11,10,2,0,51186,debbie elliott,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Existing automated MT evaluation methods often require expert human translations. These are produced for every language pair evaluated and, due to this expense, subsequent evaluations tend to rely on the same texts, which do not necessarily reflect real MT use. In contrast, we are designing an automated MT evaluation system, intended for use by post-editors, purchasers and developers, that requires nothing but the raw MT output. Furthermore, our research is based on texts that reflect corporate use of MT. This paper describes our first step in system design: a hierarchical classification scheme of fluency errors in English MT output, to enable us to identify error types and frequencies, and guide the selection of errors for automated detection. We present results from the statistical analysis of 20,000 words of MT output, manually annotated using our classification scheme, and describe correlations between error frequencies and human scores for fluency and adequacy."
W03-2201,Improving Machine Translation Quality with Automatic Named Entity Recognition,2003,14,126,2,1,12054,bogdan babych,"Proceedings of the 7th International {EAMT} workshop on {MT} and other language technology tools, Improving {MT} through other language technology tools, Resource and tools for building {MT} at {EACL} 2003",0,"Named entities create serious problems for state-of-the-art commercial machine translation (MT) systems and often cause translation failures beyond the local context, affecting both the overall morphosyntactic well-formedness of sentences and word sense disambiguation in the source text. We report on the results of an experiment in which MT input was processed using output from the named entity recognition module of Sheffield's GATE information extraction (IE) system. The gain in MT quality indicates that specific components of IE technology could boost the performance of current MT systems."
2003.eamt-1.13,Multilingual generation of controlled languages,2003,0,16,3,0,41078,richard power,EAMT Workshop: Improving MT through other language technology tools: resources and tools for building MT,0,"We describe techniques based on natural language generation which allow a user to author a document in controlled language for multiple natural languages. The author is expected to be an expert in the application domain but not in the controlled language or in more than one of the supported natural languages. Because the system can produce multiple expressions of the same input in multiple languages, the author can choose among alternative expressions satisfying the constraints of the controlled language. Because the system offers only legitimate choices of wording, correction is unnecessary. Consequently, acceptance of error reports and corrections by trained authors are non-issues."
rajman-hartley-2002-automatic,Automatic Ranking of {MT} Systems,2002,7,7,2,0,4667,martin rajman,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"In earlier work, we succeeded in automatically predicting the relative rankings of MT systems derived from human judgments on the Fluency, Adequacy or Informativeness of their output. In this paper, we present an experiment xe2x80x93 using human evaluators and additional data xe2x80x93 designed to test the robustness of our earlier results. These had yielded two promising automatically computable predictors, the D-score based on semantic features of the MT output, and the X-score based on syntactic features. We conclude that the X-score is indeed a robust and reliable predictor, even on new data for which it has not been specifically tuned."
W01-0815,Evaluating Text Quality: Judging Output Texts Without a Clear Source,2001,19,1,1,1,32117,anthony hartley,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,"We consider how far two attributes of text quality commonly used in MT evaluation -- intelligibility and fidelity -- apply within NLG. While the former appears to transfer directly, the latter needs to be completely re-interpreted. We make a crucial distinction between the needs of symbolic authors and those of end-readers. We describe a form of textual feedback, based on a controlled language used for specifying software requirements that appears well suited to authors' needs, and an approach for incrementally improving the fidelity of this feedback text to the content model."
2001.mtsummit-papers.27,{AGILE} - a system for multilingual generation of technical instructions,2001,5,10,1,1,32117,anthony hartley,Proceedings of Machine Translation Summit VIII,0,"This paper presents a multilingual Natural Language Generation system that produces technical instruction texts in Bulgarian, Czech and Russian. It generates several types of texts, common for software manuals, in two styles. We illustrate the system{'}s functionality with examples of its input and output behaviour. We discuss the criteria and procedures adopted for evaluating the system and summarise their results. The system embodies novel approaches to providing multilingual documentation, ranging from the re-use of a large-scale, broad coverage grammar of English in order to develop the lexico-grammatical resources necessary for the generation in the three target languages, through to the adoption of a {`}knowledge editing{'} approach to specifying the desired content of the texts to be generated independently of the target languages in which those texts finally appear."
bateman-hartley-2000-target,Target Suites for Evaluating the Coverage of Text Generators,2000,12,5,2,0,27622,john bateman,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Our goal is to evaluate the grammatical coverage of the surface realization component of a natural language generation system by means of target suites. We consider the utility of re-using for this purpose test suites designed to assess the coverage of natural language analysis / understanding systems. We find that they are of some interest, in helping inter-system comparisons and in providing an essential link to annotated corpora. But they have limitations. First, they contain a high proportion of ill-formed items which are inappropriate as targets for generation. Second, they omit phenomena such as discourse markers which are key issues in text production. We illustrate a partial remedy for this situation in the form of a text generator that annotates its own output to an externally specified standard, the TSNLP scheme. 1. Test suites and target suites The evaluation of natural language generation (NLG) systems is an issue that few authors have addressed seriously to date. Among the principal reasons is the difficulty of defining what the input should be and, indeed, of assessing the quality of the output (cf., (Sparck-Jones and Galliers, 1996)). Evaluations conducted to assess the adequacy of a system for a particular application have tended to focus on the quality of the output text, particularly its fluency or intelligibility (cf., (Coch, 1996; Lester and Porter, 1997)). While accepting that evaluation will inevitably bear on the output, (Mellish and Dale, 1998) suggest a finer-grained approach which aims to tease out the contribution made by a particular sub-task in the generation process to the quality of the final output. We propose here to take up that suggestion and to consider the final task in the linexe2x80x93surface realization. There is a general recognition that a crucial question about a surface realization is its grammatical coverage (cf. (van Noord and Neumann, 1997; Mellish and Dale, 1998)). This question poses itself not only for adequacy evaluation intended to assess applications potential, but also diagnostic and progress evaluation. In other words, it is relevant at all stages in a systemxe2x80x99s life-cycle, from conception to fielding. A favoured means of gauging the coverage of NLP systems designed for analysis tasks is test suites, which consist of controlled and systematically organized data (cf., (Lehmann et al., 1996; Oepen, Netter and Klein, 1997; Netter et al., 1998), in contrast to naturally occurring text corpora. It is, then, sensible to see whether any of these existing suites can be re-used or re-purposed for the benefit of systems designed for generation. Test suites as currently conceived are intended as input data. For some tasks where the output can be well-defined xe2x80x93 such as message or speech understanding xe2x80x93 test collections provide both input and output data. For NLG evaluation, we propose the use of target suites that specify a useful set of outputs for a generator (typically sentences showing particular syntactic structures), while remaining agnostic on the inputs. Practically, however, particular generation systems are then encouraged to provide corresponding inputs so that both coverage and structural treatments can be readily compared. A first step towards this is reported in (Henschel, Bateman and Matthiessen, 1999) for an extensive set of nominal referring expressions; input specifications are provided for two of the broadest coverage generators for English currently availablexe2x80x93the KPML/Nigel and FUF/Surge generators. We are now developing further target suites both for other languages and for other areas of grammar, using the mechanism described below. Two questions arise. Which of the test items already available are relevant target items for NLG? What phenomena important for NLG are missing and need therefore to be added to the target suites in order to cater for this application? This second question is addressed later in the paper. We have taken as our reference here the TSNLP test suites (Lehmann et al., 1996; Oepen, Netter and Klein, 1997). Paradoxically, the relevance for NLG of the items in the TSNLP suite turns out to be diminished by both their generality and their application-specificity. On the one hand, the compilers of the suite were motivated by the desire to ensure that the data should be reusable and not tailored to a single type of application (Estival et al., 1995). Thus, they mostly applied general guidelines for creating the data set, although they did not adhere to them in all cases. These guidelines strongly favour, therefore, unmarked word order and declarative, active, indicative sentences in the present tense with a 3rd person singular subjectxe2x80x93to mention just a few constraints which mean that any NLG system would be seriously under-exercised by the suite in its current state. On the other hand, the designers targeted three specific applicationsxe2x80x93parsers, grammar checkers and controlled language checkersxe2x80x93whose evaluation requires ungrammatical data not systematically available in text corpora. Indeed, over 35% of the 14817 English, French and German items in the TSNLP suite are ungrammatical and as such irrelevant as target items for NLG. A more recent project aimed at producing a more efficient environment for test suite construction, DiET (Netter et al., 1998), is validating its data on checkers, MT systems and translation memories. While referring to xe2x80x99NLP systemsxe2x80x99 generally, the researchers make no mention of NLG applications. In the next section we describe the implementation of a novel approach to producing resources for assessing the coverage of NLG systems. 2. Implementation of automatic annotation A key feature of all test suites is a suitable annotation scheme that allows extraction of sets of items appropriate to particular evaluation goals. The fact that NLG is goaland content-driven requires a scheme that relates items not only formally but also semantically. In this section, we show how an existing text generatorxe2x80x93KPML (Bateman, 1997)xe2x80x93 has been adapted so as to automatically annotate its own output with an externally specified annotation schemexe2x80x93provided by the TSNLP project (Oepen, Netter and Klein, 1997). Such annotations, which are both formal and functional in nature, make it easier to assess the coverage of the generator and to compare its coverage with that of other systems. Moreover, these annotations can provide an essential link to corpora, insofar as they establishxe2x80x93using a widely understood metalanguagexe2x80x93links between suite items and domain-specific corpora (cf. (Netter et al., 1998)). We propose that such facilities should become a standard part of generator design in order to ensure reliability and appropriate documentation of coverage. As we have noted, work on test suite design has been oriented, until now, to the evaluation of NLP tools for analysis. Nevertheless, many classifications developed in that work remain useful for generation. In addition, the adoption by the NLG community of test suite categories that are already to some extent in use within the Language Engineering and NLP communities will also aid comparability between linguistic resources designed for analysisxe2x80x93such as TS-GRAMxe2x80x93and those designed for generationxe2x80x93such as FUF/Surge (Elhadad and Robin, 1996), KPML/Nigel (Bateman, 1997) and RealPro (Lavoie and Rambow, 1997). 2.1. Exploiting the generation history There is an interesting difference between the construction of test suites and the construction of target suites, as we have noted above. Whereas the items included in test suites need to be first selected and then marked up, the test items in target suites are themselves being generated. This is significant since we can then make use of the complete history of the construction of any item. This history includes the grammatical constituency structure, as well as all of the semantic-functional decisions that were taken in order to reach the result. It is then possible that a substantial proportion of the information needed to create a properly annotated test suite item is already available somewhere in the generation history. Indeed, this opportunity has long been exploited in the development of NLG systems. Most large-scale systems provide sets of examples that show input-output pairs. Even if the inputs to different systems ranges from high-level semantic specifications, via abstract syntactic structures to rich syntactic specifications (cf. (Mellish and Dale, 1998)), the act of making them systematically available permits comparisons that would otherwise be obscured. The output is, minimally, a string corresponding to the input specification; we show below how it can be enriched as a by-product of the generation process. One of the earliest such sets that was intended to show definitively the coverage of a grammar was the xe2x80x9cExercise Setxe2x80x9d developed for the Nigel grammar of English within the Penman generation system. In current environments for the development of generation grammars, such as KPML, the role of the example set has been enhanced to make it a major development tool. Users seeking to extend linguistic resources typically work from examples that come close to their required output. Then, since the generation system actually generates the strings given in the example, the complete decision path and resulting grammatical structures created for that example are open to inspection. The example therefore indexes precisely those resources that have been activated in the course of generating that particular example. This information can then serve directly to provide automatic target suite annotation. As an example that will run throughtout this section, consider the following simple sentence from the TSNLP documentation (Estival et al., 1995)xe2x80x93page 42:"
P96-1026,Two Sources of Control Over the Generation of Software Instructions,1996,22,8,1,1,32117,anthony hartley,34th Annual Meeting of the Association for Computational Linguistics,1,None
C96-1050,Language-Specific Mappings from Semantics to Syntax,1996,8,7,3,1,53166,judy delin,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"We present a study of the mappings from semantic content to syntactic expression with the aim of isolating the precise locus and role of pragmatic information in the generation process. From a corpus of English, French, and Portuguese instructions for consumer products, we demonstrate the range of expressions of two semantic relations, GENERATION and ENABLEMENT (Goldman, 1970) in each language, and show how the available choices are constrained syntactically, semantically, and pragmatically. The study reveals how multilingual NLG can be informed by language-specific principles for syntactic choice."
W94-0308,Expressing Procedural Relationships in Multilingual Instructions,1994,8,55,2,1,53166,judy delin,Proceedings of the Seventh International Workshop on Natural Language Generation,0,"In this paper we discuss a study of the expression of procedural relations in multilingual user instructions, in particular the relations of Generation and Enablement. These procedural relations are defined in terms of a plan representation model, and applied in a corpus study of English, French, and Portuguese instructions. The results of our analysis indicate specific guidelines for the tactical realisation of expressions of these relations in multilingual instructional text."
1986.tc-1.2,Continuing training for the language professions: a survey of needs,1986,-1,-1,1,1,32117,anthony hartley,Proceedings of Translating and the Computer 8: A profession on the move,0,None
