2021.wat-1.1,Overview of the 8th Workshop on {A}sian Translation,2021,-1,-1,16,0,283,toshiaki nakazawa,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper presents the results of the shared tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams participated in the shared tasks and 24 teams submitted their translation results for the human evaluation. We also accepted 5 research papers. About 2,100 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2021.naacl-main.433,Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model,2021,-1,-1,3,0,4525,honai ueoka,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generation-based ones. While the latter{'}s payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based model. It is also shown to be more secure against automatic detection than a generation-based method while offering better control of the security/payload capacity trade-off."
2021.naacl-main.442,Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis,2021,-1,-1,2,1,4549,hirokazu kiyomaru,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimizes the similarity between the latter representation and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in English and Japanese and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa."
2021.findings-emnlp.165,{J}apanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning,2021,-1,-1,3,0,6844,masato umakoshi,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Parallel texts of Japanese and a non-pro-drop language have the potential of improving the performance of Japanese zero anaphora resolution (ZAR) because pronouns dropped in the former are usually mentioned explicitly in the latter. However, rule-based cross-lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences. In this paper, we propose implicit transfer by injecting machine translation (MT) as an intermediate task between pretraining and ZAR. We employ a pretrained BERT model to initialize the encoder part of the encoder-decoder model for MT, and eject the encoder part for fine-tuning on ZAR. The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT. In addition, we find that the incorporation of the masked language model training into MT leads to further gains."
2021.eacl-main.265,Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph,2021,-1,-1,2,1,10901,yin huang,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method."
2021.acl-srw.9,Video-guided Machine Translation with Spatial Hierarchical Attention Network,2021,-1,-1,4,0,12439,weiqi gu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Video-guided machine translation, as one type of multimodal machine translations, aims to engage video contents as auxiliary information to address the word sense ambiguity problem in machine translation. Previous studies only use features from pretrained action detection models as motion representations of the video to solve the verb sense ambiguity, leaving the noun sense ambiguity a problem. To address this problem, we propose a video-guided machine translation system by using both spatial and motion representations in videos. For spatial features, we propose a hierarchical attention network to model the spatial information from object-level to video-level. Experiments on the VATEX dataset show that our system achieves 35.86 BLEU-4 score, which is 0.51 score higher than the single model of the SOTA method."
2021.acl-long.226,Lightweight Cross-Lingual Sentence Representation Learning,2021,-1,-1,5,1,13030,zhuoyuan mao,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks. However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations. In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations. We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture. To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task. We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks. Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model."
2020.wat-1.1,Overview of the 7th Workshop on {A}sian Translation,2020,-1,-1,12,0,283,toshiaki nakazawa,Proceedings of the 7th Workshop on Asian Translation,0,"This paper presents the results of the shared tasks from the 7th workshop on Asian translation (WAT2020). For the WAT2020, 20 teams participated in the shared tasks and 14 teams submitted their translation results for the human evaluation. We also received 12 research paper submissions out of which 7 were accepted. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2020.wat-1.5,Meta Ensemble for {J}apanese-{C}hinese Neural Machine Translation: {K}yoto-{U}+{ECNU} Participation to {WAT} 2020,2020,-1,-1,4,1,13030,zhuoyuan mao,Proceedings of the 7th Workshop on Asian Translation,0,"This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al.,2020). We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation."
2020.nlpcovid19-2.13,A System for Worldwide {COVID}-19 Information Aggregation,2020,-1,-1,13,0,5182,akiko aizawa,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"The global pandemic of COVID-19 has made the public pay close attention to related news, covering various domains, such as sanitation, treatment, and effects on education. Meanwhile, the COVID-19 condition is very different among the countries (e.g., policies and development of the epidemic), and thus citizens would be interested in news in foreign countries. We build a system for worldwide COVID-19 information aggregation containing reliable articles from 10 regions in 7 languages sorted by topics. Our reliable COVID-19 related website dataset collected through crowdsourcing ensures the quality of the articles. A neural machine translation module translates articles in other languages into Japanese and English. A BERT-based topic-classifier trained on our article-topic pair dataset helps users find their interested information efficiently by putting articles into different categories."
2020.lrec-1.145,Adapting {BERT} to Implicit Discourse Relation Classification with a Focus on Discourse Connectives,2020,-1,-1,3,1,16897,yudai kishimoto,Proceedings of the 12th Language Resources and Evaluation Conference,0,"BERT, a neural network-based language model pre-trained on large corpora, is a breakthrough in natural language processing, significantly outperforming previous state-of-the-art models in numerous tasks. However, there have been few reports on its application to implicit discourse relation classification, and it is not clear how BERT is best adapted to the task. In this paper, we test three methods of adaptation. (1) We perform additional pre-training on text tailored to discourse classification. (2) In expectation of knowledge transfer from explicit discourse relations to implicit discourse relations, we add a task named explicit connective prediction at the additional pre-training step. (3) To exploit implicit connectives given by treebank annotators, we add a task named implicit connective prediction at the fine-tuning step. We demonstrate that these three techniques can be combined straightforwardly in a single training pipeline. Through comprehensive experiments, we found that the first and second techniques provide additional gain while the last one did not."
2020.lrec-1.281,Acquiring Social Knowledge about Personality and Driving-related Behavior,2020,-1,-1,4,1,17224,ritsuko iwai,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we introduce our psychological approach to collect human-specific social knowledge from a text corpus, using NLP techniques. It is often not explicitly described but shared among people, which we call social knowledge. We focus on the social knowledge, especially personality and driving. We used the language resources that were developed based on psychological research methods; a Japanese personality dictionary (317 words) and a driving experience corpus (8,080 sentences) annotated with behavior and subjectivity. Using them, we automatically extracted collocations between personality descriptors and driving-related behavior from a driving behavior and subjectivity corpus (1,803,328 sentences after filtering) and obtained unique 5,334 collocations. To evaluate the collocations as social knowledge, we designed four step-by-step crowdsourcing tasks. They resulted in 266 pieces of social knowledge. They include the knowledge that might be difficult to recall by themselves but easy to agree with. We discuss the acquired social knowledge and the contribution to implementations into systems."
2020.lrec-1.379,Development of a {J}apanese Personality Dictionary based on Psychological Methods,2020,-1,-1,4,1,17224,ritsuko iwai,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We propose a new approach to constructing a personality dictionary with psychological evidence. In this study, we collect personality words, using word embeddings, and construct a personality dictionary with weights for Big Five traits. The weights are calculated based on the responses of the large sample (N=1,938, female = 1,004, M=49.8years old:20-78, SD=16.3). All the respondents answered a 20-item personality questionnaire and 537 personality items derived from word embeddings. We present the procedures to examine the qualities of responses with psychological methods and to calculate the weights. These result in a personality dictionary with two sub-dictionaries. We also discuss an application of the acquired resources."
2020.lrec-1.449,{C}oursera Corpus Mining and Multistage Fine-Tuning for Improving Lectures Translation,2020,-1,-1,4,1,12440,haiyue song,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Lectures translation is a case of spoken language translation and there is a lack of publicly available parallel corpora for this purpose. To address this, we examine a framework for parallel corpus mining which is a quick and effective way to mine a parallel corpus from publicly available lectures at Coursera. Our approach determines sentence alignments, relying on machine translation and cosine similarity over continuous-space sentence representations. We also show how to use the resulting corpora in a multistage fine-tuning based domain adaptation for high-quality lectures translation. For Japanese{--}English lectures translation, we extracted parallel data of approximately 40,000 lines and created development and test sets through manual filtering for benchmarking translation performance. We demonstrate that the mined corpus greatly enhances the quality of translation when used in conjunction with out-of-domain parallel corpora via multistage training. This paper also suggests some guidelines to gather and clean corpora, mine parallel sentences, address noise in the mined data, and create high-quality evaluation splits. For the sake of reproducibility, we have released our code for parallel data creation."
2020.lrec-1.454,{JASS}: {J}apanese-specific Sequence to Sequence Pre-training for Neural Machine Translation,2020,32,0,5,1,13030,zhuoyuan mao,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Neural machine translation (NMT) needs large parallel corpora for state-of-the-art translation quality. Low-resource NMT is typically addressed by transfer learning which leverages large monolingual or parallel corpora for pre-training. Monolingual pre-training approaches such as MASS (MAsked Sequence to Sequence) are extremely effective in boosting NMT quality for languages with small parallel corpora. However, they do not account for linguistic information obtained using syntactic analyzers which is known to be invaluable for several Natural Language Processing (NLP) tasks. To this end, we propose JASS, Japanese-specific Sequence to Sequence, as a novel pre-training alternative to MASS for NMT involving Japanese as the source or target language. JASS is joint BMASS (Bunsetsu MASS) and BRSS (Bunsetsu Reordering Sequence to Sequence) pre-training which focuses on Japanese linguistic units called bunsetsus. In our experiments on ASPEC Japanese{--}English and News Commentary Japanese{--}Russian translation we show that JASS can give results that are competitive with if not better than those given by MASS. Furthermore, we show for the first time that joint MASS and JASS pre-training gives results that significantly surpass the individual methods indicating their complementary nature. We will release our code, pre-trained models and bunsetsu annotated data as resources for researchers to use in their own NLP tasks."
2020.lrec-1.561,Towards a Versatile Medical-Annotation Guideline Feasible Without Heavy Medical Knowledge: Starting From Critical Lung Diseases,2020,-1,-1,6,0,12168,shuntaro yada,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Applying natural language processing (NLP) to medical and clinical texts can bring important social benefits by mining valuable information from unstructured text. A popular application for that purpose is named entity recognition (NER), but the annotation policies of existing clinical corpora have not been standardized across clinical texts of different types. This paper presents an annotation guideline aimed at covering medical documents of various types such as radiography interpretation reports and medical records. Furthermore, the annotation was designed to avoid burdensome requirements related to medical knowledge, thereby enabling corpus development without medical specialists. To achieve these design features, we specifically focus on critical lung diseases to stabilize linguistic patterns in corpora. After annotating around 1100 electronic medical records following the annotation scheme, we demonstrated its feasibility using an NER task. Results suggest that our guideline is applicable to large-scale clinical NLP projects."
2020.findings-emnlp.23,Minimize Exposure Bias of {S}eq2{S}eq Models in Joint Entity and Relation Extraction,2020,-1,-1,8,0,4848,ranran zhang,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Joint entity and relation extraction aims to extract relation triplets from plain text directly. Prior work leverages Sequence-to-Sequence (Seq2Seq) models for triplet sequence generation. However, Seq2Seq enforces an unnecessary order on the unordered triplets and involves a large decoding length associated with error accumulation. These methods introduce exposure bias, which may cause the models overfit to the frequent label combination, thus limiting the generalization ability. We propose a novel Sequence-to-Unordered-Multi-Tree (Seq2UMTree) model to minimize the effects of exposure bias by limiting the decoding length to three within a triplet and removing the order among triplets. We evaluate our model on two datasets, DuIE and NYT, and systematically study how exposure bias alters the performance of Seq2Seq models. Experiments show that the state-of-the-art Seq2Seq model overfits to both datasets while Seq2UMTree shows significantly better generalization. Our code is available at \url{https://github.com/WindChimeRan/OpenJERE}."
2020.findings-emnlp.121,Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning,2020,-1,-1,4,0,1612,fei cheng,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Temporal relation classification is the pair-wise task for identifying the relation of a temporal link (TLINKs) between two mentions, i.e. event, time and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two strong transfer learning baselines on both the English and Japanese data."
2020.emnlp-main.192,A Method for Building a Commonsense Inference Dataset based on Basic Events,2020,-1,-1,3,0,16256,kazumasa omura,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We present a scalable, low-bias, and low-cost method for building a commonsense inference dataset that combines automatic extraction from a corpus and crowdsourcing. Each problem is a multiple-choice question that asks contingency between basic events. We applied the proposed method to a Japanese corpus and acquired 104k problems. While humans can solve the resulting problems with high accuracy (88.9{\%}), the accuracy of a high-performance transfer learning model is reasonably low (76.0{\%}). We also confirmed through dataset analysis that the resulting dataset contains low bias. We released the datatset to facilitate language understanding research."
2020.coling-main.114,{BERT}-based Cohesion Analysis of {J}apanese Texts,2020,-1,-1,3,0,16260,nobuhiro ueda,Proceedings of the 28th International Conference on Computational Linguistics,0,"The meaning of natural language text is supported by cohesion among various kinds of entities, including coreference relations, predicate-argument structures, and bridging anaphora relations. However, predicate-argument structures for nominal predicates and bridging anaphora relations have not been studied well, and their analyses have been still very difficult. Recent advances in neural networks, in particular, self training-based language models including BERT (Devlin et al., 2019), have significantly improved many natural language processing tasks, making it possible to dive into the study on analysis of cohesion in the whole text. In this study, we tackle an integrated analysis of cohesion in Japanese texts. Our results significantly outperformed existing studies in each task, especially about 10 to 20 point improvement both for zero anaphora and coreference resolution. Furthermore, we also showed that coreference resolution is different in nature from the other tasks and should be treated specially."
2020.coling-main.514,Native-like Expression Identification by Contrasting Native and Proficient Second Language Speakers,2020,-1,-1,3,0,21614,oleksandr harust,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose a novel task of native-like expression identification by contrasting texts written by native speakers and those by proficient second language speakers. This task is highly challenging mainly because 1) the combinatorial nature of expressions prevents us from choosing candidate expressions a priori and 2) the distributions of the two types of texts overlap considerably. Our solution to the first problem is to combine a powerful neural network-based classifier of sentence-level nativeness with an explainability method that measures an approximate contribution of a given expression to the classifier{'}s prediction. To address the second problem, we introduce a special label neutral and reformulate the classification task as complementary-label learning. Our crowdsourcing-based evaluation and in-depth analysis suggest that our method successfully uncovers linguistically interesting usages distinctive of native speech."
2020.acl-srw.31,Building a {J}apanese Typo Dataset from {W}ikipedia{'}s Revision History,2020,-1,-1,4,0,16259,yu tanaka,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"User generated texts contain many typos for which correction is necessary for NLP systems to work. Although a large number of typo{--}correction pairs are needed to develop a data-driven typo correction system, no such dataset is available for Japanese. In this paper, we extract over half a million Japanese typo{--}correction pairs from Wikipedia{'}s revision history. Unlike other languages, Japanese poses unique challenges: (1) Japanese texts are unsegmented so that we cannot simply apply a spelling checker, and (2) the way people inputting kanji logographs results in typos with drastically different surface forms from correct ones. We address them by combining character-based extraction rules, morphological analyzers to guess readings, and various filtering methods. We evaluate the dataset using crowdsourcing and run a baseline seq2seq model for typo correction."
2020.acl-srw.37,Pre-training via Leveraging Assisting Languages for Neural Machine Translation,2020,17,0,5,1,12440,haiyue song,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Sequence-to-sequence (S2S) pre-training using large monolingual data is known to improve performance for various S2S NLP tasks. However, large monolingual corpora might not always be available for the languages of interest (LOI). Thus, we propose to exploit monolingual corpora of other languages to complement the scarcity of monolingual corpora for the LOI. We utilize script mapping (Chinese to Japanese) to increase the similarity (number of cognates) between the monolingual corpora of helping languages and LOI. An empirical case study of low-resource Japanese-English neural machine translation (NMT) reveals that leveraging large Chinese and French monolingual corpora can help overcome the shortage of Japanese and English monolingual corpora, respectively, for S2S pre-training. Using only Chinese and French monolingual corpora, we were able to improve Japanese-English translation quality by up to 8.5 BLEU in low-resource scenarios."
W19-6704,Applying Machine Translation to Psychology: Automatic Translation of Personality Adjectives,2019,0,0,4,1,17224,ritsuko iwai,"Proceedings of Machine Translation Summit XVII: Translator, Project and User Tracks",0,None
W19-5312,{K}yoto {U}niversity Participation to the {WMT} 2019 News Shared Task,2019,-1,-1,2,1,17599,fabien cromieres,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We describe here the experiments we did for the the news translation shared task of WMT 2019. We focused on the new German-to-French language direction, and mostly used current standard approaches to develop a Neural Machine Translation system. We make use of the Tensor2Tensor implementation of the Transformer model. After carefully cleaning the data and noting the importance of the good use of recent monolingual data for the task, we obtain our final result by combining the output of a diverse set of trained models through the use of their {``}checkpoint agreement{''}."
N19-1085,Improving Event Coreference Resolution by Learning Argument Compatibility from Unlabeled Data,2019,0,0,3,1,10901,yin huang,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Argument compatibility is a linguistic condition that is frequently incorporated into modern event coreference resolution systems. If two event mentions have incompatible arguments in any of the argument roles, they cannot be coreferent. On the other hand, if these mentions have compatible arguments, then this may be used as information towards deciding their coreferent status. One of the key challenges in leveraging argument compatibility lies in the paucity of labeled data. In this work, we propose a transfer learning framework for event coreference resolution that utilizes a large amount of unlabeled data to learn argument compatibility of event mentions. In addition, we adopt an interactive inference network based model to better capture the compatible and incompatible relations between the context words of event mentions. Our experiments on the KBP 2017 English dataset confirm the effectiveness of our model in learning argument compatibility, which in turn improves the performance of the overall event coreference model."
N19-1281,Shrinking {J}apanese Morphological Analyzers With Neural Networks and Semi-supervised Learning,2019,0,0,3,1,26214,arseny tolmachev,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"For languages without natural word boundaries, like Japanese and Chinese, word segmentation is a prerequisite for downstream analysis. For Japanese, segmentation is often done jointly with part of speech tagging, and this process is usually referred to as morphological analysis. Morphological analyzers are trained on data hand-annotated with segmentation boundaries and part of speech tags. A segmentation dictionary or character n-gram information is also provided as additional inputs to the model. Incorporating this extra information makes models large. Modern neural morphological analyzers can consume gigabytes of memory. We propose a compact alternative to these cumbersome approaches which do not rely on any externally provided n-gram or word representations. The model uses only unigram character embeddings, encodes them using either stacked bi-LSTM or a self-attention network, and independently infers both segmentation and part of speech information. The model is trained in an end-to-end and semi-supervised fashion, on labels produced by a state-of-the-art analyzer. We demonstrate that the proposed technique rivals performance of a previous dictionary-based state-of-the-art approach and can even surpass it when training with the combination of human-annotated and automatically-annotated data. Our model itself is significantly smaller than the dictionary-based one: it uses less than 15 megabytes of space."
D19-6014,Diversity-aware Event Prediction based on a Conditional Variational Autoencoder with Reconstruction,2019,0,0,5,1,4549,hirokazu kiyomaru,Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing,0,"Typical event sequences are an important class of commonsense knowledge. Formalizing the task as the generation of a next event conditioned on a current event, previous work in event prediction employs sequence-to-sequence (seq2seq) models. However, what can happen after a given event is usually diverse, a fact that can hardly be captured by deterministic models. In this paper, we propose to incorporate a conditional variational autoencoder (CVAE) into seq2seq for its ability to represent diverse next events as a probabilistic distribution. We further extend the CVAE-based seq2seq with a reconstruction mechanism to prevent the model from concentrating on highly typical events. To facilitate fair and systematic evaluation of the diversity-aware models, we also extend existing evaluation datasets by tying each current event to multiple next events. Experiments show that the CVAE-based models drastically outperform deterministic models in terms of precision and that the reconstruction mechanism improves the recall of CVAE-based models without sacrificing precision."
D19-5814,Machine Comprehension Improves Domain-Specific {J}apanese Predicate-Argument Structure Analysis,2019,0,0,4,0,26490,norio takahashi,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,0,"To improve the accuracy of predicate-argument structure (PAS) analysis, large-scale training data and knowledge for PAS analysis are indispensable. We focus on a specific domain, specifically Japanese blogs on driving, and construct two wide-coverage datasets as a form of QA using crowdsourcing: a PAS-QA dataset and a reading comprehension QA (RC-QA) dataset. We train a machine comprehension (MC) model based on these datasets to perform PAS analysis. Our experiments show that a stepwise training method is the most effective, which pre-trains an MC model based on the RC-QA dataset to acquire domain knowledge and then fine-tunes based on the PAS-QA dataset."
D19-5201,Overview of the 6th Workshop on {A}sian Translation,2019,0,1,13,0,283,toshiaki nakazawa,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including JaâEn, JaâZh scientific paper translation subtasks, JaâEn, JaâKo, JaâEn patent translation subtasks, HiâEn, MyâEn, KmâEn, TaâEn mixed domain subtasks and RuâJa news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated."
D19-1581,Minimally Supervised Learning of Affective Events Using Discourse Relations,2019,0,0,3,0,27136,jun saito,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Recognizing affective events that trigger positive or negative sentiment has a wide range of natural language processing applications but remains a challenging problem mainly because the polarity of an event is not necessarily predictable from its constituent words. In this paper, we propose to propagate affective polarity using discourse relations. Our method is simple and only requires a very small seed lexicon and a large raw corpus. Our experiments using Japanese data show that our method learns affective events effectively without manually labeled data. It also improves supervised learning results when labeled data are small."
Y18-3001,Overview of the 5th Workshop on {A}sian Translation,2018,0,5,10,0,283,toshiaki nakazawa,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-1026,Annotating a Driving Experience Corpus with Behavior and Subjectivity,2018,-1,-1,4,1,17224,ritsuko iwai,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
P18-1044,Neural Adversarial Training for Semi-supervised {J}apanese Predicate-argument Structure Analysis,2018,0,0,3,1,7366,shuhei kurita,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis."
P18-1054,Entity-Centric Joint Modeling of {J}apanese Coreference Resolution and Predicate Argument Structure Analysis,2018,0,1,2,1,21565,tomohide shibata,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Predicate argument structure analysis is a task of identifying structured events. To improve this field, we need to identify a salient entity, which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an embedding, and when the result of both analyses refers to an entity, the entity embedding is updated. The analyses take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically, which is a notoriously difficult task in predicate argument structure analysis."
N18-2041,Knowledge-Enriched Two-Layered Attention Network for Sentiment Analysis,2018,14,0,3,0,24889,abhishek kumar,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,We propose a novel two-layered attention network based on Bidirectional Long Short-Term Memory for sentiment analysis. The novel two-layered attention network takes advantage of the external knowledge bases to improve the sentiment prediction. It uses the Knowledge Graph Embedding generated using the WordNet. We build our model by combining the two-layered attention network with the supervised model based on Support Vector Regression using a Multilayer Perceptron network for sentiment analysis. We evaluate our model on the benchmark dataset of SemEval 2017 Task 5. Experimental results show that the proposed model surpasses the top system of SemEval 2017 Task 5. The model performs significantly better by improving the state-of-the-art system at SemEval 2017 Task 5 by 1.7 and 3.7 points for sub-tracks 1 and 2 respectively.
L18-1050,Comprehensive Annotation of Various Types of Temporal Information on the Time Axis,2018,0,0,3,0,29559,tomohiro sakaguchi,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1637,Improving Crowdsourcing-Based Annotation of {J}apanese Discourse Relations,2018,0,0,5,1,16897,yudai kishimoto,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-2010,{J}uman++: A Morphological Analysis Toolkit for Scriptio Continua,2018,0,2,3,1,26214,arseny tolmachev,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,We present a three-part toolkit for developing morphological analyzers for languages without natural word boundaries. The first part is a C++11/14 lattice-based morphological analysis library that uses a combination of linear and recurrent neural net language models for analysis. The other parts are a tool for exposing problems in the trained model and a partial annotation tool. Our morphological analyzer of Japanese achieves new SOTA on Jumandic-based corpora while being 250 times faster than the previous one. We also perform a small experiment and quantitive analysis and experience of using development tools. All components of the toolkit is open source and available under a permissive Apache 2 License.
C18-1049,A Knowledge-Augmented Neural Network Model for Implicit Discourse Relation Classification,2018,0,1,3,1,16897,yudai kishimoto,Proceedings of the 27th International Conference on Computational Linguistics,0,"Identifying discourse relations that are not overtly marked with discourse connectives remains a challenging problem. The absence of explicit clues indicates a need for the combination of world knowledge and weak contextual clues, which can hardly be learned from a small amount of manually annotated data. In this paper, we address this problem by augmenting the input text with external knowledge and context and by adopting a neural network model that can effectively handle the augmented text. Experiments show that external knowledge did improve the classification accuracy. Contextual information provided no significant gain for implicit discourse relations, but it did for explicit ones."
C18-1128,Cross-lingual Knowledge Projection Using Machine Translation and Target-side Knowledge Base Completion,2018,0,3,4,1,20394,naoki otani,Proceedings of the 27th International Conference on Computational Linguistics,0,"Considerable effort has been devoted to building commonsense knowledge bases. However, they are not available in many languages because the construction of KBs is expensive. To bridge the gap between languages, this paper addresses the problem of projecting the knowledge in English, a resource-rich language, into other languages, where the main challenge lies in projection ambiguity. This ambiguity is partially solved by machine translation and target-side knowledge base completion, but neither of them is adequately reliable by itself. We show their combination can project English commonsense knowledge into Japanese and Chinese with high precision. Our method also achieves a top-10 accuracy of 90{\%} on the crowdsourced English{--}Japanese benchmark. Furthermore, we use our method to obtain 18,747 facts of accurate Japanese commonsense within a very short period."
W17-6301,Automatically Acquired Lexical Knowledge Improves {J}apanese Joint Morphological and Dependency Analysis,2017,24,1,4,0,3202,daisuke kawahara,Proceedings of the 15th International Conference on Parsing Technologies,0,"This paper presents a joint model for morphological and dependency analysis based on automatically acquired lexical knowledge. This model takes advantage of rich lexical knowledge to simultaneously resolve word segmentation, POS, and dependency ambiguities. In our experiments on Japanese, we show the effectiveness of our joint model over conventional pipeline models."
W17-5701,Overview of the 4th Workshop on {A}sian Translation,2017,-1,-1,9,0,283,toshiaki nakazawa,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"This paper presents the results of the shared tasks from the 4th workshop on Asian translation (WAT2017) including JâE, JâC scientific paper translation subtasks, CâJ, KâJ, EâJ patent translation subtasks, HâE mixed domain subtasks, JâE newswire subtasks and JâE recipe subtasks. For the WAT2017, 12 institutions participated in the shared tasks. About 300 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated."
W17-5714,{K}yoto {U}niversity Participation to {WAT} 2017,2017,-1,-1,4,1,17599,fabien cromieres,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"We describe here our approaches and results on the WAT 2017 shared translation tasks. Following our good results with Neural Machine Translation in the previous shared task, we continue this approach this year, with incremental improvements in models and training methods. We focused on the ASPEC dataset and could improve the state-of-the-art results for Chinese-to-Japanese and Japanese-to-Chinese translations."
W17-5014,Automatic Extraction of High-Quality Example Sentences for Word Learning Using a Determinantal Point Process,2017,8,2,2,1,26214,arseny tolmachev,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"Flashcard systems are effective tools for learning words but have their limitations in teaching word usage. To overcome this problem, we propose a novel flashcard system that shows a new example sentence on each repetition. This extension requires high-quality example sentences, automatically extracted from a huge corpus. To do this, we use a Determinantal Point Process which scales well to large data and allows to naturally represent sentence similarity and quality as features. Our human evaluation experiment on Japanese language indicates that the proposed method successfully extracted high-quality example sentences."
W17-2704,Improving Shared Argument Identification in {J}apanese Event Knowledge Acquisition,2017,4,0,2,1,10901,yin huang,Proceedings of the Events and Stories in the News Workshop,0,"Event knowledge represents the knowledge of causal and temporal relations between events. Shared arguments of event knowledge encode patterns of role shifting in successive events. A two-stage framework was proposed for the task of Japanese event knowledge acquisition, in which related event pairs are first extracted, and shared arguments are then identified to form the complete event knowledge. This paper focuses on the second stage of this framework, and proposes a method to improve the shared argument identification of related event pairs. We constructed a gold dataset for shared argument learning. By evaluating our system on this gold dataset, we found that our proposed model outperformed the baseline models by a large margin."
P17-2061,An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation,2017,16,47,3,1,293,chenhui chu,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we propose a novel domain adaptation method named {``}mixed fine tuning{''} for neural machine translation (NMT). We combine two existing approaches namely fine tuning and multi domain NMT. We first train an NMT model on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus which is a mix of the in-domain and out-of-domain corpora. All corpora are augmented with artificial tags to indicate specific domains. We empirically compare our proposed method against fine tuning and multi domain methods and discuss its benefits and shortcomings."
P17-1111,Neural Joint Model for Transition-based {C}hinese Syntactic Analysis,2017,25,12,3,1,7366,shuhei kurita,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features."
E17-1054,Improving {C}hinese Semantic Role Labeling using High-quality Surface and Deep Case Frames,2017,11,0,3,1,33036,gongye jin,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"This paper presents a method for applying automatically acquired knowledge to semantic role labeling (SRL). We use a large amount of automatically extracted knowledge to improve the performance of SRL. We present two varieties of knowledge, which we call surface case frames and deep case frames. Although the surface case frames are compiled from syntactic parses and can be used as rich syntactic knowledge, they have limited capability for resolving semantic ambiguity. To compensate the deficiency of the surface case frames, we compile deep case frames from automatic semantic roles. We also consider quality management for both types of knowledge in order to get rid of the noise brought from the automatic analyses. The experimental results show that Chinese SRL can be improved using automatically acquired knowledge and the quality management shows a positive effect on this task."
W16-5407,{SCTB}: A {C}hinese Treebank in Scientific Domain,2016,0,0,4,1,293,chenhui chu,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"Treebanks are curial for natural language processing (NLP). In this paper, we present our work for annotating a Chinese treebank in scientific domain (SCTB), to address the problem of the lack of Chinese treebanks in this domain. Chinese analysis and machine translation experiments conducted using this treebank indicate that the annotated treebank can significantly improve the performance on both tasks. This treebank is released to promote Chinese NLP research in scientific domain."
W16-4601,Overview of the 3rd Workshop on {A}sian Translation,2016,-1,-1,6,0.977748,283,toshiaki nakazawa,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper presents the results of the shared tasks from the 3rd workshop on Asian translation (WAT2016) including J â E, J â C scientific paper translation subtasks, C â J, K â J, E â J patent translation subtasks, I â E newswire subtasks and H â E, H â J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated."
W16-4616,{K}yoto {U}niversity Participation to {WAT} 2016,2016,0,11,4,1,17599,fabien cromieres,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"We describe here our approaches and results on the WAT 2016 shared translation tasks. We tried to use both an example-based machine translation (MT) system and a neural MT system. We report very good translation results, especially when using neural MT for Chinese-to-Japanese translation."
W16-4402,Large-Scale Acquisition of Commonsense Knowledge via a Quiz Game on a Dialogue System,2016,15,3,3,1,20394,naoki otani,Proceedings of the Open Knowledge Base and Question Answering Workshop ({OKBQA} 2016),0,"Commonsense knowledge is essential for fully understanding language in many situations. We acquire large-scale commonsense knowledge from humans using a game with a purpose (GWAP) developed on a smartphone spoken dialogue system. We transform the manual knowledge acquisition process into an enjoyable quiz game and have collected over 150,000 unique commonsense facts by gathering the data of more than 70,000 players over eight months. In this paper, we present a simple method for maintaining the quality of acquired knowledge and an empirical analysis of the knowledge acquisition process. To the best of our knowledge, this is the first work to collect large-scale knowledge via a GWAP on a widely-used spoken dialogue system."
W16-2349,The {K}yoto {U}niversity Cross-Lingual Pronoun Translation System,2016,8,1,4,1,286,raj dabre,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"In this paper we describe our system we designed and implemented for the crosslingual pronoun prediction task as a part of WMT 2016. The majority of the paper will be dedicated to the system whose outputs we submitted wherein we describe the simplified mathematical model, the details of the components and the working by means of an architecture diagram which also serves as a flowchart. We then discuss the results of the official scores and our observations on the same."
W16-2201,Cross-language Projection of Dependency Trees with Constrained Partial Parsing for Tree-to-Tree Machine Translation,2016,23,1,4,0,33916,yu shen,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
W16-1316,Design of Word Association Games using Dialog Systems for Acquisition of Word Association Knowledge,2016,10,2,3,0,34034,yuichiro machida,Proceedings of the 5th Workshop on Automated Knowledge Base Construction,0,None
S16-1178,{M}2{L} at {S}em{E}val-2016 Task 8: {AMR} Parsing with Neural Networks,2016,16,6,3,0,23339,yevgeniy puzikov,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes our contribution to the SemEval 2016 Workshop. We participated in the Shared Task 8 on Meaning Representation parsing using a transition-based approach, which builds upon the system of Wang et al. (2015a) and Wang et al. (2015b), with additions that utilize a Feedforward Neural Network classifier and an enriched feature set. We observed that exploiting Neural Networks in"
P16-3002,Dependency Forest based Word Alignment,2016,20,0,4,0,34384,hitoshi otsuki,Proceedings of the {ACL} 2016 Student Research Workshop,0,"A hierarchical word alignment model that searches for k-best partial alignments on target constituent 1-best parse trees has been shown to outperform previous models. However, relying solely on 1-best parses trees might hinder the search for good alignments because 1-best trees are not necessarily the best for word alignment tasks in practice. This paper introduces a dependency forest based word alignment model, which utilizes target dependency forests in an attempt to minimize the impact on limitations attributable to 1-best parse trees. We present how k-best alignments are constructed over target-side dependency forests. Alignment experiments on the Japanese-English language pair show a relative error reduction of 4% of the alignment score compared to a model with 1-best parse trees."
P16-1117,Neural Network-Based Model for {J}apanese Predicate Argument Structure Analysis,2016,16,6,3,1,21565,tomohide shibata,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
N16-1002,Flexible Non-Terminals for Dependency Tree-to-Tree Reordering,2016,0,1,4,1,30411,john richardson,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
L16-1101,Paraphrasing Out-of-Vocabulary Words with Word Embeddings and Semantic Lexicons for Low Resource Statistical Machine Translation,2016,15,3,2,1,293,chenhui chu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Out-of-vocabulary (OOV) word is a crucial problem in statistical machine translation (SMT) with low resources. OOV paraphrasing that augments the translation model for the OOV words by using the translation knowledge of their paraphrases has been proposed to address the OOV problem. In this paper, we propose using word embeddings and semantic lexicons for OOV paraphrasing. Experiments conducted on a low resource setting of the OLYMPICS task of IWSLT 2012 verify the effectiveness of our proposed method."
L16-1348,Simultaneous Sentence Boundary Detection and Alignment with Pivot-based Machine Translation Generated Lexicons,2016,10,1,4,0,35088,antoine bourlon,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Sentence alignment is a task that consists in aligning the parallel sentences in a translated article pair. This paper describes a method to perform sentence boundary detection and alignment simultaneously, which significantly improves the alignment accuracy on languages like Chinese with uncertain sentence boundaries. It relies on the definition of hard (certain) and soft (uncertain) punctuation delimiters, the latter being possibly ignored to optimize the alignment result. The alignment method is used in combination with lexicons automatically generated from the input article pairs using pivot-based MT, achieving better coverage of the input words with fewer entries than pre-existing dictionaries. Pivot-based MT makes it possible to build dictionaries for language pairs that have scarce parallel data. The alignment method is implemented in a tool that will be freely available in the near future."
L16-1350,{ASPEC}: {A}sian Scientific Paper Excerpt Corpus,2016,0,24,6,0.977748,283,toshiaki nakazawa,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we describe the details of the ASPEC (Asian Scientific Paper Excerpt Corpus), which is the first large-size parallel corpus of scientific paper domain. ASPEC was constructed in the Japanese-Chinese machine translation project conducted between 2006 and 2010 using the Special Coordination Funds for Promoting Science and Technology. It consists of a Japanese-English scientific paper abstract corpus of approximately 3 million parallel sentences (ASPEC-JE) and a Chinese-Japanese scientific paper excerpt corpus of approximately 0.68 million parallel sentences (ASPEC-JC). ASPEC is used as the official dataset for the machine translation evaluation workshop WAT (Workshop on Asian Translation)."
L16-1468,Parallel Sentence Extraction from Comparable Corpora with Neural Network Features,2016,7,4,3,1,293,chenhui chu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Parallel corpora are crucial for machine translation (MT), however they are quite scarce for most language pairs and domains. As comparable corpora are far more available, many studies have been conducted to extract parallel sentences from them for MT. In this paper, we exploit the neural network features acquired from neural MT for parallel sentence extraction. We observe significant improvements for both accuracy in sentence extraction and MT performance."
D16-1049,{IRT}-based Aggregation Model of Crowdsourced Pairwise Comparison for Evaluating Machine Translations,2016,13,0,4,1,20394,naoki otani,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1247,Insertion Position Selection Model for Flexible Non-Terminals in Dependency Tree-to-Tree Machine Translation,2016,13,1,3,0.977748,283,toshiaki nakazawa,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1029,"Consistent Word Segmentation, Part-of-Speech Tagging and Dependency Labelling Annotation for {C}hinese Language",2016,10,0,6,1,35693,mo shen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we propose a new annotation approach to Chinese word segmentation, part-of-speech (POS) tagging and dependency labelling that aims to overcome the two major issues in traditional morphology-based annotation: Inconsistency and data sparsity. We re-annotate the Penn Chinese Treebank 5.0 (CTB5) and demonstrate the advantages of this approach compared to the original CTB5 annotation through word segmentation, POS tagging and machine translation experiments."
Y15-2010,Cross-language Projection of Dependency Trees for Tree-to-tree Machine Translation,2015,14,2,4,0,33916,yu shen,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"Syntax-based machine translation (MT) is an attractive approach for introducing additional linguistic knowledge in corpus-based MT. Previous studies have shown that treeto-string and string-to-tree translation models perform better than tree-to-tree translation models since tree-to-tree models require two high quality parsers on the source as well as the target language side. In practice, high quality parsers for both languages are difficult to obtain and thus limit the translation quality. In this paper, we explore a method to transfer parse trees from the language side which has a high quality parser to the side which has a low quality parser to obtain transferred parse trees. We then combine the transferred parse trees with the original low quality parse trees. In our tree-to-tree MT experiments we have observed that the new combined trees lead to better performance in terms of BLEU score compared to when the original low quality trees and the transferred trees are used separately."
Y15-1033,Large-scale Dictionary Construction via Pivot-based Statistical Machine Translation with Significance Pruning and Neural Network Features,2015,15,0,5,1,286,raj dabre,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"We present our ongoing work on large-scale Japanese-Chinese bilingual dictionary construction via pivot-based statistical machine translation. We utilize statistical significance pruning to control noisy translation pairs that are induced by pivoting. We construct a large dictionary which we manually verify to be of a high quality. We then use this dictionary and a parallel corpus to learn bilingual neural network language models to obtain features for reranking the n-best list, which leads to an absolute improvement of 5% in accuracy when compared to a setting that does not use significance pruning and reranking."
Y15-1042,Pivot-Based Topic Models for Low-Resource Lexicon Extraction,2015,15,1,3,1,30411,john richardson,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"This paper proposes a range of solutions to the challenges of extracting large and highquality bilingual lexicons for low-resource language pairs. In such scenarios there is often no parallel or even comparable data available. We design three effective pivotbased approaches inspired by the state-ofthe-art technique of bilingual topic modelling, extending previous work to take advantage of trilingual data. The proposed models are shown to outperform traditional methods significantly and can be adapted based upon the nature of available training data. We demonstrate the accuracy of these pivot-based approaches in a realistic scenario generating an IcelandicKorean lexicon from Wikipedia."
W15-5001,Overview of the 2nd Workshop on {A}sian Translation,2015,-1,-1,5,1,283,toshiaki nakazawa,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
W15-5006,{K}yoto{EBMT} System Description for the 2nd Workshop on {A}sian Translation,2015,-1,-1,6,1,30411,john richardson,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
W15-3119,{C}hinese Semantic Role Labeling using High-quality Syntactic Knowledge,2015,19,0,3,1,33036,gongye jin,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents an application of Chinese syntactic knowledge for semantic role labeling (SRL). Besides basic morphological information, syntactic structures are crucial in SRL. However, it is difficult to learn such information from limited, small-scale, manually annotated training data. Instead of manually increasing the size of annotated data, we use a large amount of automatically extracted syntactic knowledge to improve the performance of SRL."
W15-1701,Location Name Disambiguation Exploiting Spatial Proximity and Temporal Consistency,2015,17,3,5,0,37002,takashi awamura,Proceedings of the third International Workshop on Natural Language Processing for Social Media,0,"As the volume of documents on the Web increases, technologies to extract useful information from them become increasingly essential. For instance, information extracted from social network services such as Twitter and Facebook is useful because it contains a lot of location-specific information. To extract such information, it is necessary to identify the location of each location-relevant expression within a document. Previous studies on location disambiguation have tackled this problem on the basis of word sense disambiguation, and did not make use of location-specific clues. In this paper, we propose a method for location disambiguation that takes advantage of the following two clues: spatial proximity and temporal consistency. We confirm the effectiveness of these clues through experiments on Twitter tweets with GPS information."
W15-0813,Classification and Acquisition of Contradictory Event Pairs using Crowdsourcing,2015,7,1,4,0,37105,yu takabatake,"Proceedings of the The 3rd Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",0,"We propose a taxonomy of contradictory event pairs and a method for building a database of such pairs. When a dialog system participates in an open-domain conversation with a human, it is important to avoid the generation of utterances that conflict with the context of the dialog. Here, we refer to a pair of events that are not able to co-occur or that are not inconsistent with each other as a contradictory event pair. In this study, we collected contradictory event pairs using crowdsourcing and constructed a taxonomy of such pairs. We also built a large-scale database of Japanese contradictory event pairs for each class using crowdsourcing. This database will be used for consistent utterance generation in dialog systems."
N15-1125,Leveraging Small Multilingual Corpora for {SMT} Using Many Pivot Languages,2015,24,10,3,1,286,raj dabre,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present our work on leveraging multilingual parallel corpora of small sizes for Statistical Machine Translation between Japanese and Hindi using multiple pivot languages. In our setting, the source and target part of the corpus remains the same, but we show that using several different pivot to extract phrase pairs from these source and target parts lead to large BLEU improvements. We focus on a variety of ways to exploit phrase tables generated using multiple pivots to support a direct source-target phrase table. Our main method uses the Multiple Decoding Paths (MDP) feature of Moses, which we empirically verify as the best compared to the other methods we used. We compare and contrast our various results to show that one can overcome the limitations of small corpora by using as many pivot languages as possible in a multilingual setting. Most importantly, we show that such pivoting aids in learning of additional phrase pairs which are not learned when the direct sourcetarget corpus is small. We obtained improvements of up to 3 BLEU points using multiple pivots for Japanese to Hindi translation compared to when only one pivot is used. To the best of our knowledge, this work is also the first of its kind to attempt the simultaneous utilization of 7 pivot languages at decoding time."
D15-1276,Morphological Analysis for Unsegmented Languages using Recurrent Neural Network Language Model,2015,16,23,3,1,25335,hajime morita,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a new morphological analysis model that considers semantic plausibility of word sequences by using a recurrent neural network language model (RNNLM). In unsegmented languages, since language models are learned from automatically segmented texts and inevitably contain errors, it is not apparent that conventional language models contribute to morphological analysis. To solve this problem, we do not use language models based on raw word sequences but use a semantically generalized language model, RNNLM, in morphological analysis. In our experiments on two Japanese corpora, our proposed model significantly outperformed baseline models. This result indicates the effectiveness of RNNLM in morphological analysis."
2015.mtsummit-wpslt.7,Enhancing function word translation with syntax-based statistical post-editing,2015,-1,-1,3,1,30411,john richardson,Proceedings of the 6th Workshop on Patent and Scientific Literature Translation,0,None
2015.mtsummit-papers.20,{K}orean-{C}hinese word translation using {C}hinese character knowledge,2015,-1,-1,3,0,37941,yuanmei lu,Proceedings of Machine Translation Summit XV: Papers,0,None
Y14-1032,Improving Statistical Machine Translation Accuracy Using Bilingual Lexicon Extractionwith Paraphrases,2014,30,2,3,1,293,chenhui chu,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Statistical machine translation (SMT) suffers from theaccuracy problemthat the translation pairs and their feature scores in the transla- tion model can be inaccurate. Theaccuracy problemis caused by the quality of the unsu- pervised methods used for translation model learning. Previous studies propose estimating comparable features for the translation pairs in the translation model from comparable cor- pora, to improve the accuracy of the transla- tion model. Comparable feature estimation is based on bilingual lexicon extraction (BLE) technology. However, BLE suffers from the data sparseness problem, which makes the comparable features inaccurate. In this paper, we propose using paraphrases to address this problem. Paraphrases are used to smooth the vectors used in comparable feature estimation with BLE. In this way, we improve the qual- ity of comparable features, which can improve the accuracy of the translation model thus im- prove SMT performance. Experiments con- ducted on Chinese-English phrase-based SMT (PBSMT) verify the effectiveness of our pro- posed method."
W14-7001,Overview of the 1st Workshop on {A}sian Translation,2014,26,27,4,1,283,toshiaki nakazawa,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,None
W14-7012,{K}yoto{EBMT} System Description for the 1st Workshop on {A}sian Translation,2014,17,2,4,1,30411,john richardson,Proceedings of the 1st Workshop on {A}sian Translation ({WAT}2014),0,"This paper introduces the KyotoEBMT Example-Based Machine Translation framework. Our system uses a tree-to-tree approach, employing syntactic dependency analysis for both source and target languages in an attempt to preserve non-local structure. The effectiveness of our system is maximized with online example matching and a flexible decoder. Evaluation demonstrates BLEU scores competitive with state-of-the-art SMT baselines. The system implementation is available as open-source."
P14-5014,{K}yoto{EBMT}: An Example-Based Dependency-to-Dependency Translation Framework,2014,16,5,4,1,30411,john richardson,Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"This paper introduces the KyotoEBMT Example-Based Machine Translation framework. Our system uses a tree-to-tree approach, employing syntactic dependency analysis for both source and target languages in an attempt to preserve non-local structure. The effectiveness of our system is maximized with online example matching and a flexible decoder. Evaluation demonstrates BLEU scores competitive with state-of-the-art SMT systems such as Moses. The current implementation is intended to be released as open-source in the near future."
P14-2042,{C}hinese Morphological Analysis with Character-level {POS} Tagging,2014,17,11,4,1,35693,mo shen,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The focus of recent studies on Chinese word segmentation, part-of-speech (POS) tagging and parsing has been shifting from words to characters. However, existing methods have not yet fully utilized the potentials of Chinese characters. In this paper, we investigate the usefulness of character-level part-of-speech in the task of Chinese morphological analysis. We propose the first tagset designed for the task of character-level POS tagging. We propose a method that performs character-level POS tagging jointly with word segmentation and word-level POS tagging. Through experiments, we demonstrate that by introducing character-level POS information, the performance of a baseline morphological analyzer can be significantly improved."
richardson-etal-2014-bilingual,Bilingual Dictionary Construction with Transliteration Filtering,2014,9,1,3,1,30411,john richardson,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present a bilingual transliteration lexicon of 170K Japanese-English technical terms in the scientific domain. Translation pairs are extracted by filtering a large list of transliteration candidates generated automatically from a phrase table trained on parallel corpora. Filtering uses a novel transliteration similarity measure based on a discriminative phrase-based machine translation approach. We demonstrate that the extracted dictionary is accurate and of high recall (F1 score 0.8). Our lexicon contains not only single words but also multi-word expressions, and is freely available. Our experiments focus on Katakana-English lexicon construction, however it would be possible to apply the proposed methods to transliteration extraction for a variety of language pairs."
shibata-etal-2014-large,A Large Scale Database of Strongly-related Events in {J}apanese,2014,16,0,3,1,21565,tomohide shibata,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The knowledge about the relation between events is quite useful for coreference resolution, anaphora resolution, and several NLP applications such as dialogue system. This paper presents a large scale database of strongly-related events in Japanese, which has been acquired with our proposed method (Shibata and Kurohashi, 2011). In languages, where omitted arguments or zero anaphora are often utilized, such as Japanese, the coreference-based event extraction methods are hard to be applied, and so our method extracts strongly-related events in a two-phrase construct. This method first calculates the co-occurrence measure between predicate-arguments (events), and regards an event pair, whose mutual information is high, as strongly-related events. To calculate the co-occurrence measure efficiently, we adopt an association rule mining method. Then, we identify the remaining arguments by using case frames. The database contains approximately 100,000 unique events, with approximately 340,000 strongly-related event pairs, which is much larger than an existing automatically-constructed event database. We evaluated randomly-chosen 100 event pairs, and the accuracy was approximately 68{\%}."
chu-etal-2014-constructing,Constructing a {C}hinese{---}{J}apanese Parallel Corpus from {W}ikipedia,2014,11,9,3,1,293,chenhui chu,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Parallel corpora are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as ChineseâJapanese. As comparable corpora are far more available, many studies have been conducted to automatically construct parallel corpora from comparable corpora. This paper presents a robust parallel sentence extraction system for constructing a ChineseâJapanese parallel corpus from Wikipedia. The system is inspired by previous studies that mainly consist of a parallel sentence candidate filter and a binary classifier for parallel sentence identification. We improve the system by using the common Chinese characters for filtering and two novel feature sets for classification. Experiments show that our system performs significantly better than the previous studies for both accuracy in parallel sentence extraction and SMT performance. Using the system, we construct a ChineseâJapanese parallel corpus with more than 126k highly accurate parallel sentences from Wikipedia. The constructed parallel corpus is freely available at http://orchid.kuee.kyoto-u.ac.jp/{\textasciitilde}chu/resource/wiki{\_}zh{\_}ja.tgz."
izumi-etal-2014-constructing,Constructing a Corpus of {J}apanese Predicate Phrases for Synonym/Antonym Relations,2014,9,1,5,0,39511,tomoko izumi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We construct a large corpus of Japanese predicate phrases for synonym-antonym relations. The corpus consists of 7,278 pairs of predicates such as Âreceive-permission (ACC)Â vs. Âobtain-permission (ACC)Â, in which each predicate pair is accompanied by a noun phrase and case information. The relations are categorized as synonyms, entailment, antonyms, or unrelated. Antonyms are further categorized into three different classes depending on their aspect of oppositeness. Using the data as a training corpus, we conduct the supervised binary classification of synonymous predicates based on linguistically-motivated features. Combining features that are characteristic of synonymous predicates with those that are characteristic of antonymous predicates, we succeed in automatically identifying synonymous predicates at the high F-score of 0.92, a 0.4 improvement over the baseline method of using the Japanese WordNet. The results of an experiment confirm that the quality of the corpus is high enough to achieve automatic classification. To the best of our knowledge, this is the first and the largest publicly available corpus of Japanese predicate phrases for synonym-antonym relations."
jin-etal-2014-framework,A Framework for Compiling High Quality Knowledge Resources From Raw Corpora,2014,13,1,3,1,33036,gongye jin,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The identification of various types of relations is a necessary step to allow computers to understand natural language text. In particular, the clarification of relations between predicates and their arguments is essential because predicate-argument structures convey most of the information in natural languages. To precisely capture these relations, wide-coverage knowledge resources are indispensable. Such knowledge resources can be derived from automatic parses of raw corpora, but unfortunately parsing still has not achieved a high enough performance for precise knowledge acquisition. We present a framework for compiling high quality knowledge resources from raw corpora. Our proposed framework selects high quality dependency relations from automatic parses and makes use of them for not only the calculation of fundamental distributional similarity but also the acquisition of knowledge such as case frames."
D14-1063,Translation Rules with Right-Hand Side Lattices,2014,18,4,2,1,17599,fabien cromieres,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In Corpus-Based Machine Translation, the search space of the translation candidates for a given input sentence is often defined by a set of (cyclefree) context-free grammar rules. This happens naturally in Syntax-Based Machine Translation and Hierarchical Phrase-Based Machine Translation (where the representation will be the set of the target-side half of the synchronous rules used to parse the input sentence). But it is also possible to describe Phrase-Based Machine Translation in this framework. We propose a natural extension to this representation by using lattice-rules that allow to easily encode an exponential number of variations of each rules. We also demonstrate how the representation of the search space has an impact on decoding efficiency, and how it is possible to optimize this representation."
C14-1027,Rapid Development of a Corpus with Discourse Annotations using Two-stage Crowdsourcing,2014,31,8,4,0,3202,daisuke kawahara,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We present a novel approach for rapidly developing a corpus with discourse annotations using crowdsourcing. Although discourse annotations typically require much time and cost owing to their complex nature, we realize discourse annotations in an extremely short time while retaining good quality of the annotations by crowdsourcing two annotation subtasks. In fact, our experiment to create a corpus comprising 30,000 Japanese sentences took less than eight hours to run. Based on this corpus, we also develop a supervised discourse parser and evaluate its performance to verify the usefulness of the acquired corpus."
2014.amta-wptp.15,Post-editing user interface using visualization of a sentence structure,2014,-1,-1,4,1,16897,yudai kishimoto,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,0,"Translation has become increasingly important by virtue of globalization. To reduce the cost of translation, it is necessary to use machine translation and further to take advantage of post-editing based on the result of a machine translation for accurate information dissemination. Such post-editing (e.g., PET [Aziz et al., 2012]) can be used practically for translation between European languages, which has a high performance in statistical machine translation. However, due to the low accuracy of machine translation between languages with different word order, such as Japanese-English and Japanese-Chinese, post-editing has not been used actively."
W13-5714,Towards Fully Lexicalized Dependency Parsing for {K}orean,2013,0,3,3,0,24158,jungyeul park,Proceedings of the 13th International Conference on Parsing Technologies ({IWPT} 2013),0,None
W13-2505,{C}hinese{--}{J}apanese Parallel Sentence Extraction from Quasi{--}Comparable Corpora,2013,18,8,3,1,293,chenhui chu,Proceedings of the Sixth Workshop on Building and Using Comparable Corpora,0,"Parallel sentences are crucial for statistical machine translation (SMT). However, they are quite scarce for most language pairs, such as Chinesexe2x80x90Japanese. Many studies have been conducted on extracting parallel sentences from noisy parallel or comparable corpora. We extract Chinesexe2x80x90Japanese parallel sentences from quasixe2x80x90comparable corpora, which are available in far larger quantities. The task is significantly more difficult than the extraction from noisy parallel or comparable corpora. We extend a previous study that treats parallel sentence identification as a binary classification problem. Previous method of classifier training by the Cartesian product is not practical, because it differs from the real process of parallel sentence extraction. We propose a novel classifier training method that simulates the real sentence extraction process. Furthermore, we use linguistic knowledge of Chinese character features. Experimental results on quasixe2x80x90 comparable corpora indicate that our proposed approach performs significantly better than the previous study."
P13-1016,Distortion Model Considering Rich Context for Statistical Machine Translation,2013,26,6,5,0.833333,288,isao goto,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes new distortion models for phrase-based SMT. In decoding, a distortion model estimates the source word position to be translated next (NP) given the last translated source word position (CP). We propose a distortion model that can consider the word at the CP, a word at an NP candidate, and the context of the CP and the NP candidate simultaneously. Moreover, we propose a further improved model that considers richer context by discriminating label sequences that specify spans from the CP to NP candidates. It enables our model to learn the effect of relative word order among NP candidates as well as to learn the effect of distances from the training data. In our experiments, our model improved 2.9 BLEU points for Japanese-English and 2.6 BLEU points for Chinese-English translation compared to the lexical reordering models."
I13-1005,Precise Information Retrieval Exploiting Predicate-Argument Structures,2013,25,5,4,0,3202,daisuke kawahara,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,A concept can be linguistically expressed in various syntactic constructions. Such syntactic variations spoil the effectiveness of incorporating dependencies between words into information retrieval systems. This paper presents an information retrieval method for normalizing syntactic variations via predicate-argument structures. We conduct experiments on standard test collections and show the effectiveness of our approach. Our proposed method significantly outperforms a baseline method based on word dependencies.
I13-1019,A Simple Approach to Unknown Word Processing in {J}apanese Morphological Analysis,2013,14,8,2,1,5794,ryohei sasano,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper presents a simple but effective approach to unknown word processing in Japanese morphological analysis, which handles 1) unknown words that are derived from words in a pre-defined lexicon and 2) unknown onomatopoeias. Our approach leverages derivation rules and onomatopoeia patterns, and correctly recognizes certain types of unknown words. Experiments revealed that our approach recognized about 4,500 unknown words in 100,000 Web sentences with only 80 harmful side effects and a 6% loss in speed."
I13-1020,{C}hinese Word Segmentation by Mining Maximized Substrings,2013,21,1,3,1,35693,mo shen,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"A major problem in the field of Chinese word segmentation is the identification of out-ofvocabulary words. We propose a simple yet effective approach for extracting maximized substrings, which provide good estimations of unknown word boundaries. We also develop a new semi-supervised segmentation technique that incorporates retrieved substrings using discriminative learning. The effectiveness of this novel approach is demonstrated through experiments using both in-domain and out-ofdomain data."
I13-1030,Robust Transliteration Mining from Comparable Corpora with Bilingual Topic Models,2013,25,6,3,1,30411,john richardson,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"We present a high-precision, languageindependent transliteration framework applicable to bilingual lexicon extraction. Our approach is to employ a bilingual topic model to enhance the output of a state-of-the-art graphemebased transliteration baseline. We demonstrate that this method is able to extract a high-quality bilingual lexicon from a comparable corpus, and we extend the topic model to propose a solution to the out-of-domain problem."
I13-1124,High Quality Dependency Selection from Automatic Parses,2013,13,2,3,1,33036,gongye jin,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Many NLP tasks such as question answering and knowledge acquisition are tightly dependent on dependency parsing. Dependency parsing accuracy is always decisive for the performance of subsequent tasks. Therefore, reducing dependency parsing errors or selecting high quality dependencies is a primary issue. In this paper, we present a supervised approach for automatically selecting high quality dependencies from automatic parses. Experimental results on three different languages show that our approach can effectively select high quality dependencies from the result analyzed by a dependency parser."
I13-1163,Accurate Parallel Fragment Extraction from Quasi{--}Comparable Corpora using Alignment Model and Translation Lexicon,2013,15,10,3,1,293,chenhui chu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Although parallel sentences rarely exist in quasixe2x80x90comparable corpora, there could be parallel fragments that are also helpful for statistical machine translation (SMT). Previous studies cannot accurately extract parallel fragments from quasixe2x80x90comparable corpora. To solve this problem, we propose an accurate parallel fragment extraction system that uses an alignment model to locate the parallel fragment candidates, and uses an accurate lexicon filter to identify the truly parallel ones. Experimental results indicate that our system can accurately extract parallel fragments, and our proposed method significantly outperforms a statexe2x80x90ofxe2x80x90thexe2x80x90art approach. Furthermore, we investigate the factors that may affect the performance of our system in detail."
D13-1095,{J}apanese Zero Reference Resolution Considering Exophora and Author/Reader Mentions,2013,12,10,3,1,10699,masatsugu hangyo,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"In Japanese, zero references often occur and many of them are categorized into zero exophora, in which a referent is not mentioned in the document. However, previous studies have focused on only zero endophora, in which a referent explicitly appears. We present a zero reference resolution model considering zero exophora and author/reader of a document. To deal with zero exophora, our model adds pseudo entities corresponding to zero exophora to candidate referents of zero pronouns. In addition, we automatically detect mentions that refer to the author and reader of a document by using lexico-syntactic patterns. We represent their particular behavior in a discourse as a feature vector of a machine learning model. The experimental results demonstrate the effectiveness of our model for not only zero exophora but also zero endophora."
D13-1121,Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in {J}apanese,2013,25,5,3,1,5794,ryohei sasano,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"We present a method for automatically acquiring knowledge for case alternation between the passive and active voices in Japanese. By leveraging several linguistic constraints on alternation patterns and lexical case frames obtained from a large Web corpus, our method aligns a case frame in the passive voice to a corresponding case frame in the active voice and finds an alignment between their cases. We then apply the acquired knowledge to a case alternation task and prove its usefulness."
Y12-1033,A Reranking Approach for Dependency Parsing with Variable-sized Subtree Features,2012,23,7,3,1,35693,mo shen,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Employing higher-order subtree structures in graph-based dependency parsing has shown substantial improvement over the accuracy, however suffers from the inefficiency increasing with the order of subtrees. We present a new reranking approach for dependency parsing that can utilize complex subtree representation by applying efficient subtree selection heuristics. We demonstrate the effectiveness of the approach in experiments conducted on the Penn Treebank and the Chinese Treebank. Our system improves the baseline accuracy from 91.88% to 93.37% for English, and in the case of Chinese from 87.39% to 89.16%."
Y12-1058,Building a Diverse Document Leads Corpus Annotated with Semantic Relations,2012,3,12,3,1,10699,masatsugu hangyo,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"In these days, semantic analysis has been actively studied in natural language processing. For the study of semantic analysis, corpora with semantic annotations are essential. Although there are such corpora annotated on newspaper articles, there are various genres and styles, including linguistic expressions that are not found in newspaper articles. In this paper, we build a diverse document leads corpus annotated with semantic relations. To reduce the workload of annotators and annotate as many various documents as possible, we restrict the annotation target of each document to only the first three sentences. We have completed building a corpus of 1,000 documents and report the statistics of this corpus."
W12-5210,Constrained Hidden {M}arkov Model for Bilingual Keyword Pairs Alignment,2012,0,0,3,0,42099,denny cahyadi,Proceedings of the 10th Workshop on {A}sian Language Resources,0,None
chu-etal-2012-chinese,"{C}hinese Characters Mapping Table of {J}apanese, Traditional {C}hinese and Simplified {C}hinese",2012,6,7,3,1,293,chenhui chu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Chinese characters are used both in Japanese and Chinese, which are called Kanji and Hanzi respectively. Chinese characters contain significant semantic information, a mapping table between Kanji and Hanzi can be very useful for many Japanese-Chinese bilingual applications, such as machine translation and cross-lingual information retrieval. Because Kanji characters are originated from ancient China, most Kanji have corresponding Chinese characters in Hanzi. However, the relation between Kanji and Hanzi is quite complicated. In this paper, we propose a method of making a Chinese characters mapping table of Japanese, Traditional Chinese and Simplified Chinese automatically by means of freely available resources. We define seven categories for Kanji based on the relation between Kanji and Hanzi, and classify mappings of Chinese characters into these categories. We use a resource from Wiktionary to show the completeness of the mapping table we made. Statistical comparison shows that our proposed method makes a more complete mapping table than the current version of Wiktionary."
C12-1067,Flexible {J}apanese Sentence Compression by Relaxing Unit Constraints,2012,23,0,2,1,13664,jun harashima,Proceedings of {COLING} 2012,0,"Sentence compression is important in a wide range of applications in natural language processing. Previous approaches of Japanese sentence compression can be divided into two groups. Word-based methods extract a subset of words from a sentence to shorten it, while bunsetsubased methods extract a subset of bunsetsu (where a bunsetsu is a text unit that consists of content words and following function words). Basically, bunsetsu-based methods perform better than word-based methods. However, bunsetsu-based methods have the disadvantage that they cannot drop unimportant words from each bunsetsu because they have to follow constraints under which each bunsetsu is treated as a unit. In this paper, we propose a novel compression method to overcome this disadvantage. Our method relaxes the constraints using Lagrangian relaxation and shortens each bunsetsu if it contains unimportant words. Experimental results show that our method effectively compresses a sentence while preserving its important information and grammaticality."
C12-1117,Semi-Supervised Noun Compound Analysis with Edge and Span Features,2012,39,1,2,1,4526,yugo murawaki,Proceedings of {COLING} 2012,0,"In this paper, we propose the use of spans in addition to edges in noun compound analysis. A span is a sequence of words that can represent a noun compound. Compared with edges, spans have good properties in terms of semi-supervised parsing. They can be reliably extracted from a huge amount of unannotated text. In addition, while the combinations of edges such as sibling and grandparent interactions are, in general, difficult to handle in parsing, it is quite easy to utilize spans with arbitrary width. We show that spans can be incorporated straightforwardly into the standard chart-based parsing algorithm. We create a semi-supervised discriminative parser that combines edge and span features. Experiments show that span features improve accuracy and that further gain is obtained when they are combined with edge features."
C12-1120,Alignment by Bilingual Generation and Monolingual Derivation,2012,21,8,2,1,283,toshiaki nakazawa,Proceedings of {COLING} 2012,0,"One of the main issues in a word alignment task is the difficulty of handling function words that do not have direct translations which we call unique function words. They are often aligned to some words in the other language incorrectly. This is prominent in language pairs with very different sentence structures. In this paper, we propose a novel approach for handling unique function words. The proposed model monolingually derives unique function words from bilingually generated treelet pairs. The monolingual derivation prevents incorrect alignments for unique function words. The derivation probabilities are estimated from a large monolingual corpus, which is much easier to acquire than a parallel corpus. Also, the proposed alignment model uses semantic-head dependency trees where dependency relations between words become similar in each language. Experimental results on an English-Japanese corpus show that the proposed model achieves better alignment and translation quality compared with the baseline models."
2012.iwslt-evaluation.12,{EBMT} system of {K}yoto {U}niversity in {OLYMPICS} task at {IWSLT} 2012,2012,15,1,3,1,293,chenhui chu,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the EBMT system of Kyoto University that participated in the OLYMPICS task at IWSLT 2012. When translating very different language pairs such as Chinese-English, it is very important to handle sentences in tree structures to overcome the difference. Many recent studies incorporate tree structures in some parts of translation process, but not all the way from model training (alignment) to decoding. Our system is a fully tree-based translation system where we use the Bayesian phrase alignment model on dependency trees and example-based translation. To improve the translation quality, we conduct some special processing for the IWSLT 2012 OLYMPICS task, including sub-sentence splitting, non-parallel sentence filtering, adoption of an optimized Chinese segmenter and rule-based decoding constraints."
2012.eamt-1.7,Exploiting Shared {C}hinese Characters in {C}hinese Word Segmentation Optimization for {C}hinese-{J}apanese Machine Translation,2012,14,15,4,1,293,chenhui chu,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Unknown words and word segmentation granularity are two main problems in Chinese word segmentation for ChineseJapanese Machine Translation (MT). In this paper, we propose an approach of exploiting common Chinese characters shared between Chinese and Japanese in Chinese word segmentation optimization for MT aiming to solve these problems. We augment the system dictionary of a Chinese segmenter by extracting Chinese lexicons from a parallel training corpus. In addition, we adjust the granularity of the training data for the Chinese segmenter to that of Japanese. Experimental results of Chinese-Japanese MT on a phrase-based SMT system show that our approach improves MT performance significantly."
P11-1109,Extracting Paraphrases from Definition Sentences on the Web,2011,29,22,5,0.919716,26950,chikara hashimoto,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We propose an automatic method of extracting paraphrases from definition sentences, which are also automatically acquired from the Web. We observe that a huge number of concepts are defined in Web documents, and that the sentences that define the same concept tend to convey mostly the same information using different expressions and thus contain many paraphrases. We show that a large number of paraphrases can be automatically extracted with high precision by regarding the sentences that define the same concept as parallel corpora. Experimental results indicated that with our method it was possible to extract about 300,000 paraphrases from 6 x 108 Web documents with a precision rate of about 94%."
I11-1051,Generative Modeling of Coordination by Factoring Parallelism and Selectional Preferences,2011,20,4,2,0.968778,3202,daisuke kawahara,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present a unified generative model of coordination that considers parallelism of conjuncts and selectional preferences. Parallelism of conjuncts, which frequently characterizes coordinate structures, is modeled as a synchronized generation process in the generative parser. Selectional preferences learned from a large web corpus provide an important clue for resolving the ambiguities of coordinate structures. Our experiments of Japanese dependency parsing indicate the effectiveness of our approach, particularly in the domains of newspapers and patents."
I11-1085,A Discriminative Approach to {J}apanese Zero Anaphora Resolution with Large-scale Lexicalized Case Frames,2011,22,33,2,1,5794,ryohei sasano,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present a discriminative model for Japanese zero anaphora resolution that simultaneously determines an appropriate case frame for a given predicate and its predicate-argument structure. Our model is based on a log linear framework, and exploits lexical features obtained from a large raw corpus, as well as non-lexical features obtained from a relatively small annotated corpus. We report the results of zero anaphora resolution on Web text and demonstrate the effectiveness of our approach. In addition, we also investigate the relative importance of each feature for resolving zero anaphora in Web text."
I11-1089,{B}ayesian Subtree Alignment Model based on Dependency Trees,2011,19,11,2,1,283,toshiaki nakazawa,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Word sequential alignment models work well for similar language pairs, but they are quite inadequate for distant language pairs. It is difficult to align words or phrases of distant languages with high accuracy without structural information of the sentences. In this paper, we pro- pose a Bayesian subtree alignment model that incorporates dependency relations be- tween subtrees in dependency tree struc- tures on both sides. The dependency re- lation model is a kind of tree-based re- ordering model, and can handle non-local reorderings, which sequential word-based models often cannot handle properly. The model is also capable of handling multilevel structures, making it possible to find many-to-many correspondences automatically without any heuristic rules. The size of the structures is controlled by nonparametric Bayesian priors. Experimental alignment results show that our model achieves 3.5 points better alignment error rate for English-Japanese than the word sequential alignment model, thereby verifying that the use of dependency information is effective for structurally different language pairs."
I11-1115,Acquiring Strongly-related Events using Predicate-argument Co-occurring Statistics and Case Frames,2011,16,8,2,1,21565,tomohide shibata,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper proposes a method for automatically acquiring strongly-related events from a large corpus using predicateargument co-occurring statistics and case frames. The strongly-related events are acquired in the form of strongly-related two predicates with their relevant arguments. First, strongly-related events are acquired from predicate-argument cooccurring statistics. Then, the remaining argument alignment is performed by using case frames. We conducted experiments using a Web corpus consisting of 1.6G sentences. The accuracy for the extracted event pairs was 96%, and the accuracy of the argument alignment was 79%. The number of acquired event pairs was about 20 thousands."
I11-1116,Relevance Feedback using Latent Information,2011,20,2,2,1,13664,jun harashima,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We present a novel relevance feedback (RF) method that uses not only the surface information in texts, but also the latent information contained therein. In the proposed method, we infer the latent topic distribution in user feedback and in each document in the search results using latent Dirichlet allocation, and then we modify the search results so that documents with a similar topic distribution to that of the feedback are re-ranked higher. Evaluation results show that our method is effective for both explicit and pseudo RF, and that it has the advantage of performing well even when only a small amount of user feedback is available."
D11-1047,Efficient retrieval of tree translation examples for Syntax-Based Machine Translation,2011,17,9,2,1,17599,fabien cromieres,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,We propose an algorithm allowing to efficiently retrieve example treelets in a parsed tree database in order to allow on-the-fly extraction of syntactic translation rules. We also propose improvements of this algorithm allowing several kinds of flexible matchings.
D11-1056,Non-parametric {B}ayesian Segmentation of {J}apanese Noun Phrases,2011,28,0,2,1,4526,yugo murawaki,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer."
2011.mtsummit-papers.53,{J}apanese-{C}hinese Phrase Alignment Using Common {C}hinese Characters Information,2011,-1,-1,3,1,293,chenhui chu,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-3902,Exploiting Term Importance Categories and Dependency Relations for Natural Language Search,2010,17,2,2,1,15889,keiji shinzato,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"In this paper, we propose a method that clearly separates terms (words and dependency relations) in a natural language query into important and other terms, and differently handles the terms according to their importance. The proposed method uses three types of term importance: necessary, optional, and unnecessary. The importance are detected using linguistic clues. We evaluated the proposed method using a test collection for Japanese information retrieval. Performance was resultantly improved by differently handling terms according to their importance."
W10-3903,Summarizing Search Results using {PLSI},2010,17,9,2,1,13664,jun harashima,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"In this paper, we investigate generating a set of query-focused summaries from search results. Since there may be many topics related to a given query in the search results, in order to summarize these results, they should first be classified into topics, and then each topic should be summarized individually. In this summarization process, two types of redundancies need to be reduced. First, each topic summary should not contain any redundancy (we refer to this problem as redundancy within a summary). Second, a topic summary should not be similar to any other topic summary (we refer to this problem as redundancy between summaries). In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. We also propose a method using PLSI to summarize search results. Evaluation results confirm that our method performs well in classifying search results and reducing the redundancy between summaries."
P10-1037,Using Smaller Constituents Rather Than Sentences in Active Learning for {J}apanese Dependency Parsing,2010,24,16,2,0.790923,31516,manabu sassano,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We investigate active learning methods for Japanese dependency parsing. We propose active learning methods of using partial dependency relations in a given sentence for parsing and evaluate their effectiveness empirically. Furthermore, we utilize syntactic constraints of Japanese to obtain more labeled examples from precious labeled ones that annotators give. Experimental results show that our proposed methods improve considerably the learning curve of Japanese dependency parsing. In order to achieve an accuracy of over 88.3%, one of our methods requires only 34.4% of labeled examples as compared to passive learning."
N10-1120,Dependency Tree-based Sentiment Classification using {CRF}s with Hidden Variables,2010,18,243,3,0,27752,tetsuji nakagawa,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"In this paper, we present a dependency tree-based method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables. Subjective sentences often contain words which reverse the sentiment polarities of other words. Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method. In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable. The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables. Sum-product belief propagation is used for inference. Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features."
murawaki-kurohashi-2010-online,Online {J}apanese Unknown Morpheme Detection using Orthographic Variation,2010,21,3,2,1,4526,yugo murawaki,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"To solve the unknown morpheme problem in Japanese morphological analysis, we previously proposed a novel framework of online unknown morpheme acquisition and its implementation. This framework poses a previously unexplored problem, online unknown morpheme detection. Online unknown morpheme detection is a task of finding morphemes in each sentence that are not listed in a given lexicon. Unlike in English, it is a non-trivial task because Japanese does not delimit words by white space. We first present a baseline method that simply uses the output of the morphological analyzer. We then show that it fails to detect some unknown morphemes because they are over-segmented into shorter registered morphemes. To cope with this problem, we present a simple solution, the use of orthographic variation of Japanese. Under the assumption that orthographic variants behave similarly, each over-segmentation candidate is checked against its counterparts. Experiments show that the proposed method improves the recall of detection and contributes to improving unknown morpheme acquisition."
kawahara-kurohashi-2010-acquiring,Acquiring Reliable Predicate-argument Structures from Raw Corpora for Case Frame Compilation,2010,25,16,2,0.999819,3202,daisuke kawahara,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a method for acquiring reliable predicate-argument structures from raw corpora for automatic compilation of case frames. Such lexicon compilation requires highly reliable predicate-argument structures to practically contribute to Natural Language Processing (NLP) applications, such as paraphrasing, text entailment, and machine translation. However, to precisely identify predicate-argument structures, case frames are required. This issue is similar to the question ''``what came first: the chicken or the egg?'''' In this paper, we propose the first step in the extraction of reliable predicate-argument structures without using case frames. We first apply chunking to raw corpora and then extract reliable chunks to ensure that high-quality predicate-argument structures are obtained from the chunks. We conducted experiments to confirm the effectiveness of our approach. We successfully extracted reliable chunks of an accuracy of 98{\%} and high-quality predicate-argument structures of an accuracy of 97{\%}. Our experiments confirmed that we succeeded in acquiring highly reliable predicate-argument structures that can be used to compile case frames."
C10-2061,Identifying Contradictory and Contrastive Relations between Statements to Outline Web Information on a Given Topic,2010,23,13,3,0.999819,3202,daisuke kawahara,Coling 2010: Posters,0,"We present a method for producing a bird's-eye view of statements that are expressed on Web pages on a given topic. This method aggregates statements that are relevant to the topic, and shows contradictory and contrastive relations among them. This view of contradictions and contrasts helps users acquire a top-down understanding of the topic. To realize this, we extract such statements and relations, including cross-document implicit contrastive relations between statements, in an unsupervised manner. Our experimental results indicate the effectiveness of our approach."
C10-2101,Semantic Classification of Automatically Acquired Nouns using Lexico-Syntactic Clues,2010,30,1,2,1,4526,yugo murawaki,Coling 2010: Posters,0,"In this paper, we present a two-stage approach to acquire Japanese unknown morphemes from text with full POS tags assigned to them. We first acquire unknown morphemes only making a morphology-level distinction, and then apply semantic classification to acquired nouns. One advantage of this approach is that, at the second stage, we can exploit syntactic clues in addition to morphological ones because as a result of the first stage acquisition, we can rely on automatic parsing. Japanese semantic classification poses an interesting challenge: proper nouns need to be distinguished from common nouns. It is because lapanese has no orthographic distinction between common and proper nouns and no apparent morphosyntactic distinction between them. We explore lexico-syntactic clues that are extracted from automatically parsed text and investigate their effects."
W09-3817,Capturing Consistency between Intra-clause and Inter-clause Relations in Knowledge-rich Dependency and Case Structure Analysis,2009,30,1,2,1,3202,daisuke kawahara,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We present a method for dependency and case structure analysis that captures the consistency between intra-clause relations (i.e., case structures or predicate-argument structures) and inter-clause relations. We assess intra-clause relations on the basis of case frames and inter-clause relations on the basis of transition knowledge between case frames. Both knowledge bases are automatically acquired from a massive amount of parses of a Web corpus. The significance of this study is that the proposed method selects the best dependency and case structure that are consistent within each clause and between clauses. We confirm that this method contributes to the improvement of dependency parsing of Japanese."
W09-2908,Bottom-up Named Entity Recognition using Two-stage Machine Learning Method,2009,10,3,3,0,46939,hirotaka funayama,"Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications ({MWE} 2009)",0,"This paper proposes Japanese bottom-up named entity recognition using a two-stage machine learning method. Most work has formalized Named Entity Recognition as a sequential labeling problem, in which only local information is utilized for the label estimation, and thus a long named entity consisting of several morphemes tends to be wrongly recognized. Our proposed method regards a compound noun (chunk) as a labeling unit, and first estimates the labels of all the chunks in a phrasal unit (bunsetsu) using a machine learning method. Then, the best label assignment in the bunsetsu is determined from bottom up as the CKY parsing algorithm using a machine learning method. We conducted an experimental on CRL NE data, and achieved an F measure of 89.79, which is higher than previous work."
W09-2302,Statistical Phrase Alignment Model Using Dependency Relation Probability,2009,14,4,2,1,283,toshiaki nakazawa,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"When aligning very different language pairs, the most important needs are the use of structural information and the capability of generating one-to-many or many-to-many correspondences. In this paper, we propose a novel phrase alignment method which models word or phrase dependency relations in dependency tree structures of source and target languages. The dependency relation model is a kind of tree-based reordering model, and can handle non-local reorderings which sequential word-based models often cannot handle properly. The model is also capable of estimating phrase correspondences automatically without any heuristic rules. Experimental results of alignment show that our model could achieve F-measure 1.7 points higher than the conventional word alignment model with symmetrization algorithms"
P09-4001,{WISDOM}: A Web Information Credibility Analysis Systematic,2009,6,23,6,0,47164,susumu akamine,Proceedings of the {ACL}-{IJCNLP} 2009 Software Demonstrations,0,"We demonstrate an information credibility analysis system called WISDOM. The purpose of WISDOM is to evaluate the credibility of information available on the Web from multiple viewpoints. WISDOM considers the following to be the source of information credibility: information contents, information senders, and information appearances. We aim at analyzing and organizing these measures on the basis of semantics-oriented natural language processing (NLP) techniques."
P09-2013,A Unified Single Scan Algorithm for {J}apanese Base Phrase Chunking and Dependency Parsing,2009,10,2,2,0.790923,31516,manabu sassano,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"We describe an algorithm for Japanese analysis that does both base phrase chunking and dependency parsing simultaneously in linear-time with a single scan of a sentence. In this paper, we show a pseudo code of the algorithm and evaluate its performance empirically on the Kyoto University Corpus. Experimental results show that the proposed algorithm with the voted perceptron yields reasonably good accuracy."
N09-1059,The Effect of Corpus Size on Case Frame Acquisition for Discourse Analysis,2009,21,11,3,1,5794,ryohei sasano,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper reports the effect of corpus size on case frame acquisition for discourse analysis in Japanese. For this study, we collected a Japanese corpus consisting of up to 100 billion words, and constructed case frames from corpora of six different sizes. Then, we applied these case frames to syntactic and case structure analysis, and zero anaphora resolution. We obtained better results by using case frames constructed from larger corpora; the performance was not saturated even with a corpus size of 100 billion words."
E09-1020,An Alignment Algorithm Using Belief Propagation and a Structure-Based Distortion Model,2009,20,14,2,1,17599,fabien cromieres,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"In this paper, we first demonstrate the interest of the Loopy Belief Propagation algorithm to train and use a simple alignment model where the expected marginal values needed for an efficient EM-training are not easily computable. We then improve this model with a distortion model based on structure conservation."
D09-1151,A Probabilistic Model for Associative Anaphora Resolution,2009,22,10,2,1,5794,ryohei sasano,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a probabilistic model for associative anaphora resolution in Japanese. Associative anaphora is a type of bridging anaphora, in which the anaphor and its antecedent are not coreferent. Our model regards associative anaphora as a kind of zero anaphora and resolves it in the same manner as zero anaphora resolution using automatically acquired lexical knowledge. Experimental results show that our model resolves associative anaphora with good performance and the performance is improved by resolving it simultaneously with zero anaphora."
P08-2018,Blog Categorization Exploiting Domain Dictionary and Dynamically Estimated Domains of Unknown Words,2008,11,8,2,1,26950,chikara hashimoto,"Proceedings of ACL-08: HLT, Short Papers",0,"This paper presents an approach to text categorization that i) uses no machine learning and ii) reacts on-the-fly to unknown words. These features are important for categorizing Blog articles, which are updated on a daily basis and filled with newly coined words. We categorize 600 Blog articles into 12 domains. As a result, our categorization method achieved an accuracy of 94.0% (564/600)."
shinzato-etal-2008-large,A Large-Scale Web Data Collection as a Natural Language Processing Infrastructure,2008,5,12,4,1,15889,keiji shinzato,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In recent years, language resources acquired from theWeb are released, and these data improve the performance of applications in several NLP tasks. Although the language resources based on the web page unit are useful in NLP tasks and applications such as knowledge acquisition, document retrieval and document summarization, such language resources are not released so far. In this paper, we propose a data format for results of web page processing, and a search engine infrastructure which makes it possible to share approximately 100 million Japanese web data. By obtaining the web data, NLP researchers are enabled to begin their own processing immediately without analyzing web pages by themselves."
I08-2080,{J}apanese Named Entity Recognition Using Structural Natural Language Processing,2008,7,29,2,1,5794,ryohei sasano,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"This paper presents an approach that uses structural information for Japanese named entity recognition (NER). Our NER system is based on Support Vector Machine (SVM), and utilizes four types of structural information: cache features, coreference relations, syntactic features and caseframe features, which are obtained from structural analyses. We evaluated our approach on CRL NE data and obtained a higher F-measure than existing approaches that do not use structural information. We also conducted experiments on IREX NE data and an NE-annotated web corpus and confirmed that structural information improves the performance of NER."
I08-2110,{SYNGRAPH}: A Flexible Matching Method based on Synonymous Expression Extraction from an Ordinary Dictionary and a Web Corpus,2008,12,14,5,1,21565,tomohide shibata,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,None
I08-1025,{TSUBAKI}: An Open Search Engine Infrastructure for Developing New Information Access Methodology,2008,0,48,5,1,15889,keiji shinzato,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,None
D08-1045,Online Acquisition of {J}apanese Unknown Morphemes using Morphological Constraints,2008,13,15,2,1,4526,yugo murawaki,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel lexicon acquirer that works in concert with the morphological analyzer and has the ability to run in online mode. Every time a sentence is analyzed, it detects unknown morphemes, enumerates candidates and selects the best candidates by comparing multiple examples kept in the storage. When a morpheme is unambiguously selected, the lexicon acquirer updates the dictionary of the analyzer, and it will be used in subsequent analysis. We use the constraints of Japanese morphology and effectively reduce the number of examples required to acquire a morpheme. Experiments show that unknown morphemes were acquired with high accuracy and improved the quality of morphological analysis."
C08-1054,Coordination Disambiguation without Any Similarities,2008,20,12,2,1,3202,daisuke kawahara,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"The use of similarities has been one of the main approaches to resolve the ambiguities of coordinate structures. In this paper, we present an alternative method for coordination disambiguation, which does not use similarities. Our hypothesis is that coordinate structures are supported by surrounding dependency relations, and that such dependency relations rather yield similarity between conjuncts, which humans feel. Based on this hypothesis, we built a Japanese fully-lexicalized generative parser that includes coordination disambiguation. Experimental results on web sentences indicated the effectiveness of our approach, and endorsed our hypothesis."
C08-1097,A Fully-Lexicalized Probabilistic Model for {J}apanese Zero Anaphora Resolution,2008,13,21,3,1,5794,ryohei sasano,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a probabilistic model for Japanese zero anaphora resolution. First, this model recognizes discourse entities and links all mentions to them. Zero pronouns are then detected by case structure analysis based on automatically constructed case frames. Their appropriate antecedents are selected from the entities with high salience scores, based on the case frames and several preferences on the relation between a zero pronoun and an antecedent. Case structure and zero anaphora relation are simultaneously determined based on probabilistic evaluation metrics."
C08-1132,{C}hinese Dependency Parsing with Large Scale Automatically Constructed Case Structures,2008,27,22,3,1,44132,kun yu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper proposes an approach using large scale case structures, which are automatically constructed from both a small tagged corpus and a large raw corpus, to improve Chinese dependency parsing. The case structure proposed in this paper has two characteristics: (1) it relaxes the predicate of a case structure to be all types of words which behaves as a head; (2) it is not categorized by semantic roles but marked by the neighboring modifiers attached to a head. Experimental results based on Penn Chinese Treebank show the proposed approach achieved 87.26% on unlabeled attachment score, which significantly outperformed the baseline parser without using case structures."
2008.amta-papers.15,Linguistically-motivated Tree-based Probabilistic Phrase Alignment,2008,17,5,2,1,283,toshiaki nakazawa,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In this paper, we propose a probabilistic phrase alignment model based on dependency trees. This model is linguistically-motivated, using syntactic information during alignment process. The main advantage of this model is that the linguistic difference between source and target languages is successfully absorbed. It is composed of two models: Model1 is using content word translation probability and function word translation probability; Model2 uses dependency relation probability which is defined for a pair of positional relations on dependency trees. Relation probability acts as tree-based phrase reordering model. Since this model is directed, we combine two alignment results from bi-directional training by symmetrization heuristics to get definitive alignment. We conduct experiments on a Japanese-English corpus, and achieve reasonably high quality of alignment compared with word-based alignment model."
P07-2035,Construction of Domain Dictionary for Fundamental Vocabulary,2007,6,6,2,1,26950,chikara hashimoto,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"For natural language understanding, it is essential to reveal semantic relations between words. To date, only the IS-A relation has been publicly available. Toward deeper natural language understanding, we semi-automatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words. This is the first Japanese domain resource that is fully available. Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain. As a task-based evaluation, we performed blog categorization. Also, we developed a technique for estimating the domain of unknown words."
N07-2051,A Three-Step Deterministic Parser for {C}hinese Dependency Parsing,2007,10,6,2,1,44132,kun yu,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"This paper presents a three-step dependency parser to parse Chinese deterministically. By dividing a sentence into several parts and parsing them separately, it aims to reduce the error propagation coming from the greedy characteristic of deterministic parsing. Experimental results showed that compared with the deterministic parser which parsed a sentence in sequence, the proposed parser achieved extremely significant improvement on dependency accuracy."
D07-1032,Probabilistic Coordination Disambiguation in a Fully-Lexicalized {J}apanese Parser,2007,13,6,2,1,3202,daisuke kawahara,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,This paper describes a probabilistic model for coordination disambiguation integrated into syntactic and case structure analysis. Our model probabilistically assesses the parallelism of a candidate coordinate structure using syntactic/semantic similarities and cooccurrence statistics. We integrate these probabilities into the framework of fully-lexicalized parsing based on largescale case frames. This approach simultaneously addresses two tasks of coordination disambiguation: the detection of coordinate conjunctions and the scope disambiguation of coordinate structures. Experimental results on web sentences indicate the effectiveness of our approach.
2007.mtsummit-papers.35,Development of a {J}apanese-{C}hinese machine translation system,2007,-1,-1,2,0,15925,hitoshi isahara,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.mtsummit-papers.45,Structural phrase alignment based on consistency criteria,2007,-1,-1,3,1,283,toshiaki nakazawa,Proceedings of Machine Translation Summit XI: Papers,0,None
W06-0123,{C}hinese Word Segmentation and Named Entity Recognition by Character Tagging,2006,10,2,2,1,44132,kun yu,Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper describes our word segmentation system and named entity recognition (NER) system for participating in the third SIGHAN Bakeoff. Both of them are based on character tagging, but use different tag sets and different features. Evaluation results show that our word segmentation system achieved 93.3% and 94.7% F-score in UPUC and MSRA open tests, and our NER system got 70.84% and 81.32% F-score in LDC and MSRA open tests."
P06-2097,Unsupervised Topic Identification by Integrating Linguistic and Visual Information Based on Hidden {M}arkov Models,2006,15,10,2,1,21565,tomohide shibata,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"This paper presents an unsupervised topic identification method integrating linguistic and visual information based on Hidden Markov Models (HMMs). We employ HMMs for topic identification, wherein a state corresponds to a topic and various features including linguistic, visual and audio information are observed. Our experiments on two kinds of cooking TV programs show the effectiveness of our proposed method."
N06-1023,A Fully-Lexicalized Probabilistic Model for {J}apanese Syntactic and Case Structure Analysis,2006,15,85,2,1,3202,daisuke kawahara,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We present an integrated probabilistic model for Japanese syntactic and case structure analysis. Syntactic and case structure are simultaneously analyzed based on wide-coverage case frames that are constructed from a huge raw corpus in an unsupervised manner. This model selects the syntactic and case structure that has the highest generative probability. We evaluate both syntactic structure and case structure. In particular, the experimental results for syntactic analysis on web sentences show that the proposed model significantly outperforms known syntactic analyzers."
kawahara-kurohashi-2006-case,Case Frame Compilation from the Web using High-Performance Computing,2006,9,84,2,1,3202,daisuke kawahara,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Case frames are important knowledge for a variety of NLP systems, especially when wide-coverage case frames are available. To acquire such large-scale case frames, it is necessary to automatically compile them from an enormous amount of corpus. In this paper, we consider the web as a corpus. We first build a huge text corpus from the web, and then construct case frames from the corpus. It is infeasible to do these processes by one CPU, and thus we employ a high-performance computing environment composed of 350 CPUs. The acquired corpus consists of 470M sentences, and the case frames compiled from them have 90,000 verb entries. The case frames contain most examples of usual use, and are ready to be applied to lots of NLP analyses and applications."
2006.iwslt-evaluation.9,Example-based machine translation based on deeper {NLP},2006,11,15,4,1,283,toshiaki nakazawa,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes our Kyoto-U system that attended the IWSLT06 Japanese-English machine translation task. Example-based machine translation is applied in this system to integrate our study on both structural NLP and machine translation.
I05-1017,{PP}-Attachment Disambiguation Boosted by a Gigantic Volume of Unambiguous Examples,2005,23,7,2,1,3202,daisuke kawahara,Second International Joint Conference on Natural Language Processing: Full Papers,0,We present a PP-attachment disambiguation method based on a gigantic volume of unambiguous examples extracted from raw corpus. The unambiguous examples are utilized to acquire precise lexical preferences for PP-attachment disambiguation. Attachment decisions are made by a machine learning method that optimizes the use of the lexical preferences. Our experiments indicate that the precise lexical preferences work effectively.
I05-1060,Automatic Acquisition of Basic Katakana Lexicon from a Given Corpus,2005,7,8,3,1,283,toshiaki nakazawa,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Katakana, Japanese phonogram mainly used for loan words, is a troublemaker in Japanese word segmentation. Since Katakana words are heavily domain-dependent and there are many Katakana neologisms, it is almost impossible to construct and maintain Katakana word dictionary by hand. This paper proposes an automatic segmentation method of Japanese Katakana compounds, which makes it possible to construct precise and concise Katakana word dictionary automatically, given only a medium or large size of Japanese corpus of some domain."
I05-1066,Automatic Slide Generation Based on Discourse Structure Analysis,2005,13,25,2,1,21565,tomohide shibata,Second International Joint Conference on Natural Language Processing: Full Papers,0,"In this paper, we describe a method of automatically generating summary slides from a text. The slides are generated by itemizing topic/non-topic parts that are extracted from the text based on syntactic/case analysis. The indentations of the items are controlled according to the discourse structure, which is detected by cue phrases, identification of word chain and similarity between two sentences. Our experiments demonstrates generated slides are far easier to read in comparison with original texts."
I05-1085,Lexical Choice via Topic Adaptation for Paraphrasing Written Language to Spoken Language,2005,13,5,2,1,26283,nobuhiro kaji,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Our research aims at developing a system that paraphrases written language text to spoken language style. In such a system, it is important to distinguish between appropriate and inappropriate words in an input text for spoken language. We call this task lexical choice for paraphrasing. In this paper, we describe a method of lexical choice that considers the topic. Basically, our method is based on the word probabilities in written and spoken language corpora. The novelty of our method is topic adaptation. In our framework, the corpora are classified into topic categories, and the probability is estimated using such corpora that have the same topic as input text. The result of evaluation showed the effectiveness of topic adaptation."
2005.mtsummit-papers.29,Probabilistic Model for Example-based Machine Translation,2005,-1,-1,2,1,165,eiji aramaki,Proceedings of Machine Translation Summit X: Papers,0,"Example-based machine translation (EBMT) systems, so far, rely on heuristic measures in retrieving translation examples. Such a heuristic measure costs time to adjust, and might make its algorithm unclear. This paper presents a probabilistic model for EBMT. Under the proposed model, the system searches the translation example combination which has the highest probability. The proposed model clearly formalizes EBMT process. In addition, the model can naturally incorporate the context similarity of translation examples. The experimental results demonstrate that the proposed model has a slightly better translation quality than state-of-the-art EBMT systems."
2005.iwslt-1.27,Example-based Machine Translation Pursuing Fully Structural {NLP},2005,7,6,1,1,297,sadao kurohashi,Proceedings of the Second International Workshop on Spoken Language Translation,0,We are conducting Example-Based Machine Translation research aiming at the improvement both of structural NLP and machine translation. This paper describes UTokyo system challenged IWSLT05 Japanese-English translation tasks.
N04-1031,Paraphrasing Predicates from Written Language to Spoken Language Using the Web,2004,19,12,3,1,26283,nobuhiro kaji,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,"There are a lot of differences between expressions used in written language and spoken language. It is one of the reasons why speech synthesis applications are prone to produce unnatural speech. This paper represents a method of paraphrasing unsuitable expressions for spoken language into suitable ones. Those two expressions can be distinguished based on the occurrence probability in written and spoken language corpora which are automatically collected from the Web. Experimental results indicated the effectiveness of our method. The precision of the collected corpora was 94%, and the accuracy of learning paraphrases was 76 %."
kawahara-etal-2004-toward,Toward Text Understanding: Integrating Relevance-tagged Corpus and Automatically Constructed Case Frames,2004,7,4,3,1,3202,daisuke kawahara,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper proposes a wide-range anaphora resolution system toward text understanding. This system resolves zero, direct and indirect anaphors in Japanese texts by integrating two sorts of linguistic resources: a hand-annotated corpus with various relations and automatically constructed case frames. The corpus has relevance tags which consist of predicate-argument relations, relations between nouns and coreferences, and is utilized for learning parameters of the system and testing it. The case frames are indispensable knowledge both for detecting zero/indirect anaphors and estimating appropriate antecedents. Our preliminary experiments showed promising results."
C04-1050,Improving {J}apanese Zero Pronoun Resolution by Global Word Sense Disambiguation,2004,10,5,2,1,3202,daisuke kawahara,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper proposes unsupervised word sense disambiguation based on automatically constructed case frames and its incorporation into our zero pronoun resolution system. The word sense disambiguation is applied to verbs and nouns. We consider that case frames define verb senses and semantic features in a thesaurus define noun senses, respectively, and perform sense disambiguation by selecting them based on case analysis. In addition, according to the one sense per discourse heuristic, the word sense disambiguation results are cached and applied globally to the subsequent words. We integrated this global word sense disambiguation into our zero pronoun resolution system, and conducted experiments of zero pronoun resolution on two different domain corpora. Both of the experimental results indicated the effectiveness of our approach."
C04-1174,Automatic Construction of Nominal Case Frames and its Application to Indirect Anaphora Resolution,2004,11,8,3,1,5794,ryohei sasano,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper proposes a method to automatically construct Japanese nominal case frames. The point of our method is the integrated use of a dictionary and example phrases from large corpora. To examine the practical usefulness of the constructed nominal case frames, we also built a system of indirect anaphora resolution based on the case frames. The constructed case frames were evaluated by hand, and were confirmed to be good quality. Experimental results of indirect anaphora resolution also indicated the effectiveness of our approach."
2004.iwslt-evaluation.15,Example-based machine translation using structural translation examples,2004,6,6,2,1,165,eiji aramaki,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper proposes an example-based machine translation system which handles structural translation examples. The structural translation examples have the potential advantage of high-usability. However, technologies which build such translation examples are still being developed. In such a situation, the comparison of the proposed system and the other approach systems is meaningful. This paper presents the system algorithm and its performance on the IWSLT04 Japanese-English unrestricted task."
W03-0312,Word Selection for {EBMT} based on Monolingual Similarity and Translation Confidence,2003,12,9,2,1,165,eiji aramaki,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,0,"We propose a method of constructing an example-based machine translation (EBMT) system that exploits a content-aligned bilingual corpus. First, the sentences and phrases in the corpus are aligned across the two languages, and the pairs with high translation confidence are selected and stored in the translation memory. Then, for a given input sentences, the system searches for fitting examples based on both the monolingual similarity and the translation confidence of the pair, and the obtained results are then combined to generate the translation. Our experiments on translation selection showed the accuracy of 85% demonstrating the basic feasibility of our approach."
P03-2027,Dialog Navigator : A Spoken Dialog {Q}-A System based on Large Text Knowledge Base,2003,7,7,2,1,41987,yoji kiyota,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"This paper describes a spoken dialog Q-A system as a substitution for call centers. The system is capable of making dialogs for both fixing speech recognition errors and for clarifying vague questions, based on only large text knowledge base. We introduce two measures to make dialogs for fixing recognition errors. An experimental evaluation shows the advantages of these measures."
P02-1028,Verb Paraphrase based on Case Frame Alignment,2002,5,34,3,1,26283,nobuhiro kaji,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a method of translating a predicate-argument structure of a verb into that of an equivalent verb, which is a core component of the dictionary-based paraphrasing. Our method grasps several usages of a headword and those of the def-heads as a form of their case frames and aligns those case frames, which means the acquisition of word sense disambiguation rules and the detection of the appropriate equivalent and case marker transformation."
kawahara-etal-2002-construction,Construction of a {J}apanese Relevance-tagged Corpus,2002,5,56,2,1,3202,daisuke kawahara,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes our corpus annotation project. The annotated corpus has relevance tags which consist of predicate-argument relations, relations between nouns, and coreferences. To construct this relevance-tagged corpus, we investigated a large corpus and established the specification of the annotation. This paper shows the specification and dicult tagging problems which have emerged through the annotation so far."
C02-1084,{``}Dialog Navigator{''}: A Question Answering System Based on Large Text Knowledge Base,2002,12,42,2,1,41987,yoji kiyota,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper describes a dialog based QA system, Dialog Navigator, which can answer questions based on large text knowledge base. In real world QA systems, vagueness of questions is a big problem. Our system can navigates users to the desired answers using the following methods: asking users back with dialog cards, and description extraction of each retrieved text. Another feature of the system is that it retrieves relevant texts precisely, using question types, synonymous expression dictionary, and modifier-head relations in Japanese sentences."
C02-1122,Fertilization of Case Frame Dictionary for Robust {J}apanese Case Analysis,2002,4,28,2,1,3202,daisuke kawahara,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"This paper proposes a method of fertilizing a Japanese case frame dictionary to handle complicated expressions: double nominative sentences, non-gapping relation of relative clauses, and case change. Our method is divided into two stages. In the first stage, we parse a large corpus and construct a Japanese case frame dictionary automatically from the parse results. In the second stage, we apply case analysis to the large corpus utilizing the constructed case frame dictionary, and upgrade the case frame dictionary by incorporating newly acquired information."
Y01-1017,Building domain-independent text generation system,2001,5,0,2,0,51027,xinyu deng,"Proceedings of the 15th Pacific Asia Conference on Language, Information and Computation",0,"This paper investigates an effective method of building domain-independent text generation system. In our English text generation system, Semantic Network is used as the internal Knowledge Representation. Nodes and links of the semantic network are classified according to word class and grammatical relations respectively. Generation results prove that the system works well and can generate coherent text flexibly."
S01-1009,{SENSEVAL}-2 {J}apanese Translation Task,2001,5,11,1,1,297,sadao kurohashi,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"This paper reports an overview of Senseval-2 Japanese translation task. In this task, word senses are defined according to translation distinction. A translation Memory (TM) was constructed, which contains, for each Japanese head word, a list of typical Japanese expressions and their English translations. For each target word instance, a TM record best approximating that usage had to be submitted. Alternatively, submission could take the form of actual target word translations. 9 systems from 7 organizations participated in the task."
H01-1043,{J}apanese Case Frame Construction by Coupling the Verb and its Closest Case Component,2001,6,27,2,1,3202,daisuke kawahara,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes a method to construct a case frame dictionary automatically from a raw corpus. The main problem is how to handle the diversity of verb usages. We collect predicate-argument examples, which are distinguished by the verb and its closest case component in order to deal with verb usages, from parsed results of a corpus. Since these couples multiply to millions of combinations, it is difficult to make a wide-coverage case frame dictionary from a small corpus like an analyzed corpus. We, however, use a raw corpus, so that this problem can be addressed. Furthermore, we cluster and merge predicate-argument examples which does not have different usages but belong to different case frames because of different closest case components. We also report on an experimental result of case structure analysis using the constructed case frame dictionary."
2001.mtsummit-papers.5,Finding translation correspondences from parallel parsed corpus for example-based translation,2001,9,14,2,1,165,eiji aramaki,Proceedings of Machine Translation Summit VIII,0,"This paper describes a system for finding phrasal translation correspondences from parallel parsed corpus that are collections paired English and Japanese sentences. First, the system finds phrasal correspondences by Japanese-English translation dictionary consultation. Then, the system finds correspondences in remaining phrases by using sentences dependency structures and the balance of all correspondences. The method is based on an assumption that in parallel corpus most fragments in a source sentence have corresponding fragments in a target sentence."
W00-1707,Discourse Structure Analysis for News Video,2000,11,0,3,0,44119,yasuhiko watanabe,Proceedings of the {COLING}-2000 Workshop on Semantic Annotation and Intelligent Content,0,"Various kinds of video recordings have discourse structures. Therefore, it is important to determine how video segments are combined and what kind of coherence relations they are connected with. In this paper, we propose a method for estimating the discourse structure of video news reports."
W00-1310,Nonlocal Language Modeling based on Context Co-occurrence Vectors,2000,9,1,1,1,297,sadao kurohashi,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"This paper presents a novel nonlocal language model which utilizes contextual information. A reduced vector space model calculated from co-occurrences of word pairs provides word co-occurrence vectors. The sum of word co-occurrence vectors represents the context of a document, and the cosine similarity between the context vector and the word co-occurrence vectors represents the long-distance lexical dependencies. Experiments on the Mainichi Newspaper corpus show significant improvement in perplexity (5.0% overall and 27.2% on target vocabulary)"
W00-1016,Dialogue Helpsystem based on Flexible Matching of User Query with Natural Language Knowledge Base,2000,6,17,1,1,297,sadao kurohashi,1st {SIG}dial Workshop on Discourse and Dialogue,0,"This paper describes a dialog help-system which advises users in using computer facilities and software applications provided by the Center for Information and Multimedia Studies, Kyoto University. The system employs a knowledge base written in natural language and retrieves a proper knowledge unit by flexible matching of user query with the knowledge base. The system is running since July 1999, received about 2,000 queries for the first seven months, and answered about 40% of them satisfactory."
sutcliffe-kurohashi-2000-parallel,A Parallel {E}nglish-{J}apanese Query Collection for the Evaluation of On-Line Help Systems,2000,3,5,2,0,20594,richard sutcliffe,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,An experiment concerning the creation of parallel evaluation data for information retrieval is presented. A set of English queries was gathered for the domain of wordprocessing using Lotus Ami Pro. A set of Japanese queries was then created from these. The answers to the queries were elicited from eight respondents comprising four native speakers of each language. We first describe how the queries were created and the answers elicited. We then present analyses of the responses in each language. The results show a lower level of agreeement between respondents than was expected. We discuss a refinement of the elicitation process which is designed to address this problem as well as measuring the integrity of individual respondents.
C00-2131,Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-based Translation,2000,12,64,2,0,52368,hideo watanabe,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"In this paper, we describe a system and methods for finding structural correspondences from the paired dependency structures of a source sentence and its translation in a target language. The system we have developed finds word correspondences first, then finds phrasal correspondences based on word correspondences. We have also developed a GUI system with which a user can check and correct the correspondences retrieved by the system. These structural correspondences will be used as raw translation patterns in a corpus-based translation system."
C00-1063,{J}apanese Case Structure Analysis,2000,4,0,3,1,3202,daisuke kawahara,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"In Japanese, case structure analysis is very imt)ortant to handle several t roublesome characteristics of Japanese snch as scrambling, onfission of ease components, mid disappearance of case markers. However, fi)r lack of a widecoverage ease frame dictionary, it has been difficult to perfornl case structure analysis accurat;ely. Although several methods to construct a ease fl'mne dictionary from analyzed corpora have been proposed, they cannot avoid data sparseness 1)rol)lem. This paper proposes an unsupervised method of constructing a case frame dictionary from an enormous raw corpus by using a robust and accurate parser. It also prorides a case structure analysis method based on the constructed dictionary. 1 I n t r o d u c t i o n Syntactic analysis, or parsing has been a main objective in Natural Language Processing. In case of Jat)anese , however, syntactic analysis cannot clarify relations between words ill sentences because of several t roublesome characteristics of Japanese such as scrambling, omission of case components, and disappearance of case markers. Therefore, in Japanese sentence analysis, case s tructure analysis is an important issue, and a case frame dictionary is necessary for the analysis. Some research institutes have constructed Japanese case frmne dictiouaries manually (Ikehara et al., 1997; Infbrmation-Technology Promotion Agency, Japan, 1987). However, it is quite expensive, or almost impossible to construct a wide-coverage ease fl'anm dictionary by hand. Others have tried to construct a case fl'mne dictionary automatical ly from analyzed corpora (Utsuro et al., 1998). However, existing syntactically analyzed corpora are too small to learn a dictionary, since case fl'ame iuformation consists of relations between nouns and verbs, which rnultiplies to millions of combinations. Based on such a consideration, we took the fbllowing unsupervised learning strategy to the .Japanese case structure analysis: 1. At first, a robust and accurate parser is developed, which does not utilize a case fl'mne dictionary, 2. a very large corI)us is parsed by the parser, 3. reliable noun-verb relations are extracted from the parse results, and a case frmne dict ionary is constructed from them, and 4. the dictionary is utilized for case structure analysis. 2 Characteristics of Japanese language and necessity of case s t r u c t u r e a n a l y s i s In Japanese, postposit ions function as case markers ( ( M s ) mid a verb is final in a sentence. The basic s tructure of a Japanese sentence is as fbllows: (1) k a t e 9a coat wo ki~'u. he nominative-CM coat accusative-CM wear (lie wears a coat) A clause modifier is left to the modified noun as follows: (2) k a t e 9 a k i t e i r u coat lie nom-CM wear coat (the coat he wears) The modified noun followed by a postposit ion then becomes a case component of a matrix verb. The typical s tructure of a Japanese complex sentence is as fbllows:"
P99-1062,Semantic Analysis of {J}apanese Noun Phrases - A New Approach to Dictionary-Based Understanding,1999,10,15,1,1,297,sadao kurohashi,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"This paper presents a new method of analyzing Japanese noun phrases of the form N1 no N2. The Japanese postposition no roughly corresponds to of, but it has much broader usage. The method exploits a definition of N2 in a dictionary. For example, rugby no coach can be interpreted as a person who teaches technique in rugby. We illustrate the effectiveness of the method by the analysis of 300 test noun phrases."
W98-0701,General Word Sense Disambiguation Method Based on a Full Sentential Context,1998,32,47,2,0,55195,jiri stetina,Usage of {W}ord{N}et in Natural Language Processing Systems,0,"This paper presents a new general supervised word sense disambiguation method based on a relatively small syntactically parsed and semantically tagged training corpus.The method exploits a full sentential context and all the explicit semantic relations in a sentence to identify the senses of all of that sentence's content words. It solves the sparse data problem of a small training corpus by substituting the words by their semantic classes.In spite of a very small training corpus, we report an overall accuracy of 80.3% (85.7, 63.9, 83.6 and 86.5%, for nouns, verbs, adjectives and adverbs, respectively), which exceeds the accuracy of a statistical sense-frequency based semantic tagging, the only really applicable general disambiguating technique. Because the method uses the sentential syntactic structure it is particularly suitable for integration with a probabilistic syntactic analyser."
W98-0605,Construction of {J}apanese Nominal Semantic Dictionary using {``}A {NO} {B}{''} Phrases in Corpora,1998,1,1,1,1,297,sadao kurohashi,The Computational Treatment of Nominals,0,None
1995.iwpt-1.17,Analyzing Coordinate Structures Including Punctuation in {E}nglish,1995,-1,-1,1,1,297,sadao kurohashi,Proceedings of the Fourth International Workshop on Parsing Technologies,0,We present a met hod of identifying coordinate structure scopes and determining usages of commas in sentences at the same time. All possible interpretations concerning comma usages and coordinate structure scopes are ranked by taking advantage of parallelism between conjoined phrases/clauses/sentences and calculating their similarity scores. We evaluated this method through experiments on held-out test sentences and obtained promising results: both the success ratio of interpreting commas and that of detecting CS scopes were about 80{\%}.
J94-4001,A Syntactic Analysis Method of Long {J}apanese Sentences Based on the Detection of Conjunctive Structures,1994,8,126,1,1,297,sadao kurohashi,Computational Linguistics,0,"This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures. Analysis of long sentences is one of the most difficult problems in natural language processing. The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences. Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts. Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure. This is realized using a dynamic programming technique. A long sentence can be reduced into a shorter form by recognizing conjunctive structures. Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules. A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components. Through our dependency analysis process, we can find the ellipses and recover the omitted components. We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method."
C94-2183,Automatic Detection of Discourse Structure by Checking Surface Information in Sentences,1994,10,81,1,1,297,sadao kurohashi,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we propose an automatic method for detecting discourse structure using a variety of clues existing in the surface information of sentences. We have considered three types of clue information: clue expressions, occurrence of identical/synonymous words/phrases, and similarity between two sentences. Experimental results have shown that, in the case of scientific and technical texts, considerable part of the discourse structure can be estimated by incorporating the three types of clue information, without performing sentence understanding processes which requires giving knowledge to computers."
1993.iwpt-1.11,Structural Disambiguation in {J}apanese by Evaluating Case Structures based on Examples in a Case Frame Dictionary,1993,-1,-1,1,1,297,sadao kurohashi,Proceedings of the Third International Workshop on Parsing Technologies,0,"A case structure expression is one of the most important forms to represent the \textit{meaning} of a sentence. Case structure analysis is usually performed by consulting \textit{case frame information} in verb dictionaries and by selecting a \textit{proper case frame} for an input sentence. However, this analysis is very difficult because of \textit{word sense ambiguity} and \textit{structural ambiguity}. A conventional method for solving these problems is to use the method of \textit{selectional restriction}, but this method has a drawback in the semantic marker (SM) system {--} the trade-off between descriptive power and construction cost. This paper describes a method of case structure analysis of Japanese sentences which overcomes the drawback in the SM system, concentrating on the structural disambiguation. This method selects a proper case frame for an input by the similarity measure between the input and typical example sentences of each case frame. When there are two or more possible readings for an input because of structural ambiguity, the best reading will be selected by evaluating case structures in each possible reading by the similarity measure with typical example sentences of case frames."
C92-1029,Dynamic Programming Method for Analyzing Conjunctive Structures in {J}apanese,1992,0,26,1,1,297,sadao kurohashi,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Parsing a long sentence is very difficult, since long sentences often have conjunctions which result in ambiguities. If the conjunctive structures existing in a long sentence can be analyzed correctly, ambiguities can be reduced greatly and a sentence can be parsed in a high successful rate. Since the prior part and the posterior part of a conjunctive structure have a similar structure very often, finding two similar series of words is an essential point in solving this problem. Similarities of all pairs of words are calculated and then the two series of words which have the greatest sum of similarities are found by a technique of dynamic programming. We deal with not only conjunctive noun phrases, but also conjunctive predicative clauses created by Renyoh chuushi-ho. We will illustrate the effectiveness of this method by the analysis of 180 long Japanese sentences."
A92-1037,A Method of Automatic Hypertext Construction from an Encyclopedic Dictionary of a Specific Field,1992,0,8,1,1,297,sadao kurohashi,Third Conference on Applied Natural Language Processing,0,None
