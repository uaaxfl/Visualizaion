2020.acl-main.609,J08-1001,0,0.0779341,"ibly due to the word “if” in the second sentence, which is one indicator for the After relation. When the syntactic information is encoded by the GAT encoder, the GAT-enc+dec model can learn the fined-grained dependency reduced by the word “if”, and thus is able to obtain the accurate relation of the two sentences (i.e., Conti.) 6 Related work Discourse parsing is one important topic in the NLP. There are several main types of discourse parsing tasks in the literature, including rhetorical structure theory (RST; MANN and Thompson, 1988) based parsing, centering theory (CT; Grosz et al., 1995; Barzilay and Lapata, 2008) based parsing and DRT based parsing in this study. Discourse Representation Theory (DRT) based parsing is a relatively classic, yet not fully researched semantic analysis task because of its complexity. Le and Zuidema (2012) present the first work of a data-driven DRT parser, using a graphbased representation of DRT structures. Recently, van Noord et al. (2018b) apply the idea of neural machine translation for graph-based DRT parsing, achieving impressing performance. These studies only focus on sentence-level DRT representations, as the complexity would increase much at the paragraph level."
2020.acl-main.609,D17-1209,0,0.0567964,"Missing"
2020.acl-main.609,D14-1082,0,0.00847887,"hmarks, AVGsent and AVGword denote the average number of sentences and words per document, respectively. shared linear transformation and LeakyReLU is a non-linearity activation function. 4.2 GAT for the Encoder On the encoder side, we equip the inputs with dependency syntax structures, which have been demonstrated helpful for closely-related tasks such as RST discourse parsing. A GAT module is used to represent the encoder output as mentioned in Section 4.1. We transform the document into a dependency graph represented by a undirected adjacent matrix using an off-the shelf dependency parser (Chen and Manning, 2014). The hidden states of each node is updated with a multi-layer GAT network on the adjacent matrix A: H g-enc = G ATenc (H enc ⊕ E syn , A; W ), (9) where E syn is the embedding outputs of the syntactic labels in the dependency tree. The learned representation H g-enc is used to substitute the original H enc for predictions. 4.3 GAT for the Decoder We further enhance the baseline model by exploiting the partial output after skeleton prediction step is finished. On one hand, the skeleton structures can guide for DRU parsing. On the other hand, the joint skeleton and DRU parsing can further help"
2020.acl-main.609,P14-1048,0,0.0291782,"lifies graphs into trees. There are two existing papers in this line. Liu et al. (2018) are the first to work on DRTS parsing, who propose an end-to-end sequence-to-sequence model for the task. Further, Liu et al. (2019) improve the model by suggesting several effective strategies including supervised attention, copying from alignments, and constraint-based inference. In this work, we improve DRTS parsing instead of Liu et al. (2019) with two types of structure information. Syntax information has been widely exploited for NLP tasks. Seminal work exploits discrete features designed by experts (Feng and Hirst, 2014; Heilman and Sagae, 2015). Recently, a range of neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information ("
2020.acl-main.609,J95-2003,0,0.539379,"sleading may be possibly due to the word “if” in the second sentence, which is one indicator for the After relation. When the syntactic information is encoded by the GAT encoder, the GAT-enc+dec model can learn the fined-grained dependency reduced by the word “if”, and thus is able to obtain the accurate relation of the two sentences (i.e., Conti.) 6 Related work Discourse parsing is one important topic in the NLP. There are several main types of discourse parsing tasks in the literature, including rhetorical structure theory (RST; MANN and Thompson, 1988) based parsing, centering theory (CT; Grosz et al., 1995; Barzilay and Lapata, 2008) based parsing and DRT based parsing in this study. Discourse Representation Theory (DRT) based parsing is a relatively classic, yet not fully researched semantic analysis task because of its complexity. Le and Zuidema (2012) present the first work of a data-driven DRT parser, using a graphbased representation of DRT structures. Recently, van Noord et al. (2018b) apply the idea of neural machine translation for graph-based DRT parsing, achieving impressing performance. These studies only focus on sentence-level DRT representations, as the complexity would increase m"
2020.acl-main.609,D19-1549,0,0.0816607,"2018; Li et al., 2015). For decoding, the skeleton structure of DRTS can be also beneficial for our task. As a two-phase decoding strategy is exploited, the skeleton tree from the first phase could be helpful for DRU parsing of the second phase. We propose to improve DRTS parsing by making use of the above structure information, modeling dependency-based syntax of the input sentences as well as the skeleton structure to enhance the baseline model of Liu et al. (2019) using Graph Attention Network (GAT) (Veliˇckovi´c et al., 2018), which has been demonstrated effective for tree/graph encoding (Huang and Carley, 2019; Linmei et al., 2019). In particular, we first derive dependency tree structures for each sentence in a paragraph from the Stanford Parser, and then encode them directly via one GAT module, which are fed as inputs for decoding. Second, after the first-state skeleton parsing is finished, we encode the skeleton structures by another GAT module, feeding the outputs for DRU parsing. Following Liu et al. (2019), we conduct experiments on the Groningen Meaning Bank (GMB) dataset. Results show that structural information is highly useful for our task, bring a significantly better performance over th"
2020.acl-main.609,N19-1075,0,0.0660953,"Missing"
2020.acl-main.609,C12-1094,0,0.240065,"rd “if”, and thus is able to obtain the accurate relation of the two sentences (i.e., Conti.) 6 Related work Discourse parsing is one important topic in the NLP. There are several main types of discourse parsing tasks in the literature, including rhetorical structure theory (RST; MANN and Thompson, 1988) based parsing, centering theory (CT; Grosz et al., 1995; Barzilay and Lapata, 2008) based parsing and DRT based parsing in this study. Discourse Representation Theory (DRT) based parsing is a relatively classic, yet not fully researched semantic analysis task because of its complexity. Le and Zuidema (2012) present the first work of a data-driven DRT parser, using a graphbased representation of DRT structures. Recently, van Noord et al. (2018b) apply the idea of neural machine translation for graph-based DRT parsing, achieving impressing performance. These studies only focus on sentence-level DRT representations, as the complexity would increase much at the paragraph level. In contrast, we investigate the paragraph level DRT parsing. DRTS parsing simplifies graphs into trees. There are two existing papers in this line. Liu et al. (2018) are the first to work on DRTS parsing, who propose an end-t"
2020.acl-main.609,P15-1107,0,0.0646558,"Missing"
2020.acl-main.609,D18-1262,0,0.052862,"Missing"
2020.acl-main.609,D19-1488,0,0.15692,"For decoding, the skeleton structure of DRTS can be also beneficial for our task. As a two-phase decoding strategy is exploited, the skeleton tree from the first phase could be helpful for DRU parsing of the second phase. We propose to improve DRTS parsing by making use of the above structure information, modeling dependency-based syntax of the input sentences as well as the skeleton structure to enhance the baseline model of Liu et al. (2019) using Graph Attention Network (GAT) (Veliˇckovi´c et al., 2018), which has been demonstrated effective for tree/graph encoding (Huang and Carley, 2019; Linmei et al., 2019). In particular, we first derive dependency tree structures for each sentence in a paragraph from the Stanford Parser, and then encode them directly via one GAT module, which are fed as inputs for decoding. Second, after the first-state skeleton parsing is finished, we encode the skeleton structures by another GAT module, feeding the outputs for DRU parsing. Following Liu et al. (2019), we conduct experiments on the Groningen Meaning Bank (GMB) dataset. Results show that structural information is highly useful for our task, bring a significantly better performance over the baseline. In particu"
2020.acl-main.609,P18-1040,1,0.887824,"TS for a clause in a document: “The letterx4 warns Jewish womenx16 that they will suffer if they date Arab men. ” p4 Introduction Discourse representation tree structure (DRTS) is a form of discourse structure based on Discourse Representation Theory of Kamp and Reyle (1993), a popular theory of meaning representation (Kamp, 1981; Asher, 1993; Asher and Lascarides, 2003). It is designed to account for a variety of linguistic phenomena, including the interpretation of pronouns and temporal expressions within and across sentences. Correspondingly, as one type of discourse parsing, DRTS parsing (Liu et al., 2018) can be helpful for paragraph or document-level text understanding by converting DRS to tree-style DRTS. (Liu et al., 2019). Figure 1 shows an example of DRTS, where the leaf nodes are discourse representation units (DRUs), upon which a discourse tree structure built. In particular, a DRU consists of several individual tuples, where each tuple denotes a relation inside the DRU. For example, there is a relationship “That” between the specific entity x16 and a proposition p4 . The relationships between the DRUs are organized by a tree skeleton, which includes three types of nodes: the S(DRS) nod"
2020.acl-main.609,P19-1629,1,0.139836,"uction Discourse representation tree structure (DRTS) is a form of discourse structure based on Discourse Representation Theory of Kamp and Reyle (1993), a popular theory of meaning representation (Kamp, 1981; Asher, 1993; Asher and Lascarides, 2003). It is designed to account for a variety of linguistic phenomena, including the interpretation of pronouns and temporal expressions within and across sentences. Correspondingly, as one type of discourse parsing, DRTS parsing (Liu et al., 2018) can be helpful for paragraph or document-level text understanding by converting DRS to tree-style DRTS. (Liu et al., 2019). Figure 1 shows an example of DRTS, where the leaf nodes are discourse representation units (DRUs), upon which a discourse tree structure built. In particular, a DRU consists of several individual tuples, where each tuple denotes a relation inside the DRU. For example, there is a relationship “That” between the specific entity x16 and a proposition p4 . The relationships between the DRUs are organized by a tree skeleton, which includes three types of nodes: the S(DRS) nodes to introduce DRU, the relation nodes for inter-DRU relationship, and the variable nodes, which are used to define S(DRS)"
2020.acl-main.609,L18-1267,0,0.0878823,"Missing"
2020.acl-main.609,Q18-1043,0,0.133286,"Missing"
2020.acl-main.609,P02-1040,0,0.112337,"o obtain the F1-scores of our results compared with the gold-standard clause form. Note that C OUNTER is computationally expensive, requiring more than 50 hours for the entire test dataset by using more than 100 threads. To facilitate development and analysis experiments, we suggest three alternatives for evaluation particularly for development experiments: (10) where E skt is the embedding outputs of the node labels in the generated skeleton tree, and the global skeleton-aware representation H g-skt is used instead of the original H skt for future predictions. 6822 (1) BLEU: a standard BLEU (Papineni et al., 2002) value is adopted as the metric to evaluate the resulting node sequence against the gold-standard output, since we model the task as a sequence-to-sequence task. (2) Skeleton: The bracket scoring method of constituent parsing is exploited to evaluate the skeleton performance, by regarding terminal DRU nodes as words in comparison with a constituent tree.3 3 https://nlp.cs.nyu.edu/evalb/ BLEU 50 Final -syntax -skelt 50.04 49.64 BLEU 50 Baseline +GAT-encoder +GAT-decoder +GAT-enc+dec 49 49.73 Model Head=1 Head=2 Head=3 Head=4 Head=5 50.04 49.64 48 49.11 49 (a) structure labels 47 46.83 Model lay"
2020.acl-main.609,P18-1150,1,0.789795,"de syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent treestructural information for DRTS parsing. 7 Conclusion We investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements. In particular, we use GAT for representing syntax in encoding, and representing a structural backbone for decoding. Experiments on the standard GMB dataset show that our method is"
2020.acl-main.609,P15-1150,0,0.150094,"Missing"
2020.acl-main.609,P19-1342,1,0.848305,"e been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent treestructural information for DRTS parsing. 7 Conclusion We investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements. In particular, we use GAT for representing syntax in encoding, and representing a structural backbone for decoding. Experiments on the standard GMB dataset show"
2020.acl-main.609,P18-1030,1,0.854939,"f neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent treestructural information for DRTS parsing. 7 Conclusion We investigated the representation of structural information for discourse representation tree structure parsing, showing that a graph neural network can bring significant improvements. In particular, we use GAT for representing syntax in encoding, and representing a structural backbone for decoding. Experiments on the st"
2020.acl-main.609,P18-1070,0,0.0273566,"ve strategies including supervised attention, copying from alignments, and constraint-based inference. In this work, we improve DRTS parsing instead of Liu et al. (2019) with two types of structure information. Syntax information has been widely exploited for NLP tasks. Seminal work exploits discrete features designed by experts (Feng and Hirst, 2014; Heilman and Sagae, 2015). Recently, a range of neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we explo"
2020.acl-main.609,C18-1047,1,0.935855,"oducing, as illustrated by Figure 1. Although highly effective, the above model ig6818 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6818–6828 c July 5 - 10, 2020. 2020 Association for Computational Linguistics nores some useful structure information in both the encoder and the decoder, which can be potentially useful for our task. Specifically, for encoding, syntax-based tree structure information has been demonstrated effective for a number of NLP tasks (Kasai et al., 2019; Li et al., 2018), including several other types of discourse parsing (Yu et al., 2018; Li et al., 2015). For decoding, the skeleton structure of DRTS can be also beneficial for our task. As a two-phase decoding strategy is exploited, the skeleton tree from the first phase could be helpful for DRU parsing of the second phase. We propose to improve DRTS parsing by making use of the above structure information, modeling dependency-based syntax of the input sentences as well as the skeleton structure to enhance the baseline model of Liu et al. (2019) using Graph Attention Network (GAT) (Veliˇckovi´c et al., 2018), which has been demonstrated effective for tree/graph encoding (Huan"
2020.acl-main.609,P19-1457,1,0.82088,"uding supervised attention, copying from alignments, and constraint-based inference. In this work, we improve DRTS parsing instead of Liu et al. (2019) with two types of structure information. Syntax information has been widely exploited for NLP tasks. Seminal work exploits discrete features designed by experts (Feng and Hirst, 2014; Heilman and Sagae, 2015). Recently, a range of neural modules have been proposed to encode syntax, such as Tree-LSTM (Tai et al., 2015; Zhu et al.; Teng and Zhang, 2016), Tree-CNN (Roy et al., 2020) and the recently proposed implicit approaches (Yin et al., 2018; Zhang et al., 2019). Syntax has been demonstrated effective for RST based discourse parsing as well (Yu et al., 2018). Our work is to build a syntax tree-aware model and we are the first to use syntax for DRT based discourse parsing. GNN has received increasing interests for its strong capability of encoding structural information (Kipf and Welling, 2016; Bastings et al., 2017; Zhang et al., 2018; Zhang and Zhang, 2019; Song et al., 2018). GAT is one representative model, which demonstrates success in a number of NLP tasks (Huang and Carley, 2019; Linmei et al., 2019). In this work, we exploit GAT to represent t"
2020.acl-main.627,C10-1081,0,\N,Missing
2020.acl-main.627,D07-1002,0,\N,Missing
2020.acl-main.627,kingsbury-palmer-2002-treebank,0,\N,Missing
2020.acl-main.627,W11-1608,0,\N,Missing
2020.acl-main.627,N12-1052,0,\N,Missing
2020.acl-main.627,H01-1035,0,\N,Missing
2020.acl-main.627,W09-1206,0,\N,Missing
2020.acl-main.627,P11-2052,0,\N,Missing
2020.acl-main.627,petrov-etal-2012-universal,0,\N,Missing
2020.acl-main.627,J05-1004,0,\N,Missing
2020.acl-main.627,P15-2111,0,\N,Missing
2020.acl-main.627,D15-1039,0,\N,Missing
2020.acl-main.627,P15-1039,0,\N,Missing
2020.acl-main.627,P15-2115,0,\N,Missing
2020.acl-main.627,W15-1824,0,\N,Missing
2020.acl-main.627,P11-2051,0,\N,Missing
2020.acl-main.627,P16-4001,0,\N,Missing
2020.acl-main.627,P17-1044,0,\N,Missing
2020.acl-main.627,P18-1192,0,\N,Missing
2020.acl-main.627,C18-1324,0,\N,Missing
2020.acl-main.627,D18-1039,0,\N,Missing
2020.acl-main.627,D18-1262,0,\N,Missing
2020.acl-main.627,D18-1269,0,\N,Missing
2020.acl-main.627,D18-1498,0,\N,Missing
2020.acl-main.627,W19-0417,0,\N,Missing
2020.acl-main.627,P19-1299,0,\N,Missing
2020.acl-main.627,N19-1423,0,\N,Missing
2020.acl-main.627,Q18-1039,0,\N,Missing
2020.acl-main.627,D19-1575,0,\N,Missing
2020.acl-main.627,D19-1092,1,\N,Missing
2020.acl-main.627,K19-1029,0,\N,Missing
2020.acl-main.627,W14-1614,0,\N,Missing
2020.acl-main.627,N13-1073,0,\N,Missing
2020.coling-main.263,W99-0611,0,0.451022,"ord Distance The distance between the two separation words should be an important factor in coreference recognition. Intuitively, as the distance increases, the difficulty should be also increased greatly. Here we conduct analysis to verify this intuition. Figure 5(b) shows the comparison results, which is consistent with our supposition. In addition, we can see that our final end-to-end model behaves much better, with relatively smaller decreases as the distance increases. 5 Related Work Coreference recognition has been investigated extensively in NLP for decades (McCarthy and Lehnert, 1995; Cardie and Wagstaff, 1999; Ng and Cardie, 2002; Elango, 2005). Lexical fusion can be regarded as one kind of coreference, however, it has received little attention in the NLP community. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There"
2020.coling-main.263,P18-1045,0,0.0161733,"hat our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarization, machine translation and etc., which can hinder the overall understanding and lead to degraded performance, the new task can offer informative knowledge to these t"
2020.coling-main.263,P16-1061,0,0.0292208,"which is consistent with our supposition. In addition, we can see that our final end-to-end model behaves much better, with relatively smaller decreases as the distance increases. 5 Related Work Coreference recognition has been investigated extensively in NLP for decades (McCarthy and Lehnert, 1995; Cardie and Wagstaff, 1999; Ng and Cardie, 2002; Elango, 2005). Lexical fusion can be regarded as one kind of coreference, however, it has received little attention in the NLP community. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable m"
2020.coling-main.263,N19-1423,0,0.619902,"h is referred to as mention detection. Second, coreference recognition is performed over the detected mentions, linking each character in the fusion words to their coreferences, respectively. By the second step, full lexical fusion coreferences are also recognized concurrently. The two steps can be conducted jointly in a single end-to-end model (Lee et al., 2017), which helps avoid the error propagation problem, and meanwhile, enable the two subtasks with full interaction. In this paper, we present a competitive end-to-end model for lexical fusion recognition. Contextual BERT representations (Devlin et al., 2019) are adopted as encoder inputs as they have achieved great success in a number of NLP tasks (Tian et al., 2019; Zhou et al., 2019; Xu et al., 2019). For mention detection, a CRF decoder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing"
2020.coling-main.263,D19-1033,0,0.221852,"oder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words. HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007; Gu et al., 2018; Ding et al., 2019; Li et al., 2019). Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder. Finally, we manually construct a high-quality dataset to evaluate our models. The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances. To train our proposed models, we construct a pseudo dataset automatically from the web resource. Experimental results show that the auto-constructed training dataset is highly effective for our"
2020.coling-main.263,P19-1064,0,0.0229756,"wledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarizati"
2020.coling-main.263,D18-1493,0,0.175235,"ection, a CRF decoder (Huang et al., 2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words. HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007; Gu et al., 2018; Ding et al., 2019; Li et al., 2019). Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder. Finally, we manually construct a high-quality dataset to evaluate our models. The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances. To train our proposed models, we construct a pseudo dataset automatically from the web resource. Experimental results show that the auto-constructed training dataset is highl"
2020.coling-main.263,D19-1588,0,0.0194355,"et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in"
2020.coling-main.263,P18-2063,0,0.0410707,"Missing"
2020.coling-main.263,D17-1018,0,0.395671,"urther enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such a"
2020.coling-main.263,P08-1049,0,0.0342123,"↔下调 (reduce)” and “息↔利率 (interest rate)”. Lexical fusion is used frequently in the Chinese language (Chen and Ren, 2020). Moreover, the fusion words are usually rarer words compared with their separation words coreferred, which are more difficult to be handled by NLP models (Zhang and Yang, 2018; Gui et al., 2019). Luckily, the meaning of the fusion words can be derived from that of the separation words. Thus recognizing the lexical fusion would be beneficial for downstream paragraph (or document)-level NLP applications such as machine translation, information extraction, summarization, etc. (Li and Yarowsky, 2008; Ferreira et al., 2013; Kundu et al., 2018). For example, for deep semantic parsing or translation, the fusion words “受访” (UNK) can be substituted directly by the separation words “接受” (accept) and “访问” (interview), as the same fusion words are rarely occurred in other paragraphs. The recognition of lexical fusion can be accomplished by two subtasks. Given one paragraph, the fusion words, as well as the separation words should be detected as the first step, which is referred to as mention detection. Second, coreference recognition is performed over the detected mentions, linking each characte"
2020.coling-main.263,P19-1430,0,0.0822444,"2015) is exploited to detect all mention words, including both the fusion and the separation words. Further, we use a BiAffine decoder for coreference recognition (Zhang et al., 2017; Bekoulis et al., 2018), determining a given mention pair either to be a coreference or not. Since our task is semantic oriented, we use the sememe knowledge provided in HowNet (Dong and Dong, 2003) to help capturing the semantic similarity between the characters and the separation words. HowNet has achieved success in many Chinese NLP tasks in recent years (Duan et al., 2007; Gu et al., 2018; Ding et al., 2019; Li et al., 2019). Both Chinese characters and words are defined by senses of sememe graphs in it, and we exploit graph attention networks (GAT) (Velickovic et al., 2018) to model the sememe graphs to enhance our encoder. Finally, we manually construct a high-quality dataset to evaluate our models. The dataset consists of 7,271 cases of the lexical fusion, which are all exploited as the test instances. To train our proposed models, we construct a pseudo dataset automatically from the web resource. Experimental results show that the auto-constructed training dataset is highly effective for our task. The end-to-"
2020.coling-main.263,W19-1908,0,0.0186543,"and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in Chinese and then pr"
2020.coling-main.263,P19-1441,0,0.0245254,"s abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extractio"
2020.coling-main.263,D15-1166,0,0.0603718,"The second part is obtained straightforwardly by the embedding of the position offset of the sense’s source word. The position offset is denoted by [s, e], where s and e indicate the relative position of the start and end characters of the source word to the current character, which has been illustrated in Figure 3. We use the position offset as a single unit for embedding. Following, we concatenate the two parts, resulting in the sense representation: Pm hsmi hsn = i=1 ⊕ e[s,t] , (10) M where ⊕ denotes vector concatenation. Finally, we aggregate all sense representations by global attention (Luong et al., 2015) with the guide of the BERT outputs to obtain character-level representations. Let {sn1 , · · · , snN } denote the set of sense representations for one character ci , the sememe-enhanced representation for character ci can be computed by:  exp tanh(v[hi ⊕ hsnj ]) aj = PN , exp tanh(v[h ⊕ h ]) i sn k k=1 (11) N X hsem = aj · hsnj , i j=1 sem where v is a model parameter for attention calculation, and hsem 1 · · · hn are the desired outputs which is used instead of the BERT outputs h1 · · · hn for decoder. 4 Experiments 4.1 Dataset Test Data We build a lexical fusion dataset manually in this w"
2020.coling-main.263,P16-1101,0,0.0944677,"Missing"
2020.coling-main.263,P09-1113,0,0.0143127,"two words in the same paragraph contain these characters each, or if they appear in the dictionary definition of the characters, then we obtain one context-independent triple. Finally, we collect 1,608 well-formed triples of lexical fusions and treat them as seeds to construct pseudo training instances. Note that the fusion words of these triples are currently acceptable and widely used by users, such as “停 车 (parking vehicles)↔停放 (parking)/车辆 (vehicle)”. Then, we search for the paragraphs containing all three words of a certain triple, regarding them as valid cases of Chinese lexical fusion (Mintz et al., 2009). Finally, we obtain 11,034 paragraphs, which are divided into training and development sections for model parameter learning and hyper-parameter tuning, respectively. Table 2 summarizes the overall data statistics of the training, development and testing sections, including the number of cases (lexical fusion), the averaged paragraph length (by character count) and the number of unique triples, respectively. Training Development Testing #Case #U.Triple #A.Length #N.Case #N.U.Triple 7,400 2,213 7,271 1,531 1,001 1,661 106 105 91 94 3959 77 957 Table 2: Data statistics of our corpus, where A. ,"
2020.coling-main.263,P98-2143,0,0.0204688,"then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-level tasks such as reading comprehension, summarization, machine translation and etc., which can hinder the overall understanding and lead to degraded pe"
2020.coling-main.263,P02-1014,0,0.0884658,"the encoder, and is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotate a benchmark dataset for the task and then conduct experiments on it. Results demonstrate that our final model is effective and competitive for the task. Detailed analysis is offered for comprehensively understanding the new task and our proposed model. 1 Introduction Coreference is one important topic in linguistics (Gordon and Hendrick, 1998; Pinillos, 2011), and coreference recognition has been extensively researched in the natural language processing (NLP) community (Ng and Cardie, 2002; Lee et al., 2017; Qi et al., 2012; Fei et al., 2019). There are different kinds of coreferences, such as pronoun anaphora and abbreviation (Mitkov, 1998; Mitkov, 1999; Mu˜noz and Montoyo, 2002; Choubey and Huang, 2018). Here we examine the phenomenon of Chinese lexical fusion, also called separable coreference (Chen and Ren, 2020), where two closely related words in a paragraph are united by a fusion form of the same meaning, and the fusion word can be seen as the coreference of the two separation words. Since the fusion words are always out-of-vocabulary (OOV) words in downstream paragraph-"
2020.coling-main.263,P17-1187,0,0.0186764,"should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in Chinese and then presented an end-to-end model for the new task. BERT representation was exploited as the basic input for the models, and the model is further enhanced with the sememe knowledge from HowNet by graph attention networ"
2020.coling-main.263,W12-4304,0,0.073625,"Missing"
2020.coling-main.263,P19-1571,0,0.0145635,"r free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 2019), which inspires our work by using it as basic inputs aiming for competitive performance. The sense and sememe information has been demonstrated effective for several semantic-oriented NLP tasks (Niu et al., 2017; Gu et al., 2018; Zeng et al., 2018; Ding et al., 2019; Qi et al., 2019). HowNet offers a large knowledge base of sememe-based (Dong and Dong, 2003), which has been adopted for sememe extraction. We encode the sememes by the form of a graph naturally to enhance our task encoder. 2943 6 Conclusion In this work, we introduced the task of lexical fusion recognition in Chinese and then presented an end-to-end model for the new task. BERT representation was exploited as the basic input for the models, and the model is further enhanced with the sememe knowledge from HowNet by graph attention networks. We manually annotated a benchmark dataset for the task, which was use"
2020.coling-main.263,P17-1114,0,0.0238973,"our supposition. In addition, we can see that our final end-to-end model behaves much better, with relatively smaller decreases as the distance increases. 5 Related Work Coreference recognition has been investigated extensively in NLP for decades (McCarthy and Lehnert, 1995; Cardie and Wagstaff, 1999; Ng and Cardie, 2002; Elango, 2005). Lexical fusion can be regarded as one kind of coreference, however, it has received little attention in the NLP community. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is"
2020.coling-main.263,N19-1242,0,0.0552695,"Missing"
2020.coling-main.263,L18-1325,0,0.0366173,"mmunity. Our proposed models are inspired by the work of neural coreference resolution (Fern´andez-Gallego et al., 2016; Clark and Manning, 2016; Xu et al., 2017; Lee et al., 2017; Zhang et al., 2018). We adapt these models by considering task-specific features of Chinese lexical fusion, for example, enhancing the encoder with a GAT module for structural sememe information. Another most closely-related topic is abbreviation (Zhong, 1985). There have been several studies on abbreviation prediction, recovery and dictionary construction (Sun et al., 2008; Li and Yarowsky, 2008; Liu et al., 2009; Zhang and Sun, 2018). Lexical fusion is different from abbreviation in many points. For example, abbreviation always refers to one inseparable mention, which is not necessary for lexical fusion. Besides, lexical fusion should abide by certain word construction rules, while abbreviation is for free. BERT and its variations have achieved the leading performances for GLUE benchmark datasets (Devlin et al., 2019; Liu et al., 2019a; Liu et al., 2019b). For the close tasks such as coreference resolution and relation extraction, BERT representations have also shown competitive results (Joshi et al., 2019; Lin et al., 20"
2020.coling-main.263,P18-1144,0,0.0433728,"Missing"
2020.coling-main.263,E17-1063,0,0.027293,"Missing"
2020.coling-main.263,P19-1328,0,0.0563967,"Missing"
2020.coling-main.263,C98-2138,0,\N,Missing
2020.coling-main.263,E12-2021,0,\N,Missing
2021.acl-long.372,W18-2501,0,0.0220696,"Missing"
2021.acl-long.372,P19-1024,0,0.127751,"are entity fragments. Following, fragment relations are predicted by another classifier to determine whether two specific fragments involve a certain relation. We define two relations for our goal: Overlapping or Succession, which are used for overlapped and discontinuous entities, respectively. In essence, the joint model can be regarded as one kind of relation extraction models, which is adapted for our goal. To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019). We evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Str"
2021.acl-long.372,H05-1048,0,0.127879,"enhanced by dependencyguided graph convolutional network (GCN) (Zhang et al., 2018; Guo et al., 2019). To our knowledge, syntax information is commonly neglected in most previous work for overlapped or discontinuous NER, except Finkel and Manning (2009). The work employs a constituency parser to transform a sentence into a nested entity tree, and syntax information is used naturally to facilitate NER. By contrast, syntax information has been utilized in some studies for traditional regular NER. Under the traditional statistical setting, syntax information is used by manually-crafted features (Hacioglu et al., 2005; Ling and Weld, 2012) or auxiliary tasks (Florian et al., 2006) for NER. Recently, Jie et al. (2017) build a semi-CRF model based on dependency information to optimize the research space of NER recognition. Jie and Lu (2019) stack the dependency-guided graph convolutional network (Zhang et al., 2018; Guo et al., 2019) on top of the BiLSTM layer. These studies have demonstrated that syntax information could be an effective feature source for NER. 3 Method The key idea of our model includes two mechanisms. First, our model enumerates all possible text spans in a sentence and then exploits a mul"
2021.acl-long.372,N16-1030,0,0.0422792,"dered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018). Other appr"
2021.acl-long.372,2020.acl-main.519,0,0.0666574,"Missing"
2021.acl-long.372,P19-1511,0,0.0365435,"Missing"
2021.acl-long.372,P19-1016,0,0.064587,"be overlapping or succession. In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities. As a whole, our model can be regarded as a relation extraction paradigm essentially. Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER. 1 Introduction Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019). Traditionally, NER is presented as a sequence labeling problem and widely solved by conditional random field (CRF) based models (Lafferty et al., 2001). However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral lea"
2021.acl-long.372,D16-1008,0,0.31922,"ENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER. 1 Introduction Named entity recognition (NER) (Sang and De Meulder, 2003) is one fundamental task for natural language processing (NLP), due to its wide application in information extraction and data mining (Lin et al., 2019b; Cao et al., 2019). Traditionally, NER is presented as a sequence labeling problem and widely solved by conditional random field (CRF) based models (Lafferty et al., 2001). However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019"
2021.acl-long.372,D17-1276,0,0.14249,"However, this framework is difficult to handle overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER"
2021.acl-long.372,D14-1162,0,0.090372,"ethods without contextualized representations. 5.4 Result Analysis based on Entity Types Comparing with BiLSTM-CRF To show the necessity of building one model to recognize regular, overlapped and discontinuous entities simultaneously, we analyze the predicted entities in the CLEF-Dis dataset and classify them based on their types, as shown in Figure 4. In addition, we compare our model with BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018), to show our model does not influence the performance of regular NER significantly. For a fair comparison, we replace BERT with Glove (Pennington et al., 2014) and keep the setting of our model the same with the setting of the BiLSTM-CRF model used in previous work (Yang et al., 2018). As seen, if only considering regular entities, the 8 Many discontinuous entities are also overlapped, but we do not count them as overlapped entities in this figure. r+o r+d r+o+d Figure 5: Result analysis based on entity types on the CLEF-Dis dataset, comparing with Dai et al. (2020) (blue). BiLSTM-CRF model can achieve a better performance compared with our model, especially the precision value is much higher. One likely reason might be that the BiLSTM-CRF model is"
2021.acl-long.372,P17-1001,0,0.0576066,"Missing"
2021.acl-long.372,N18-1202,0,0.00874374,"odel enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017"
2021.acl-long.372,P11-1037,0,0.0585102,"that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Strakov´a et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finke"
2021.acl-long.372,S14-2007,0,0.0634049,"Missing"
2021.acl-long.372,W03-0419,0,0.258164,"Missing"
2021.acl-long.372,P19-1527,0,0.034235,"Missing"
2021.acl-long.372,P19-1138,0,0.0386649,"Missing"
2021.acl-long.372,C18-1327,0,0.0176749,"u et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to recognize overlapped entities and lead to a number of extensions (Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018). Moreover, recurrent neural networks (RNNs) are also used for overlapped NER (Ju et al., 2018; Wang et al., 2018). Other approaches include multi-grained detection"
2021.acl-long.372,P16-1040,1,0.825641,"ER could be conducted universally without any assumption to exclude overlapped or discontinuous entities, which could be more practical in real applications. The hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020) are flexible to be adapted for different tasks, achieving great successes for overlapped or discontinuous NER. However, these models need to manually define graph nodes, edges and transition actions. Moreover, these models build graphs or generate transitions along the words in the sentences gradually, which may lead to error propagation (Zhang et al., 2016). In contrast, the spanbased scheme might be a good alternative, which is much simpler including only span-level classification. Thus, it needs less manual intervention and meanwhile span-level classification can be fully parallelized without error propagation. Recently, Luan et al. (2019) utilized the span-based model for information extraction effectively. In this work, we propose a novel span-based joint model to recognize overlapped and discon4814 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural L"
2021.acl-long.372,D18-1244,0,0.360802,"t al., 2019), classifying whether they are entity fragments. Following, fragment relations are predicted by another classifier to determine whether two specific fragments involve a certain relation. We define two relations for our goal: Overlapping or Succession, which are used for overlapped and discontinuous entities, respectively. In essence, the joint model can be regarded as one kind of relation extraction models, which is adapted for our goal. To enhance our model, we utilize the syntax information as well by using a dependency-guided graph convolutional network (Kipf and Welling, 2017; Zhang et al., 2018; Jie and Lu, 2019; Guo et al., 2019). We evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan e"
2021.acl-long.372,D19-1034,0,0.0371452,"Missing"
2021.acl-long.372,P15-1117,0,0.0176781,"2019; Strakov´a et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou et al., 2015). Moreover, contextualized word representations such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have also achieved great success. As for NER, the end-to-end bi-directional LSTM CRF models (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2018) is one representative architecture. These models are only capable of recognizing regular named entities. For overlapped NER, the earliest model to our knowledge is proposed by Finkel and Manning (2009), where they convert overlapped NER as a parsing task. Lu and Roth (2015) propose a hypergraph model to r"
2021.acl-long.372,D19-1585,0,0.143846,"2019; Guo et al., 2019). We evaluate our proposed model on several benchmark datasets which includes both overlapped and discontinuous entities (e.g., CLEF (Suominen et al., 2013)). The results show that our model outperforms the hypergraph (Muis and Lu, 2016; Wang and Lu, 2019) and transition-based models (Dai et al., 2020). Besides, we conduct experiments on two benchmark datasets including only overlapped entities (i.e., GENIA (Kim et al., 2003) and ACE05). Experimental results show that our model can also obtain comparable performances with the state-of-the-art models (Luan et al., 2019; Wadden et al., 2019; Strakov´a et al., 2019). In addition, we observe that our approaches for model enhancement are effective in the benchmark datasets. Our code is available at https://github.com/foxlf823/sodner. 2 Related Work In the NLP domain, NER is usually considered as a sequence labeling problem (Liu et al., 2018; Lin et al., 2019b; Cao et al., 2019). With well-designed features, CRF-based models have achieved the leading performance (Lafferty et al., 2001; Finkel et al., 2005; Liu et al., 2011). Recently, neural network models have been exploited for feature representations (Chen and Manning, 2014; Zhou"
2021.acl-long.372,D18-1019,0,0.0168956,"le overlapped and discontinuous entities (Lu and Roth, 2015; Muis and Lu, 2016), which we illustrate using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER could be conducted universally without any a"
2021.acl-long.372,D19-1644,0,0.354757,"using two examples as shown in Figure 1. The two entities “Pennsylvania” and “Pennsylvania radio station” are nested with each other,1 and the sec∗ 1 Corresponding author. We consider “nested” as a special case of “overlapped”. ond example shows a discontinuous entity “mitral leaflets thickened” involving three fragments. There have been several studies to investigate overlapped or discontinuous entities (Finkel and Manning, 2009; Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018; Ju et al., 2018; Wang et al., 2018; Fisher and Vlachos, 2019; Luan et al., 2019; Wang and Lu, 2019). The majority of them focus on overlapped NER, with only several exceptions to the best of our knowledge. Muis and Lu (2016) present a hypergraph model that is capable of handling both overlapped and discontinuous entities. Wang and Lu (2019) extend the hypergraph model with long short-term memories (LSTMs) (Hochreiter and Schmidhuber, 1997). Dai et al. (2020) proposed a transition-based neural model for discontinuous NER. By using these models, NER could be conducted universally without any assumption to exclude overlapped or discontinuous entities, which could be more practical in real appl"
2021.acl-long.372,D18-1124,0,0.0405263,"Missing"
2021.acl-long.432,P17-2082,0,0.0168677,"lscale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations. 1 Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al., 2008; Callison-Burch and Dredze, 2010) to Corresponding author. Andrea B-PER Ferrigato sprinted to his World I-PER O O O B-ORG Cup I-ORG win O A-2 O B-PER O O O B-MISC I-MISC O A-3 B-PER I-PER O O O B-ORG I-ORG O EXP B-PER I-PER O O O B-MISC I-MISC O Figure 1: A NER example with crowdsourced labels, A and EXP denote annotator and expert, respectively. Introduction ∗ text A-1 collect a large scale dataset for supervised model training. In contrast to the gold-standard annotations"
2021.acl-long.432,W10-0701,0,0.0199133,"ve, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations. 1 Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al., 2008; Callison-Burch and Dredze, 2010) to Corresponding author. Andrea B-PER Ferrigato sprinted to his World I-PER O O O B-ORG Cup I-ORG win O A-2 O B-PER O O O B-MISC I-MISC O A-3 B-PER I-PER O O O B-ORG I-ORG O EXP B-PER I-PER O O O B-MISC I-MISC O Figure 1: A NER example with crowdsourced labels, A and EXP denote annotator and expert, respectively. Introduction ∗ text A-1 collect a large scale dataset for supervised model training. In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al.,"
2021.acl-long.432,C18-1111,0,0.146823,"ds a specific task, and they annotate the task consistently according to their individual principles by the understandings, where the experts can reach an oracle principle by consensus. The above view indicates that crowdsourcing learning aims to train a model based on the understandings of crowd annotators, and then test the model by the oracle understanding from experts. Based on the assumption, we find that crowdsourcing learning is highly similar to domain adaptation, which is one important topic that has been investigated extensively for decades (Ben-David et al., 2006; Daum´e III, 2007; Chu and Wang, 2018; Jia and Zhang, 2020). We treat each annotator as one domain specifically, and then crowdsourcing learning is essentially almost a multi-source domain adaptation problem. Thus, one natural question arises: What is the performance when a state-of-the-art domain adaptation model is applied directly to crowdsourcing learning. Here we take NER as a study case to investigate crowdsourcing learning as domain adaptation, considering that NER has been one popular task for crowdsourcing learning in the NLP community (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016). We suggest a st"
2021.acl-long.432,P07-1033,0,0.365237,"Missing"
2021.acl-long.432,C16-1111,0,0.254629,"gate both unsupervised and supervised crowdsourcing learning, assuming that no or only smallscale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations. 1 Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al., 2008; Callison-Burch and Dredze, 2010) to Corresponding author. Andrea B-PER Ferrigato sprinted to his World I-PER O O O B-ORG Cup I-ORG win O A-2 O B-PER O O O B-MISC I-MISC O A-3 B-PER I-PER O O O B-ORG I-ORG O EXP B-PER I-PER O O O B-MISC I-MISC O Figure 1: A NER example with crowdsourced labels, A and EXP denote annotator and expert, respectively. Introduction ∗ text A-1 collect a large scale"
2021.acl-long.432,N19-1423,0,0.126146,"rning model that can effectively capture annotator(domain)-aware features. Also, we investigate two settings of crowdsourcing learning, one being the unsupervised setting with no expert annotation, which has been widely studied before, and the other being the supervised setting where a certain scale of expert annotations exists, which is inspired by domain adaptation. Finally, we conduct experiments on a benchmark crowdsourcing NER dataset (Tjong Kim Sang and De Meulder, 2003; Rodrigues et al., 2014) to evaluate our methods. We take a standard BiLSTM-CRF (Lample et al., 2016) model with BERT (Devlin et al., 2019) word representations as the baseline, and adapt it to our representation learning model. Experimental results show that our method is able to model crowdsourced annotations effectively. Under the unsupervised setting, our model can give a strong performance, outperforming previous work significantly. In addition, the model performance can be greatly boosted by feeding with small-scale expert annotations, which can be a prospective direction for low-resource scenarios. Annotator1 ··· AnnotatorM xji = a (x i i j) Domain1 ··· DomainM Crowdsourcing Learning xij → yji =⇒ xij → yji =⇒ Domaintgt Exp"
2021.acl-long.432,W10-0713,0,0.530088,"features. We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only smallscale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations. 1 Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al., 2008; Callison-Burch and Dredze, 2010) to Corresponding author. Andrea B-PER Ferrigato sprinted to his World I-PER O O O B-ORG Cup I-ORG win O A-2 O B-PER O O O B-MISC I-MISC O A-3 B-PER I-PER O O O B-ORG I-ORG O EXP B-PER I-PER O O O B-MISC I-MISC O Figure 1: A NER example with crowdsourced labels, A and EXP denote annotator and expert, respectively. Introduction ∗ text"
2021.acl-long.432,N09-1068,0,0.0621386,"k, we present a totally different viewpoint for crowdsourcing, regarding all crowdsourced annotations as golden in terms of individual annotators, just like the primitive gold-standard labels corresponded to the experts, and further propose a domain adaptation paradigm for crowdsourcing learning. 5.2 Domain Adaptation Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 2006; Mansour et al., 2009), which has also received great attention in the NLP community (Daum´e III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020). Typical methods include self-training to produce pseudo training instances for the target domain (Yu et al., 2015) and representation learning to capture transferable features across the source and target domains (Sener et al., 2016). In this work, we make correlations between domain adaptation and crowdsourcing learning, enabling crowdsourcing learning to benefit from the advances of domain adaptation, and then present a representation learning model borrowed from Jia ¨ un et al. (2020). et al. (2019) and Ust¨ Named Entity R"
2021.acl-long.432,N13-1132,0,0.018732,"ed model training. In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018). However, these annotations are relatively lower-quality with much-unexpected noise since the crowd annotators are not professional enough, which can make errors in complex and ambiguous contexts (Sheng et al., 2008). Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020). Most of these studies assume the crowdsou"
2021.acl-long.432,P14-2062,0,0.0158828,"g learning, assuming that no or only smallscale expert annotations are available. Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance. In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations. 1 Crowdsourcing has gained a growing interest in the natural language processing (NLP) community, which helps hard NLP tasks such as named entity recognition (Finin et al., 2010; Derczynski et al., 2016), part-of-speech tagging (Hovy et al., 2014), relation extraction (Abad et al., 2017), translation (Zaidan and Callison-Burch, 2011), argument retrieval (Mayhew et al., 2020), and others (Snow et al., 2008; Callison-Burch and Dredze, 2010) to Corresponding author. Andrea B-PER Ferrigato sprinted to his World I-PER O O O B-ORG Cup I-ORG win O A-2 O B-PER O O O B-MISC I-MISC O A-3 B-PER I-PER O O O B-ORG I-ORG O EXP B-PER I-PER O O O B-MISC I-MISC O Figure 1: A NER example with crowdsourced labels, A and EXP denote annotator and expert, respectively. Introduction ∗ text A-1 collect a large scale dataset for supervised model training. In c"
2021.acl-long.432,W09-1904,0,0.0544803,"collect a large scale dataset for supervised model training. In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018). However, these annotations are relatively lower-quality with much-unexpected noise since the crowd annotators are not professional enough, which can make errors in complex and ambiguous contexts (Sheng et al., 2008). Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020)."
2021.acl-long.432,W10-2604,0,0.0361739,"e, denoted by Dsrc = {(Xi , Yi )}M i=1 , Ni Ni 2 i i where Xi = {xj }j=1 and Yi = {yj }j=1 , and we aim to train a model on Dsrc to adapt to a specific target domain with the help of a large scale t raw corpus Xtgt = {xi }N i=1 of the target domain. Note that under this setting, all Xs, including source and target domains, are generated individually according to their unknown distributions, thus the abstract representations learned from the source domain dataset Dsrc would inevitably be biased to the target domain, which is the primary reason for the degraded performance of the target domain (Huang and Yates, 2010; Ganin et al., 2016). A number of domain adaptation models have struggled for better transferable high-level representations as domain shifts (Ramponi and Plank, 2020). 2.2 Crowdsourcing Learning Crowdsourcing aims to produce a set of large-scale annotated examples created by crowd annotators, which is used to train supervised models for a given task (Raykar et al., 2010). As the majority of NLP models assume that gold-standard highquality training corpora are already available (Manning and Schutze, 1999), crowdsourcing learning has received much less interest than cross-domain adaptation, al"
2021.acl-long.432,D15-1261,0,0.0215554,"rded as low-quality, including much noise regarding expert annotations as the gold-standard. Initial studies of crowdsourcing learning try to arrive at a high-quality corpus by majority voting or control the quality by sophisticated strategies during the crowd annotation process (Khattak and Salleb-Aouissi, 2011; Liu et al., 2017; Tang and Lease, 2011). Recently, the majority work focuses on full exploration of all annotated corpus by machine learning models, taking the information from crowd annotators into account including annotator reliability (Rodrigues et al., 2014), annotator accuracy (Huang et al., 2015), worker-label confusion matrix (Nguyen et al., 2017), and sequential confusion matrix (Simpson and Gurevych, 2019). In this work, we present a totally different viewpoint for crowdsourcing, regarding all crowdsourced annotations as golden in terms of individual annotators, just like the primitive gold-standard labels corresponded to the experts, and further propose a domain adaptation paradigm for crowdsourcing learning. 5.2 Domain Adaptation Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 200"
2021.acl-long.432,D15-1035,0,0.0199752,"In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018). However, these annotations are relatively lower-quality with much-unexpected noise since the crowd annotators are not professional enough, which can make errors in complex and ambiguous contexts (Sheng et al., 2008). Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020). Most of these studies assume the crowdsourced annotations as untrustwo"
2021.acl-long.432,P19-1236,0,0.254783,"adaptation together, as shown in Figure 2, based on the assumption that all Y s are gold-standard for crowdsourced annotations when crowd annotators are united as joint inputs. And finally, we need to perform predictions by regarding xexpert = expert(x), and in particular, the learning of expert differs from that of the target domain in domain adaptation. 3 A Case Study On NER In this section, we take NER as a case study, which has been investigated most frequently in NLP (Yadav and Bethard, 2018), and propose a representation learning model mainly inspired by the domain adaptation model of (Jia et al., 2019) to perform crowdsourcing learning. In addition, we introduce the unsupervised and supervised settings for crowdsourcing learning which are directly borrowed from the domain adaptation. 3.1 The Representation Learning Model We convert NER into a standard sequence labeling problem by using the BIO schema, following the majority of previous works, and extend a stateof-the-art BERT-BiLSTM-CRF model (Mayhew 5560 a et al., 2020) to our crowdsourcing learning. Figure 3 shows the overall network structure of our representation learning model. By using a sophisticated parameter generator module (Plata"
2021.acl-long.432,2020.acl-main.524,0,0.13725,"and they annotate the task consistently according to their individual principles by the understandings, where the experts can reach an oracle principle by consensus. The above view indicates that crowdsourcing learning aims to train a model based on the understandings of crowd annotators, and then test the model by the oracle understanding from experts. Based on the assumption, we find that crowdsourcing learning is highly similar to domain adaptation, which is one important topic that has been investigated extensively for decades (Ben-David et al., 2006; Daum´e III, 2007; Chu and Wang, 2018; Jia and Zhang, 2020). We treat each annotator as one domain specifically, and then crowdsourcing learning is essentially almost a multi-source domain adaptation problem. Thus, one natural question arises: What is the performance when a state-of-the-art domain adaptation model is applied directly to crowdsourcing learning. Here we take NER as a study case to investigate crowdsourcing learning as domain adaptation, considering that NER has been one popular task for crowdsourcing learning in the NLP community (Finin et al., 2010; Rodrigues et al., 2014; Derczynski et al., 2016). We suggest a state-of-the-art represe"
2021.acl-long.432,P07-1034,0,0.168453,"ch, 2019). In this work, we present a totally different viewpoint for crowdsourcing, regarding all crowdsourced annotations as golden in terms of individual annotators, just like the primitive gold-standard labels corresponded to the experts, and further propose a domain adaptation paradigm for crowdsourcing learning. 5.2 Domain Adaptation Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 2006; Mansour et al., 2009), which has also received great attention in the NLP community (Daum´e III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020). Typical methods include self-training to produce pseudo training instances for the target domain (Yu et al., 2015) and representation learning to capture transferable features across the source and target domains (Sener et al., 2016). In this work, we make correlations between domain adaptation and crowdsourcing learning, enabling crowdsourcing learning to benefit from the advances of domain adaptation, and then present a representation learning model borrowed from Jia ¨ un et al. (2020). et al. (201"
2021.acl-long.432,N16-1030,0,0.172852,"a state-of-the-art representation learning model that can effectively capture annotator(domain)-aware features. Also, we investigate two settings of crowdsourcing learning, one being the unsupervised setting with no expert annotation, which has been widely studied before, and the other being the supervised setting where a certain scale of expert annotations exists, which is inspired by domain adaptation. Finally, we conduct experiments on a benchmark crowdsourcing NER dataset (Tjong Kim Sang and De Meulder, 2003; Rodrigues et al., 2014) to evaluate our methods. We take a standard BiLSTM-CRF (Lample et al., 2016) model with BERT (Devlin et al., 2019) word representations as the baseline, and adapt it to our representation learning model. Experimental results show that our method is able to model crowdsourced annotations effectively. Under the unsupervised setting, our model can give a strong performance, outperforming previous work significantly. In addition, the model performance can be greatly boosted by feeding with small-scale expert annotations, which can be a prospective direction for low-resource scenarios. Annotator1 ··· AnnotatorM xji = a (x i i j) Domain1 ··· DomainM Crowdsourcing Learning x"
2021.acl-long.432,2020.coling-main.507,0,0.0290314,"Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020). Most of these studies assume the crowdsourced annotations as untrustworthy answers, proposing sophisticated strategies to recover the golden answers from crowdsourced labels. In this work, we take a different view for crowdsourcing learning, regarding the crowdsourced annotations as the gold standard in terms of individual 5558 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5558–5570 August 1–6, 2021. ©2021 Association for Computational Linguistics annotators. In other w"
2021.acl-long.432,P17-1028,0,0.221013,"ences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020). Most of these studies assume the crowdsourced annotations as untrustworthy answers, proposing sophisticated strategies to recover the golden answers from crowdsourced labels. In this work, we take a different view for crowdsourcing learning, regarding the crowdsourced annotations as the gold standard in terms of individual 5558 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5558–5570 August 1–6, 2021. ©2021 Association for Co"
2021.acl-long.432,P18-1019,0,0.0172538,"redze, 2010) to Corresponding author. Andrea B-PER Ferrigato sprinted to his World I-PER O O O B-ORG Cup I-ORG win O A-2 O B-PER O O O B-MISC I-MISC O A-3 B-PER I-PER O O O B-ORG I-ORG O EXP B-PER I-PER O O O B-MISC I-MISC O Figure 1: A NER example with crowdsourced labels, A and EXP denote annotator and expert, respectively. Introduction ∗ text A-1 collect a large scale dataset for supervised model training. In contrast to the gold-standard annotations labeled by experts, the crowdsourced annotations can be constructed quickly at a low cost with masses of crowd annotators (Snow et al., 2008; Nye et al., 2018). However, these annotations are relatively lower-quality with much-unexpected noise since the crowd annotators are not professional enough, which can make errors in complex and ambiguous contexts (Sheng et al., 2008). Previous crowdsourcing learning models struggle to reduce the influences of noises of the crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues"
2021.acl-long.432,D18-1039,0,0.0695218,"2019) to perform crowdsourcing learning. In addition, we introduce the unsupervised and supervised settings for crowdsourcing learning which are directly borrowed from the domain adaptation. 3.1 The Representation Learning Model We convert NER into a standard sequence labeling problem by using the BIO schema, following the majority of previous works, and extend a stateof-the-art BERT-BiLSTM-CRF model (Mayhew 5560 a et al., 2020) to our crowdsourcing learning. Figure 3 shows the overall network structure of our representation learning model. By using a sophisticated parameter generator module (Platanios et al., 2018), it can capture annotator-aware features. Following, we introduce the proposed model by four components: (1) word representation, (2) annotator switcher, (3) BiLSTM Encoding, and (4) CRF inference and training. where Θ ∈ R|V |×|e |, x = r10 · · · rn0 is the annotator-aware representations of annotator a for x = w1 · · · wn , and ea is the annotator embedding. Word Representation Given a sentence of n words x = w1 · · · wn , we first convert it to vectorial representations by BERT. Different from the standard BERT exploration, here we use Adapter◦BERT (Houlsby et al., 2019), where two extra ad"
2021.acl-long.432,2020.coling-main.603,0,0.568954,"b.com/izhx/CLasDA under the Apache License 2.0 to facilitate future research. 2 The Basic Idea Here we describe the concepts of the domain adaptation and crowdsourcing learning in detail, and show how they are connected together. 2.1 Domain Adaptation Domain adaptation happens when a supervised model trained on a fixed set of training corpus, including several specific domains, is required to test on a different domain (Ben-David et al., 2006; Mansour et al., 2009). The scenario is quite frequent in practice, and thus has received extensive attention with massive investigations (Csurka, 2017; Ramponi and Plank, 2020). The major problem lies in the different input distributions between source and target domains, leading to biased predictions over the inputs with a large gap to the source domains. 5559 Here we focus on multi-source cross-domain adaptation, which would suit our next corresponding mostly. Following Mansour et al. (2009); Zhao et al. (2019), the multi-source domain adaptation assumes a set of labeled examples from M domains 1 available, denoted by Dsrc = {(Xi , Yi )}M i=1 , Ni Ni 2 i i where Xi = {xj }j=1 and Yi = {yj }j=1 , and we aim to train a model on Dsrc to adapt to a specific target dom"
2021.acl-long.432,D19-1101,0,0.233748,"e crowdsourced annotations (Hsueh et al., 2009; Raykar and Yu, 2012a; Hovy et al., 2013; Jamison and Gurevych, 2015). Majority voting (MV) is one straightforward way to aggregate high-quality annotations, which has been widely adopted (Snow et al., 2008; Fernandes and Brefeld, 2011; Rodrigues et al., 2014), but it requires multiple annotations for a given input. Recently, the majority of models concentrate on monitoring the distances between crowdsourced and gold-standard annotations, obtaining better performances than MV by considering the annotator information together (Nguyen et al., 2017; Simpson and Gurevych, 2019; Li et al., 2020). Most of these studies assume the crowdsourced annotations as untrustworthy answers, proposing sophisticated strategies to recover the golden answers from crowdsourced labels. In this work, we take a different view for crowdsourcing learning, regarding the crowdsourced annotations as the gold standard in terms of individual 5558 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5558–5570 August 1–6, 2021. ©2021 Association for Computational Linguistics anno"
2021.acl-long.432,D08-1027,0,0.392199,"Missing"
2021.acl-long.432,C18-1182,0,0.0959579,"is generated from different distributions as well. In this way, we are able to connect crowdsourcing learning and domain adaptation together, as shown in Figure 2, based on the assumption that all Y s are gold-standard for crowdsourced annotations when crowd annotators are united as joint inputs. And finally, we need to perform predictions by regarding xexpert = expert(x), and in particular, the learning of expert differs from that of the target domain in domain adaptation. 3 A Case Study On NER In this section, we take NER as a case study, which has been investigated most frequently in NLP (Yadav and Bethard, 2018), and propose a representation learning model mainly inspired by the domain adaptation model of (Jia et al., 2019) to perform crowdsourcing learning. In addition, we introduce the unsupervised and supervised settings for crowdsourcing learning which are directly borrowed from the domain adaptation. 3.1 The Representation Learning Model We convert NER into a standard sequence labeling problem by using the BIO schema, following the majority of previous works, and extend a stateof-the-art BERT-BiLSTM-CRF model (Mayhew 5560 a et al., 2020) to our crowdsourcing learning. Figure 3 shows the overall"
2021.acl-long.432,W15-2201,0,0.0740945,"corresponded to the experts, and further propose a domain adaptation paradigm for crowdsourcing learning. 5.2 Domain Adaptation Domain adaptation has been studied extensively to reduce the performance gap between the resourcerich and resource-scarce domains (Ben-David et al., 2006; Mansour et al., 2009), which has also received great attention in the NLP community (Daum´e III, 2007; Jiang and Zhai, 2007; Finkel and Manning, 2009; Glorot et al., 2011; Chu and Wang, 2018; Ramponi and Plank, 2020). Typical methods include self-training to produce pseudo training instances for the target domain (Yu et al., 2015) and representation learning to capture transferable features across the source and target domains (Sener et al., 2016). In this work, we make correlations between domain adaptation and crowdsourcing learning, enabling crowdsourcing learning to benefit from the advances of domain adaptation, and then present a representation learning model borrowed from Jia ¨ un et al. (2020). et al. (2019) and Ust¨ Named Entity Recognition NER is a fundamental and challenging task of NLP (Yadav and Bethard, 2018). The BiLSTM-CRF (Lample et al., 2016) architecture, as well as BERT (Devlin et al., 2019), are ab"
2021.acl-long.432,P11-1122,0,0.11168,"Missing"
2021.emnlp-main.314,E14-1023,0,0.0257791,"a whole, which has previous subtasks can influence the later subtasks received great interest recently. This task can be as well. To address the two weaknesses, end-to-end helpful for a number of tasks, including informa- modeling is one promising alternative, which has tion extraction (Surdeanu et al., 2003), question been widely adopted in natural language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet semantic by a graph, and then et al., 2017; Bastianelli et al., 2020) usually divide design an end-to-end neural model to construct the the task into three subtasks, including target iden- graph incrementally. An encode"
2021.emnlp-main.314,S07-1018,0,0.138675,"Missing"
2021.emnlp-main.314,P98-1013,0,0.559076,"jointly. Concretely, we exploit a graphbased method, regarding frame semantic parsing as a graph construction problem. All predicates and roles are treated as graph nodes, and their relations are taken as graph edges. Experiment results on two benchmark datasets of frame semantic parsing show that our method is highly competitive, resulting in better performance than pipeline models. There have also been some power outages due to power lines being damaged Existence Entity Effect Causation Cause Patient Damaging Figure 1: An example involving frame semantic structures, taken from the FrameNet (Baker et al., 1998). Frame-evoking predicates are highlighted in the sentence, and corresponding frames are shown in colored blocks below. The frame-specific roles are underlined with their frames in the same row. works focus on either one or two of the three subtasks, treating them separately (Yang and Mitchell, 2017; Botschen et al., 2018; Swayamdipta et al., 2018; Peng et al., 2018). The above formalization has two weaknesses. First, the individual modeling of the three subtasks is inefficient to utilize the relationship among them. Apparently, the earlier subtasks can not exploit the 1 Introduction informati"
2021.emnlp-main.314,J02-3001,0,0.767815,"Missing"
2021.emnlp-main.314,N18-1134,0,0.0312884,"Missing"
2021.emnlp-main.314,E17-1045,0,0.0481984,"Missing"
2021.emnlp-main.314,C18-1233,0,0.0526351,"Missing"
2021.emnlp-main.314,P18-2058,0,0.104099,"from the error propa2002) aims to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received great interest recently. This task can be as well. To address the two weaknesses, end-to-end helpful for a number of tasks, including informa- modeling is one promising alternative, which has tion extraction (Surdeanu et al., 2003), question been widely adopted in natural language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet semantic by a graph, and then et al., 2017; Bastianelli et al"
2021.emnlp-main.314,P14-1136,0,0.0428757,"Missing"
2021.emnlp-main.314,C12-1042,0,0.0201795,"Missing"
2021.emnlp-main.314,J14-1002,0,0.347468,"language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet semantic by a graph, and then et al., 2017; Bastianelli et al., 2020) usually divide design an end-to-end neural model to construct the the task into three subtasks, including target iden- graph incrementally. An encoder-decoder model is tification, frame classification and semantic role presented to achieve the graph building goal, where labeling (SRL), respectively. By performing the the encoder is equipped with contextualized BERT three subtasks sequentially, the whole frame seman- representation (Devlin et al., 2019), and the detic parsing can be accomplish"
2021.emnlp-main.314,N19-1423,0,0.0764309,"We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet semantic by a graph, and then et al., 2017; Bastianelli et al., 2020) usually divide design an end-to-end neural model to construct the the task into three subtasks, including target iden- graph incrementally. An encoder-decoder model is tification, frame classification and semantic role presented to achieve the graph building goal, where labeling (SRL), respectively. By performing the the encoder is equipped with contextualized BERT three subtasks sequentially, the whole frame seman- representation (Devlin et al., 2019), and the detic parsing can be accomplished. The majority of coder includes node generation and edge building ∗ Corresponding author. sequentially. Our final model is elegant and easy to 3864 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3864–3874 c November 7–11, 2021. 2021 Association for Computational Linguistics understand as a whole. We conduct experiments on two benchmark datasets to evaluate the effectiveness of our proposed model. First, we study our graph-based framework in two settings, the end-to-end scenario and the pipeline manner, w"
2021.emnlp-main.314,P19-1525,0,0.0215952,"applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly. Graph-Based Methods Recently, graph-based methods have been widely used in a range of other tasks, such as dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016; Ji et al., 2019), AMR parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a,b) and relation extraction (Sun et al., 2019; Fu et al., 2019; Dixit and Al-Onaizan, 2019). In this work, we aims for frame semantic parsing, organizing the three included subtasks by a well designed graph, converting it into graph-based parsing task naturally. 3 3.1 Method Task Formulation The goal of frame-semantic parsing is to extract semantic predicate-argument structures from texts, where each predicate-argument structure includes a predicate by a span of words, a well-defined semantic frame to express the key roles of the predicate, and the values of these roles by word spans. Formally, given by a sentence X with n words w1 , w2 , . . . , wn , frame-semantic parsing aims to"
2021.emnlp-main.314,P81-1022,0,0.619534,"Missing"
2021.emnlp-main.314,2020.aacl-main.13,0,0.0323686,"ors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received great interest recently. This task can be as well. To address the two weaknesses, end-to-end helpful for a number of tasks, including informa- modeling is one promising alternative, which has tion extraction (Surdeanu et al., 2003), question been widely adopted in natural language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet semantic by a graph, and then et al., 2017; Bastianelli et al., 2020) usually divide design an end-to-end neural model to construct the the task into three subtask"
2021.emnlp-main.314,P14-1134,0,0.0300927,"odels as well (Cai et al., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly. Graph-Based Methods Recently, graph-based methods have been widely used in a range of other tasks, such as dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016; Ji et al., 2019), AMR parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a,b) and relation extraction (Sun et al., 2019; Fu et al., 2019; Dixit and Al-Onaizan, 2019). In this work, we aims for frame semantic parsing, organizing the three included subtasks by a well designed graph, converting it into graph-based parsing task naturally. 3 3.1 Method Task Formulation The goal of frame-semantic parsing is to extract semantic predicate-argument structures from texts, where each predicate-argument structure includes a predicate by a span of words, a well-defined semantic frame to express the key roles of the predicate, and the val"
2021.emnlp-main.314,P19-1136,0,0.0612692,"Missing"
2021.emnlp-main.314,P19-1237,0,0.0233333,"-style semantic role labeling models as well (Cai et al., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly. Graph-Based Methods Recently, graph-based methods have been widely used in a range of other tasks, such as dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016; Ji et al., 2019), AMR parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a,b) and relation extraction (Sun et al., 2019; Fu et al., 2019; Dixit and Al-Onaizan, 2019). In this work, we aims for frame semantic parsing, organizing the three included subtasks by a well designed graph, converting it into graph-based parsing task naturally. 3 3.1 Method Task Formulation The goal of frame-semantic parsing is to extract semantic predicate-argument structures from texts, where each predicate-argument structure includes a predicate by a span of words, a well-defined semantic frame to express the key"
2021.emnlp-main.314,Q16-1023,0,0.0241306,"are several end-to-end Propbank-style semantic role labeling models as well (Cai et al., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly. Graph-Based Methods Recently, graph-based methods have been widely used in a range of other tasks, such as dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016; Ji et al., 2019), AMR parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a,b) and relation extraction (Sun et al., 2019; Fu et al., 2019; Dixit and Al-Onaizan, 2019). In this work, we aims for frame semantic parsing, organizing the three included subtasks by a well designed graph, converting it into graph-based parsing task naturally. 3 3.1 Method Task Formulation The goal of frame-semantic parsing is to extract semantic predicate-argument structures from texts, where each predicate-argument structure includes a predicate by a span of words, a well-defined semantic frame"
2021.emnlp-main.314,P15-2036,0,0.0188002,"we make the following two major contributions in this work: (1) We propose a novel graph-based model for frame semantic parsing which can achieve competitive results for the end-to-end task as well as the individual subtasks. (2) To the best of our knowledge, we present the first work of end-to-end frame semantic parsing to solve all included subtasks together in a single model. We will release our codes as well as experimental setting public available on https://github. com/Ch4osMy7h/FramenetParser to help result reproduction and facilitate future researches. 2 Related Work most extensively (Kshirsagar et al., 2015; Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018; Marcheggiani and Titov, 2020). It is also highly related to the Propbank-style semantic role labeling (Palmer et al., 2005) as while with only differences in the frame definition. Thus the models between the two types of semantic role labeling can be mutually borrowed. There are several end-to-end Propbank-style semantic role labeling models as well (Cai et al., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the addition"
2021.emnlp-main.314,C10-1081,0,0.0406037,"s to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received great interest recently. This task can be as well. To address the two weaknesses, end-to-end helpful for a number of tasks, including informa- modeling is one promising alternative, which has tion extraction (Surdeanu et al., 2003), question been widely adopted in natural language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet semantic by a graph, and then et al., 2017; Bastianelli et al., 2020) usually divide design an e"
2021.emnlp-main.314,P18-1037,0,0.0255748,"l., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly. Graph-Based Methods Recently, graph-based methods have been widely used in a range of other tasks, such as dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016; Ji et al., 2019), AMR parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a,b) and relation extraction (Sun et al., 2019; Fu et al., 2019; Dixit and Al-Onaizan, 2019). In this work, we aims for frame semantic parsing, organizing the three included subtasks by a well designed graph, converting it into graph-based parsing task naturally. 3 3.1 Method Task Formulation The goal of frame-semantic parsing is to extract semantic predicate-argument structures from texts, where each predicate-argument structure includes a predicate by a span of words, a well-defined semantic frame to express the key roles of the predicate, and the values of these roles by"
2021.emnlp-main.314,2020.emnlp-main.322,0,0.030706,"model for frame semantic parsing which can achieve competitive results for the end-to-end task as well as the individual subtasks. (2) To the best of our knowledge, we present the first work of end-to-end frame semantic parsing to solve all included subtasks together in a single model. We will release our codes as well as experimental setting public available on https://github. com/Ch4osMy7h/FramenetParser to help result reproduction and facilitate future researches. 2 Related Work most extensively (Kshirsagar et al., 2015; Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018; Marcheggiani and Titov, 2020). It is also highly related to the Propbank-style semantic role labeling (Palmer et al., 2005) as while with only differences in the frame definition. Thus the models between the two types of semantic role labeling can be mutually borrowed. There are several end-to-end Propbank-style semantic role labeling models as well (Cai et al., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-d"
2021.emnlp-main.314,J05-1004,0,0.229419,"as the individual subtasks. (2) To the best of our knowledge, we present the first work of end-to-end frame semantic parsing to solve all included subtasks together in a single model. We will release our codes as well as experimental setting public available on https://github. com/Ch4osMy7h/FramenetParser to help result reproduction and facilitate future researches. 2 Related Work most extensively (Kshirsagar et al., 2015; Yang and Mitchell, 2017; Peng et al., 2018; Swayamdipta et al., 2018; Marcheggiani and Titov, 2020). It is also highly related to the Propbank-style semantic role labeling (Palmer et al., 2005) as while with only differences in the frame definition. Thus the models between the two types of semantic role labeling can be mutually borrowed. There are several end-to-end Propbank-style semantic role labeling models as well (Cai et al., 2018; He et al., 2018; Li et al., 2019; Fu et al., 2019). However, these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly."
2021.emnlp-main.314,N18-1135,0,0.0961416,"pipeline models. There have also been some power outages due to power lines being damaged Existence Entity Effect Causation Cause Patient Damaging Figure 1: An example involving frame semantic structures, taken from the FrameNet (Baker et al., 1998). Frame-evoking predicates are highlighted in the sentence, and corresponding frames are shown in colored blocks below. The frame-specific roles are underlined with their frames in the same row. works focus on either one or two of the three subtasks, treating them separately (Yang and Mitchell, 2017; Botschen et al., 2018; Swayamdipta et al., 2018; Peng et al., 2018). The above formalization has two weaknesses. First, the individual modeling of the three subtasks is inefficient to utilize the relationship among them. Apparently, the earlier subtasks can not exploit the 1 Introduction information from their future subtasks. Second, the Frame semantic parsing (Gildea and Jurafsky, pipeline strategy can suffer from the error propa2002) aims to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received great interest recen"
2021.emnlp-main.314,2020.acl-main.296,0,0.0213966,"e edges between nodes which are predicate fragments (i.e., PPRD nodes). In detail, we treat it as a binary classification problem considering whether two nodes alongside the edge can form parts of a predicate or not. Formally, given two PPRD nodes with the corresponding spans spi and spj and their encoding representations gip and gjp , we utilize one MLP layer to classify their edge type: ppe = softmax(MLPpe ([gip , gjp , gip ∗ gjp ])), (5) where ppe indicates the probabilities of two types, namely Connected and NULL (i.e., cannot be connected), and the feature representation is borrowed from Zhao et al. (2020). Predicate-Role Edge For extracting framespecific roles, we build the edges between predicates nodes (i.e., node type by FPRD or PPRD) and role nodes (i.e., node type by ROLE). Given a predicate node spi and a role node srj , assuming their neural representations being gip and gjr , respectively, we utilize another MLP layer to determine their edge type by multi-class classification: pre = softmax(MLPre ([gip , gjr , gip ∗ gjr ])), (6) where pre indicates the probabilities of predicaterole edge types (i.e., frame roles as well as a NULL label indicating no relation). where pc is the output pr"
2021.emnlp-main.314,J08-2005,0,0.0909793,"en times faster speed in comparison to Semi-CRF. Even though the SemiCRF implementation7 uses dynamic programming to optimize the time complexity, it still needs to iterate over segments of each sentence in the batch one by one, which might not take advantage of the GPU’s parallel capabilities to accelerate the process. Nevertheless, our model as a whole adopts batch-based learning, which enables more efficient inference. 7 Frame classification without dictionary Following Swayamdipta et al. (2017), we also adopt the Lexical Unit (LU) dictionary in our model empirically. However, according to Punyakanok et al. (2008), sometimes the dictionary might be quite limited. Therefore, we offer one eaxmple in Table 9 to illustrate the capability of our model for frames not in the dictionary. As shown, our model could predict the appropriate frame outside the dictionary as well and might additionally enrich the gold-standard annotations (i.e., the blue texts which do not appear in the Ground Truth). https://github.com/swabhs/scaffolding. Conclusion In this paper, we proposed a novel graph-based model to address the end-to-end frame semantic parsing task. The full frame semantic parsing result of one sentence is org"
2021.emnlp-main.314,D07-1002,0,0.120233,"semantic parsing (Gildea and Jurafsky, pipeline strategy can suffer from the error propa2002) aims to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received great interest recently. This task can be as well. To address the two weaknesses, end-to-end helpful for a number of tasks, including informa- modeling is one promising alternative, which has tion extraction (Surdeanu et al., 2003), question been widely adopted in natural language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We organize all predicates Previous studies (Das et al., 2014; Swayamdipta and their FrameNet"
2021.emnlp-main.314,P19-1131,0,0.0281365,", these models are difficult to be applied directly for frame semantic parsing due to the additional frame classification as well as the discontinuous predicates. In this work, we present a totally-different graph construction style model to solve end-to-end frame semantic parsing elegantly. Graph-Based Methods Recently, graph-based methods have been widely used in a range of other tasks, such as dependency parsing (Dozat and Manning, 2016; Kiperwasser and Goldberg, 2016; Ji et al., 2019), AMR parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a,b) and relation extraction (Sun et al., 2019; Fu et al., 2019; Dixit and Al-Onaizan, 2019). In this work, we aims for frame semantic parsing, organizing the three included subtasks by a well designed graph, converting it into graph-based parsing task naturally. 3 3.1 Method Task Formulation The goal of frame-semantic parsing is to extract semantic predicate-argument structures from texts, where each predicate-argument structure includes a predicate by a span of words, a well-defined semantic frame to express the key roles of the predicate, and the values of these roles by word spans. Formally, given by a sentence X with n words w1 , w2"
2021.emnlp-main.314,P03-1002,0,0.0685492,"n not exploit the 1 Introduction information from their future subtasks. Second, the Frame semantic parsing (Gildea and Jurafsky, pipeline strategy can suffer from the error propa2002) aims to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received great interest recently. This task can be as well. To address the two weaknesses, end-to-end helpful for a number of tasks, including informa- modeling is one promising alternative, which has tion extraction (Surdeanu et al., 2003), question been widely adopted in natural language processanswering (Shen and Lapata, 2007), machine trans- ing (NLP) (Cai et al., 2018; He et al., 2018; Sun lation (Liu and Gildea, 2010) and others (Coyne et al., 2019; Fu et al., 2019; Fei et al., 2020). et al., 2012; Chen et al., 2013; Agarwal et al., 2014). In this work, we propose a novel graph-based Figure 1 shows an example, where all predicates model to tackle frame semantic parsing in an endas well as their semantic frame and roles in the to-end way, using a single model to perform the sentence are depicted. three subtasks jointly. We"
2021.emnlp-main.314,D18-1412,0,0.196377,"n better performance than pipeline models. There have also been some power outages due to power lines being damaged Existence Entity Effect Causation Cause Patient Damaging Figure 1: An example involving frame semantic structures, taken from the FrameNet (Baker et al., 1998). Frame-evoking predicates are highlighted in the sentence, and corresponding frames are shown in colored blocks below. The frame-specific roles are underlined with their frames in the same row. works focus on either one or two of the three subtasks, treating them separately (Yang and Mitchell, 2017; Botschen et al., 2018; Swayamdipta et al., 2018; Peng et al., 2018). The above formalization has two weaknesses. First, the individual modeling of the three subtasks is inefficient to utilize the relationship among them. Apparently, the earlier subtasks can not exploit the 1 Introduction information from their future subtasks. Second, the Frame semantic parsing (Gildea and Jurafsky, pipeline strategy can suffer from the error propa2002) aims to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous subtasks can influence the later subtasks received"
2021.emnlp-main.314,D17-1128,0,0.0824554,"at our method is highly competitive, resulting in better performance than pipeline models. There have also been some power outages due to power lines being damaged Existence Entity Effect Causation Cause Patient Damaging Figure 1: An example involving frame semantic structures, taken from the FrameNet (Baker et al., 1998). Frame-evoking predicates are highlighted in the sentence, and corresponding frames are shown in colored blocks below. The frame-specific roles are underlined with their frames in the same row. works focus on either one or two of the three subtasks, treating them separately (Yang and Mitchell, 2017; Botschen et al., 2018; Swayamdipta et al., 2018; Peng et al., 2018). The above formalization has two weaknesses. First, the individual modeling of the three subtasks is inefficient to utilize the relationship among them. Apparently, the earlier subtasks can not exploit the 1 Introduction information from their future subtasks. Second, the Frame semantic parsing (Gildea and Jurafsky, pipeline strategy can suffer from the error propa2002) aims to analyze all sentential predicates as gation problem, where the errors occurring in the well as their FrameNet roles as a whole, which has previous su"
2021.emnlp-main.314,P19-1009,0,0.0457035,"Missing"
2021.emnlp-main.314,D19-1392,0,0.0379641,"Missing"
2021.emnlp-main.796,P15-1040,0,0.147002,"human refinement. Second, we investigate the performance of unsupervised cross-lingual transfer for Chinese ORL based on the annotated corpus. We investigate the Contextual Parameter Generator Networks (PGN) in multilingual BERT with Adapter (known as parameter efficient in learning) method (Üstün et al., 2020) (we call it PGNAdapter) and discover the complementarity of the model transfer and corpus translation methods. We conduct experiments on the newly constructed Chinese dataset to evaluate our methods, together with the English MPQA corpus (Wiebe et al., 2005) and the Portuguese dataset (Almeida et al., 2015a) for cross-lingual transfer. We observe that for the unsupervised cross-lingual transfer from the English corpus, the translation-based method is better than the model transfer, and their combination leads to further improvements. Although the scale of the Portuguese corpus is much smaller, adding it into the multilingual transfer still outperforms the bilingual counterpart. To summarize, in this paper, we have the following contributions: • We manually translate and annotate a Chinese fine-grained ORL corpus for research purposes, especially for the cross-lingual ORL study. • We conduct cro"
2021.emnlp-main.796,2020.acl-main.297,1,0.787448,"and cons of the different approaches. (2012)’s semi-Markov CRF model outperforms the standard CRF, and Irsoy and Cardie (2014) and Liu et al. (2015) use recurrent neural network for opinion mining. Johansson and Moschitti (2013) and Katiyar and Cardie (2016a) propose joint models for opinion expressions, holders and targets. Opinion Role Labeling As for ORL, Marasovi´c and Frank (2018) exploit multi-task learning about how to use SRL information to improve ORL scores. Zhang et al. (2019a) utilize semantic role labeling to enhance ORL, where three different integrating approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 20"
2021.emnlp-main.796,W06-1651,0,0.0788592,"nsfers establish a foundation for the future research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy an"
2021.emnlp-main.796,H05-1045,0,0.0741382,"ds with BERT. About the Adapter 2 Related Work method which a pre-trained network added between the transformer encoder layer, there are many Fine-Grained Opinion Mining There have studies on using adapter modules (Rebuffi et al., been a number of studies in fine-grained opinion mining (Wilson et al., 2009; Qiu et al., 2011; Wie- 2018; Stickland and Murray, 2019; Houlsby et al., 2019). PGN is first proposed by Platanios et al. gand et al., 2016). Kim and Hovy (2006a) exploit a semantic role labeller to extract opinion hold- (2018) for universal neural machine translation task. ers and topics. Choi et al. (2005) and Breck et al. And Üstün et al. (2020) integrated that two methods above in dependency parsing which inspired (2007) model the task by sequence labeling with us to merge this idea into our ORL task. CRF to discover opinion holders and recognize opinion expressions, respectively. Yang and Cardie The transfer method doesn’t work in Eger et al. 10140 Raw Translated Revision Raw Translated Revision The president had sidelined Masire after accusing him . 这位总统在指控马西尔之后退居二线。 在指控马西尔之后，总统把他排挤到了一边。 Russian guards seize 87 kg of heroin on Tajik-Afghan border . 俄罗斯警卫在塔吉克-阿富汗边境抓获87公斤海洛因。 俄罗斯士兵在塔吉克-阿富汗边境缴"
2021.emnlp-main.796,D19-1056,0,0.0258772,"ataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to the target 2016a). Here we focus on the opinion role labeling language. The corpus translation appr"
2021.emnlp-main.796,K15-1012,0,0.0265524,"ese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used for merging different languages to same space by Adapter and PGN methods with BERT. About the Adapter 2 Related Work method which a pre-trained network added between the transformer encoder layer, there are many Fine-Grained Opinion Mining There have studies on using adapter modules (Rebuffi et al."
2021.emnlp-main.796,N13-1073,0,0.0337349,"t these issues. On the other hand, the automatic sentences may not match the style of native speakers, and we let our annotators rewrite these sentences. Table 1 shows two examples of the two conditions, respectively. Opinion Projection Third, we project all opinions (expressions, holders and targets) from the English sentence into its Chinese translation. Before the projection, we use the Stanford Segmentor tool for word segmentation3 . The overall projection is supported by automatic word alignments, which can be produced by using a word-alignment tool. Here we exploit the fast-align tool4 (Dyer et al., 2013) to calculate the alignment probabilities. Figure 2 shows an example to illustrate the projection process. Concretely, given an EnglishChinese sentence pair (e1 · · · en , c1 · · · cm ) and its English-to-Chinese alignment probabilities a(cj |ei ), the projection is performed as follows: (1) We incrementally obtain the text spans in the Chinese sentences for the opinion expressions as well as their holders and targets in the English sentence. (2) For each word ei in the English sentence, we find its corresponding word cpi in the Chinese sentence by using pi = arg maxj a(cj |ei ), resulting in"
2021.emnlp-main.796,C18-1071,0,0.0441554,"Missing"
2021.emnlp-main.796,2020.acl-main.627,1,0.858068,"Missing"
2021.emnlp-main.796,P81-1022,0,0.2155,"Missing"
2021.emnlp-main.796,D14-1080,0,0.026599,"nsfer still outperforms the bilingual counterpart. To summarize, in this paper, we have the following contributions: • We manually translate and annotate a Chinese fine-grained ORL corpus for research purposes, especially for the cross-lingual ORL study. • We conduct cross-lingual ORL (to Chinese) through unsupervised model transfer and corpus translation with PGN-Adapter, setting up strong baselines for future research. • We perform extensive experiments and analyses to demonstrate the pros and cons of the different approaches. (2012)’s semi-Markov CRF model outperforms the standard CRF, and Irsoy and Cardie (2014) and Liu et al. (2015) use recurrent neural network for opinion mining. Johansson and Moschitti (2013) and Katiyar and Cardie (2016a) propose joint models for opinion expressions, holders and targets. Opinion Role Labeling As for ORL, Marasovi´c and Frank (2018) exploit multi-task learning about how to use SRL information to improve ORL scores. Zhang et al. (2019a) utilize semantic role labeling to enhance ORL, where three different integrating approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these stu"
2021.emnlp-main.796,J13-3002,0,0.106625,"re research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Car"
2021.emnlp-main.796,P16-1087,0,0.272295,"f fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful"
2021.emnlp-main.796,D17-1302,0,0.0200117,"pose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used f"
2021.emnlp-main.796,W06-0301,0,0.350489,"and multilingual transfers establish a foundation for the future research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et"
2021.emnlp-main.796,P19-1314,0,0.0712203,": (1) sentence translation, (2) manual revision, (3) opinion projection, and (4) manual correction. The first and third steps formalize into automatic corpus translation, which has been used as one approach for unsupervised cross-lingual transfer, and the second and fourth steps are used to ensure the final quality. The whole construction is conducted at the sentence-level. Sentence Translation Neural machine translation (NMT) has achieved state-of-the-art performances for a range of language pairs (Vaswani et al., 2017). In particular, the state-of-the-art NMT can reach a BLEU score over 45 (Li et al., 2019). Thus it is applicable to use NMT for automatic sentence translation. Here we first translate all the English sentences of the MPQA dataset into Chinese by using the google translator2 automatically. Manual Revision Next, we let several native speakers check the translation quality, and make revisions to the imperfect translations. There can be two types of revisions. On the one hand, the translated sentences may have errors, and human 2 intervention is required to correct these issues. On the other hand, the automatic sentences may not match the style of native speakers, and we let our annot"
2021.emnlp-main.796,E17-2002,0,0.0515297,"Missing"
2021.emnlp-main.796,D15-1168,0,0.145657,"neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions S"
2021.emnlp-main.796,L18-1236,0,0.040109,"Missing"
2021.emnlp-main.796,N18-1054,0,0.0350211,"Missing"
2021.emnlp-main.796,P13-2017,0,0.0337103,". (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to"
2021.emnlp-main.796,D11-1006,0,0.0480317,"ing supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used for merging different languages to same space by Adapter and PGN methods with BERT. About the Adapter 2 Related Work method which a pre-trained ne"
2021.emnlp-main.796,P19-1493,0,0.0375505,"Missing"
2021.emnlp-main.796,D18-1039,0,0.0558953,"Missing"
2021.emnlp-main.796,J11-1002,0,0.445765,", 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods:"
2021.emnlp-main.796,K16-1019,0,0.0122267,"5; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to the target 2016a). Here we focus on the opinion role labeling language. The"
2021.emnlp-main.796,Q13-1001,0,0.0127775,"ing approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese"
2021.emnlp-main.796,W14-1614,0,0.0259925,"rforms the bilingual counterpart. To summarize, in this paper, we have the following contributions: • We manually translate and annotate a Chinese fine-grained ORL corpus for research purposes, especially for the cross-lingual ORL study. • We conduct cross-lingual ORL (to Chinese) through unsupervised model transfer and corpus translation with PGN-Adapter, setting up strong baselines for future research. • We perform extensive experiments and analyses to demonstrate the pros and cons of the different approaches. (2012)’s semi-Markov CRF model outperforms the standard CRF, and Irsoy and Cardie (2014) and Liu et al. (2015) use recurrent neural network for opinion mining. Johansson and Moschitti (2013) and Katiyar and Cardie (2016a) propose joint models for opinion expressions, holders and targets. Opinion Role Labeling As for ORL, Marasovi´c and Frank (2018) exploit multi-task learning about how to use SRL information to improve ORL scores. Zhang et al. (2019a) utilize semantic role labeling to enhance ORL, where three different integrating approaches are compared. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these stu"
2021.emnlp-main.796,2020.emnlp-main.180,0,0.025557,"Missing"
2021.emnlp-main.796,N16-1094,0,0.0190372,"models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016"
2021.emnlp-main.796,D18-1268,0,0.0657663,"Missing"
2021.emnlp-main.796,D12-1122,0,0.0256885,"EXPRESSION conveys attitudes includ- methods: model transfer (McDonald et al., 2013; ing sentiments, agreements, beliefs, or intentions Swayamdipta et al., 2016; Daza and Frank, 2019) (e.g., voiced his condolences in Figure 1); the enti- and corpus translation (Zhang et al., 2019b). The ties consist of the HOLDER who expresses the opin- model transfer trains a model on a resource-rich lanion (e.g., Chen.) and the TARGET which the opin- guage by using only language-independent features ion is expressed to (e.g., the families) (Breck et al., such as multilingual BERT (Devlin et al., 2018; 2007; Yang and Cardie, 2012; Katiyar and Cardie, Pires et al., 2019) and then apply it to the target 2016a). Here we focus on the opinion role labeling language. The corpus translation approach firstly (ORL) task which is to identify opinion holders and obtains parallel corpora through either human or ∗ Corresponding author. machine translation and then projects the annota1 We release the code and way of obtaining Chitions from the source language to the target side. nese dataset at https://github.com/zenRRan/ ChineseORL-with-Corpus-Translation. In this work, we present the first study of the 10139 Proceedings of the 20"
2021.emnlp-main.796,P13-1161,0,0.0193987,"foundation for the future research of this task1 . [Chen]holder [voiced his condolences]expression to [the families]target . Figure 1: An example of fine-grained opinion mining. targets (Marasovi´c and Frank, 2017; Zhang et al., 2019a) when the expressions are given. Most of the previous researches focus on the English ORL, benefiting from the benchmark MPQA dataset (Wiebe et al., 2005) which includes spanbased annotations of opinion expressions, holders and targets. The task is commonly solved by sequence labeling models with the BIO conversion scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which"
2021.emnlp-main.796,N19-1066,1,0.866988,"Missing"
2021.emnlp-main.796,D19-1092,1,0.899403,"Missing"
2021.emnlp-main.796,P15-2064,0,0.0178979,"etworks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PGN-Adapter is used for merging different languages to same space by Adapter"
2021.emnlp-main.796,wilson-2008-annotating,0,0.218516,"nce MT system enhances the translation-based approach. 2) Projection strategy is also different from ours (Section 5.1). We choose to project the non-cross labels only, in order to ensure the mapping quality. 3) In addition, the more advanced methods like PGN, Adapter and BERT also play a significant role in cross-lingual tasks. 3 The Construction of Chinese Dataset We manually construct a Chinese ORL dataset to facilitate our research. In order to reduce the overall cost, we exploit corpus translation to assist the construction process, converting the English MPQA corpus (Wiebe et al., 2005; Wilson, 2008) into Chinese. The conversion contains the following four steps by order: (1) sentence translation, (2) manual revision, (3) opinion projection, and (4) manual correction. The first and third steps formalize into automatic corpus translation, which has been used as one approach for unsupervised cross-lingual transfer, and the second and fourth steps are used to ensure the final quality. The whole construction is conducted at the sentence-level. Sentence Translation Neural machine translation (NMT) has achieved state-of-the-art performances for a range of language pairs (Vaswani et al., 2017)."
2021.emnlp-main.796,J09-3003,0,0.173216,"scheme (Kim and Hovy, 2006b; Choi et al., 2006; Yang and Cardie, 2013; Johansson and Moschitti, 2013). Recently, neural BiLSTM-CRF models have achieved state-of-the-art performance on this task (Katiyar and Cardie, 2016b; Marasovi´c and 1 Introduction Frank, 2017; Zhang et al., 2019a). However, the Fine-grained opinion mining has been a crucial task studies on other languages are relatively rare due in natural language processing (NLP) for a long to the scarcity of annotated datasets. To our best time (Kim and Hovy, 2006a; Breck et al., 2007; knowledge, there is only one exception by Almeida Wilson et al., 2009; Qiu et al., 2011; Irsoy and et al. (2015a), which has annotated a small-scale Cardie, 2014; Liu et al., 2015; Wiegand et al., 2016) dataset for the Portuguese language. and it aims to discover useful structural informaUnsupervised cross-lingual transfer (Xu et al., tion of user opinions from unstructured text, which 2018) is one promising way to address the low is the relation between expression and entities such resource problem for ORL. Under the neural setas Who expressed what kind of sentiment towards ting, there are two representative categories of what?. The EXPRESSION conveys attitude"
2021.emnlp-main.796,D14-1187,0,0.0232509,"red. Bo et al. (2020) propose a dependency-based graph convolutional networks to enhance ORL with syntax information. All these studies focus on the English ORL by using supervised models, assuming that a training corpus is already available. In this work, we investigate Chinese ORL, building a benchmark dataset for Chinese manually and then studying unsupervised cross-lingual transferring for the task. Cross-Lingual Transfer Learning Crosslingual transfer learning has been extensively applied in NLP, including sentiment classification (Zhou et al., 2016), POS tagging (Täckström et al., 2013; Wisniewski et al., 2014; Kim et al., 2017), named entity recognition (Zirikly and Hagiwara, 2015), semantic role labeling (Fei et al., 2020), and dependency parsing (McDonald et al., 2011; Tiedemann et al., 2014; Guo et al., 2016; Zhang et al., 2019b). Unsupervised cross-lingual transferring has received great interest (Duong et al., 2015; Xu et al., 2018), which is our major focus. The work of Zhang et al. (2019b) is mostly related to our study, which applies model transferring and corpus translation to dependency parsing. Our work focuses on ORL, applying the two approaches for the Chinese language. PGN-Adapter PG"
2021.findings-emnlp.149,D19-1188,0,0.0211495,", achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. D"
2021.findings-emnlp.149,D14-1082,0,0.188043,"Missing"
2021.findings-emnlp.149,D18-1217,0,0.0295431,"ark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is defined as d = {(h, m, l),"
2021.findings-emnlp.149,P07-1033,0,0.407388,"Missing"
2021.findings-emnlp.149,D18-1498,0,0.02412,"target-domain training data is small. Most recently, Li et al. (2019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the sour"
2021.findings-emnlp.149,W09-1201,0,0.0382346,"rce domain which is a balanced corpus (BC) from news-wire, three target domains which are the product comments (PC) data from Taobao, the product blog (PB) data from Taobao headline, and a web fiction data named “ZhuXian” (ZX). Table 1 shows the detailed illustration of the data statistics. In this work, we pick one target dataset as the target domain, and the rest are the source domains. For example, if the target domain is PC, source domains are BC, PB, and ZX. Evaluation. We use unlabeled attachment score (UAS) and labeled attachment score (LAS) to evaluate the dependency parsing accuracy (Hajic et al., 2009). Each model is trained for at most 1, 000 iterations, and the performance is evaluated on the dev data after each iteration for model selection. We stop the training if the peak performance does not increase in 100 consecutive iterations. Baseline models. To verify the effectiveness and advantage of our proposed model, we select the following approaches as our strong baselines. 1 http://hlt.suda.edu.cn/index.php/ Nlpcc-2019-shared-task • Parameter generation network (PGN). Motivated by Jia et al. (2019), we exploit the PGN based on distributed domain representations to generate domain-related"
2021.findings-emnlp.149,P19-1236,0,0.37234,"n , E) n 0 = BiLSTM (x0 . . . xn , V = W ⊗ E) (6) where ⊗ denotes matrix multiplication; W ∈ RU×D is a parameter matrix to be trained; E ∈ RD is distributed domain-aware sentence representation vector and will be explained later. Distributed domain-aware sentence representation. The distributed domain-aware sentence representation vector can be regarded as a sum of weighted domain embeddings, where higher weights are expected to be assigned to domains that are more similar to the input sentence. First, we compute domain distribution probabilities of each word via simple domain classification. Jia et al. (2019) first propose PGN to generate BiLSTM parameters based on fixed task and domain embeddings for NER domain adaptation, finding  that the PGN can effectively extract domain differzi = softmax MLP hdom (7) i ences. However, the vanilla PGN requires crosswhere hdom is the representation vector of the i-th domain language model task as a bridge to help i word generated by a separated standard BiLSTM. fixed domain embeddings training. Considering the development of pre-training techniques with Then, we compute a distributed domain-aware language model loss and computational complex- word represent"
2021.findings-emnlp.149,2020.acl-main.625,0,0.0360117,"e translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively s"
2021.findings-emnlp.149,P17-1060,0,0.0166215,"te BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (C"
2021.findings-emnlp.149,C16-1038,0,0.0166719,"main NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tr"
2021.findings-emnlp.149,Q16-1023,0,0.0267691,"eveloped (Peng et al., 2019). Intuitively, n, 1 ≤ m ≤ n, l ∈ L}, where (h, m, l) is a depen- an effective exploration of all these corpora can dency from the head word wh to the child word give better performance for the target domain comwm with the relation label l ∈ L. pared with the single-source domain adaptation. Recently, supervised neural dependency parsSeparating domain-invariant and domaining models have achieved great success, leading specific features is one popular way for domain to impressive performance (Chen and Manning, adaptation to distinguish the similarity and discrep2014; Kiperwasser and Goldberg, 2016; Dozat and ancy of different domains (Daumé III, 2007; Kim Manning, 2017; Li et al., 2019a). Remarkably, the et al., 2016; Sato et al., 2017). Domain-invariant BiAffine parsing model can obtain a UAS of 96.67 features indicate the shared feature space across ∗ Corresponding author domains, which have been widely-adopted as 1724 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1724–1733 November 7–11, 2021. ©2021 Association for Computational Linguistics knowledge transferring. Domain-specific features imply the differences between domains, which could be helpful if"
2021.findings-emnlp.149,P08-1068,0,0.0956171,"s. and a LAS of 95.03 on standard Penn Treebank benchmark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from diff"
2021.findings-emnlp.149,2020.coling-main.338,1,0.883628,"language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is defined as d = {(h, m, l), 0 ≤ h ≤ have bee"
2021.findings-emnlp.149,P19-1229,1,0.815406,"exploration of all these corpora can dency from the head word wh to the child word give better performance for the target domain comwm with the relation label l ∈ L. pared with the single-source domain adaptation. Recently, supervised neural dependency parsSeparating domain-invariant and domaining models have achieved great success, leading specific features is one popular way for domain to impressive performance (Chen and Manning, adaptation to distinguish the similarity and discrep2014; Kiperwasser and Goldberg, 2016; Dozat and ancy of different domains (Daumé III, 2007; Kim Manning, 2017; Li et al., 2019a). Remarkably, the et al., 2016; Sato et al., 2017). Domain-invariant BiAffine parsing model can obtain a UAS of 96.67 features indicate the shared feature space across ∗ Corresponding author domains, which have been widely-adopted as 1724 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1724–1733 November 7–11, 2021. ©2021 Association for Computational Linguistics knowledge transferring. Domain-specific features imply the differences between domains, which could be helpful if the domain gaps could be accurately measured and effectively modeled. The learning of dom"
2021.findings-emnlp.149,P13-2017,0,0.0950635,"Missing"
2021.findings-emnlp.149,D17-1155,0,0.0228326,") propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation corpus and the essential difficulty of multi-source domain adaptation, there still lacks such studies on dependency parsing. Inspired by these prior works, we propose a novel approach to separate domain-invariant and domain-specific features by the utilization of adversarial and parameter generation networks. Domain adaptation has been extensively studied in many research areas, including machine learning (Wang et al., 2017; Kim et al., 2017), computer vision (Ganin and Lempitsky, 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples"
2021.findings-emnlp.149,2020.emnlp-main.639,0,0.0244174,"l. Most recently, Li et al. (2019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Rece"
2021.findings-emnlp.149,W15-2201,0,0.0135495,") and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1731 baselines, eve"
2021.findings-emnlp.149,D18-1041,0,0.0243399,"019b) propose to leverage an extra domain embedding to indicate domain source and achieve better performance on semi-supervised domain adaptation. In this work, we adjust the domain embedding method as our strong baseline. Multi-source domain adaptation. Multisource domain adaptation assumes the training data comes from multiple source domains. Many approaches of multi-source domain adaptation focus on leveraging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of wo"
2021.findings-emnlp.149,D18-1039,0,0.028153,"ging domain knowledge to extract domainrelated features, thus boosting the performance of target domain (Daumé III, 2007; Guo et al., 2018; Li et al., 2020a; Wright and Augenstein, 2020). Zeng et al. (2018) design a domain classifier and an adversarial network to capture domain-specific and domain-invariant features, achieving good performances on machine translation. Guo et al. (2018) apply meta-training and adversarial learning to compute the point-to-set distance as the weights of multi-task learning network, leading to improvement on classification tasks. As another interesting direction, Platanios et al. (2018) propose a parameter generation network to generate the parameters of the encoder and decoder by accepting the source and target language embeddings as input. Recently, a number of works attempt to use the parameter generation network to improve the cross-domain or cross-language performance (Cai et al., 2019; Stoica et al., 2020; Jin et al., 2020; Nekvinda and Dusek, 2020). Particularly, Jia et al. (2019) propose to generate BiLSTM parameters based on task and domain representation vectors, leading to very promising performances on cross-domain NER task. Due to the limitation of annotation co"
2021.findings-emnlp.149,P07-1078,0,0.0809956,", 2015; Rozantsev et al., 2019) and natural language processing (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1"
2021.findings-emnlp.149,N01-1023,0,0.169558,"ng (Kim et al., 2016; Sun et al., 2020). Here, we first simply review single-source domain adaptation researches, and then give more detailed illustration about the studies of multi-source domain adaptation. Single-source domain adaptation. Singlesource domain adaptation assumes training data comes from a source domain. Due to lacking targetdomain labeled data, previous researches mainly investigate unsupervised domain adaptation, which attempt to create pseudo training samples by selftraining (Charniak, 1997; Steedman et al., 2003; Reichart and Rappoport, 2007; Yu et al., 2015), co-training (Sarkar, 2001), or tri-training (Li et al., 2019c). However, selecting high confidence samples is a challenge. Thanks to large-scale labeled web data released by parsing communities, recent existing works pay 6 Conclusion more attention to semi-supervised scenario. Yu et al. (2013) give detailed error analysis on cross- This work for the first time apply the APGN apdomain dependency parsing and solve the ambigu- proach to multi-source cross-domain dependency ous features problem. Sato et al. (2017) propose to parsing, obtaining better performance than multiple 1731 baselines, even when all models are enhanc"
2021.findings-emnlp.149,K17-3007,0,0.320371,"enn Treebank benchmark for the English language. In order to obtain competitive performance, supervised dependency parsing models rely on a sufficient amount of training data, which is inevitably dominated to several fixed domains. When the test data is sourced from similar domains, good performance could be achieved. However, the performance could be decreased significantly when the test data is from a different domain which has a large gap between the training domains. Thus domain adaptation for dependency parsing has been concerned by a number of studies (Koo et al., 2008; Yu et al., 2013; Sato et al., 2017; Clark et al., 2018; Li et al., 2020b). These works mostly focus on single-source cross-domain dependency parsing, 1 Introduction assuming the training data is from a single source Dependency parsing aims to derive syntactic and domain (Yu et al., 2013; Sato et al., 2017). In fact, semantic tree structures over input words (Mc- multi-source cross-domain dependency parsing is a Donald et al., 2013). Given an input sentence more practical setting, considering that several des = w1 w2 . . . wn , a dependency tree, as depicted pendency parsing corpora from different domains in Figure 1, is define"
C12-1188,J04-4004,0,0.0330972,"rithm that is first proposed by (Wolpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Depen"
C12-1188,D12-1133,0,0.0193385,"out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS taggin"
C12-1188,D07-1101,0,0.172376,"nd the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error pattern"
C12-1188,A00-2018,0,0.0563471,"olpert, 1992) and (Breiman, 1996). It has been exploited in a number of NLP tasks for integration. We mainly concern the works of stacked learning applied on POS tagging and dependency parsing. The work of (Li et al., 2011a) presented a mostly recent work for stacking POS taggers. They exploit the output of a CRF POS tagger to help a perceptronbased POS tagger with syntactic features. (McDonald, 2006) proposed the first stacking work of dependency parsing. The author incorporated parse decisions of two constituent-based parsers, Collins parser (Collins, 1999; Bikel, 2004) and Charniak parser (Charniak, 2000), into the second-order MST parser. Then (Nivre and McDonald, 2008) suggested integrating graph- and transition-based models by stacking, and more detailed analysis was given in (McDonald and Nivre, 2011). (Martins et al., 2008) also demonstrated that stacking transition- and graphbased parsers can improve parsing performance significantly and meanwhile offer theoretical interpretations for stacking. In our paper, stacked leaning is applied on the joint tasks of Chinese POS tagging and dependency parsing. 3 Two Models for Joint Chinese POS Tagging and Dependency parsing A dependency tree for a"
C12-1188,P05-1022,0,0.0310453,"specially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat"
C12-1188,P12-2003,1,0.910792,"stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing acc"
C12-1188,W02-1001,0,0.0331342,"odifier word (or child) is w m . The task of dependency parsing is to find an optimum dependency tree d for the input sentence x. Generally, the POS tag sequence of the sentence t = t 1 · · · t n (where t i ∈ T, 1 ≤ i ≤ n, T is the POS tag set) is taken as an input for dependency parsing, which is determined by the task of POS tagging, thus forming a pipeline model of the two tasks. POS tagging is a typical sequence labeling problems which can be resolved by algorithms such as maximum-entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and averaged perceptron (Collins, 2002). The goal of joint models of the two tasks is to find an optimum dependency tree and an optimum POS ˆ for x concurrently. tag sequence (ˆt, d) 3.1 Graph-based Joint Model The graph-based joint model is first proposed by (Li et al., 2011b). Such a model is extended from a graph-based model for dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). In the model, the score of a dependency tree along with POS tags on each node is factored into scores of small parts. (Li et al., 2011b) have introduced several different graph-based joint model"
C12-1188,W03-0433,0,0.0480722,"Missing"
C12-1188,I11-1136,0,0.107354,"tem: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependen"
C12-1188,P10-1110,0,0.0722255,"cy Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagg"
C12-1188,P10-1001,0,0.0624319,"ansition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different"
C12-1188,I11-1171,1,0.250948,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D11-1109,1,0.320009,"Mumbai, December 2012. 3071 1 Introduction Part-of-speech (POS) tagging and dependency parsing are two fundamental natural language processing (NLP) tasks. Typically, POS tagging is a preprocessing step for dependency parsing, especially in a pipeline architecture. There are two main problems in a pipeline system: (1) Dependency parsing suffers the problem of error propagation; (2) POS tagging cannot exploit useful, important syntactic information for disambiguation. For Chinese POS tagging and dependency parsing, a pipeline system seriously suffers these two problems. The study presented in (Li et al., 2011b) demonstrates the error propagation factor. The authors develop a graph-based joint model for Chinese POS tagging and dependency parsing. The most interesting thing they found is that even with lower tagging accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked lear"
C12-1188,D08-1017,0,0.11201,"he performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic con"
C12-1188,P05-1012,0,0.376473,"dels including the guided graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al.,"
C12-1188,J11-1007,0,0.0608408,"ey propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parse"
C12-1188,E06-1011,0,0.241065,"ed graph-based joint model and the guided transition-based joint model. Section 5 describes our constituent-based model and the further stacking model. Section 6 reports the experimental results. Section 7 gives the systematic analysis of the joint models. Finally, in Section 8 we conclude this paper and point out our future works. 2 Related Works Related Works on Joint Models of Chinese POS Tagging and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certa"
C12-1188,J08-4003,0,0.161113,"and Dependency Parsing (Li et al., 2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG pars"
C12-1188,P08-1108,0,0.37237,"odel can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that,"
C12-1188,P06-1055,0,0.370553,"that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent"
C12-1188,N07-1051,0,0.546016,"of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can als"
C12-1188,C10-1120,0,0.0226267,"ndency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) p"
C12-1188,P11-1139,0,0.0211513,"ing accuracy, a joint model could achieve higher parsing accuracy. The work presented in (Hatori et al., 2011) also demonstrates a joint model can largely improve the performance of dependency parsing further. They propose a transition-based joint model for Chinese POS tagging and dependency parsing. Recently, ensemble models have been gained a lot of interests in NLP community. Stacked learning (stacking) (Wolpert, 1992; Breiman, 1996), which is a typical method for ensemble models, has been applied to a number of NLP tasks for its elegance and conciseness, such as Chinese Word segmentation (Sun, 2011), POS tagging (Li et al., 2011a), named entity recognition (Dekai Wu and Carpuat, 2003) and dependency parsing (McDonald, 2006; Nivre and McDonald, 2008; Martins et al., 2008; Søgaard and Rishøj, 2010; McDonald and Nivre, 2011). Especially, (Nivre and McDonald, 2008) demonstrate that the performance of dependency parsing can be largely improved by stacking a graph-based dependency parser and a transitionbased dependency parser. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from grap"
C12-1188,P12-1026,0,0.110473,"uent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 2012) proposed to improve the Chinese parsing accuracy by a PCFG parser. Similarly, (Sun and Uszkoreit, 2012) exploited a PCFG parser to enhance Chinese POS tagging. Thus it is reasonable to investigate the performance of constituent-based joint models and to improve the performance of joint Chinese POS tagging and dependency parsing by a constituent-based joint model. In this paper, first we study the integration of a graph-based joint model (JGraph) and a transition-based joint model (JTrans) by stacked learning. The stacked learning is implemented using a two-level architecture, where the level-0 consists of one or more predictors of which the results are exploited as input to enhance the level-1"
C12-1188,W03-3023,0,0.373203,"er. Thus it is interesting to investigate the effect of stacked learning when it is applied to joint models. Graph- and transition-based joint models are extended from graph- and transition-based models of dependency parsing respectively. They are the two mainstream approaches for dependency parsing. It is noteworthy that, the probabilistic context-free grammar (PCFG) parsers, such as Brown parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), which are traditionally used for constituent parsing, have also been suggested for dependency parsing (Yamada and Matsumoto, 2003; McDonald, 2006; Sun, 2012; Che et al., 2012). We denote these methods by constituent-based models. The precondition of constituentbased models is that the output constituent structure of the PCFG parsers can be transformed into dependency structure by rules adequately. This is satisfied for Chinese. Moreover, the PCFG models can process POS tagging simultaneously in constituent parsing. They treat POS tagging as a submodule of constituent parsing. Thus we can also adopt a PCFG model for joint Chinese POS tagging and dependency parsing. We denote it by constituent-based joint model. (Sun, 201"
C12-1188,D08-1059,0,0.0212821,"odels, the probability should be much lower with a negative impact. We name the features related with the consistency of the two level-0 models as guided consistent features. Table 2 lists the guided consistent features used in this work. Both the guided graph-based joint model and the guided transition-based joint model are considered. 6 6.1 Experiments Experimental Settings We use CTB5.1 to conduct our experiments. Following the works of (Li et al., 2011b) and (Hatori et al., 2011), we use the standard split of CTB5.1 described in (Duan et al., 2007) and the conversion rules of CS-to-DS in (Zhang and Clark, 2008). We use the standard tagging accuracy to evaluate POS tagging. For dependency parsing, we use word accuracy (also known as dependency accuracy or UAS), root accuracy and complete 3078 The Guided Graph-based Joint Model: JGraph(JTrans, JConst) pos dep JTrans JConst JTrans JTrans {Whether ˆt m is identical to ˆt m ?} ⊗{ˆt m ◦ t m , ˆt m ◦ wm ◦ t m } ˆ JTrans and d ˆ JConst ?} ⊗ {Whether hx m is in {Whether the heads of m are identical in d ˆ JTrans ?} ⊗{t h , t m , t h ◦ t m } d The Guided Transition-based Joint Model: JTrans(JGraph, JConst) pos syn JGraph JConst JGraph JGraph {Whether ˆt m is"
C12-1188,P11-2033,0,0.0805884,"2011b) present the first joint model for Chinese POS tagging and dependency parsing. They extend models of graph-based dependency parsing (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), making them enable to handle POS tagging simultaneously. They conclude that joint models can achieve better performance in dependency parsing and can do much better some certain POS tagging error patterns. Secondly, (Hatori et al., 2011) and (Bohnet and Nivre, 2012) propose joint models based on transition-based dependency parsing (Nivre, 2008; Huang and Sagae, 2010; Zhang and Nivre, 2011). (Hatori et al., 2011) also examine the results of their joint model carefully, demonstrating similar conclusions to that of (Li et al., 2011b). However, their certain error patterns are slightly different with that of (Li et al., 2011b). The differences may be induced by the different manners of modeling the joint task. Thirdly, the constituent-based joint model processes joint Chinese POS tagging and dependency parsing in an indirectly way. It performs the POS tagging and dependency parsing by a conversion from the initial output of a PCFG parser. In Chinese POS tagging, (Sun and Uszkoreit,"
C12-1188,J03-4003,0,\N,Missing
C14-1051,D12-1133,0,0.168726,"r the Zhang scheme, and increases of 0.30 on UAS and 0.36 on LAS for the Stanford scheme. The results also demonstrate similar conclusions with the experiments on English dataset. 5 Related Work Our work is mainly inspired by the work of joint models. There are a number of successful studies on joint modeling pipelined tasks where one task is a prerequisite step of another task, for example, the joint model of word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees"
C14-1051,Q13-1034,0,0.0186811,"nd Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have"
C14-1051,P13-1104,0,0.0498278,"r the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues, 2007). There are different conversion schemes for the Chinese Penn Tr"
C14-1051,P04-1015,0,0.0101157,"top element S0 and the second top element S1 on the stack, with the dependency label being specified by l; and the pop-root action defines the root node of a dependency tree when there is only one element on the stack and no element in the queue. During decoding, each state may have several actions. We employ a fixed beam to reduce the search space. The low-score states are pruned from the beam when it is full. The feature templates in our baseline are shown by Table 1, referring to baseline feature templates. We learn the feature weights by the averaged percepron algorithm with early-update (Collins and Roark, 2004; Zhang and Clark, 2011). 3 The Proposed Joint Model The aforementioned baseline model can only handle a single dependency tree. In order to parse multiple dependency trees for a sentence, we usually use individual dependency parsers. This method is not able to exploit the correlations across different dependency schemes. The joint model to parse multiple dependency trees with a single model is an elegant way to exploit these correlations fully. Inspired by this, we make a novel extension to the baseline arc-standard transition system, arriving at a joint model to parse two heterogeneous depen"
C14-1051,W08-1301,0,0.151442,"Missing"
C14-1051,N13-1070,0,0.051856,"Missing"
C14-1051,I11-1136,0,0.0705947,"it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamad"
C14-1051,P12-1110,0,0.240838,"lations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. Th"
C14-1051,D09-1127,0,0.595101,"full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH schem"
C14-1051,P08-1102,0,0.0759747,"Missing"
C14-1051,P09-1059,0,0.241947,"se two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang et al., 2009). The transition-based framework is easily adapted to a number of joint models, including joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing (Hatori e"
C14-1051,W07-2416,0,0.0902035,"Missing"
C14-1051,P10-1001,0,0.141219,"elations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 2008) and the LTH scheme (Johansson and Nugues"
C14-1051,P09-1058,0,0.0793126,"rom the Yamada scheme and the label “pobj” from the Stanford scheme on a same dependency “atypoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a"
C14-1051,D11-1109,1,0.915722,"typoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments"
C14-1051,P12-1071,1,0.867181,"ectively exploited according to the above analysis. We assume that the first and second processing schemes are s1 and s2 respectively, to facilitate the below descriptions. We can see that the joint model behaves similarly to a pipeline reranking model, in optimizing scheme s1 ’s parsing performances. First we get K-best (K equals the beam size of the joint model) candidates for scheme s1 , and then employ additional evidences from scheme s2 ’s result, to rerank the K-best candidates, obtaining a better result. The joint model also behaves similarly to a pipeline feature-based stacking model (Li et al., 2012), in optimizing scheme s2 ’s parsing performances. After acquiring the best result of scheme s1 , we can use it to generate guided features to parse dependencies of scheme s2 . Thus additional information from scheme s1 can be imported into the parsing model of scheme s2 . Different with the pipeline reranking and the feature-based stacking models, we employ a single model to achieve the two goals, making the interactions between the two schemes be better performed. 4 Experiments 4.1 Experimental Settings In order to evaluate the baseline and joint models, we conduct experiments on English and"
C14-1051,P13-2109,0,0.0479035,"LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results wi"
C14-1051,P05-1012,0,0.146151,"Missing"
C14-1051,J08-4003,0,0.0685433,"Missing"
C14-1051,N12-1054,0,0.272811,"heme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford p"
C14-1051,P13-1014,0,0.0321302,"Missing"
C14-1051,P12-1025,0,0.32961,"dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011), extended from a arc-standard transition-based parsing model (Huang et al., 2009). The transition-based framework is easily adapted to a number of joint models, including joint word segmentation and POS-tagging (Zhang and Clark, 2010), the joint POS-tagging and parsing (Hatori et al., 2012; Bohnet"
C14-1051,W03-3023,0,0.121476,"Missing"
C14-1051,D08-1059,0,0.558312,"t model which can make full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models. 1 Introduction Dependency parsing has been intensively studied in recent years (McDonald et al., 2005; Nivre, 2008; Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and Nivre, 2011; Sartorio et al., 2013; Choi and McCallum, 2013; Martins et al., 2013). Widely-used corpus for training a dependency parser is usually constructed according to a specific constituent-to-dependency conversion scheme. Several conversion schemes for certain languages have been available. For example, the English language has at least four schemes based on the Penn Treebank (PTB), including the Yamada scheme (Yamada and Matsumoto, 2003), the CoNLL 2007 scheme (Nilsson et al., 2007), the Stanford scheme (de Marneffe and Manning, 200"
C14-1051,D10-1082,0,0.259738,"the label “pobj” from the Stanford scheme on a same dependency “atypoint” can make it more reliable than one alone. In this paper, we investigate the influences of the correlations between different dependency schemes on parsing performances. We propose a joint model to parse heterogeneous dependencies from two schemes simultaneously, so that the correlations can be fully used by their interactions in a single model. Joint models have been widely studied to enhance multiple tasks in NLP community, including joint word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transi"
C14-1051,J11-1005,0,0.149178,"d POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Li et al., 2011; Hatori et al., 2011), and the joint word segmentation, POS-tagging and dependency parsing (Hatori et al., 2012). These models are proposed over pipelined tasks. We apply the joint model into parallel tasks, and parse heterogeneous dependencies together. To our knowledge, we are the first work to investigate joint models on parallel tasks. We exploit a transition-based framework with global learning and beam-search decoding to implement the joint model (Zhang and Clark, 2011). The joint model is extended from a state-of-the-art transition-based dependency parsing model. We conduct experiments on PTB with the Yamada and the Stanford schemes, and also on CTB 5.1 with the Zhang and the Stanford schemes. The results show that our joint model gives improved performances over the individual baseline models for both schemes on both English and Chinese languages, demonstrating positive effects of the correlations between the two schemes. We make the source code freely available at http://sourceforge.net/ projects/zpar/,version0.7. 2 Baseline Traditionally, the dependency"
C14-1051,D12-1030,0,0.0156056,"h the Zhang and the Stanford schemes. The results show that our joint model gives improved performances over the individual baseline models for both schemes on both English and Chinese languages, demonstrating positive effects of the correlations between the two schemes. We make the source code freely available at http://sourceforge.net/ projects/zpar/,version0.7. 2 Baseline Traditionally, the dependency parsers of different schemes are trained with their corpus separately, using a state-of-the-art dependency parsing algorithm (Zhang and Clark, 2008; Huang et al., 2009; Koo and Collins, 2010; Zhang and McDonald, 2012; Choi and McCallum, 2013). In this work, we exploit a transition-based arc-standard dependency parsing model combined with global learning and beam-search decoding as the baseline. which is initially proposed by Huang et al. (2009). In the following, we give a detailed description of the model. In a typical transition-based system for dependency parsing, we define a transition state, which consists of a stack to save partial-parsed trees and a queue to save unprocessed words. The parsing is performed incrementally via a set of transition actions. The transition actions are used to change cont"
C14-1051,P14-2107,0,0.0472412,"rd scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the numbers for the Stanford dependencies fro"
C14-1051,P11-2033,0,0.160805,"ures proposed by us. For the Yamada scheme, losses of 0.15 on UAS and 0.21 on LAS are shown without the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the"
C14-1051,D13-1093,0,0.10027,"the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the"
C14-1051,P13-1013,1,0.684759,"the new features. While for Stanford scheme, larger decreases are shown by 0.57 on UAS and 0.58 on LAS, respectively. The results demonstrate the new features are effective in the joint model. 535 Model Our joint model Our joint model/wo ∆ UAS 93.04 92.89 -0.15 Yamada LAS 92.01 91.80 -0.21 CM 48.65 48.25 -0.40 UAS 93.52 92.95 -0.57 Stanford LAS 91.15 90.57 -0.58 CM 52.59 50.62 -1.97 Table 3: Feature ablation results. Yamada UAS LAS CM Baseline 92.71 91.67 47.48 Our joint model 92.89 91.86 48.39 Zhang and Nivre (2011) 92.9 91.8 48.0 Rush and Petrov (2012) – – – 93.07 – – Martins et al. (2013) Zhang et al. (2013a) 93.50 92.41 – Zhang and McDonald (2014) 93.57 92.48 – Kong and Smith (2014) – – – Model Stanford UAS LAS 92.72 90.61 93.30‡ 91.19‡ – – 92.7∗ – 92.82∗ – 93.64∗ 91.28∗ 93.71∗ /93.01∗∗ 91.37∗ /90.64∗∗ 92.20∗∗ 89.67∗∗ CM 47.76 50.37 – – – – – – Table 4: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. Our Stanford dependencies are slightly different with previous works, where the results with mark ∗ show the numbers for the Stanford dependencies from Stanford parser version 2.0.5 and the results with mark ∗∗ show the"
C14-1051,P14-1125,1,0.90386,"word segmentation and POS-tagging (Jiang et al., 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010), the joint model of POS-tagging and parsing (Li et al., 2011; Hatori et al., 2011; Bohnet and Nivre, 2012), the joint model of word segmentation, POS-tagging and parsing (Hatori et Model Baseline Our joint model UAS 79.07 80.20‡ Zhang LAS 76.08 77.07‡ CM 27.96 30.10 UAS 80.33 80.63 Stanford LAS CM 75.29 31.14 75.65 31.20 Table 6: The final results on the test data set, where the results with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All"
C14-1051,P13-2019,0,0.0201629,"s with mark ‡ demonstrates that the p-value is below 10−3 using t-test. 537 al., 2012; Zhang et al., 2013b; Zhang et al., 2014), and the joint model of morphological and syntactic analysis tasks (Bohnet et al., 2013). In our work, we propose a joint model on parallel tasks, to parse two heterogeneous dependency trees simultaneously. There has been a line of work on exploiting multiple treebanks with heterogeneous dependencies to enhance dependency parsing. Li et al. (2012) proposed a feature-based stacking model to enhance a specific target dependency parser with the help of another treebank. Zhou and Zhao (2013) presented a joint inference framework to combine the parsing results based on two different treebanks. All these work are case studies of annotation adaptation from different sources, which have been done for Chinese word segmentation and POS-tagging as well (Jiang et al., 2009; Sun and Wan, 2012). In contrast to their work, we study the heterogeneous annotations derived from the same source. We use a unified model to parsing heterogeneous dependencies together. Our joint parsing model exploits a transition-based framework with global learning and beam-search decoding (Zhang and Clark, 2011),"
C14-1051,D07-1096,0,\N,Missing
C16-1231,K16-1017,0,0.588274,"of tweet content features, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2449–2460, Osaka, Japan, December 11-17 2016. making manual feature engineering unnecessary. Such neural features can capture long-range and subtle semantic patterns,"
C16-1231,D14-1082,0,0.0158272,"80 100 120 140 160 180 200 Figure 4: Developmental results with respect to different number of history tweet words. 6.3.1 Initialization of Word Embeddings We use the neural model with only local features to evaluate the effect of different word embedding initialization methods. As shown in Figure 2(a), a better accuracy is obtained by using GloVe embeddings for initialization compared with random initialization. The finding is consistent with previous results in the literature on other NLP tasks, which show that pre-trained word embeddings can bring better accuracies (Collobert et al., 2011; Chen and Manning, 2014). 6.3.2 Differentiating Local and Contextual Word Embeddings We can obtain embeddings of contextual tweet words using the same looking-up function as target tweet words, thereby giving each word a unique embedding regardless whether it comes from the target tweet to classify or its history tweets. However, the behavior of contextual tweet words should intuitively be different, because they are used as different features. An interesting research question is that whether separate embeddings lead to improved results. We investigate the question by using two embedding look-up matrices E and E 0 ,"
C16-1231,W14-4012,0,0.0217807,"Missing"
C16-1231,D14-1179,0,0.0042832,"Missing"
C16-1231,J81-4005,0,0.733725,"Missing"
C16-1231,W10-2914,0,0.549973,"89; Utsumi, 2000; Gibbs and Colston, 2007). Detecting sarcasm automatically is useful for opinion mining and reputation management, and hence has received growing interest from the natural language processing community (Joshi et al., 2016a). Social media such as Twitter exhibit rich sarcasm phenomena, and recent work on automatic sarcasm detection has focused on tweet data. Tweet sarcasm detection can be modeled as a binary document classification task. Two main sources of features have been used. First, most previous work extracts rich discrete features according to the tweet content itself (Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Riloff et al., 2013; Pt´acˇ ek et al., 2014), including lexical unigrams, bigrams, tweet sentiment, word sentiment, punctuation marks, emoticons, quotes, character ngrams and pronunciations. Some of these work uses more sophisticated features, including POS tags, dependency-based tree structures, Brown clusters and sentiment indicators, which depend on external resources. Overall, ngrams have been among the most useful features. Second, recent work has exploited contextual tweet features for sarcasm de"
C16-1231,C14-1008,0,0.0787993,"ng a set of statistical indicators extracted from both the target tweet and relevant history tweets. Bamman and Smith (2015) study the influences of tweet content features, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2449–2460, Osaka, Japan, D"
C16-1231,filatova-2012-irony,0,0.0138126,"al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015).. To our knowledge, we are among the first to investigate the effect of neural networks on this task (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). Corpora. With respect to sarcasm corpora, early work relied on small-scale manual annotation. Filatova (2012) constructed a sarcasm corpus from Amazon product reviews using crowdsourcing. Davidov et al. (2010) discussed the strong influence of hashtags on sarcasm detection. Inspired by this, Gonz´alezIb´anez et al. (2011) used sarcasm-related hashtags as gold labels for sarcasm, creating a tweet corpus by treating tweets without such hashtags as negative examples. Their work is similar in spirit to the work of Go et al. (2009), who constructed a tweet sentiment automatically by taking emoticons as gold sentiment labels. The method of Gonz´alez-Ib´anez et al. (2011) was adopted by Pt´acˇ ek et al. (20"
C16-1231,W16-0425,0,0.628753,"eatures, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2449–2460, Osaka, Japan, December 11-17 2016. making manual feature engineering unnecessary. Such neural features can capture long-range and subtle semantic patterns, which are difficult to"
C16-1231,P11-2102,0,0.133285,"Missing"
C16-1231,P15-2124,0,0.0614254,"ve been exploited to enhance sarcasm detection. Tsur et al. (2010) applied features based on semi-supervised syntactic patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that comprehensively studies the effect of various features (Gonz´alez-Ib´anez et al., 2011; Gonz´alez-Ib´anez et al., 2011; Joshi et al., 2015). Recently, contextual information has been exploited for sarcasm detection (Wallace et al., 2015; Karoui et al., 2015). In particular, contextual features extracted from history tweets by the same author has shown great effectiveness for tweet sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamma"
C16-1231,D16-1104,0,0.225986,"l network to extract contextual features automatically from history tweets. Results show that neural features give improved accuracies for sarcasm detection, with different error distributions compared with discrete manual features. 1 Introduction Sarcasm has received much research attention in linguistics, psychology and cognitive science (Gibbs, 1986; Kreuz and Glucksberg, 1989; Utsumi, 2000; Gibbs and Colston, 2007). Detecting sarcasm automatically is useful for opinion mining and reputation management, and hence has received growing interest from the natural language processing community (Joshi et al., 2016a). Social media such as Twitter exhibit rich sarcasm phenomena, and recent work on automatic sarcasm detection has focused on tweet data. Tweet sarcasm detection can be modeled as a binary document classification task. Two main sources of features have been used. First, most previous work extracts rich discrete features according to the tweet content itself (Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Riloff et al., 2013; Pt´acˇ ek et al., 2014), including lexical unigrams, bigrams, tweet sentiment, word sentiment, punctuati"
C16-1231,P14-1062,0,0.0171835,"dataset for Czech. More recently, both Rajadesingan et al. (2015) and Bamman and Smith (2015) followed the method for building a sarcasm corpus. We take the corpus of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Gho"
C16-1231,P15-2106,0,0.0204433,"patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that comprehensively studies the effect of various features (Gonz´alez-Ib´anez et al., 2011; Gonz´alez-Ib´anez et al., 2011; Joshi et al., 2015). Recently, contextual information has been exploited for sarcasm detection (Wallace et al., 2015; Karoui et al., 2015). In particular, contextual features extracted from history tweets by the same author has shown great effectiveness for tweet sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015).. To our knowledge, we are among the first to investigate the effect of neural networks on this task"
C16-1231,W07-0101,0,0.189299,"ly better accuracies compared to the discrete baseline, demonstrating the advantage of the automatically extracted neural features in capturing global semantic information. Further analysis shows that features from history tweets are as useful to the neural model as to the discrete model. We make our source code publicly available under GPL at https://github.com/zhangmeishan/SarcasmDetection. 2 Related Work Features. Sarcasm detection is typically regarded as a classification problem. Discrete models have been used and most existing research efforts have focused on finding effective features. Kreuz and Caucci (2007) studied lexical features for sarcasm detection, finding that words, such as interjections and punctuation, are effective for the task. Carvalho et al. (2009) demonstrated that oral or gestural expressions represented by emoticons and special keyboard characters are useful indicators of sarcasm. Both Kreuz and Caucci (2007) and Carvalho et al. (2009) rely on unigram lexical features for sarcasm detection. More recently, Lukin and Walker (2013) extended the idea by using n-gram features as well as lexicon-syntactic patterns. External sources of information have been exploited to enhance sarcasm"
C16-1231,W13-1104,0,0.0460713,"ally regarded as a classification problem. Discrete models have been used and most existing research efforts have focused on finding effective features. Kreuz and Caucci (2007) studied lexical features for sarcasm detection, finding that words, such as interjections and punctuation, are effective for the task. Carvalho et al. (2009) demonstrated that oral or gestural expressions represented by emoticons and special keyboard characters are useful indicators of sarcasm. Both Kreuz and Caucci (2007) and Carvalho et al. (2009) rely on unigram lexical features for sarcasm detection. More recently, Lukin and Walker (2013) extended the idea by using n-gram features as well as lexicon-syntactic patterns. External sources of information have been exploited to enhance sarcasm detection. Tsur et al. (2010) applied features based on semi-supervised syntactic patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that com"
C16-1231,D14-1162,0,0.104612,"omputed by using logistic regression over the output vector o in Eq (1) and (2) for the discrete and neural models, respectively. Online AdaGrad (Duchi et al., 2011) is used to minimize the objective function for both discrete and neural models. All the matrix and vector parameters are initialized by uniform sampling in (−0.01, 0.01). The initial values of the embedding matrix E can be assigned either by using the same random initialization as the other parameters, or by using word embeddings pre-trained over a large-scale tweet corpus. We obtain pre-trained tweet word embeddings using GloVe (Pennington et al., 2014)3 . Embeddings are fine-tuned during training, with E belonging to model parameters. 6 Experiments 6.1 Experimental Settings 6.1.1 Data We use the dataset of Rajadesingan et al. (2015) to conduct our experiments, collected by querying the Twitter API using the keywords #sarcasm and #not, and filtering retweets and non-English tweets automatically. In total, Rajadesingan et al. (2015) collected 9,104 tweets that are self-described as sarcasm by the authors. We stream the tweet corpus using the tweet IDs they provide.4 We remove the #sarcasm and #not hashtags from the tweets, assigning to them t"
C16-1231,C14-1022,0,0.409337,"has received growing interest from the natural language processing community (Joshi et al., 2016a). Social media such as Twitter exhibit rich sarcasm phenomena, and recent work on automatic sarcasm detection has focused on tweet data. Tweet sarcasm detection can be modeled as a binary document classification task. Two main sources of features have been used. First, most previous work extracts rich discrete features according to the tweet content itself (Davidov et al., 2010; Tsur et al., 2010; Gonz´alez-Ib´anez et al., 2011; Reyes et al., 2012; Reyes et al., 2013; Riloff et al., 2013; Pt´acˇ ek et al., 2014), including lexical unigrams, bigrams, tweet sentiment, word sentiment, punctuation marks, emoticons, quotes, character ngrams and pronunciations. Some of these work uses more sophisticated features, including POS tags, dependency-based tree structures, Brown clusters and sentiment indicators, which depend on external resources. Overall, ngrams have been among the most useful features. Second, recent work has exploited contextual tweet features for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). Intuitively, the history behaviors for a tweet author can be a good indicato"
C16-1231,D13-1066,0,0.45437,"Missing"
C16-1231,D13-1170,0,0.0286654,"ach to model sarcasm, using a set of statistical indicators extracted from both the target tweet and relevant history tweets. Bamman and Smith (2015) study the influences of tweet content features, author features, audience features and environment features, finding that contextual features are very useful for tweet sarcasm detection. So far, most existing sarcasm detection methods in the literature leverage discrete models. While on the other hand, neural network models have gained much attention for related tasks such as sentiment analysis and opinion extraction, achieving the best results (Socher et al., 2013; dos Santos and Gatti, 2014; Vo and Zhang, 2015; Zhang et al., 2016). Success on these tasks shows potentials of neural network on sarcasm detection (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). There are two main advantages of using neural models. First, neural layers are used to induce features automatically, This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2449 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, page"
C16-1231,P15-1098,0,0.00514376,"a sarcasm corpus. We take the corpus of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). This motivates our work. 3 Baseline Discrete Model We follow previous work in"
C16-1231,P15-1100,0,0.0463935,"i-supervised syntactic patterns extracted from sarcastic sentences of Amazon product reviews. Davidov et al. (2010) further extracted these features from sarcastic tweets. Riloff et al. (2013) identified a main type of sarcasm, namely contrast between a positive and negative sentiment, which can be regarded as detecting sarcasm using sentiment information. There has been work that comprehensively studies the effect of various features (Gonz´alez-Ib´anez et al., 2011; Gonz´alez-Ib´anez et al., 2011; Joshi et al., 2015). Recently, contextual information has been exploited for sarcasm detection (Wallace et al., 2015; Karoui et al., 2015). In particular, contextual features extracted from history tweets by the same author has shown great effectiveness for tweet sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015). We consider both traditional lexical features and the contextual features from history tweets under a unified neural network framework. Our observation is consistent with prior work: both sources of features are highly effective for sarcasm detection (Rajadesingan et al., 2015; Bamman and Smith, 2015).. To our knowledge, we are among the first to investigate the effect of neural"
C16-1231,D15-1073,1,0.836572,"s of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b). This motivates our work. 3 Baseline Discrete Model We follow previous work in the literature, building a strong d"
C16-1231,C14-1127,0,0.0268479,"n et al. (2015) and Bamman and Smith (2015) followed the method for building a sarcasm corpus. We take the corpus of Rajadesingan et al. (2015) for our experiments. Neural network models. Although only very limited work has been done on using neural networks for sarcasm detection, neural models have seen increasing applications in sentiment analysis, which is a closely-related task. Different neural network architectures have been applied for sentiment analysis, including recursive auto-encoders (Socher et al., 2013), dynamic pooling networks (Kalchbrenner et al., 2014), deep belief networks (Zhou et al., 2014), deep convolutional networks (dos Santos and Gatti, 2014; Tang et al., 2015) and neural CRF (Zhang et al., 2015). This line of work gives highly competitive results, demonstrating large potentials for neural networks on sentiment analysis. One important reason is the power of neural networks in automatic feature induction, which can potentially discover subtle semantic patterns that are difficult to capture by using manual features. Sarcasm detection can benefit from such induction, and several work has already attempted for it (Amir et al., 2016; Ghosh and Veale, 2016; Joshi et al., 2016b)."
C18-1047,D16-1171,0,0.056794,"Missing"
C18-1047,P14-1129,0,0.0884712,"Missing"
C18-1047,P15-1033,0,0.05612,"Missing"
C18-1047,P12-1007,0,0.120558,"Missing"
C18-1047,P14-1048,0,0.475384,"Missing"
C18-1047,I17-1052,0,0.106516,"Missing"
C18-1047,C12-1059,0,0.0465191,"Missing"
C18-1047,W16-3616,0,0.215218,"Missing"
C18-1047,P14-1002,0,0.310529,"Missing"
C18-1047,P13-1048,0,0.452259,"Missing"
C18-1047,J15-3002,0,0.364527,"Missing"
C18-1047,D14-1220,0,0.456505,"Missing"
C18-1047,P15-1107,0,0.05089,"Missing"
C18-1047,D15-1278,0,0.0580008,"Missing"
C18-1047,D16-1035,0,0.564723,"Missing"
C18-1047,D17-1133,0,0.282459,"Missing"
C18-1047,P14-5010,0,0.00558393,"Missing"
C18-1047,P97-1013,0,0.104124,"Missing"
C18-1047,P99-1047,0,0.257924,"Missing"
C18-1047,D17-1136,0,0.768511,"Missing"
C18-1047,D14-1162,0,0.0797251,"Missing"
C18-1047,W09-3813,0,0.157241,"Missing"
C18-1047,N03-1030,0,0.460548,"Missing"
C18-1047,N15-3001,0,0.103052,"Missing"
C18-1047,Q17-1012,0,0.0761531,"Missing"
C18-1047,P17-2029,0,0.613299,"Missing"
C18-1047,P16-1040,1,0.77321,"Missing"
C18-1047,D17-1182,1,0.76598,"Missing"
C18-1047,P13-1043,0,0.0741812,"Missing"
D15-1073,C14-1008,0,0.0907572,"erpool and Chelsea in tweets. 612 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X. Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to some extent. In this paper, we exploit structured neura"
D15-1073,D14-1012,0,0.0108147,"potentials remain the same as the baseline. By Integrated Models Gleaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of input nodes, with the hidden layer"
D15-1073,P11-1016,0,0.63938,"acting the mentions to all persons and organizations, together with the sentiments towards each mention, from a news archive or a collection of novels. There are two sub tasks in targeted sentiment analysis, namely entity recognition and sentiment classification for each entity mention which apply to both scenarios above. In scenario (1), entity recognition is relatively trivial, and can typically be achieved by pattern matching. Partly due to this reason, most previous work has addressed targeted sentiment analysis as a pure classification task, assuming that target mentions have been given (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014; Vo and Zhang, 2015). For scenario (2), a named entity recognition (NER) system can be used to extract targets, before the same targeted sentiment classification algorithms are applied. There has also been work that concentrates on extracting opinion targets (Jin et al., 2009; Jakob and Gurevych, 2010). In both cases, the data in Figure 1 can be used for training sentiment classifiers. Mitchell et al. (2013) took a different approach, extracting named entities and their sentiment classes jointly. They model the joint task Introduction Targeted sentiment a"
D15-1073,J92-4003,0,0.0963337,"label dependencies, which are constraints for decoding. For example, if yi = O, then zi must be φ. We apply Viterbi decoding for all tasks, and training is performed using a max-margin objective, which is discussed in Section 6. Our training algorithm is different from that of Mitchell et al. (2013), but gives similar discrete CRF accuracies in our experiments. Wang and Mori (2009) also applied a max-margin trainig strategy to train CRF models. The set of features is taken from Mitchell et al. (2013) without changes, as shown in Table 1. Here the cluster features refer to Brown word clusters (Brown et al., 1992). n o Ψ(~x, yi ) = exp θ~ · f~(~x, yi ) , |x| Y |x| XY Neural Models We extend the discrete baseline system with two salient changes, which are illustrated in Figure 4. First, the input discrete features are replaced with continuous word embeddings. Each node in the input takes a real value between 0 and 1, as represented by grey nodes in Figure 4. Second, a hidden , 1 Note the difference between neural and NULL sentiments. The former indicates that a target does not bare any sentiment, and the latter simply means that the term is not a part of a target. 615 ··· O   B   I O ···  B  ··"
D15-1073,P14-1062,0,0.0178598,"towards Manchester United, Liverpool and Chelsea in tweets. 612 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X. Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to some extent. In this paper"
D15-1073,P14-1038,0,0.00815065,"n targeted sentiment analysis, identifying and classifying all targets in a sentence simultaneously. As discussed in the introduction, targeted sentiment analysis falls into two main settings. The first is targeted sentiment classification, assuming that entity mentions are given. Most previous work fall under this category (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014). The second is open domain targeted sentiment, which has been discussed by Mitchell et al. (2013). The task jointly extracts entities and sentiment classes, and is analogous to joint entity and relation extraction (Li and Ji, 2014) in that both are information extraction tasks with multi-label outputs. Our work is related to the line of work on using neural networks for sentiment analysis. Socher et al. (2011) use recursive auto-encoders for sentiment analysis on the sentence level. They further extend the method to a syntactic treebank annotated with sentiment labels (Socher et al., 2013). More recently, Kalchbrenner et al. (2014) use a dynamic pooling network to include the structure 3 Discrete CRF Baselines As shown in Figure 2, the input ~x to our tasks is a word sequence. Assuming no external resources, there is no"
D15-1073,C10-1074,0,0.0944627,"Missing"
D15-1073,P14-1014,1,0.824114,"leaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of input nodes, with the hidden layer for the embeddings being the same as the neural baseline:  ~hi ="
D15-1073,D13-1171,0,0.51862,"Missing"
D15-1073,W04-3236,0,0.0154897,"Missing"
D15-1073,P13-1161,0,0.0372023,"e HMM to extract opinionbaring expressions and opinion targets. Li et al. (2010) improve the results by using CRF to identify the opinion expressions and targets jointly. The task is sometimes referred to as fine-grained sentiment analysis (Wiebe et al., 2005). It is different from our setting in that the predicate-argument relation between opinion-baring expressions and target entities are not explicitly modeled. +   + ···  ··· ··· ··· my (O) baby (B) Farah (I) step 2: sentiment (a) pipeline ··· Φ   ··· O +     B + ···    I ···  ··· ··· ··· my baby Farah (b) joint Recently, Yang and Cardie (2013) use CRF to extract opinion-baring expressions, opinion holders and opinion targets simultaneously. Their method is also centralized on opinion-baring expressions and therefore in line with Jin et al. (2009) and Li et al. (2010). In contrast, targeted sentiment analysis directly studies entity mentions and the sentiment on each mention, without explicitly modeling the way in which the opinion is expressed. As a result, our task is more useful for applications such as broad-stroke reputation management, but offer less fine-grained operational insight. It requires less fine-grained manual annota"
D15-1073,D11-1014,0,0.0511645,"main settings. The first is targeted sentiment classification, assuming that entity mentions are given. Most previous work fall under this category (Jiang et al., 2011; Chen et al., 2012; Dong et al., 2014). The second is open domain targeted sentiment, which has been discussed by Mitchell et al. (2013). The task jointly extracts entities and sentiment classes, and is analogous to joint entity and relation extraction (Li and Ji, 2014) in that both are information extraction tasks with multi-label outputs. Our work is related to the line of work on using neural networks for sentiment analysis. Socher et al. (2011) use recursive auto-encoders for sentiment analysis on the sentence level. They further extend the method to a syntactic treebank annotated with sentiment labels (Socher et al., 2013). More recently, Kalchbrenner et al. (2014) use a dynamic pooling network to include the structure 3 Discrete CRF Baselines As shown in Figure 2, the input ~x to our tasks is a word sequence. Assuming no external resources, there is no POS given to each input word xi . For 614 surface features word identity; word length; message length; punctuation characters; has digit; has dash; is lower case; is 3 or 4 letters;"
D15-1073,N13-1063,0,0.0127686,"the edge clique potentials remain the same as the baseline. By Integrated Models Gleaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of input nodes, wit"
D15-1073,D13-1170,0,0.0153076,"xts, or the sentiment towards Manchester United, Liverpool and Chelsea in tweets. 612 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 612–621, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. other hand, there do exist cases where entity boundaries and sentiment classes reinforce each other. For example, in a tweet such as ‘I like X.’, the contextual pattern indicate both a positive sentiment and an entity in the place of X. Recently, neural network models have been increasingly used for sentiment analysis (Socher et al., 2013; Kalchbrenner et al., 2014; dos Santos and Gatti, 2014), achieving highly competitive results, which show large potentials of neural network models for this task. The main advantages of neural networks are two-fold. First, neural models use real-valued hidden layers to automatically learn feature combinations, which can capture complex semantic information that are difficult to express using traditional discrete manual features. Second, neural networks take distributed word embeddings as inputs, which can be trained from large-scale raw text, thus alleviating the scarcity of annotated data to"
D15-1073,P08-1101,1,0.324745,"and the middle of a negative entity, respectively. The collapsed labels allow joint entity recognition and sentiment classification to be achieved using a standard sequence labeler. Mitchell et al. (2013) compare a pipeline model, a joint model and a collapsed model under the same conditional random field (CRF) framework, finding that the pipeline method outperforms the joint model on a tweet dataset. Intuitively, the interaction between entity boundaries and sentiment classes might not be as strong as that between more closely-coupled sources of information, such as word boundaries and POS (Zhang and Clark, 2008), or named entities and constituents (Finkel and Manning, 2009), for which joint models significantly outperform pipeline models. On the 2 Related Work Targeted sentiment analysis is closely related prior work on aspect-oriented (Hu and Liu, 2004), feature-oriented (Popescu and Etzioni, 2007) and topic-oriented (Yi et al., 2003) sentiment analysis. These related tasks are typically concentrated on product review settings. In contrast, targeted sentiment analysis has a more general setting. Recently, Wang et al. (2011) proposed a topicoriented model, which extracts sentiments towards certain to"
D15-1073,C14-1127,0,0.0637696,"In contrast, targeted sentiment analysis directly studies entity mentions and the sentiment on each mention, without explicitly modeling the way in which the opinion is expressed. As a result, our task is more useful for applications such as broad-stroke reputation management, but offer less fine-grained operational insight. It requires less fine-grained manual annotation. ··· O   B+   I+ ···  ··· ··· ··· my baby Farah (c) collapsed Figure 3: Discrete CRF models for pipeline, joint and collapsed targeted sentiment labeling. of a sentence automatically, before classifying its sentiment. Zhou et al. (2014) apply deep belief networks for semi-supervised sentiment classification. dos Santos and Gatti (2014) use deep convolution neural networks with rich features to classify sentiments over tweets and movie reviews. These methods use different models to represent sentence structures, performing sentiment analysis on the sentence level, without modeling targets. Dong et al. (2014) perform targeted sentiment classification by using a recursive neural network to model the transmission of sentiment signal from opinion baring expressions to a target. They assume that the target mention is given, and pe"
D15-1073,P10-1040,0,0.30476,"model parameter, and the edge clique potentials remain the same as the baseline. By Integrated Models Gleaning different sources of information, neural features and discrete linear features comple616 where (~xn , ~yn )|N n=1 are the set of training examples, λ is a regularization parameter, and l(~xn , ~yn , Θ) is the loss function towards one example (~xn , ~yn ). The loss function is defined as: ments each other. As a result, a model that integrates both features can potentially achieve performance improvements. Most work attempts to add neural word embeddings into a discrete linear model (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014), or add discreted features into a neural model (Ma et al., 2014). We make a novel combination of the discrete models and the neural models by integrating both types of inputs into a same CRF framework.2 The architectures of the integrated models are shown in Figure 5. The main difference between Figure 5 and Figure 3 is the input layer. The integrated model takes both continuous word embeddings, which are shown in grey nodes, and discrete manual features, which are shown in black or white nodes, as the input. A separate hidden layer is given to each type of"
D15-1073,I13-1183,0,0.0195219,"Missing"
D15-1073,D10-1101,0,\N,Missing
D15-1073,N09-1037,0,\N,Missing
D15-1073,H05-2017,0,\N,Missing
D15-1073,H05-1043,0,\N,Missing
D15-1073,P14-2009,0,\N,Missing
D15-1153,P81-1022,0,0.738436,"Missing"
D15-1153,D14-1082,0,0.627029,"f a traditional linear sparse feature model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Determinis"
D15-1153,P13-1104,0,0.0355427,"Missing"
D15-1153,P15-1033,0,0.0878404,"Missing"
D15-1153,Q14-1010,0,0.015097,"nstruct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs, which can be pre-trai"
D15-1153,P15-2042,0,0.0265467,"Missing"
D15-1153,D14-1012,0,0.357814,"model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy loca"
D15-1153,W13-3518,0,0.0120721,"d parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representations of words as inputs"
D15-1153,P14-2128,1,0.902091,"d significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 3 Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd (x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by → −  Score(a) = σ Φd (x) · θ d,a , where σ represents the sigmoid activation func→ − tion, θ d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe (x). In"
D15-1153,P14-1014,1,0.896451,"d significantly. In addition, deterministic parsers use standard neural classifiers, which allows isolated study of feature influences. 3 Following Chen and Manning (2014), training of all the models using a cross-entropy loss objective with a L2-regularization, and mini-batched AdaGrad (Duchi et al., 2011). We unify below the five deterministic parsing models in Figure 1. 3.1 Baseline linear (L) We build a baseline linear model using logistic regression (Figure 1(a)). Given a parsing state x, a vector of discrete features Φd (x) is extracted according to the arc-standard feature templates of Ma et al. (2014a), which is based on the arc-eager templates of Zhang and Nivre (2011). The score of an action a is defined by → −  Score(a) = σ Φd (x) · θ d,a , where σ represents the sigmoid activation func→ − tion, θ d is the set of model parameters, denoting the feature weights with respect to actions, a can be SHIFT, LEFT(l) and RIGHT(l). 3.2 Baseline Neural (NN) We take the Neural model of Chen and Manning (2014) as another baseline (Figure 1(b)). Given a parsing state x, the words are first mapped into continuous vectors by using a set of pre-trained word embeddings. Denote the mapping as Φe (x). In"
D15-1153,W04-2407,0,0.0572757,"··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transition-based parsers with discrete and continuous features. A neural network model takes continuous vector representat"
D15-1153,J08-4003,0,0.273943,"inear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action c"
D15-1153,D12-1110,0,0.0536815,"he Penn Treebank. Their work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is"
D15-1153,P13-1045,0,0.104511,"et al. (2014), we use compounded clusters learnt by K-means algorithm of different granularities. 3.5 #Sent 30,060 1,336 1,640 1,744 1,195 1,906 NN Turian +T This (d) reviews Figure 2: Dev results on fine-tuning (UAS). Following Chen and Manning (2014), we use the pre-trained word embedding released by Collobert et al. (2011), and set h = 200 for the hidden layer size, λ = 10−8 for L2 regularization, and α = 0.01 for the initial learning rate of Adagrad. 4.2 Development Results Fine-tuning of embeddings. Chen and Manning (2014) fine-tune word embeddings in supervised training, consistent with Socher et al. (2013). Intuitively, fine-tuning embeddings allows in-vocabulary words to join the parameter space, thereby giving better fitting to in-domain data. However, it also forfeits the benefit of large-scale pre-training, because out-of-vocabulary (OOV) words do not have their embeddings fine-tuned. In this sense, the method of Chen and Manning resembles a traditional supervised sparse linear model, which can be weak on OOV. On the other hand, the semi-supervised learning methods such as Turian et al. (2010) and Guo et al. (2014), do not fine-tune the word embeddings. Embeddings are taken as inputs rather"
D15-1153,P14-1146,0,0.0317137,"ir work shows that more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different fro"
D15-1153,D14-1101,0,0.0197133,"uo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b) in the hidden layer. While they use a form of restricted Boltzmann machine to pre-train the embeddings and hidden layer from large-scale ngrams, we fully rely on supervised learning to train complex feature combinations. Wang and Manning (2013) consider integrating embeddings and discrete features into a neural CRF. They show that combin"
D15-1153,P10-1040,0,0.727528,"ture model and a multi-layer neural network model for deterministic transition-based dependency parsing, by integrating the sparse features into the neural model. Correlations are drawn between the hybrid model and previous work on integrating word embedding features into a discrete linear model. By analyzing the results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers w"
D15-1153,I13-1183,0,0.0799544,"Missing"
D15-1153,N13-1063,0,0.0390131,"Missing"
D15-1153,N15-1155,0,0.021729,"more sophisticated neural network structures with long term memories can significantly improve the accuracy over local classifiers. Their work is orthogonal to ours. 6 Related Work As discussed in the introduction, our work is related to previous work on integrating word embeddings into discrete models (Turian et al., 2010; Yu et al., 2013; Guo et al., 2014). Along this line, there has also been work that uses a neural network to automatically vectorize the structures of a sentence, and then taking the resulting vector as features in a linear NLP model (Socher et al., 2012; Tang et al., 2014; Yu et al., 2015). Our results show that the use of a hidden neural layer gives superior results compared with both direct integration and integration via a hard-coded transformation function (e.g binarization or clustering). There has been recent work integrating continuous and discrete features for the task of POS tagging (Ma et al., 2014b; Tsuboi, 2014). Both models have essentially the same structure as our model. In contrast to their work, we systematically compare various ways to integrate discrete and continuous features, for the dependency parsing task. Our model is also different from Ma et al. (2014b"
D15-1153,D08-1059,1,0.78857,"results of various parsers on web-domain parsing, we show that the integrated model is a better way to combine traditional and embedding features compared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning ("
D15-1153,P11-2033,1,0.92863,"pared with previous methods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear mode"
D15-1153,C12-2136,1,0.859868,"hods. 1 ··· (a) discrete linear (eg. MaltParser) ··· ··· (b) continuous NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1"
D15-1153,P14-1019,0,0.0132147,"NN (eg. Chen and Manning (2014)) ··· (c) Turian et al. (2010) ··· ··· transform (d) Guo et al. (2014) Introduction ··· Transition-based parsing algorithms construct output syntax trees using a sequence of shift-reduce actions. They are attractive in computational efficiency, allowing linear time decoding with deterministic (Nivre, 2008) or beam-search (Zhang and Clark, 2008) algorithms. Using rich non-local features, transition-based parsers achieve state-ofthe-art accuracies for dependency parsing (Zhang and Nivre, 2011; Zhang and Nivre, 2012; Bohnet and Nivre, 2012; Choi and McCallum, 2013; Zhang et al., 2014). Deterministic transition-based parsers works by making a sequence of greedy local decisions (Nivre et al., 2004; Honnibal et al., 2013; Goldberg et al., 2014; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). They are attractive by very fast speeds. Traditionally, a linear model has been used for the local action classifier. Recently, Chen and Manning (2014) use a neural network (NN) to replace linear models, and report improved accuracies. A contrast between a neural network model and a linear model is shown in Figure 1 (a) and (b). ··· (e) this paper Figure 1: Five deterministic transitio"
D15-1153,P15-1117,1,0.859084,"Missing"
D15-1153,D12-1133,0,\N,Missing
D15-1211,P06-2005,0,0.187504,"et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefo"
D15-1211,P15-1168,0,0.0201398,"Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not"
D15-1211,W09-2010,0,0.201583,"as shown that off-the-shelf NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual depen"
D15-1211,P11-2008,0,0.121397,"Missing"
D15-1211,W11-0704,0,0.029173,"cular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky"
D15-1211,P11-1038,0,0.32066,"three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precis"
D15-1211,D12-1039,0,0.402557,"ization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, c Lisbon, Portugal, 17-21 September 2015. 2015 Associa"
D15-1211,P08-1102,0,0.0537527,"Missing"
D15-1211,D14-1011,0,0.0548928,"start of a new word in a state, SEPS operates SEP and replaces the last word by a possible standard form. 3.4 Features In the experiments, we use the segmentation feature templates of Zhang and Clark (2011). These features are effective for segmentation on formal text. However, for text normalization, these features contain insufficient information. Our experiments show that by using Zhang and Clark’s features, the F-Score on normalization is only 0.4207. Prior work has shown that the language statistic information is important for text normalization (Wang et al., 2013; Li and Yarowsky, 2008; Kaji and Kitsuregawa, 2014). As a result, we extract language model features by using word-based language model learned from a large quantity of standard texts. In particular, 1-gram, 2-gram, 3-gram features are extracted. Every type of n-gram is divided into ten probability ranges. For example, if the probability of the word bigram: “ ‹›- '” (high pressure) is in the 2nd range, the feature is represented as “word-2-gram=2”. In our experiments, language models are trained on the Gigaword corpus1 with SRILM tools2 . To train a word-based language model, we segmented the corpus using our re-implementation of Zhang and Cla"
D15-1211,P09-1058,0,0.0310177,"ttp://www.speech.sri.com/projects/srilm/ Text wR_ðwŸ (Overseas returnees is also referred to as turtles.) õ Ø b † } º i Ùõ ëØp ØIpf (A tree, seemingly a little high, fails a lot of people. Well, this tree is called high number (advanced mathematics)) mance of text normalization, but also increases the performance of word-segmentation. 4 4.1 Extension for Joint Segmentation, Normalization and POS tagging Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci ), removing ci from Q, and appending it to the last (partial) word in S with the same POS tag, . (2) SEP(ci , pos), removing ci from Q, making the last word in S as completed, and adding ci as a new pa"
D15-1211,D08-1108,0,0.117583,"be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline systems. 2 Text Normalization Text normalization is a relatively new research topic. There are no precise definitions of a text 1837 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1837–1846, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational L"
D15-1211,P14-1028,0,0.0258405,"microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an evaluation has not been conducted in previous work due to the lack of annotated corpora for Chinese microblogs. 8 Conclusion We proposed a joint model of word segmentation, POS tagging and normalization, in which the three tasks ben"
D15-1211,D12-1046,0,0.0377294,"eas returnees is also referred to as turtles.) õ Ø b † } º i Ùõ ëØp ØIpf (A tree, seemingly a little high, fails a lot of people. Well, this tree is called high number (advanced mathematics)) mance of text normalization, but also increases the performance of word-segmentation. 4 4.1 Extension for Joint Segmentation, Normalization and POS tagging Joint Segmentation and POS Tagging In order to reduce the error propagation of word segmentation, joint models have been applied to some NLP tasks, such as POS tagging (Zhang and Clark, 2010; Kruengkrai et al., 2009) and Parsing (Zhang et al., 2014a; Qian and Liu, 2012; Zhang et al., 2014b). We take the joint word segmentation and POS tagging model of Zhang and Clark (2010) as the joint baseline. It extends from transition-based segmenter, adding POS arguments to the original actions. In Figure 1, when the current character ci is processing, the transition system for ST would operate as follows : (1) APP(ci ), removing ci from Q, and appending it to the last (partial) word in S with the same POS tag, . (2) SEP(ci , pos), removing ci from Q, making the last word in S as completed, and adding ci as a new partial word with a POS tag “pos”. Given the sentence “"
D15-1211,J11-1005,1,0.920973,"alization dictionary, avoiding diversity on informal words. 3 3.1 Joint Segmentation and Normalization Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F (x) = argmax Score(y) yGen(x) (1) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Stack ... S2 S1 S0 Queue C1 ... Cn Figure 1: A state of transition-based model. Here a transition model is defined as a quadruple M = (C, T, W, Ct ), where C is a state space, T is a set of transitions, each of which is a function: C → C, W is an input sentence c1 ... cn , Ct is a set of terminal states. A model scores the output by scoring the corresponding transition sequence. As shown in Figure 1, a state is a tuple ST = (S, Q), where S contains partially segmented sequences, and Q = (ci , ci+1 , ..., cn ) is the sequen"
D15-1211,D13-1031,0,0.0249816,"guistic information. Gimpel et al. (2011) and Foster et al. (2011) annotate English microblog posts with POS tags. Han and Baldwin (2011) release a microblog corpus annotated with normalized words. Duan et al. (2012) develop a Chinese microblog corpus annotated with segmentation for SIGHAN bakeoff. Wang et al. (2013) release a Chinese microblog corpus for word segmentation and informal word detection. However, there are no microblog corpora annotated Chinese word segmentation, POS tags, and normalized sentences. Our work is alse related to the work of word segmentation (Zhang and Clark, 2007; Zhang et al., 2013; Chen et al., 2015) and joint word segmentation and POS-tagging (Jiang et al., 2008; Zhang and Clark, 2010). A comprehensive survey is out of the scope of this paper, but interested readers can refer to Pei et al. (Pei et al., 2014) for a recent literature review of the fields. To evaluate our model, we develop an annotated microblog corpus with word segmentation, POS tags, and normalization. Furthermore, we train our model by using a standard segmented and POS tagged corpus. We also present a comprehensive evaluation in terms of precision and recall on our 1844 microblog test corpus. Such an"
D15-1211,P14-1125,1,0.925763,"ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) prop"
D15-1211,E14-1062,1,0.939348,"ne of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) prop"
D15-1211,D14-1037,0,0.0801416,"periments can partly reflect some conclusions. Table 7 shows the results of normalization by different systems. The performance of our model is the best among the three systems. In particular, the precision in our SNT model improves upon the baselines significantly. The main reason is that our model is based on global features over whole sentences, while the two baselines based on local windows features. 7 Related Work There has much work on text normalization. The task is generally treated as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese in"
D15-1211,P13-1072,0,0.0766895,"2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word segmentation and normalization, and therefore two tasks should be solved jointly. Wang and Kan (2013) proposed a joint model to process word segmentation and informal word detection. However, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation,"
D15-1211,W12-2106,0,0.0705481,"Missing"
D15-1211,I13-1015,0,0.511944,"discourse-level normalization. In this paper we focus on lexical-level normalization, which aims to transform informal words into their standard forms. Lexical normalization can be regarded as a spelling correction problem. However, researches on spelling correction focus on typographic and cognitive/orthographic errors (Kukich, 1992), while text normalization focuses on lexical variants, such as phonetic substitutions, abbreviation and paraphrases. Unlike English, for which informal words are detected according to whether they are out of vocabulary, Chinese informal words manifest diversity. Wang et al. (2013) divided informal words into three types: phonetic substitutions, abbreviations and neologisms. Li and Yarowsky (2008) classified them into four types: homophone, abbreviation, transliteration and others. Due to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standa"
D15-1211,I05-3013,0,0.0401119,"ook and Stevenson, 2009; Yang and Eisenstein, 2013; Sonmez and Ozgur, 2014) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c). For English, most recent work (Han and Baldwin, 2011; Gouws et al., 2011; Han et al., 2012) uses two-step unsupervised approaches to first detect and then normalize informal words. They aim to produce and use informal/formal word lexicons and mappings. In processing Chinese informal text, Wong and Xia (2008) address the problem of informal words in bulletin board system (BBS) chats by employing pattern matching. Xia et al. (2005) also use SVM-based classification to recognize Chinese informal sentences chats. Both methods have their advantages: the learning-based method does better on recall, while the pattern matching performs better on precision. Li and Yarowsky (2008) tackle the problem of identifying informal/formal Chinese word pairs by generating candidates from Baidu search engine and ranking using a conditional log-linear model. Zhang et al. (2014c) analyze the phenomena of mixed text in Chinese microblogs, proposing a two-stage method to normalise mixed texts. However, their models employ pipelined words segm"
D15-1211,D13-1007,0,0.151872,"f NLP tools can perform poorly on microblogs (Foster et al., 2011; Gimpel et al., 2011; Han and Baldwin, 2011). One of the major challenges for microblog processing is the issue of informal words. For example, “tmrw” has been frequently used in tweets for “tomorrow”, causing OOV problems. Text normalization has been introduced as a pre-processing step for microblog processing, which transforms informal words into their standard forms. Most work in the literature focuses on English microblog normalization, treating it as a noisy channel problem (Pennell and Liu, 2014; Cook and Stevenson, 2009; Yang and Eisenstein, 2013) or a translation problem (Aw et al., 2006; Contractor et al., 2010; Li and Liu, 2012; Zhang et al., 2014c), and training models based on words. Lack of annotated corpora, text normalization is more challenging for Chinese. Unlike English, Chinese informal words are more difficult ∗ corresponding author to mechanically normalize for two main reasons. First, Chinese does not have word delimiters. Second, Chinese informal words manifest diversity, such as abbreviations, neologisms, unconventional spellings and phonetic substitutions. Intuitively, there is mutual dependency between Chinese word s"
D15-1211,P07-1106,1,0.905444,"to variant characteristics, they normalise informal words by training a model per type, leading to increased system complexity. Research reveals that most lexical variants have an unambiguous standard form (Han et al., 2012; Li and Yarowsky, 2008). The validity of this assumption is also empirically assessed on our corpus annotation in Section 6.1. Based on this assumption, we seek standard forms of informal words from a constructed normalization dictionary, avoiding diversity on informal words. 3 3.1 Joint Segmentation and Normalization Transition-based Segmentation We adapt the segmenter of Zhang and Clark (2007) as our baseline segmenter. Given an input sentence x, the baseline segmenter finds a segmentation by maximizing: F (x) = argmax Score(y) yGen(x) (1) where Gen(x) denotes the set of all possible segmentations for an input sentence. Zhang and Clark (2007) proposed a graphbased scoring model, with features based on complete words and word sequences. We adapt their method slightly, under a transition-based framework (Zhang and Clark, 2011), which gives us a consistent way of defining all models in this paper. Stack ... S2 S1 S0 Queue C1 ... Cn Figure 1: A state of transition-based model. Here a"
D15-1211,D10-1082,1,0.862042,"r, text normalization was not included in the joint model. Kaji et al (2014) proposed a joint model for word segmentation, POS tagging and normalization for Japanese Microblogs, which was trained on a partially annotated microblog corpus. Their method requires special annotation for text normalization, which can be expensive. In this paper, we propose a joint model for Chinese text normalization, word-segmentation and POS tagging, which can be trained using standard segmentation and POS tagging annotation, overcoming the lack of an annotated corpus on Chinese microblogs. Our model is based on Zhang and Clark (2010), with an extended set of transition actions to handle joint normalization. In our model, word segmentation and POS tagging are based on normalized text transformed from informal text. Assuming that the majority of informal words can be normalized into formal equivalents (Han et al., 2012; Li and Yarowsky, 2008), we seek standard forms of informal words from an automatically constructed normalization dictionary. To evaluate our model, we developed an annotated corpus of microblog texts. Results show that our model achieves the best performances on three tasks compared with several baseline sys"
D15-1211,W12-6307,0,\N,Missing
D15-1211,C10-2022,0,\N,Missing
D17-1182,P16-1231,0,0.123962,"methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016). In particular, we follow Miwa and Sasaki (2014), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First,"
D17-1182,D15-1041,0,0.00705726,"table-filling labels during decoding for training and testing. For example, T (i, j) must be ⊥ if T (i, i) or T (j, j) equals O. hw e0w L ew L et L hchar CNN character sequence Figure 3: Word representations. tuned during training and represented by ew , and the other being a pre-trained external word embedding from Ew0 , which is fixed and represented by e0w .3 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew . The above two components have also been used by Miwa and Bansal (2016). We further enhance the word representation by using its character sequence (Ballesteros et al., 2015; Lample et al., 2016), taking a convolution neural network (CNN) to derive a character-based word representation hchar , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014). We obtain the final hw i based on a non-linear feed0 forward layer on ew ⊕ ew ⊕ et ⊕ hchar , where ⊕ denotes concatenation. 2.3.2 Label Representation In addition to the word sequence, the history label sequence l1 l2 · · · li−1 , and especially the labels representing detected entities, are also useful disambiguation. For example, the previous entity boundary label can be helpful to d"
D17-1182,H05-1091,0,0.522928,"and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic form"
D17-1182,P15-2142,0,0.0264483,") is the major evaluation metric. tity label LSTM, we only use the segment features of entityi and entityj . 2.3.5 Syntactic Features Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, we take state-of-the-art syntactic parsers that use encoder-decoder neural models (Buys and Blunsom, 2015; Kiperwasser and Goldberg, 2016), where the encoder represents the syntactic features of the input sentences. For example, LSTM hidden states over the input word/tag sequences has been used frequently as syntactic features (Kiperwasser and Goldberg, 2016). Such features represent input words with syntactic information. The parser decoder also leverages partially-parsed results, such as features from partial syntactic trees, although we do not use explicit output features. Table 1 shows the encoder structures of three state-of-the-art dependency parsers. Our method is to leverage trained synta"
D17-1182,C10-1018,0,0.0209201,"g framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a sin"
D17-1182,P11-1056,0,0.226015,"n syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networ"
D17-1182,W02-1001,0,0.0343994,"Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (Ch"
D17-1182,P04-1015,0,0.0146416,"t each step. When each action of table filling is taken, all hypotheses in the agenda are expanded by enumerating the next labels, and the B highest-scored resulting tables are used to replace the agenda for the next step. Search begins with the agenda containing an empty table, and finishes when all cells of the tables in the agenda have been filled. When the beam size is 1, the algorithm is the same as greedy decoding. When the beam size is larger than 1, however, error propagation is alleviated. For training, the same beam search algorithm is applied to training examples, and early-update (Collins and Roark, 2004) is used to fix search errors. 3 Experiments 3.1 Data and Evaluation We evaluate the proposed model on two datasets, namely the ACE05 data and the corpus of Roth and Yih (2004) (CONLL04), respectively. The ACE05 dataset defines seven coarse-grained entity types and six coarse-grained relation categories, while the CONLL04 dataset defines four entity types and five relation categories. For the ACE05 dataset, we follow Li and Ji (2014) and Miwa and Bansal (2016), splitting and preprocessing the dataset into training, development and test sets.5 For the CONLL04 dataset, we follow Miwa and Sasaki"
D17-1182,D16-1001,0,0.0106253,"(Tai et al., 2015). Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; Zhang et al., 2016). We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016). Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as effective as traditional approaches based on discrete parser outputs. We make our code publicly ORG-AFF PHYS Associated Press writer Patrick McDowell in Kuwait City ORG PER PER GPE Figure 1: Relation extraction. The example is chosen from the ACE05 dataset, where ORG, PER and GPE denote organization, person and"
D17-1182,P04-1054,0,0.0428757,"is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method"
D17-1182,doddington-etal-2004-automatic,0,0.0236899,"representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recogniti"
D17-1182,P81-1022,0,0.59941,"Missing"
D17-1182,P15-1033,0,0.00489077,"j), j] denote the two entities above, where s(·) denotes the start position of an entity, the resulted segments are [0, s(i) − 1] (i.e., left, in Figure 5(b)), [s(i), i] (i.e., entityi ), [i+1, s(j)−1] (i.e., middle), [s(j), j] (i.e., entityj ) and [j + 1, n] (i.e., right), respectively. For the word LSTMs, we extract all five segment features, while the en1733 Models S-LSTM (2015) K&G (2016) D&M (2016) Encoder 1-Layer LSTM 2-Layer Bi-LSTM 4-Layer Bi-LSTM LAS 90.9 91.9 93.8 Table 1: Encoder structures and performances of three state-of-the-art dependency parsers, where S-LSTM (2015) refers to Dyer et al. (2015), K&G (2016) refers to the best parser of Kiperwasser and Goldberg (2016), D&M (2016) refers to Dozat and Manning (2016), and LAS (labeled attachment score) is the major evaluation metric. tity label LSTM, we only use the segment features of entityi and entityj . 2.3.5 Syntactic Features Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without ne"
D17-1182,N04-1001,0,0.0979213,"d-to-end relation extraction, achieving the best results on standard benchmarks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relati"
D17-1182,P06-1060,0,0.112679,"t global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li"
D17-1182,D10-1033,0,0.0224257,"ults on standard benchmarks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work condu"
D17-1182,C16-1239,0,0.0470022,"Missing"
D17-1182,P05-1051,0,0.0147433,"ion (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016)"
D17-1182,N07-1015,0,0.0321668,"cal classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et"
D17-1182,Q16-1023,0,0.0225602,"start position of an entity, the resulted segments are [0, s(i) − 1] (i.e., left, in Figure 5(b)), [s(i), i] (i.e., entityi ), [i+1, s(j)−1] (i.e., middle), [s(j), j] (i.e., entityj ) and [j + 1, n] (i.e., right), respectively. For the word LSTMs, we extract all five segment features, while the en1733 Models S-LSTM (2015) K&G (2016) D&M (2016) Encoder 1-Layer LSTM 2-Layer Bi-LSTM 4-Layer Bi-LSTM LAS 90.9 91.9 93.8 Table 1: Encoder structures and performances of three state-of-the-art dependency parsers, where S-LSTM (2015) refers to Dyer et al. (2015), K&G (2016) refers to the best parser of Kiperwasser and Goldberg (2016), D&M (2016) refers to Dozat and Manning (2016), and LAS (labeled attachment score) is the major evaluation metric. tity label LSTM, we only use the segment features of entityi and entityj . 2.3.5 Syntactic Features Previous work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures. In particular, w"
D17-1182,C16-1087,0,0.00686066,"marks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference"
D17-1182,N16-1030,0,0.0744593,"ng decoding for training and testing. For example, T (i, j) must be ⊥ if T (i, i) or T (j, j) equals O. hw e0w L ew L et L hchar CNN character sequence Figure 3: Word representations. tuned during training and represented by ew , and the other being a pre-trained external word embedding from Ew0 , which is fixed and represented by e0w .3 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew . The above two components have also been used by Miwa and Bansal (2016). We further enhance the word representation by using its character sequence (Ballesteros et al., 2015; Lample et al., 2016), taking a convolution neural network (CNN) to derive a character-based word representation hchar , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014). We obtain the final hw i based on a non-linear feed0 forward layer on ew ⊕ ew ⊕ et ⊕ hchar , where ⊕ denotes concatenation. 2.3.2 Label Representation In addition to the word sequence, the history label sequence l1 l2 · · · li−1 , and especially the labels representing detected entities, are also useful disambiguation. For example, the previous entity boundary label can be helpful to deciding the boundary l"
D17-1182,D15-1278,0,0.032968,"arate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests"
D17-1182,P14-1038,0,0.396461,"006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged t"
D17-1182,P16-1200,0,0.0424043,"o benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2"
D17-1182,N10-1069,0,0.0162108,"; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Sp"
D17-1182,P16-1105,0,0.065397,"man, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai et al., 2015) to encod"
D17-1182,D09-1013,0,0.0871128,"del achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 201"
D17-1182,N16-1065,0,0.0197766,"Missing"
D17-1182,D14-1200,0,0.328873,"lations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines. Miwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged treestructured LSTM (Tai"
D17-1182,P17-1159,1,0.833271,"Miwa and Bansal (2016), because these paths rely on explicit discrete outputs from a syntactic parser. Our method can avoid the problem since we do not compute parser outputs. On the other hand, the computation complexity is largely reduced by using our method since sequential LSTMs are based on inputs only, while the dependency path LSTMs should be computed based on the dynamic entity detection outputs. When beam search is exploited during decoding, increasing number of dependency paths can be used by a surge of entity pairs from beam outputs. Our method can be extended into neural stacking Wang et al. (2017), by doing back-propagation training of the parser parameters during model training, which are leave for future work. 2.4 Training and Search 2.4.1 Local Optimization Previous work (Miwa and Bansal, 2016; Gupta et al., 2016) trains model parameters by modeling each step for labeling one input sentence separately. Given a partial table T , its neural representation hT is first obtained, and then compute the next label scores {l1 , l2 , · · · , ls } using Equation 1. The output scores are regularized into a probability distribution {pl1 , pl2 , · · · , pls } by using a softmax layer. The trainin"
D17-1182,P13-1147,0,0.154678,"optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents erro"
D17-1182,P16-1218,0,0.311701,"uch as dependency grammar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015). Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; Zhang et al., 2016). We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016). Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntact"
D17-1182,D10-1034,0,0.0129758,"d on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning an"
D17-1182,P15-1113,0,0.0655736,"nlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016). In particular, we follow Miwa and Sasaki (2014), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Ba"
D17-1182,C08-1088,0,0.0168489,"wn to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a particular syntactic formalism, such as depen"
D17-1182,D16-1137,0,0.0235343,"Missing"
D17-1182,W09-1119,0,0.0333976,", achieving the best results on standard benchmarks. 4 We investigated a globally normalized end-to-end relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both t"
D17-1182,W04-2401,0,0.0552428,"2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the outpu"
D17-1182,C14-1008,0,0.0164801,"uned during training and represented by ew , and the other being a pre-trained external word embedding from Ew0 , which is fixed and represented by e0w .3 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew . The above two components have also been used by Miwa and Bansal (2016). We further enhance the word representation by using its character sequence (Ballesteros et al., 2015; Lample et al., 2016), taking a convolution neural network (CNN) to derive a character-based word representation hchar , which has been demonstrated effective for several NLP tasks (dos Santos and Gatti, 2014). We obtain the final hw i based on a non-linear feed0 forward layer on ew ⊕ ew ⊕ et ⊕ hchar , where ⊕ denotes concatenation. 2.3.2 Label Representation In addition to the word sequence, the history label sequence l1 l2 · · · li−1 , and especially the labels representing detected entities, are also useful disambiguation. For example, the previous entity boundary label can be helpful to deciding the boundary label of the current word. During relation classification, the types of the entities involved can indicate the relation category between them. We exploit the diagonal label sequence of part"
D17-1182,P11-1053,0,0.0409213,"build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-t"
D17-1182,D16-1181,0,0.0194133,"t of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Special business expenses in Heilongjiang Province (China) grant 2016-KYYWF-0183, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 fr"
D17-1182,D15-1206,0,0.026816,"tions jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study. LSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional dis"
D17-1182,P16-1040,1,0.45235,"mar, constituent grammar or combinatory categorial grammar, requiring only hidden representations on word that contain syntactic information. In contrast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be structurally different (Tai et al., 2015). Second, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful, and has been investigated in several studies (Wang and Chang, 2016; Zhang et al., 2016). We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016). Evaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Detailed analysis shows that our integration of syntactic features is as eff"
D17-1182,J11-1005,1,0.341065,"16), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segment features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction. Global optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and im5 Conclusion Acknowledgments We thank the anonymous reviewers for their constructive comments, which help to improve the paper, and Zhiyang Teng for dumping intermediate outputs from the bi-affine parser. This work is supported by National Natural Science Foundation of China (NSFC) grants 61602160 and 61672211, Natural Science Foundation of Heilongjiang Province (China) grant F2016036, Special business expenses"
D17-1182,P05-1052,0,0.212735,"rformances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations. In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend. Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks. 1 Introduction Extracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities. In recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and"
D17-1182,P15-1150,0,0.113895,"Missing"
D17-1182,P05-1053,0,0.473255,"based method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016). First, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses. Our method is also free from a"
D17-1182,D16-1169,1,0.777112,"Missing"
D17-1182,D07-1076,0,0.0443029,"eural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows t"
D17-1182,P15-1117,1,0.597622,"ecisions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016). In particular, we follow Miwa and Sasaki (2014), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to 1730 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1730–1740 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential"
D17-1182,N16-1103,0,0.0150134,"tions are learned from several LSTM structures over the inputs, and a novel simple method is used to integrate syntactic information. Experiments show the effectiveness of both global normalization and syntactic features. Our final model achieved the best performances on two benchmark datasets. Related Work Entity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al., 2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014;"
D17-1296,P16-1231,0,0.109656,"ons taken always equals to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing"
D17-1296,D16-1254,0,0.0289924,"Missing"
D17-1296,N01-1016,0,0.785208,"nnotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PARSED/MRG/SWBD. Following the experiment settings in Charniak and Johnson (2001), the training subcorpus contains directories 2 and 3 in PARSED/MRG/SWBD and directory 4 is split into test, development sets and others. Following Honnibal and Johnson (2014), we lower-case the text and remove all punctuations and partial words2 . We also discard the ‘um’ and ‘uh’ tokens and merge ‘you know’ and ‘i mean’ into single tokens. Automatic POS-tags generated from pocket crf (Qian and Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them wit"
D17-1296,P04-1015,0,0.377117,"As shown in Figure 2, the model state consists of four components: (i) O, a conventional sequential LSTM (Hochreiter and Schmidhuber, 1997) to store the words that have been labeled as fluency. (ii) S, a stack LSTM to represent partial disfluency chunks, which captures chunklevel information. (iii) A, a conventional sequential LSTM to represent history of actions. (iiii) B, a Bi-LSTM to represent words that have not yet been processed. A sequence of transition actions are used to consume input tokens and construct the output from left to right. To reduce error propagation, we use beam-search (Collins and Roark, 2004) and scheduled sampling (Bengio et al., 2015), respectively. We evaluate our model on the commonly used English Switchboard test set and a in-house annotated Chinese data set. Results show that our model outperforms previous state-of-the-art systems. The code is released1 . 2 Background For a background, we briefly introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com"
D17-1296,P15-1033,0,0.413262,"S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR). We denote the word sequence as w1n = (w1 , ..., wn ). The output of the task is a sequence of binary tags denoted as D1n = (d1 , ..., dn ), where each di corresponds to the word wi , indicating whether wi is a disfluent word or not. Hence the task can be modeled as searching for the best sequenc D∗ given the stream of words w1n D∗ = argmaxD P (D1n |w1n ) Wu et al. (2015) proposes a statistical transitionbased disfl"
D17-1296,N15-1029,0,0.310716,"Missing"
D17-1296,N09-2028,0,0.441731,", it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chun"
D17-1296,Q14-1011,0,0.60128,"spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic information. Our model increment"
D17-1296,N16-1030,0,0.0415195,"uencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state"
D17-1296,N06-2019,0,0.507578,"lish Switchboard test data. attention-based model can capture a global representation of the input sentence by using a RNN when encoding. It can strongly capture long-range dependencies and achieves good performance, but are also not powerful enough to capture chunklevel information. To capture chunk-level information, Ferguson et al. (2015) try to use semi-CRF for disfluency detection, and reports improved results. Semi-CRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. Many syntax-based approaches (Lease and Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre,"
D17-1296,P14-1038,0,0.0262712,"n, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced"
D17-1296,N15-1142,0,0.0289563,"d Liu, 2013) are used as POStag in our experiments. For Chinese experiments, we collect 25k spoken sentences from meeting minutes, which are transcribed using the iflyrec toolkit3 , and annotate them with only disfluency annotations according to the guideline proposed by Meteer et al. (1995). 2 words are recognized as partial words if they are tagged as ‘XX’ or end with ‘-’ 3 the iflyrec toolkit is available at http://www.iflyrec.com/ Neural Network Training Pretrained Word Embeddings. Following Dyer et al. (2015) and Wang et al. (2016), we use a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram”, to create word embeddings. The AFP portion of English Gigaword corpus (version 5) is used as the training corpus. Word embeddings for Chinese are trained on Chinese baike corpus. We use an embedding dimension of 100 for English, 300 for chinese. Hyper-Parameters. Both the Bi-LSTMs and the stack LSTMs have two hidden layers and their dimensions are set to 100. Pretrained word embeddings have 100 dimensions and the learned word embeddings have also 100 dimensions. Pos-tag embeddings have 12 dimensions. The dimension of action embeddings is set to 20. 4.3 Perform"
D17-1296,J93-2004,0,0.0579037,"(wi wi+1 , wi+k wi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if wi wi+1 equals wi+k wi+k+1 , the value is 1, others 0 Duplicate(pi pi+1 , pi+k pi+k+1 ), −4 ≤ k ≤ +4 and k 6= 0: if pi pi+1 equals pi+k pi+k+1 , the value is 1, others 0 similarity features f uzzyM atch(wi , wi+k ), k ∈ {−1, +1}: similarity = 2 ∗ num same letters/(len(wi ) + len(wi+k )). if similarity > 0.8, the value is 1, others 0 Table 3: Discrete features used in our transition-based neural networks. p-POS tag. w-word. 4 Experiments 4.1 4.2 Settings Dataset. Our training data include the Switchboard portion of the English Penn Treebank (Marcus et al., 1993) and a in-house Chinese data set. For English, two annotation layers are provided: one for syntactic bracketing (MRG files), and the other for disfluencies (DPS files). The Switchboard annotation project was not fully completed. Because disfluency annotation is cheaper to produce, many of the DPS training files do not have matching MRG files. Only 619,236 of the 1,482,845 tokens in the DPS disfluency detection training data have gold-standard syntactic parses. To directly compare with transitionbased parsing methods (Honnibal and Johnson, 2014; Wu et al., 2015), we also use the subcorpus of PA"
D17-1296,J08-4003,0,0.0365412,"y introduce transitionbased parsing and its extention for joint disfluency detection. An arc-eager transition-based parsing system consists of a stack σ containing words being processed, a buffer β containing words to be processed and a memory A storing dependency 1 https://github.com/hitwsl/transition disfluency OUT DEL et=max{0,W[st;bt;ot;at]+d} O ot at st B bt Bi-LSTM Subtraction want a flight A DEL DEL TOP S to boston to denver Figure 2: model state when processing the sentence “want a flight to boston to denver”. arcs which have been generated. There are four types of transition actions (Nivre, 2008) • Shift : Remove the front of the buffer and push it to the stack. • Reduce : Pop the top of the stack. • LeftArc : Pop the top of the stack, and link the popped word to the front of the buffer. • RightArc : Link the front of the buffer to the top of the stack, remove the front of the buffer and push it to the stack. Many neural network parsers have been constructed under this framework, such as (Dyer et al., 2015), who use different LSTM structure to represent information from σ to β. For disfluency detection, the input is a sentence with disfluencies from automatic speech recognition (ASR)."
D17-1296,N13-1102,0,0.666823,"fifteen words. Hence, it is very important to capture long-range dependencies for disfluency detection. Since there is large parallelism between the reparandum chunk and the following repair chunk (for example, in Figure 1, the reparandum begins with to and ends before another occurrence of to), it is also useful to exploit chunk-level representation, which explicitly makes use of resulted infelicity disfluency chunks. Common approaches take disfluency detection as a sequence labeling problem, where each sentential word is assigned with a label (Zayats et al., 2016; Hough and Schlangen, 2015; Qian and Liu, 2013; Georgila, 2009). These methods achieve good performance, but are not powerful enough to capture complicated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only u"
D17-1296,D13-1013,0,0.490055,"ated disfluencies with longer spans or distances. Another drawback of these approaches is that they are unable to exploit chunk2785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2785–2794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics level features. Semi-CRF (Ferguson et al., 2015) is used to alleviate this issue to some extent. SemiCRF models still have their inefficiencies because they can only use the local chunk information limited by the markov assumption when decoding. A different line of work (Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) adopts transition-based parsing models for disfluency detection. This line of work can be seen as a joint of disfluency detection and parsing. The main advantage of the joint models is that they can capture long-range dependency of disfluencies as well as chunk-level information. However, they introduce additional annotated syntactic structure, which is very expensive to produce, and can cause noise by significantly enlarging the output search space. Inspired by the above observations, we investigate a transition-based model without syntactic info"
D17-1296,C16-1027,1,0.916833,"t set and a set of in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 give"
D17-1296,P16-1218,0,0.0207443,"Scheduled sampling is used to solve the discrepancy by gently changing the training process from a fully guided scheme using the true previous action, towards a less guided scheme which mostly uses the predicting action instead. We take the action gaining higher p(zt |et ) with a certain probability p, and a probability (1 − p) for the correct action when training. 3.2 State Representation For better capturing non-local context information, we use LSTM structures to represent different components of each state, including buffer, action, stack, and output. In particular, we exploit LSTM-Minus (Wang and Chang, 2016) to model the buffer segment, conventional LSTM to model the action and ouptut segment, and stack LSTM (Dyer et al., 2015) to model the stack segments, which demonstrates highly effectively in parsing task. Buffer Representation In order to construct more informative representation, we use a Bi-LSTM to represent the buffer following the work of Wang and Chang (2016), where the subtraction between a unidirectional 2788 O want a flight S to boston B to denver hb(to) hf(to) hb(denver) hf(denver) to boston to bb = hb(denver) - hb(to) denver bf = hf(to) - hf(denver) Figure 3: Illustration for learn"
D17-1296,P15-1113,0,0.024077,"ngineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-house annotated Chinese data set. Acknowledgments We thank the anonymous reviewers for their valuable suggestions. This work wa"
D17-1296,P15-1048,0,0.410628,"in-house annotated Chinese data. 1 RP Figure 1: Sentence with disfluencies annotated in English Switchboard corpus. RM=Reparandum, IM=Interregnum, RP=Repair. The preceding RM is corrected by the following RP. Type repair repair repetition restart Annotation [ I just + I ] enjoy working [ we want + {well} in our area we want ] to [it’s + {uh} it’s ] almost like [ we would like + ] let’s go to the Table 1: Different types of disfluencies. Introduction Disfluency detection is the task of recognizing non-fluent word sequences in spoken language transcripts (Zayats et al., 2016; Wang et al., 2016; Wu et al., 2015). As shown in Figure 1, standard annotation of disfluency structure (Shriberg, 1994) indicates the reparandum (words that are discarded, or corrected by the following words), the interruption point (+) marking the end of the reparandum, the associated repair, and an optional interregnum after the interruption point (filled pauses, discourse cue words, etc.). Ignoring the interregnum, disfluencies can be categorized into three types: restarts, repetitions, and corrections, based on whether the repair is empty, the same as the reparandum or different, respectively. Table 1 gives a few examples."
D17-1296,P13-1013,1,0.811416,"which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detect"
D17-1296,P14-1125,1,0.862598,"Missing"
D17-1296,P16-1040,1,0.852594,"luency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigated before. 2792 7 Conclusion We introduced a transition-based model for disfluency detection, which does not use any syntax information, learning represention of both chunks and global contexts. Experiments showed that our model achieves the state-of-the-art F-scores on both the commonly used English Switchboard test set and a in-ho"
D17-1296,P11-2033,1,0.739192,"nd Johnson, 2006; Rasooli and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has"
D17-1296,P15-1117,1,0.930687,"als to the number of input sentence for every valid path, it is straightforward to use beam search. We use beamsearch for both training and testing. The early update strategy from Collins and Roark (2004) is applied for training. In particular, each training sequence is decoded, and we keep track of the location of the gold path in the beam. If the gold path falls out of the beam at step t, decoding process is stopped and parameter update is performed using the gold path as a positive example, and beam items as negative examples. We also use the global optimization method (Andor et al., 2016; Zhou et al., 2015) to train our beam-search model. Scheduled Sampling Scheduled sampling (Bengio et al., 2015) can also be used to reduce error propagation. The training goal of the greedy baseline is to maximize the likelihood of each action given the current model state, which means that the correct action is taken at each step. Doing inference, the action predicted by the model itself is taken instead. This discrepancy between training and inference can yield errors that accumulate quickly along the searching process. Scheduled sampling is used to solve the discrepancy by gently changing the training process"
D17-1296,P13-1043,1,0.849581,"li and Tetreault, 2013; Honnibal and Johnson, 2014; Wu et al., 2015) have been proposed which jointly perform dependency parsing and disfluency detection. The main advantage of joint models is that they can capture longrange dependency of disfluencies. However, it requires that the training data contains both syntax trees and disfluency annotations, which reduces the practicality of the algorithm. The performance relies heavily on manual feature engineering. Transition-based framework has been widely exploited in a number of other NLP tasks, including syntactic parsing (Zhang and Nivre, 2011; Zhu et al., 2013), information extraction (Li and Ji, 2014) and joint syntactic models (Zhang et al., 2013, 2014). Recently, deep learning methods have been widely used in many nature language processing tasks, such as name entity recognition (Lample et al., 2016), zero pronoun resolution (Yin et al., 2017) and word segmentation (Zhang et al., 2016). The effectiveness of neural features has also been studied for this framework (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016). We apply the transition-based neural framework to disfluency detection, which to our knowledge has not been investigat"
D19-1092,Q16-1022,0,0.0637934,"Missing"
D19-1092,D11-1005,0,0.0237765,"scale training instances full of automatic dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingua"
D19-1092,P81-1022,0,0.713689,"Missing"
D19-1092,K15-1012,0,0.0618523,"l researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferring knowledge between source and target language sentences. These studies enable us to train lexicalized models on code-mixed treebanks as well. Thus here we integrate the cross-lingual word representations as well, which gives more direct interaction between source and target words. Our work follows another mainstream method of this line of work, namely treebank translation"
D19-1092,P15-2139,0,0.0388253,"l researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferring knowledge between source and target language sentences. These studies enable us to train lexicalized models on code-mixed treebanks as well. Thus here we integrate the cross-lingual word representations as well, which gives more direct interaction between source and target words. Our work follows another mainstream method of this line of work, namely treebank translation"
D19-1092,N13-1073,0,0.0291554,"nducted on the Google Universal Dependency Treebanks (v2.0) (McDonald et al., 2013; Nivre et al., 2016), using English as the source language, and choosing six languages, including Spanish (ES), German (DE), French (FR), Italian (IT), Portuguese (PT) and Swedish (sv), as the target languages. Google 1001 Translate3 is used to translate the sentences in the English training set into other languages. In order to generate high-quality word-level alignments, we merge the translated sentence pairs and the parallel data of EuroParl (Koehn, 2005) to obtain word alignments. We use the fastAlign tool (Dyer et al., 2013) to obtain word alignments. We use the cross-lingual word embeddings and clusters by Guo et al. (2016b) for the baseline system. The dimension size of word embeddings is 50 and the word cluster number across of all languages is 256. For network building and training, we use the same setting as Dozat and Manning (2016), including the dimensional sizes, the dropout ratio, as well as the parameter optimization method. We assume that no labeled corpus is available for the target language. Thus training is performed for 50 iterations over the whole training data without early-stopping. To evaluate"
D19-1092,P09-1042,0,0.0834438,"cts by heuristic rules. In contrast, we use partial translation instead to avoid unnecessary noise. Related Work Existing work on cross-lingual transfer can be classified into two categories. The first aims to train a dependency parsing model on source treebanks (McDonald et al., 2011; Guo et al., 2016a,b), or their adapted versions (Zhao et al., 2009; Tiedemann et al., 2014; Wang et al., 2017) in the target language. The second category, 1 Annotation Projection. The annotation projection approach relies on a set of parallel sentences between the source and target languages (Hwa et al., 2005; Ganchev et al., 2009). In parhttps://github.com/zhangmeishan/CodeMixedTreebank 998 ... s 1x ∗ hLSTM · · · hLSTM . For head finding, two nonn 1 linear feed-forward neural layers are used on dep dep hLSTM · · · hLSTM to obtain h1 · · · hn and n 1 hhead · · · hhead n . We compute the score for each de1 pendency ix j by: s nx ∗ BiAffine dep h1 ... hhead 1 dep ... hlstm 1 hhead n hn hlstm n dep six j = BiAffine(hi , hhead ) j BiLSTM × 3 x1 L e(c1 ) e(w1 ) e(t1 ) c1 w1 t1 ... xn ... ... e(cn ) e(wn ) e(tn ) L cn wn The above process is also used for scoring a lal tn beled dependency ix j, by extending the 1-dim vector s"
D19-1092,P15-1119,0,0.294242,"s in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 997–1006, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics lost or mistaken dependency arcs. We consider a different approach for translation-based syntactic knowledge transfer, which aims at making the best use of source syntax with the minimum noise being introduced. To this end, we leverage recent advances in cross-lingual word representations, such as cross-lingual word clusters (T¨ackstr¨om et al., 2012) and cross-lingual word embeddings (Guo et al., 2015), which allow words from different languages to reside within a consistent feature vector space according to structural similarities between words. Thus, they offer a bridge on the lexical level between different languages (Ammar et al., 2016). A cross-lingual model can be trained by directly using cross-lingual word representations on a source treebank (Guo et al., 2015). Using this method, knowledge transfer can be achieved on the level of token correspondences. We take this approach as a naive baseline. To further introduce structural feature transfer, we transform a source treebank into a"
D19-1092,2005.mtsummit-papers.11,0,0.0272045,"roposed models in this section. 5.1 Settings Our experiments are conducted on the Google Universal Dependency Treebanks (v2.0) (McDonald et al., 2013; Nivre et al., 2016), using English as the source language, and choosing six languages, including Spanish (ES), German (DE), French (FR), Italian (IT), Portuguese (PT) and Swedish (sv), as the target languages. Google 1001 Translate3 is used to translate the sentences in the English training set into other languages. In order to generate high-quality word-level alignments, we merge the translated sentence pairs and the parallel data of EuroParl (Koehn, 2005) to obtain word alignments. We use the fastAlign tool (Dyer et al., 2013) to obtain word alignments. We use the cross-lingual word embeddings and clusters by Guo et al. (2016b) for the baseline system. The dimension size of word embeddings is 50 and the word cluster number across of all languages is 256. For network building and training, we use the same setting as Dozat and Manning (2016), including the dimensional sizes, the dropout ratio, as well as the parameter optimization method. We assume that no labeled corpus is available for the target language. Thus training is performed for 50 ite"
D19-1092,N16-1121,0,0.576819,"e details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive code-mixed trees from source dependency trees by partial translation, projecting words and the corresponding dependencies having highconfidence alignments with machine-translated target sentences. Our approach assumes that sentence level trans"
D19-1092,D18-1543,0,0.0391853,"Missing"
D19-1092,P14-1126,0,0.370007,"T algorithm to ensure tree-structural outputs. For training, we accumulate the crossentropy loss at the word-level by treating the normalized scores as prediction probabilities. The reader is referred to Dozat and Manning (2016) for more details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive co"
D19-1092,D11-1006,0,0.824924,"l syntactic transfer. Take dependency grammar for instance. Given a source treebank, machine translation is used to find target translations of its sentences. Then word alignment is used to find mappings between source and target words, so that source syntactic dependencies can be projected to the target translations. Following, a postprocessing step is applied by removing unaligned target words, in order to ensure that the resulting target syntax forms a valid dependency tree, The method has shown promising performance for unsupervised cross-lingual dependency parsing among transfer methods (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Rasooli and Collins, 2015; Guo et al., 2016b). ∗ en The treebank translation method, however, suffers from various sources of noise. For example, machine translation errors directly affect the resulting treebank, by introducing ungrammatical word sequences. In addition, the alignments between source and target words may not be isomorphic due to inherent differences between languages or paraphrasing during translation. For example, in the case of Figure 1, the English words “are” and “being”, and the Swedish word “med”, do not have corresponding word-level translatio"
D19-1092,P12-1066,0,0.0595013,"dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embed"
D19-1092,D15-1039,0,0.671112,"instance. Given a source treebank, machine translation is used to find target translations of its sentences. Then word alignment is used to find mappings between source and target words, so that source syntactic dependencies can be projected to the target translations. Following, a postprocessing step is applied by removing unaligned target words, in order to ensure that the resulting target syntax forms a valid dependency tree, The method has shown promising performance for unsupervised cross-lingual dependency parsing among transfer methods (McDonald et al., 2011; T¨ackstr¨om et al., 2012; Rasooli and Collins, 2015; Guo et al., 2016b). ∗ en The treebank translation method, however, suffers from various sources of noise. For example, machine translation errors directly affect the resulting treebank, by introducing ungrammatical word sequences. In addition, the alignments between source and target words may not be isomorphic due to inherent differences between languages or paraphrasing during translation. For example, in the case of Figure 1, the English words “are” and “being”, and the Swedish word “med”, do not have corresponding word-level translation. In addition, it can be perfect to express “as soon"
D19-1092,Q17-1020,0,0.576924,"xed treebank can bring significantly better performance compared to a fully translated treebank, resulting in averaged improvements of 4.30 points on LAS. The code and related data will be released publicly available under Apache License 2.0.1 2 namely annotation projection, aims to produce a set of large-scale training instances full of automatic dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to e"
D19-1092,N19-1385,0,0.393276,"Missing"
D19-1092,P15-2040,0,0.222173,"et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferrin"
D19-1092,E17-1021,0,0.012567,"ntropy loss at the word-level by treating the normalized scores as prediction probabilities. The reader is referred to Dozat and Manning (2016) for more details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive code-mixed trees from source dependency trees by partial translation, projecting words and the correspo"
D19-1092,N13-1126,0,0.31626,"Missing"
D19-1092,N12-1052,0,0.152277,"Missing"
D19-1092,W15-1824,0,0.512515,"eebanks, giving highly competitive performances among cross-lingual parsing methods. 1 we are being accused unfairly sv vi beskylls med or¨att d¨ar root here advmod advmod nsubjpass vi(we) beskylls(accused) or¨att(unfairly) d¨ar(there) (a) full-scale translation. root nsubjpass auxpass vi(we) sv being en advmod advmod beskylls(accused) sv or¨att(unfairly) sv here en (b) this method, partial translation. Figure 1: An example to illustrate our method, where the source and target languages are English (en) and Swedish (sv), respectively. Introduction Treebank translation (Tiedemann et al., 2014; Tiedemann, 2015; Tiedemann and Agi´c, 2016) has been considered as a method for cross-lingual syntactic transfer. Take dependency grammar for instance. Given a source treebank, machine translation is used to find target translations of its sentences. Then word alignment is used to find mappings between source and target words, so that source syntactic dependencies can be projected to the target translations. Following, a postprocessing step is applied by removing unaligned target words, in order to ensure that the resulting target syntax forms a valid dependency tree, The method has shown promising performan"
D19-1092,W14-1614,0,0.512706,"Missing"
D19-1092,P17-1159,1,0.856702,"ne translation. In addition, the targetside sentences are produced by machine translation. Previous work aims to build a well-formed tree (Tiedemann and Agi´c, 2016) from source dependencies, solving word alignment conflicts by heuristic rules. In contrast, we use partial translation instead to avoid unnecessary noise. Related Work Existing work on cross-lingual transfer can be classified into two categories. The first aims to train a dependency parsing model on source treebanks (McDonald et al., 2011; Guo et al., 2016a,b), or their adapted versions (Zhao et al., 2009; Tiedemann et al., 2014; Wang et al., 2017) in the target language. The second category, 1 Annotation Projection. The annotation projection approach relies on a set of parallel sentences between the source and target languages (Hwa et al., 2005; Ganchev et al., 2009). In parhttps://github.com/zhangmeishan/CodeMixedTreebank 998 ... s 1x ∗ hLSTM · · · hLSTM . For head finding, two nonn 1 linear feed-forward neural layers are used on dep dep hLSTM · · · hLSTM to obtain h1 · · · hn and n 1 hhead · · · hhead n . We compute the score for each de1 pendency ix j by: s nx ∗ BiAffine dep h1 ... hhead 1 dep ... hlstm 1 hhead n hn hlstm n dep six"
D19-1092,K15-1008,0,0.0161341,". For training, we accumulate the crossentropy loss at the word-level by treating the normalized scores as prediction probabilities. The reader is referred to Dozat and Manning (2016) for more details. Figure 2: The overall architecture of the BiAffine parser. ticular, a source parser trained on the source treebank is used to parse the source-side sentences of the parallel corpus. The source dependencies are then projected onto the target sentences according to word alignments. Different strategies can be applied for the dependency projection task (Ma and Xia, 2014; Rasooli and Collins, 2015; Xiao and Guo, 2015; Agi´c et al., 2016; Schlichtkrull and Søgaard, 2017). For example, one can project only dependency arcs whose words are aligned to target-side words with high confidence (Lacroix et al., 2016). The resulting treebank can be highly noisy due to the auto-parsed source dependency trees. Recently Lacroix et al. (2016) and Rasooli and Collins (2017) propose to filter the results from the large-scale parallel corpus. Our work is different in that the source dependencies are from gold-standard treebanks. 3 4 Code-Mixed Treebank Translation We derive code-mixed trees from source dependency trees by"
D19-1092,C18-1327,1,0.767354,"only. • Src+Mix: The BiAffine model trained on the combination dataset of the source and code-mixed treebanks. Models We compare performances on the following models: 3 71.5 The Src and Tgt methods have been discussed in Section 4. The PartProj model is another way to leverage imperfect word alignments (Lacroix et al., 2016). The training corpus of PartProj may be incomplete dependency trees with a number of words missing heads, because no word is deleted from machine translation outputs. The POS tags of words in PartProj with low-confidence alignments are obtained by a supervised POS tagger (Yang et al., 2018) trained on the corresponding universal treebank. 5.3 Development Results We conduct several developmental experiments on the Swedish dataset to examine important factors to our model. 5.3.1 Influence of The Translation Ratio λ Our model has an important hyper-parameter λ to control the percentage of translation. Figure 6 shows the influence of this factor, where the percentages increase from 0 to 1 by intervals of 0.1. A 1002 Model Mix −Sentence Reordering −Word Deletion −Both UAS 80.33 79.79 79.82 79.46 LAS 71.29 70.47 70.64 69.59 tence reordering, the mix model shows decreases of 0.82 and 0"
D19-1092,I08-3008,0,0.118085,"produce a set of large-scale training instances full of automatic dependencies by parsing parallel sentences (Hwa et al., 2005; Rasooli and Collins, 2015). The two broad methods are orthogonal to each other, and can both make use of the lexicalized dependency models trained with cross-lingual word representations (Rasooli and Collins, 2017, 2019). Source Treebank Adaption. There has been much work on unsupervised cross-lingual dependency parsing by direct source treebank transferring. Several researchers investigate delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al.,"
D19-1092,D15-1213,0,0.0994529,"te delexicalized models where only non-lexical features are used in the models (Zeman and Resnik, 2008; Cohen et al., 2011; McDonald et al., 2011; Naseem et al., 2012; T¨ackstr¨om et al., 2013; Rosa and Zabokrtsky, 2015). All the features in these models are language independent, and are consistent across languages and treebanks. Thus they can be applied into target languages directly. Subsequent research proposes to exploit lexicalized features to enhance the parsing models, by resorting to cross-lingual word representations (T¨ackstr¨om et al., 2012; Guo et al., 2015; Duong et al., 2015b,a; Zhang and Barzilay, 2015; Guo et al., 2016b; Ammar et al., 2016; Wick et al., 2016; de Lhoneux et al., 2018). Cross-lingual word clusters and cross-lingual word embeddings are two main sources of features for transferring knowledge between source and target language sentences. These studies enable us to train lexicalized models on code-mixed treebanks as well. Thus here we integrate the cross-lingual word representations as well, which gives more direct interaction between source and target words. Our work follows another mainstream method of this line of work, namely treebank translation (Tiedemann et al., 2014; Tie"
D19-1092,P09-1007,0,0.0497553,"treebank into the target language by machine translation. In addition, the targetside sentences are produced by machine translation. Previous work aims to build a well-formed tree (Tiedemann and Agi´c, 2016) from source dependencies, solving word alignment conflicts by heuristic rules. In contrast, we use partial translation instead to avoid unnecessary noise. Related Work Existing work on cross-lingual transfer can be classified into two categories. The first aims to train a dependency parsing model on source treebanks (McDonald et al., 2011; Guo et al., 2016a,b), or their adapted versions (Zhao et al., 2009; Tiedemann et al., 2014; Wang et al., 2017) in the target language. The second category, 1 Annotation Projection. The annotation projection approach relies on a set of parallel sentences between the source and target languages (Hwa et al., 2005; Ganchev et al., 2009). In parhttps://github.com/zhangmeishan/CodeMixedTreebank 998 ... s 1x ∗ hLSTM · · · hLSTM . For head finding, two nonn 1 linear feed-forward neural layers are used on dep dep hLSTM · · · hLSTM to obtain h1 · · · hn and n 1 hhead · · · hhead n . We compute the score for each de1 pendency ix j by: s nx ∗ BiAffine dep h1 ... hhead 1"
E14-1062,P08-1085,0,0.0501682,"Missing"
E14-1062,W06-1615,0,0.066482,"omain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et al., 2007). The baseline self-training approach (Liu and Zhang, 2012) belongs to this category. The second considers the differences in the two domains in terms of features (Blitzer et al., 2006; Daume III, 2007), classifying features into domain-independent source domain and target domain groups and training these types consistently. The third considers differences between the distributions of instances in the two domains, treating them differently (Jiang and Zhai, 2007). Our type-supervised method is closer to the second category. However, rather than splitting features into domain-independent and domain-specific types, we use domain-specific dictionaries to capture domain differences, and train a model on the source domain only. Our method can be treated as an approach specific to"
E14-1062,P07-1094,0,0.0159784,"Missing"
E14-1062,D10-1056,0,0.0427273,"Missing"
E14-1062,P12-1110,0,0.0349042,"Missing"
E14-1062,P04-1015,0,0.0111374,"the partial results from the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domai"
E14-1062,P11-1061,0,0.0132514,"from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods wi"
E14-1062,P09-1058,0,0.0311136,"omain data to find improved target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (200"
E14-1062,P07-1033,0,0.154058,"Missing"
E14-1062,C12-2073,1,0.643353,"ually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-dependent versions of features using domain-specific data. Our method tunes a set of lexicon-based features, so that domain-dependent models are derived from ins"
E14-1062,I05-3025,0,0.271209,"Missing"
E14-1062,D12-1075,0,0.0619082,"Missing"
E14-1062,N13-1014,0,0.0106972,"ctical situation. In particular, we split Chinese words into domain-independent and domain-specific categories, and define unlexicalized features for domain-specific words. We train lexicalized domain-independent and unlexicalized domainspecific features using the source domain annotated sentences and a source-domain lexicon, and then apply the resulting model to the target domain by replacing the source-domain lexicon with a target domain lexicon. Combined with unsupervised learning with unlabeled target-domain of sentences, the conceptually simple method worked highly effectively. Following Garrette and Baldridge (2013), we address practical questions 590 Action Lexicon Feature templates Separate in-lex(w−1 ), l(w−1 ) ◦ in-lex(w−1 ), in-lex(w−1 , t−1 ), l(w−1 ) ◦ in-lex(w−1 , t−1 ) ing words those that occur more than 3 times for words specific to the source domain. We assume that the domain-independent lexicon applies to all target domains also. For some target domains, we can obtain domain-specific terminologies easily from the Internet. However, this can be a very small portion depending on the domain. Thus, it may still be necessary to obtain new lexicons by manual annotation. Table 1: Dictionary feature"
E14-1062,J94-2001,0,0.0592843,"Missing"
E14-1062,W04-3236,0,0.0174807,"Missing"
E14-1062,P13-1057,0,0.109698,"cate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually annotating target domain sentences, given similar efforts? Our investigations are performed under two Chines"
E14-1062,C04-1081,0,0.596785,"Missing"
E14-1062,P13-1076,0,0.0190385,"cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervised learning methods with labeled source domain data and unlabeled target domain data (Dai et al., 2007; Raina et"
E14-1062,petrov-etal-2012-universal,0,0.0207072,"Missing"
E14-1062,P09-1057,0,0.0155387,"s replaced with a raw tag dictionary gleaned from data, without any human intervention. These facts indicate that careful considerations need to be given for effective typesupervision. In addition, significant manual work might be required to ensure the quality of lexicons. To compare type- and token-supervised tagging, Garrette and Baldridge (2013) performed a set of experiments by conducting each type of annotation for two hours. They showed that for lowresource languages, a tag-dictionary can be reasonably effective if label propagation (Talukdar and Crammer, 2009) and model minimizations (Ravi and Knight, 2009) are applied to expand and filter the lexicons. Similar findings were reported in Garrette et al. (2013). Do the above findings carry over to the Chinese language? In this paper, we perform an empirical study on the effects of tag-dictionaries for domain adaptation of Chinese POS-tagging. We aim to answer the following research questions: (a) Is domain adaptation feasible with only a target-domain lexicon? (b) Can we further improve type-supervised domain adaptation using unlabeled target-domain sentences? (c) Is crafting a tag dictionary for domain adaptation more effective than manually anno"
E14-1062,P08-1101,1,0.450623,"Missing"
E14-1062,D10-1017,0,0.0102399,"ver, their efforts differ from ours in several aspects: (1) they focus on in-domain POS-tagging, while our concern is cross-domain tagging; (2) they study POS-tagging on segmented sentences, while we investigate joint segmentation and POStagging for Chinese; (3) their tag-dictionaries are not tag-dictionaries literally, but statistics of wordtag associations. with Garrette et al. (2013), we also find that the type-supervised method is a competitive choice to token-supervised adaptation. There has been a line of work on using graphbased label propagation to expand tag-lexicons for POS-tagging (Subramanya et al., 2010; Das and Petrov, 2011). Similar methods have been applied to character-level Chinese tagging (Zeng et al., 2013). We found that label propagation from neither the source domain nor auto-labeled target domain sentences can improve domain adaptation. The main reason could be significant domain differences. Due to space limitations, we omit this negative result in our experiments. With respect to domain adaptation, existing methods can be classified into three categories. The first category does not explicitly model differences between the source and target domains, but use standard semi-supervi"
E14-1062,D10-1082,1,0.765811,"ed target domain accuracies over bare CTB training. are available for Chinese. For example, the Chinese Treebank (CTB) (Xue et al., 2005) contains over 50,000 manually tagged news sentences. Hence rather than studying purely type-supervised POS-tagging, we make use of CTB as the source domain, and study domain adaptation to the Internet literature. Second, one uniqueness of Chinese POStagging, in contrast to the POS-tagging of alphabetical languages, is that word segmentation can be performed jointly to avoid error propagation (Ng and Low, 2004; Zhang and Clark, 2008; Kruengkrai et al., 2009; Zhang and Clark, 2010). We adopt this approach for a strong baseline. Previous studies showed that unsupervised domain adaptation can give moderate improvements (Liu and Zhang, 2012). We show that accuracies can be much more significantly improved by using targetdomain knowledge in the form of lexicons. Both token-supervised and type-supervised domain adaptation rely on a set of source-domain annotations; while the former makes additional use of a small set of target annotations, the latter leverages a target-domain lexicon. We take a feature-based method, analogous to that of Daume III (2007), which tunes domain-d"
E14-1062,P11-1139,0,0.023051,"fy CTB words into domainindependent and domain-specific categories. Consisting of semantic information for nearly 100,000 common Chinese words, HowNet can serve as a resource of domain-independent Chinese words. We choose out of all words in the source domain training data those that also occur in HowNet for domain-independent words, and out of the remain4 4.1 Experiments Setting We use annotated sentences from the CTB5 for source-domain training, splitting the corpus into training, development and test sections in the same way as previous work (Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Following Liu and Zhang (2012), we use the free Internet novel “Zhuxian” (henceforth referred to as ZX; also known as “Jade dynasty”) as our target domain data. The writing style of the novel is in the literature genre, with the style of Ming and Qing novels, very different from news in CTB. Ex591 CTB sentences ZX sentences 乔石会见俄罗斯议员团 天下之大，无奇不有，山川灵秀，亦多妖魔鬼怪。 (Qiaoshi meets the Russian delegates.) (The world was big. It held everything. There were fascinating 李鹏强调要加快推行公务员制度 landscapes. There were haunting ghosts.) (Lipeng stressed on speeding the reform of official regulations.) 时间无多，我去请出诛仙古剑。"
E14-1062,J11-1005,1,0.489487,"the beam to generate new partial results, using two types of actions: (1) Append, which appends ci to the last (partial) word in a partial result; (2) Separate(p), which makes the last word in the partial result as completed and adds ci as a new partial word with a POS tag p. Partial results in the beam are scored globally over all actions used to build them, so that the Nbest can be put back to the agenda for the next step. For each action, features are extracted differently. We use the features from Zhang and Clark (2010). Discriminative learning with early-update (Collins and Roark, 2004; Zhang and Clark, 2011) is used to train the model with beam-search. 2.2 Baseline Unsupervised Adaptation by Self-Training A simple unsupervised approach for POS-tagging with unlabeled data is EM. For a generative model such as HMM, EM can locally maximize the likelihood of training data. Given a good start, EM can result in a competitive HMM tagging model (Goldberg et al., 2008). For discriminative models with source-domain training examples, an initial model can be trained using the source-domain data, and self-training can be applied to find a locally-optimized model using raw target domain sentences. The trainin"
E14-1062,Q13-1001,0,0.0296522,"Missing"
E14-1062,I11-1035,0,0.0274135,"Missing"
L16-1034,N13-1006,0,0.0685385,"Missing"
L16-1034,P15-1030,0,0.0234407,"tion, pooling, window function and embedding lookup, which are needed by most NLP tasks. We support flexible objective functions and optimization methods, such as max-margin, max likelihood criterions and AdaGrad (Duchi et al., 2011), and also verification functions such as gradient check. One uniqueness of our toolkit is the support of both dense continuous features and sparse indicator features in neural layers, making it convenient also to build traditional discrete models such as the perceptron, logistic regression and CRF, and to combine discrete and continuous features (Ma et al., 2014; Durrett and Klein, 2015; Zhang and Zhang, 2015). Taking word segmentation, POS-tagging and name entity recognition (NER) as typical examples, we show how stateof-the-art discrete, neural and hybrid models can be built using our toolkit. For example, we show how a bidirectional LSTM model can be built for POS tagging in only 23-lines (12 for inference and 11 for back-propagation) of codes, which gives highly competitive accuracies on standard benchmarks. 2. 2.1. Classes Base Layers Shown in Table 1, we provide several basic classes, which are widely used in neural networks and discrete machine learning algorithms, in"
L16-1034,P15-1033,0,0.0306763,"ar models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large degree of variety f"
L16-1034,P14-1062,0,0.0147815,"layer linear and non-linear transformations. By using these layers, we can conveniently implement linear models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programmin"
L16-1034,D14-1093,1,0.845166,"Missing"
L16-1034,P14-1014,1,0.801107,"such as concatenation, pooling, window function and embedding lookup, which are needed by most NLP tasks. We support flexible objective functions and optimization methods, such as max-margin, max likelihood criterions and AdaGrad (Duchi et al., 2011), and also verification functions such as gradient check. One uniqueness of our toolkit is the support of both dense continuous features and sparse indicator features in neural layers, making it convenient also to build traditional discrete models such as the perceptron, logistic regression and CRF, and to combine discrete and continuous features (Ma et al., 2014; Durrett and Klein, 2015; Zhang and Zhang, 2015). Taking word segmentation, POS-tagging and name entity recognition (NER) as typical examples, we show how stateof-the-art discrete, neural and hybrid models can be built using our toolkit. For example, we show how a bidirectional LSTM model can be built for POS tagging in only 23-lines (12 for inference and 11 for back-propagation) of codes, which gives highly competitive accuracies on standard benchmarks. 2. 2.1. Classes Base Layers Shown in Table 1, we provide several basic classes, which are widely used in neural networks and discrete machin"
L16-1034,P13-1045,0,0.0113582,"iently implement linear models and simple neural models. Besides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large"
L16-1034,N03-1033,0,0.143942,"mbedding Ewi of the word wi and its vector representation vci derived from its character sei quence cm 1 (mi is the length of word wi ). vci is constructed according to neural network structures shown in Figure 2. For NER, ti consists of three parts, including Ewi , vci and the word’s POS tag embedding Epi . The deep neural POS tagging model consists of only 23 lines of code, as marked by red superscripts in Table 3, Figure 2 and Figure 1. Besides the neural models above, we also implement discrete models for the three tasks. The discrete features are extracted according to Liu et al. (2014), Toutanova et al. (2003) and Che et al. (2013) for word segmentation, POS tagging and NER, respectively. We simply apply the sparse atomic layer and exploit the same CRF max-margin for training model parameters. Finally, we make combinations of the discrete and neural models by aggregating their output vectors. Results. We conduct experiments on several datasets. For Chinese word segmentation, we exploit PKU, MSR and CTB60 datasets, where the training and testing corpus of PKU and MSR can be downloaded from BakeOff2005 website5 . For POS tagging, we perform experiments on both English and Chinese datasets. For Englis"
L16-1034,P15-1032,0,0.0211264,"sides, this package also integrates several complex layers by composing those basic layers, such as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large degree of variety for building NLP models by using or adapt"
L16-1034,P07-1106,1,0.795528,".42 97.01 97.39 97.20 95.68 95.64 95.66 94.50 N/A N/A 97.20 N/A N/A 95.05 POS Tagging English Chinese Acc Acc 97.23 93.97 97.28 94.02 97.47 95.07 97.24 94.10 NER English Chinese P R F P R F 80.14 79.29 79.71 72.67 73.92 73.29 77.25 80.19 78.69 65.59 71.84 68.57 81.90 83.26 82.57 72.98 80.15 76.40 82.95 76.67 79.68 76.90 63.32 69.45 Table 4: Main results. split Ontonotes 4.0 to get the English and Chinese datasets. Our experimental results are shown in Table 4. As can be seen for the table, our neural models give competitive results compared the state-of-the-art results on each task, which are Zhang and Clark (2007) for Chinese word segmentation, Toutanova et al. (2003) for English POS tagging, Li et al. (2015) for Chinese POS tagging and Che et al. (2013) for English and Chinese NER. 4. Code Our code and examples in this paper is available under GPL at https://github.com/SUTDNLP/, including repositories of LibN3L, NNSegmentation, NNPOSTagging and NNNamedEntity. 5. Acknowledgments We thank the anonymous reviewers for their constructive comments, which helped to improve the paper. This work is supported by the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301, SRG ISTD 2012 038 from Sing"
L16-1034,D15-1153,1,0.841051,"ction and embedding lookup, which are needed by most NLP tasks. We support flexible objective functions and optimization methods, such as max-margin, max likelihood criterions and AdaGrad (Duchi et al., 2011), and also verification functions such as gradient check. One uniqueness of our toolkit is the support of both dense continuous features and sparse indicator features in neural layers, making it convenient also to build traditional discrete models such as the perceptron, logistic regression and CRF, and to combine discrete and continuous features (Ma et al., 2014; Durrett and Klein, 2015; Zhang and Zhang, 2015). Taking word segmentation, POS-tagging and name entity recognition (NER) as typical examples, we show how stateof-the-art discrete, neural and hybrid models can be built using our toolkit. For example, we show how a bidirectional LSTM model can be built for POS tagging in only 23-lines (12 for inference and 11 for back-propagation) of codes, which gives highly competitive accuracies on standard benchmarks. 2. 2.1. Classes Base Layers Shown in Table 1, we provide several basic classes, which are widely used in neural networks and discrete machine learning algorithms, including atomic layers, p"
L16-1034,P14-1125,1,0.882227,"Missing"
L16-1034,D15-1073,1,0.86174,"ch as RNN, Attention Pooling, LSTM and gated RNN. Those complex layers can be used to implement deep neural models directly. Keywords: neural network, sequence labeling, recurrent neural network 1. Introduction Deep learning methods have received increasing research attention in natural language processing (NLP), with neural models being built for classification (Kalchbrenner et al., 2014), sequence labeling (Collobert et al., 2011), parsing (Socher et al., 2013; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015), machine translation (Cho et al., 2014), fine-grained sentiment analysis (Zhang et al., 2015) and other tasks. This surge of the interest gives rise to a demand of software libraries, which can facilitate research by allowing fast prototyping and modeling for experimentation. For traditional methods such as conditional random fields (CRF) (Lafferty et al., 2001) and SVM (Vapnik, 1995), there has been various software toolkits, implemented in different programming languages, including Java, Python and C++. These toolkits offer a large degree of variety for building NLP models by using or adapting the machine learning algorithms. For deep learning, a number of software tools have been d"
L16-1034,P15-1117,1,0.842891,"Missing"
L16-1034,W14-4012,0,\N,Missing
L16-1034,P15-1172,0,\N,Missing
N19-1066,N16-1094,0,0.121033,"features for ORL. The main drawback of the method is that direct exploration of SRL outputs may lead to the error propagation problem. SRL errors can be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representations from SRL to enhance ORL. The method is referred to as SRL-SAWR for"
N19-1066,P17-1044,0,0.464763,"and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representations from SRL to enhance ORL. The method is referred to as SRL-SAWR for brief. Thanks to the recent advances of encoder-decoder neural SRL models (Zhou and Xu, 2015; He et al., 2017), we can extract implicit vectorized features from the intermediate encoder module instead, avoiding the direct exploration of the final onebest SRL outputs. The vectorized features from the encoder part are implicit semantic-aware representations for input sentences. By taking the semantic-aware representations from SRL as ORL inputs, we are able to make use of SRL information and meanwhile alleviate the error propagation problem. Here we exploit a neural conditional random field (CRF) model with deep bi-directional long short-term memory networks (Bi-LSTMs) as a baseline, most of which is bo"
N19-1066,J09-3003,0,0.0678882,". The method is essentially a feature-based method, treating SRL outputs as a source of features for ORL. The main drawback of the method is that direct exploration of SRL outputs may lead to the error propagation problem. SRL errors can be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware"
N19-1066,D14-1080,0,0.122144,"Missing"
N19-1066,P16-1087,0,0.815765,"ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representations from SRL to enhance ORL. The method is referred to as SRL-SAWR for brief. Thanks to the recent advances of encoder-decoder neural SRL models (Zhou and Xu, 2015; He et al., 2017), we can extract implicit vectorized features from the intermediate enco"
N19-1066,D12-1122,0,0.0851588,"n be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representations from SRL to enhance ORL. The method is referred to as SRL-SAWR for brief. Thanks to the recent advances of encoder-decoder neural SRL models (Zhou and Xu, 2015; He et al., 2017), we can extract implicit vectorized fe"
N19-1066,W06-0301,0,0.794615,"tencies between SRL and ORL, arriving at a full exploration of SRL. The method is essentially a feature-based method, treating SRL outputs as a source of features for ORL. The main drawback of the method is that direct exploration of SRL outputs may lead to the error propagation problem. SRL errors can be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association fo"
N19-1066,P15-1109,0,0.187399,"xpressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representations from SRL to enhance ORL. The method is referred to as SRL-SAWR for brief. Thanks to the recent advances of encoder-decoder neural SRL models (Zhou and Xu, 2015; He et al., 2017), we can extract implicit vectorized features from the intermediate encoder module instead, avoiding the direct exploration of the final onebest SRL outputs. The vectorized features from the encoder part are implicit semantic-aware representations for input sentences. By taking the semantic-aware representations from SRL as ORL inputs, we are able to make use of SRL information and meanwhile alleviate the error propagation problem. Here we exploit a neural conditional random field (CRF) model with deep bi-directional long short-term memory networks (Bi-LSTMs) as a baseline, m"
N19-1066,D15-1168,0,0.0445336,"uts as a source of features for ORL. The main drawback of the method is that direct exploration of SRL outputs may lead to the error propagation problem. SRL errors can be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representations from SRL to enhance ORL. The method is refer"
N19-1066,N18-1054,0,0.543549,"Missing"
N19-1066,D14-1162,0,0.0833188,"overlapped region with the gold-standard entity, and the proportional overlap assigns a partial score proportional to the ratio of the overlapped region. 3.3 3.1 Setting There are several hyper-parameters to define our neural network structures. We simply set their values according to previous work (He et al., 2017; Marasovi´c and Frank, 2018), without much tuning work. Concretely, we set the dimension size of all embeddings to 100, the output hidden size of LSTMs to 200 and the layer number of BiLSTM to 3. For external word embeddings, we use the pretrained 100-dimensional glove embeddings (Pennington et al., 2014). We exploit online training to learn model parameters, and train on the entire training instances for 40 epochs, choosing the best-epoch model according to the performance on the development corpus. We use Adam (Kingma and Ba, 2014) with a learning rate 10−3 to update model parameters, and use gradient clipping by a max norm 1.0 and l2 -regularization by a parameter 10−8 . We apply dropout with a ratio of 0.2 over word represen(1) where WSRL is a projection matrix which is a model parameter, xi is the baseline word representation of word wi , and x∗i is the new word representation, which will"
N19-1066,J11-1002,0,0.0923027,"tially a feature-based method, treating SRL outputs as a source of features for ORL. The main drawback of the method is that direct exploration of SRL outputs may lead to the error propagation problem. SRL errors can be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using implicit semantic-aware word representati"
N19-1066,ruppenhofer-etal-2008-finding,0,0.617954,"a full exploration of SRL. The method is essentially a feature-based method, treating SRL outputs as a source of features for ORL. The main drawback of the method is that direct exploration of SRL outputs may lead to the error propagation problem. SRL errors can be further propagated into ORL outputs, resulting in degraded ORL performance. In this work, we propose a simple and novel Introduction Fine-grained opinion mining aims to detect structured user opinions in text, which has drawn much attention in the natural language processing (NLP) community (Kim and Hovy, 2006; Breck et al., 2007; Ruppenhofer et al., 2008; Wilson et al., 2009; Qiu et al., 2011; Irsoy and Cardie, 2013, 2014; Liu et al., 2015; Wiegand et al., 2016). A structured opinion includes the key arguments of one opinion, such as expressions, holders and targets (Breck et al., 2007; Yang and Cardie, 2012, 2013; Katiyar and Cardie, 2016). Here we focus on opinion role labeling (ORL) (Marasovi´c and Frank, 2018), which identifies opinion holders and ∗ target Corresponding author. 641 Proceedings of NAACL-HLT 2019, pages 641–646 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics method by using im"
N19-1118,P17-2021,0,0.0240852,"e structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its b"
N19-1118,D17-1209,0,0.232385,"mework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential"
N19-1118,P17-1177,0,0.407881,"propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classica"
N19-1118,D17-1304,0,0.0354014,"Missing"
N19-1118,P18-1163,0,0.07328,"drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019"
N19-1118,W14-4012,0,0.16658,"Missing"
N19-1118,D14-1179,0,0.0643719,"Missing"
N19-1118,W06-1628,0,0.1279,"Missing"
N19-1118,W17-3203,0,0.04525,"Missing"
N19-1118,P81-1022,0,0.687283,"Missing"
N19-1118,P16-1078,0,0.150303,"al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integra"
N19-1118,D17-1012,0,0.247924,"e source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate ba"
N19-1118,W15-3014,0,0.0253779,"ne Input Encoder Decoder 教育 o1 o2 o3 o4 o5 o6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence"
N19-1118,W04-3250,0,0.132416,"h et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively: • Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to the model proposed by Chen et al. (2017a). • Tree-Linearization: We first convert dependency trees into constituent trees (Sun and Wan, 2013), and then feed it into the NMT model propose"
N19-1118,W17-3204,0,0.0210465,"ore overlapping with other network components. This further demonstrates that pretrained syntax-aware word representations are helpful for NMT. 4.4.2 Alignment Study Alignment quality is an important metric to illustrate and evaluate machine translation outputs. Here we study how syntax features influence the alignment results for NMT. We approximate the alignment scores by the attention probabilities as shown in Equation 4.8 For better understanding 8 We aim to offer an intuitive interpretation by a carefullyselected example. In fact, the alignment computation method here may be problematic (Koehn and Knowles, 2017). 1156 System Baseline×3 SAWR×3 Tree-RNN×3 Tree-Linearization×3 Hybrid MT03 40.90 41.94 42.03 41.74 42.72 MT04 43.25 44.59 44.15 44.23 45.14 MT05 40.64 41.91 41.50 41.32 42.38 MT06 40.16 41.97 41.41 41.44 42.15 Average/∆ 41.24 42.60/+1.36 42.27/+1.03 42.18/+0.94 43.10/+1.86 Table 4: Ensemble performances, where the Hybrid model denotes SAWR + Tree-RNN + Tree-Linearization. System ... 现代 (modern) ... 的 (’s) ... Baseline Tree-RNN Baseline SAWR Tree-Linearization 42 BLEU SAWR Tree-RNN 38 Tree-Linearization 34 Figure 3: Alignments for the baseline and syntaxintegrated systems, where the same examp"
N19-1118,E17-2093,0,0.0238477,"ecent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn1158 tactic tags. The method is as effective as TreeRNN approaches yet"
N19-1118,D15-1278,0,0.0190868,"network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbo"
N19-1118,N13-1060,0,0.0581759,"Missing"
N19-1118,P17-1064,1,0.696123,"of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system. To solve the problem, Tree-Linearization is a good alternative for syntax encoding. The main idea is to linearize syntax trees into sequential symbols, and then exploit the resulting sequences as inputs for NMT. Li et al. (2017) propose a depth-first method to traverse a constituent tree, converting it into a sequence of symbols mixed with sentential words and syntax labels. Similarly, Wu et al. (2017b) combine several strategies of tree traversing for dependency syntax integration. In this work, we present an implicit syntax encoding method for NMT, enhancing NMT models by syntax-aware word representations (SAWRs). Figure 1 illustrates the basic idea, where trees are modeled indirectly by sequential vectors extracted from an encoder-decoder dependency parser. On the one hand, the method avoids the structural heterog"
N19-1118,P02-1040,0,0.105406,"t datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE) (Sennrich et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). Alternatively, in order to compare the effectiveness of our model with other syntax integration methods, we implement a Tree-RNN approach and a Tree-Linearization approach, respectively: • Tree-RNN: We build a one-layer bidirectional Tree-RNN with GRU over input word embeddings, producing syntaxenhanced word representations, which are then fed into the encoder of NMT as basic inputs. The method is similar to t"
N19-1118,N18-1202,0,0.0504328,"rocess can be formalized as follows:  h = Bi-RNN ex1 ⊕ s1 , · · · , exn ⊕ sn . (6) Noticeably, the SAWR method can be regarded as an adaption of joint learning as well. We can train both dependency parsing and machine translation model parameters concurrently. In this work, we focus on the machine translation task and do not involve the training objective of dependency parsing. However, we can still finetune model parameters of the encoder part of dependency parsing by back-propagating the training losses of NMT into this part as well. Actually, SAWRs are also similar to the ELMO embeddings (Peters et al., 2018). ELMO learns context word representations by using language model as objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem. 4 the TED tst2012 and tst2013 as the development and test datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, whil"
N19-1118,W16-2209,0,0.0414485,"see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present th"
N19-1118,P16-1162,0,0.0740184,"objective, while SAWRs learn syntaxaware word representations by using dependency parsing as objective. On the other hand, compared with the Tree-RNN and Tree-Linearization methods which encode syntax trees by neural networks directly, SAWRs are less sensitive to the output syntax trees. Thus the SAWR method can alleviate the error propagation problem. 4 the TED tst2012 and tst2013 as the development and test datasets, respectively. For the source side sentences, we construct vocabularies of the most frequent 50K words, while for the target side sentences, we apply byte-pair encodings (BPE) (Sennrich et al., 2016) with 32K merges to obtain subword units, and construct the target vocabularies by the most frequent 32K subwords. During training, we use only the sentence pairs whose source and target lengths both are no longer than 50 and 150 for Chinese-English and English-Vietnamese translations, respectively. Evaluation. We use the case insensitive 4gram BLEU score as the main evaluation metrics (Papineni et al., 2002), and adopt the script multi-bleu.perl in the Mose toolkit.3 Significance tests are conducted based on the bestBLEU results for each approach by using bootstrap resampling (Koehn, 2004). A"
N19-1118,P16-1159,0,0.0239934,"明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predic"
N19-1118,D16-1159,0,0.226869,"the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151"
N19-1118,Q13-1025,0,0.0621683,"Missing"
N19-1118,P06-1077,0,0.132793,"on. We implement Tree-RNN and TreeLinearization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because"
N19-1118,2015.iwslt-evaluation.11,0,0.117502,"Missing"
N19-1118,P15-1150,0,0.104527,"1 o2 o3 o4 o5 o6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden"
N19-1118,D15-1166,0,0.266251,"6 head=1, top 是 现代 文明 的 基石 head=0, root head=4, amod head=6, assmod head=4, assm head=2, attr • SAWRs, where the encoder outputs are used as inputs for NMT similar to source-side word embeddings. Figure 1: An example to illustrate our method of encoding source dependency syntax, where the English translation is “Education is the cornerstone of modern civilization” for the source Chinese input. In the past few years, neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then i"
N19-1118,P08-1114,0,0.0513682,"arization for Transformer in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network"
N19-1118,D16-1096,0,0.0201671,", neural machine translation (NMT) has drawn increasing interests due to its simplicity and promising performance (Bahdanau et al., 2014; Jean et al., 2015; Luong and Manning, 2015; Luong et al., 2015; Shen et al., 2016; Vaswani et al., 2017). The widely used Corresponding author. 是 is assm • An example of input dependency tree. Introduction ∗ amod sequence-to-sequence (Seq2Seq) framework combined with attention mechanism achieves significant improvement over the traditional statistical machine translation (SMT) models on a variety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapol"
N19-1118,P16-1105,0,0.0360613,"14; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words and syn1158 tactic tags. The method is as eff"
N19-1118,P17-1065,0,0.296249,"ariety of language pairs, such as Chinese-English (Shi et al., 2016; Mi et al., 2016; Vaswani et al., 2017; Cheng et al., 2018). Under an encoder-decoder architecture, the Seq2Seq framework first encodes the source sentence into a sequence of hidden vectors, and then incrementally predicts the target sentence (Cho et al., 2014a). Recently, inspired by the success of syntaxbased SMT (Williams et al., 2016), researchers propose a range of interesting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that i"
N19-1118,1983.tc-1.13,0,0.712779,"Missing"
N19-1118,D11-1020,0,0.0301346,"in a similar way, only adapting the source input word representing. We adopt a widely-used setting with 8 heads, 6 layers and the hidden dimension size of 512. Table 5 shows the results. As shown, the transformer results are indeed much better than RNNbased baseline. The BLEU scores show an average increase of 40.74 − 37.09 = 3.65. In addition, we can see that syntax information can still give Related Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing a"
N19-1118,D17-1150,0,0.0770327,"esting approaches for exploiting syntax information in NMT models, as syntactic trees could offer long-distance relations in sentences (Shi et al., 2016; Wu et al., 2017b; Li 1151 Proceedings of NAACL-HLT 2019, pages 1151–1161 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics et al., 2017; Bastings et al., 2017; Hashimoto and Tsuruoka, 2017). As a straightforward method, tree-structured recurrent neural network (Tree-RNN) can elegantly model the source-side syntax and globally encode the whole trees. Eriguchi et al. (2016), Chen et al. (2017a) and Yang et al. (2017) show that Tree-RNN can effectively integrate syntaxoriented trees into Seq2Seq NMT models. Regardless of the effectiveness of Tree-RNN, we find that it suffers from a severe low-efficiency problem because of the heterogeneity of different syntax trees, which leads to increasing difficulties for batch computation compared with sequential inputs. Even with deliberate batching method of Neubig et al. (2017), our preliminary experiments show that Tree-RNN with gated recurrent unit (GRU) can lead to nearly four times slower performance when it is integrated into a classical Seq2Seq system. To solv"
N19-1118,N16-1035,0,0.02831,"es such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tree-LSTM for NMT. The major drawback is that its bottomup composing strategy is insufficient for bottom nodes. Thus bi-directional extensions have been suggested (Chen et al., 2017a; Yang et al., 2017). Since Tree-RNN suffers serious inefficiency problem, Li et al. (2017) suggest a Tree-Linearization alternative, which converts constituent trees into a sequence of symbols mixed with words"
N19-1118,Q18-1011,0,0.0337858,"Missing"
N19-1118,P17-2092,0,0.0126484,"lated Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tre"
N19-1118,P17-2060,0,0.0194939,"lated Work By explicitly expressing the structural connections between words and phrases, syntax trees been demonstrated helpful in SMT (Liu et al., 2006; Cowan et al., 2006; Marton and Resnik, 2008; Xie et al., 2011; Li et al., 2013; Williams et al., 2016). Although the representative Seq2Seq NMT models are able to capture latent long-distance relations by using neural network structures such GRU and LSTM (Sutskever et al., 2014; Wu et al., 2016), recent studies show that explicitly integrating syntax trees into NMT models can bring further gains (Sennrich and Haddow, 2016; Shi et al., 2016; Zhou et al., 2017a; Wu et al., 2017a; Aharoni and Goldberg, 2017). Under the NMT setting, the exploration of syntax trees could be more flexible, because of the strong capabilities of neural network in representing arbitrary structures. Recursive neural networks based on LSTM or GRU have been one natural method to model syntax trees (Zhu et al., 2015; Tai et al., 2015; Li et al., 2015; Zhang et al., 2016; Teng and Zhang, 2016; Miwa and Bansal, 2016; Kokkinos and Potamianos, 2017), which are capable of representing the entire trees globally. Eriguchi et al. (2016) present the first work to apply a bottom-up Tre"
P13-1013,P12-1110,0,0.289433,"syntax trees (Figure 1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-s"
P13-1013,P09-1058,0,0.179797,"Missing"
P13-1013,D12-1132,0,0.68711,"1(b)). With richer information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging"
P13-1013,P11-1141,0,0.689122,"c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-b NN-i NN-i NN"
P13-1013,W03-1025,0,0.0140119,"for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be performed jointly, using an efficient CKY-style or shift-reduce algorithm. Luo (2003) exploited this advantage by adding flat word structures without manually annotation to CTB trees, and building a generative character-based parser. Compared to a pipeline system, the advantages of a joint system include reduction of error propagation, and the integration of segmentation, POS tagging and syntax features. With hierarchical structures and head character information, our annotated words are more informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based l"
P13-1013,W12-6304,0,0.0342964,"ore informative than flat word structures, and hence can bring further improvements to phrase-structure parsing. 2 Word Structures and Syntax Trees The Chinese language is a character-based language. Unlike alphabetical languages, Chinese characters convey meanings, and the meaning of most Chinese words takes roots in their character. For example, the word “计算机 (computer)” is composed of the characters “计 (count)”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation a"
P13-1013,W04-3236,0,0.0796187,"and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, with one person annot"
P13-1013,P04-1015,0,0.337377,"erform word segmentation, POS tagging and phrase-structure parsing. To our knowledge, this is the first work to develop a transition-based system that jointly performs the above three tasks. Trained using annotated word structures, our parser also analyzes the internal structures of Chinese words. Our character-based Chinese parsing model is based on the work of Zhang and Clark (2009), which is a transition-based model for lexicalized constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model."
P13-1013,D10-1082,1,0.605411,"a word is in a tag dictionary, which is collected by extracting all multi-character subwords that occur more than five times in the training corpus. For string features, c0 , c−1 and c−2 represent the current character and its previous two characters, respectively; w−1 and w−2 represent the previous two words to the current character, respectively; t0 , t−1 and t−2 represent the POS tags of the current word and the previous two words, respectively. The string features are used for word segmentation and POS tagging, and are adapted from a state-of-the-art joint segmentation and tagging model (Zhang and Clark, 2010). In summary, our character-based parser contains the word-based features of constituent parser presented in Zhang and Clark (2009), the wordbased and shallow character-based features of joint word segmentation and POS tagging presented in Zhang and Clark (2010), and additionally the deep character-based features that encode word structure information, which are the first presented by this paper. 4 4.1 95 90 90 80 85 70 80 64b 16b 4b 1b 75 70 60 64b 16b 4b 1b 50 40 65 30 0 10 20 30 40 (a) Joint segmentation and POS tagging F-scores. 0 10 20 30 40 (b) Joint constituent parsing F-scores. Figure"
P13-1013,D12-1046,0,0.0993978,"information than word-level trees, this form of parse trees can be useful for all the aforementioned Chinese NLP applications. (d) modifier-noun. Figure 2: Inner word structures of “库存 (repertory)”,“考古 (archaeology)”, “科技 (science and technology)” and “败类 (degenerate)”. art joint segmenter and POS tagger, and our baseline word-based parser. Our word annotations lead to further improvements to the joint system, especially for phrase-structure parsing accuracy. Our parser work falls in line with recent work of joint segmentation, POS tagging and parsing (Hatori et al., 2012; Li and Zhou, 2012; Qian and Liu, 2012). Compared with related work, our model gives the best published results for joint segmentation and POS tagging, as well as joint phrase-structure parsing on standard CTB5 evaluations. With linear-time complexity, our parser is highly efficient, processing over 30 sentences per second with a beam size of 16. An open release of the parser is freely available at http://sourceforge.net/projects/zpar/, version 0.6. With regard to task of parsing itself, an important advantage of the character-level syntax trees is that they allow word segmentation, part-of-speech (POS) tagging and parsing to be pe"
P13-1013,J11-1005,1,0.212902,"Missing"
P13-1013,P11-1139,0,0.0424243,"Missing"
P13-1013,E09-1100,0,0.443661,"Missing"
P13-1013,P07-1031,0,0.0309607,"Linguistics, pages 125–134, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (constituent) trees, adding recursive structures of characters for words. We manually annotate the structures of 37,382 words, which cover the entire CTB5. Using these annotations, we transform CTB-style constituent trees into character-level trees (Figure 1(b)). Our word structure corpus, together with a set of tools to transform CTB-style trees into character-level trees, is released at https://github.com/zhangmeishan/wordstructures. Our annotation work is in line with the work of Vadas and Curran (2007) and Li (2011), which provide extended annotations of Penn Treebank (PTB) noun phrases and CTB words (on the morphological level), respectively. NN-rNN-r NN-r NN-r NN-bNN-b NN-iNN-i NN-b NN-b NN-i NN-i NN-lNN-l NN-l NN-l VV-bVV-b VV-iVV-i VV-b VV-b VV-i VV-i 库 库 存 存 考 考 古 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (investigate) (ancient) 库 存 考 古 (repository) (saving) (saving) (investigate) (investigate)(ancient) (ancient) (repository) (a) subject-predicate. (b) verb-object. NN-rNN-r NN-cNN-c NNc NN-r NN-r NN-c NN-bNN-b NN-iNN-i NN-bNN-b NN-iNN-i NN-b NN-b NN-"
P13-1013,I11-1035,0,0.0680725,"Missing"
P13-1013,O03-4002,0,0.0938348,"f NN-f NN-f and morphological-level word structures. 朋友 (friend) 们 (plural) 教育 (education) 界 (field) structures. For each word or subword, we specify its POS and head direction. We use “l”, “r” and “c” to indicate the “left”, “right” and “coordination” head directions, respectively. The “coordination” direction is mostly used in coordination structures, while a very small number of transliteration words, such as “奥巴马 (Obama)” and “洛 杉矶 (Los Angeles)”, have flat structures, and we use “coordination” for their left binarization. For leaf characters, we follow previous work on word segmentation (Xue, 2003; Ng and Low, 2004), and use “b” and “i” to indicate the beginning and nonbeginning characters of a word, respectively. The vast majority of words do not have structural ambiguities. However, the structures of some words may vary according to different POS. For example, “制 服” means “dominate” when it is tagged as a verb, of which the head is the left character; the same word means “uniform dress” when tagged as a noun, of which the head is the right character. Thus the input of the word structure annotation is a word together with its POS. The annotation work was conducted by three persons, wi"
P13-1013,D08-1059,1,0.600383,"zed constituent parsing. They use a beam-search decoder so that the transition action sequence can be globally optimized. The averaged perceptron with early-update (Collins and Roark, 2004) is used to train the model parameters. Their transition system contains four kinds of actions: (1) SHIFT, (2) REDUCE-UNARY, (3) REDUCE-BINARY and (4) TERMINATE. The system can provide binarzied CFG trees in Chomsky Norm Form, and they present a reversible conversion procedure to map arbitrary CFG trees into binarized trees. In this work, we remain consistent with their work, using the head-finding rules of Zhang and Clark (2008), and the same binarization algorithm.1 We apply the same beam-search algorithm for decoding, and employ the averaged perceptron with early-update to train our model. We make two extensions to their work to enable joint segmentation, POS tagging and phrasestructure parsing from the character level. First, we modify the actions of the transition system for • SHIFT-SEPARATE(t): remove the head character cj from Q, pushing a subword node S0 2 0 cj onto S, assigning S .t = t. Note that the parse tree S0 must correspond to a full-word or a phrase node, and the character cj is the first character of"
P13-1013,W09-3825,1,0.954764,"”, “算 (calculate)” and “机 (machine)”. An informal name of “computer” is “电脑”, which is composed of “电 (electronic)” and “脑 (brain)”. Chinese words have internal structures (Xue, 2001; Ma et al., 2012). The way characters interact within words can be similar to the way words interact within phrases. Figure 2 shows the structures of the four words “库存 (repertory)”, “ 考古 To analyze word structures in addition to phrase structures, our character-based parser naturally performs joint word segmentation, POS tagging and parsing jointly. Our model is based on the discriminative shift-reduce parser of Zhang and Clark (2009; 2011), which is a state-of-the-art word-based phrase-structure parser for Chinese. We extend their shift-reduce framework, adding more transition actions for word segmentation and POS tagging, and defining novel features that capture character information. Even when trained using character-level syntax trees with flat word structures, our joint parser outperforms a strong pipelined baseline that consists of a state-of-the126 VV-lV VV-l VV-bVV-b VV-b VV-b 烧 烧 烧 (burn) 烧(burn) (burn)(burn) AD-lA AD-l AD-bAD-b AD-b AD-b 徒 徒 徒 (vain) 徒(vain) (vain)(vain) V-l VV-b 横 (fiercely) 横 (fiercely) VV-l V"
P13-1013,N07-1051,0,\N,Missing
P14-1125,D12-1133,0,0.10455,"d of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhan"
P14-1125,W06-2925,0,0.0254782,"Missing"
P14-1125,W08-0336,0,0.0274826,"flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotation"
P14-1125,W09-2307,0,0.0174117,"ting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 ("
P14-1125,D09-1127,0,0.015966,"Missing"
P14-1125,P10-1001,0,0.0106105,"ake speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows"
P14-1125,D12-1132,0,0.15249,"his work, we extend their formulation, making use of largescale annotations of Zhang et al. (2013), so that the syntactic word-level dependencies can be parsed together with intra-word dependencies. Hatori et al. (2012) proposed a joint model for Chinese word segmentation, POS-tagging and dependency parsing, studying the influence of joint model and character features for parsing, Their model is extended from the arc-standard transition-based model, and can be regarded as an alternative to the arc-standard model of our work when pseudo intra-word dependencies are used. Similar work is done by Li and Zhou (2012). Our proposed arc-standard model is more concise while obtaining better performance than Hatori et al. (2012)’s work. With respect to word structures, real intra-word dependencies are often more complicated, while pseudo word structures cannot be used to correctly guide segmentation. Zhao (2009), Hatori et al. (2012) and our work all study character-level dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and"
P14-1125,P12-1071,1,0.880086,"(c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy directo"
P14-1125,P11-1141,0,0.0156388,"d increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level t"
P14-1125,P13-1104,0,0.0118312,"Missing"
P14-1125,P05-1012,0,0.0232684,"make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on"
P14-1125,P04-1015,0,0.0960443,"011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features fo"
P14-1125,P05-1013,0,0.00967053,"长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, bui"
P14-1125,W02-1001,0,0.090003,"ng and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be either the arc-eager (Zhang and Clark, 2008) or the arc-standard (Huang et al., 2009) transition systems. When the internal structures of words are annotated, character-level dependency parsing can be treated as a special case of word-level dependency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to we"
P14-1125,J08-4003,0,0.437215,"ee by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author."
P14-1125,tsarfaty-goldberg-2008-word,0,0.311687,"el dependency parsing. While Zhao (2009) focus on word internal structures using pseudo inter-word dependencies, Hatori et al. (2012) investigate a joint model using pseudo intra-word dependencies. We use manual dependencies for both inner- and inter-word structures, studying their influences on each other. Zhang et al. (2013) was the first to perform Chinese syntactic parsing over characters. They extended word-level constituent trees by annotated word structures, and proposed a transition-based approach to parse intra-word structures and wordlevel constituent structures jointly. For Hebrew, Tsarfaty and Goldberg (2008) investigated joint segmentation and parsing over characters using a graph-based method. Our work is similar in exploiting character-level syntax. We study the dependency grammar, another popular syntactic representation, and propose two novel transition systems for character-level dependency parsing. Nivre (2008) gave a systematic description of the arc-standard and arc-eager algorithms, currently two popular transition-based parsing methods for word-level dependency parsing. We extend both algorithms to character-level joint word segmentation, POS-tagging and dependency parsing. To our knowl"
P14-1125,I05-3017,0,0.0666397,"ee Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the issue that no universal standard exists for Chinese word segmentation. In the well-known Chinese word segmentation bakeoff tasks, for example, different segmentation standards have been used by different data sets (Emerson, 2005). On the other hand, most disagreement on segmentation standards boils down to disagreement on segmentation granularity. As demonstrated by Zhao (2009), one can extract both finegrained and coarse-grained words from characterlevel dependency trees, and hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented a"
P14-1125,I11-1035,0,0.0124878,"a-word dependencies and pseudo inter-word dependencies; same as those of the character-level arc-standard model, shown in Table 1. 4 • STD (pseudo, real): the arc-standard model with pseudo intra-word dependencies and real inter-word dependencies; Experiments 4.1 Experimental Settings We use the Chinese Penn Treebank 5.0, 6.0 and 7.0 to conduct the experiments, splitting the corpora into training, development and test sets according to previous work. Three different splitting methods are used, namely CTB50 by Zhang and Clark (2010), CTB60 by the official documentation of CTB 6.0, and CTB70 by Wang et al. (2011). The dataset statistics are shown in Table 2. We use the head rules of Zhang and Clark (2008) to convert phrase structures into dependency structures. The intra-word dependencies are extracted from the annotations of Zhang et al. (2013)2 . The standard measures of word-level precision, recall and F1 score are used to evaluate word segmentation, POS-tagging and dependency parsing, following Hatori et al. (2012). In addition, we use the same measures to evaluate intra-word dependencies, which indicate the performance of predicting word structures. A word’s structure is correct only if all the i"
P14-1125,P12-1110,0,0.264853,"ency parsing, with “words” being “characters”. A big weakness of this approach is that full words and POS-tags cannot be used for feature engineering. Both are crucial to well-established features for word segmentation, POS-tagging and syntactic parsing. In this section, we introduce novel extensions to the arc-standard and the arc-eager transition systems, so that word-based and characterbased features can be used simultaneously for character-level dependency parsing. 3.1 The Arc-Standard Model The arc-standard model has been applied to joint segmentation, POS-tagging and dependency parsing (Hatori et al., 2012), but with pseudo word structures. For unified processing of annotated word structures and fair comparison between character-level arc-eager and arc-standard systems, we define a different arc-standard transition system, consistent with our character-level arceager system. In the word-based arc-standard model, the transition state includes a stack and a queue, where the stack contains a sequence of partially-parsed dependency trees, and the queue consists of unprocessed input words. Four actions are defined for state transition, including arc-left (AL, which creates a left arc between the top"
P14-1125,D08-1059,1,0.940802,"ter-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Correspo"
P14-1125,P10-1110,0,0.021232,"he smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the set of actions can be ei"
P14-1125,D10-1082,1,0.927638,"ows an example, where the smallest left subword of “大法官 (chief lawyer)” is “法官 (lawyer)”, and the smallest right subword 1327 大 big 法 law 官 officer (a) smallest left subword 合 agree with 法 law 化 ize (b) smallest right subword Figure 2: An example to illustrate the innermost left/right subwords. of “合法化 (legalize)” is “合法 (legal)”. 3 Character-Level Dependency Parsing A transition-based framework with global learning and beam search decoding (Zhang and Clark, 2011) has been applied to a number of natural language processing tasks, including word segmentation, POS-tagging and syntactic parsing (Zhang and Clark, 2010; Huang and Sagae, 2010; Bohnet and Nivre, 2012; Zhang et al., 2013). It models a task incrementally from a start state to an end state, where each intermediate state during decoding can be regarded as a partial output. A number of actions are defined so that the state advances step by step. To learn the model parameters, it usually uses the online perceptron algorithm with early-update under the inexact decoding condition (Collins, 2002; Collins and Roark, 2004). Transition-based dependency parsing can be modeled under this framework, where the state consists of a stack and a queue, and the s"
P14-1125,J11-1005,1,0.727336,"for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al. (2013)’s annotations and based on a transition-based parsing framework (Zhang and Clark, 2011). There are two dominant transitionbased dependency parsing systems, namely the arc-standard and the arc-eager parsers (Nivre, 2008). We study both algorithms for characterlevel dependency parsing in order to make a comprehensive investigation. For direct comparison with word-based parsers, we incorporate the traditional word segmentation, POS-tagging and dependency parsing stages in our joint parsing models. We make changes to the original transition systems, and arrive at two novel transition-based character-level parsers. We conduct experiments on three data sets, including CTB 5.0, CTB 6.0"
P14-1125,P11-2033,1,0.953288,"intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependenc"
P14-1125,W08-0335,0,0.0212139,"hence can adapt to flexible segmentation standards using this formalism. In Figure 1(c), for example, “副局长 (deputy 1326 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1326–1336, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics director)” can be segmented as both “副 (deputy) |局 长 (director)” and “副 局 长 (deputy director)”, but not “副 (deputy) 局 (office) |长 (manager)”, by dependency coherence. Chinese language processing tasks, such as machine translation, can benefit from flexible segmentation standards (Zhang et al., 2008; Chang et al., 2008). Second, word internal structures can also be useful for syntactic parsing. Zhang et al. (2013) have shown the usefulness of word structures in Chinese constituent parsing. Their results on the Chinese Treebank (CTB) showed that characterlevel constituent parsing can bring increased performances even with the pseudo word structures. They further showed that better performances can be achieved when manually annotated word structures are used instead of pseudo structures. In this paper, we make an investigation of character-level Chinese dependency parsing using Zhang et al"
P14-1125,P13-1013,1,0.528047,"ng interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohnet, 2010; Zhang and Nivre, 2011; Choi and McCallum, 2013). Chinese dependency trees were conventionally defined over words (Chang et al., 2009; Li et al., 2012), requiring word segmentation and POS-tagging as pre-processing steps. Recent work on Chinese analysis has embarked on investigating the syntactic roles of characters, leading to large-scale annotations of word internal structures (Li, 2011; Zhang et al., 2013). Such annotations enable dependency parsing on the character level, building dependency trees over Chinese characters. Figure 1(c) shows an example of Corresponding author. 会 meeting (a) a word-based dependency tree Introduction ∗ 副局长 deputy director Figure 1: An example character-level dependency tree. “林业局副局长在大会上发言 (The deputy director of forestry administration make a speech in the meeting)”. a character-level dependency tree, where the leaf nodes are Chinese characters. Character-level dependency parsing is interesting in at least two aspects. First, character-level trees circumvent the i"
P14-1125,E09-1100,0,0.097413,"over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods. 1 林业局 forestry administration 上 in 发言 make a speech 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (b) a character-level dependency tree by Zhao (2009) with real intra-word and pseudo inter-word dependencies 林 业 局 副 局 长 会 上 发 言 woods industry office deputy office manager meeting in make speech (c) a character-level dependency tree investigated in this paper with both real intra- and inter-word dependencies As a light-weight formalism offering syntactic information to downstream applications such as SMT, the dependency grammar has received increasing interest in the syntax parsing community (McDonald et al., 2005; Nivre and Nilsson, 2005; Carreras et al., 2006; Duan et al., 2007; Koo and Collins, 2010; Zhang and Clark, 2008; Nivre, 2008; Bohn"
P14-1125,P13-1043,1,0.251531,"ion is performed by the SHw action. The actions for intra-word dependencies include intra-word arc-left (ALc ), intra-word arcright (ARc ), pop-word (PW) and inter-word shift (SHc ). The definitions of ALc , ARc and SHc are the same as the word-based arc-standard model, while PW changes the top element on the stack into a full-word node, which can only take interword dependencies. One thing to note is that, due to variable word sizes in character-level parsing, the number of actions can vary between different sequences of actions corresponding to different analyses. We use the padding method (Zhu et al., 2013), adding an IDLE action to finished transition action sequences, for better alignments between states in the beam. 1328 In the character-level arc-standard transition step 0 1 2 3 4 5 6 7 ··· 12 13 ··· step 0 1 2 3 4 5 6 7 ··· 13 14 ··· action SHw (NR) SHc ALc SHc ALc PW SHw (NN) ··· PW ALw ··· (a) action SHc (NR) ALc SHc ALc SHc PW SHw ··· PW ALw ··· stack queue dependencies φ 林 业 ··· φ 林/NR 业 局 ··· φ 林/NR 业/NR 局 副 ··· φ x 业/NR 局 副 ··· A1 = {林 业} 业/NR 局/NR 副 局 ··· A1 S x 局/NR 副 局 ··· A2 = A1 {业 局} 林业局/NR 副 局 ··· A2 林业局/NR 副/NN 局 长 ··· A2 ··· ··· ··· 林业局/NR 副局长/NN 会 上 · · · Ai S x 副局长/NN 会 上 ·"
P14-6008,P14-1021,1,0.829043,"troduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks,"
P14-6008,P07-1106,1,0.91772,"l discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010),"
P14-6008,P08-1101,1,0.827347,"coding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing ("
P14-6008,D08-1059,1,0.822846,"coding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing ("
P14-6008,W09-3825,1,0.756058,"tion to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentatio"
P14-6008,D10-1082,1,0.847244,"(Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on"
P14-6008,P11-1069,1,0.847476,"g, tliu}@ir.hit.edu.cn † Abstract This tutorial discusses a framework for incremental left-to-right structured predication, which makes use of global discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has a"
P14-6008,J11-1005,1,0.848919,"g, tliu}@ir.hit.edu.cn † Abstract This tutorial discusses a framework for incremental left-to-right structured predication, which makes use of global discriminative learning and beam-search decoding. The method has been applied to a wide range of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has a"
P14-6008,P11-2033,1,0.838007,"ange of NLP tasks in recent years, and achieved competitive accuracies and efficiencies. We give an introduction to the algorithms and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), j"
P14-6008,C12-2136,1,0.812736,"ch the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework are also enabled by direct interactions between learning and search (Daum´e III and Marcu, 2005; Huang et al., 2012; Zhang and Nivre, 2012). 2 Tutorial Overview In this tutorial, we make an introduction to the framework, illustrating how it can be applied to a range of NLP problems, giving theoretical discussions and demonstrating a software implementation. We start with a detailed introduction of the framework, describing the averaged perceptron algorithm (Collins, 2002) and its efficient implementation issues (Zhang and Clark, 2007), as well as beam-search and the early-update strategy (Collins and Roark, 2004). We then illustrate how the framework can be applied to NLP tasks, including word segmentation, joint segmentation & P"
P14-6008,P13-1013,1,0.83742,"tional categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuraci"
P14-6008,P14-1125,1,0.795356,"ammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and parsing (Zhang et al., 2013; Zhang et al., 2014). In addition to the aforementioned tasks, the 13 framework can be applied to all structural prediction tasks for which the output can be constructed using an incremental process. The advantage of this framework is two-fold. First, beamsearch enables highly efficient decoding, which typically has linear time complexity, depending on the incremental process. Second, free from DPstyle constraints and Markov-style independence assumptions, the framework allows arbitrary features to be defined to capture structural patterns. In addition to feature advantages, the high accuracies of this framework"
P14-6008,P13-1043,1,0.847813,"and efficient implementations, and discuss their applications to a range of NLP tasks. 1 Introduction This tutorial discusses a framework of online global discriminative learning and beam-search decoding for syntactic processing (Zhang and Clark, 2011b), which has recently been applied to a wide variety of natural language processing (NLP) tasks, including word segmentation (Zhang and Clark, 2007), dependency parsing (Zhang and Clark, 2008b; Huang and Sagae, 2010; Zhang and Nivre, 2011; Bohnet and Kuhn, 2012), context free grammar (CFG) parsing (Collins and Roark, 2004; Zhang and Clark, 2009; Zhu et al., 2013), combinational categorial grammar (CCG) parsing (Zhang and Clark, 2011a; Xu et al., 2014) and machine translation (Liu, 2013), achieving stateof-the-art accuracies and efficiencies. In addition, due to its high efficiencies, it has also been applied to a range of joint structural problems, such as joint segmentation and POS-tagging (Zhang and Clark, 2008a; Zhang and Clark, 2010), joint POS-tagging and dependency parsing (Hatori et al., 2011; Bohnet and Nivre, 2012), joint morphological analysis, POS-tagging and dependency parsing (Bohnet et al., 2013), and joint segmentation, POS-tagging and"
P14-6008,E12-1009,0,\N,Missing
P14-6008,W02-1001,0,\N,Missing
P14-6008,P04-1015,0,\N,Missing
P14-6008,P10-1110,0,\N,Missing
P14-6008,P13-1001,0,\N,Missing
P14-6008,I11-1136,0,\N,Missing
P14-6008,N12-1015,0,\N,Missing
P15-1045,C10-1037,0,0.205831,"w(ei ) = salext (s) (2) where salabs (·) denotes the word salience score of an abstractive model, salext (·) denotes the sentence salience score of an extractive model, and Sen(ei ) denotes the sentence set where ei is extracted from. We exploit our baseline sentence ranking method, SentRank, to obtain the sentence salience score, and use our baseline phrase ranking method, PhraseRank, to obtain the phrase salience score. 3.2 Headline Generation We use a graph-based multi-sentence compression (MSC) model to generate the final title for the proposed event-driven model. The model is inspired by Filippova (2010). First, a weighted directed acyclic word graph is built, with a start node and an end node in the graph. A headline can be obtained by any path from the start node to the end node. We measure each candidate path by a scoring function. Based on the measurement, we exploit a beam-search algorithm to find the optimum path. (1) i=1 (lj ,ei )∈Gbi X s∈Sen(ei ) rij × sal(lj ) rij × sal(ei ) Events w(lj ) · w(ei ) A 3.2.1 Word-Graph Construction Given a set of candidate events CE, we extract all the sentences that contain the events. In particular, we add two artificial words, hSi and hEi, to the sta"
P15-1045,P13-1122,0,0.38366,"e #K Multi-Sentence Compression Headline Headline Generation Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated titles, given that sentence synthesis is still very inaccurate based on a set of phrases with little grammatical information (Zhang, 2013). In this paper, we propose an event-driven model for headline generation, which alleviates the Introduction Headline generation (HG) is a text summarization task, which aims t"
P15-1045,N10-1131,0,0.0313243,"rade-off between sentences and phrases, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced"
P15-1045,W97-0703,0,0.110622,"extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities a"
P15-1045,J95-2003,0,0.490343,"Missing"
P15-1045,J05-3002,0,0.0528138,"of EventRank is better, capturing the major event in the reference title. 469 a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between senten"
P15-1045,D13-1036,0,0.142968,"Missing"
P15-1045,N13-1030,0,0.043263,"Missing"
P15-1045,D14-1076,0,0.0195508,"lems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013;"
P15-1045,J10-3005,0,0.05359,"Missing"
P15-1045,W04-1013,0,0.0153886,"ne sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A DD T O B EAM(beam, candidate) end for end for candidates ← T OP -K(beam, B) if candidates all endwith hEi : break end loop best ← B EST(candidates) conducted using the Stanford NLP tools (Marneffe and Manning, 2008). The MRP iteration number is set to 10. We use ROUGE (Lin, 2004) to automatically measure the model performance, which has been widely used in summarization tasks (Wang et al., 2013; Ng et al., 2014). We focus on Rouge1 and Rouge2 scores, following Xu et al. (2010). In addition, we conduct human evaluations, using the same method as Woodsend et al. (2010). Four participants are asked to rate the generated headlines by three criteria: informativeness (how much important information in the article does the headline describe?), fluency (is it fluent to read?) and coherence (does it capture the topic of article?). Each headline is given a subjective score from"
P15-1045,P04-1015,0,0.038794,"m language model is trained using SRILM6 on English Gigaword (LDC2011T07). e∈CE where sal(e) is the salience score of an event from the candidate extraction step, Vi .w denotes the word of vertex Vi , and dist(w, e) denotes the distance from the word w to the event e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4 ) when using a trigram language model, which is time consuming. 5 The distance is +∞ when e and w are not in one sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A"
P15-1045,W09-1801,0,0.0753187,"Missing"
P15-1045,D14-1148,1,0.812388,"POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is per"
P15-1045,W13-2117,0,0.0238691,"2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has be"
P15-1045,W03-0501,0,0.414777,"nd McKeown, 2004). This task is challenging in not only informativeness and readability, which are challenges to common summarization tasks, but also the length reduction, which is unique for headline generation. Previous headline generation models fall into two main categories, namely extractive HG and abstractive HG (Woodsend et al., 2010; Alfonseca et al., 2013). Both consist of two steps: candidate extraction and headline generation. Extractive models choose a set of salient sentences in candidate extraction, and then exploit sentence compression techniques to achieve headline generation (Dorr et al., 2003; 462 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predica"
P15-1045,P14-1115,0,0.0355339,"Missing"
P15-1045,W11-1607,0,0.0240977,"uring the major event in the reference title. 469 a word graph are treated equally, and the edges in the graph are constructed according to the adjacent order in original sentence. Our MSC model is also inspired by Filippova (2010). Our approach is more aggressive than their approach, generating compressions with arbitrary length by using a different edge construction strategy. In addition, our search algorithm is also different from theirs. Our graph-based MSC model is also similar in spirit to sentence fusion, which has been used for multi-document summarization (Barzilay and McKeown, 2005; Elsner and Santhanam, 2011). abstractive models have the problem of poor grammaticality. The event-driven mothod can alleviate both issues since event offer a trade-off between sentence and phrase. 5 Related Work Our event-driven model is different from traditional extractive (Dorr et al., 2003; Erkan and Radev, 2004; Alfonseca et al., 2013) and abstractive models (Zajic et al., 2005; Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010) in that events are used as the basic processing units instead of sentences and phrases. As mentioned above, events are a trade-off between sentences and phrases, avoiding spa"
P15-1045,J91-1002,0,0.655589,"ons, nsubj and dobj, for extracting event arguments. Event arguments that have the same predicate are merged into one event, represented by tuple (Subject, Predicate, Object). For example, given the sentence, “the Keenans could demand the Aryan Nations’ assets”, Figure 2 present its partial parsing tree. Based on the parsing results, two event arguments are obtained: nsubj(demand, Keenans) and dobj(demand, assets). The two event arguments are merged into one event: (Keenans, demand, assets). 3.1.2 Extracting Lexical Chains Lexical chains are used to link semanticallyrelated words and phrases (Morris and Hirst, 1991; Barzilay and Elhadad, 1997). A lexical chain is analogous to a semantic synset. Compared with words, lexical chains are less sparse for event ranking. Given a text, we follow Boudin and Morin (2013) to construct lexical chains based on the following principles: 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not ha"
P15-1045,W08-1301,0,0.0796257,": 1. All words that are identical after stemming are treated as one word; 2. All NPs with the same head word fall into one lexical chain;2 3.1.1 Extracting Events We apply an open-domain event extraction approach. Different from traditional event extraction, for which types and arguments are predefined, open event extraction does not have a closed set of entities and relations (Fader et al., 2011). We follow Hu’s work (Hu et al., 2013) to extract events. Given a text, we first use the Stanford dependency parser1 to obtain the Stanford typed dependency structures of the sentences (Marneffe and Manning, 2008). Then we focus on 1 poss aux 3. A pronoun is added to the corresponding lexical chain if it refers to a word in the chain (The coreference resolution is performed using the Stanford Coreference Resolution system);3 4. Lexical chains are merged if their main words are in the same synset of WordNet.4 2 NPs are extracted according to the dependency relations nn and amod. As shown in Figure 2, we can extract the noun phrase Aryan Nations according to the dependency relation nn(Nations, Aryan). 3 http://nlp.stanford.edu/software/dcoref.shtml 4 http://wordnet.princeton.edu/ http://nlp.stanford.edu/"
P15-1045,P14-1087,0,0.148337,"could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an event is a tuple (S, P, O), where S is the subject, P is the predicate and O is the object. For example, for the sentence “Ukraine Delays Announcement of New Government”, the event is (Ukraine, Delays, Announcement). This type of event structures has been used in open information extraction (Fader et al., 2011), and has a range of NLP applications (Ding et al., 2014; Ng et al., 2014). A sentence is a well-formed structure with complete syntactic information, but can contain redundant information for text summarization, which makes sentences very sparse. Phrases can be used to avoid the sparsity problem, but with little syntactic information between phrases, fluent headline generation is difficult. Events can be regarded as a trade-off between sentences and phrases. They are meaningful structures without redundant components, less sparse than sentences and containing more syntactic information than phrases. In our system, candidate event extraction is performed on a bipart"
P15-1045,D11-1142,0,0.474229,"ternational Joint Conference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed base"
P15-1045,D14-1201,1,0.841711,"nference on Natural Language Processing, pages 462–472, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics sentence ranking method. In this paper, we use SentRank to denote this method. disadvantages of both extractive and abstractive HG. The framework of the proposed model is shown in Figure 1. In particular, we use events as the basic processing units for candidate extraction. We use structured tuples to represent the subject, predicate and object of an event. This form of event representation is widely used in open information extraction (Fader et al., 2011; Qiu and Zhang, 2014). Intuitively, events can be regarded as a trade-off between sentences and phrases. Events are meaningful structures, containing necessary grammatical information, and yet are much less sparse than sentences. We use salience measures of both sentences and phrases for event extraction, and thus our model can be regarded as a combination of extractive and abstractive HG. During the headline generation step, A graphbased multi-sentence compression (MSC) model is proposed to generate a final title, given multiple events. First a directed acyclic word graph is constructed based on the extracted eve"
P15-1045,J11-1005,1,0.849206,"ed using SRILM6 on English Gigaword (LDC2011T07). e∈CE where sal(e) is the salience score of an event from the candidate extraction step, Vi .w denotes the word of vertex Vi , and dist(w, e) denotes the distance from the word w to the event e, which are defined by the minimum distance from w to all the related words of e in a sentence by the dependency path5 between them. Intuitively, equation 3 demonstrates that a vertex is salient when its corresponding word is close to salient 3.2.3 Beam Search Beam search has been widely used aiming to find the sub optimum result (Collins and Roark, 2004; Zhang and Clark, 2011), when exact inference is extremely difficult. Assuming our word graph has a vertex size of n, the worst computation complexity is O(n4 ) when using a trigram language model, which is time consuming. 5 The distance is +∞ when e and w are not in one sentence. 6 466 http://www.speech.sri.com/projects/srilm/ Input: G ← (V, E), LM, B Output: best candidates ← { {hSi} } loop do beam ← { } for each candidate in candidates if candidate endwith hEi A DD T O B EAM(beam, candidate) continue for each Vi in V candidate ← A DDV ERTEX(candidate, Vi ) C OMPUTE S CORE(candidate, LM) A DD T O B EAM(beam, candi"
P15-1045,P14-1117,0,0.0129353,"ar, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2"
P15-1045,C14-1155,0,0.0185322,"multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path in their work. Mehdad et al. (2013; Mehdad et al. (2014) introduce the MSC based on word graph into meeting summarization. Tzouridis et al. (2014) cast multi-sentence compression as a structured predication problem. They use a largemargin approach to adapt parameterised edge weights to the data in order to acquire the shortest path. In their work, the sentences introduced to 6 Conclusion and Future Work We proposed an event-driven model headline generation, introducing a graph-based MSC model to generate the final title, based on a set of events. Our event-driven model can incorporate sentence and phrase salience, which has been used in extractive and abstractive HG models. The proposed graph-based MSC model is not limited to our event-"
P15-1045,C04-1079,0,0.160236,"ce realization preferences jointly. Previous extractive and abstractive models take two main steps, namely candidate extraction and headline generation. Here, we introduce these two types of models according to the two steps. 2.1 Abstractive Headline Generation Extractive Headline Generation Candidate Extraction. Extractive models exploit sentences as the basic processing units in this step. Sentences are ranked by their salience according to specific strategies (Dorr et al., 2003; Erkan and Radev, 2004; Zajic et al., 2005). One of the stateof-the-art approaches is the work of Erkan and Radev (2004), which exploits centroid, position and length features to compute sentence salience. We re-implemented this method as our baseline 463 3 Our Model dobj Similar to extractive and abstractive models, the proposed event-driven model consists of two steps, namely candidate extraction and headline generation. 3.1 nsubj det nn DT NNPS MD VB DT NNP NNP POS NNS the Keenans could demand the Aryan Nations ’ assets Figure 2: Dependency tree for the sentence “the Keenans could demand the Aryan Nations’ assets”. Candidate Extraction We exploit events as the basic units for candidate extraction. Here an ev"
P15-1045,P13-1136,0,0.317702,"also from the proposed graph-based MSC model. Both our candidate extraction and headline generation methods outperform competitive baseline methods, and our model achieves the best results compared with previous state-of-the-art systems. 2 Headline Generation. Given a set of sentences, extractive models exploit sentence compression techniques to generate a final title. Most previous work exploits single-sentence compression (SSC) techniques. Dorr et al. (2003) proposed the Hedge Trimmer algorithm to compress a sentence by making use of handcrafted linguistically-based rules. Alfonseca et al. (2013) introduce a multi-sentence compression (MSC) model into headline generation, using it as a baseline in their work. They indicated that the most important information is distributed across several sentences in the text. 2.2 Candidate Extraction. Different from extractive models, abstractive models exploit phrases as the basic processing units. A set of salient phrases are selected according to specific principles during candidate extraction (Schwartz, 01; Soricut and Marcu, 2007; Xu et al., 2010; Woodsend et al., 2010). Xu et al. (2010) propose to rank phrases using background knowledge extrac"
P15-1045,D10-1050,0,0.138899,"adline generation, combining the advantages of both methods using event structures. Standard evaluation shows that our model achieves the best performance compared with previous state-of-the-art systems. 1 Candidate Extraction Events Phrases Sentences Candidate Ranking Candidate #1 ... Candidate #i ... Candidate #K Multi-Sentence Compression Headline Headline Generation Figure 1: System framework. Zajic et al., 2005). Abstractive models choose a set of informative phrases for candidate extraction, and then exploit sentence synthesis techniques for headline generation (Soricut and Marcu, 2007; Woodsend et al., 2010; Xu et al., 2010). Extractive HG and abstractive HG have their respective advantages and disadvantages. Extractive models can generate more readable headlines, because the final title is derived by tailoring human-written sentences. However, extractive models give less informative titles (Alfonseca et al., 2013), because sentences are very sparse, making high-recall candidate extraction difficult. In contrast, abstractive models use phrases as the basic processing units, which are much less sparse. However, it is more difficult for abstractive HG to ensure the grammaticality of the generated"
P15-1045,P12-2068,0,0.0198855,"ses, avoiding sparsity and structureless problems. In particular, our event-driven model can interact with sentences and phrases, thus is a light combination for two traditional models. The event-driven model is mainly inspired by Alfonseca et al. (2013), who exploit events for multi-document headline generation. They leverage titles of sub-documents for supervised training. In contrast, we generate a title for a single document using an unsupervised model. We use novel approaches for event ranking and title generation. In recent years, sentence compression (Galanis and Androutsopoulos, 2010; Yoshikawa and Iida, 2012; Wang et al., 2013; Li et al., 2014; Thadani, 2014) has received much attention. Some methods can be directly applied for multidocument summarization (Wang et al., 2013; Li et al., 2014). To our knowledge, few studies have been explored on applying them in headline generation. Multi-sentence compression based on word graph was first proposed by Filippova (2010). Some subsequent work was presented recently. Boudin and Morin (2013) propose that the key phrase is helpful to sentence generation. The key phrases are extracted according to syntactic pattern and introduced to identify shortest path"
P15-1045,P14-2054,0,\N,Missing
P15-1045,R13-1033,0,\N,Missing
P16-1040,P16-1039,0,0.329018,"with previous work. Finally, we conducted several comparisons to study the differences between our word-based model with character-based neural models, showing that they have different error characteristics. For word-based segmentation, Andrew (2006) used a semi-CRF model to integrate word features, Zhang and Clark (2007) used a perceptron algorithm with inexact search, and Sun et al. (2009) used a discriminative latent variable model to make use of word features. Recently, there have been several neural-based models using word-level embedding features (Morita et al., 2015; Liu et al., 2016; Cai and Zhao, 2016), which are different from our work in the basic framework. For instance, Liu et al. (2016) follow Andrew (2006) using a semi-CRF for structured inference. We followed the global learning and beamsearch framework of Zhang and Clark (2011) in building a word-based neural segmentor. The main difference between our model and that of Zhang and Clark (2011) is that we use a neural network to induce feature combinations directly from character and word embeddings. In addition, the use of a bi-directional LSTM allows us to leverage non-local information from the word sequence, and look-ahead informat"
P16-1040,P15-1167,0,0.371099,"Missing"
P16-1040,P15-1168,0,0.789778,"ubsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches."
P16-1040,D15-1141,0,0.273242,"Missing"
P16-1040,I13-1181,0,0.0491099,"s and date/time characters are also differentiated for extracting features. Zheng et al. (2013) built a neural network segmentor, which essentially substitutes the manual discrete features of Peng et al. (2004), with dense real-valued features induced automatically from character embeddings, using a deep neural network structure (Collobert et al., 2011). A tag transition matrix is used for inference, which makes the model effectively. Most subsequent work on neural segmentation followed this method, improving the extraction of emission features by using more complex neural network structures. Mansur et al. (2013) experimented with embeddings of richer features, and in particular characFigure 8: F-measure against character length. we examine the error distribution on individual sentences. Figure 7 shows the F-measure values of each test sentence by word- and characterbased neural models, respectively, where the xaxis value denotes the F-measure value of the word-based neural model, and the y-axis value denotes its performance of the character-based neural model. We can see that the majority scatter points are off the diagonal line, demonstrating strong differences between the two models. This results f"
P16-1040,P04-1015,0,0.0488927,"of “中 国 (Chinese) 外 企 (foreign company) 业 务 (business) 发展 (develop) 迅速 (quickly)”. • Separate (SEP), which moves the first character of the queue onto the buffer as a new (sub) word. character-based methods, we take inspiration from word-based discrete segmentation instead. In particular, we follow Zhang and Clark (2007), using the transition-based framework to decode a sentence from left-to-right incrementally, scoring partially segmented results using both character-level and word-level features. Beam-search is applied to reduce error propagation and large-margin training with early-update (Collins and Roark, 2004) is used for learning from inexact search. We replace the discrete word and character features of Zhang and Clark (2007) with word and character embeddings, respectively, and change their linear model into a deep neural network. Following Zheng et al. (2013) and Chen et al. (2015b), we use convolution neural networks to achieve local feature combination and LSTM to learn global sentence-level features, respectively. The resulting model is a word-based neural segmenter that can leverage rich embedding features. Its correlation with existing work on Chinese segmentation is shown in Figure 1. Res"
P16-1040,D15-1276,0,0.0243707,"that achieved top performances compared with previous work. Finally, we conducted several comparisons to study the differences between our word-based model with character-based neural models, showing that they have different error characteristics. For word-based segmentation, Andrew (2006) used a semi-CRF model to integrate word features, Zhang and Clark (2007) used a perceptron algorithm with inexact search, and Sun et al. (2009) used a discriminative latent variable model to make use of word features. Recently, there have been several neural-based models using word-level embedding features (Morita et al., 2015; Liu et al., 2016; Cai and Zhao, 2016), which are different from our work in the basic framework. For instance, Liu et al. (2016) follow Andrew (2006) using a semi-CRF for structured inference. We followed the global learning and beamsearch framework of Zhang and Clark (2011) in building a word-based neural segmentor. The main difference between our model and that of Zhang and Clark (2011) is that we use a neural network to induce feature combinations directly from character and word embeddings. In addition, the use of a bi-directional LSTM allows us to leverage non-local information from the"
P16-1040,P14-1028,0,0.835364,"ntext window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (And"
P16-1040,C04-1081,0,0.936237,"is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a 1 The term in this paper is used to denote the neural network structure with convolutional layers, which is different from the typical convolution neural network that has a pooling layer upon convolutional layers (Krizhevsky et al., 2012). 421 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 421–431, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguist"
P16-1040,P15-1030,0,0.0204482,"Missing"
P16-1040,P15-1033,0,0.0236858,"ural(-tune) (c) neural(+tune) 0.96 neural Figure 5: Accuracies against the training epoch using beam sizes 1, 2, 4, 8 and 16, respectively. 5.2.1 P 95.21 91.81 94.89 94.93 95.00 Embeddings and beam size 0.92 0.88 0.84 We study the influence of beam size on the baseline and neural models. Our neural model has two choices of using pre-trained word embeddings. We can either fine-tune or fix the embeddings during training. In case of fine-tuning, only words in the training data can be learned, while embeddings of out-of-vocabulary (OOV) words could not be used effectively.5 In addition, following Dyer et al. (2015) we randomly set words with frequency 1 in the training data as the OOV words in order to learn the OOV embedding, while avoiding overfitting. If the pretrained word embeddings are not fine-tuned, we can utilize all word embeddings. Figure 5 shows the development results, where the training curve of the discrete baseline is shown in Figure 5(a) and the curve of the neural model without and with fine tuning are shown in 5(b) and 5(c), respectively. The performance increases with a larger beam size in all settings. When the beam increases into 16, the gains levels out. The results of the discret"
P16-1040,D11-1090,0,0.0881911,"word features lead to comparable performances to the best systems in the literature, and a further combination of discrete and neural features gives top accuracies. 1 Figure 1: Word segmentation methods. word, respectively, employing conditional random field (CRF) to model the correspondence between the input character sequence and output label sequence. For each character, features are extracted from a five-character context window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word s"
P16-1040,I05-3017,0,0.513427,"Missing"
P16-1040,N09-1007,0,0.853958,"changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of the two models with respect to sentences, respectively. As shown in Fig"
P16-1040,P12-1027,0,0.0598188,"gth, where the boxes with red dots denote the performances of word-based neural model, and the boxes with blue slant lines denote character-based neural model. 1 word word 1 45 50+ 6 Related Work Xue (2003) was the first to propose a charactertagging method to Chinese word segmentation, using a maximum entropy model to assign B/I/E/S tags to each character in the input sentence separately. Peng et al. (2004) showed that better results can be achieved by global learning using a CRF model. This method has been followed by most subsequent models in the literature (Tseng et al., 2005; Zhao, 2009; Sun et al., 2012). The most effective features have been character unigrams, bigrams and trigrams within a five-character window, and a bigram tag window. Special characters such as alphabets, numbers and date/time characters are also differentiated for extracting features. Zheng et al. (2013) built a neural network segmentor, which essentially substitutes the manual discrete features of Peng et al. (2004), with dense real-valued features induced automatically from character embeddings, using a deep neural network structure (Collobert et al., 2011). A tag transition matrix is used for inference, which makes th"
P16-1040,C10-2139,0,0.601041,"s in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of the two models with respect to sentences, respectivel"
P16-1040,I05-3027,0,0.638401,"ensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a 1 The term in this"
P16-1040,P14-1038,0,0.0132384,"s from the differences in feature sources. Second, we study the F-measure distribution of the two neural models with respect to sentence lengths. We divide the test sentences into ten bins, with bin i denoting sentence lengths in [5 ∗ (i − 1), 5 ∗ i]. Figure 8 shows the results. According to the figure, we observe that word-based neural model is relatively weaker for sentences with length in [5, 10], while can better tackle long sentences. Third, we compare the two neural models by their capabilities of modeling words with different lengths. Figure 9 shows the results. The perfor428 traction (Li and Ji, 2014) and the work of joint models (Zhang et al., 2013b; Zhang et al., 2014). Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes Zhou et al. (2015), Weiss et al. (2015), Watanabe and Sumita (2015) and Andor et al. (2016). In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature c"
P16-1040,P10-1040,0,0.0272047,"Missing"
P16-1040,D13-1031,0,0.773659,"ram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of"
P16-1040,I13-1183,0,0.0195275,"Missing"
P16-1040,P13-1013,1,0.902048,"ram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integration to our word-based segmentor on the development dataset. We study it by two ways. First, we compare the error distributions between the discrete and the neural models. Intuitively, different error distributions are necessary for improvements by integration. We draw a scatter graph to show their differences, with the (x, y) values of each point denoting the F-measure scores of"
P16-1040,P14-1125,1,0.884053,"egrating discrete features Prior work has shown the effectiveness of integrating discrete and neural features for several NLP tasks (Turian et al., 2010; Wang and Manning, 5 6 We perform experiments using random initialized word embeddings as well when fine-tune is used, which is a fully supervised model. The performance is slightly lower. In all our experiments, we fix the character unigram and bigram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefuln"
P16-1040,I11-1035,0,0.0492711,"atures Prior work has shown the effectiveness of integrating discrete and neural features for several NLP tasks (Turian et al., 2010; Wang and Manning, 5 6 We perform experiments using random initialized word embeddings as well when fine-tune is used, which is a fully supervised model. The performance is slightly lower. In all our experiments, we fix the character unigram and bigram embeddings, because fine-tuning of these embeddings results in little changes. 426 Models word-based models discrete neural combined character-based models discrete neural combined other models Zhang et al. (2014) Wang et al. (2011) Zhang and Clark (2011) P R F 95.29 95.34 96.11 95.26 94.69 95.79 95.28 95.01 95.95 95.38 94.59 95.63 95.12 94.92 95.60 95.25 94.76 95.61 N/A 95.83 95.46 N/A 95.75 94.78 95.71 95.79 95.13 Models our word-based models discrete neural combined character-based models discrete neural combined other models Cai and Zhao (2016) Ma and Hinrichs (2015) Pei et al. (2014) Zhang et al. (2013a) Sun et al. (2012) Zhang and Clark (2011) Sun (2010) Sun et al. (2009) Table 5: Main results on CTB60 test dataset. 2013; Durrett and Klein, 2015; Zhang and Zhang, 2015). We investigate the usefulness of such integra"
P16-1040,Y06-1012,0,0.241425,"atures in a word-based segmentation framework. Experimental results demonstrate that word features lead to comparable performances to the best systems in the literature, and a further combination of discrete and neural features gives top accuracies. 1 Figure 1: Word segmentation methods. word, respectively, employing conditional random field (CRF) to model the correspondence between the input character sequence and output label sequence. For each character, features are extracted from a five-character context window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al"
P16-1040,P14-2032,0,0.196418,"ain difference between our model and that of Zhang and Clark (2011) is that we use a neural network to induce feature combinations directly from character and word embeddings. In addition, the use of a bi-directional LSTM allows us to leverage non-local information from the word sequence, and look-ahead information from the incoming character sequence. The automatic neural features are complementary to the manual discrete features of Zhang and Clark (2011). We show that our model can accommodate the integration of both types of features. This is similar in spirit to the work of Sun (2010) and Wang et al. (2014), who integrated features of character-based and word-based segmentors. Acknowledgments We thank the anonymous reviewers, Yijia Liu and Hai Zhao for their constructive comments, which help to improve the final paper. This work is supported by National Natural Science Foundation of China (NSFC) under grant 61170148, Natural Science Foundation of Heilongjiang Province (China) under grant No.F2016036, the Singapore Ministry of Education (MOE) AcRF Tier 2 grant T2MOE201301 and SRG ISTD 2012 038 from Singapore University of Technology and Design. Yue Zhang is the corresponding author. References Da"
P16-1040,E09-1100,0,0.0124881,"nst word length, where the boxes with red dots denote the performances of word-based neural model, and the boxes with blue slant lines denote character-based neural model. 1 word word 1 45 50+ 6 Related Work Xue (2003) was the first to propose a charactertagging method to Chinese word segmentation, using a maximum entropy model to assign B/I/E/S tags to each character in the input sentence separately. Peng et al. (2004) showed that better results can be achieved by global learning using a CRF model. This method has been followed by most subsequent models in the literature (Tseng et al., 2005; Zhao, 2009; Sun et al., 2012). The most effective features have been character unigrams, bigrams and trigrams within a five-character window, and a bigram tag window. Special characters such as alphabets, numbers and date/time characters are also differentiated for extracting features. Zheng et al. (2013) built a neural network segmentor, which essentially substitutes the manual discrete features of Peng et al. (2004), with dense real-valued features induced automatically from character embeddings, using a deep neural network structure (Collobert et al., 2011). A tag transition matrix is used for infere"
P16-1040,D13-1061,0,0.36697,"tures are extracted from a five-character context window and a twolabel history window. Subsequent work explores different label sets (Zhao et al., 2006), feature sets (Shi and Wang, 2007) and semi-supervised learning (Sun and Xu, 2011), reporting state-of-the-art accuracies. Recently, neural network models have been investigated for the character tagging approach. The main idea is to replace manual discrete features with automatic real-valued features, which are derived automatically from distributed character representations using neural networks. In particular, convolution neural network1 (Zheng et al., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 200"
P16-1040,P15-1113,0,0.0210521,"hat word-based neural model is relatively weaker for sentences with length in [5, 10], while can better tackle long sentences. Third, we compare the two neural models by their capabilities of modeling words with different lengths. Figure 9 shows the results. The perfor428 traction (Li and Ji, 2014) and the work of joint models (Zhang et al., 2013b; Zhang et al., 2014). Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes Zhou et al. (2015), Weiss et al. (2015), Watanabe and Sumita (2015) and Andor et al. (2016). In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature combinations, capturing the interaction between characters and tags. Chen et al. (2015a) used a recursive network structure to the same end, extracting more combined features to model complicated character combinations in a five-character window. Chen et al. (2015b) used a LSTM model to capture long-range dependencies between"
P16-1040,P15-1032,0,0.0189251,"figure, we observe that word-based neural model is relatively weaker for sentences with length in [5, 10], while can better tackle long sentences. Third, we compare the two neural models by their capabilities of modeling words with different lengths. Figure 9 shows the results. The perfor428 traction (Li and Ji, 2014) and the work of joint models (Zhang et al., 2013b; Zhang et al., 2014). Recently, the effectiveness of neural features has been studied for this framework. In the natural language parsing community, it has achieved great success. Representative work includes Zhou et al. (2015), Weiss et al. (2015), Watanabe and Sumita (2015) and Andor et al. (2016). In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature combinations, capturing the interaction between characters and tags. Chen et al. (2015a) used a recursive network structure to the same end, extracting more combined features to model complicated character combinations in a five-character window. Chen et al. (2015b) used a LSTM model to capture long"
P16-1040,P15-1117,1,0.444949,"rization parameter and η is used to tune the loss margins. For the discrete models, f (·) denotes the features extracted according to the feature templates in Table 1. For the neural models, f (·) denotes the corresponding hsep and happ . Thus only the output layer is updated, and we further use backpropagation to learn the parameters of the other layers (LeCun et al., 2012). We use online Ada5.2 Development Results To better understand the word-based neural models, we perform several development experiments. All the experiments in this section are conducted on the CTB6 development dataset. 3 Zhou et al. (2015) find that max-margin training did not yield reasonable results for neural transition-based parsing, which is different from our findings. One likely reason is that when the number of labels is small max-margin is effective. 4 425 http://word2vec.googlecode.com/ b16 b8 b4 b2 Model neural -word -character unigram -character bigram -action +discrete features (combined) b1 96 94 92 90 88 R 95.69 92.00 95.56 95.53 95.31 F 95.45 91.90 95.22 95.23 95.17 96.38 96.22 96.30 Table 4: Feature experiments. 86 5 10 15 (a) discrete 20 5 10 15 20 5 10 15 20 1 (b) neural(-tune) (c) neural(+tune) 0.96 neural F"
P16-1040,P16-2092,0,0.353963,"In this work, we apply the transition-based neural framework to Chinese segmentation, in order to exploit wordlevel neural features such as word embeddings. ter bigrams. Pei et al. (2014) used a tensor neural network to achieve extensive feature combinations, capturing the interaction between characters and tags. Chen et al. (2015a) used a recursive network structure to the same end, extracting more combined features to model complicated character combinations in a five-character window. Chen et al. (2015b) used a LSTM model to capture long-range dependencies between characters in a sentence. Xu and Sun (2016) proposed a dependency-based gated recursive neural network to efficiently integrate local and long-distance features. The above methods are all character-based models, making no use of full word information. In contrast, we leverage both character embeddings and word embeddings for better accuracies. 7 Conclusion We proposed a word-based neural model for Chinese segmentation, which exploits not only character embeddings as previous work does, but also word embeddings pre-trained from large scale corpus. The model achieved comparable performances compared with a discrete word-based baseline, a"
P16-1040,P13-1043,1,0.903254,"Missing"
P16-1040,O03-4002,0,0.954327,"., 2013), tensor neural network (Pei et al., 2014), recursive neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of"
P16-1040,P07-1106,1,0.729273,"neural network (Chen et al., 2015a) and longshort-term-memory (LSTM) (Chen et al., 2015b) have been used to derive neural feature representations from input word sequences, which are fed into a CRF inference layer. In this paper, we investigate the effectiveness of word embedding features for neural network segmentation using transition-based models. Since it is challenging to integrate word features to the CRF inference framework of the existing Introduction Statistical word segmentation methods can be categorized character-based (Xue, 2003; Tseng et al., 2005) and word-based (Andrew, 2006; Zhang and Clark, 2007) approaches. The former casts word segmentation as a sequence labeling problem, using segmentation tags on characters to mark their relative positions inside words. The latter, in contrast, ranks candidate segmented outputs directly, extracting both character and full-word features. An influential character-based word segmentation model (Peng et al., 2004; Tseng et al., 2005) uses B/I/E/S labels to mark a character as the beginning, internal (neither beginning nor end), end and only-character (both beginning and end) of a 1 The term in this paper is used to denote the neural network structure"
P16-1040,J11-1005,1,0.863676,"sequence “SEP APP SEP APP SEP APP SEP APP SEP APP”, as shown in Figure 2. Search. Based on the transition system, the decoder searches for an optimal action sequence for a given sentence. Denote an action sequence as A = a1 · · · an . We define the score of A as the total score of all actions in the sequence, which is computed by: X X score(A) = score(a) = w · f (s, q, a), a∈A where w is the model parameters, f is a feature extraction function, s and q are the buffer and queue of a certain state before the action a is applied. The feature templates are shown in Table 1, which are the same as Zhang and Clark (2011). These base features include three main source of information. First, characters in the front of the queue and the end of the buffer are used for scoring both separate and append actions (e.g. c0 ). Second, words that are identified are used to guide separate actions (e.g. w0 ). Third, relevant information of identified words, such as their lengths and first/last characters are utilized for additional features (e.g. len(w−1 )). We follow Zhang and Clark (2011) in using beam-search for decoding, shown in Algorith 1, where Θ is the set of model parameters. Initially the beam contains only the i"
P16-1040,P11-2033,1,0.479981,"Missing"
P16-1040,D15-1153,1,0.831268,"Missing"
P16-1040,P16-1231,0,\N,Missing
S12-1050,W06-2920,0,0.0898414,"the three sections. Data Set CTB files 1-10; 36-65;81-121; Training 1001-1078; 1100-1119; 1126-1140 Devel 66-80; 1120-1125 Test 11-35; 1141-1151 Total 1-121; 1001-1078 1100-1151 # sent. # words. 8301 250311 534 15329 1233 34311 10068 299951 Table 3: Statistics of training, development and test data. 3.2 Data Format The data format is identical to that of a syntactic dependency parsing shared task. All the sentences are in one text file, with each sentence separated by a blank line. Each sentence consists of one or more tokens, and each token is represented on one line consisting of 10 fields. Buchholz and Marsi (2006) provide more detailed information on the format. Fields are separated from each other by a tab. Only five of the 10 fields are used: token id, form, pos tagger, head, and deprel. Head denotes the semantic dependency of each word, and deprel denotes the corresponding semantic relations of the dependency. In the data, the lemma column is filled with the form and the cpostag column with the postag. Figure 2 shows an example. 3.3 Evaluation Method LAS, which is a method widely used in syntactic dependency parsing, is used to evaluate the performance of the semantic dependency parsing system. LAS"
S12-1050,P10-1110,0,0.0245126,"vel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qiaoli-3 system achieved the best results with LAS of 61.84. The LAS val"
S12-1050,P10-1001,0,0.0410518,"semicolon should be used to split the sentence. Second, the last character in a Chinese word is extracted as the lemma, since it usually contains the main sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the se"
S12-1050,W03-1712,0,0.174219,"lization, easy comprehension, high efficiency, and so on. Dependency parsing has been studied intensively in recent decades, with most related work focusing on syntactic structure. Many research papers on Chinese linguistics demonstrate the remarkable difference between semantics and syntax (Jin, 2001; Zhou and Zhang, 2003). Chinese is a meaning-combined language with very flexible syntax, and semantics are more stable than syntax. The word is the basic unit of semantics, and the structure and meaning of a sentence consists mainly of a series of semantic dependencies between individual words (Li et al., 2003). Thus, a reasonable endeavor is to exploit dependency parsing for semantic analysis of Chinese languages. Figure 1 shows an example of Chinese semantic dependency parsing. 378 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 378–384, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics root d-genetive d-restrictive content d-restrictive d-genetive 国际 货币 International Monetary 基金 Fund d-restrictive prep-depend agent 组织 调低 organization turn down 对 for 全球 global d-domain 经济 economy aux-depend 增长 increasing 的 of 预测 prediction Figure 1: An"
S12-1050,D11-1109,1,0.841365,"h dependency. The system-combining strategy involves three steps: 1. Zhou Qiaoli-1, Zhou Qiaoli-2, Zhou Qiaoli-3 These three systems propose a divide-andconquer strategy for semantic dependency parsing. The Semantic Role (SR) phrases are identified (Cai et al., 2011) and then replaced by their head or the SR of the head. The original sentence is thus divided into two types of parts that can be parsed separately. The first type is SR phrase parsing, and the second involves the replacement of SR phrases with either their head or the SR of the head. Finally, the paper takes a graph-based parser (Li et al., 2011) as the semantic dependency parser for all parts. These three systems differ in their phrase identification strategies. 2. NJU-Parser-1, NJU-Parser-2 The NJU-Parser is based on the state-of-theart MSTParser (McDonald, 2006). NJU-Parser applies three methods to enhance semantic dependency parsing. First, sentences are split into sub-sentences using commas and semicolons: (a) sentences are split using only commas and semicolons, as in the primary system, and (b) classifiers are used to determine whether a comma or semicolon should be used to split the sentence. Second, the last character in a Ch"
S12-1050,P05-1013,0,0.0275768,"usually contains the main sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qia"
S12-1050,J08-4003,0,0.0232911,"n sense or semantic class. Third, the multilevel-label is introduced into the semantic relation, for example, the r-{Main Semantic Roles}, with NJUParser exploiting special strategies to handle it. However, this third method does not show positive performance. 3. Zhijun Wu-1 This system extends the second-order of the MSTParser by adding third-order features, and then applying this model to Chinese semantic dependency parsing. In contrast to Koo and Collins (2010) this system does not implement 382 • Parsing each sentence using Nivre’s arc standard, Nivre’s arc eager (Nivre and Nilsson, 2005; Nivre, 2008), and Liang’s dynamic algorithm (Huang and Sagae, 2010); • Combining parses given by the three parsers into a weighted directed graph; • Using the Chu-Liu-Edmonds algorithm to search for the final parse for each sentence. 5. Giuseppe Attardi-SVM-1-R, Giuseppe AttardiSVM-1-rev We didn’t receive the system description of these two systems. 5 Results & Analysis LAS is the main evaluation metric in Chinese Semantic Dependency Parsing, whereas UAS is the secondary metric. Table 4 shows the results for these two indicators in all participating systems. As shown in Table 4, the Zhou Qiaoli-3 system a"
S12-1050,W03-1707,0,0.242389,"Missing"
S12-1050,J03-4003,0,\N,Missing
W12-6316,C10-3004,1,0.878455,"Missing"
W12-6316,D11-1090,0,0.0789906,"ow. Information of unlabeled data can be easily computed and benefit the word segmentation model. When integrated into machine learning framework, it will help reduce sparsity issue caused by the out of vocabulary words. 2.3.1 Mutual Information In probability theory, mutual information measures the mutual dependency of two random variables. Empirical study shows that observation of high mutual information between two characters may indicates real association of these two characters in a word, while low mutual information usually means they belongs to different words. In this paper, we follow Sun and Xu (2011)’s definition of mutual information. For a character bigram ci ci+1 , their mutual information is computed as follow: • character unigram: cs (i − 2 ≤ s ≤ i + 2) • character bigram: cs cs+1 (i − 2 ≤ s ≤ i + 1), cs cs+2 (i − 2 ≤ s ≤ i) • character trigram: cs−1 cs cs+1 (s = i) • repetition of characters: is cs equals cs+1 (i− 1 ≤ s ≤ i), is cs equals cs+2 (i − 2 ≤ s ≤ i) • character type: is ci an alphabet, digit, punctuation or others M I(ci ci+1 ) = log 2.2 Rule Detection Features p(ci ci+1 ) p(ci )p(ci+1 ) For each character ci , M I(ci ci+1 ) and M I(ci−1 ci ) are computed and rounded down"
W12-6316,I11-1035,0,0.0608607,"Missing"
W12-6330,N06-2033,0,0.0369784,"s been investigated by many researchers. Most methods of PSG parsing exploited some manly annotated corpus and proposed a single statistical model (Petrov and Klein, 2007; Zhang and Clark, 2009) based on the corpus. For Chinese, Tsinghua Chinese Treebank (TCT) (Qiang, 2004) and Penn Chinese TreeBank (CTB) (Xue et al., 2005) are two most popular manly annotated corpus. In this paper, we are especially interested in parser combination. Many past works have suggest a number of methods for parser combination. These methods concern on combing different parsers which are trained on the same corpus. Sagae and Lavie (2006) proposed a constituent reparsing method for multiple parsers combina2. The grammars of TCT corpus are very different that of CTB corpus. We should transform CTB grammars into TCT grammars before final combination. If these two issues have been done already, we can apply CKY reparsing algorithm and get the final parsing result. The rest of the paper is organized as follows. Section 2 introduces the overall system architecture. And then we introduce our method in detail. In section 3 we present the binarization algorithm used in the system. Section 4 describes the CKY reparsing algorithm. Secti"
W12-6330,D08-1018,0,0.0588326,"Missing"
W12-6330,W09-3825,0,0.0614032,"Missing"
W12-6330,D09-1161,0,0.0291345,"Missing"
W12-6330,P06-1055,0,0.0839699,"Missing"
W12-6330,N07-1051,0,\N,Missing
W12-6330,J03-4003,0,\N,Missing
