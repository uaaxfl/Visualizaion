2005.sigdial-1.9,C04-1035,1,\N,Missing
2005.sigdial-1.9,daelemans-hoste-2002-evaluation,0,\N,Missing
2005.sigdial-1.9,P05-1031,0,\N,Missing
2005.sigdial-1.9,P04-1044,0,\N,Missing
2005.sigdial-1.9,P04-1019,0,\N,Missing
2007.sigdial-1.9,font-llitjos-black-2002-evaluation,0,0.0274263,"ctive setting and in one with restricted interactivity (push-to-talk). The completely non-interactive material collected here gives us a good further comparison. We were especially interested in the use subjects made of the player tool to recreate some semblance of ‘interactivity’ through stopping, skipping and repeating audio material. The analysis of this is still going on. 4 Related Work As mentioned in the introduction, conducting experiments over the Internet is common practice in Psychology these days (Birnbaum, 2001; Reips, 2002),7 However, these experiments rarely involve audio. (Font Llitjos and Black, 2002; Black and Tokuda, 2005) present experiments on collecting evaluations of speech over the Internet; SpeechRecorder (Draxler, 2006) offers recording over the Internet much like our system, but with no provisions for recording other behavioural measures like reaction times. The combination of experiment / collection with instant user-based quality assessment that our approach offers is, to our knowledge, novel. 5 Conclusions and Future Work We have presented an implemented methodology for distributed collection of speech data. The implemented tool is flexible in the kind of stimuli that can be"
2020.acl-main.365,P08-2063,0,0.239098,"time intervals, one usage per 20-year period over the century. All possible pairwise combinations are generated for each target word, resulting in a total of 3,285 usage pairs. We use the crowdsourcing platform Figure Eight4 to collect five similarity judgements for each of these usage pairs. Annotators are shown pairs of usages of the same word: each usage shows the target word in its sentence, together with the previous and the following sentences (67 tokens on average). Annotators are asked to assign a similarity score on a 4-point scale, ranging from unrelated to identical, as defined by Brown (2008) and used e.g., by Schlechtweg et al. (2018).5 A total of 380 annotators participated in the task. The interrater agreement, measured as the average pairwise Spearman’s correlation between common annotation subsets, is 0.59. This is in line with previous approaches such as Schlechtweg et al. (2018), who report agreement scores between 0.57 and 0.68. Results To obtain a single human similarity judgement per usage pair, we average the scores given by five annotators. We encode all averaged human similarity judgements for a given word in a square matrix. We then compute similarity scores over pai"
2020.acl-main.365,C14-1154,0,0.123069,"Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense induction across time periods (Lau et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al., 2019). Word senses, however, have shortcomings themselves as they are a discretisation of word meaning, which is continuous in nature and modulated by context to convey ad-hoc interpretations (Brugman, 1988; Kilgarriff, 1997; Paradis, 2011). In this work, we propose a usage-based approach to lexical semantic change, where sentential context modulates lexical meaning “on the fly” (Ludlow, 2014). We present a novel method that (1) exploits a pre-trained neural language model (BERT; Devlin et al., 2019) to obtain"
2020.acl-main.365,N19-1210,1,0.905892,"Missing"
2020.acl-main.365,N19-1423,0,0.673228,"et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al., 2019). Word senses, however, have shortcomings themselves as they are a discretisation of word meaning, which is continuous in nature and modulated by context to convey ad-hoc interpretations (Brugman, 1988; Kilgarriff, 1997; Paradis, 2011). In this work, we propose a usage-based approach to lexical semantic change, where sentential context modulates lexical meaning “on the fly” (Ludlow, 2014). We present a novel method that (1) exploits a pre-trained neural language model (BERT; Devlin et al., 2019) to obtain contextualised representations for every occurrence of a word of interest, (2) clusters these representations into usage types, and (3) measures change along time. More concretely, we make the following contributions: • We present the first unsupervised approach to lexical semantic change that makes use of stateof-the-art contextualised word representations. • We propose several metrics to measure semantic change with this type of representation. Our code is available at https://github.com/ glnmario/cwr4lsc. • We create a new evaluation dataset of human sim3960 Proceedings of the 58"
2020.acl-main.365,P09-1002,0,0.0744112,"d by humans (Section 5.2). 5.1 Evaluation of Usage Types The clustering of contextualised representations into usage types is one of the main steps in our method (see Section 3.3). It relies on the similarity values between pairs of usage representations created by the language model. To quantitatively evaluate the quality of these similarity values (and thus, by extension, the quality of usage representations and usage types), we compare them to similarity judgements by human raters. New dataset of similarity judgements We create a new evaluation dataset, following the annotation approach of Erk et al. (2009, 2013) for rating pairs of usages of the same word. Since we need to collect human judgements for pairs of usages, annotating the entire GEMS dataset would be extremely costly and time consuming. Therefore, to limit the scope of the annotation, we select a subset of words. For each shift score value s in the GEMS dataset, we sample a word uniformly at random from the words annotated with s. This results in 16 words. To ensure that our selection of usages is sufficiently varied, for each of these words, we sample five usages from each of their usage types (the number of usage types is word-spe"
2020.acl-main.365,J13-3003,0,0.11021,"Missing"
2020.acl-main.365,D08-1094,0,0.173004,"Missing"
2020.acl-main.365,P10-2017,0,0.0910495,"Missing"
2020.acl-main.365,Q16-1003,0,0.499014,"al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense induction across time periods (Lau et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al., 2019). Word senses, however, have shortcomings themselves as they are a discretisation of word meaning, which is continuous in nature and modulated by context to convey ad-hoc interpretations (Brugman, 1988; Kilgarriff, 1997; Paradis, 2011). In this work, we propose a usage-based approach to lexical semantic change, where sentential context modulates lexical meaning “on the fly” (Ludlow, 2014). We present a novel method that (1) exploits a pre-trained neural language model (BERT; Devlin et al., 2019) to obtain contextualised representations for every occurr"
2020.acl-main.365,W11-2508,0,0.503232,"orrect understanding of the meaning of individual words underpins general machine reading comprehension, it has become increasingly relevant for computational linguists to detect and characterise lexical semantic change—e.g., in the form of laws of semantic change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016)—with the aid of quantitative and reproducible evaluation procedures (Schlechtweg et al., 2018). Most recent studies have focused on shift detection, the task of deciding whether and to what extent the concept evoked by a word has changed between time periods (e.g., Gulordava and Baroni, 2011; Kim et al., 2014; Kulkarni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense inducti"
2020.acl-main.365,P16-1141,0,0.647204,"girl, designating exclusively female individuals, whereas by the sixteenth century boy had lost its servile connotation and was more broadly used to refer to any male child, becoming the masculine counterpart of girl (Bybee, 2015). Word meaning is indeed in constant mutation and, since correct understanding of the meaning of individual words underpins general machine reading comprehension, it has become increasingly relevant for computational linguists to detect and characterise lexical semantic change—e.g., in the form of laws of semantic change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016)—with the aid of quantitative and reproducible evaluation procedures (Schlechtweg et al., 2018). Most recent studies have focused on shift detection, the task of deciding whether and to what extent the concept evoked by a word has changed between time periods (e.g., Gulordava and Baroni, 2011; Kim et al., 2014; Kulkarni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of"
2020.acl-main.365,P18-1031,0,0.0292637,"oduced a clustering-based disambiguation algorithm for word usage vectors, Erk and Pad´o (2008) proposed creating multiple vectors for the same word and Erk and Pad´o (2010) proposed to directly learn usage-specific representations based on the set of exemplary contexts within which the target word occurs. Recently, neural contextualised word representations have gained widespread use in NLP, thanks to deep learning models which learn usage-dependent representations while optimising tasks such as machine translation (CoVe; McCann et al., 2017) and language modelling (Dai and Le, 2015, ULMFiT; Howard and Ruder, 2018, ELMo; Peters et al., 2018, GPT; Radford et al., 2018, 2019, BERT; Devlin et al., 2019). State-of-the-art language models typically use stacked attention layers (Vaswani et al., 2017), they are pre-trained on a very large amount of textual data, and they can be fine-tuned for specific downstream tasks (Howard and Ruder, 2018; Radford et al., 2019; Devlin et al., 2019). Contextualised representations have been shown to encode lexical meaning dynamically, reaching high accuracy on, e.g., the binary usage similarity judgements of the WiC evaluation set (Pilehvar and Camacho-Collados, 2019), perf"
2020.acl-main.365,P19-1379,0,0.506127,"k, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense induction across time periods (Lau et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al., 2019). Word senses, however, have shortcomings themselves as they are a discretisation of word meaning, which is continuous in nature and modulated by context to convey ad-hoc interpretations (Brugman, 1988; Kilgarriff, 1997; Paradis, 2011). In this work, we propose a usage-based approach to lexical semantic change, where sentential context modulates lexical meaning “on the fly” (Ludlow, 2014). We present a novel method that (1) exploits a pre-trained neural language model (BERT; Devlin et al., 2019) to obtain contextualised representations for every occurrence of a word of interest, (2) clusters t"
2020.acl-main.365,W14-2517,0,0.360637,"meaning of individual words underpins general machine reading comprehension, it has become increasingly relevant for computational linguists to detect and characterise lexical semantic change—e.g., in the form of laws of semantic change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016)—with the aid of quantitative and reproducible evaluation procedures (Schlechtweg et al., 2018). Most recent studies have focused on shift detection, the task of deciding whether and to what extent the concept evoked by a word has changed between time periods (e.g., Gulordava and Baroni, 2011; Kim et al., 2014; Kulkarni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense induction across time per"
2020.acl-main.365,C18-1117,0,0.337882,"contextualised word representations for modelling and analysing lexical semantic change and opens the door to further work in this direction. 2 Related Work Semantic change modelling Lexical semantic change models build on the assumption that meaning change results in the modification of a word’s linguistic distribution. In particular, with the exception of a few methods based on word frequencies and parts of speech (Michel et al., 2011; Kulkarni et al., 2015), lexical semantic change detection has been addressed following two main approaches: form-based and sense-based (for an overview, see Kutuzov et al., 2018; Tang, 2018). In form-based approaches independent models are trained on the time intervals of a diachronic corpus and the distance between representations of the same word in different intervals is used as a semantic change score (Gulordava and Baroni, 2011; Kulkarni et al., 2015). Representational coherence between word vectors across different periods can be guaranteed by incremental training procedures (Kim et al., 2014) as well as by post hoc alignment of semantic spaces (Hamilton et al., 2016). More recent methods capture diachronic word usage by learning dynamic word embeddings that va"
2020.acl-main.365,P14-1025,0,0.0607879,"Missing"
2020.acl-main.365,E12-1060,0,0.274804,"arni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense induction across time periods (Lau et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al., 2019). Word senses, however, have shortcomings themselves as they are a discretisation of word meaning, which is continuous in nature and modulated by context to convey ad-hoc interpretations (Brugman, 1988; Kilgarriff, 1997; Paradis, 2011). In this work, we propose a usage-based approach to lexical semantic change, where sentential context modulates lexical meaning “on the fly” (Ludlow, 2014). We present a novel method that (1) exploits a pre-trained neural language model (BERT; Devlin et al"
2020.acl-main.365,P14-1096,0,0.147892,"Missing"
2020.acl-main.365,S13-2035,0,0.0229648,"s of the word atom in COHA (Davies, 2012). Colours encode usage types. occurrence (a 128-token window around the target word) as well as a temporal label tw indicating the time interval of the usage. 3.3 Usage Types Once we have obtained a word-specific matrix of usage vectors Uw , we standardise it and cluster its entries using K-Means.2 This step partitions usage representations into clusters of similar usages of the same word, or usage types (see Figure 1a), and thus it is directly related to automatic word sense discrimination (Sch¨utze, 1998; Pantel and Lin, 2002; Manandhar et al., 2010; Navigli and Vannella, 2013, among others). For each word independently, we automatically select the number of clusters K that maximises the silhouette score (Rousseeuw, 1987), a metric of cluster quality which favours intra-cluster coherence and penalises inter-cluster similarity, without the need for gold labels. For each value of K, we execute 10 iterations of Expectation Maximization to alleviate the influence of different initialisation values (Arthur and Vassilvitskii, 2007). The final clustering for a given K is the one that yields the minimal distortion value across the 10 runs, i.e., the minimal sum of squared"
2020.acl-main.365,N18-1202,0,0.112941,"mbiguation algorithm for word usage vectors, Erk and Pad´o (2008) proposed creating multiple vectors for the same word and Erk and Pad´o (2010) proposed to directly learn usage-specific representations based on the set of exemplary contexts within which the target word occurs. Recently, neural contextualised word representations have gained widespread use in NLP, thanks to deep learning models which learn usage-dependent representations while optimising tasks such as machine translation (CoVe; McCann et al., 2017) and language modelling (Dai and Le, 2015, ULMFiT; Howard and Ruder, 2018, ELMo; Peters et al., 2018, GPT; Radford et al., 2018, 2019, BERT; Devlin et al., 2019). State-of-the-art language models typically use stacked attention layers (Vaswani et al., 2017), they are pre-trained on a very large amount of textual data, and they can be fine-tuned for specific downstream tasks (Howard and Ruder, 2018; Radford et al., 2019; Devlin et al., 2019). Contextualised representations have been shown to encode lexical meaning dynamically, reaching high accuracy on, e.g., the binary usage similarity judgements of the WiC evaluation set (Pilehvar and Camacho-Collados, 2019), performing on a 3961 par with s"
2020.acl-main.365,N19-1128,0,0.0511493,"Missing"
2020.acl-main.365,N18-1044,0,0.0956186,"or computational linguists to detect and characterise lexical semantic change—e.g., in the form of laws of semantic change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016)—with the aid of quantitative and reproducible evaluation procedures (Schlechtweg et al., 2018). Most recent studies have focused on shift detection, the task of deciding whether and to what extent the concept evoked by a word has changed between time periods (e.g., Gulordava and Baroni, 2011; Kim et al., 2014; Kulkarni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word meaning hardly ever shifts directly from one sense to another, but rather typically goes through polysemous stages (Hopper et al., 1991). This limitation has motivated recent work on word sense induction across time periods (Lau et al., 2012; Cook et al., 2014; Mitra et al., 2014; Frermann and Lapata, 2016; Rudolph and Blei, 2018; Hu et al"
2020.acl-main.365,N18-2027,0,0.665876,"lost its servile connotation and was more broadly used to refer to any male child, becoming the masculine counterpart of girl (Bybee, 2015). Word meaning is indeed in constant mutation and, since correct understanding of the meaning of individual words underpins general machine reading comprehension, it has become increasingly relevant for computational linguists to detect and characterise lexical semantic change—e.g., in the form of laws of semantic change (Dubossarsky et al., 2015; Xu and Kemp, 2015; Hamilton et al., 2016)—with the aid of quantitative and reproducible evaluation procedures (Schlechtweg et al., 2018). Most recent studies have focused on shift detection, the task of deciding whether and to what extent the concept evoked by a word has changed between time periods (e.g., Gulordava and Baroni, 2011; Kim et al., 2014; Kulkarni et al., 2015; Del Tredici et al., 2019; Hamilton et al., 2016; Bamler and Mandt, 2017; Rosenfeld and Erk, 2018). This line of work relies mainly on distributional semantic models, which produce one abstract representation for every word form. However, aggregating all senses of a word into a single representation is particularly problematic for semantic change as word mea"
2020.acl-main.365,J98-1004,0,0.91112,"Missing"
2020.acl-main.365,S10-1011,0,\N,Missing
2020.coling-main.477,D19-1477,1,0.849072,"Missing"
2020.coling-main.477,W18-5408,0,0.0527688,"nd in NLP (Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2017). We therefore propose to leverage user-generated language, an abundant resource in social media, to create user representations based solely on users’ language production. We expect, in this way, to indirectly capture the factors characterizing people who spread fake news. We implement a model for fake news detection which jointly models news and user-generated texts. We use Convolutional Neural Networks (CNNs), which were shown to perform well on text classification tasks (Kalchbrenner et al., 2014) and are highly interpretable (Jacovi et al., 2018), i.e., they allow us to extract the informative linguistic features of the input texts. We test our model on two public English This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 5467 Proceedings of the 28th International Conference on Computational Linguistics, pages 5467–5479 Barcelona, Spain (Online), December 8-13, 2020 datasets for fake news detection based on Twitter data, both including news and, for each news, the users who spread them on Twitter. We leverage two kinds of user-generated l"
2020.coling-main.477,P14-1062,0,0.0196412,"nd Linguistics (Pennebaker et al., 2003; De Fina, 2012) and in NLP (Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2017). We therefore propose to leverage user-generated language, an abundant resource in social media, to create user representations based solely on users’ language production. We expect, in this way, to indirectly capture the factors characterizing people who spread fake news. We implement a model for fake news detection which jointly models news and user-generated texts. We use Convolutional Neural Networks (CNNs), which were shown to perform well on text classification tasks (Kalchbrenner et al., 2014) and are highly interpretable (Jacovi et al., 2018), i.e., they allow us to extract the informative linguistic features of the input texts. We test our model on two public English This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 5467 Proceedings of the 28th International Conference on Computational Linguistics, pages 5467–5479 Barcelona, Spain (Online), December 8-13, 2020 datasets for fake news detection based on Twitter data, both including news and, for each news, the users who spread them on"
2020.coling-main.477,W17-1604,0,0.0276023,"Missing"
2020.coling-main.477,I17-2043,0,0.0288459,"t fake news originate from users with a few followers, and are spread by influential people. A parallel line of research considers the users who spread fake news. Some works focus on the detection of non-human agents (bots) involved in the spreading process (Bessi and Ferrara, 2016), while others model the characteristics of human spreaders, as we do in this paper. Gupta et al. (2013) and Zubiaga et al. (2016) represent users with simple features such as longevity on Twitter and following/friends relations, and show that these features have limited predictive power. Kirilin and Strube (2018), Long et al. (2017) and Reis et al. (2019) use more informative features, such as users’ political party affiliation, job and credibility scores. While leading to improvements on the task, features of this kind are usually either hard to retrieve or have to be manually defined, which hinders the possibility to scale the methodology to large sets of unseen users. Guess et al. (2019) and Shu et al. (2019a) rely on manually annotated lists of news providers, thus presenting a similar scalability problem. Finally, Shu et al. (2019b) represent users by mixing different kinds of information, e.g., previous tweets, loc"
2020.coling-main.477,P17-1066,0,0.0266668,"Mihalcea and Strapparava, 2009; Rashkin et al., 2017; P´erez-Rosas et al., 2018). Despite some positive results, this approach is inherently undermined by the fact that fake news are often written in such a way as to look like real news. More recently, researchers have explored the possibility to leverage social information together with the one derived from news texts. Some works focus on the patterns of propagation of fake news in social networks. Vosoughi et al. (2018) show that, compared to real news, fake news have deeper propagation trees, spread faster and reach a wider audience, while Ma et al. (2017) show that fake news originate from users with a few followers, and are spread by influential people. A parallel line of research considers the users who spread fake news. Some works focus on the detection of non-human agents (bots) involved in the spreading process (Bessi and Ferrara, 2016), while others model the characteristics of human spreaders, as we do in this paper. Gupta et al. (2013) and Zubiaga et al. (2016) represent users with simple features such as longevity on Twitter and following/friends relations, and show that these features have limited predictive power. Kirilin and Strube"
2020.coling-main.477,P09-2078,0,0.304805,"media, and their alarming effects on our lives (Lazer et al., 2018). Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020). In NLP a considerable amount of work has been dedicated to fake news detection, i.e., the task of classifying a news as either real or fake – see Zhou and Zafarani (2020), Kumar and Shah (2018) and Oshikawa et al. (2020) for overviews. While initial work focused uniquely on the textual content of the news (Mihalcea and Strapparava, 2009), subsequent research has considered also the social context in which news are consumed, characterizing, in particular, the users who spread news in social media. In line with the results reported in other classification tasks of user-generated texts (Del Tredici et al., 2019; Pan and Ding, 2019), several studies show that leveraging user representations, together with news’ ones, leads to improvements in fake news detection. In these studies, user representations are usually computed using informative but costly features, such as manually assigned credibility scores (Kirilin and Strube, 2018)"
2020.coling-main.477,2020.lrec-1.747,0,0.0540923,"of paramount relevance in our society, due to their large diffusion in public discourse, especially on social media, and their alarming effects on our lives (Lazer et al., 2018). Several works show that fake news played a role in major events such as the US Presidential Elections (Allcott and Gentzkow, 2017), stock market trends (Rapoza, 2017), and the Coronavirus disease outbreak (Shimizu, 2020). In NLP a considerable amount of work has been dedicated to fake news detection, i.e., the task of classifying a news as either real or fake – see Zhou and Zafarani (2020), Kumar and Shah (2018) and Oshikawa et al. (2020) for overviews. While initial work focused uniquely on the textual content of the news (Mihalcea and Strapparava, 2009), subsequent research has considered also the social context in which news are consumed, characterizing, in particular, the users who spread news in social media. In line with the results reported in other classification tasks of user-generated texts (Del Tredici et al., 2019; Pan and Ding, 2019), several studies show that leveraging user representations, together with news’ ones, leads to improvements in fake news detection. In these studies, user representations are usually"
2020.coling-main.477,D14-1162,0,0.087248,"indicates a significant improvement over the News setup. Hyperparameters For each setup we perform grid hyperparameter search on the validation set using early stopping with patience value 10. We experiment with values 10, 20 and 40 for the number of filters, and 0.0, 0.2, 0.4 and 0.6 for dropout. In all setups batch size is equal to 8, filters focus on uni-grams, bi-grams and tri-grams, and we use Adam optimizer (Kingma and Ba, 2015) with learning rate of 0.001, β1 = 0.9 and β2 = 0.999. All the CNN modules have depth 1, and are initialized with 200-d GloVe embeddings pretrained on Twitter (Pennington et al., 2014). We train the SVM baseline on uni-grams, bi-grams and tri-grams. When modelling user information, we concatenate the user-generated texts of the users spreading the target news. We use the rbf kernel, and perform grid hyperparameter search on the validation set. We explore values 1, 2, 5, 10, 15 and 30 for the hyperparameter C, and 1e−05 , 1e−04 , 1e−03 , 1e−02 , 1.0 for γ. For both CNN and SVM models, we use binary F-score as optimization metric, and indicate the fake class as the target class. 6 Results We report the results of the fake news detection task in Table 2. The results of our CNN"
2020.coling-main.477,C18-1287,0,0.0577452,"Missing"
2020.coling-main.477,W15-2913,0,0.0153417,"ormative features (e.g., connections on social media), report less encouraging results (Zubiaga et al., 2016). Our work also focuses on users. We build on psychological studies that show that some people are more prone than others to spread fake news, and that these people usually share a set of cognitive and social factors, such as personality traits, beliefs and ideology (Pennycook et al., 2015; Pennycook and Rand, 2017). Also, we rely on studies showing a relation between these factors and language use, both in Psychology and Linguistics (Pennebaker et al., 2003; De Fina, 2012) and in NLP (Plank and Hovy, 2015; Preot¸iuc-Pietro et al., 2017). We therefore propose to leverage user-generated language, an abundant resource in social media, to create user representations based solely on users’ language production. We expect, in this way, to indirectly capture the factors characterizing people who spread fake news. We implement a model for fake news detection which jointly models news and user-generated texts. We use Convolutional Neural Networks (CNNs), which were shown to perform well on text classification tasks (Kalchbrenner et al., 2014) and are highly interpretable (Jacovi et al., 2018), i.e., the"
2020.coling-main.477,P17-1068,0,0.0608964,"Missing"
2020.coling-main.477,D17-1317,0,0.0141436,"function of the similarity between the content of connected users and their distance in the social graph. By applying our methodology, we show that the Echo Chamber effect is at play, to different extent, in both the datasets under scrutiny. Modelling user-generated data requires careful consideration of the possible ethical aspects related to the treatment of such data. We provide an ethics statement in Appendix A with details on how we have dealt with these aspects. 2 Related Work Several studies on fake news detection focus uniquely on the text of the news (Mihalcea and Strapparava, 2009; Rashkin et al., 2017; P´erez-Rosas et al., 2018). Despite some positive results, this approach is inherently undermined by the fact that fake news are often written in such a way as to look like real news. More recently, researchers have explored the possibility to leverage social information together with the one derived from news texts. Some works focus on the patterns of propagation of fake news in social networks. Vosoughi et al. (2018) show that, compared to real news, fake news have deeper propagation trees, spread faster and reach a wider audience, while Ma et al. (2017) show that fake news originate from"
2020.coling-main.477,W18-0801,0,0.020379,"Missing"
2020.coling-main.477,Q17-1021,0,0.0431204,"Missing"
2020.emnlp-main.353,W05-0909,0,0.0187872,"on and their corresponding image target from the dialogues as follows. Within a game round, we consider all the utterances up to the point where a given image i has been identified by the participants2 as candidate referring utterances for i – see Figure 2. We then compare each candidate against a reference set of descriptions made up of the MS COCO (Lin et al., 2014) captions for i and the attributes and relationship tokens of i in the Visual Genome (Krishna et al., 2017). We score each candidate utterance with the sum of its BERTScore3 (Zhang et al., 2020) for captions and its METEOR score (Banerjee and Lavie, 2005) for attributes and relationships. The top-scoring utterance in the game round is selected as a referring utterance for i and used as an additional caption for extracting subsequent references in the following game rounds. As a result of this procedure, for a given dialogue and an image i, we obtain a reference chain made up of the referring utterances—maximum one per round—that refer to i in the dialogue. Since images do not always reappear in each round, chains can have different 1 Haber et al. (2019) extracted co-reference chains made up of multi-utterance dialogue excerpts. Our chains incl"
2020.emnlp-main.353,W09-0612,0,0.0740217,"Missing"
2020.emnlp-main.353,W16-3622,0,0.066043,"Missing"
2020.emnlp-main.353,P02-1026,0,0.0657432,"essing, pages 4350–4368, c November 16–20, 2020. 2020 Association for Computational Linguistics tures of subsequent mentions: (1) Reduction: Utterances tend to become shorter—a well attested phenomenon since the work of Krauss and Weinheimer (1967)—as a result of interlocutors’ reliance on their common ground (Stalnaker, 2002): As more shared information is accumulated, it becomes predictable and can be left implicit (Grice, 1975; Clark and Wilkes-Gibbs, 1986; Clark and Brennan, 1991; Clark, 1996). Sentence compression also takes place in discourse, as predicted by the entropy rate principle (Genzel and Charniak, 2002; Keller, 2004). (2) Lexical entrainment: Speakers tend to reuse words that were effective in previous mentions (Garrod and Anderson, 1987; Brennan and Clark, 1996) possibly due to priming effects (Pickering and Garrod, 2004). Thus, besides being a challenging problem intriguing from a linguistic and psycholinguistic point of view, computationally modelling the generation of subsequent references can contribute to better user adaptation in dialogue systems and to more natural humancomputer interaction. For our study, we use data from the PhotoBook dataset (Haber et al., 2019), developed to eli"
2020.emnlp-main.353,J12-1006,0,0.0502648,"Missing"
2020.emnlp-main.353,W04-1013,0,0.014974,"ction, we report average scores and standard deviations over 5 runs with different random seeds. Further details on hyperparameter selection, model configurations, and reproducibility can be found in Appendix E. 5 5.1 Results Evaluation Measures We evaluate the performance of the reference resolution model by means of both accuracy and Mean Reciprocal Rank (MRR). As for the generation models, we compute several metrics that are commonly used in the domain of Natural Language Generation. In particular, we consider three measures based on n-gram matching: BLEU-2 (Papineni et al., 2002),8 ROUGE (Lin, 2004), and observed. 7 Thus, some of the candidate images have multimodal representations (if they were already mentioned in the dialogue), while others do not. 8 BLEU-2, which is based on bigrams, appears to be more informative than BLEU with longer n-grams in dialogue reCIDEr (Vedantam et al., 2015). We also compute BERTScore F1 (Zhang et al., 2020) (used for model selection), which in our setup compares the contextual embeddings of the generated sentence to those of the set of referring utterances in the given chain. Further details of the metrics are in Appendix D. All these measures capture th"
2020.emnlp-main.353,T78-1009,0,0.640115,"g on the red chair Figure 1: Two chains of referring utterances from two games with different participants, including the first description of the target image in that dialogue and two subsequent references (;). In the game, each participant sees 5 additional images besides the target shown here. The distractor images change at every round of the game, i.e., each co-referring utterance within a dialogue is produced in a slightly different visual context. Introduction When speakers engage in conversation, they often refer to the same objects or situations more than once. Subsequent references (McDonald, 1978) are dependent on the shared knowledge that speakers accumulate during dialogue. For example, dialogue participants may first mention “a white fuzzy dog with a wine glass up to his face” and later refer to it as “the wine glass dog”, as shown in Figure 1, dialogue 1. Speakers establish ‘conceptual pacts’, i.e., particular ways of conceptualising referents that condition what is perceived as coherent in a given dialogue (Garrod and Anderson, 1987; Brennan and Clark, 1996). While “the wine glass dog” may be odd as a standalone description, it is an appropriate referring expression in the above c"
2020.emnlp-main.353,N13-1137,0,0.0677275,"Missing"
2020.emnlp-main.353,D11-1107,0,0.0621944,"Missing"
2020.emnlp-main.377,D08-1026,0,0.0285195,"a single speaker by directly inputting information about where that speaker looks at during description production, and compare this to the aggregation approach. In addition, we exploit the sequential nature of gaze patterns, i.e., the so-called scanpath, and contrast this with the use of static saliency maps. Gaze scanpaths have been used in NLP for diverse purposes: For example, to aid part-of-speech tagging (Barrett et al., 2016) and chunking (Klerke and Plank, 2019); to act as a regulariser in sequence classification tasks (Barrett et al., 2018); as well as for automatic word acquisition (Qu and Chai, 2008) and reference resolution (Kennington et al., 2015). To our knowledge, the present study is the first attempt to investigate sequential gaze information for the specific task of image description generation. 3 Data We utilise the Dutch Image Description and EyeTracking Corpus (DIDEC; van Miltenburg et al., 2018). In particular, we use the data collected as part of the description-view task in DIDEC, where participants utter a spoken description in Dutch for each image they look at. The gaze of the participants is recorded with an SMI RED 250 eyetracking device while they describe an image. Ove"
2020.emnlp-main.377,L16-1652,0,0.0120909,"positional distance, and normalise it by the length of the longest sentence in the pair (here R), obtaining |2 − 6|/9 ≈ 0.44. We then sum up the cosine distance and the positional distance to obtain a score for ‘lovely’: 0.33 + 0.44 = 0.77. To obtain the overall gr value, we add up the scores for all words in G. We compute rg in a similar manner and obtain SSD as follows: SSD = (gr + rg)/2. 5 Cross-Modal Coordination Analysis where Rs (i) is the semantically closest element to Gi in R, and cos in our experiments is computed over word2vec embeddings trained on the 4Btoken corpus in Dutch, COW (Tulkens et al., 2016). To empirically motivate our generation models, as a preliminary experiment we investigate the level of coordination between visual attention and linguistic production in the DIDEC dataset. In particular, we test whether scanpath similarity and sentence similarity are correlated and whether taking into account the sequential nature of the two modalities results in higher cross-modal alignment. We transform gaze data into time-ordered sequences of object labels, i.e., scanpaths, (e.g., S = ‘cat’, ‘person’, ‘cat’, ‘table’) using the annotations of object bounding boxes in the MS COCO image data"
2020.emnlp-main.377,W15-0111,0,0.266174,"e and vision is complex. On the one hand, eye movements are influenced by the task at hand, such as locating objects or verbally describing an image (Buswell, 1935; Yarbus, 1967). On the other hand, visual information processing plays a role in guiding linguistic production (e.g., Griffin, 2004; Gleitman et al., 2007). Such cross-modal coordination unfolds sequentially in the specific task of image description (Coco and Keller, 2012)—i.e., objects tend to be looked at before being mentioned. Yet, the temporal alignment between the two modalities is not straightforward (Griffin and Bock, 2000; Vaidyanathan et al., 2015) In this paper, we follow up on these findings and investigate cross-modal alignment in image description by modelling the description generation process computationally. We take a state-of-the-art system for automatic image captioning (Anderson et al., 2018) and develop several model variants that exploit information derived from eye-tracking data. To train these models, we use a relatively small dataset of image descriptions in Dutch (DIDEC; van Miltenburg et al., 2018) that includes information on gaze patterns collected during language production. We hypothesise that a system that encodes"
2020.emnlp-main.377,P18-2022,0,0.0638859,"Missing"
2020.emnlp-main.377,D19-1053,0,0.0235381,"005) and SPICE (Anderson et al., 2016) also make use of n-grams (or tuples in a scene’s graph, in the case of SPICE) and take into account semantic similarity by matching synonyms using WordNet (Pedersen et al., 2004). This allows for some flexibility, but can be too restrictive to grasp overall semantic similarity. To address this, Kilickaya et al. (2017) proposed using WMD, which builds on word2vec embeddings (Mikolov et al., 2013); more recently, several metrics capitalising on contextual embeddings (Devlin et al., 2019) were proposed, such as BERTScore (Zhang et al., 2020) and MoverScore (Zhao et al., 2019). However, these metrics neglect the sequential alignment of sentences.3 SSD We propose Semantic and Sequential Distance (SSD), a metric which takes into account both semantic similarity and the overall relative order of words. Regarding the latter, SSD is related to Ordering-based Sequence Similarity (OSS; G´omez-Alonso and Valls, 2008), a measure used by Coco and Keller (2010) to compare sequences of categories representing gaze patterns.4 Given two sequences of words, i.e., a generated sentence G and a reference sentence R, SSD provides a single positive value representing the overall dissi"
2021.emnlp-main.652,2020.emnlp-main.353,1,0.891326,"a subset reappears, thus triggering subsequent references to previously described photographs. This task design allows us to investigate the following types of contextual unit: a) the overall dialogue: throughout a game, all the photographs are about a certain domain (e.g., food or dogs); b) a dialogue round: different images are described in succession as participants try to figure out which ones they share in a given round; c) an image reference chain: the (non-adjacent) utterances that refer to a certain image across rounds (we use the automatic annotation of referring utterance chains by Takmaz et al., 2020). We hypothesise that, in MT, the UID principle will be more visible at the transaction level, where the context is more topically coherent, than at the dialogue level, where a dozen different landmarks are brought up in succession—in particular when only information-transmission dialogue acts are taken into account. In PB, we expect the strongest effect to be present at the level of reference chains. Chains are determined both topically, by the target image, and lexically, by the conceptual pacts established in previous mentions of a target (Brennan and Clark, 1996). In rounds and dialogues,"
2021.emnlp-main.652,2021.acl-long.404,0,0.0184005,"nd thus of the informativity of its linguistic context I(Si ; Ci , Li ). The study presented in this paper provides new empirical evidence on language production in dialogue which we believe can directly inform the development of natural language generation models. Our findings suggest that models that take relevant contextual units into account (Takmaz et al., 2020; Hawkins et al., 2020) are better suited for reproducing human patterns of information transmission, and confirm that the use of training objectives that enforce a uniform organisation of information density (Meister et al., 2020; Wei et al., 2021) is a promising avenue for training language models. We investigated to what extent the principle of uniform information density holds in two corpora of English task-oriented dialogues. We have related the properties of task-determined contextual units to patterns of information transmission and have hypothesised that the UID principle holds to Acknowledgements a stronger degree in more topically coherent and reference-specific contextual units. Our hypotheses are confirmed in PhotoBook, We would like to thank Jaap Jumelet for a helpful where we find evidence that dialogue participants discuss"
2021.repl4nlp-1.16,Q19-1004,0,0.127102,"scene” (Figure 1). Such multi-step relational reasoning is at play in many real-life situations: e.g., the same exact pan may count as ‘big’ in all contexts except a restaurant kitchen. We experiment with two types of models to solve this task: a modular neural network (Hu et al., 2017) and LXMERT, a pre-trained multimodal transformer (Tan and Bansal, 2019). We probe the learned representations of LXMERT to assess whether, and to what extent, it has learned the underlying structure of the data. By means of two experiments with probing classifiers (Alain and Bengio, 2017; Hupkes et al., 2018; Belinkov and Glass, 2019), we first verify that it is able to perform the task at the image level (i.e., to compute the relative size of the target object at the image level); then, we test its ability to reason at the multi-image level and detect the image that stands out. The experiments show that LXMERT is able to solve the multi-step relational reasoning task 152 Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021), pages 152–162 Bangkok, Thailand (Online), August 6, 2021. ©2021 Association for Computational Linguistics T there is exactly one blue triangle that is small in its image i"
2021.repl4nlp-1.16,N18-2070,0,0.0173076,"derstanding (Jhamtani and Berg-Kirkpatrick, 2018); when it does, the changes often involve one object’s fixed attribute (color, shape, material, etc.) rather than a contextually-defined property whose applicability depends on the other objects in the image.2 A similar, partially overlapping task is discriminative captioning: systems are fed with a set of similar images and asked to provide a description that unequivocally refers to a target one. Many approaches have been proposed focusing on synthetic (Andreas and Klein, 2016; Achlioptas et al., 2019) or natural scenes (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Vered et al., 2019), very often embedding pragmatic components based on the Rational Speech Acts framework (RSA; Goodman and Frank, 2016). Also in this case, however, differences among images mainly involve intrinsic attributes of the objects rather than relational properties defined at the level of the image. 4 4.1 a positive value < 0.5.3 Thus, an object with a certain area can count as big in one image and as small in another one. In total, the POS1 dataset contains 20K himage, statementi datapoints (16K train, 2K val, 2K test), where statements are about the size of a target object based"
2021.repl4nlp-1.16,N19-1423,0,0.0774388,"Missing"
2021.repl4nlp-1.16,D19-1065,0,0.0159132,"he single image and the multi-image context. 1 The code to generate the data, and to train and evaluate the models, is available at https://github.com/ jig-san/multi-step-size-reasoning. 153 3.2 Multi-Image Approaches Our approach is also related to other work in language and vision involving multiple images. One is the spot-the-difference task: in Jhamtani and Berg-Kirkpatrick (2018), models are fed with pairs of video-surveillance images that only differ in one detail, and asked to generate text which describes such difference. The same task—with different real-scene datasets—is explored by Forbes et al. (2019) and Su et al. (2017); others experiment with pairs of similar images drawn from CLEVR (Johnson et al., 2017) or similar synthetic 3D datasets (Park et al., 2019; Qiu et al., 2020). This task is akin to ours since it requires a higherlevel reasoning step: systems must reason over the two independent representations to describe what is different. However, in practice, it does not always require semantic understanding (Jhamtani and Berg-Kirkpatrick, 2018); when it does, the changes often involve one object’s fixed attribute (color, shape, material, etc.) rather than a contextually-defined proper"
2021.repl4nlp-1.16,D16-1125,0,0.0240677,"to describe what is different. However, in practice, it does not always require semantic understanding (Jhamtani and Berg-Kirkpatrick, 2018); when it does, the changes often involve one object’s fixed attribute (color, shape, material, etc.) rather than a contextually-defined property whose applicability depends on the other objects in the image.2 A similar, partially overlapping task is discriminative captioning: systems are fed with a set of similar images and asked to provide a description that unequivocally refers to a target one. Many approaches have been proposed focusing on synthetic (Andreas and Klein, 2016; Achlioptas et al., 2019) or natural scenes (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Vered et al., 2019), very often embedding pragmatic components based on the Rational Speech Acts framework (RSA; Goodman and Frank, 2016). Also in this case, however, differences among images mainly involve intrinsic attributes of the objects rather than relational properties defined at the level of the image. 4 4.1 a positive value < 0.5.3 Thus, an object with a certain area can count as big in one image and as small in another one. In total, the POS1 dataset contains 20K himage, statementi datapoin"
2021.repl4nlp-1.16,D19-1275,0,0.0439159,"Missing"
2021.repl4nlp-1.16,D18-1436,0,0.0465296,"Missing"
2021.repl4nlp-1.16,P17-2034,0,0.112121,"any of these steps. In other words, the training data does not indicate which images contain an object that counts as big/small nor explicitly how many images contain a big/small target. 3 3.1 Related Work Visual Reasoning To evaluate reasoning abilities of multimodal models, several datasets of synthetic scenes and questions, such as CLEVR (Johnson et al., 2017), ShapeWorld (Kuhnle and Copestake, 2017), and MALeViC (Pezzelle and Fern´andez, 2019) have been proposed in recent years. Our work directly builds on them, and particularly on approaches adopting a multi-image setting, such as NLVR (Suhr et al., 2017) and NLVR2 (which, however, contains pairs of natural scenes; Suhr et al., 2019). In NLVR, in particular, a crowdsourced statement is coupled with a synthetic scene including 3 independent images, and models must verify whether the statement is true or false with respect to the entire visual input. This involves handling phenomena such as counting, negation or comparisons, that require perform relational reasoning over the entire scene, e.g.: There is a black item in every box, There is a tower with yellow base, etc. However, most hscene, statementi pairs do not challenge models to do the same"
2021.repl4nlp-1.16,P19-1644,0,0.231937,"mages contain an object that counts as big/small nor explicitly how many images contain a big/small target. 3 3.1 Related Work Visual Reasoning To evaluate reasoning abilities of multimodal models, several datasets of synthetic scenes and questions, such as CLEVR (Johnson et al., 2017), ShapeWorld (Kuhnle and Copestake, 2017), and MALeViC (Pezzelle and Fern´andez, 2019) have been proposed in recent years. Our work directly builds on them, and particularly on approaches adopting a multi-image setting, such as NLVR (Suhr et al., 2017) and NLVR2 (which, however, contains pairs of natural scenes; Suhr et al., 2019). In NLVR, in particular, a crowdsourced statement is coupled with a synthetic scene including 3 independent images, and models must verify whether the statement is true or false with respect to the entire visual input. This involves handling phenomena such as counting, negation or comparisons, that require perform relational reasoning over the entire scene, e.g.: There is a black item in every box, There is a tower with yellow base, etc. However, most hscene, statementi pairs do not challenge models to do the same at the level of the single image (or box), where a low-level understanding of t"
2021.repl4nlp-1.16,D19-1514,0,0.208115,"defined size and one image stands out in this regard. The task requires verifying whether a simple natural language statement standing for a first-order logical form describes a scene, e.g., “There is exactly one blue triangle that is small in its image in this scene” (Figure 1). Such multi-step relational reasoning is at play in many real-life situations: e.g., the same exact pan may count as ‘big’ in all contexts except a restaurant kitchen. We experiment with two types of models to solve this task: a modular neural network (Hu et al., 2017) and LXMERT, a pre-trained multimodal transformer (Tan and Bansal, 2019). We probe the learned representations of LXMERT to assess whether, and to what extent, it has learned the underlying structure of the data. By means of two experiments with probing classifiers (Alain and Bengio, 2017; Hupkes et al., 2018; Belinkov and Glass, 2019), we first verify that it is able to perform the task at the image level (i.e., to compute the relative size of the target object at the image level); then, we test its ability to reason at the multi-image level and detect the image that stands out. The experiments show that LXMERT is able to solve the multi-step relational reasoning"
2021.repl4nlp-1.16,D19-1285,1,0.787317,"Missing"
2021.starsem-1.3,2020.acl-main.463,0,0.0287107,"sociated with, even if the meaning of either sense hasn’t changed on its own.10 However, many theories of semantic change emphasize the role of changing sense distributions as a mechanism for lexical semantic change, so it is not necessarily contrary to our aims of quantifying semantic change over the lexicon. A related weakness of distributional semantics has to do with the distinction between meaningin-use and lexical meaning. Even if we assume that distributional context is a faithful (if noisy) representation of the situated meaning of a word (cf. L¨ucking et al., 2019; Bisk et al., 2020; Bender and Koller, 2020), it might not capture the word’s full meaning potential (Nor´en and Linell, 2007)— in the extreme, a word may have common ground semantic content that could be activated, but that happens not to appear in the corpus. Moreover, changes in the topics discussed by the community may cause changes in the context distribution of words that don’t reflect actual change in meaning. Consider the words at the top of the list for /r/toronto (Table 1). It’s possible that some of those words appear due to changes in the sociopolitical topics people were discussing on the forum between 2015 and 2017. Simila"
2021.starsem-1.3,2020.emnlp-main.703,0,0.0289357,"Missing"
2021.starsem-1.3,N19-1423,0,0.00709613,"dence intervals increase dramatically as clustering increases, since our sample of communities found fewer examples with high clustering. We did not find significant correlations for any of the community-level features on their own. It is possible that a larger study with more communities or a more diverse set of communities would reveal some more universal effect, but we cannot make any conclusions from these results. The fact that the three-way interaction has a significant effect while none of the individual features did on their 10 Contextualized word representations (Peters et al., 2018; Devlin et al., 2019) don’t have this shortcoming and have recently been used to investigate semantic change (Giulianelli et al., 2020; Vani et al., 2020), but extracting one vector per occurrence is computationally expensive and has therefore only been applied to small sets of target words. 33 Future work This work offers some insight into how semantic change and community structure interact, but there are still many open questions, including how these results generalize to communities in different communicative settings and over different time frames. Future work should take a closer look at the kinds of change"
2021.starsem-1.3,D17-1118,0,0.0427033,"Missing"
2021.starsem-1.3,2020.acl-main.365,1,0.88571,"Missing"
2021.starsem-1.3,W19-1101,1,0.892329,"Missing"
2021.starsem-1.3,D16-1229,0,0.275259,"Section 4.1). We define na¨ıve cosine change for the communityspecific and “generic” lexicons (Section 4.2). In Section 4.3, we use a control procedure adapted from Dubossarsky and Weinshall (2017) to account for noise in the na¨ıve metric. 4.1 Na¨ıve cosine change cos c (w) = cos 1 (cos sim(w ~ c,2015 , w ~ c,2017 )) ⇡ (1) where cos sim(v1 , v2 ) = v1 · v2 kv1 kkv2 k (2) Generic na¨ıve cosine change, cos G , is defined analogously. Generally speaking, na¨ıve cosine change has a strong track record as a semantic change metric, performing well in both human-annotated and synthetic evaluations (Hamilton et al., 2016b; Shoemark et al., 2019; Schlechtweg et al., 2020). Especially relevant to this work, Del Tredici et al. (2019) found cosine change to correlate with aggregated semantic change judgments collected from members of the /r/LiverpoolFC community on Reddit. Model drift can distort cosine change, although this is mainly a problem with many serially-trained time periods (Shoemark et al., 2019). In a pilot study, we experimented with post-hoc aligned vector spaces and a neighborhood-based change metric (Hamilton et al., 2016a), but found minimal differences from the na¨ıve metric. A more serious conc"
2021.starsem-1.3,P16-1141,0,0.403705,"Section 4.1). We define na¨ıve cosine change for the communityspecific and “generic” lexicons (Section 4.2). In Section 4.3, we use a control procedure adapted from Dubossarsky and Weinshall (2017) to account for noise in the na¨ıve metric. 4.1 Na¨ıve cosine change cos c (w) = cos 1 (cos sim(w ~ c,2015 , w ~ c,2017 )) ⇡ (1) where cos sim(v1 , v2 ) = v1 · v2 kv1 kkv2 k (2) Generic na¨ıve cosine change, cos G , is defined analogously. Generally speaking, na¨ıve cosine change has a strong track record as a semantic change metric, performing well in both human-annotated and synthetic evaluations (Hamilton et al., 2016b; Shoemark et al., 2019; Schlechtweg et al., 2020). Especially relevant to this work, Del Tredici et al. (2019) found cosine change to correlate with aggregated semantic change judgments collected from members of the /r/LiverpoolFC community on Reddit. Model drift can distort cosine change, although this is mainly a problem with many serially-trained time periods (Shoemark et al., 2019). In a pilot study, we experimented with post-hoc aligned vector spaces and a neighborhood-based change metric (Hamilton et al., 2016a), but found minimal differences from the na¨ıve metric. A more serious conc"
2021.starsem-1.3,W14-2517,0,0.281449,"c word vectors to study semantic change in political and media discourse, including in UK parliamentary debates, finding that word meaning changes differently depending on the political viewpoint of the speaker. Stewart et al. (2017) use diachronic word vectors to measure semantic change in the VKontakte social network during the Russia-Ukriaine crisis and find that changes in word frequency are predictive of semantic shift. Del Tredici et al. (2019) studied short-term semantic shift in the /r/LiverpoolFC community on Reddit, empirically validating the diachronic word vector model proposed by Kim et al. (2014) by correlating cosine distance between vectors from two different time periods with semantic change judgments collected from members of the community. In another study Del Tredici and Fern´andez (2017) find variations in word meaning across different Reddit communities, including communities organized around the same topic. 3 Data To investigate semantic change in different communities, we use comments collected from the social media website Reddit.3 On Reddit, users create posts, which consist of a link, image, or user-generated text, along with a comment section. Comments are threaded: user"
2021.starsem-1.3,N18-1202,0,0.0391262,"owever that the confidence intervals increase dramatically as clustering increases, since our sample of communities found fewer examples with high clustering. We did not find significant correlations for any of the community-level features on their own. It is possible that a larger study with more communities or a more diverse set of communities would reveal some more universal effect, but we cannot make any conclusions from these results. The fact that the three-way interaction has a significant effect while none of the individual features did on their 10 Contextualized word representations (Peters et al., 2018; Devlin et al., 2019) don’t have this shortcoming and have recently been used to investigate semantic change (Giulianelli et al., 2020; Vani et al., 2020), but extracting one vector per occurrence is computationally expensive and has therefore only been applied to small sets of target words. 33 Future work This work offers some insight into how semantic change and community structure interact, but there are still many open questions, including how these results generalize to communities in different communicative settings and over different time frames. Future work should take a closer look a"
2021.starsem-1.3,C18-1117,0,0.0148315,"s 2015–2017. In addition to the subreddit corpora, we created a generic Reddit corpus, consisting of comments sampled from every subreddit, including communities not in our sample. For both the generic corpus and the community-specific corpora, we constructed separate datasets for 2015 and 2017, leaving a one-year gap between them. Social network analysis In an early example of using social network analysis to study the language online communities, Paolillo (1999) categorizes the relationships of users of an IRC channel as 1 https://www.reddit.com See Tahmasebi et al. (2018), Tang (2018), and Kutuzov et al. (2018) for recent surveys. 2 3 27 Obtained from pushshift.io (Baumgartner et al., 2020). 4.2 The generic corpus consists of 55M comments for 2015 and 54M for 2017. For each of the selected subreddits, we sampled comments from 2015 and 2017 to construct two datasets of 5.4M tokens each (averaging 158K comments).4 4 We define na¨ıve cosine change as the angular distance between corresponding word vectors from the two different time periods.6 For a community c, na¨ıve cosine change is defined for all words in the vocabulary as follows: Semantic change model In this section, we describe how we quantify"
2021.starsem-1.3,2020.semeval-1.1,0,0.0231543,"r the communityspecific and “generic” lexicons (Section 4.2). In Section 4.3, we use a control procedure adapted from Dubossarsky and Weinshall (2017) to account for noise in the na¨ıve metric. 4.1 Na¨ıve cosine change cos c (w) = cos 1 (cos sim(w ~ c,2015 , w ~ c,2017 )) ⇡ (1) where cos sim(v1 , v2 ) = v1 · v2 kv1 kkv2 k (2) Generic na¨ıve cosine change, cos G , is defined analogously. Generally speaking, na¨ıve cosine change has a strong track record as a semantic change metric, performing well in both human-annotated and synthetic evaluations (Hamilton et al., 2016b; Shoemark et al., 2019; Schlechtweg et al., 2020). Especially relevant to this work, Del Tredici et al. (2019) found cosine change to correlate with aggregated semantic change judgments collected from members of the /r/LiverpoolFC community on Reddit. Model drift can distort cosine change, although this is mainly a problem with many serially-trained time periods (Shoemark et al., 2019). In a pilot study, we experimented with post-hoc aligned vector spaces and a neighborhood-based change metric (Hamilton et al., 2016a), but found minimal differences from the na¨ıve metric. A more serious concern for our purposes is the fact that na¨ıve cosine"
2021.starsem-1.3,D19-1007,0,0.0229441,"Missing"
2021.starsem-1.3,2020.semeval-1.26,0,0.0204089,"g. We did not find significant correlations for any of the community-level features on their own. It is possible that a larger study with more communities or a more diverse set of communities would reveal some more universal effect, but we cannot make any conclusions from these results. The fact that the three-way interaction has a significant effect while none of the individual features did on their 10 Contextualized word representations (Peters et al., 2018; Devlin et al., 2019) don’t have this shortcoming and have recently been used to investigate semantic change (Giulianelli et al., 2020; Vani et al., 2020), but extracting one vector per occurrence is computationally expensive and has therefore only been applied to small sets of target words. 33 Future work This work offers some insight into how semantic change and community structure interact, but there are still many open questions, including how these results generalize to communities in different communicative settings and over different time frames. Future work should take a closer look at the kinds of change (e.g., Blank, 1999) taking place. For example, are the meanings of words broadening or narrowing? How are existing community-level co"
aloni-etal-2012-building,J08-4004,0,\N,Missing
aloni-etal-2012-building,J96-2004,0,\N,Missing
C02-1135,W02-0222,0,\N,Missing
C02-1135,W01-1616,1,\N,Missing
C02-1135,P01-1031,1,\N,Missing
C02-1135,J96-2004,0,\N,Missing
C02-1135,W00-0302,0,\N,Missing
C04-1035,J96-2004,0,0.114576,"nput is too poor to make a decision as to its resolution, as in the following example: (5) Unknown : <unclear> <pause> Josephine: Why? [KCN, 5007] After annotating the first sample, we decided to add a new category to the above set. The sluices in the second sample were classified according to a set of five categories, including the following: Wh-anaphor The antecedent of the sluice is a wh-phrase. (6) Larna: We’re gonna find poison apple and I know where that one is. Charlotte: Where? [KD1, 2371] 2.3 Reliability To evaluate the reliability of the annotation, we use the kappa coefficient (K) (Carletta, 1996), which measures pairwise agreement between a set of coders making category judgements, correcting for expected chance agreement. 2 The agreement on the coding of the first sample of sluices was moderate (K = 52).3 There were important differences amongst sluice classes: The lowest agreement was on the annotation for why (K = 29), what (K = 32) and how (K = 32), which suggests that these categories are highly ambiguous. Examination of the coincidence matrices shows that the largest confusions were between reprise and clarification in the case of what, and between direct and reprise for why and"
C14-1145,J08-4004,0,0.245199,"RTE dataset (almost as well as Agr). Intuitively speaking, it can track item difficulty, by first settling the easy items (with clear majorities) and thereby learning which annotators are most reliable to then have them decide on the harder items. Here we have not included this rule as there is no single most natural way of generalising it to the multi-category case. Arriving at such a generalisation in a principled manner is an important direction for future work. It would also be interesting to get a clearer understanding of the links between methods for assessing inter-annotator agreement (Artstein and Poesio, 2008) and methods of aggregation (i.e., methods that may be applied to data of possibly rather poor inter-annotator agreement, as is the case for parts of our datasets). A relevant observation in this context is that the notions of individual and global frequency at the core of our BCR’s also play a role in agreement coefficients, namely to compute chance agreement: π (Scott, 1955) uses global frequencies and κ (Cohen, 1960) uses individual frequencies. While the definition of Agr was motivated by a simple probabilistic model, the BCR’s were motivated by rules of thumb regarding links between obser"
C14-1145,W10-1806,0,0.0783787,"s that the possible biases amongst the participants we manage to recruit may distort the outcome. Also, given the nature of crowsourcing (rewarding speed rather than quality), some participants may not respond truthfully according to their intuitions as speakers. To address these issues, we propose aggregation methods that go beyond majority voting by taking into account the reliability of individual annotators at the time of aggregation.1 Our approach is related to existing work on analysing the quality of annotated data by examining, for instance, (dis)agreement patterns amongst annotators (Bhardwaj et al., 2010; Peldszus and Stede, 2013; Ramanath et al., 2013). However, while the main aim of this kind of studies is to gain insight into the difficulty of an annotation task or into the feasibility of using untrainned annotators for particular tasks, our focus is on exploiting patterns of judgements for the purpose of aggregation into a single collective annotation—an aspect that has received far less attention in the literature. We make the following contributions: (i) we make available two new datasets of judgements gathered with AMT for two multi-category annotation tasks; (ii) we define several agg"
C14-1145,P13-1053,1,0.753118,"Missing"
C14-1145,C14-1145,1,0.0512624,"Missing"
C14-1145,S07-1005,0,0.209478,"Missing"
C14-1145,W13-2323,0,0.060784,"k. Note that Com and Inv do not take global frequencies into account, while Diff and Rat do. 2.3 Agreement-based Aggregation Suppose each item has a true (but unknown) category (its gold standard). We may view an annotator’s judgement as a noisy signal of the gold standard. We now want to design an aggregator as a maximum likelihood estimator for this ground truth. This approach has been pioneered by Dawid and Skene (1979). Variants have been used for diverse purposes by, amongst others, Snow et al. (2008), Carpenter (2008), Raykar et al. (2010), Ipeirotis et al. (2010), Li et al. (2013), and Passonneau and Carpenter (2013). Let p(aij = k |gj = k ? ), with k not necessarily distinct from k ? , be the probability of agent i ∈ Nj annotating item j with category k ∈ K, given that the gold standard category of j is k ? ∈ K. If we can obtain estimates of these probabilities, then we can use them to calibrate the weights of the annotators. The challenge, particularly for multi-category annotation tasks, is that the number of probabilities to estimate is fairly large (in particular, it is quadratic in |K|). To be able to provide reasonable estimates, we need a large amount of data from every individual annotator. But t"
C14-1145,W13-2324,0,0.0491064,"ses amongst the participants we manage to recruit may distort the outcome. Also, given the nature of crowsourcing (rewarding speed rather than quality), some participants may not respond truthfully according to their intuitions as speakers. To address these issues, we propose aggregation methods that go beyond majority voting by taking into account the reliability of individual annotators at the time of aggregation.1 Our approach is related to existing work on analysing the quality of annotated data by examining, for instance, (dis)agreement patterns amongst annotators (Bhardwaj et al., 2010; Peldszus and Stede, 2013; Ramanath et al., 2013). However, while the main aim of this kind of studies is to gain insight into the difficulty of an annotation task or into the feasibility of using untrainned annotators for particular tasks, our focus is on exploiting patterns of judgements for the purpose of aggregation into a single collective annotation—an aspect that has received far less attention in the literature. We make the following contributions: (i) we make available two new datasets of judgements gathered with AMT for two multi-category annotation tasks; (ii) we define several aggregation methods based, on"
C14-1145,P13-1168,0,0.309811,"ts we manage to recruit may distort the outcome. Also, given the nature of crowsourcing (rewarding speed rather than quality), some participants may not respond truthfully according to their intuitions as speakers. To address these issues, we propose aggregation methods that go beyond majority voting by taking into account the reliability of individual annotators at the time of aggregation.1 Our approach is related to existing work on analysing the quality of annotated data by examining, for instance, (dis)agreement patterns amongst annotators (Bhardwaj et al., 2010; Peldszus and Stede, 2013; Ramanath et al., 2013). However, while the main aim of this kind of studies is to gain insight into the difficulty of an annotation task or into the feasibility of using untrainned annotators for particular tasks, our focus is on exploiting patterns of judgements for the purpose of aggregation into a single collective annotation—an aspect that has received far less attention in the literature. We make the following contributions: (i) we make available two new datasets of judgements gathered with AMT for two multi-category annotation tasks; (ii) we define several aggregation methods based, on This work is licensed u"
C14-1145,W11-1510,0,0.153701,"eates several challenges: amongst others, how to obtain good quality annotations from untrainned and unmonitored individuals, and how to combine large numbers of possibly conflicting judgements into a single joint annotation. In this paper we focus on the latter challenge. Our aim is to investigate and empirically test methods for aggregating the judgements of large numbers of individuals in a linguistic annotation task conducted via crowdsourcing into a collective judgement. Most researchers who turn to crowdsourcing to collect data use majority voting to combine the participants’ responses (Sayeed et al., 2011; Zarcone and R¨ud, 2012; Venhuizen et al., 2013). Although in the limit it makes sense to take the judgement of the majority as reflecting the view of the community, in practice we cannot reach out to the full population of speakers, which means that the possible biases amongst the participants we manage to recruit may distort the outcome. Also, given the nature of crowsourcing (rewarding speed rather than quality), some participants may not respond truthfully according to their intuitions as speakers. To address these issues, we propose aggregation methods that go beyond majority voting by t"
C14-1145,D08-1027,0,0.692605,"Missing"
C14-1145,W13-0215,0,0.0433601,"to obtain good quality annotations from untrainned and unmonitored individuals, and how to combine large numbers of possibly conflicting judgements into a single joint annotation. In this paper we focus on the latter challenge. Our aim is to investigate and empirically test methods for aggregating the judgements of large numbers of individuals in a linguistic annotation task conducted via crowdsourcing into a collective judgement. Most researchers who turn to crowdsourcing to collect data use majority voting to combine the participants’ responses (Sayeed et al., 2011; Zarcone and R¨ud, 2012; Venhuizen et al., 2013). Although in the limit it makes sense to take the judgement of the majority as reflecting the view of the community, in practice we cannot reach out to the full population of speakers, which means that the possible biases amongst the participants we manage to recruit may distort the outcome. Also, given the nature of crowsourcing (rewarding speed rather than quality), some participants may not respond truthfully according to their intuitions as speakers. To address these issues, we propose aggregation methods that go beyond majority voting by taking into account the reliability of individual"
C14-1145,zarcone-rued-2012-logical,0,0.287479,"Missing"
C18-1104,W10-4342,0,\N,Missing
C18-1104,E09-1081,0,\N,Missing
C18-1104,W13-4065,0,\N,Missing
C18-1104,W15-4610,0,\N,Missing
C18-1104,D16-1127,0,\N,Missing
C18-1104,W16-3630,0,\N,Missing
C18-1104,W16-3631,0,\N,Missing
C18-1104,W16-3643,0,\N,Missing
C18-1104,I17-1047,0,\N,Missing
C18-1104,W17-5539,0,\N,Missing
C18-1104,D12-1008,0,\N,Missing
C18-1104,W15-4643,0,\N,Missing
C18-1104,N16-1014,0,\N,Missing
C18-1135,Q16-1003,0,\N,Missing
C18-1135,P16-1141,0,\N,Missing
C18-1135,W17-6804,1,\N,Missing
D19-1285,N18-2070,0,0.0530309,"gle is the biggest triangle; POS1 : The white square is a small square; POS: The red circle is a big object; SET+POS: The white rectangle is a big rectangle. Best viewed in color. Thus, rather than modeling the meaning of GAs, this work focuses on the relative size of object types, which is crucially different from our goal. Second, visual gradable attributes have been used as fine-grained features to answer questions about entities in natural scenes (Antol et al., 2015; Hudson and Manning, 2019), to perform discriminative and pragmatically-informative image captioning (Vedantam et al., 2017; Cohn-Gordon et al., 2018), and to discriminate between similar images in reference games (Su et al., 2017). In these works, however, GAs are labels standing for fixed attributes: e.g., an airplane is annotated as ‘large’ or ‘small’ with no or little relation to the visual context. Third, gradable attributes are explored in work on visual reasoning (Johnson et al., 2017a; Yi et al., 2018), where multi-modal models are challenged to answer complex, high-level questions about objects and their relations in synthetic scenes. Surprisingly, however, questions involving GAs are treated as non-relational: e.g., a given size a"
D19-1285,Q13-1023,0,0.0673382,"Missing"
D19-1285,W06-1420,0,0.0445314,"Missing"
D19-1285,D13-1169,0,0.059842,"Missing"
D19-1285,W16-1607,0,0.0307772,"Missing"
D19-1285,P10-1018,0,0.0811591,"Missing"
D19-1285,W10-4210,0,0.0610364,"Missing"
D19-1285,N13-1137,0,0.0605976,"Missing"
D19-1285,Q17-1023,0,0.0318426,"+SA FiLM chance avg seen ± sd 0.608 ± 0.01 0.7813 ± 0.009 0.8489 ± 0.014 0.5 avg unseen ± sd 0.4036 ± 0.003 0.235 ± 0.006 0.153 ± 0.02 0.5 Acknowledgments Table 3: Compositional task. Results by the models. 8 dence showing that deep learning models do not rely on systematic compositional rules (Baroni, 2019; Bahdanau et al., 2019). An interesting open question, which we plan to explore in future work, is whether training models to jointly learn superlative, comparative, and positive GAs (similarly to how Pezzelle et al. (2018) did for quantities), or framing the task in a dialogue setting (as Monroe et al. (2017) did for colors) could lead to more compositional models. Moreover, it might be worth exploring whether equipping models with similar inductive biases as those leading speakers of any language to develop abstract, compositional representations of size adjectives is needed to properly handle these expressions. In parallel, the present work could be extended to other GAs and threshold functions. In the long term, we aim to move to natural images. This requires world knowledge, a confounding factor intentionally abstracted away in synthetic data. Since children learn to exploit world knowledge af"
D19-1285,N18-1039,1,0.832534,"ied by ‘big’ (seen combinations) or ‘small’ (unseen combinations). model CNN+LSTM CNN+LSTM+SA FiLM chance avg seen ± sd 0.608 ± 0.01 0.7813 ± 0.009 0.8489 ± 0.014 0.5 avg unseen ± sd 0.4036 ± 0.003 0.235 ± 0.006 0.153 ± 0.02 0.5 Acknowledgments Table 3: Compositional task. Results by the models. 8 dence showing that deep learning models do not rely on systematic compositional rules (Baroni, 2019; Bahdanau et al., 2019). An interesting open question, which we plan to explore in future work, is whether training models to jointly learn superlative, comparative, and positive GAs (similarly to how Pezzelle et al. (2018) did for quantities), or framing the task in a dialogue setting (as Monroe et al. (2017) did for colors) could lead to more compositional models. Moreover, it might be worth exploring whether equipping models with similar inductive biases as those leading speakers of any language to develop abstract, compositional representations of size adjectives is needed to properly handle these expressions. In parallel, the present work could be extended to other GAs and threshold functions. In the long term, we aim to move to natural images. This requires world knowledge, a confounding factor intentional"
D19-1285,L18-1529,0,0.033801,"Missing"
D19-1285,N15-1051,0,0.0404901,"Missing"
D19-1285,J06-2002,0,0.149485,"Missing"
D19-1285,W08-1109,0,0.0981889,"Missing"
D19-1285,W11-2702,0,0.0626017,"Missing"
D19-1477,W17-5202,1,0.843619,"tenated and fed into a classifier. 0 ∈ Rd+d ×c , where c is a model parameter, and a layer W2 ∈ Rc×o , where o is the number of output classes. The final prediction is computed as follows, where σ is a ReLU function (Nair and Hinton, 2010): yˆ = softmax (W2 (σ (W1 (lks)))) 3.2 (1) Linguistic Module The linguistic module is implemented using a recurrent neural network, concretely an LSTM (Hochreiter and Schmidhuber, 1997). Since LSTMs have become ubiquitous in NLP, we omit a detailed description of the inner workings of the model here and refer readers to Tai et al. (2015); Tang et al. (2016); Barnes et al. (2017) for overviews. We use a bidirectional LSTM (B I LSTM) (Graves, 2012), whose final states are concatenated in order to obtain the representation of the input text. 3.3 Social Module The goal of the social module is to return author representations which encode homophily relations among users, i.e., which assign similar vectors to users who are socially related. We model social relations using graphs G = (V, E), where V is the set of nodes representing individuals and E the set of edges representing relations among them. We use vi ∈ V to refer to a node in the social graph, and eij ∈ E to denot"
D19-1477,D14-1179,0,0.0326514,"Missing"
D19-1477,C18-1156,0,0.154726,"o make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current models by using state-of-the-art graph neural networks to encode and leverage homophily relations."
D19-1477,P15-1073,0,0.0294632,"valuate its performance against commonly used models for user representation. • We show that exploiting social information leads to improvements in two tasks (stance and hate speech detection) and that our model significantly outperforms competing alternatives. • We perform an extended error analysis, in which we show the robustness across tasks of user representations based on social graphs, and the superiority of dynamic representations over static ones. 2 Related Work Several strands of research have explored different social features to create user representations for NLP in social media. Hovy (2015) and Hovy and Fornaciari (2018) focus on demographic information (age and gender), while Bamman et al. (2014) and Hovy and Purschke (2018) exploit geographic location to account for regional variation. Demographic and geographic information, however, need to be made explicit by users and thus are often not available or not reliable. To address this drawback, other studies have aimed at extracting user information by just observing users’ behaviour on social platforms. To tackle sarcasm detection on Reddit, Kolchinski and Potts (2018) assign to each user a random embedding that is updated durin"
D19-1477,D18-1469,0,0.0263307,"ads to improvements in two tasks (stance and hate speech detection) and that our model significantly outperforms competing alternatives. • We perform an extended error analysis, in which we show the robustness across tasks of user representations based on social graphs, and the superiority of dynamic representations over static ones. 2 Related Work Several strands of research have explored different social features to create user representations for NLP in social media. Hovy (2015) and Hovy and Fornaciari (2018) focus on demographic information (age and gender), while Bamman et al. (2014) and Hovy and Purschke (2018) exploit geographic location to account for regional variation. Demographic and geographic information, however, need to be made explicit by users and thus are often not available or not reliable. To address this drawback, other studies have aimed at extracting user information by just observing users’ behaviour on social platforms. To tackle sarcasm detection on Reddit, Kolchinski and Potts (2018) assign to each user a random embedding that is updated during the training phase, with the goal of learning individualised patterns of sarcasm usage. Wallace et al. (2016) and Hazarika et al. (2018)"
D19-1477,D18-1140,0,0.101488,"rent social features to create user representations for NLP in social media. Hovy (2015) and Hovy and Fornaciari (2018) focus on demographic information (age and gender), while Bamman et al. (2014) and Hovy and Purschke (2018) exploit geographic location to account for regional variation. Demographic and geographic information, however, need to be made explicit by users and thus are often not available or not reliable. To address this drawback, other studies have aimed at extracting user information by just observing users’ behaviour on social platforms. To tackle sarcasm detection on Reddit, Kolchinski and Potts (2018) assign to each user a random embedding that is updated during the training phase, with the goal of learning individualised patterns of sarcasm usage. Wallace et al. (2016) and Hazarika et al. (2018) address the same task, using ParagraphVector (Le and Mikolov, 2014) to condense all the past comments/posts of a user into a low dimensional vector, which is taken to capture their interests and opinions. All these studies use the concatenation of author and post embeddings for the final prediction, showing that adding author information leads to significant improvements. While the approaches disc"
D19-1477,C18-1093,1,0.938598,"nd combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current models by using state-of-the-art graph neural network"
D19-1477,N19-1221,1,0.816547,"line of work has focused on leveraging the social connections of users in Twitter data. This methodology relies on creating a social graph where users are nodes connected to each other by their retweeting, mentioning, or following behaviour. Techniques such as Line (Tang et al., 2015), Node2Vec (Grover and Leskovec, 2016) or Graph Convolutional Networks (GCNs, Kipf and Welling, 2017) are then used to learn low-dimensional embeddings for each user, which have been shown to be beneficial in different downstream tasks when combined with textual information. For example, Mishra et al. (2018) and Mishra et al. (2019) use the concatenation strategy mentioned above for abusive language detection; Yang et al. (2016) optimise social and linguistic representations with two distinct scoring functions to perform entity linking; while Yang and Eisenstein (2017) use an ensemble learning setup for sentiment analysis, where the final prediction is given by the weighted combination of several classifiers, each exploring the social graph independently. Methods like Line, Node2Vec and GCNs create user representations by aggregating the information coming from their connections in the social graph, without making any di"
D19-1477,S16-1003,0,0.123827,"Missing"
D19-1477,D16-1244,0,0.0952175,"Missing"
D19-1477,D14-1162,0,0.103455,"vector size (200) and epochs (20). For the GAT encoder, we experiment with values 10, 15, 20, 25, 30, 50 for the size of the hidden layer; for the number of heads, we explore values 1, 2, 3, 4. We keep the number of hops equal to 1 and the alpha value for the Leaky ReLU of the attention heads equal to 0.2 across all the settings.5 Since our focus is on social information, we keep the hyperparameters of the linguistic module and the classifier fixed across all the settings. Namely, the BiLSTM has depth of 1, the hidden layer has 50 units, and uses 200-d GloVe embeddings pretrained on Twitter (Pennington et al., 2014). For the classifier, we set the dimensionality of the non-linear layer to 50. 4 We also experimented with updating author embeddings during training, but did not observe any difference in the results. 5 We implement PV using the Gensim library: https://radimrehurek.com/gensim/models/ doc2vec.html. For N2V, we use the implementation at: https://github.com/aditya-grover/ node2vec. For GAT, the implementation at: https://github.com/Diego999/pyGAT. 4710 4.3 Tasks and Datasets We test all the models on three Twitter datasets annotated for different tasks. For all datasets we tokenise and lowercase"
D19-1477,S17-2088,0,0.0569418,"Missing"
D19-1477,S15-2078,0,0.0194674,"the models using the same metrics used for the optimization process (see Section 4.4). In Table 2 we report the results, that we compute as the average of ten runs with random parameter initialization.9 We use the unpaired Welch’s t test to check for statistically significant difference between models. Tasks The results show that social information helps improve the performance on Stance and Hate Speech detection, while it has no effect for Sentiment Analysis. While this result contrasts with the one reported by Yang and Eisenstein (2017), who use a previous version of the Sentiment dataset (Rosenthal et al., 2015), it is not 9 Frequency LING LING+random LING+PV LING+N2V LING+GAT Sentiment 0.332 0.676 0.657 0.671 0.672 0.666 Stance 0.397 0.569 0.571 0.601∗ 0.629∗ 0.640∗† Hate 0.057 0.624 0.600 0.667∗ 0.656∗ 0.674∗† Table 2: Results for all the models on the three datasets in our experiment. Marked with ∗ are the results which significantly improve over LING and LING+random (p < 0.05, also for the following results);  indicates a significant improvement over LING+PV; † a significant improvement over LING+N2V. surprising given the analysis made in the previous section regarding the amount of homophily"
D19-1477,P15-1150,0,0.102031,"Missing"
D19-1477,C16-1311,0,0.0202065,"wo modules are concatenated and fed into a classifier. 0 ∈ Rd+d ×c , where c is a model parameter, and a layer W2 ∈ Rc×o , where o is the number of output classes. The final prediction is computed as follows, where σ is a ReLU function (Nair and Hinton, 2010): yˆ = softmax (W2 (σ (W1 (lks)))) 3.2 (1) Linguistic Module The linguistic module is implemented using a recurrent neural network, concretely an LSTM (Hochreiter and Schmidhuber, 1997). Since LSTMs have become ubiquitous in NLP, we omit a detailed description of the inner workings of the model here and refer readers to Tai et al. (2015); Tang et al. (2016); Barnes et al. (2017) for overviews. We use a bidirectional LSTM (B I LSTM) (Graves, 2012), whose final states are concatenated in order to obtain the representation of the input text. 3.3 Social Module The goal of the social module is to return author representations which encode homophily relations among users, i.e., which assign similar vectors to users who are socially related. We model social relations using graphs G = (V, E), where V is the set of nodes representing individuals and E the set of edges representing relations among them. We use vi ∈ V to refer to a node in the social graph"
D19-1477,K16-1017,0,0.29446,"apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current models by using state-of-the-art graph neural networks to encode and leverage homophily relations. Most current models re"
D19-1477,D16-1152,0,0.0199429,"ology relies on creating a social graph where users are nodes connected to each other by their retweeting, mentioning, or following behaviour. Techniques such as Line (Tang et al., 2015), Node2Vec (Grover and Leskovec, 2016) or Graph Convolutional Networks (GCNs, Kipf and Welling, 2017) are then used to learn low-dimensional embeddings for each user, which have been shown to be beneficial in different downstream tasks when combined with textual information. For example, Mishra et al. (2018) and Mishra et al. (2019) use the concatenation strategy mentioned above for abusive language detection; Yang et al. (2016) optimise social and linguistic representations with two distinct scoring functions to perform entity linking; while Yang and Eisenstein (2017) use an ensemble learning setup for sentiment analysis, where the final prediction is given by the weighted combination of several classifiers, each exploring the social graph independently. Methods like Line, Node2Vec and GCNs create user representations by aggregating the information coming from their connections in the social graph, without making any distinction among them. In contrast, we use Graph Attention Networks (GATs, Velickovic et al., 2018)"
D19-1477,Q17-1021,0,0.199647,"on given the most relevant connections for a target task, and combines it with linguistic information to make a prediction. We apply our model to three different tasks, evaluate it against alternative models, and analyse the results extensively, showing that it significantly outperforms other current methods. 1 Introduction The idea that extra-linguistic information about speakers can help language understanding has recently gained traction in NLP. Several studies have successfully exploited social information to classify user-generated language in downstream tasks such as sentiment analysis (Yang and Eisenstein, 2017), abusive speech identification (Mishra et al., 2018) and sarcasm detection (Hazarika et al., 2018; Wallace et al., 2016). The underlying goal is to capture the sociological phenomenon of homophily (McPherson et al., 2001) – i.e., people’s tendency to group together with others they share ideas, beliefs, and practices with – and to exploit it jointly with linguistic information to obtain richer text representations. In this paper, we advance ∗ Research conducted when the author was at the University of Amsterdam. this line of research. In particular, we address a common shortcoming in current"
D19-6403,D19-1285,1,0.507335,"Missing"
D19-6403,W19-4801,0,0.0122073,"for a circle), FiLM was shown to use a default strategy which ignores the adjective rather than applying it compositionally. This finding is in line with previous evidence showing the lack of compositionality in neural networks (Baroni, 2019), either in multimodal tasks like visual question answering (Agrawal et al., 2017) and visual reasoning (Johnson et al., 2017), or when coping with language data (Lake and Baroni, 2018; Loula et al., 2018). To solve this well-known issue, several attempts have been made to develop new models and techniques (Agrawal et al., 2018; Ramakrishnan et al., 2018; Korrel et al., 2019), and several datasets have been proposed to test compositional abilities of systems (Agrawal et al., 2017, 2018). A recently proposed visual reasoning task challenges models to learn the meaning of size adjectives (‘big’, ‘small’) from visually-grounded contexts (MALeViC; Pezzelle and Fern´andez, 2019). Differently from standard approaches in language and vision treating size as a fixed attribute of objects (Johnson et al., 2017), in MALeViC what counts as ‘big’ or ‘small’ is defined contextually, based on a cognitively-motivated threshold function evaluating the size of all the relevant obje"
D19-6403,W18-5413,0,0.0674524,"Missing"
E03-3005,P94-1001,0,0.0473653,"to model three different scenarios: asking and responding to a question, integrating a proposition into the commonly agreed facts, and asking for clarification when the content of an utterance has not been grounded. In (Fernandez, 2003) these scenarios were modelled in the form of complex DL programs corresponding to conventional protocols. From an abstract point of view, protocols can be thought of as a way to characterise the range of possible followups in cooperative dialogue or, alternatively, as a representation of the obligations the dialogue participants are socially committed to (see (Traum and Allen, 1994; Kreutel and Matheson, 1999)). In the present paper, however, we opt for a different strategy: our aim here is to describe the appropriateness conditions for each particular scenario by means of a set of axioms, that is, a set of formulas we postulate to be valid in the model. The aim of these formulas is to restrict the operations that can be performed on the DGB components. In this sense, they can be seen as constraints characterising the appropriateness conditions of simple programs like UT T := (i, c 1 r , r) (asking a clarification question) or FACT S.push (x) (integrating an item into t"
E09-1032,E06-1007,0,0.0365356,"Missing"
E09-1032,P07-1103,0,0.0304588,"Missing"
E09-1032,P04-1085,0,0.0882653,"Missing"
E09-1032,2007.sigdial-1.40,0,0.526827,"ou know, noises like something hitting. (2) Often, you need to know specific button sequences to get certain functionalities done. (3) I think it’s good. You’ve done a good review. However, unlike it, you is ambiguous between singular and plural interpretations - an issue that is particularly problematic in multi-party conversations. While you clearly has a plural referent in (4), in (3) the number of its referent is ambiguous.2 Introduction The English pronoun you is the second most frequent word in unrestricted conversation (after I and right before it).1 Despite this, with the exception of Gupta et al. (2007b; 2007a), its resolution has received very little attention in the literature. This is perhaps not surprising since the vast amount of work on anaphora and reference resolution has focused on text or discourse - mediums where second-person deixis is perhaps not as prominent as it is in dialogue. For spoken dialogue pronoun resolution modules however, resolving you is an essential task that has an important impact on the capabilities of dialogue summarization systems. (4) I don’t know if you guys have any questions. When an utterance contains a singular referential you, resolving the you amoun"
E09-1032,P03-1022,0,0.062355,"Missing"
E09-1032,P07-2027,0,0.488416,"ou know, noises like something hitting. (2) Often, you need to know specific button sequences to get certain functionalities done. (3) I think it’s good. You’ve done a good review. However, unlike it, you is ambiguous between singular and plural interpretations - an issue that is particularly problematic in multi-party conversations. While you clearly has a plural referent in (4), in (3) the number of its referent is ambiguous.2 Introduction The English pronoun you is the second most frequent word in unrestricted conversation (after I and right before it).1 Despite this, with the exception of Gupta et al. (2007b; 2007a), its resolution has received very little attention in the literature. This is perhaps not surprising since the vast amount of work on anaphora and reference resolution has focused on text or discourse - mediums where second-person deixis is perhaps not as prominent as it is in dialogue. For spoken dialogue pronoun resolution modules however, resolving you is an essential task that has an important impact on the capabilities of dialogue summarization systems. (4) I don’t know if you guys have any questions. When an utterance contains a singular referential you, resolving the you amoun"
E09-1032,E06-1022,0,0.244995,"Missing"
E09-1032,P02-1011,0,\N,Missing
J07-3005,C04-1035,1,0.677209,"Missing"
J07-3005,W00-0302,0,0.0295248,"nzburg (forthcoming) and Fern´andez (2006). The approach sketched here has been implemented as part of the SHARDS system (Ginzburg, Gregory, and Lappin 2001; Fern´andez et al., in press), which provides a procedure for computing the interpretation of some NSU classes in dialogue. The system currently handles short answers, direct and reprise sluices, as well as plain affirmative answers to polar questions. SHARDS has been extended to cover several types of clarification requests and used as a part of the information-state-based dialogue system CLARIE (Purver 2004b). The dialogue system GoDiS (Larsson et al. 2000; Larsson 2002) also uses a QUD-based approach to handle short answers. 3. Pilot Study: Sluice Reading Classification The first study we present focuses on the different interpretations or readings that sluices can convey. We first describe a corpus study that aims at providing empirical evidence about the distribution of sluice readings and establishing possible correlations between these readings and particular sluice types. After this, we report the results of a pilot machine learning experiment that investigates the automatic disambiguation of sluice interpretations. 3.1 The Sluicing Corpu"
J07-3005,W03-2106,0,0.336964,"the first of the examples, the NSU in bold face is a typical “short answer,” which despite having the form of a simple NP would most likely be understood as conveying the proposition Richard and James want Beethoven music. The NSU in (1b) is an example of what has been called a “sluice.” Again, despite being realized by a bare wh-phrase, the meaning conveyed by the NSU could be paraphrased as the question When is Ruth’s birthday? Although short answers and short queries like those in (1) are perhaps two of the most prototypical NSU classes, recent corpus studies (Fern´andez and Ginzburg 2002; Schlangen 2003) show that other less well-known types of NSUs—each with its own resolution constraints—are also pervasive in real conversations. This variety of NSU classes, together with their inherent concise form and their highly context-dependent meaning, often make NSUs ambiguous. Consider, for instance, example (2): (2) a. A: I left it on the table. B: On the table. b. A: Where did you leave it? B: On the table. c. A: I think I put it er. . . B: On the table. d. A: Should I put it back on the shelf? B: On the table. An NSU like B’s response in (2a) can be understood either as a clarification question o"
J07-3005,P05-1031,0,0.123861,"9 Node on a tree using NSU and antecedent features. 420 Fern´andez, Ginzburg, and Lappin Classifying NSUs in Dialogue Table 12 Comparison of weighted F-scores. System Majority class baseline One rule baseline Four rule baseline (NSU features) NSU and antecedent features Full feature set: - J4.8 - SLIPPER - TiMBL - MaxEnt Weighted F-score 35.31 53.03 67.99 85.44 89.53 92.01 92.02 92.21 at least in English, expressions like yeah (considered here as yes-words) are potentially ambiguous between acknowledgments and affirmative answers.8 This ambiguity and the problems it entails are also noted by Schlangen (2005), who addresses the problem of identifying NSUs automatically. As he points out, the ambiguity of yes-words is one of the difficulties encountered when trying to distinguish between backchannels (plain acknowledgments in our taxonomy) and non-backchannel fragments. This is a tricky problem for Schlangen as his NSU identification procedure does not have access to the context. Although in the present experiments we do use features that capture contextual information, determining whether the antecedent utterance is declarative or interrogative (which one would expect to be the best clue to disamb"
J07-3005,W02-2018,0,\N,Missing
K16-1011,W07-0604,0,0.0280342,"t, irregular past – Unbound morphology: determiner, preposition, auxiliary, present progressive • Error Type: – Omission, Addition, Substitution transcripts utterances candidate CF pairs Avg. per child 67.32 63,953.52 5,446.08 Table 1: Overview of our dataset containing longitudinal data from 25 different children. more across children, but overall also lies around 2 years.4 Preprocessing. Most of the transcripts in the dataset already include part-of-speech tagging, morphological analysis, and dependency parsing. We used the CLAN toolbox (MacWhinney, 2000b) and the MEGRASP dependency parser (Sagae et al., 2007) to add POS tags and to morphologically and syntactically parse the transcripts where this information was not available. We also automatically coded each adult response to a child utterance with information on overlap using the CHIP programme (Sokolov and MacWhinney, 1990), also part of the CLAN toolbox. CHIP provides information on added ($ADD), deleted ($DEL), and exactly matching ($EXA) morphemes in the source and response utterances, as well as the proportion of morphemes in the response utterance which match exactly morphemes in the source ($REP). Figure 1 shows a sample child-adult exch"
L16-1019,C14-1189,0,0.0257379,"ional database that can be easily processed and queried across the different experimental settings in PentoRef. 2. Related Work Compared to other resources used in dialogue research, PentoRef follows a tradition perhaps best exemplified by the HCRC Map Task Corpus (Anderson et al., 1991; MacMahon et al., 2006) in that it combines the naturalness of unscripted conversation with the advantages of taskoriented dialogue, such as careful control over aspects of the linguistic and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, conseq"
L16-1019,D15-1224,0,0.0148599,"xt. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects of the different domains on the reference task (Gkatzia et al., 2015), the comparability of existing reference corpora is limited as they are based on very different types of visual stimuli. PentoRef provides an unusually wide spectrum of experimental settings that have been invest"
L16-1019,W10-4302,1,0.760804,"erring expression generation (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not"
L16-1019,D14-1086,0,0.0700623,"and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects of the different domains on the reference task (Gkatzia et al., 2015), the comparability of existing reference corpora is limited as they are based on very different types of visual stimuli. PentoRef provides an unusually wide spectrum of experimental setting"
L16-1019,P15-1029,1,0.854429,"is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not be directly exploited for systematic studies of referring"
L16-1019,W13-4030,1,0.855465,"ration (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not be directly exploited for"
L16-1019,koolen-krahmer-2010-tuna,0,0.0962069,"aps best exemplified by the HCRC Map Task Corpus (Anderson et al., 1991; MacMahon et al., 2006) in that it combines the naturalness of unscripted conversation with the advantages of taskoriented dialogue, such as careful control over aspects of the linguistic and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects"
L16-1019,W10-4210,0,0.0784762,"Missing"
L16-1019,W09-3905,1,0.796519,"resolution (RR) and referring expression generation (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the"
L16-1019,tokunaga-etal-2012-rex,0,0.141485,"Missing"
N16-1038,P14-1023,0,0.161823,"e) words sharing a phonetic unit traditionally considered to be a phonestheme (Fordyce, 1989; Abelin, 1999; Magnus, 2000). More systematic studies have subsequently been carried out by Hutchins (1998) and Bergen (2004), but overall the phenomenon of conventional linguistic iconicity as reflected in phonesthemes remains largely understudied, certainly within the computational linguistics community. In this paper, we investigate phonesthemes by analyzing their orthographic correlates in a large corpus of written English, leveraging word embeddings constructed with word2vec and made available by Baroni et al. (2014). In particular, we make the following contributions: • We develop a stricter test than previously done in the literature for deciding whether a unit exhibits conventional iconicity. • We propose a new unsupervised method to induce the meaning conveyed by a phonesthemic unit. • We evaluate our meaning induction method with new automatic evaluation techniques and compare its performance to a WordNet-based method proposed by Abramova et al. (2013), obtaining a very substantial improvement. • We additionally evaluate our automatically derived meanings with human judgments collected via a crowdsou"
N16-1038,P12-1074,0,0.0246854,"ally derived meanings with human judgments collected via a crowdsourcing experiment. We believe phonesthemes deserve thorough investigation for both theoretical and practical reasons. Theoretically, adding more data-driven methods can 344 substantially enhance work in linguistics and psycholinguistics. Within computational linguistics itself, cross-fertilization with computational morphology (Wang et al., 2012; Marelli and Baroni, 2015), is an exciting avenue to be pursued. With respect to potential applications of automatic phonestheme meaning induction, creative brand naming (Klink, ¨ 2000; Ozbal and Strapparava, 2012), sentiment analysis (Sokolova and Bobicev, 2009) and construction of more appropriate language teaching materials (Imai et al., 2008) are viable possibilities. 2 Related Work Psycholinguistic studies on the nature of phonesthemes have shown that people tend to associate certain sounds with a particular meaning. Such studies were conducted on different languages, employing different methods and exhibiting various degrees of scale and systematicity (Fordyce, 1989; Abelin, 1999; Magnus, 2000; Hutchins, 1998). Recently, it has also been shown that phonesthemes affect online implicit language proc"
N16-1038,R09-1075,0,0.0162274,"ed via a crowdsourcing experiment. We believe phonesthemes deserve thorough investigation for both theoretical and practical reasons. Theoretically, adding more data-driven methods can 344 substantially enhance work in linguistics and psycholinguistics. Within computational linguistics itself, cross-fertilization with computational morphology (Wang et al., 2012; Marelli and Baroni, 2015), is an exciting avenue to be pursued. With respect to potential applications of automatic phonestheme meaning induction, creative brand naming (Klink, ¨ 2000; Ozbal and Strapparava, 2012), sentiment analysis (Sokolova and Bobicev, 2009) and construction of more appropriate language teaching materials (Imai et al., 2008) are viable possibilities. 2 Related Work Psycholinguistic studies on the nature of phonesthemes have shown that people tend to associate certain sounds with a particular meaning. Such studies were conducted on different languages, employing different methods and exhibiting various degrees of scale and systematicity (Fordyce, 1989; Abelin, 1999; Magnus, 2000; Hutchins, 1998). Recently, it has also been shown that phonesthemes affect online implicit language processing (Bergen, 2004) and language learning (Para"
N16-1038,N03-1036,0,0.0631594,"at sets of random words which do not overlap in form have a priori lower chance of being semantically related than sets of words that share a phonestheme. Therefore, in the first part of our study (Section 4) we present a stricter validation method for candidate phonesthemes that also includes considerations related to morphological diversity, which were ignored in previous work. Abramova et al. (2013) presented the first attempt to automatically assign meaning to sets of phonestheme-bearing words. The authors viewed the task as an instance of unsupervised ontology acquisition in the style of Widdows (2003) and used WordNet to assign over-arching labels to phonesthemic groups of words. While the approach was moderately successful in inducing WordNet labels that were in the direction predicted by the literature for a few phonesthemes (e.g., gl-containing words were assigned light-related labels), most phonesthemes did not receive meaningful labels according to the meanings typically associated with phonesthemes in the sound iconicity literature. The authors surmise that the failure could be due to the nature of WordNet, e.g., that it reflects only one type of semantic relation (hypernymy) which m"
N16-1043,W07-0607,1,0.711698,"eing able to associate a word with a visual extension, our model is simultaneously learning word representations that allow us to deal with a variety of other tasks—for example, as mentioned above, guessing the appearance of the object denoted by a new word from a purely verbal description, grouping concepts into categories by their similarity, or having both abstract and concrete words represented in the same space. 6 Related Work While there is work on learning from multimodal data (Roy, 2000; Yu, 2005, a.o.) as well as work on learning distributed representations from childdirected speech (Baroni et al., 2007; Kievit-Kylar and Jones, 2011, a.o.), to the best of our knowledge ours is the first method which learns distributed representations from multimodal child-directed data. For example, in comparison to Yu (2005)’s model, our approach (1) induces distributed representations for words, based on linguistic and visual context, and (2) operates entirely on distributed representations through similarity measures without positing a categorical level on which to learn wordsymbol/category-symbol associations. This leads to rich multimodal conceptual representations of words in terms of distributed multi"
N16-1043,P15-1029,0,0.0199639,"d multimodal features, while in Yu’s approach words are simply distributions over categories. It is therefore not clear how Yu’s approach could capture phenomena such as predicting appearance from a verbal description or representing abstract words–all tasks that our model is at least in principle well-suited for. Note also that Frank et al. (2007)’s Bayesian model we compare against could be extended to include realistic visual data in a similar vein to Yu’s, but it would then have the same limitations. Our work is also related to research on reference resolution in dialogue systems, such as Kennington and Schlangen (2015). However, unlike Kennington 391 Conclusion Our very encouraging results suggest that multimodal distributed models are well-suited to simulating human word learning. We think the most pressing issue to move ahead in this direction is to construct larger corpora recording the linguistic and visual environment in which children acquire language, in line with the efforts of the Human Speechome Project (Roy, 2009; Roy et al., 2015). Having access to such data will enable us to design agents that acquire semantic knowledge by leveraging all available cues present in multimodal communicative setups"
N16-1043,N15-1016,1,0.851398,"e objects present in a communicative episode. Inspired by recent computational models of meaning (Bruni et al., 2014; Kiros et al., 2014; Silberer 1 See K´ad´ar et al. (2015) for a recent review of this line of work, and another learning model using, like ours, real visual input. 2 Attentive Social MSG Model Like the original MSG, our model learns multimodal word embeddings by reading an utterance sequentially and making, for each word, two sets of predictions: (a) the preceding and following words, and (b) the visual representations of objects co-occurring with the utterance. However, unlike Lazaridou et al. (2015), we do not assume we know the right object to be associated with a word. We consider instead a more realistic scenario where multiple words in an utterance co-occur with multiple objects in the corresponding scene. Under this referential uncertainty, the model needs to induce word-object associations as part of learning, relying on current knowledge about word-object affinities as well as on any social clues present in the scene. Similar to the standard skipgram, the model’s parameters are context word embeddings W0 and tar387 Proceedings of NAACL-HLT 2016, pages 387–392, c San Diego, Califor"
N16-1043,P14-1068,0,0.221684,"Missing"
N19-1210,J08-4004,0,0.0331416,"Missing"
N19-1210,W17-6804,1,0.923893,"Missing"
N19-1210,C18-1135,1,0.891887,"Missing"
N19-1210,Q16-1003,0,0.041818,", 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distributional model to capture these phenomena, which is what we do in this paper for short-term meaning shift. Kulkarni et al. (2015) consider meaning shift in short time periods on Twitter data, but without providing an analysis of the observed shift nor systematically assessing the performance of the model, as we do here. Evaluation of semantic shift is difficult, due to the lack of annotated datasets (Frermann and Lapata, 2016). For this reason, even for long-term shift, evaluation is usually performed by manually inspecting the n words whose representation changes the most according to the model under investigation (Hamilton et al., 2016; Kim et al., 2014). Our dataset allows for a more systematic evaluation and analysis, and enables comparison in future studies. 3 Experimental Setup 3.1 Data We exploit user-generated language from an online forum of football fans, namely, the r/LiverpoolFC subreddit, one of the many communities hosted by the Reddit platform.2 Del Tredici and Fern´andez (2018) showed that this subr"
N19-1210,W11-2508,0,0.38761,"sis that a change in context of use mirrors a change in meaning. This in turn stems from the Distributional Hypothesis, that states that similarity in meaning results in similarity in context of use (Harris, 1954). Therefore, all models (including ours) spot semantic shift as a change in the word representation in different time periods. Among the most widely used techniques are Latent Semantic Analysis (Sagi et al., 2011; Jatowt and Duh, 2014), Topic Modeling (Wijaya and Yeniterzi, 2011), classic distributional representations based on co-occurence matrices of target words and context terms (Gulordava and Baroni, 2011). More recently, researchers have used 1 Data and code are available at: https://github. com/marcodel13/Short-term-meaning-shift. 2069 Proceedings of NAACL-HLT 2019, pages 2069–2075 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Aza"
N19-1210,P16-1141,0,0.798273,"McConnell-Ginet, 1992). This paper is, to the best of our knowledge, the first exploration of the latter phenomenon—which we call short-term meaning shift—using distributional representations. More concretely, we focus on meaning shift arising within a period of 8 years, and explore it on data from an online community of speakers, because there the adoption of new meanings happens at a fast pace (Clark, 1996; Hasan, 2009). Indeed, short-term shift is usually hard to observe in standard language, such as the language of books or news, which has been the focus of long-term shift studies (e.g., Hamilton et al., 2016; Kulkarni et al., 2015), since it takes a long time for a new meaning to be widely accepted in the standard language. Our contribution is twofold. First, we create a small dataset of short-term shift for analysis and evaluation, and qualitatively analyze the types of meaning shift we find.1 This is necessary because, unlike studies of long-term shift, we cannot rely on material previously gathered by linguists or lexicographers. Second, we test the behavior of a standard distributional model of semantic change when applied to short-term shift. Our results show that this model successfully det"
N19-1210,W14-2517,0,0.777345,"meaning-shift. 2069 Proceedings of NAACL-HLT 2019, pages 2069–2075 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Azarbonyad et al., 2017; Hamilton et al., 2016) or by initializing the embeddings at ti+1 using those computed at ti (Kim et al., 2014; Del Tredici et al., 2016; Phillips et al., 2017; Szymanski, 2017). We adopt the latter methodology (see Section 3.2). Unlike most previous work, we focus on the language of online communities. Recent studies of this type of language have investigated the spread of new forms and meanings (Del Tredici and Fern´andez, 2017, 2018; Stewart and Eisenstein, 2018), competing lexical variants (Rotabi et al., 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distrib"
N19-1210,C18-1117,0,0.282566,"du Abstract We present the first exploration of meaning shift over short periods of time in online communities using distributional representations. We create a small annotated dataset and use it to assess the performance of a standard model for meaning shift detection on shortterm meaning shift. We find that the model has problems distinguishing meaning shift from referential phenomena, and propose a measure of contextual variability to remedy this. 1 Introduction Semantic change has received increasing attention in empirical Computational Linguistics / NLP in the last few years (Tang, 2018; Kutuzov et al., 2018). Almost all studies so far have focused on meaning shift in long periods of time—decades to centuries. However, the genesis of meaning shift and the mechanisms that produce it operate at much shorter time spans, ranging from the online agreement on words’ meaning in dyadic interactions (Brennan and Clark, 1996) to the rapid spread of new meanings in relatively small communities of people in (Wenger, 1998; Eckert and McConnell-Ginet, 1992). This paper is, to the best of our knowledge, the first exploration of the latter phenomenon—which we call short-term meaning shift—using distributional rep"
N19-1210,Q15-1016,0,0.0590412,"and includes 157k words. For Reddit13 , we include only words that occur at least 20 times in the sample, so as to ensure meaningful representations for each word, while for the other two samples we do not use any frequency threshold: Since the embeddings used for the initialization of LiverpoolFC13 encode community-independent meanings, if a word doesn’t occur in LiverpoolFC13 its representation will simply be as in Reddit13 , which reflects the idea that if a word is not used in a community, then its meaning is not altered within that community. We train with standard skip-gram parameters (Levy et al., 2015): window 5, learning rate 0.01, embedding dimension 200, hierarchical softmax. 3.3 Evaluation dataset Our dataset consists of 97 words from the r/LiverpoolFC subreddit with annotations by members of the subreddit —that is, community members with domain knowledge (needed for this task) but no linguistic background. To ensure that we would get enough cases of semantic shift to enable a meaningful analysis, we started out from content words that increase their relative frequency between t1 and t2 .5 A threshold of 2 standard deviations above the mean yielded ∼200 words. The first author manually"
N19-1210,W17-2624,0,0.0194587,"2019, pages 2069–2075 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Azarbonyad et al., 2017; Hamilton et al., 2016) or by initializing the embeddings at ti+1 using those computed at ti (Kim et al., 2014; Del Tredici et al., 2016; Phillips et al., 2017; Szymanski, 2017). We adopt the latter methodology (see Section 3.2). Unlike most previous work, we focus on the language of online communities. Recent studies of this type of language have investigated the spread of new forms and meanings (Del Tredici and Fern´andez, 2017, 2018; Stewart and Eisenstein, 2018), competing lexical variants (Rotabi et al., 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distributional model to capture these phenomena, which i"
N19-1210,P17-2071,0,0.0143727,"c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word embeddings computed using the skip-gram model by Mikolov et al. (2013). Since embeddings computed in different semantic spaces are not directly comparable, time related representations are usually made comparable either by aligning different semantic spaces through a transformation matrix (Kulkarni et al., 2015; Azarbonyad et al., 2017; Hamilton et al., 2016) or by initializing the embeddings at ti+1 using those computed at ti (Kim et al., 2014; Del Tredici et al., 2016; Phillips et al., 2017; Szymanski, 2017). We adopt the latter methodology (see Section 3.2). Unlike most previous work, we focus on the language of online communities. Recent studies of this type of language have investigated the spread of new forms and meanings (Del Tredici and Fern´andez, 2017, 2018; Stewart and Eisenstein, 2018), competing lexical variants (Rotabi et al., 2017), and the relation between conventions in a community and the social standing of its members (Danescu-Niculescu-Mizil et al., 2013). None of these works has analyzed the ability of a distributional model to capture these phenomena, which is what we do in th"
N19-1265,N16-1014,0,0.0429711,"e state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks t"
N19-1265,D16-1127,0,0.0416633,"e state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks t"
N19-1265,W18-5045,0,0.0315275,"otated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two participants ask and answer questions about an image."
N19-1265,C18-1104,1,0.831206,"Missing"
N19-1265,N15-1020,0,0.0310712,"symbolic representations to characterise the state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in"
N19-1265,P17-1062,0,0.0143494,"e and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two partic"
N19-1265,W13-4065,0,0.0202117,"sier to train than RL. • A first in-depth study to compare cooperative learning to a state-of-the-art RL system. Our study shows that the linguistic skills of the models differ dramatically, despite approaching comparable task success levels. This underlines the importance of linguistic analysis to complement solely numeric evaluation. 2 Related Work Task-oriented dialogue systems The conventional architecture of task-oriented dialogue systems includes a pipeline of components, and the task of tracking the dialogue state is typically modelled as a partially-observable Markov decision process (Williams et al., 2013; Young et al., 2013; Kim et al., 2014) that operates on a symbolic dialogue state consisting of predefined variables. The use of symbolic representations to characterise the state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neur"
N19-1265,L16-1019,1,0.80446,"Missing"
N19-1265,W16-3601,0,0.0150046,"to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two participants ask and answer que"
P05-1029,P01-1031,1,0.831606,"Type Theory with Records (TTR). This allows simple interfacing with the grammar, a Constraint-based Grammar closely modelled on HPSG but formulated in TTR (see (Ginzburg, forthcoming)). Grounding Interaction Grounding an utterance u : T (‘the sign associated with u is of type T’) is modelled as involving the following interaction. (a) Addressee B tries to anchor the contextual parameters of T. If successful, B acknowledges u (directly, gesturally or implicitly) and responds to the content of u. (b) If unsuccessful, B poses a Clarification Request (CR), that arises via utterance coercion (see (Ginzburg and Cooper, 2001)). For reasons of space we do not formulate an explicit protocol here— the structure of such a protocol resembles the assertion protocol. Our subsequent discussion of assertion can be modified mutatis mutandis to grounding. NSU Resolution We assume the account of NSU resolution developed in (Ginzburg and Sag, 2000). The essential idea they develop is that NSUs get their main predicates from context, specifically via unification with the question that is currently under discussion, an entity dubbed the maximal question under discussion (MAX QUD ). NSU resolution is, consequently, tied to conver"
P05-1029,A00-2001,0,0.0651727,"Missing"
P05-1029,C96-1073,0,\N,Missing
P13-1053,J08-4004,0,0.405846,"wds” could or should be used to develop useful annotated linguistic resources. Those who have looked into this increasingly important issue have mostly concentrated on validating the quality of multiple non-expert annotations in terms of how they compare to expert gold standards; but they have only used simple aggregation methods based on majority voting to combine the judgments of individual annotators (Snow et al., 2008; Venhuizen et al., 2013). 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation"
P13-1053,E12-2019,0,0.0171348,"In both cases, the aggregators have the effect of disregarding some outlier annotators. Related Work There is an increasing number of projects using crowdsourcing methods for labelling data. Online Games with a Purpose, originally conceived by von Ahn and Dabbish (2004) to annotate images, have been used for a variety of linguistic tasks: Lafourcade (2007) created JeuxDeMots to develop a semantic network by asking players to label words with semantically related words; Phrase Detectives (Chamberlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 12 See also the papers presented at the NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (tinyurl.com/amtworkshop2010). 13 Creating a gold standard often involves adjudication of disagreements by experts, or even the removal of cases with disagreement from the dataset. See, e.g., the papers cited by Beigman Klebanov and Beigman (2009). 11 Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. 546 Other researchers have explored ways to directly identify “low-quality” annotators. For instance, Snow e"
P13-1053,P09-1002,0,0.029743,"not win by a sufficiently significant margin or does not make a particular quota. This is the most common approach in the literature (see, e.g., Venhuizen et al., 2013). (2) Independent aggregation of complex annotations. This is a natural generalisation of the first approach, resulting in a wider range of possible methods. We shall not explore it here, but only point out that in case annotators provide linear orders over categories, there is a close resemblance to classical voting the1 Some authors have combined qualitative and quantitative ratings; e.g., for the Graded Word Sense dataset of Erk et al. (2009) coders were asked to classify each relevant WordNet sense for a given item on a 5-point scale: 1 completely different, 2 mostly different, 3 similar, 4 very similar, 5 identical. 540 ory (Taylor, 2005); in case only partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. Item 3 Annotator 1 An"
P13-1053,J09-4005,0,0.025133,"JeuxDeMots to develop a semantic network by asking players to label words with semantically related words; Phrase Detectives (Chamberlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 12 See also the papers presented at the NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (tinyurl.com/amtworkshop2010). 13 Creating a gold standard often involves adjudication of disagreements by experts, or even the removal of cases with disagreement from the dataset. See, e.g., the papers cited by Beigman Klebanov and Beigman (2009). 11 Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. 546 Other researchers have explored ways to directly identify “low-quality” annotators. For instance, Snow et al. (2008) and Raykar et al. (2010) propose Bayesian methods for identifying and correcting annotators’ biases, while Ipeirotis et al. (2010) propose an algorithm for assigning a quality score to annotators that distinguishes intrinsic error rate from an annotator’s bias. In our approach, we do not directly rate annotators or recalibrate their annotations—rather, some ou"
P13-1053,J97-1002,0,0.11269,"ng to combine the judgments of individual annotators (Snow et al., 2008; Venhuizen et al., 2013). 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment point"
P13-1053,W08-2230,0,0.0188127,"annotation. Our greedy consensus rule also makes use of agreement to ensure a minimum level of consensus. In both cases, the aggregators have the effect of disregarding some outlier annotators. Related Work There is an increasing number of projects using crowdsourcing methods for labelling data. Online Games with a Purpose, originally conceived by von Ahn and Dabbish (2004) to annotate images, have been used for a variety of linguistic tasks: Lafourcade (2007) created JeuxDeMots to develop a semantic network by asking players to label words with semantically related words; Phrase Detectives (Chamberlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 12 See also the papers presented at the NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (tinyurl.com/amtworkshop2010). 13 Creating a gold standard often involves adjudication of disagreements by experts, or even the removal of cases with disagreement from the dataset. See, e.g., the papers cited by Beigman Klebanov and Beigman (2009). 11 Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. 546 Oth"
P13-1053,W10-0702,0,0.0183104,"tional Linguistics, pages 539–549, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t know” or “unclear” category). For large-scale annotation projects run over the Internet it is furthermore very likely t"
P13-1053,rumshisky-etal-2012-word,0,0.0177408,"at eliminating only the most extreme outlier annotators is a useful strategy, and on the other hand, that a high-quality collective annotation can be obtained from a group of annotators that disagree substantially.11 6 have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al., 2008; Rumshisky, 2011; Rumshisky et al., 2012), amongst others.12 All these efforts face the problem of how to aggregate the information provided by a group of volunteers into a collective annotation. However, by and large, the emphasis so far has been on issues such as experiment design, data quality, and costs, with little attention being paid to the aggregation methods used, which are typically limited to some form of majority vote (or taking averages if the categories are numeric). In contrast, our focus has been on investigating different aggregation methods for arriving at a collective annotation. Our work has connections with the l"
P13-1053,W11-0409,0,0.0188686,"the one hand, that eliminating only the most extreme outlier annotators is a useful strategy, and on the other hand, that a high-quality collective annotation can be obtained from a group of annotators that disagree substantially.11 6 have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al., 2008; Rumshisky, 2011; Rumshisky et al., 2012), amongst others.12 All these efforts face the problem of how to aggregate the information provided by a group of volunteers into a collective annotation. However, by and large, the emphasis so far has been on issues such as experiment design, data quality, and costs, with little attention being paid to the aggregation methods used, which are typically limited to some form of majority vote (or taking averages if the categories are numeric). In contrast, our focus has been on investigating different aggregation methods for arriving at a collective annotation. Our work h"
P13-1053,D08-1027,0,0.517483,"Missing"
P13-1053,W13-0215,0,0.235483,"volution for the creation of annotated corpora, within the computational linguistics community there so far is no clear understanding of how the so-called “wisdom of the crowds” could or should be used to develop useful annotated linguistic resources. Those who have looked into this increasingly important issue have mostly concentrated on validating the quality of multiple non-expert annotations in terms of how they compare to expert gold standards; but they have only used simple aggregation methods based on majority voting to combine the judgments of individual annotators (Snow et al., 2008; Venhuizen et al., 2013). 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al.,"
P13-1053,rosenthal-etal-2010-towards,0,0.0234479,"Association for Computational Linguistics, pages 539–549, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t know” or “unclear” category). For large-scale annotation projects run over the Internet it is furthe"
P16-1144,D15-1075,0,0.0561594,"n about one fifth of the cases, the annotators could not guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sen"
P16-1144,P82-1020,0,0.82838,"Missing"
P16-1144,C14-3002,0,0.0424602,"Missing"
P16-1144,P15-1007,0,0.0212447,"Missing"
P16-1144,D13-1020,0,0.058654,"guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sentences entail each other. Moreover, models can have access"
P16-1144,N15-1020,0,0.0259137,"ately, the system responses are appropriate for the respective questions. However, when taken together, they are incoherent. The system behaviour is somewhat parrot-like. It can locally produce perfectly sensible language fragments, but it fails to take the meaning of the broader discourse context into account. Much research effort has consequently focused on designing systems able to keep information from the broader context into memory, and possibly even perform simple forms of reasoning about it (Hermann et al., 2015; Hochreiter and Schmidhuber, 1997; Ji et al., 2015; Mikolov et al., 2015; Sordoni et al., 2015; Sukhbaatar et al., 2015; Wang and Cho, 2015, a.o.). In this paper, we introduce the LAMBADA dataset (LAnguage Modeling Broadened to Account for Discourse Aspects). LAMBADA proposes a word prediction task where the target item is difficult to guess (for English speakers) when only the sentence in which it appears is available, but becomes easy when a broader context is presented. Consider Example (1) in Figure 1. The sentence Do you honestly think that I would want you to have a ? has a multitude of possible continuations, but the broad context clearly indicates that the missing word is misca"
P19-1184,P17-1162,0,0.0987842,"Missing"
P19-1184,W18-6547,0,0.0207087,"sequences of questionanswer pairs, the PhotoBook dataset includes diverse dialogue acts. Qualitative examination shows that, not surprisingly, a large proportion of messages include an image description. These descriptions however are interleaved with clarification questions, acceptances/rejections, and acknowledgements. For an example, see the dialogue excerpt in Figure 1. Further data samples are available in Appendix C. A deeper analysis of the task-specific dialogue acts would require manual annotation, which could be added in the future. 5.3 Reference Chains In a small-scale pilot study, Ilinykh et al. (2018) find that the pragmatics of goal-oriented dialogue leads to consistently more factual scene descriptions and reasonable referring expressions than traditional, context-free labelling of the same images. We argue that in the PhotoBook task referring expressions are not only adapted based on the goal-oriented nature of the interaction but also by incorporating the developing common ground between the participants. This effect becomes most apparent when collecting all referring expressions for a specific target image produced during the different rounds of a game in its coreference chain. The fo"
P19-1184,D14-1086,0,0.0953428,"n a reference chain. Our results show that this information is particularly important to resolve later descriptions and underline the need to develop more sophisticated models of common ground in dialogue interaction.1 1 Introduction The past few years have seen an increasing interest in developing computational agents for visually grounded dialogue, the task of using natural language to communicate about visual content in a multi-agent setup. The models developed for this task often focus on specific aspects such as image labelling (Mao et al., 2016; Vedantam et al., 2017), object reference (Kazemzadeh et al., 2014; De Vries et al., 2017a), visual question answering (Antol et al., 2015), and first attempts of visual dialogue proper (Das et al., 2017), but fail to produce consistent outputs over a conversation. 1 The PhotoBook dataset is being released by the Dialogue Modelling Group led by Raquel Fern´andez at the University of Amsterdam. The core of this work was done while Janosch Haber and Elia Bruni were affiliated with the group. We hypothesise that one of the main reasons for this shortcoming is the models’ inability to effectively utilise dialogue history. Human interlocutors are known to collabo"
P19-1184,D17-2014,0,0.0713086,"Missing"
P19-1184,P82-1020,0,0.808458,"Missing"
P19-1184,tokunaga-etal-2012-rex,0,0.02045,"t, together with the data collection protocol, the automatically extracted reference chains, and the code used for our analyses and models are available at the following site: https://dmg-photobook.github.io. 2 Related Work Seminal works on cooperative aspects of dialogue have developed their hypotheses and models based on a relatively small number of samples collected through lab-based conversation tasks (e.g., Krauss and Weinheimer, 1964, 1966; Clark and WilkesGibbs, 1986; Brennan and Clark, 1996; Anderson et al., 1991). Recent datasets inspired by this line of work include the REX corpora (Takenobu et al., 2012) and PentoRef (Zarrieß et al., 2016). With the development of online data collection methods (von Ahn et al., 2006) a new, game-based approach to quick and inexpensive collection of dialogue data became available. PhotoBook builds on these traditions to provide a large-scale dataset suitable for data-driven development of computational dialogue agents. The computer vision community has recently developed large-scale datasets for visually grounded dialogue (Das et al., 2017; De Vries et al., 2017b). These approaches extend earlier work on visual question answering (Antol et al., 2015) to a mult"
P19-1184,L16-1019,1,\N,Missing
P19-1350,P15-2123,0,0.0320022,"those learned by the model trained on Wh-q independently: Y/N questions result in a big hard-to-distinguish “blob”, and are confused with Wh-q about size, as visible in Fig. 2 and the confusion matrix analysis (in the SM). In contrast, Rehearsal remembers how to distinguish among all kinds of Wh-q and between Wh-q and Y/N-q. The error analysis confirms that the model hardly makes any mistakes related to task confusion. However, despite the higher performance than EWC, Rehearsal is still not able to discern well between different kinds of Y/N-q. 5 Related Work Early work on life-long learning (Chen et al., 2015; Mitchell et al., 2015) is related to ours, but typically concerns a single task (e.g., relation extraction). Lee (2017) aims to transfer conversational skills from a synthetic domain to a customer-specific application in dialogue agents, while Yogatama et al. (2019) show that current models for different NLP tasks are not able to properly reuse previously learned knowledge. In general, continual learning has been mostly studied in computer vision. To the best of our knowledge, little has been done on catastrophic forgetting in VQA. A study on forgetting in the context of VQA and closest to o"
S12-1013,J06-2002,0,0.0751157,"Missing"
S14-1019,W14-1409,1,0.425275,"Missing"
S14-1019,C04-1181,0,0.0683323,"Missing"
S16-2015,W15-1104,1,0.828511,"Missing"
S16-2015,walker-etal-2012-corpus,0,0.0139425,"d our own with a boolean term) is that it does not account for utterance length: clearly, a longer response has more chances to contain a marker m than a shorter response. Indeed length has been observed to be an important confounding factor in the computation of stylistic coordination (Gao et al., 2015). We therefore proposed an extension of the original measure to account for both aspects independently: the presence of a marker in a post (1 vs. 0) and its frequency given the post length. In our model, alignment between Q and R and the prior for the author of R with respect to 1 According to Walker et al. (2012), these α scores were computed using an ordinal scale (except for sarcasm) on a dataset comprising both the set of Q-R pairs we take as starting point here and data from an additional experiment referred to as P123 by the authors. See their paper for details. 2 These lists of markers are based on Linguistic Inquiry and Word Count (LIWC) by Pennebaker et al. (2007). 121 a1 : presence of m in R given that Q contains m a2 : proportion of words in R that are m 0.5 0.4 0.3 Cohen's d marker class m correspond to feature vectors ~a and ~b, respectively, with a first feature indicating marker presence"
W02-0203,W02-0222,0,\N,Missing
W02-0203,W01-1616,1,\N,Missing
W02-0203,P01-1031,1,\N,Missing
W02-0203,J96-2004,0,\N,Missing
W02-0203,W00-0302,0,\N,Missing
W08-0125,P04-1085,0,0.220016,"assistants that process, understand, summarize and report the output of meetings; meeting tracking systems that assist in implementing decisions; and group decision support systems that, for instance, help in constructing group memory (Romano and Nunamaker, 2001; Post et al., 2004; Voss et al., 2007). Previously researchers have focused on the interactive aspects of argumentative and decisionmaking dialogue, tackling issues such as the detection of agreement and disagreement and the level of emotional involvement of conversational participants (Hillard et al., 2003; Wrede and Shriberg, 2003; Galley et al., 2004; Gatica-Perez et al., 2005). From a perhaps more formal perspective, Verbree et al. (2006) have created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only Hsueh and Moore (2007a; 2007b), however, have specifically investigated the automatic detection of decisions. Using the AMI Meeting Corpus, Hsueh and Moore (2007b) attempt to identify the dialogue acts (DAs) in a meeting transcript that are “decisionrelated”. The authors define these DAs on the basis of two kinds of manually created summaries: an e"
W08-0125,N03-2012,0,0.316116,"range of applications, such as automatic meeting assistants that process, understand, summarize and report the output of meetings; meeting tracking systems that assist in implementing decisions; and group decision support systems that, for instance, help in constructing group memory (Romano and Nunamaker, 2001; Post et al., 2004; Voss et al., 2007). Previously researchers have focused on the interactive aspects of argumentative and decisionmaking dialogue, tackling issues such as the detection of agreement and disagreement and the level of emotional involvement of conversational participants (Hillard et al., 2003; Wrede and Shriberg, 2003; Galley et al., 2004; Gatica-Perez et al., 2005). From a perhaps more formal perspective, Verbree et al. (2006) have created an argumentation scheme intended to support automatic production of argument structure diagrams from decision-oriented meeting transcripts. Only Hsueh and Moore (2007a; 2007b), however, have specifically investigated the automatic detection of decisions. Using the AMI Meeting Corpus, Hsueh and Moore (2007b) attempt to identify the dialogue acts (DAs) in a meeting transcript that are “decisionrelated”. The authors define these DAs on the basis o"
W08-0125,N07-1004,0,0.365105,"that were made and the trains of reasoning that led to those decisions. Such a capability would allow work groups to keep track of courses of action that were shelved or rejected, and could allow new team members to get quickly up to speed. Thanks to the recent availability of substantial meeting corpora—such as the ISL (Burger et al., 2002), ICSI (Janin et al., 2004), and AMI (McCowan et al., 2005) Meeting Corpora—current research on the structure of decision-making dialogue and its use for automatic decision detection has helped to bring this vision closer to reality (Verbree et al., 2006; Hsueh and Moore, 2007b). Our aim here is to further that research by applying a simple notion of dialogue structure to the task of automatically detecting decisions in multiparty dialogue. A central hypothesis underlying our approach is that this task is best addressed by taking into account the roles that different utterances play in the decision-making process. Our claim is that this approach facilitates both the detection of regions of discourse where decisions are discussed and adopted, and also the identification of important aspects of the decision discussions themselves, thus opening the way to better and m"
W08-0125,lisowska-etal-2004-user,0,0.256809,"Missing"
W08-0125,2007.sigdial-1.4,1,0.731463,"cisions in multiparty dialogue. A central hypothesis underlying our approach is that this task is best addressed by taking into account the roles that different utterances play in the decision-making process. Our claim is that this approach facilitates both the detection of regions of discourse where decisions are discussed and adopted, and also the identification of important aspects of the decision discussions themselves, thus opening the way to better and more concise reporting. In the next section, we describe prior work on related efforts, including our own work on action item detection (Purver et al., 2007). Sections 3 and 4 then present our decision annotation scheme, which distinguishes several types of decision-related dialogue acts (DAs), and the corpus used as data (in this study a section of the AMI Meeting Corpus). Next, in Section 5, we describe our experimental methodology, including the basic conception of our classification approach, the features we used in classification, and our evaluation metrics. Section 6 then presents our results, obtained with a hierarchical classifier that first trains individual sub-classifiers to detect the different types of decision DAs, and then uses a su"
W08-0125,N07-4009,1,\N,Missing
W09-3944,E09-1032,1,0.722128,"Missing"
W09-3944,2007.sigdial-1.40,1,0.884909,"Missing"
W09-3944,E06-1022,0,0.174502,"Missing"
W14-4321,P04-1085,0,0.17031,"linguistic features that are useful for inferring acceptances and rejections, often building on observations made by conversational analysts (Pomerantz, 1984; Brown and Levinson, 1987). Furthermore, recent work by Bousmalis et al. (2013) suggests that there are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the s"
W14-4321,W11-0702,0,0.247751,"Missing"
W14-4321,baccianella-etal-2010-sentiwordnet,0,0.0423251,"Missing"
W14-4321,N06-2014,0,0.685401,"Missing"
W14-4321,P97-1023,0,0.483702,"Missing"
W14-4321,N03-2012,0,0.223132,"Missing"
W14-4321,W08-0125,1,0.843449,"Missing"
W14-4321,W13-4006,0,0.304788,"are specific nonverbal behaviours associated with agreement and disagreement, such as different types of head, lip, and hand movements. However, to our knowledge, the role of logical polarity has not been investigated in any detail by computational approaches. Several systems make use of subjective polarity, i.e., sentiment. For instance, Galley et al. (2004) use the list of subjective adjectives compiled by Hatzivassiloglou and McKeown (1997) to assign a positive and a negative polarity value to an utterance given the number of subjective positive/negative adjectives it contains. Similarly, Misra and Walker (2013) use the MPQA Subjec152 tivity Lexicon (Wilson et al., 2005) to capture the local sentiment of an online post response given the number of words in the response with strongly subjective positive/negative polarity according to the subjectivity lexicon. Yin et al. (2012) assign a positive and a negative score to a post by aggregating the sentiment scores of those words that can be found in SentiWordNet (Baccianella et al., 2010). Although subjective polarity may be helpful (e.g., utterances with a high positive sentiment score may be more likely to be acceptances), this is not the kind of polari"
W14-4321,2007.sigdial-1.4,0,0.0750939,"Missing"
W14-4321,walker-etal-2012-corpus,0,0.110569,"Missing"
W14-4321,P11-2065,0,0.0304858,"Missing"
W14-4321,H05-1044,0,0.0930486,"Missing"
W14-4321,W12-3710,0,0.681224,"Missing"
W15-0106,P05-1030,0,0.136545,"at an indicative was intended as an inform act and hence requires a consistency check on level 4.2) are again distinct categories. 3 We thank the authors for providing us with their annotated corpus; in the dialogues, I is explaining to K how to assemble a paper airplane. We had the German-language examples translated to English by a native speaker of German. 4 Retrieved from the British National Corpus (BNC) (Burnard, 2000) using SCoRE (Purver, 2001). 47 3 3.1 Corpus Study Previous Studies Our work builds on previous corpus studies of CRs (Purver et al., 2003; Rodr´ıguez and Schlangen, 2004; Rieser and Moore, 2005). However, existent studies are not perfectly suited for investigating grounding at the level of intentions.5 Firstly, the annotation scheme of Purver et al. (2003; 2004), which the authors apply to a section of the BNC (Burnard, 2000), makes use of semantic categories that cannot easily be mapped to the intention-level distinctions introduced in the previous section. Secondly, while the schemes employed by Rodr´ıguez and Schlangen (2004) and Rieser and Moore (2005) (both based on Schlangen, 2004) do include a category for intention-level CRs, the corpora they annotate—the Bielefeld Corpus and"
W15-0106,W04-2325,0,0.345483,"common ground of the dialogue participants, the information they mutually take for granted (Stalnaker, 1978). This common ground is changed and expanded over the course of a conversation in a process called grounding (Clark, 1996). We are interested in the mechanisms used to establish agreement, i.e., in the conversational means to establish a belief as joint. To investigate this issue, in this paper we examine cases where grounding (partially) fails, as indicated by the presence of clarifications requests (CRs). In contrast to previous work (i.a., Gabsdil, 2003; Purver, 2004; Rodr´ıguez and Schlangen, 2004), which has mostly focused on CRs triggered by acoustic and semantic understanding problems, we are particularly concerned with problems related to intention recognition (going beyond semantic interpretation) and intention adoption (i.e., mutual agreement). The following examples, from the AMI Meeting Corpus (Carletta, 2007), are cases in point: (1) A: I think that’s all. B: Meeting’s over? (2) A: Just uh do that quickly. B: How do you do it? (3) A: I’d say two. B: Why? In these examples, it cannot be said that B has fully grounded A’s proposal, but also not that B rejects A’s utterance. Rathe"
W15-0106,W09-3929,0,\N,Missing
W15-0106,W01-1616,0,\N,Missing
W15-0129,P04-1085,0,0.0528626,"ground can be necessary to interpret elliptical, anaphoric, fragmented and otherwise non-sentential expressions (Ginzburg, 2012). Establishing and maintaining common ground is a complicated process, even for human interlocutors (Clark, 1996). A basic issue is to determine which proposals in the dialogue have been accepted and which have been rejected: Accepted proposals are committed to common ground; rejected ones are not (Stalnaker, 1978). An important area of application is the automated summarisation of meeting transcripts, where it is vital to retrieve only mutually agreed propositions (Galley et al., 2004). Determining the acceptance or rejection function of an utterance can be a highly nontrivial matter (Walker, 1996; Lascarides and Asher, 2009) as the utterance’s surface form alone is oftentimes not explicit enough (Horn, 1989; Schl¨oder and Fern´andez, 2014). Acceptance may merely be inferable from a relevant next contribution (Clark, 1996), and some rejections require substantial contextual awareness and inference capabilities to be detected—for example, when the intuitive meaning of ‘yes’ and ‘no’ is reversed, as in (1), or when the rejection requires some pragmatic enrichment, such as com"
W15-0129,N06-2014,0,0.0566741,"Missing"
W15-0129,N03-2012,0,0.128997,"Missing"
W15-0129,W09-3920,0,0.0769189,"Missing"
W15-0129,W13-4006,0,0.0385292,"Missing"
W15-0129,W14-4321,1,0.805058,"Missing"
W15-0129,W12-3710,0,0.0472991,"Missing"
W15-2712,W13-3209,0,0.122551,"ifferent, identical, brown cardboard boxes. Adrian accidentally puts a pair of red socks in the box containing blue objects, and Barbara remarks ‘no, no, these belong in the red box’. Thus, even if red when modifying box (or indeed any noun denoting a physical ob(1) a. If you ate some of the cookies, then I won’t have enough for the party. ; some and possibly all b. A: Did you eat all the cookies? B: I ate some. ; some but not all Distributional models have so far not been particularly successful in modelling the meaning of function words (but see Baroni et al. (2012); Bernardi et al. (2013); Hermann et al. (2013)). We believe that discourse-aware distributional semantics may fare better in this respect. We elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is."
W15-2712,W13-0104,1,0.883712,"in the lexical substitution task (McCarthy and Navigli, 2007; Erk et al., 2013), which predicts one or more paraphrases for a word in a given sentence. Unlike Word Sense Disambiguation, word meaning in context is specific to a given use of a word, that is, it doesn’t assume a pre-defined list of senses and can account for highly specific contextual effects. However, in this tradition context is restricted to one sentence, so the semantic phenomena modeled do not extend to discourse or dialogue. (3) Compositional distributional semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013), which predicts the meaning of a phrase or sentence from the meaning of its component units. For instance, compositional distributional semantics accounts for how the generic distributional representation of, say, red makes different contributions when composed with nouns like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2"
W15-2712,W13-3214,0,0.0237673,"utions when composed with nouns like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2.2, we have highlighted the context update potential of utterances as a feature that should be captured by compositional distributional models beyond the word/phrase level. Recent work has evaluated such models on dialogue act tagging tasks (Kalchbrenner and Blunsom, 2013; Milajevs et al., 2014). However, these approaches consider utterances in isolation and rely on a predefined set of dialogue act types that are to a large extent arbitrary, and in any case of a metalinguistic nature. Similar comments would apply to the task of identifying discourse relations connecting isolated pairs of sentences. Instead, we argue that pragmatically-aware distributional models should help us to induce dialogue acts in an unsupervised way and to model them as context update functions. Thus, we suggest to adopt tasks that target coherence and the evolution of common ground — w"
W15-2712,D10-1113,0,0.0256717,"on discourse and dialogue context and then consider the dynamic meaning of sentences as context-change potential. 2.1 Word and Phrase Meaning As is well known, standard distributional models provide a single meaning representation for a word, which implicitly encodes all its possible senses and meaning nuances in general. A few recent models do account for some contextual effects within the scope of a sentence: For instance, the different shades of meaning that an adjective like red takes depending on the noun it modifies (e.g., car vs. cheek). However, such models, e.g. Erk and Pad´o (2008), Dinu and Lapata (2010), and Erk et al. (2013), typically use just a single word or sentence as context. They do not look into how word meaning gets progressively constrained by the common ground of the speakers as the discourse unfolds. A prominent type of “meaning adjustment” in discourse and dialogue is the interaction with the properties of the referent a particular word is associated to. For example, when we use a word like box, which a priori can be used for entities with very different properties, we typically use it to refer to a specific box in a given context, and this constrains its interpretation. The re"
W15-2712,D08-1094,0,0.0821638,"Missing"
W15-2712,P15-1029,0,0.0176232,"part. For instance, a vector for a phrase like red box in a context where red refers to the box’ contents should be mapped to different types of images depending on whether it has been constructed by a pragmatically aware model or not. Such a dataset could be constructed by creating images of referents of the same phrase used in different contexts, where the task would be to pick the best image for each context. A related task would be reference resolution in a situated visual dialogue context (which can be seen as a situated version of image retrieval). This task has recently been tackled by Kennington and Schlangen (2015), who present an incremental acWe propose to take a different look on what the distributional meaning of a sentence is. Sentences are part of larger communicative situations and, as highlighted in the Dynamic Semantic tradition, can be considered relations between the discourse so far and what is to come next. We thus challenge the distributional semantics community to develop dynamic distributional semantic models that are able to encode the “context change potential” that sentences and utterances bring about as well as their coherence within a discourse context, including but not limited to"
W15-2712,J13-3003,0,0.107601,"rnardi∗ Gemma Boleda∗ Raquel Fern´andez† Denis Paperno∗ ∗ Center for Mind/Brain Sciences University of Trento † Institute for Logic, Language and Computation University of Amsterdam Abstract their unawareness of the unfolding discourse context. Standardly, distributional models are constructed from large amounts of data in batch mode by aggregating information into a vector that synthesises the general distributional meaning of an expression. Some of the recent distributional models account for contextual effects within the scope of a phrase or a sentence, (e.g., (Baroni and Zamparelli, 2010; Erk et al., 2013)), but they are not intended to capture how the meaning depends on the incrementally built discourse context where an expression is used. Since words and sentences are not used in isolation but are typically part of a discourse, the traditional distributional view is not sufficient. We argue that, to grow into an empirically adequate, full-fledged theory of meaning and interpretation, distributional models must evolve to provide meaning representations for actual language use in discourse and dialogue. Specifically, we discuss how the type of information they encode needs to be extended, and p"
W15-2712,P14-1132,0,0.0273218,"nd and coherence, it is critical to capture the discourse context-dependent and incremental nature of meaning. Here we sketch out a series of tasks related to some of the main phenomena we have discussed, against which new models could be evaluated. In Section 2.1 we have considered the need to interface conceptual meaning with referential meaning incrementally built up as a discourse unfolds. A good testbed for evaluating these aspects is offered by the recent development of crossmodal distributional semantic frameworks that are able to map between language and vision (Karpathy et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Current models have shown that images representing a concept can be retrieved by mapping a word vector into a visual space, and more recently image generation systems that create images from word vectors have also been introduced (Lazaridou et al., 2015a; Lazaridou et al., 2015b). These frameworks could be used to test whether an incrementally constructed, discoursecontextualised word vector is able to retrieve and generate different, more contextually appropriate images than its out-of-context vector counterpart. For instance, a vector for a phrase like red box in a co"
W15-2712,D11-1129,0,0.157329,"Missing"
W15-2712,N15-1016,0,0.289155,"idered the need to interface conceptual meaning with referential meaning incrementally built up as a discourse unfolds. A good testbed for evaluating these aspects is offered by the recent development of crossmodal distributional semantic frameworks that are able to map between language and vision (Karpathy et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Current models have shown that images representing a concept can be retrieved by mapping a word vector into a visual space, and more recently image generation systems that create images from word vectors have also been introduced (Lazaridou et al., 2015a; Lazaridou et al., 2015b). These frameworks could be used to test whether an incrementally constructed, discoursecontextualised word vector is able to retrieve and generate different, more contextually appropriate images than its out-of-context vector counterpart. For instance, a vector for a phrase like red box in a context where red refers to the box’ contents should be mapped to different types of images depending on whether it has been constructed by a pragmatically aware model or not. Such a dataset could be constructed by creating images of referents of the same phrase used in differen"
W15-2712,J86-3001,0,0.186101,"tributional models mainly capture the latter stable conventions. The challenge is thus to be able to also capture the former, discourse-dependent meaning. Moreover, even function words, which are notreferential and are usually considered to have a precise (logical) meaning, are subject to pragmatic effects. For instance, the meaning of the determiner some is typically taken to be that of an existential quantifier (i.e., there exists at least one object with certain properties). Yet, its ‘at least one’ meaning may be refined in particular discourse contexts, as shown in the following examples: Grosz and Sidner, 1986; Kamp and Reyle, 1993; Asher and Lascarides, 2003; Ginzburg, 2012), namely, that the meaning of an expression consists in its context-change potential, where context is incrementally built up as a discourse proceeds. We contend that a distributional semantics for language use should account for the discourse context-dependent, dynamic, and incremental nature of language. Generic semantic knowledge won’t suffice: one needs to encode somehow the discourse state or common ground, which will enable modeling discourse and dialogue coherence. In this section, we first look into examples that illust"
W15-2712,W14-4321,1,0.874434,"Missing"
W15-2712,D12-1110,0,0.447686,"should capture, and propose concrete tasks on which they could be tested. 1 Introduction Distributional semantics has revolutionised computational semantics by representing the meaning of linguistic expressions as vectors that capture their co-occurrence patterns in large corpora (Turney et al., 2010; Erk, 2012). This strategy has been shown to be very successful for modelling word meaning, and it has recently been expanded to capture the meaning of phrases and even sentences in a compositional fashion (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). Distributional semantic models are often presented as a robust alternative to representing meaning, compared to symbolic and logic-based approaches in formal semantics, thanks to their flexible representations and their data-driven nature. However, current models fail to account for aspects of meaning that are central in formal semantics, such as the relation between linguistic expressions and their referents or the truth conditions of sentences. In this position paper we focus on one of the main limitations of current distributional approaches, namely, 2 Meaning in Discourse As we just poin"
W15-2712,S14-2001,1,0.826578,"e elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena discussed above is certainly challenging. However, we believe that, given the many recent advances in the field, the distributional semantics community is ready to take up this challenge. We have argued that, in order to account for the dynamics of situated common ground and coherence, i"
W15-2712,D13-1170,0,0.00520688,"d Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena discussed above is certainly challenging. However, we believe that, given the many recent advances in the field, the distributional semantics community is ready to take up this challenge. We have argued that, in order to account for the dynamics of situated common ground and coherence, it is critical to capture the discourse context-dependent and incremental nature of meaning. Here we sketch out a series of tasks rela"
W15-2712,S07-1009,0,0.0220861,"fic word occurrence in context. These approaches offer a very valuable starting point, but their scope differs from ours. In particular, we can identify the following three main traditions: (1) Word Sense Disambiguation (Navigli, 2009, offers an overview), which aims to assign one of the predefined list of word senses to a given word, depending on the context. These are typically dictionary senses, and so do not capture semantic nuances that depend on the specific use of the word in a given discourse or dialogue context. (2) Word meaning in context as modeled in the lexical substitution task (McCarthy and Navigli, 2007; Erk et al., 2013), which predicts one or more paraphrases for a word in a given sentence. Unlike Word Sense Disambiguation, word meaning in context is specific to a given use of a word, that is, it doesn’t assume a pre-defined list of senses and can account for highly specific contextual effects. However, in this tradition context is restricted to one sentence, so the semantic phenomena modeled do not extend to discourse or dialogue. (3) Compositional distributional semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013), which predicts the meaning of a phrase"
W15-2712,N15-1020,0,0.0682292,"Missing"
W15-2712,D14-1079,0,0.0132662,"like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2.2, we have highlighted the context update potential of utterances as a feature that should be captured by compositional distributional models beyond the word/phrase level. Recent work has evaluated such models on dialogue act tagging tasks (Kalchbrenner and Blunsom, 2013; Milajevs et al., 2014). However, these approaches consider utterances in isolation and rely on a predefined set of dialogue act types that are to a large extent arbitrary, and in any case of a metalinguistic nature. Similar comments would apply to the task of identifying discourse relations connecting isolated pairs of sentences. Instead, we argue that pragmatically-aware distributional models should help us to induce dialogue acts in an unsupervised way and to model them as context update functions. Thus, we suggest to adopt tasks that target coherence and the evolution of common ground — which is what discourse r"
W15-2712,P14-1009,1,0.848888,"not all Distributional models have so far not been particularly successful in modelling the meaning of function words (but see Baroni et al. (2012); Bernardi et al. (2013); Hermann et al. (2013)). We believe that discourse-aware distributional semantics may fare better in this respect. We elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena"
W15-2712,D10-1115,0,\N,Missing
W15-2712,E12-1004,1,\N,Missing
W15-2712,P83-1007,0,\N,Missing
W15-2712,Q14-1017,0,\N,Missing
W15-2712,2014.lilt-9.5,1,\N,Missing
W15-2712,P13-2010,1,\N,Missing
W15-2712,W15-0120,0,\N,Missing
W17-5534,D16-1127,0,0.00601317,"nvestigate the potential of adversarial evaluation methods for open-domain dialogue generation systems, comparing the performance of a discriminative agent to that of humans on the same task. Our results show that the task is hard, both for automated models and humans, but that a discriminative agent can learn patterns that lead to above-chance performance. 1 Introduction End-to-end dialogue response generation systems trained to produce a plausible utterance given some limited dialogue context are receiving increased attention (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016). However, for systems dealing with chatbot-style open-dialogue, where task completion is not applicable, evaluating the quality of their responses remains a challenge. Most current models are evaluated with measures such as perplexity and overlapbased metrics like BLEU, that compare the generated response to the ground-truth response in an actual dialogue. This kind of measures, however, correlate very weakly or not at all with human judgements on response quality (Liu et al., 2016). In this paper, we explore a different approach to evaluating open-domain dialogue response generation systems,"
W17-5534,D17-1230,0,0.128142,"are the generated response to the ground-truth response in an actual dialogue. This kind of measures, however, correlate very weakly or not at all with human judgements on response quality (Liu et al., 2016). In this paper, we explore a different approach to evaluating open-domain dialogue response generation systems, inspired by the classic Turing Test (Turing, 1950): measuring the quality of the generated responses on their indistinguishability from human output. This approach has been preliminary explored in recent work under the heading of adversarial evaluation (Kannan and Vinyals, 2016; Li et al., 2017), drawing a parallel with generative adversarial learning (Goodfellow et al., 2014). Here we concentrate on exploring the potential and the limits of such an adversarial eval2 The Discriminative Agent Our discriminative agent is a binary classifier which takes as input a sequence of dialogue utterances and predicts whether the dialogue is real or fake. The agent treats as positive examples of coherent dialogue actual dialogue passages and as negative examples passages where the last utterance has been randomly replaced. Random replacement has been used in the past to study discourse coherence"
W17-5534,D16-1230,0,0.050216,"Missing"
W17-5534,P17-1103,0,0.148723,". The output of the attention is then fed to a sigmoid function, which returns the probability of the input being real or fake: p = sigmoid(Wcv + bc ) 3 To assess the performance of our discriminative model, we conduct an experiment with human annotators. To our knowledge, this is the first study of its kind ever conducted. Previous human evaluation experiments of dialogue generation systems have mostly consisted in asking participants to choose the better response between two options generated by different models or to rate a generated dialogue along several dimensions (Vinyals and Le, 2015; Lowe et al., 2017; Li et al., 2017). In contrast, here we present humans with the same task faced by the discriminator: We show them a dialogue passage and ask them to decide whether, given the first one or two utterances of context, the shown continuation is the actual follow-up utterance in the original dialogue or a random response. The data for this experiment consists of 900 pas[6] As loss function we then use the negative log likelihood of the correct labels: L=− 2.2 P d log pdj Human Evaluation [7] Training Details We trained the discriminator with a combination of three different datasets: MovieTriples"
W17-5534,P12-2040,0,0.00646822,"e passage and ask them to decide whether, given the first one or two utterances of context, the shown continuation is the actual follow-up utterance in the original dialogue or a random response. The data for this experiment consists of 900 pas[6] As loss function we then use the negative log likelihood of the correct labels: L=− 2.2 P d log pdj Human Evaluation [7] Training Details We trained the discriminator with a combination of three different datasets: MovieTriples, SubTle and Switchboard. MovieTriples (Serban et al., 2016) has been created from the Movie-Dic corpus of film transcripts (Banchs, 2012) and contains 3-utterance passages between two interlocutors who alternate in the conversation. SubTle 1 All remaining words are converted into the universal token <unk&gt;. 285 data SWB MOV SUB Acc .583 .677 .737 P .549 .645 .763 discriminator real random R F1 P R F1 .933 .691 .778 .233 .359 .787 .709 .726 .567 .637 .687 .723 .715 .787 .749 Acc .670 .677 .640 P .650 .664 .635 real R .714 .713 .660 humans F1 .690 .688 .647 random P R .695 .604 .690 .640 .646 .620 F1 .647 .664 .633 agreement Fleiss’ π hum disc .299 .068 .303 .258 .304 .301 Table 1: Accuracy, Precision, Recall, and F-score of discr"
W17-5534,N15-1020,0,0.00507482,"ator in a significant amount of cases. We investigate the potential of adversarial evaluation methods for open-domain dialogue generation systems, comparing the performance of a discriminative agent to that of humans on the same task. Our results show that the task is hard, both for automated models and humans, but that a discriminative agent can learn patterns that lead to above-chance performance. 1 Introduction End-to-end dialogue response generation systems trained to produce a plausible utterance given some limited dialogue context are receiving increased attention (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016). However, for systems dealing with chatbot-style open-dialogue, where task completion is not applicable, evaluating the quality of their responses remains a challenge. Most current models are evaluated with measures such as perplexity and overlapbased metrics like BLEU, that compare the generated response to the ground-truth response in an actual dialogue. This kind of measures, however, correlate very weakly or not at all with human judgements on response quality (Liu et al., 2016). In this paper, we explore a different approach to evaluating open-domai"
W17-5534,N16-1174,0,0.00640312,"e in modelling long sequences by considering different word locations in the dialogue in a relatively even manner: uit = tanh(Ww hit + bw ) αit = exp(uT it uw ) P T u ), exp(u t it w P i α i hi [4] [5] We first compute the hidden representation of hit through a one-layer MLP uit ; we then weight the importance of uit by computing its similarity to a word-level context vector, normalized via a softmax function. The context vector is learned endto-end by the classifier and is meant to represent a general query about the level of “discriminability” of a word (see, e.g., Sukhbaatar et al. 2015 or Yang et al. 2016). The output of the attention is then fed to a sigmoid function, which returns the probability of the input being real or fake: p = sigmoid(Wcv + bc ) 3 To assess the performance of our discriminative model, we conduct an experiment with human annotators. To our knowledge, this is the first study of its kind ever conducted. Previous human evaluation experiments of dialogue generation systems have mostly consisted in asking participants to choose the better response between two options generated by different models or to rate a generated dialogue along several dimensions (Vinyals and Le, 2015;"
W17-5534,D14-1218,0,0.00460561,"drawing a parallel with generative adversarial learning (Goodfellow et al., 2014). Here we concentrate on exploring the potential and the limits of such an adversarial eval2 The Discriminative Agent Our discriminative agent is a binary classifier which takes as input a sequence of dialogue utterances and predicts whether the dialogue is real or fake. The agent treats as positive examples of coherent dialogue actual dialogue passages and as negative examples passages where the last utterance has been randomly replaced. Random replacement has been used in the past to study discourse coherence (Li and Hovy, 2014). 2.1 Model The classifier is modelled as an attention-based bidirectional LSTM. LSTMs are indeed very effective to model word sequences, and are especially suited for learning on data with long distance dependencies (Hochreiter and Schmidhuber, 1997) such as multi-turn dialogues. The bidirectional LSTM includes both a forward function −−−−→ (LSTM, which reads the sentence si from wi1 ←−−−− to wiT ) and a backward function (LSTM, which reads the sentence si from wiT to wi1 ): xit = We wit , t ∈ [1, T ] 284 Proceedings of the SIGDIAL 2017 Conference, pages 284–288, c Saarbr¨ucken, Germany, 15-1"
W17-6804,P14-2134,0,0.211871,"es of practice (Wenger, 2000; Eckert and McConnell-Ginet, 1992), that is, aggregates of individuals not defined by a location or a population, but rather by social engagement in some common endeavour. Using computational modelling techniques and statistical analyses, we show that community-specific conventional meanings of common word forms (as opposed to jargon) do arise and can be reliably detected, which is consistent with the theoretical standpoint of Clark (1996), among others. The paper makes the following contributions: We adapt a model for geographically located language introduced by Bamman et al. (2014) to learn word representations for different online communities of practice. We introduce a framework for quantifying semantic variation and apply it to several Reddit sub-communities engaged in discussing two broad domains, Football and Programming. We evaluate our framework extrinsically against a language modelling task, showing that the semantic shifts we detect on common words are strong enough to affect performance. Our results show that distinct meaning conventions arise within communities engaged in discussing a shared domain, but also that the domain itself is not the only determinant"
W17-6804,D11-1120,0,0.0135825,"arising during interaction. In addition, our findings indicate that, besides frequency-related factors, the level of dissemination of a word among community members plays a key role in understanding the dynamics of meaning variants. 2 Related Work The present investigation is related to several strands of research in computational sociolinguistics and historical linguistics. Within the former, a substantial amount of work has used NLP techniques to study correlations between linguistic variables and macro-sociological categories such as age (Nguyen et al., 2013), gender (Nguyen et al., 2014; Burger et al., 2011; Ciot et al., 2013), and other demographic factors (Eisenstein et al., 2014). A related line of research has explored the interplay between language use and social relations among community members. For example, Cassell and Tversky (2005) and Huffaker et al. (2006) investigate the correlation between linguistic features and the strength of the relations among users in newborn communities; Danescu-Niculescu-Mizil et al. (2012) and Noble and Fern´andez (2015) show how variations in linguistic style can provide information about power differences in social groups. Yet other related work has focu"
W17-6804,D13-1114,0,0.0240571,"action. In addition, our findings indicate that, besides frequency-related factors, the level of dissemination of a word among community members plays a key role in understanding the dynamics of meaning variants. 2 Related Work The present investigation is related to several strands of research in computational sociolinguistics and historical linguistics. Within the former, a substantial amount of work has used NLP techniques to study correlations between linguistic variables and macro-sociological categories such as age (Nguyen et al., 2013), gender (Nguyen et al., 2014; Burger et al., 2011; Ciot et al., 2013), and other demographic factors (Eisenstein et al., 2014). A related line of research has explored the interplay between language use and social relations among community members. For example, Cassell and Tversky (2005) and Huffaker et al. (2006) investigate the correlation between linguistic features and the strength of the relations among users in newborn communities; Danescu-Niculescu-Mizil et al. (2012) and Noble and Fern´andez (2015) show how variations in linguistic style can provide information about power differences in social groups. Yet other related work has focused on how acceptanc"
W17-6804,E14-1011,0,0.0215734,"d by geographical location — including that of Bamman et al. (2014) — has often focused on dialectal varieties in the USA using data from Twitter (Eisenstein et al., 2010; programming 71,986 learn.prog 60,732 liverpool 28,988 red.devils 35,913 soccer 146,546 python 41,340 community programming python learn.prog soccer liverpool red.devils global years 10 8 7 8 8 6 – million tokens 21 18 21 65 55 66 50 Figure 1: Left: total number of members and their overlap in the Programming and Football supracommunities. Right: main statistics (time span and number of word token) in each community dataset. Doyle, 2014; Eisenstein et al., 2014). In contrast to this line of work, as pointed out in the introduction, we are interested in investigating semantic variation in communities of practice (Wenger, 2000; Eckert and McConnell-Ginet, 1992): communities defined by social engagement rather than geo-location or other demographic variables. 3 Experimental Setup Online communities offer an unprecedented opportunity to study linguistic variation and its dynamics. For our investigation of semantic variation, we collected data from Reddit, a large on-line community which includes approximately 1 million sub-commu"
W17-6804,J93-1003,0,0.1082,"ence. In contrast, we hypothesise that common words exhibiting semantic variation as a result of community conventions — which are our focus here — are likely to not be singled out by very high prominence values. Nevertheless, their level of prominence may still be a determiner of variation. Specificity. Besides frequency-related aspects, we also want to capture the extent to which a given word w appears in a restricted set of contexts. We approximate this by computing the collocational score of every bigram containing w and then scoring them using log-likelihood ratio as association measure (Dunning, 1993; Manning and Sch¨utze, 1999).10 We take the value of the highest ranked bigram as a proxy for the contextual specificity of w in community c (Spe(w, c)) or domain D (Spe(w, D)). The feature values are normalised to obtain scores in the range [0, 1]. Dissemination. Finally, we consider the range of individuals using a given word. A priori, words with the same frequency, prominence, or contextual specificity may differ in their level of social dissemination, i.e., in the proportion of community members using them. We compute a word’s dissemination within a community c as follows: Dis(w, c) = (U"
W17-6804,D10-1124,0,0.0798889,"Missing"
W17-6804,Q16-1003,0,0.0800018,"996). In the present study we concentrate on precisely this type of semantic variation. Our approach takes a synchronic perspective, i.e., we do not look into the temporal dynamics of meaning variation. Nevertheless, in terms of methodology, our work is related to computational historical linguistics. Diachronic meaning change has been studied at different time scales, from a few decades to several centuries. A variety of techniques have been explored: Latent Semantic Analysis (Sagi et al., 2011; Jatowt and Duh, 2014), topic clustering (Wijaya and Yeniterzi, 2011) and dynamic topic modelling (Frermann and Lapata, 2016). More recently, word embeddings (Mikolov et al., 2013) have proved useful for investigating meaning change over time. The most common approach consists in creating independent vector representations for consecutive time spans and then using a transformation matrix to map vectors from one space to another one (Kulkarni et al., 2015; Zhang et al., 2015; Hamilton et al., 2016). Similarly to this strand of research, our work leverages the power of word embeddings, but exploits a different approach originally introduced by Bamman et al. (2014) to account for geographical variation. As we will expl"
W17-6804,P16-1141,0,0.359211,"w decades to several centuries. A variety of techniques have been explored: Latent Semantic Analysis (Sagi et al., 2011; Jatowt and Duh, 2014), topic clustering (Wijaya and Yeniterzi, 2011) and dynamic topic modelling (Frermann and Lapata, 2016). More recently, word embeddings (Mikolov et al., 2013) have proved useful for investigating meaning change over time. The most common approach consists in creating independent vector representations for consecutive time spans and then using a transformation matrix to map vectors from one space to another one (Kulkarni et al., 2015; Zhang et al., 2015; Hamilton et al., 2016). Similarly to this strand of research, our work leverages the power of word embeddings, but exploits a different approach originally introduced by Bamman et al. (2014) to account for geographical variation. As we will explain in detail in Section 4, this approach is an extension of the skip-gram vector model (Mikolov et al., 2013) that allows us to learn meaning representations per community that build upon shared representations. Meaning variation determined by geographical location — including that of Bamman et al. (2014) — has often focused on dialectal varieties in the USA using data from"
W17-6804,P15-1073,0,0.0161541,"tion between the two approaches is relevant: while the former is based on the idea that, for a given word, a finite list of discrete senses is available, the latter builds on a more dynamic concept, namely that a new meaning can emerge in any interaction among speakers, who use it in order to make communication more effective. Understanding the intricate ways in which patterns of word use and communities of individuals are related is essential for characterising the interests and the expressive means of sub-cultures, as well as to develop NLP tools that are effective in the face of variation (Hovy, 2015; Yang and Eisenstein, 2017). In this paper, we study how word meaning (as captured by distributed vector representations) varies across and within different online communities. We take online communities, such as online discussion forums, to be excellent examples of communities of practice (Wenger, 2000; Eckert and McConnell-Ginet, 1992), that is, aggregates of individuals not defined by a location or a population, but rather by social engagement in some common endeavour. Using computational modelling techniques and statistical analyses, we show that community-specific conventional meanings o"
W17-6804,W06-3403,0,0.0477586,"vestigation is related to several strands of research in computational sociolinguistics and historical linguistics. Within the former, a substantial amount of work has used NLP techniques to study correlations between linguistic variables and macro-sociological categories such as age (Nguyen et al., 2013), gender (Nguyen et al., 2014; Burger et al., 2011; Ciot et al., 2013), and other demographic factors (Eisenstein et al., 2014). A related line of research has explored the interplay between language use and social relations among community members. For example, Cassell and Tversky (2005) and Huffaker et al. (2006) investigate the correlation between linguistic features and the strength of the relations among users in newborn communities; Danescu-Niculescu-Mizil et al. (2012) and Noble and Fern´andez (2015) show how variations in linguistic style can provide information about power differences in social groups. Yet other related work has focused on how acceptance into existing communities is mediated by the adoption of community norms (Nguyen and Ros´e, 2011; Tran and Ostendorf, 2016) and on how the process whereby linguistic innovations become norms can be leveraged to predict the permanence of a user"
W17-6804,W11-0710,0,0.115774,"Missing"
W17-6804,C14-1184,0,0.0346878,"Missing"
W17-6804,W15-1104,1,0.763362,"Missing"
W17-6804,D16-1108,0,0.0201482,"he interplay between language use and social relations among community members. For example, Cassell and Tversky (2005) and Huffaker et al. (2006) investigate the correlation between linguistic features and the strength of the relations among users in newborn communities; Danescu-Niculescu-Mizil et al. (2012) and Noble and Fern´andez (2015) show how variations in linguistic style can provide information about power differences in social groups. Yet other related work has focused on how acceptance into existing communities is mediated by the adoption of community norms (Nguyen and Ros´e, 2011; Tran and Ostendorf, 2016) and on how the process whereby linguistic innovations become norms can be leveraged to predict the permanence of a user in a community (Danescu-Niculescu-Mizil et al., 2013). Common to all approaches mentioned above is the exploitation of language features to implement predictive models for non-linguistic features (such as gender, power differences, or community permanence). Less attention has been payed to investigating linguistic variation in its own right. Those approaches that do address this aspect have concentrated almost exclusively on community-specific jargon and slang, i.e., neologi"
W17-6804,W10-4001,0,0.032073,"munities, others are community-specific, and therefore independent from the discussed topic. We propose such findings as evidence in favour of sociolinguistic theories of socially-driven semantic variation. Results are evaluated using an independent language modelling task. Furthermore, we investigate extralinguistic features and show that factors such as prominence and dissemination of words are related to semantic variation. 1 Introduction In computational linguistics and NLP, variation in word meaning has mostly been studied in the abstract, as lists of possible word senses (Navigli, 2009; Yarowsky, 2010). In contrast, other neighbouring fields such as sociolinguistics and psycholinguistics have emphasised the link between semantic variation and the activities and interactions of speakers. For example, the psychologist Herbert Clark appeals to the notion of ‘common ground’ to characterise patterns of word usage: “Word knowledge, properly viewed, divides into what I will call communal lexicons, by which I mean sets of word conventions in individual communities [...] When I meet Ann, she and I must establish as common ground which communities we both belong to simply in order to know what Englis"
W17-6804,P15-1063,0,0.0144883,"me scales, from a few decades to several centuries. A variety of techniques have been explored: Latent Semantic Analysis (Sagi et al., 2011; Jatowt and Duh, 2014), topic clustering (Wijaya and Yeniterzi, 2011) and dynamic topic modelling (Frermann and Lapata, 2016). More recently, word embeddings (Mikolov et al., 2013) have proved useful for investigating meaning change over time. The most common approach consists in creating independent vector representations for consecutive time spans and then using a transformation matrix to map vectors from one space to another one (Kulkarni et al., 2015; Zhang et al., 2015; Hamilton et al., 2016). Similarly to this strand of research, our work leverages the power of word embeddings, but exploits a different approach originally introduced by Bamman et al. (2014) to account for geographical variation. As we will explain in detail in Section 4, this approach is an extension of the skip-gram vector model (Mikolov et al., 2013) that allows us to learn meaning representations per community that build upon shared representations. Meaning variation determined by geographical location — including that of Bamman et al. (2014) — has often focused on dialectal varieties in"
W18-5419,J99-4003,0,0.113879,"in dialogue interaction, such as repetitions and self-corrections (e.g., I’d like to make a reservation for six, I mean, for eight people). Disfluencies have been investigated extensively in psycholinguistics, with a range of studies showing that they affect sentence processing in intricate ways (Levelt, 1983; Fox Tree, 1995; Bailey and Ferreira, 2003; Ferreira and Bailey, 2004; Lau and Ferreira, 2005; Brennan and Schober, 2001). Most computational work on disfluencies, however, has focused on detection rather than on disfluency processing and interpretation (e.g., Stolcke and Shriberg, 1996; Heeman and Allen, 1999; Zwarts et al., 2010; Qian and Liu, 2013; ?; ?). In contrast, our aim is to get a better understanding of how RNNs process disfluent utterances and to analyse the impact of such disfluencies on a downstream task—in this case, issuing an API request reflecting the preferences of the user in a task-oriented dialogue. We investigate how encoder-decoder models trained on a synthetic dataset of task-oriented dialogues process disfluencies, such as hesitations and self-corrections. We find that, contrary to earlier results, disfluencies have very little impact on the task success of seq-to-seq mode"
W18-5419,P14-1062,0,0.0146763,"t are incrementally built by the model, and discover that models develop little to no awareness of the structure of disfluencies. However, adding disfluencies to the data appears to help the model create clearer representations overall, as evidenced by the attention patterns the different models exhibit. 1 Introduction The use of Recurrent Neural Networks (RNNs) to tackle sequential language tasks has become standard in natural language processing, after impressive accomplishments in speech recognition, machine translation, and entailment (e.g., Sutskever et al., 2014; Bahdanau et al., 2015b; Kalchbrenner et al., 2014). Recently, RNNs have also been exploited as tools to model dialogue systems. Inspired by neural machine translation, researchers such as Ritter et al. (2011) and Vinyals and Le (2015) pioneered an approach to opendomain chit-chat conversation based on sequenceto-sequence models (Sutskever et al., 2014). In this paper, we focus on task-oriented dialogue, where the conversation serves to fulfil an independent goal in a given domain. Current neural dialogue models for task-oriented dialogue tend to equip systems with external memory components (Bordes et al., 2017), since key information needs t"
W18-5419,N13-1102,0,0.0286027,"and self-corrections (e.g., I’d like to make a reservation for six, I mean, for eight people). Disfluencies have been investigated extensively in psycholinguistics, with a range of studies showing that they affect sentence processing in intricate ways (Levelt, 1983; Fox Tree, 1995; Bailey and Ferreira, 2003; Ferreira and Bailey, 2004; Lau and Ferreira, 2005; Brennan and Schober, 2001). Most computational work on disfluencies, however, has focused on detection rather than on disfluency processing and interpretation (e.g., Stolcke and Shriberg, 1996; Heeman and Allen, 1999; Zwarts et al., 2010; Qian and Liu, 2013; ?; ?). In contrast, our aim is to get a better understanding of how RNNs process disfluent utterances and to analyse the impact of such disfluencies on a downstream task—in this case, issuing an API request reflecting the preferences of the user in a task-oriented dialogue. We investigate how encoder-decoder models trained on a synthetic dataset of task-oriented dialogues process disfluencies, such as hesitations and self-corrections. We find that, contrary to earlier results, disfluencies have very little impact on the task success of seq-to-seq models with attention. Using visualisations a"
W18-5419,D11-1054,0,0.0433637,"Missing"
W18-5419,D17-1236,0,0.0671481,"Missing"
W18-5419,C10-1154,0,0.062911,"Missing"
W18-6524,W17-4912,0,0.0214532,"o Do you love me? by the Personality model for personality types of given characters and extreme types not seen during training 192 6 Related Work and Conclusion Our study shows that the models under investigation produce output that retains some stylistic features related to personality, and can learn surface patterns that generalise beyond the training data. In recent years, there has been a surge of work on modelling different stylistic aspects, such as politeness and formality, in Natural Language Generation with deep learning methods (among others, Sennrich et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Niu and Bansal, 2018). Regarding generation in dialogue systems, besides the two response generation models we have tested, other recent approaches to open-domain dialogue have considered stylistic aspects. For example, Yang et al. (2017) leverage metadata about speakers’ personal information, such as age and gender, to condition generation using domain adaptation methods; while Luan et al. (2017) use multi-task learning to incorporate an autoencoder that learns the speaker’s language style from nonconversational data such as blog posts. The output of these models could also be assessed for"
W18-6524,W17-3541,0,0.078658,"development of data-driven conversational agents for informal open-domain dialogue (see Serban et al., 2016a, for a review). These chatbot systems model conversation as a sequence-tosequence (S EQ 2S EQ) problem (Sutskever et al., 2014) and rely on large amounts of unannotated dialogue data for training. We investigate whether such models are able to generate responses that reflect different personality traits. We test two kinds of models: The speaker-based model by Li et al. (2016b), where response generation is conditioned on the individual speaker, and a personality-based model similar to Herzig et al. (2017), where generation is conditioned on a personality type. Evaluating the output of chatbot systems is remarkably difficult (Liu et al., 2016). To make progress in this direction with regards to personality aspects, we propose a new statistical evaluation method that leverages an existing personality recogniser (Mairesse et al., 2007), thus avoiding the need for specialised corpora or manual annotations. We adopt the Big Five psychological model of personality (Norman, 1963), also called OCEAN for the initials of the five personality traits considered: Openness, Conscientiousness, Extraversion,"
W18-6524,N16-1014,0,0.301415,"predicted as: Q (1) p(Y |X) = nt=1 p(yt |y1 , . . . , yt−1 , X) Introduction The advent of deep learning methods has led to the development of data-driven conversational agents for informal open-domain dialogue (see Serban et al., 2016a, for a review). These chatbot systems model conversation as a sequence-tosequence (S EQ 2S EQ) problem (Sutskever et al., 2014) and rely on large amounts of unannotated dialogue data for training. We investigate whether such models are able to generate responses that reflect different personality traits. We test two kinds of models: The speaker-based model by Li et al. (2016b), where response generation is conditioned on the individual speaker, and a personality-based model similar to Herzig et al. (2017), where generation is conditioned on a personality type. Evaluating the output of chatbot systems is remarkably difficult (Liu et al., 2016). To make progress in this direction with regards to personality aspects, we propose a new statistical evaluation method that leverages an existing personality recogniser (Mairesse et al., 2007), thus avoiding the need for specialised corpora or manual annotations. We adopt the Big Five psychological model of personality (Nor"
W18-6524,P16-1094,0,0.435552,"predicted as: Q (1) p(Y |X) = nt=1 p(yt |y1 , . . . , yt−1 , X) Introduction The advent of deep learning methods has led to the development of data-driven conversational agents for informal open-domain dialogue (see Serban et al., 2016a, for a review). These chatbot systems model conversation as a sequence-tosequence (S EQ 2S EQ) problem (Sutskever et al., 2014) and rely on large amounts of unannotated dialogue data for training. We investigate whether such models are able to generate responses that reflect different personality traits. We test two kinds of models: The speaker-based model by Li et al. (2016b), where response generation is conditioned on the individual speaker, and a personality-based model similar to Herzig et al. (2017), where generation is conditioned on a personality type. Evaluating the output of chatbot systems is remarkably difficult (Liu et al., 2016). To make progress in this direction with regards to personality aspects, we propose a new statistical evaluation method that leverages an existing personality recogniser (Mairesse et al., 2007), thus avoiding the need for specialised corpora or manual annotations. We adopt the Big Five psychological model of personality (Nor"
W18-6524,D16-1230,0,0.215032,"Missing"
W18-6524,N15-1020,0,0.0815286,"Missing"
W18-6524,I17-1061,0,0.0154012,"k on modelling different stylistic aspects, such as politeness and formality, in Natural Language Generation with deep learning methods (among others, Sennrich et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Niu and Bansal, 2018). Regarding generation in dialogue systems, besides the two response generation models we have tested, other recent approaches to open-domain dialogue have considered stylistic aspects. For example, Yang et al. (2017) leverage metadata about speakers’ personal information, such as age and gender, to condition generation using domain adaptation methods; while Luan et al. (2017) use multi-task learning to incorporate an autoencoder that learns the speaker’s language style from nonconversational data such as blog posts. The output of these models could also be assessed for personality differences using our method. More recently, Oraby et al. (2018) have used the statistical rule-based generator PERSONAGE (Mairesse and Walker, 2010) to create a synthetic corpus with personality variation within the restaurant domain. They use the data to train and evaluate neural generation models that produce linguistic output given a dialogue act and a set of semantic slots, plus dif"
W18-6524,Q18-1027,0,0.0439239,"rsonality model for personality types of given characters and extreme types not seen during training 192 6 Related Work and Conclusion Our study shows that the models under investigation produce output that retains some stylistic features related to personality, and can learn surface patterns that generalise beyond the training data. In recent years, there has been a surge of work on modelling different stylistic aspects, such as politeness and formality, in Natural Language Generation with deep learning methods (among others, Sennrich et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Niu and Bansal, 2018). Regarding generation in dialogue systems, besides the two response generation models we have tested, other recent approaches to open-domain dialogue have considered stylistic aspects. For example, Yang et al. (2017) leverage metadata about speakers’ personal information, such as age and gender, to condition generation using domain adaptation methods; while Luan et al. (2017) use multi-task learning to incorporate an autoencoder that learns the speaker’s language style from nonconversational data such as blog posts. The output of these models could also be assessed for personality differences"
W18-6524,W18-5019,0,0.0231867,"systems, besides the two response generation models we have tested, other recent approaches to open-domain dialogue have considered stylistic aspects. For example, Yang et al. (2017) leverage metadata about speakers’ personal information, such as age and gender, to condition generation using domain adaptation methods; while Luan et al. (2017) use multi-task learning to incorporate an autoencoder that learns the speaker’s language style from nonconversational data such as blog posts. The output of these models could also be assessed for personality differences using our method. More recently, Oraby et al. (2018) have used the statistical rule-based generator PERSONAGE (Mairesse and Walker, 2010) to create a synthetic corpus with personality variation within the restaurant domain. They use the data to train and evaluate neural generation models that produce linguistic output given a dialogue act and a set of semantic slots, plus different degrees of personality information, and show that the generated output correlates reasonably well with the synthetic data generated by PERSONAGE. Our work differs from Oraby et al. (2018) in several respects: We focus on open-domain chit-chat dialogue, where the inpu"
W18-6524,P18-1205,0,0.112547,"Missing"
W18-6524,N16-1005,0,0.0196407,"I, I love you too. Table 3: Responses to Do you love me? by the Personality model for personality types of given characters and extreme types not seen during training 192 6 Related Work and Conclusion Our study shows that the models under investigation produce output that retains some stylistic features related to personality, and can learn surface patterns that generalise beyond the training data. In recent years, there has been a surge of work on modelling different stylistic aspects, such as politeness and formality, in Natural Language Generation with deep learning methods (among others, Sennrich et al., 2016; Hu et al., 2017; Ficler and Goldberg, 2017; Niu and Bansal, 2018). Regarding generation in dialogue systems, besides the two response generation models we have tested, other recent approaches to open-domain dialogue have considered stylistic aspects. For example, Yang et al. (2017) leverage metadata about speakers’ personal information, such as age and gender, to condition generation using domain adaptation methods; while Luan et al. (2017) use multi-task learning to incorporate an autoencoder that learns the speaker’s language style from nonconversational data such as blog posts. The output"
W19-0418,K17-1037,0,0.0232022,"reach their best performance on the FOIL task. We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer"
W19-0418,W18-6402,0,0.061223,"Missing"
W19-0418,D18-1119,0,0.0317612,"sis of the multimodal semantic spaces learned by the encoders In this section, we analyse the encoders by comparing the similarity of the multimodal spaces they learn and by comparing the learned multimodal spaces to the visual and linguistic representations they receive as input in terms on nearest neighbours. Representation similarity analysis Representation Similarity Analysis (RSA) is a technique from neuroscience (Kriegeskorte et al., 2008) that has been recently leveraged in computational linguistics, for example to compare the semantic spaces learned by artificial communicating agents (Bouchacourt and Baroni, 2018). It compares different semantic spaces by comparing their internal similarity relations, given a common set N of input data points. Each input k ∈ N is processed by an encoder for a given task T i, producing vector hkT i . Let HTNi be the set of vector representations created by the encoder of T i for all the items in N ; and let HTNj be the corresponding set of representations by the encoder of task T j. These two semantic spaces, HTNi and HTNj , are not directly comparable as they have been produced independently. RSA remedies this by instead comparing their structure in terms of internal s"
W19-0418,D18-2029,0,0.0186085,"n to a task-specific component: an MLP in the case of the pre-training retrieval tasks and a fully connected layer in the case of the FOIL classification task. common initial base across models and diminishes the effects of using different datasets for each specific task (the datasets are described in Section 5). Visual and language embeddings To represent visual data, we use ResNet152 features (He et al., 2016), which yield state of the art performance in image classification tasks and can be computed efficiently. To represent linguistic data, we use Universal Sentence Encoder (USE) vectors (Cer et al., 2018) since they yield near state-of-the-art results on several NLP tasks and are suitable both for short texts (such as the descriptions in ReferIt) and longer ones (such as the dialogues in GuessWhat).2 In order to gain some insight into the semantic spaces that emerge from these visual and linguistic representations, we consider a sample of 5K datapoints sharing the images across the three tasks and use average cosine similarity as a measure of space density. We find that the semantic space of the input images is denser (0.57 average cosine similarity) than the semantic space of the linguistic i"
W19-0418,P18-2074,0,0.0283991,"when the same set of inputs is given. We give as input 5,000 data points from the FOIL test set, randomly sampled from only the ones with original captions and containing unique images, and compare the representations produced by the encoders under investigation. Figure 4 shows that the semantic space produced by the encoder fully trained on FOIL is rather different from all the other models, and that the VQA semantic space is very similar to the one produced by the randomly initialized encoder. Nearest neighbour overlap We analyse the encoder representations using nearest neighbour overlap. Collell and Moens (2018) proposed this measure to compare the structure of functions that map concepts from an input to a target space. It is defined as the number of k nearest neighbours that two paired vectors share in their respective semantic space. For instance, if k = 3 and the 3 nearest neighbours of the vector for ‘cat’ vcat in space V are {vdog , vtiger , vlion }, and those of the vector of ‘cat’ zcat in space Z are {vmouse , vtiger , vlion }, the nearest neighbour overlap (NN) is 2. The value is then normalized with respect to the number of data points and the number of k nearest neighbours. k=1 ResNet152 U"
W19-0418,D17-1070,0,0.0291437,", the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et al. (2014)) as well as in computational linguistics (e.g., Conneau et al. (2017)). However, little has been done in this respect for visually grounded natural language processing models. In this work, we combine these different research lines and explore transfer learning techniques in the domain of language and vision tasks. In particular, we use the FOIL diagnostic dataset (Shekhar et al., 2017) and investigate to what extent skills learned through different multimodal tasks transfer. While transfering the knowledge learned by a pre-trained model can be useful in principle, Conneau et al. (2018) found that randomly initialized models provide strong baselines that can ev"
W19-0418,P18-1198,0,0.0227038,"We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et al. (2014)) as well as"
W19-0418,W18-1401,0,0.0668427,"Missing"
W19-0418,D14-1086,0,0.101691,"assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities. 1 Introduction In recent years, a lot of progress has been made within the emerging field at the intersection of computational linguistics and computer vision thanks to the use of deep neural networks. The most common strategy to move the field forward has been to propose different multimodal tasks—such as visual question answering (Antol et al., 2015), visual question generation (Mostafazadeh et al., 2016), visual reference resolution (Kazemzadeh et al., 2014), and visual dialogue (Das et al., 2017)—and to develop task-specific models. The benchmarks developed so far have put forward complex and distinct neural architectures, but in general they all share a common backbone consisting of an encoder which learns to merge the two types of representation to perform a certain task. This resembles the bottom-up processing in the ‘Hub and Spoke’ model proposed in Cognitive Science to represent how the brain processes and combines multi-sensory inputs (Patterson and Ralph, 2015). In this model, a ‘hub’ module merges the input processed by the sensor-specif"
W19-0418,Q16-1037,0,0.0313427,"ta size they need to reach their best performance on the FOIL task. We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved"
W19-0418,N18-2069,0,0.0225537,"tion sets and compare the results to chance performance. Given the number of candidate answers and objects per task in our datasets, chance P@1 is 0.055 for VQA and 0.05 for ReferIt and GuessWhat. Our task-specific models obtain P@1 values of 0.14 for VQA (mean rank 2.84), 0.12 for ReferIt (mean rank 3.32), and 0.08 for GuessWhat (mean rank 4.14). Not surprisingly given the challenging nature of these tasks, the results are not high. Nevertheless, the representations learned by the models allow them to perform above chance level and thus provide a reasonable basis for further investigation. 3 Madhysastha et al. (2018) found that an earlier version of the FOIL dataset was biased. We have used the latest version of the dataset available at https://foilunitn.github.io/, which does not have this problem. 6.1 Analysis via diagnostic task In this first analysis, we assess the quality of the multimodal representations learned by the three multimodal tasks considered in terms of their potential to perform the FOIL task, i.e., to spot semantic (in)congruence between an image and a caption. Besides comparing the models with respect to task accuracy, we also investigate how they learn to adapt to the FOIL task over t"
W19-0418,P16-1170,0,0.0253913,"anguage tasks on an existing diagnostic task designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities. 1 Introduction In recent years, a lot of progress has been made within the emerging field at the intersection of computational linguistics and computer vision thanks to the use of deep neural networks. The most common strategy to move the field forward has been to propose different multimodal tasks—such as visual question answering (Antol et al., 2015), visual question generation (Mostafazadeh et al., 2016), visual reference resolution (Kazemzadeh et al., 2014), and visual dialogue (Das et al., 2017)—and to develop task-specific models. The benchmarks developed so far have put forward complex and distinct neural architectures, but in general they all share a common backbone consisting of an encoder which learns to merge the two types of representation to perform a certain task. This resembles the bottom-up processing in the ‘Hub and Spoke’ model proposed in Cognitive Science to represent how the brain processes and combines multi-sensory inputs (Patterson and Ralph, 2015). In this model, a ‘hub’"
W19-0418,P17-1024,1,0.822556,"ficial multimodal systems, with the goal of assessing its ability to compute multimodal representations that are useful beyond specific tasks. While current visually grounded models perform remarkably well on the task they have been trained for, it is unclear whether they are able to learn representations that truly merge the two modalities and whether the skill they have acquired is stable enough to be transferred to other tasks. In this paper, we investigate these questions in detail. To do so, we evaluate an encoder trained on different multimodal tasks on an existing diagnostic task—FOIL (Shekhar et al., 2017)—designed to assess multimodal semantic understanding and carry out an in-depth analysis to study how the encoder merges and exploits the two modalities. We also exploit two techniques to investigate the structure of the learned semantic spaces: Representation Similarity Analysis (RSA) (Kriegeskorte et al., 2008) and Nearest Neighbour overlap (NN). We use RSA to compare the outcome of the various encoders given the same vision-and-language input and NN to compare the multimodal space produced by an encoder with the ones built with the input visual and language embeddings, respectively, which a"
W19-0418,P17-2034,0,0.0302679,"ers give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et al. (2014)) as well as in computational linguistics (e.g., Conneau et al. (2017)). However, little has been done in this respect for visually grounded natural language processing models. In this work, we combine these different research lines and explore transfer lear"
W19-0418,W18-5448,0,0.131622,"mance on the FOIL task. We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et"
