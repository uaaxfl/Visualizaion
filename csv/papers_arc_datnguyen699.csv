2021.naacl-main.173,{COVID}-19 Named Entity Recognition for {V}ietnamese,2021,-1,-1,3,0,3794,thinh truong,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manually-annotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity recognition (NER) task with newly-defined entity types that can be used in other future epidemics. Our dataset also contains the largest number of entities compared to existing Vietnamese NER datasets. We empirically conduct experiments using strong baselines on our dataset, and find that: automatic Vietnamese word segmentation helps improve the NER results and the highest performances are obtained by fine-tuning pre-trained language models where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) produces higher results than the multilingual model XLM-R (Conneau et al., 2020). We publicly release our dataset at: https://github.com/VinAIResearch/PhoNER{\_}COVID19"
2021.naacl-demos.1,"{P}ho{NLP}: A joint multi-task learning model for {V}ietnamese part-of-speech tagging, named entity recognition and dependency parsing",2021,-1,-1,2,0,4821,linh nguyen,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstrations,0,"We present the first multi-task learning model {--} named PhoNLP {--} for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP"
2021.emnlp-main.369,{P}ho{MT}: A High-Quality and Large-Scale Benchmark Dataset for {V}ietnamese-{E}nglish Machine Translation,2021,-1,-1,5,0,9465,long doan,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We introduce a high-quality and large-scale Vietnamese-English parallel dataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark Vietnamese-English machine translation corpus IWSLT15. We conduct experiments comparing strong neural baselines and well-known automatic translation engines on our dataset and find that in both automatic and human evaluations: the best performance is obtained by fine-tuning the pre-trained sequence-to-sequence denoising auto-encoder mBART. To our best knowledge, this is the first large-scale Vietnamese-English machine translation study. We hope our publicly available dataset and study can serve as a starting point for future research and applications on Vietnamese-English machine translation. We release our dataset at: https://github.com/VinAIResearch/PhoMT"
2020.wnut-1.41,{WNUT}-2020 Task 2: Identification of Informative {COVID}-19 {E}nglish Tweets,2020,-1,-1,1,1,3796,dat nguyen,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),0,"In this paper, we provide an overview of the WNUT-2020 shared task on the identification of informative COVID-19 English Tweets. We describe how we construct a corpus of 10K Tweets and organize the development and evaluation phases for this task. In addition, we also present a brief summary of results obtained from the final system evaluation submissions of 55 teams, finding that (i) many systems obtain very high performance, up to 0.91 F1 score, (ii) the majority of the submissions achieve substantially higher results than the baseline fastText (Joulin et al., 2017), and (iii) fine-tuning pre-trained language models on relevant language data followed by supervised training performs well in this task."
2020.textgraphs-1.1,A survey of embedding models of entities and relationships for knowledge graph completion,2020,-1,-1,1,1,3796,dat nguyen,Proceedings of the Graph-based Methods for Natural Language Processing (TextGraphs),0,"Knowledge graphs (KGs) of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge graphs are typically incomplete, it is useful to perform knowledge graph completion or link prediction, i.e. predict whether a relationship not in the knowledge graph is likely to be true. This paper serves as a comprehensive survey of embedding models of entities and relationships for knowledge graph completion, summarizing up-to-date experimental results on standard benchmark datasets and pointing out potential future research directions."
2020.findings-emnlp.92,{P}ho{BERT}: Pre-trained language models for {V}ietnamese,2020,35,0,1,1,3796,dat nguyen,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and Natural language inference. We release PhoBERT to facilitate future research and downstream applications for Vietnamese NLP. Our PhoBERT models are available at https://github.com/VinAIResearch/PhoBERT"
2020.findings-emnlp.364,A Pilot Study of Text-to-{SQL} Semantic Parsing for {V}ietnamese,2020,-1,-1,3,0,7732,anh nguyen,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020)."
2020.emnlp-demos.2,{BERT}weet: A pre-trained language model for {E}nglish Tweets,2020,38,0,1,1,3796,dat nguyen,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet"
W19-5035,Improving Chemical Named Entity Recognition in Patents with Contextualized Word Embeddings,2019,36,4,2,1,23967,zenan zhai,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"Chemical patents are an important resource for chemical information. However, few chemical Named Entity Recognition (NER) systems have been evaluated on patent documents, due in part to their structural and linguistic complexity. In this paper, we explore the NER performance of a BiLSTM-CRF model utilising pre-trained word embeddings, character-level word representations and contextualized ELMo word representations for chemical patents. We compare word embeddings pre-trained on biomedical and chemical patent corpora. The effect of tokenizers optimized for the chemical domain on NER performance in chemical patents is also explored. The results on two patent corpora show that contextualized word representations generated from ELMo substantially improve chemical NER performance w.r.t. the current state-of-the-art. We also show that domain-specific resources such as word embeddings trained on chemical patents and chemical-specific tokenizers, have a positive impact on NER performance."
U19-1004,"A neural joint model for {V}ietnamese word segmentation, {POS} tagging and dependency parsing",2019,-1,-1,1,1,3796,dat nguyen,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,
U19-1014,Detecting Chemical Reactions in Patents,2019,0,1,2,0,10520,hiyori yoshikawa,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,"Extracting chemical reactions from patents is a crucial task for chemists working on chemical exploration. In this paper we introduce the novel task of detecting the textual spans that describe or refer to chemical reactions within patents. We formulate this task as a paragraph-level sequence tagging problem, where the system is required to return a sequence of paragraphs which contain a description of a reaction. To address this new task, we construct an annotated dataset from an existing proprietary database of chemical reactions manually extracted from patents. We introduce several baseline methods for the task and evaluate them over our dataset. Through error analysis, we discuss what makes the task complex and challenging, and suggest possible directions for future research."
N19-1226,A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization,2019,0,21,4,1,22809,dai nguyen,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"In this paper, we introduce an embedding model, named CapsE, exploring a capsule network to model relationship triples (subject, relation, object). Our CapsE represents each triple as a 3-column matrix where each column vector represents the embedding of an element in the triple. This 3-column matrix is then fed to a convolution layer where multiple filters are operated to generate different feature maps. These feature maps are reconstructed into corresponding capsules which are then routed to another capsule to produce a continuous vector. The length of this vector is used to measure the plausibility score of the triple. Our proposed CapsE obtains better performance than previous state-of-the-art embedding models for knowledge graph completion on two benchmark datasets WN18RR and FB15k-237, and outperforms strong search personalization baselines on SEARCH17."
W18-5605,Comparing {CNN} and {LSTM} character-level embeddings in {B}i{LSTM}-{CRF} models for chemical and disease named entity recognition,2018,0,4,2,1,23967,zenan zhai,Proceedings of the Ninth International Workshop on Health Text Mining and Information Analysis,0,"We compare the use of LSTM-based and CNN-based character-level word embeddings in BiLSTM-CRF models to approach chemical and disease named entity recognition (NER) tasks. Empirical results over the BioCreative V CDR corpus show that the use of either type of character-level word embeddings in conjunction with the BiLSTM-CRF models leads to comparable state-of-the-art performance. However, the models using CNN-based character-level word embeddings have a computational performance advantage, increasing training time over word-based models by 25{\%} while the LSTM-based character-level word embeddings more than double the required training time."
W18-2314,Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings,2018,45,5,1,1,3796,dat nguyen,Proceedings of the {B}io{NLP} 2018 workshop,0,"We investigate the incorporation of character-based word representations into a standard CNN-based relation extraction model. We experiment with two common neural architectures, CNN and LSTM, to learn word vector representations from character embeddings. Through a task on the BioCreative-V CDR corpus, extracting relationships between chemicals and diseases, we show that models exploiting the character-based word representations improve on models that do not use this information, obtaining state-of-the-art result relative to previous neural approaches."
S18-1085,{NIHRIO} at {S}em{E}val-2018 Task 3: A Simple and Accurate Neural Network Model for Irony Detection in {T}witter,2018,31,2,2,0.961538,13715,thanh vu,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our NIHRIO system for SemEval-2018 Task 3 {``}Irony detection in English tweets.{''} We propose to use a simple neural network architecture of Multilayer Perceptron with various types of input features including: lexical, syntactic, semantic and polarity features. Our system achieves very high performance in both subtasks of binary and multi-class irony detection in tweets. In particular, we rank at least fourth using the accuracy metric and sixth using the F1 metric. Our code is available at: \url{https://github.com/NIHRIO/IronyDetectionInTwitter}"
P18-1052,Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach,2018,19,2,3,0,3407,shafiq joty,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures."
N18-5012,{V}n{C}ore{NLP}: A {V}ietnamese Natural Language Processing Toolkit,2018,17,6,2,0.961538,13715,thanh vu,Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We present an easy-to-use and fast toolkit, namely VnCoreNLP{---}a Java NLP annotation pipeline for Vietnamese. Our VnCoreNLP supports key natural language processing (NLP) tasks including word segmentation, part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing, and obtains state-of-the-art (SOTA) results for these tasks. We release VnCoreNLP to provide rich linguistic annotations to facilitate research work on Vietnamese NLP. Our VnCoreNLP is open-source and available at: \url{https://github.com/vncorenlp/VnCoreNLP}"
N18-2053,A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network,2018,0,43,3,1,22809,dai nguyen,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"In this paper, we propose a novel embedding model, named ConvKB, for knowledge base completion. Our model ConvKB advances state-of-the-art models by employing a convolutional neural network, so that it can capture global relationships and transitional characteristics between entities and relations in knowledge bases. In ConvKB, each triple (head entity, relation, tail entity) is represented as a 3-column matrix where each column vector represents a triple element. This 3-column matrix is then fed to a convolution layer where multiple filters are operated on the matrix to generate different feature maps. These feature maps are then concatenated into a single feature vector representing the input triple. The feature vector is multiplied with a weight vector via a dot product to return a score. This score is then used to predict whether the triple is valid or not. Experiments show that ConvKB achieves better link prediction performance than previous state-of-the-art embedding models on two benchmark datasets WN18RR and FB15k-237."
L18-1410,A Fast and Accurate {V}ietnamese Word Segmenter,2018,-1,-1,1,1,3796,dat nguyen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
K18-2008,An Improved Neural Network Model for Joint {POS} Tagging and Dependency Parsing,2018,0,10,1,1,3796,dat nguyen,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We propose a novel neural network model for joint part-of-speech (POS) tagging and dependency parsing. Our model extends the well-known BIST graph-based dependency parser (Kiperwasser and Goldberg, 2016) by incorporating a BiLSTM-based tagging component to produce automatically predicted POS tags for the parser. On the benchmark English Penn treebank, our model obtains strong UAS and LAS scores at 94.51{\%} and 92.87{\%}, respectively, producing 1.5+{\%} absolute improvements to the BIST graph-based parser, and also obtaining a state-of-the-art POS tagging accuracy at 97.97{\%}. Furthermore, experimental results on parsing 61 {``}big{''} Universal Dependencies treebanks from raw texts show that our model outperforms the baseline UDPipe (Straka and Strakova, 2017) with 0.8{\%} higher average POS tagging score and 3.6{\%} higher average LAS score. In addition, with our model, we also obtain state-of-the-art downstream task scores for biomedical event extraction and opinion analysis applications. Our code is available together with all pre-trained models at: \url{https://github.com/datquocnguyen/jPTDP}"
U17-1013,From Word Segmentation to {POS} Tagging for {V}ietnamese,2017,19,4,1,1,3796,dat nguyen,Proceedings of the Australasian Language Technology Association Workshop 2017,0,"This paper presents an empirical comparison of two strategies for Vietnamese Part-of-Speech (POS) tagging from unsegmented text: (i) a pipeline strategy where we consider the output of a word segmenter as the input of a POS tagger, and (ii) a joint strategy where we predict a combined segmentation and POS tag for each syllable. We also make a comparison between state-of-the-art (SOTA) feature-based and neural network-based models. On the benchmark Vietnamese treebank (Nguyen et al., 2009), experimental results show that the pipeline strategy produces better scores of POS tagging from unsegmented text than the joint strategy, and the highest accuracy is obtained by using a feature-based model."
S17-1015,A Mixture Model for Learning Multi-Sense Word Embeddings,2017,40,10,2,1,22809,dai nguyen,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks."
P17-1121,A Neural Local Coherence Model,2017,21,17,1,1,3796,dat nguyen,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin."
K17-3014,A Novel Neural Network Model for Joint {POS} Tagging and Graph-based Dependency Parsing,2017,41,18,1,1,3796,dat nguyen,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural network-based Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art. Our code is open-source and available together with pre-trained models at: \url{https://github.com/datquocnguyen/jPTDP}"
I17-2007,Sequence to Sequence Learning for Event Prediction,2017,17,1,2,1,22809,dai nguyen,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper presents an approach to the task of predicting an event description from a preceding sentence in a text. Our approach explores sequence-to-sequence learning using a bidirectional multi-layer recurrent neural network. Our approach substantially outperforms previous work in terms of the BLEU score on two datasets derived from WikiHow and DeScript respectively. Since the BLEU score is not easy to interpret as a measure of event prediction, we complement our study with a second evaluation that exploits the rich linguistic annotation of gold paraphrase sets of events."
U16-1017,An empirical study for {V}ietnamese dependency parsing,2016,26,5,1,1,3796,dat nguyen,Proceedings of the Australasian Language Technology Association Workshop 2016,0,"This paper presents an empirical comparison of different dependency parsers for Vietnamese, which has some unusual characteristics such as copula drop and verb serialization. Experimental results show that the neural network-based parsers perform significantly better than the traditional parsers. We report the highest parsing scores published to date for Vietnamese with the labeled attachment score (LAS) at 73.53% and the unlabeled attachment score (UAS) at 80.66%."
Q16-1016,{J}-{NERD}: Joint Named Entity Recognition and Disambiguation with Rich Linguistic Features,2016,41,21,1,1,3796,dat nguyen,Transactions of the Association for Computational Linguistics,0,"Methods for Named Entity Recognition and Disambiguation (NERD) perform NER and NED in two separate stages. Therefore, NED may be penalized with respect to precision by NER false positives, and suffers in recall from NER false negatives. Conversely, NED does not fully exploit information computed by NER such as types of mentions. This paper presents J-NERD, a new approach to perform NER and NED jointly, by means of a probabilistic graphical model that captures mention spans, mention types, and the mapping of mentions to entities in a knowledge base. We present experiments with different kinds of texts from the CoNLL{'}03, ACE{'}05, and ClueWeb{'}09-FACC1 corpora. J-NERD consistently outperforms state-of-the-art competitors in end-to-end NERD precision, recall, and F1."
N16-1054,{ST}rans{E}: a novel embedding model of entities and relationships in knowledge bases,2016,56,80,1,1,3796,dat nguyen,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform link prediction, i.e., predict whether a relationship not in the knowledge base is likely to be true. This paper combines insights from several previous link prediction models into a new embedding model STransE that represents each entity as a lowdimensional vector, and each relation by two matrices and a translation vector. STransE is a simple combination of the SE and TransE models, but it obtains better link prediction performance on two benchmark datasets than previous embedding models. Thus, STransE can serve as a new baseline for the more complex models in the link prediction task."
K16-1005,Neighborhood Mixture Model for Knowledge Base Completion,2016,50,22,1,1,3796,dat nguyen,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,"Knowledge bases are useful resources for many natural language processing tasks, however, they are far from complete. In this paper, we define a novel entity representation as a mixture of its neighborhood in the knowledge base and apply this technique on TransE-a well-known embedding model for knowledge base completion. Experimental results show that the neighborhood information significantly helps to improve the results of the TransE model, leading to better performance than obtained by other state-of-the-art embedding models on three benchmark datasets for triple classification, entity prediction and relation prediction tasks."
W15-2813,Do Distributed Semantic Models Dream of Electric Sheep? Visualizing Word Representations through Image Synthesis,2015,29,1,2,0,20683,angeliki lazaridou,Proceedings of the Fourth Workshop on Vision and Language,0,"We introduce the task of visualizing distributed semantic representations by generating images from word vectors. Given the corpus-based vector encoding the word broccoli, we convert it to a visual representation by means of a cross-modal mapping function, and then use the mapped representation to generate an image of broccoli as xe2x80x9cdreamedxe2x80x9d by the distributed model. We propose a baseline dream synthesis method based on averaging pictures whose visual representations are topologically close to the mapped vector. Two experiments show that we generate dreams that generally belong to the the right semantic category, and are sometimes accurate enough for subjects to distinguish the intended concept from a related one."
U15-1014,Improving Topic Coherence with Latent Feature Word Representations in {MAP} Estimation for Topic Modeling,2015,39,1,1,1,3796,dat nguyen,Proceedings of the Australasian Language Technology Association Workshop 2015,0,None
Q15-1022,Improving Topic Models with Latent Feature Word Representations,2015,54,128,1,1,3796,dat nguyen,Transactions of the Association for Computational Linguistics,0,"Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature vector representations of words have been used to obtain high performance in many NLP tasks. In this paper, we extend two different Dirichlet multinomial topic models by incorporating latent feature vector representations of words trained on very large corpora to improve the word-topic mapping learnt on a smaller corpus. Experimental results show that by using information from the external corpora, our new models produce significant improvements on topic coherence, document clustering and document classification tasks, especially on datasets with few or short documents."
W14-5418,Coloring Objects: Adjective-Noun Visual Semantic Compositionality,2014,8,5,1,1,3796,dat nguyen,Proceedings of the Third Workshop on Vision and Language,0,"This paper reports preliminary experiments aiming at verifying the conjecture that semantic compositionality is a general process irrespective of the underlying modality. In particular, we model compositionality of an attribute with an object in the visual modality as done in the case of an adjective with a noun in the linguistic modality. Our experiments show that the concept topologies in the two modalities share similarities, results that strengthen our conjecture. 1 Language and Vision Recently, fields like computational linguistics and computer vision have converged to a common way of capturing and representing the linguistic and visual information of atomic concepts, through vector space models. At the same time, advances in computational semantics have lead to effective and linguistically inspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g. phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010). Compositionality is not to be considered only an important component from a linguistic perspective, but also from a cognitive perspective and there has been efforts to validate it as a general cognitive process. However, in computer vision so far compositionality has received limited attention. Thus, in this work, we study the phenomenon of visual compositionality and we complement limited previous literature that has focused on event compositionality (Stxc2xa8"
W14-2621,Sentiment Classification on Polarity Reviews: An Empirical Study Using Rating-based Features,2014,23,12,2,1,22809,dai nguyen,"Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"We present a new feature type named rating-based feature and evaluate the contribution of this feature to the task of document-level sentiment analysis. We achieve state-of-the-art results on two publicly available standard polarity movie datasets: on the dataset consisting of 2000 reviews produced by Pang and Lee (2004) we obtain an accuracy of 91.6% while it is 89.87% evaluated on the dataset of 50000 reviews created by Maas et al. (2011). We also get a performance at 93.24% on our own dataset consisting of 233600 movie reviews, and we aim to share this dataset for further research in sentiment polarity analysis task."
E14-2005,{RDRPOST}agger: A Ripple Down Rules-based Part-Of-Speech Tagger,2014,7,28,1,1,3796,dat nguyen,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper describes our robust, easyto-use and language independent toolkit namely RDRPOSTagger which employs an error-driven approach to automatically construct a Single Classification Ripple Down Rules tree of transformation rules for POS tagging task. During the demonstration session, we will run the tagger on data sets in 15 different languages."
I13-1114,A Two-Stage Classifier for Sentiment Analysis,2013,25,8,2,1,22809,dai nguyen,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we present a study applying reject option to build a two-stage sentiment polarity classification system. We construct a Naive Bayes classifier at the first stage and a Support Vector Machine at the second stage, in which documents rejected at the first stage are forwarded to be classified at the second stage. The obtained accuracies are comparable to other state-of-the-art results. Furthermore, experiments show that our classifier requires less training data while still maintaining reasonable classification accuracy."
R11-1056,Systematic Knowledge Acquisition for Question Analysis,2011,12,7,1,1,3796,dat nguyen,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"For the task of turning a natural language question into an explicit intermediate representation of the complexity in question answering systems, all published works so far use rulebased approach to the best of our knowledge. We believe it is because of the complexity of the representation and the variety of question types and also there are no publicly available corpus of a decent size. In these rule-based approaches, the process of creating rules is not discussed. It is clear that manually creating the rules in an ad-hoc manner is very expensive and error-prone. In this paper, we focus on the process of creating those rules manually, in a way that consistency between rules is maintained and the effort to create a new rule is independent of the size of the current rule set. Experimental results are promising where our system achieves better performance and requires much less time and cognitive load compared to previous work."
N07-2032,Subtree Mining for Relation Extraction from {W}ikipedia,2007,17,36,1,1,3796,dat nguyen,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"In this study, we address the problem of extracting relations between entities from Wikipedia's English articles. Our proposed method first anchors the appearance of entities in Wikipedia's articles using neither Named Entity Recognizer (NER) nor coreference resolution tool. It then classifies the relationships between entity pairs using SVM with features extracted from the web structure and subtrees mined from the syntactic structure of text. We evaluate our method on manually annotated data from actual Wikipedia articles."
