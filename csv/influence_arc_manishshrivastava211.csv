2019.icon-1.22,E14-2007,0,0.0230587,"Missing"
2019.icon-1.22,aziz-etal-2012-pet,0,0.0335632,"ranslation memories, glossaries, concordances and terminologies). 3 Related Work There are some translation workbenches with separate resource management systems such as: Smartcat 1 , Memsource 2 and Matecat (Federico et al., 2014) are web based CAT tools, provide central resources management for a project but they do not provide the provision for the linguistic resources. CASMACAT (Alabau et al., 2014) is a translation workbench which is web based and offers advanced functionality for computer-aided translation. It offers TMs but it also lacks the provision for the linguistic resources. PET (Aziz et al., 2012) is a tool to postedit to for evaluating In this paper, background and motivation for Kunji is described in Section-2 followed by related work in Section-3. The detailed architecture and functionality of Kunji is described in Section1 2 185 https://www.smartcat.ai https://www.memsource.com the quality of translations. It evaluates the efforts required in translations in order to be fixed. SDL Trados, MemoQ3 and Anubis (Jaworski, 2013) are CAT tools which are not web based and both lack the provision to use linguistic resources. Brat (Stenetorp et al., 2012) is basically a web based annotation"
2019.icon-1.22,C14-2028,0,0.0713302,"Missing"
2019.icon-1.22,I08-2141,0,0.239345,"ssaries to translators but there is a lack of the resource management systems which provide the provision by which a translator can use additional linguistic resources like NE, MWE and lexeme dictionaries. Additionally there is a lack of the provisions of reuse, management and verification of these resources which can lead a process to improvement and development of the MT systems. There are several tools for resource development but they are built with separate purposes and languages as they are for task specific. Like for annotators they have separate tools in Indian languages like Sanchay (Singh, 2008) but it is desktop based application. Some tools like GATE (Cunningham, 2002) addresses the some of the problems faced by NLP researchers while developing NLP resources but it is desktop based application. It motivated us to develop Kunji, a resource management tool, which facilitates the translators in translator workbenches as well as MT system development process. It tries to address the issues faced by both translators as well as MT module developers. It allows the translators to use and manage the linguistic resources along with their terminologies and glossaries. Additionally we performe"
2019.icon-1.5,W06-0901,0,0.0738313,"k Neural approaches to sequence tagging are common due to extensive developments in named entity recognition. Huang et al. (2015) introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF. Ma and Hovy (2016)’s architecture and the NeuroNER program (Dernoncourt et al., 2017) provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event detection and extraction (Araki, 2018). The task of event extraction in any language involves the identification of the event nugget (Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures. Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information (Nguyen and Grishman, 2015), modeling the skipgram architecture to learn lexical feature representations (Chen et al"
2019.icon-1.5,W09-3007,0,0.041101,"see that an increase in the F1-scores. We attribute this to a combination of factors, including well defined verbal affixes which are attributed to events, and effective weighted combination of character and word embeddings. For Italian, we train and test solely on the ItaTimeBank, whereas the current state of the art system trained on an augmented Ita-TimeBank (Caselli et al., 2011b), which was enriched with more labeled data. Similarly, in French, we use the established French TimeBank, while experiments in French so far have been on self-annotated (Arnulphy et al., 2015) or TimeML corpora (Bittar, 2009). Since these repositories of augmented data were not available to us at the time of writing this paper, the values reflect the same. However, it is to be noted that our system does provide an accuracy that is close to the currently reported stateof-the-art even in the absence of language specific features, explaining the fact that sub-word information is necessary for event detection in Italian and French as well. For Hindi, our architecture provides a good baseline. However, the training data consists of far too many words that are out of vocabulary, which is a major issue in working with wo"
2019.icon-1.5,Q17-1010,0,0.0219292,"= t0 ); t, t0 ∈ L i=1 (12) 39 Therefore the probability of that sequence Y computed above is calculated as: exp (Seq(Y, h) 0 y 0 ∈L exp (Seq(y , h)) p(Y |h) = P 4 I represents all the other tokens of an event and O represents the tokens which are not a part of any event in the sentence. We train the model for 50 epochs, but the loss tends to stabilize at 25 to 35 epochs. We use a 40 dimensional character embedding, which we create ourselves, as mentioned in section 3.1. The CNN uses 40 filters with a window size of 3. For our contextual word embeddings, we use fastText embeddings for English (Bojanowski et al., 2017) which are pretrained on commonCrawl and the Wikipedia corpus. FastText embeddings are also used for Hindi, French, Spanish and Italian word representations (Grave et al., 2018). The bi-LSTM trains on a fixed 300 hidden dimensions for all the bi-LSTMs in the architecture. For the linear and dropout layers, the dropout is fixed to 0.3. The initial learning rate parameter is 0.015, which increases with a momentum of 0.9. On approaching the end of an epoch, the learning rate decays at a rate of 0.05. We train on a negative log-likelihood loss function (13) Experimental Setup In this section, we g"
2019.icon-1.5,W11-0418,0,0.294778,"ection Suhan Prabhu, Pranav Goel, Alok Debnath and Manish Shrivastava International Institute of Information Technology Hyderabad, Telangana, India {suhan.prabhuk, pranav.goel, alok.debnath }@research.iiit.ac.in m.shrivastava@iiit.ac.in Abstract been met with equally fast paced evolution of automatic event annotation and detection methodologies in the last few years (Doddington et al., 2004; Pustejovsky et al., 2010; O’Gorman et al., 2016). On a larger scale, event extraction has extended to many languages beyond English, including French (Bittar et al., 2011), Spanish (Saurı, 2010), Italian (Caselli et al., 2011a) and very recently, Hindi (Goud et al., 2019b). Event detection architectures have their origins in statistical models such as K-means and hierarchical clustering methods (Arnulphy et al., 2015), which have more recently given way to neural models. Deep neural architectures on event annotation vary based on the approach taken to identifying and handling the data. However, event detection as a problem shifts when we move away from the annotation paradigm of datasets such as ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2015) to TimeML datasets such as TimeBank (Pustejovsky et al"
2019.icon-1.5,L18-1550,0,0.0224094,"e other tokens of an event and O represents the tokens which are not a part of any event in the sentence. We train the model for 50 epochs, but the loss tends to stabilize at 25 to 35 epochs. We use a 40 dimensional character embedding, which we create ourselves, as mentioned in section 3.1. The CNN uses 40 filters with a window size of 3. For our contextual word embeddings, we use fastText embeddings for English (Bojanowski et al., 2017) which are pretrained on commonCrawl and the Wikipedia corpus. FastText embeddings are also used for Hindi, French, Spanish and Italian word representations (Grave et al., 2018). The bi-LSTM trains on a fixed 300 hidden dimensions for all the bi-LSTMs in the architecture. For the linear and dropout layers, the dropout is fixed to 0.3. The initial learning rate parameter is 0.015, which increases with a momentum of 0.9. On approaching the end of an epoch, the learning rate decays at a rate of 0.05. We train on a negative log-likelihood loss function (13) Experimental Setup In this section, we go over the various experiments, implementation details such as number of epochs, training time, datasets and the like. These are covered in detail for the replicability of our r"
2019.icon-1.5,R11-1074,0,0.381493,"ection Suhan Prabhu, Pranav Goel, Alok Debnath and Manish Shrivastava International Institute of Information Technology Hyderabad, Telangana, India {suhan.prabhuk, pranav.goel, alok.debnath }@research.iiit.ac.in m.shrivastava@iiit.ac.in Abstract been met with equally fast paced evolution of automatic event annotation and detection methodologies in the last few years (Doddington et al., 2004; Pustejovsky et al., 2010; O’Gorman et al., 2016). On a larger scale, event extraction has extended to many languages beyond English, including French (Bittar et al., 2011), Spanish (Saurı, 2010), Italian (Caselli et al., 2011a) and very recently, Hindi (Goud et al., 2019b). Event detection architectures have their origins in statistical models such as K-means and hierarchical clustering methods (Arnulphy et al., 2015), which have more recently given way to neural models. Deep neural architectures on event annotation vary based on the approach taken to identifying and handling the data. However, event detection as a problem shifts when we move away from the annotation paradigm of datasets such as ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2015) to TimeML datasets such as TimeBank (Pustejovsky et al"
2019.icon-1.5,P15-1017,0,0.0241,"Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures. Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information (Nguyen and Grishman, 2015), modeling the skipgram architecture to learn lexical feature representations (Chen et al., 2015) as well as using dynamic CNNs in order to extract lexical and syntactic features in parallel (Nguyen and Grishman, 2016). Recurrent neural architectures have also been employed for this task, which predict the location of the trigger based on combining the for1. Syntactic and lexical information captured by previous event detection tasks should be accounted for. 2. Furthermore, sub-word information is essential as morphological features are also useful in identifying event semantics if the language is morphologically rich, or has a free word order structure. Fundamentally, our architecture ge"
2019.icon-1.5,S13-2004,0,0.0640191,"Missing"
2019.icon-1.5,D17-2017,0,0.0132441,"asks 2 3 Model Description In this section, we describe the ALINED model for the event detection. Primarily, we focus on how to capture event representation at both a character and a word level. In this model, we had to focus on the following major considerations: Related Work Neural approaches to sequence tagging are common due to extensive developments in named entity recognition. Huang et al. (2015) introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF. Ma and Hovy (2016)’s architecture and the NeuroNER program (Dernoncourt et al., 2017) provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event detection and extraction (Araki, 2018). The task of event extraction in any language involves the identification of the event nugget (Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional"
2019.icon-1.5,S13-2014,0,0.0366086,"Missing"
2019.icon-1.5,doddington-etal-2004-automatic,0,0.0161652,"guages beyond English, including French (Bittar et al., 2011), Spanish (Saurı, 2010), Italian (Caselli et al., 2011a) and very recently, Hindi (Goud et al., 2019b). Event detection architectures have their origins in statistical models such as K-means and hierarchical clustering methods (Arnulphy et al., 2015), which have more recently given way to neural models. Deep neural architectures on event annotation vary based on the approach taken to identifying and handling the data. However, event detection as a problem shifts when we move away from the annotation paradigm of datasets such as ACE (Doddington et al., 2004) and TAC KBP (Mitamura et al., 2015) to TimeML datasets such as TimeBank (Pustejovsky et al., 2006), which are used in this paper. There has been limited use of deep learning methods on TimeBanks due to fewer event mentions and a need for data augmentation and bootstrapping. However, in this paper, we show that using subword level information, a language invariant deep learning model can provide similar event detection accuracies as heavily feature engineered language specific statistical methods without using any augmented data. This paper has two main contributions. First, we introduce our m"
2019.icon-1.5,P13-1008,0,0.0326022,"uang et al. (2015) introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF. Ma and Hovy (2016)’s architecture and the NeuroNER program (Dernoncourt et al., 2017) provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event detection and extraction (Araki, 2018). The task of event extraction in any language involves the identification of the event nugget (Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures. Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information (Nguyen and Grishman, 2015), modeling the skipgram architecture to learn lexical feature representations (Chen et al., 2015) as well as using dynamic CNNs in order to extract lexical and syntactic features in parallel (Nguyen and Gri"
2019.icon-1.5,W17-2606,0,0.0131681,"o incorporate features that could be used for sequence tagging using a CRF. Ma and Hovy (2016)’s architecture and the NeuroNER program (Dernoncourt et al., 2017) provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event detection and extraction (Araki, 2018). The task of event extraction in any language involves the identification of the event nugget (Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures. Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information (Nguyen and Grishman, 2015), modeling the skipgram architecture to learn lexical feature representations (Chen et al., 2015) as well as using dynamic CNNs in order to extract lexical and syntactic features in parallel (Nguyen and Grishman, 2016). Recurrent neural architectures have also been employed for this ta"
2019.icon-1.5,P16-2060,0,0.0178886,"of NLP and information retrieval such as automatic summarization, question answering and knowledge graph embeddings ˇ (Chieu and Lee, 2004; Glavaˇs and Snajder, 2014), as events are a representation of temporal information and sequences in text. Various developments in guidelines and datasets for event detection have 36 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 36–44 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ward and backward features of sentences in which events occur (Nguyen et al., 2016; Ghaeini et al., 2016). Note that in both cases architectures focused on dealing with structural, lexical and contextual features. In the domain of multi-lingual and cross lingual event detection, Feng et al. (2018) uses a combination of both LSTMs and CNNs for creating a language independent architecture for capturing events, while Goud et al. (2019a) used stacked RNNs for sequence labeling and a language discriminator to learn language features. The latter architecture implements the use of the character embeddings, but does not identify the relevant features independent of the word embeddings. statistical models"
2019.icon-1.5,P16-1101,0,0.0371162,"extended to other semantically oriented sequence labeling tasks 2 3 Model Description In this section, we describe the ALINED model for the event detection. Primarily, we focus on how to capture event representation at both a character and a word level. In this model, we had to focus on the following major considerations: Related Work Neural approaches to sequence tagging are common due to extensive developments in named entity recognition. Huang et al. (2015) introduced and cultivated the use of bidirectional LSTMs to incorporate features that could be used for sequence tagging using a CRF. Ma and Hovy (2016)’s architecture and the NeuroNER program (Dernoncourt et al., 2017) provided a basic architecture and influenced multiple developments to most sequence labeling tasks, including event detection and extraction (Araki, 2018). The task of event extraction in any language involves the identification of the event nugget (Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination"
2019.icon-1.5,N16-1034,0,0.0176492,"ttention in subfields of NLP and information retrieval such as automatic summarization, question answering and knowledge graph embeddings ˇ (Chieu and Lee, 2004; Glavaˇs and Snajder, 2014), as events are a representation of temporal information and sequences in text. Various developments in guidelines and datasets for event detection have 36 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 36–44 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ward and backward features of sentences in which events occur (Nguyen et al., 2016; Ghaeini et al., 2016). Note that in both cases architectures focused on dealing with structural, lexical and contextual features. In the domain of multi-lingual and cross lingual event detection, Feng et al. (2018) uses a combination of both LSTMs and CNNs for creating a language independent architecture for capturing events, while Goud et al. (2019a) used stacked RNNs for sequence labeling and a language discriminator to learn language features. The latter architecture implements the use of the character embeddings, but does not identify the relevant features independent of the word embeddi"
2019.icon-1.5,P15-2060,0,0.0244039,"ki, 2018). The task of event extraction in any language involves the identification of the event nugget (Ahn, 2006). Prominent work has been done to analyze the lexical and semantic features of event representation (Li et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures. Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information (Nguyen and Grishman, 2015), modeling the skipgram architecture to learn lexical feature representations (Chen et al., 2015) as well as using dynamic CNNs in order to extract lexical and syntactic features in parallel (Nguyen and Grishman, 2016). Recurrent neural architectures have also been employed for this task, which predict the location of the trigger based on combining the for1. Syntactic and lexical information captured by previous event detection tasks should be accounted for. 2. Furthermore, sub-word information is essential as morphological features are also useful in identifying event semantics if the languag"
2019.icon-1.5,C16-1117,0,0.0125584,"twork. hc can be seen as a lexically context-aware character representation of the words of the input sentence. where E wi [i : i + b − 1] accounts for all the characters of given window size of the word. The obtained embedding ewi ∈ Rn−b+1 . The function f is a non-linear function such as a hyperbolic tangent or a sigmoid. It is applied over the Frobenius inner product of the filter and the embedding value as A · B = Tr(AB T ) for any two matrices A and B. We use max-pooling over the output embedding (instead of mean-pooling as it better incorporates the nature of natural language sequences (Xiang et al., 2016)) as: i wic = max ew i i (3) [hc1 , hc2 , ..., hck ], 3.2 Using Contextual Word Embeddings To capture structural information well, we use contextual word embeddings. Let w = [w1 , w2 , ..., wk ] be the words in a sentence. Let their corresponding pre-trained word embeddings w w be ew = [ew 1 , e2 , ..., ek ]. We aggregate the meaning of the sentence by passing the word embeddings through a bidirectional LSTM layer, as follows: (2) For a total of h filters, each of varying widths, we get different representations of wi . Therefore wic = [w1c , w2c , . . . , whc ] is the representation of the it"
2019.icon-1.5,D16-1085,0,0.0176718,"et al., 2013), which served as a basis for neural event nugget detection (Liang et al., 2017). The task of neural event detection has been attempted using a combination of networks, but mostly revolving around the use of convolutional neural architectures. Work in this approach focused on various aspects such as max-pooling to retrieve the structure of event nugget information (Nguyen and Grishman, 2015), modeling the skipgram architecture to learn lexical feature representations (Chen et al., 2015) as well as using dynamic CNNs in order to extract lexical and syntactic features in parallel (Nguyen and Grishman, 2016). Recurrent neural architectures have also been employed for this task, which predict the location of the trigger based on combining the for1. Syntactic and lexical information captured by previous event detection tasks should be accounted for. 2. Furthermore, sub-word information is essential as morphological features are also useful in identifying event semantics if the language is morphologically rich, or has a free word order structure. Fundamentally, our architecture generates character embeddings through convolution and aggregates this information using bidirectional LSTMs (Hochreiter an"
2019.icon-1.5,S13-2010,0,0.0607213,"Missing"
2019.icon-1.5,W16-5706,0,0.0618659,"Missing"
2019.icon-1.5,pustejovsky-etal-2010-iso,0,0.0604013,"Missing"
2019.icon-1.5,S13-2001,0,0.11396,"are covered in detail for the replicability of our results, which are highlighted in section 5. 4.1 Datasets To train and evaluate our model, we use the following datasets for each of the languages we work with multiple corpora, as our experiments span multiple languages. 1. The TempEval-3 TimeBank dataset was used for English (UzZaman et al., 2012). The corpus consists of 61,418 tokens for training and 6,756 event mentions. 5 2. For Spanish, we use the ModeS TimeBank (Modern Spanish TimeBank 1.0) (Nieto and Saur´ı, 2012) for training and testing. This was used in SemEval-2013 Task 1 Task B (UzZaman et al., 2013). The corpus consists of 57,977 tokens. Results and Analysis In this section, we analyze the results of the ALINED model, and compare them to the current state of the art systems for the various languages we train on. We also provide a rigorous error analysis of our system and methodology. Since no single system has compared work in event detection across the five languages that we have chosen for the experiments here, we draw comparisons to the various systems that trained on the individual or group of languages that have been used. Table 1 ahows the direct comparison of results. 3. For Itali"
2019.icon-1.6,W06-0901,0,0.0688931,"d their relation to the event, a knowledge graph is generated, which can then be queried for basic questions. In Hindi NLP, the representation, identification and extraction of events is a fairly new concept. Event extraction from twitter data (Kuila and 2 Related Work Entity or participant extraction is a vital subdomain of event detection and related information extraction tasks. The ACE project (Doddington et al., 2004) and many of the relevant event extraction tasks that followed it had entity detection and tracking as one of the main components for event detection and extraction systems (Ahn, 2006). ACE also provided twenty-four different types of relations between entities. Hong et al. (2011) establishes a mechanism of using entity links in order to more accurately detect event mentions, by associating some entities as event par45 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 45–55 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ticipants or arguments. Joint extraction of event and entity mentions has been attempted (Yang and Mitchell, 2016) by learning intra-event structures and possible forms"
2019.icon-1.6,L16-1727,0,0.0240631,"ic components are essential preprocessing tasks such as POS tagging and dependency parsing. The dependency parse provides the 1 While we define entities by participation similar to ACE (Doddington et al., 2004), the definition of event (Goud et al., 2019) allows for multiple events in a single sentence. 47 Figure 2: Pipeline for Participant Extraction 4.2 Semantic and Discourse Relation Extraction tities are partially indexed, that is that only coreferent entities are indexed. The semantic role of the arguments to an event are extracted by the semantic role labeler (SRL) for Indian languages (Anwar and Sharma, 2016). The SRL uses POS tagged text as an input and provides the semantic role of the nouns and adverbs in the sentence. For the purpose of participant extraction, the adverbs are ignored. However, before the semantic role extraction can be done, event coreference, entity coreference and anaphora resolution are permormed, in order to determine the possible overlap of event mentions (multiple event mentions for the same event) (Chen et al., 2009). Both anaphora and event coreference are done automatically using a combination of role extraction and verb relations as mentioned above, as well as using"
2019.icon-1.6,I13-1130,0,0.0252776,"e analyzed using verb frame data (Soni et al., 2013). The verb frame data provides the possible karaka relations which can be used to determine the mandatory and optional syntactic expectancy of the verb in different senses. A maximal matching algorithm (Algorithm 1) is used across all senses, and the sense with all mandatory and the maximum number of optional karaka arguments is chosen as the sense of that verb. Nominal entity participant identification follows two steps, jointly referred to as entity disambiguation. First, we use a naive coreference resolution using a feature set similar to Dakwale et al. (2013)’s rule based implementation, for entities and events. The syntactic roles of significance are sambandh relations. Some of the design choices in Lee et al. (2012), including features such as number of coreferent arguments and argument roles are crucial to determining participation, as shown in Algorithm 2. Finally, we analyze and resolve co-participation ambiguities. For sentences with multiple events, it is necessary to verify whether all the entities necessarily participate in, or are modified by, the attributed events. In verbal events, maximal matching is done on the entities syntactically"
2019.icon-1.6,W16-6320,1,0.822298,"nal Institute of Information Technology Hyderabad, Telangana, India {pranav.goel, suhan.prabhuk, alok.debnath }@research.iiit.ac.in m.shrivastava@iiit.ac.in Abstract Sarkar, 2017) and in news data (Ramrakhiyani and Majumder, 2013; Goud et al., 2019) are still developing areas of research. However, extensive work has been done on argument structure for Hindi verbs, therefore the syntactic analysis of verbal events has been a topic of sufficient inquiry (Butt, 2010). On the other hand, nominal events, while not studied under that paradigm, have been referenced in entity linking in NER research (Athavale et al., 2016). This paper, given the definition of events in Hindi (Goud et al., 2019), identifies the arguments of these events. We employ a syntactico-semantic approach of entity identification by using dependency parsing to determine the syntactic roles of the arguments and their dependency length from the event mention (Gulordava et al., 2015), and a semantic role labeler which is used to determine the semantic case or functions of the participating entities (Carreras and M`arquez, 2005). For verbal events, verb frame data has also been used for verifying the arguments. This information is constructed"
2019.icon-1.6,C14-1172,0,0.0139913,"tagged text as an input and provides the semantic role of the nouns and adverbs in the sentence. For the purpose of participant extraction, the adverbs are ignored. However, before the semantic role extraction can be done, event coreference, entity coreference and anaphora resolution are permormed, in order to determine the possible overlap of event mentions (multiple event mentions for the same event) (Chen et al., 2009). Both anaphora and event coreference are done automatically using a combination of role extraction and verb relations as mentioned above, as well as using pretrained models (Devi et al., 2014) and manual editing of the output. After this, if a noun also happens to be an event, the dependency relation between it and a verbal event in the sentence (if any) is retained, while the semantic relation is removed. Event-event relations are beyond the scope of this paper, and for the sake of simplicity, it is assumed that events can not be arguments to other events. Retaining the dependency information, however, as it is a feature used in entity disambiguation if an entity happens to participate in a nominal and a verbal event. As with the dependency parse, the semantic relations between tw"
2019.icon-1.6,W02-1202,0,0.118376,"Missing"
2019.icon-1.6,doddington-etal-2004-automatic,0,0.197168,"labeling in Hindi, using syntactic measures such as dependency parsing and semantic measures such as verb frame comparisons and semantic role labeling. Using the entities extracted from the text and their relation to the event, a knowledge graph is generated, which can then be queried for basic questions. In Hindi NLP, the representation, identification and extraction of events is a fairly new concept. Event extraction from twitter data (Kuila and 2 Related Work Entity or participant extraction is a vital subdomain of event detection and related information extraction tasks. The ACE project (Doddington et al., 2004) and many of the relevant event extraction tasks that followed it had entity detection and tracking as one of the main components for event detection and extraction systems (Ahn, 2006). ACE also provided twenty-four different types of relations between entities. Hong et al. (2011) establishes a mechanism of using entity links in order to more accurately detect event mentions, by associating some entities as event par45 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 45–55 Hyderabad, India, December 2019. ©2019 NLP Association o"
2019.icon-1.6,W09-3036,0,0.0169338,"tactically, two major concerns of verb argument analysis are verb phrase ellipsis and complex predicate analysis (Manetta, 2018b,a). Table 1: Dataset and Annotation Statistics 3 Dataset and Annotation Specifications We use a gold-standard corpus of 810 news articles of Goud et al. (2019), and annotate it for entities and their relations with the events. The entityevent relations are annotated based on a syntactic as well as a semantic role. The syntactic role is simply a dependency label (Tandon et al., 2016), while the semantic labels are provided according to Hindi and Urdu PropBank labels (Bhatt et al., 2009). Knowledge graphs are extensively used in semantic information retrieval and has numerous other applications cross language document retrieval (Franco-Salvador et al., 2014), cross lingual plagiarism detection Franco-Salvador et al. (2016), question answering (Indurthi et al., 2017) and summarization (Zheng et al., 2016). The annotated sentence shown in Figure 1 is: 46 frans France vishva World aur and yudha War england England mein in ke of maare killed sipahi soldiers gaye. got. syntactic role information. Particularly for verbal events, the parse also provides the distance from the event m"
2019.icon-1.6,N04-1001,0,0.0291223,". Named entity recognition has been another broader form of approach to entity identification and linking. Entity mention detection and tracking its use in the corpus (Xu et al., 2017) is considered the most fundamental method in this approach. Yamada et al. (2015) approaches the problem of named entity recognition from the perspective of entity linking. Hybrid joint approaches to participant extraction and linking (Plu et al., 2015) have been treated as an extension of this problem, and the OKE 2017 task (Plu et al., 2017) performed participant extraction and linking for ontology enrichment. Florian et al. (2004) and Lin et al. (2016) perform cross lingual entity linking over an enriched knowledge base, one of the languages being Hindi. These approaches are important for understanding and disambiguating the links between nominal events and their participants. Figure 1: Example of a tagged pair of sentences. The event indexes are intra-sentence. Overall statistics Number of Articles 810 Total Number of Sentences 13949 Total Number of Events 20190 Nominal Events 1841 Verbal Events 18349 Total Number of Entities 41847 Average per sentence Length (Words) 18 Number of Entities 3 Number of Events 1.48 Numbe"
2019.icon-1.6,W05-0620,0,0.242715,"Missing"
2019.icon-1.6,E14-1044,0,0.0219637,"Statistics 3 Dataset and Annotation Specifications We use a gold-standard corpus of 810 news articles of Goud et al. (2019), and annotate it for entities and their relations with the events. The entityevent relations are annotated based on a syntactic as well as a semantic role. The syntactic role is simply a dependency label (Tandon et al., 2016), while the semantic labels are provided according to Hindi and Urdu PropBank labels (Bhatt et al., 2009). Knowledge graphs are extensively used in semantic information retrieval and has numerous other applications cross language document retrieval (Franco-Salvador et al., 2014), cross lingual plagiarism detection Franco-Salvador et al. (2016), question answering (Indurthi et al., 2017) and summarization (Zheng et al., 2016). The annotated sentence shown in Figure 1 is: 46 frans France vishva World aur and yudha War england England mein in ke of maare killed sipahi soldiers gaye. got. syntactic role information. Particularly for verbal events, the parse also provides the distance from the event mention, which is essential in order to determine participation in sentences with multiple events. This participation is then verified using verb frame data.1 The procedure fo"
2019.icon-1.6,J12-4003,0,0.0317118,"participation, as shown in Algorithm 2. Finally, we analyze and resolve co-participation ambiguities. For sentences with multiple events, it is necessary to verify whether all the entities necessarily participate in, or are modified by, the attributed events. In verbal events, maximal matching is done on the entities syntactically closest to it, which performs well in default word order. For entities linked to both nominal and verbal events, semantic role information is considered. Nominal events characteristically only take agentive, thematic and locative arguments over the verbal predicate (Gerber and Chai, 2012), while only those temporal arguments are taken which are not attributed to the verbal event. Algorithm 1 Maximal matching Verb Verification 1: procedure M AX M ATCH V ERB 2: VFD ← Verb Frame Data 3: V ← Verbal Event 4: part ← list [(Parent, Participant, Role)] 5: max all ← 0 6: max ← −1 7: for verb in V F D do 8: for sense in verb do 9: if (V = verb) and (part[2] = sense) then 10: max = max + 1 11: if (max all < max) then 12: max all ← max 13: return max all After completing this analysis, the output of the pipeline is condensed and reformatted as inputs to a knowledge graph. Algorithm 2 Enti"
2019.icon-1.6,P15-1136,0,0.0173663,"ts. 4.3 Role Analysis and Verification In order to accurately determine the roles assigned by the two modules above, our pipeline is equipped with an analysis module. In line with the Paninian tradition, we use the notion of yogyata (capability) (Kulkarni et al., 2010) to verify whether an event can take the types and roles of the arguments that have been assigned to it. The output of this system are then analyzed as tem• Entity coreference is taken care of determining the role that the entity performs in the event. This is one of the primary entity based features used for entity coreference (Clark and Manning, 2015). In the corpus, the en48 plates of entity-event relations, which are used to create the knowledge graph. Verbal events are analyzed using verb frame data (Soni et al., 2013). The verb frame data provides the possible karaka relations which can be used to determine the mandatory and optional syntactic expectancy of the verb in different senses. A maximal matching algorithm (Algorithm 1) is used across all senses, and the sense with all mandatory and the maximum number of optional karaka arguments is chosen as the sense of that verb. Nominal entity participant identification follows two steps,"
2019.icon-1.6,P11-1113,0,0.0307217,"for basic questions. In Hindi NLP, the representation, identification and extraction of events is a fairly new concept. Event extraction from twitter data (Kuila and 2 Related Work Entity or participant extraction is a vital subdomain of event detection and related information extraction tasks. The ACE project (Doddington et al., 2004) and many of the relevant event extraction tasks that followed it had entity detection and tracking as one of the main components for event detection and extraction systems (Ahn, 2006). ACE also provided twenty-four different types of relations between entities. Hong et al. (2011) establishes a mechanism of using entity links in order to more accurately detect event mentions, by associating some entities as event par45 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 45–55 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ticipants or arguments. Joint extraction of event and entity mentions has been attempted (Yang and Mitchell, 2016) by learning intra-event structures and possible forms of entity relations to events. Named entity recognition has been another broader form of approac"
2019.icon-1.6,W06-1205,0,0.0577418,"tification 0.89 Semantic Role Identification 0.79 Coreferent Mention Identification 0.91 Argument analysis for verbs in Hindi has been a well-researched topic, as mentioned above. Palmer et al. (2009) studies the computational properties of verbal predicates from a dependency annotation perspective, while Vaidya et al. (2016) and Vaidya et al. (2019) focuses on the syntactic argument structure of light verbs in Hindi. Light verbs are one of the syntactic constructions observed in representation of eventive verbs. Compound verb detection (Chakrabarti et al., 2008), complex predicate detection (Mukerjee et al., 2006) and argument identification in complex predicates (Montaut, 2016) can be modeled together in syntactic argument detection for verbal events. The study of noun incorporation in verb complexes (Dayal, 2015) provide a semantic perspective of argument structure and event participation. Syntactically, two major concerns of verb argument analysis are verb phrase ellipsis and complex predicate analysis (Manetta, 2018b,a). Table 1: Dataset and Annotation Statistics 3 Dataset and Annotation Specifications We use a gold-standard corpus of 810 news articles of Goud et al. (2019), and annotate it for ent"
2019.icon-1.6,E17-1036,0,0.028957,"(2019), and annotate it for entities and their relations with the events. The entityevent relations are annotated based on a syntactic as well as a semantic role. The syntactic role is simply a dependency label (Tandon et al., 2016), while the semantic labels are provided according to Hindi and Urdu PropBank labels (Bhatt et al., 2009). Knowledge graphs are extensively used in semantic information retrieval and has numerous other applications cross language document retrieval (Franco-Salvador et al., 2014), cross lingual plagiarism detection Franco-Salvador et al. (2016), question answering (Indurthi et al., 2017) and summarization (Zheng et al., 2016). The annotated sentence shown in Figure 1 is: 46 frans France vishva World aur and yudha War england England mein in ke of maare killed sipahi soldiers gaye. got. syntactic role information. Particularly for verbal events, the parse also provides the distance from the event mention, which is essential in order to determine participation in sentences with multiple events. This participation is then verified using verb frame data.1 The procedure for syntactic participation detection is as follows: ye sun kar ram ko This listen to Ram (acc) bohot dukh hua m"
2019.icon-1.6,P08-1030,0,0.0459048,"best of our knowledge, no other architecture exists for this purpose. The extracted combined role information is incorporated in a knowledge graph that can be queried via subgraph extraction for basic questions. The architectures presented in this paper can be used for participant extraction and evententity linking in most Indo-Aryan languages, due to similar syntactic and semantic properties of event arguments. 1 Introduction Events are defined as situations that happen or occur (Saur´ı et al., 2006). Events therefore involve participating entities, sometimes referred to as event arguments (Ji and Grishman, 2008). The extraction of role information of entities participating in events is a fast-evolving area of research in information retrieval as well as subfields of NLP such as question answering and summarization (Lin and Liang, 2008). This paper handles the challenge of participant detection and labeling in Hindi, using syntactic measures such as dependency parsing and semantic measures such as verb frame comparisons and semantic role labeling. Using the entities extracted from the text and their relation to the event, a knowledge graph is generated, which can then be queried for basic questions. I"
2019.icon-1.6,D12-1045,0,0.0356465,"onal syntactic expectancy of the verb in different senses. A maximal matching algorithm (Algorithm 1) is used across all senses, and the sense with all mandatory and the maximum number of optional karaka arguments is chosen as the sense of that verb. Nominal entity participant identification follows two steps, jointly referred to as entity disambiguation. First, we use a naive coreference resolution using a feature set similar to Dakwale et al. (2013)’s rule based implementation, for entities and events. The syntactic roles of significance are sambandh relations. Some of the design choices in Lee et al. (2012), including features such as number of coreferent arguments and argument roles are crucial to determining participation, as shown in Algorithm 2. Finally, we analyze and resolve co-participation ambiguities. For sentences with multiple events, it is necessary to verify whether all the entities necessarily participate in, or are modified by, the attributed events. In verbal events, maximal matching is done on the entities syntactically closest to it, which performs well in default word order. For entities linked to both nominal and verbal events, semantic role information is considered. Nominal"
2019.icon-1.6,W16-2701,0,0.0136182,"has been another broader form of approach to entity identification and linking. Entity mention detection and tracking its use in the corpus (Xu et al., 2017) is considered the most fundamental method in this approach. Yamada et al. (2015) approaches the problem of named entity recognition from the perspective of entity linking. Hybrid joint approaches to participant extraction and linking (Plu et al., 2015) have been treated as an extension of this problem, and the OKE 2017 task (Plu et al., 2017) performed participant extraction and linking for ontology enrichment. Florian et al. (2004) and Lin et al. (2016) perform cross lingual entity linking over an enriched knowledge base, one of the languages being Hindi. These approaches are important for understanding and disambiguating the links between nominal events and their participants. Figure 1: Example of a tagged pair of sentences. The event indexes are intra-sentence. Overall statistics Number of Articles 810 Total Number of Sentences 13949 Total Number of Events 20190 Nominal Events 1841 Verbal Events 18349 Total Number of Entities 41847 Average per sentence Length (Words) 18 Number of Entities 3 Number of Events 1.48 Number of Entities per Even"
2019.icon-1.6,C16-1308,0,0.0170257,"events. Retaining the dependency information, however, as it is a feature used in entity disambiguation if an entity happens to participate in a nominal and a verbal event. As with the dependency parse, the semantic relations between two nouns is retained regardless of their eventiveness, as the semantic relation acts as a verification for the detected primary participant. • Event coreference is taken care of by indexing the event. All event mentions in the annotated input are indexed by a numerical subscript. Corefernent events have similar event triggers and overlapping argument structures (Lu et al., 2016), which are crucial features in the annotation of these events. The indices of coreferent event mentions are the same, which indicates that they share their arguments. 4.3 Role Analysis and Verification In order to accurately determine the roles assigned by the two modules above, our pipeline is equipped with an analysis module. In line with the Paninian tradition, we use the notion of yogyata (capability) (Kulkarni et al., 2010) to verify whether an event can take the types and roles of the arguments that have been assigned to it. The output of this system are then analyzed as tem• Entity cor"
2019.icon-1.6,I13-1151,0,0.0205457,"the Paninian tradition, we use the notion of yogyata (capability) (Kulkarni et al., 2010) to verify whether an event can take the types and roles of the arguments that have been assigned to it. The output of this system are then analyzed as tem• Entity coreference is taken care of determining the role that the entity performs in the event. This is one of the primary entity based features used for entity coreference (Clark and Manning, 2015). In the corpus, the en48 plates of entity-event relations, which are used to create the knowledge graph. Verbal events are analyzed using verb frame data (Soni et al., 2013). The verb frame data provides the possible karaka relations which can be used to determine the mandatory and optional syntactic expectancy of the verb in different senses. A maximal matching algorithm (Algorithm 1) is used across all senses, and the sense with all mandatory and the maximum number of optional karaka arguments is chosen as the sense of that verb. Nominal entity participant identification follows two steps, jointly referred to as entity disambiguation. First, we use a naive coreference resolution using a feature set similar to Dakwale et al. (2013)’s rule based implementation, f"
2019.icon-1.6,E12-2021,0,0.0693241,"Missing"
2019.icon-1.6,W16-1716,0,0.0193961,"complexes (Dayal, 2015) provide a semantic perspective of argument structure and event participation. Syntactically, two major concerns of verb argument analysis are verb phrase ellipsis and complex predicate analysis (Manetta, 2018b,a). Table 1: Dataset and Annotation Statistics 3 Dataset and Annotation Specifications We use a gold-standard corpus of 810 news articles of Goud et al. (2019), and annotate it for entities and their relations with the events. The entityevent relations are annotated based on a syntactic as well as a semantic role. The syntactic role is simply a dependency label (Tandon et al., 2016), while the semantic labels are provided according to Hindi and Urdu PropBank labels (Bhatt et al., 2009). Knowledge graphs are extensively used in semantic information retrieval and has numerous other applications cross language document retrieval (Franco-Salvador et al., 2014), cross lingual plagiarism detection Franco-Salvador et al. (2016), question answering (Indurthi et al., 2017) and summarization (Zheng et al., 2016). The annotated sentence shown in Figure 1 is: 46 frans France vishva World aur and yudha War england England mein in ke of maare killed sipahi soldiers gaye. got. syntacti"
2019.icon-1.6,C16-1125,0,0.0271047,"Entities 41847 Average per sentence Length (Words) 18 Number of Entities 3 Number of Events 1.48 Number of Entities per Event 2.08 Most Common Relation Entity - Nominal Event (ARG0) Entity - Verbal Event (K1, ARG0) Inter-Annotator Statistics Participant Identification 0.86 Syntactic Role Identification 0.89 Semantic Role Identification 0.79 Coreferent Mention Identification 0.91 Argument analysis for verbs in Hindi has been a well-researched topic, as mentioned above. Palmer et al. (2009) studies the computational properties of verbal predicates from a dependency annotation perspective, while Vaidya et al. (2016) and Vaidya et al. (2019) focuses on the syntactic argument structure of light verbs in Hindi. Light verbs are one of the syntactic constructions observed in representation of eventive verbs. Compound verb detection (Chakrabarti et al., 2008), complex predicate detection (Mukerjee et al., 2006) and argument identification in complex predicates (Montaut, 2016) can be modeled together in syntactic argument detection for verbal events. The study of noun incorporation in verb complexes (Dayal, 2015) provide a semantic perspective of argument structure and event participation. Syntactically, two ma"
2019.icon-1.6,2019.lilt-17.1,0,0.0286253,"r sentence Length (Words) 18 Number of Entities 3 Number of Events 1.48 Number of Entities per Event 2.08 Most Common Relation Entity - Nominal Event (ARG0) Entity - Verbal Event (K1, ARG0) Inter-Annotator Statistics Participant Identification 0.86 Syntactic Role Identification 0.89 Semantic Role Identification 0.79 Coreferent Mention Identification 0.91 Argument analysis for verbs in Hindi has been a well-researched topic, as mentioned above. Palmer et al. (2009) studies the computational properties of verbal predicates from a dependency annotation perspective, while Vaidya et al. (2016) and Vaidya et al. (2019) focuses on the syntactic argument structure of light verbs in Hindi. Light verbs are one of the syntactic constructions observed in representation of eventive verbs. Compound verb detection (Chakrabarti et al., 2008), complex predicate detection (Mukerjee et al., 2006) and argument identification in complex predicates (Montaut, 2016) can be modeled together in syntactic argument detection for verbal events. The study of noun incorporation in verb complexes (Dayal, 2015) provide a semantic perspective of argument structure and event participation. Syntactically, two major concerns of verb argu"
2019.icon-1.6,P17-1114,0,0.0191456,"associating some entities as event par45 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 45–55 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ticipants or arguments. Joint extraction of event and entity mentions has been attempted (Yang and Mitchell, 2016) by learning intra-event structures and possible forms of entity relations to events. Named entity recognition has been another broader form of approach to entity identification and linking. Entity mention detection and tracking its use in the corpus (Xu et al., 2017) is considered the most fundamental method in this approach. Yamada et al. (2015) approaches the problem of named entity recognition from the perspective of entity linking. Hybrid joint approaches to participant extraction and linking (Plu et al., 2015) have been treated as an extension of this problem, and the OKE 2017 task (Plu et al., 2017) performed participant extraction and linking for ontology enrichment. Florian et al. (2004) and Lin et al. (2016) perform cross lingual entity linking over an enriched knowledge base, one of the languages being Hindi. These approaches are important for u"
2019.icon-1.6,W15-4320,0,0.0257123,"ngal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 45–55 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ticipants or arguments. Joint extraction of event and entity mentions has been attempted (Yang and Mitchell, 2016) by learning intra-event structures and possible forms of entity relations to events. Named entity recognition has been another broader form of approach to entity identification and linking. Entity mention detection and tracking its use in the corpus (Xu et al., 2017) is considered the most fundamental method in this approach. Yamada et al. (2015) approaches the problem of named entity recognition from the perspective of entity linking. Hybrid joint approaches to participant extraction and linking (Plu et al., 2015) have been treated as an extension of this problem, and the OKE 2017 task (Plu et al., 2017) performed participant extraction and linking for ontology enrichment. Florian et al. (2004) and Lin et al. (2016) perform cross lingual entity linking over an enriched knowledge base, one of the languages being Hindi. These approaches are important for understanding and disambiguating the links between nominal events and their partic"
2019.icon-1.6,N16-1033,0,0.0181088,"of the main components for event detection and extraction systems (Ahn, 2006). ACE also provided twenty-four different types of relations between entities. Hong et al. (2011) establishes a mechanism of using entity links in order to more accurately detect event mentions, by associating some entities as event par45 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 45–55 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) ticipants or arguments. Joint extraction of event and entity mentions has been attempted (Yang and Mitchell, 2016) by learning intra-event structures and possible forms of entity relations to events. Named entity recognition has been another broader form of approach to entity identification and linking. Entity mention detection and tracking its use in the corpus (Xu et al., 2017) is considered the most fundamental method in this approach. Yamada et al. (2015) approaches the problem of named entity recognition from the perspective of entity linking. Hybrid joint approaches to participant extraction and linking (Plu et al., 2015) have been treated as an extension of this problem, and the OKE 2017 task (Plu"
2019.icon-1.6,P14-1090,0,0.0725436,"Missing"
2019.icon-1.6,P15-1128,0,0.177565,"events in Hindi (Goud et al., 2019), identifies the arguments of these events. We employ a syntactico-semantic approach of entity identification by using dependency parsing to determine the syntactic roles of the arguments and their dependency length from the event mention (Gulordava et al., 2015), and a semantic role labeler which is used to determine the semantic case or functions of the participating entities (Carreras and M`arquez, 2005). For verbal events, verb frame data has also been used for verifying the arguments. This information is constructed as a knowledge graph, a query graph (Yih et al., 2015) of which can then be used for question answering. We describe the development of a knowledge graph from an event annotated corpus by presenting a pipeline that identifies and extracts the relations between entities and events from Hindi news articles. Due to the semantic implications of argument identification for events in Hindi, we use a combined syntactic argument and semantic role identification methodology. To the best of our knowledge, no other architecture exists for this purpose. The extracted combined role information is incorporated in a knowledge graph that can be queried via subgr"
2020.acl-srw.13,W04-1013,0,0.0392631,"Missing"
2020.acl-srw.13,N19-1071,0,0.0364531,"Missing"
2020.acl-srw.13,E06-1038,0,0.132537,"kens. SCAR achieves higher ROUGE scores on benchmark datasets than the existing stateof-the-art methods and baselines. We also conduct a user study to demonstrate the application of our model as a text highlighting system. Using our model to underscore salient information facilitates speed-reading and reduces the time required to skim a document. 1 Figure 1: An example of a system that highlights the salient content, allowing the user to skim through the document quickly. In the past, compression approaches have revolved around statistical methods (Knight and Marcu, 2000) and syntactic rules (McDonald, 2006). Current state-of-the-art methods model the problem as a sequence-to-sequence learning task (Filippova et al., 2015). Although these methods perform well, they require massive parallel training datasets that are difficult to collect (Filippova and Altun, 2013). Recently, unsupervised approaches have been explored to overcome this limitation. Fevry and Phang (2018) model compression as a denoising task but barely reach the baselines. Baziotis et al. (2019) propose SEQ3 , an autoencoder which uses a Gumbel-softmax to represent the distribution over summaries. But a qualitative analysis of their"
2020.acl-srw.13,P11-1049,0,0.0318393,"uman-readable summaries. An additional topic loss is required to ensure that the summary contains relevant words, making the model non-generic and fine-tuned to the domain. A qualitative analysis of the outputs shows that SEQ3 merely mimics the lead baseline and generates compressions by blindly copying a prefix of the input. Related Work Early compression algorithms were formulated using strong linguistic priors and language heuristics (Jing, 2000; Knight and Marcu, 2002; Dorr et al., 2003; Cohn and Lapata, 2008). McDonald (2006) use syntactical evidence to condition the output of the model. Berg-Kirkpatrick et al. (2011) prune dependency edges to remove constituents for compression. Deep learning-based approaches have gained popularity owing to their success in core NLP tasks such as machine translation (Bahdanau et al., 2014). Filippova et al. (2015) propose an RNN based encoder-decoder network for deletion based compression. Although this approach achieves superior performance over metric-based approaches, a large amount of paired sentences are needed to train the network. The first attempt to reduce the dependence on paired corpora for deletion based deep learning compression models was made by Miao and Bl"
2020.acl-srw.13,D16-1031,0,0.0191177,"al. (2011) prune dependency edges to remove constituents for compression. Deep learning-based approaches have gained popularity owing to their success in core NLP tasks such as machine translation (Bahdanau et al., 2014). Filippova et al. (2015) propose an RNN based encoder-decoder network for deletion based compression. Although this approach achieves superior performance over metric-based approaches, a large amount of paired sentences are needed to train the network. The first attempt to reduce the dependence on paired corpora for deletion based deep learning compression models was made by Miao and Blunsom (2016). They train separate compressor and reconstruction models, to allow for both supervised and unsupervised training. The compressor consists of a discrete variational autoencoder. The model is trained end-to-end using the REINFORCE algorithm. However, the reported results still use a sizeable amount of labeled data. Recent approaches have sought completely unsupervised solutions. Fevry and Phang (2018) use a denoising autoencoder (DAE) for sentence compression. The input sentence is shuffled and extended to add noise. DAE tries to reconstruct the original denoised sentence from the noisy input."
2020.acl-srw.13,C08-1018,0,0.0411263,"g the summary. A pre-trained language model acts as a prior, to incentivize the compressor to produce human-readable summaries. An additional topic loss is required to ensure that the summary contains relevant words, making the model non-generic and fine-tuned to the domain. A qualitative analysis of the outputs shows that SEQ3 merely mimics the lead baseline and generates compressions by blindly copying a prefix of the input. Related Work Early compression algorithms were formulated using strong linguistic priors and language heuristics (Jing, 2000; Knight and Marcu, 2002; Dorr et al., 2003; Cohn and Lapata, 2008). McDonald (2006) use syntactical evidence to condition the output of the model. Berg-Kirkpatrick et al. (2011) prune dependency edges to remove constituents for compression. Deep learning-based approaches have gained popularity owing to their success in core NLP tasks such as machine translation (Bahdanau et al., 2014). Filippova et al. (2015) propose an RNN based encoder-decoder network for deletion based compression. Although this approach achieves superior performance over metric-based approaches, a large amount of paired sentences are needed to train the network. The first attempt to redu"
2020.acl-srw.13,W03-0501,0,0.211735,"ruct the input using the summary. A pre-trained language model acts as a prior, to incentivize the compressor to produce human-readable summaries. An additional topic loss is required to ensure that the summary contains relevant words, making the model non-generic and fine-tuned to the domain. A qualitative analysis of the outputs shows that SEQ3 merely mimics the lead baseline and generates compressions by blindly copying a prefix of the input. Related Work Early compression algorithms were formulated using strong linguistic priors and language heuristics (Jing, 2000; Knight and Marcu, 2002; Dorr et al., 2003; Cohn and Lapata, 2008). McDonald (2006) use syntactical evidence to condition the output of the model. Berg-Kirkpatrick et al. (2011) prune dependency edges to remove constituents for compression. Deep learning-based approaches have gained popularity owing to their success in core NLP tasks such as machine translation (Bahdanau et al., 2014). Filippova et al. (2015) propose an RNN based encoder-decoder network for deletion based compression. Although this approach achieves superior performance over metric-based approaches, a large amount of paired sentences are needed to train the network. T"
2020.acl-srw.13,D14-1162,0,0.092757,"d in the summary. The summary is represented as s0 = s Iv , where represents element-wise multiplication. Therefore, words corresponding to Ivi ≈ 0 are effectively skipped. The network tries to reconstruct the input sentence from s0. Formally, the network tries to find an Iv∗ such that Pk the probability p(s|s Iv ) is maximized and t=1 Ivt is minimized, jointly. The probability p(s|s Iv ) can be decomposed further as shown in Eq.(1) Iv∗ = arg max Iv k Y p(wt |(w1 × Iv1 ), t=1 ..., (wk−1 × Ivk−1 )) (1) For every word in the sentence, we learn a 300-dimensional embedding initialized with GloVe (Pennington et al., 2014). These embeddings are sequentially fed as input to the Sentence Encoder (Es ), composed of a bi-LSTM. The input is fed forwards and backward. The hidden states are a concatenation of the forward and backward states. The sentence representation is obtained from the final hidden state of Es (i.e., he1 ). The Indicator Extraction Module (IEM), a bi-LSTM decoder, is initialized using he1 . The output of this decoder at each time step is passed through a network of two fully connected layers to generate 89 Figure 2: The figure shows the proposed SCAR architecture (details are described in Section"
2020.acl-srw.13,K18-1040,0,0.0816682,"An example of a system that highlights the salient content, allowing the user to skim through the document quickly. In the past, compression approaches have revolved around statistical methods (Knight and Marcu, 2000) and syntactic rules (McDonald, 2006). Current state-of-the-art methods model the problem as a sequence-to-sequence learning task (Filippova et al., 2015). Although these methods perform well, they require massive parallel training datasets that are difficult to collect (Filippova and Altun, 2013). Recently, unsupervised approaches have been explored to overcome this limitation. Fevry and Phang (2018) model compression as a denoising task but barely reach the baselines. Baziotis et al. (2019) propose SEQ3 , an autoencoder which uses a Gumbel-softmax to represent the distribution over summaries. But a qualitative analysis of their outputs shows that SEQ3 mimics the lead baseline. In this work, we present an unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is composed of a compressor and a reconstructor. For each word in the input, the compressor determines whether or not to include it in the compression. A length loss restricts the compression length"
2020.acl-srw.13,D15-1044,0,0.0985918,"Missing"
2020.acl-srw.13,D15-1042,0,0.257479,"Missing"
2020.acl-srw.13,E17-2007,0,0.0139791,"ssions SEQ3 DAE SCAR Table 2: Average correctness and time scores. of a bi-gram. Without Linkage loss (Llnk ), SCAR loses its ability to drop inferable portions of the input. Without Dstop , a mechanism to re-distribute probability mass from stop words, SCAR tends only to drop stopwords. Lower values of r, cause the model to generate smaller compressions. As expected, all of the above factors cause a dip in performance. 4.3 Qualitative evaluation 5 ROUGE only measures the content overlap and does not account for coherence. We conduct a Qualitative study to address the known issues with ROUGE (Schluter, 2017) and evaluate SCAR’s effectiveness as a speed reading system. Human evaluators are asked to match the reference/compression that they are shown with the correct headline from a set of 5 options. 3 incorrect options are generated by selecting Gigaword headlines that share tokens with the reference. The Conclusion and Future Work SCAR addresses a significant limitation of the unavailability of labeled data for sentence compression. It outperforms the existing state-of-the-art unsupervised models. Since SCAR learns to drop inferable components of the input and therefore reduces noise, it can be u"
2020.acl-srw.13,P17-1099,0,0.0722474,"Missing"
2020.acl-srw.13,D13-1155,0,0.0180766,"re salient information facilitates speed-reading and reduces the time required to skim a document. 1 Figure 1: An example of a system that highlights the salient content, allowing the user to skim through the document quickly. In the past, compression approaches have revolved around statistical methods (Knight and Marcu, 2000) and syntactic rules (McDonald, 2006). Current state-of-the-art methods model the problem as a sequence-to-sequence learning task (Filippova et al., 2015). Although these methods perform well, they require massive parallel training datasets that are difficult to collect (Filippova and Altun, 2013). Recently, unsupervised approaches have been explored to overcome this limitation. Fevry and Phang (2018) model compression as a denoising task but barely reach the baselines. Baziotis et al. (2019) propose SEQ3 , an autoencoder which uses a Gumbel-softmax to represent the distribution over summaries. But a qualitative analysis of their outputs shows that SEQ3 mimics the lead baseline. In this work, we present an unsupervised deep learning framework (SCAR) for deletion-based sentence compression. SCAR is composed of a compressor and a reconstructor. For each word in the input, the compressor"
2020.acl-srw.13,A00-1043,0,0.412487,"and a reconstructor tries to reconstruct the input using the summary. A pre-trained language model acts as a prior, to incentivize the compressor to produce human-readable summaries. An additional topic loss is required to ensure that the summary contains relevant words, making the model non-generic and fine-tuned to the domain. A qualitative analysis of the outputs shows that SEQ3 merely mimics the lead baseline and generates compressions by blindly copying a prefix of the input. Related Work Early compression algorithms were formulated using strong linguistic priors and language heuristics (Jing, 2000; Knight and Marcu, 2002; Dorr et al., 2003; Cohn and Lapata, 2008). McDonald (2006) use syntactical evidence to condition the output of the model. Berg-Kirkpatrick et al. (2011) prune dependency edges to remove constituents for compression. Deep learning-based approaches have gained popularity owing to their success in core NLP tasks such as machine translation (Bahdanau et al., 2014). Filippova et al. (2015) propose an RNN based encoder-decoder network for deletion based compression. Although this approach achieves superior performance over metric-based approaches, a large amount of paired s"
2020.acl-srw.19,I08-2099,1,0.650058,"); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is around 2400 sentences. The treebank is annotated using Computational Paninian grammar (Bharati et al., 1995; Begum et al., 2008) proposed for Indian languages. The treebank is annotated at interchunk level (Bharati et al., 2009) in SSF (Bharati et al., 2007) format. Only chunk heads in a sentence are annotated with dependency labels. Figure 1: Inter-chunk dependency tree. B ∗ denotes the beginning of a new chunk. We automatically annotate the intra-chunk dependencies (Bhat, 2017) using a Shift-Reduce parser based on Context Free Grammar rules within a chunk, written for Telugu1 . Annotating the intrachunk dependencies provides a complete parse tree for each sentence. Figure 2: Intra-chunk dependency tree The treebank i"
2020.acl-srw.19,J95-3006,0,0.793513,"Kulmizev et al. (2019); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is around 2400 sentences. The treebank is annotated using Computational Paninian grammar (Bharati et al., 1995; Begum et al., 2008) proposed for Indian languages. The treebank is annotated at interchunk level (Bharati et al., 2009) in SSF (Bharati et al., 2007) format. Only chunk heads in a sentence are annotated with dependency labels. Figure 1: Inter-chunk dependency tree. B ∗ denotes the beginning of a new chunk. We automatically annotate the intra-chunk dependencies (Bhat, 2017) using a Shift-Reduce parser based on Context Free Grammar rules within a chunk, written for Telugu1 . Annotating the intrachunk dependencies provides a complete parse tree for each sentence. Figure 2: Intra-chunk dependenc"
2020.acl-srw.19,W06-2920,0,0.177613,"ebank is annotated at interchunk level (Bharati et al., 2009) in SSF (Bharati et al., 2007) format. Only chunk heads in a sentence are annotated with dependency labels. Figure 1: Inter-chunk dependency tree. B ∗ denotes the beginning of a new chunk. We automatically annotate the intra-chunk dependencies (Bhat, 2017) using a Shift-Reduce parser based on Context Free Grammar rules within a chunk, written for Telugu1 . Annotating the intrachunk dependencies provides a complete parse tree for each sentence. Figure 2: Intra-chunk dependency tree The treebank is converted from SSF to CoNLLX format (Buchholz and Marsi, 2006)2 . 4 Our Approach We propose to replace the rich hand-crafted feature templates used in Malt parser systems with a minimally defined feature set which uses automatically learned word representations from BERT. We do not make use of any additional pipeline features like POS or morphological information assuming this information is captured within the vectors. We train a BERT baseline model (Devlin et al., 2019) on the Telugu wikipedia data, which comprises 71289 articles. We use the ILMT tokenizer included in the Telugu shallow parser 3 to segment the data into sentences. The sentence segmente"
2020.acl-srw.19,K18-2005,0,0.154549,"istic features using dense vector representations in a neural network based parser (Tandon and Sharma, 2017). Recent developments in the field of NLP led to the arrival of contextual word vectors (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) and their extensive use in downstream NLP tasks, from POS tagging (Peters et al., 2018) to more complex tasks like Question Answering and Natural Language Inference tasks (Devlin et al., 2019). Contextual vectors have also been applied to dependency parsing systems. The top-ranked system in CoNLL-2018 shared task on Dependency Parsing(Che et al., 2018) used ELMo representations along with conventional word vectors in a graph based parser. Kulmizev et al. (2019); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is"
2020.acl-srw.19,D14-1082,0,0.0525434,"ons (Kosaraju et al., 2010; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2018). 143 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dep"
2020.acl-srw.19,N19-1423,0,0.456537,"et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model and each word can be uniquely represented based on its context. One such model is BERT (Devlin et al., 2019). BERT vectors are deep bidirectional representations pre-trained by jointly conditioning on both left and right context of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that for a morphologically rich, agglutinative lan"
2020.acl-srw.19,P18-1031,0,0.119682,"like part-of-speech and morphology (Kosaraju et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model and each word can be uniquely represented based on its context. One such model is BERT (Devlin et al., 2019). BERT vectors are deep bidirectional representations pre-trained by jointly conditioning on both left and right context of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that fo"
2020.acl-srw.19,W12-5617,0,0.0251942,"nsition based approach. The resulting parser has a very simple architecture with minimal feature engineering and achieves state-of-the-art results for Telugu. 1 Introduction Dependency parsing is extremely useful for many downstream tasks. However, robust dependency parsers are not available for several Indian languages. One reason is the unavailability of annotated treebanks. Another reason is that most of the existing dependency parsers for Indian languages use hand-crafted features using linguistic information like part-of-speech and morphology (Kosaraju et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model"
2020.acl-srw.19,Q16-1023,0,0.0183137,"10; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2018). 143 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dependency parsing has been focused"
2020.acl-srw.19,D19-1279,0,0.144727,"in the field of NLP led to the arrival of contextual word vectors (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) and their extensive use in downstream NLP tasks, from POS tagging (Peters et al., 2018) to more complex tasks like Question Answering and Natural Language Inference tasks (Devlin et al., 2019). Contextual vectors have also been applied to dependency parsing systems. The top-ranked system in CoNLL-2018 shared task on Dependency Parsing(Che et al., 2018) used ELMo representations along with conventional word vectors in a graph based parser. Kulmizev et al. (2019); Kondratyuk and Straka (2019) use contextual vector representations for multilingual dependency parsing. In this paper, we train a BERT-baseline model (Devlin et al., 2019) on Telugu Wikipedia data and use these vector representations to improve Telugu dependency parsing. 3 Telugu Dependency Treebank We use the Telugu treebank made available for ICON 2010 tools contest. We extend this treebank by another 900 sentences from the HCU Telugu treebank. The size of the combined treebank is around 2400 sentences. The treebank is annotated using Computational Paninian grammar (Bharati et al., 1995; Begum et al., 2008) proposed fo"
2020.acl-srw.19,D19-1277,0,0.0296941,"Missing"
2020.acl-srw.19,P18-1130,0,0.0163753,"harma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2018). 143 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dependency parsing has been focused on rule based systems (Kesidi et al., 2011) or data-d"
2020.acl-srw.19,W04-0308,0,0.389163,"of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that for a morphologically rich, agglutinative language like Telugu, just three word features with good quality vector representations can effectively capture the information required for parsing. We put the feature representations through a feed forward network and train using a greedy transition based parser (Nivre, 2004, 2008). Past work on Telugu dependency parsing has only been focused on predicting inter-chunk dependency relations (Kosaraju et al., 2010; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several"
2020.acl-srw.19,J08-4003,0,0.118423,"sequentially and treat parsing as a sequence of actions that produce a parse tree. They predict a sequence of transition operations starting from an initial configuration to a terminal configuration, and construct a dependency parse tree in the process. A configuration consists of a stack, an input buffer of words, and a set of relations representing a dependency tree. They make use of a classifier to predict the next transition operation based on a set of features derived from the current configuration. A couple of widely used transition systems are Arc-standard (Nivre, 2004) and Arc-eager (Nivre, 2008). We make use of the Arc-standard transition system in our parser and briefly describe it here. 4.1.1 • LEFT-ARC: Adds a head-dependent relation between the word at the top of stack and the 5 • RIGHT-ARC: Adds a head-dependent relation between the second word on the stack and the top word and removes the top word from the stack. • SHIFT: Moves the word from the front of the buffer onto the stack. In the labeled version of parsing, there are a total of 2` + 1 transitions, where ` is the number of different dependency labels. There is a left-arc and a right-arc transition corresponding to each l"
2020.acl-srw.19,nivre-etal-2006-maltparser,0,0.11085,"ional Linguistics: Student Research Workshop, pages 143–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics Graph based approaches to dependency parsing have also become more common over the last few years (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017, inter alia). However, there hasn’t been much recent work on parsing Indian languages and much less on Telugu. Most of the previous work on Telugu dependency parsing has been focused on rule based systems (Kesidi et al., 2011) or data-driven transition based systems (Kanneganti et al., 2016) using Malt parser (Nivre et al., 2006). The Malt parser uses a classifier to predict the transition operations taking a feature template as input. The feature templates used in Telugu parsers commonly consist of several hand-crafted features like words, their partof-speech tags, gender, number and other morphological features (Kosaraju et al., 2010; Kanneganti et al., 2016). There has been some work done on representing these linguistic features using dense vector representations in a neural network based parser (Tandon and Sharma, 2017). Recent developments in the field of NLP led to the arrival of contextual word vectors (Howard"
2020.acl-srw.19,L16-1262,0,0.0399096,"Missing"
2020.acl-srw.19,N18-1202,0,0.669699,"morphology (Kosaraju et al., 2010; Bharati et al., 2008; Jain et al., 2012) which are very expensive to annotate. Telugu is a low resource language and there hasn’t been much recent work done on parsing. Most of the previous work on Telugu dependency parsing has been focused on rule based systems or datadriven transition based systems. This paper focuses on improving feature representations for a low resource, agglutinative language like Telugu using the latest developments in the field of NLP such as contextual vector representations. Contextual word representations (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019) are derived from a language model and each word can be uniquely represented based on its context. One such model is BERT (Devlin et al., 2019). BERT vectors are deep bidirectional representations pre-trained by jointly conditioning on both left and right context of a word and have been shown to perform better on variety of NLP tasks. In this paper, we use BERT representations for parsing Telugu. We replace the rich hand-crafted linguistic features with a minimal feature function using a small number of contextual word representations. We show that for a morphologically r"
2020.acl-srw.19,P16-1162,0,0.0821748,"uses automatically learned word representations from BERT. We do not make use of any additional pipeline features like POS or morphological information assuming this information is captured within the vectors. We train a BERT baseline model (Devlin et al., 2019) on the Telugu wikipedia data, which comprises 71289 articles. We use the ILMT tokenizer included in the Telugu shallow parser 3 to segment the data into sentences. The sentence segmented data consists of approximately 2.6M sentences. We convert all of the data from UTF to WX4 notation for faster processing. We use byte-pair encoding (Sennrich et al., 2016) to tokenize the data and generate a vocabulary file. We pass this vocabulary 144 1 https://github.com/ltrc/ Shift-Reduce-Chunk-Expander 2 https://github.com/ltrc/ SSF-to-CONLL-Convertor 3 https://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow_parser.php 4 https://en.wikipedia.org/wiki/WX_ notation file to BERT 5 for pre-training. After pre-training, we extract contextual token representations for all the sentences in the treebank from the pre-trained BERT model. In case a single word is split into multiple tokens, we treat these tokens as continuous bag of words and add the represen"
2020.acl-srw.19,W17-6529,1,0.893888,"feature function using a small number of contextual word representations. We show that for a morphologically rich, agglutinative language like Telugu, just three word features with good quality vector representations can effectively capture the information required for parsing. We put the feature representations through a feed forward network and train using a greedy transition based parser (Nivre, 2004, 2008). Past work on Telugu dependency parsing has only been focused on predicting inter-chunk dependency relations (Kosaraju et al., 2010; Kesidi et al., 2011; Kanneganti et al., 2016, 2017; Tandon and Sharma, 2017). In this paper, we also report parser accuracies on intra-chunk annotated Telugu treebank for the first time. 2 Related Work Extensive work has been done on dependency parsing in the last decade, especially due to the CoNLL shared tasks on dependency parsing. Creation of Universal Dependencies (Nivre et al., 2016) led to an increased focus on common approaches to parsing several different languages. There were new transition based approaches making use of more robust input representations (Chen and Manning, 2014; Kiperwasser and Goldberg, 2016) and improved network architectures (Ma et al., 2"
2020.acl-srw.19,P19-1452,0,0.0250688,"t al. (2018) and Che et al. (2018) suggest that concatenating conventional word vectors with contextual word vectors could result in a boost in accuracies. We try out the same by concatenating word2vec vectors with BERT vectors and observe some improvement in label scores. The results are mentioned in Table 6. BERT layers: We also experiment with vector representations from different layers of BERT. The results are mentioned in Table 7. We find that the 4th layer from the top of our BERT baseline model results in the highest accuracy for the parser. This finding is consistent with the work of Tenney et al. (2019) which suggests that dependencies are better captured between layers 6 and 9. We use the vector representations from 4th layer from the top in all our experiments. 146 BPE vs Inverse-BPE: Byte-pair encoding (Sennrich et al., 2016) segments words from left to right. In Telugu, most grammatical information System Annotation Method UAS LS LAS Baseline Baseline + POS Baseline + POS + suffix Tandon et al, 2017 re-impl This work Intra-chunk Intra-chunk Intra-chunk Intra-chunk Intra-chunk MLP with word MLP with word, POS MLP with word, POS, suffix MLP with word, POS, suffix MLP using BERT 84.56 88.90"
2020.coling-main.552,W17-3006,0,0.0162298,"u et al., 2018; Lima et al., 2018; Mathew et al., 2019; Finkelstein et al., 2018) have presented basic statistical analysis of data extracted from Gab. Recently, Qian et al. (2019) presented a dataset of 33,776 posts on Gab annotated on binary labels hate/non-hate. While some papers have focused on racism versus sexism (Badjatiya et al., 2017), others have focused on sarcasm, cyber-bullying etc. (Founta et al., 2019). Initial works in this area focused on feature engineering based methods. With the emergence of deep learning, most of the recent works (Founta et al., 2019; Serr`a et al., 2017; Park and Fung, 2017) have relied on deep learning techniques for abuse detection. To the best of our knowledge, there is no publicly available corpus or prediction system which focuses on fine-grained abusive language classification across all three tasks: prediction of abuse presence, abuse severity prediction and abuse target prediction. 3 Abuse Severity and Targets 3.1 Abuse Severity Anti-Defamation League6 presents a pyramid of hate from a sociology perspective (also supported by Sandu and Lyamouri-Bajja (2018)). Although the behaviors at each level of the hate pyramid negatively impacts individuals and group"
2020.coling-main.552,D14-1162,0,0.0840558,", hes a, he was, a jew, she was, he has, illegal alien they are, the jews, the left, jews are, these people, white people, they will, the US, all of, all the Table 2: Frequent unigrams and bigrams for each of the abuse severity and abuse target classes. Prediction Results: We experiment with multiple statistical ML methods (Support Vector Machines (SVM), XGBoost and Logistic Regression (LR)) using TF-IDF features. We also trained two Deep Learning based models: (1) Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) using transfer learning and (2) GloVe-based (Pennington et al., 2014) Long Short Term Memory (Hochreiter and Schmidhuber, 1997) networks (referred as GloVe+LSTM). With BERT, we use an additional 2-layer multi-layer Perceptron (MLP) for classification with a dropout value of 0.2.We trained both the DL networks using Adam optimizer (Kingma and Ba, 2014). Table 3 shows 5-fold cross validation accuracy (micro F1) and macro F1 for each of the methods. We observe that our BERT based model outperforms other methods with SVM being the best out of the ML models. Classifier SVM XGBoost LR BERT GloVe+LSTM Presence Macro F1 Micro F1/Acc 0.7277 ± 0.0112 0.7279 ± 0.0113 0.71"
2020.coling-main.552,D19-1482,0,0.0200006,"er web communities like 4chan and Whisper (Hine et al., 2017; Silva et al., 2016). But web communities differ from each other through subtleties in language and demographic differences. Gab poses an altogether different challenge as it differs from older web groups primarily in its use of online communities to congregate, organize, and disseminate information in weaponized form (Marwick and Lewis, 2017). Some previous papers (Zannettou et al., 2018; Lima et al., 2018; Mathew et al., 2019; Finkelstein et al., 2018) have presented basic statistical analysis of data extracted from Gab. Recently, Qian et al. (2019) presented a dataset of 33,776 posts on Gab annotated on binary labels hate/non-hate. While some papers have focused on racism versus sexism (Badjatiya et al., 2017), others have focused on sarcasm, cyber-bullying etc. (Founta et al., 2019). Initial works in this area focused on feature engineering based methods. With the emergence of deep learning, most of the recent works (Founta et al., 2019; Serr`a et al., 2017; Park and Fung, 2017) have relied on deep learning techniques for abuse detection. To the best of our knowledge, there is no publicly available corpus or prediction system which foc"
2020.coling-main.552,N16-2013,0,0.0840659,"Missing"
2020.coling-main.587,H05-1073,0,0.363413,"Missing"
2020.coling-main.587,C10-1021,0,0.0757812,"Missing"
2020.coling-main.587,P14-1029,0,0.0263492,"Missing"
2020.conll-1.10,N19-1423,0,0.00687708,"nt and test set respectively, and follow the 5-fold cross validation procedure to capture both classes properly in each case. Experiments To get representations of the word and its context, we experiment with both static and contextual types of word embeddings. For the former, we choose state-of-the-art fastText (FT) embeddings (Bojanowski et al., 2016) as they are able to provide representations of rare words and non words that might be frequent in movie dialogues. For the latter, we use BERT (Bidirectional Encoder Representations from Transformers) base uncased word-piece model for English (Devlin et al., 2019) as it currently provides the most powerful word embeddings taking into account a large left and right context. For the first subtask, we take word embeddings for the one-anaphora candidate and its context; and for the second subtask, we take word embeddings for the antecedent candidate, the gold one-anaphora vector from the annotations and their context. This way, we are able to evaluate the performance of both the subtasks separately. For fastText, we use pretrained embeddings and sumpool the embeddings of the given word and its context to obtain a single vector that we employ for training o"
2020.conll-1.10,P97-1051,0,0.484148,"ng 921 oneanaphora marked in our annotated dataset as our positive set. For the negative set, we take an equal number of sentences from BNC that contain instances of one other than one-anaphora. Hence, our data size becomes 1824 sentences. We perform a standard 70-10-20 split to obtain the train, development and test set respectively, and follow the 5-fold cross validation procedure to capture both classes properly in each case. 5.2 Selecting Antecedents There is evidence that parallelism in discourse can be applied to resolve possible readings for anaphoric entities and reference phenomenon (Hobbs and Kehler, 1997). Linguistic research also shows structural similarities between antecedent and anaphoric clauses (Luperfoy, 1991; Halliday and Hasan, 1976). An antecedent selection procedure can possibly benefit from capturing this similarity. 5.2.1 Task Description This subtask involves selecting the right antecedent for one-anaphora, if it can be resolved. Formally, in a given context c, for an instance of one-anaphora anai , and the antecedent candidate antj ; the task of antecedent selection can be defined as follows: f (ant j , anai , c) → − {0, 1} where 1 denotes that the antecedent candidate antj is t"
2020.conll-1.10,D15-1162,0,0.0253252,"Missing"
2020.conll-1.10,R19-1063,1,0.683128,"Missing"
2020.conll-1.10,2020.lrec-1.5,1,0.679025,"All the officers wore hats so Joe wore one too. The problem here is that the occurrence such as in (7) is not an anaphoric noun; it is the determinative anaphor. Note that the plural form of this element is some, and not ones. Further, the constituent whose repetition this one word avoids is not hats, but the entire NP a hat. In ellipsis theory, this determinative one word here is not one-anaphora, but the licensor or trigger of an elided noun. Detection and resolution of this determinative one anaphor has actually been carried out in a part of our previous computational research on ellipsis (Khullar et al., 2020, 2019) Right from Baker (1978), the traditional linguistic literature on one-anaphora and noun ellipsis too has confused between the noun and determiner uses of the word one, using them interchangeably in discussions and analysis. The faulty understanding on this phenomenon in earlier syntactic discourse, 134 No. 1. POS String Template Determiner – Adjective – “one” 2. Determiner – (Adverb)+ – Adjective – “one” 3. Determiner – “one” – Preposition 4. Determiner – “one” – Gerund/Participle 5. Determiner – “one” –(Punct) Complementizer Example Sentences from BNC Her idea of the value of art crit"
2020.conll-1.10,P13-1162,0,0.0356386,"observed in the way these forms inflect (morphology), behave in a sentence (syntax) and impart meaning (semantics) (Payne et al., 2013). Previous efforts to classify the word one in English involve classification based on different functions of the word in discourse– numeric, partitive, anaphoric, generic, idiomatic, and unclassifiable; and in terms of the type of antecedent the anaphoric one takes– a kind, a set, an individual instance (Payne et al., 2013; Gardiner, 2003; Luperfoy, 1991; Dahl, 1985). This scheme has been extended for classification of other sense anaphoric relations as well (Recasens et al., 2013). This distinction clubs closely related types like numeric and partitive (both are determinative, roughly mean ”1”) to different classes. It also treats the regular count noun anaphora and determinative anaphora together as the anaphoric class. This makes the previous research miss important underlying linguistic generalisations in these forms. In syntactic literature, one-anaphora refers to an anaphoric instance of the word one, where its syntactic properties resemble that of a count noun (Payne et al., 2013; HuddlestonRodnry and Pullum, 2005). Like an En133 One in English Regular, thirdPron"
2020.conll-1.10,W16-0701,0,0.174125,"as Machine Translation (MT) and Question Answering (QA). To the best of our knowledge, the earliest computational approach to one-anaphora detection and resolution comes from Gardiner (2003), who presented several linguistically-motivated heuristics to distinguish one-anaphora from other non-anaphoric uses of one in English. For the resolution task, she used web search to select potential antecedent candidates. The second seminal work comes from Ng et al. (2005) that uses Gardiner’s heuristics as features to train a Machine Learning (ML) model. The most recent work on one-anaphora comes from Recasens et al. (2016) where it has been treated as Getting to Know Every One English has three distinct lexemes spelled as one– the regular third person indefinite pronoun, the indefinite cardinal numeral (determinative) and regular common count noun. There is no visible difference in their orthographic base form. However, they are totally different with respect to their morphological, syntactic, and semantic properties. On the surface, this difference can be observed in the way these forms inflect (morphology), behave in a sentence (syntax) and impart meaning (semantics) (Payne et al., 2013). Previous efforts to"
2020.finnlp-1.12,M95-1012,0,0.479562,"challenge which contained 6 noisy financial PDF documents for training and 2 documents used for testing. The data consisted json files for each document with the start and end indices of each class namely - sentences, lists and items, along with the complete document text available as a single string. The coordinates of the starting and the ending characters for each of the elements in the class were also Related Work Traditionally the task of SBD has been solved using heuristics and rules based on the regular grammar. One of the popular approaches presented by Alembic information extraction [2] built an extensive regular-expression-based approach to solve 76 #Docs #Characters #Tokens #Sentences #Lists #Items #OOV Tokens Train data 5 1,322,767 290,092 7,282 207 843 1,079 Val data 1 169,546 39,816 788 42 268 248 Test data 2 558,611 141,138 2,450 69 332 443 potential split token is a positive sentence split token or not. Domain specific documents contain several words which are not present in the pretrained open-domain word embeddings. As a result, the POS embeddings helped us overcome the ambiguity caused due to the out of vocabulary words while keeping the model computationally less"
2020.finnlp-1.12,J02-3002,0,0.228547,"a simple, fast and robust approach to identify structure in their documents. Keywords: Sentence Boundary Detection, NLP, Deep Learning, LSTM, Sentence overlap, Token classifier, Document structure identification, Attention mechanism 1 Introduction A sentence forms the basic unit of text documents which are used for a wide variety of tasks in Natural Language Process∗ † Equal Contribution. Listing order is random. Contact Author 75 Proceedings of the Second Workshop on Financial Technology and Natural Language Processing this problem. There have been other rule-based approaches by [3], [4] and [5]. Palmer et al.[6], however, recognized the shortcomings of the rule based approaches which were problem statement specific and required large manual effort. Hence they developed the Satz system which predicted if any punctuation mark was a sentence split point or not. They were among the first to develop machine learning based approach for solving this problem and since then many machine learning based approaches by [7], [8], [9], [10] and [11] approaches. Recently deep learning tools [12] have been used to solve this problem which have been able to produce state of the art results. Many of t"
2020.finnlp-1.12,J97-2002,0,0.085161,"robust approach to identify structure in their documents. Keywords: Sentence Boundary Detection, NLP, Deep Learning, LSTM, Sentence overlap, Token classifier, Document structure identification, Attention mechanism 1 Introduction A sentence forms the basic unit of text documents which are used for a wide variety of tasks in Natural Language Process∗ † Equal Contribution. Listing order is random. Contact Author 75 Proceedings of the Second Workshop on Financial Technology and Natural Language Processing this problem. There have been other rule-based approaches by [3], [4] and [5]. Palmer et al.[6], however, recognized the shortcomings of the rule based approaches which were problem statement specific and required large manual effort. Hence they developed the Satz system which predicted if any punctuation mark was a sentence split point or not. They were among the first to develop machine learning based approach for solving this problem and since then many machine learning based approaches by [7], [8], [9], [10] and [11] approaches. Recently deep learning tools [12] have been used to solve this problem which have been able to produce state of the art results. Many of these approaches ha"
2020.finnlp-1.12,N09-2061,0,0.0498993,"Author 75 Proceedings of the Second Workshop on Financial Technology and Natural Language Processing this problem. There have been other rule-based approaches by [3], [4] and [5]. Palmer et al.[6], however, recognized the shortcomings of the rule based approaches which were problem statement specific and required large manual effort. Hence they developed the Satz system which predicted if any punctuation mark was a sentence split point or not. They were among the first to develop machine learning based approach for solving this problem and since then many machine learning based approaches by [7], [8], [9], [10] and [11] approaches. Recently deep learning tools [12] have been used to solve this problem which have been able to produce state of the art results. Many of these approaches have however been confined to clean texts and have tested their results on WSJ corpus [13] and the Brown Corpus [7]. Azzi et al. [14] presented their solution for detecting sentence boundaries in Noisy text in the financial domain but their solution was limited to detecting sentences and not the lists and items contained within these documents. Our work improves upon this shortcoming to identify the lists"
2020.finnlp-1.12,J06-4003,0,0.014377,"or 75 Proceedings of the Second Workshop on Financial Technology and Natural Language Processing this problem. There have been other rule-based approaches by [3], [4] and [5]. Palmer et al.[6], however, recognized the shortcomings of the rule based approaches which were problem statement specific and required large manual effort. Hence they developed the Satz system which predicted if any punctuation mark was a sentence split point or not. They were among the first to develop machine learning based approach for solving this problem and since then many machine learning based approaches by [7], [8], [9], [10] and [11] approaches. Recently deep learning tools [12] have been used to solve this problem which have been able to produce state of the art results. Many of these approaches have however been confined to clean texts and have tested their results on WSJ corpus [13] and the Brown Corpus [7]. Azzi et al. [14] presented their solution for detecting sentence boundaries in Noisy text in the financial domain but their solution was limited to detecting sentences and not the lists and items contained within these documents. Our work improves upon this shortcoming to identify the lists and"
2020.finnlp-1.12,W15-5938,0,0.0181692,"f the Second Workshop on Financial Technology and Natural Language Processing this problem. There have been other rule-based approaches by [3], [4] and [5]. Palmer et al.[6], however, recognized the shortcomings of the rule based approaches which were problem statement specific and required large manual effort. Hence they developed the Satz system which predicted if any punctuation mark was a sentence split point or not. They were among the first to develop machine learning based approach for solving this problem and since then many machine learning based approaches by [7], [8], [9], [10] and [11] approaches. Recently deep learning tools [12] have been used to solve this problem which have been able to produce state of the art results. Many of these approaches have however been confined to clean texts and have tested their results on WSJ corpus [13] and the Brown Corpus [7]. Azzi et al. [14] presented their solution for detecting sentence boundaries in Noisy text in the financial domain but their solution was limited to detecting sentences and not the lists and items contained within these documents. Our work improves upon this shortcoming to identify the lists and list items inside th"
2020.finnlp-1.12,W19-5512,0,0.0132261,"nual effort. Hence they developed the Satz system which predicted if any punctuation mark was a sentence split point or not. They were among the first to develop machine learning based approach for solving this problem and since then many machine learning based approaches by [7], [8], [9], [10] and [11] approaches. Recently deep learning tools [12] have been used to solve this problem which have been able to produce state of the art results. Many of these approaches have however been confined to clean texts and have tested their results on WSJ corpus [13] and the Brown Corpus [7]. Azzi et al. [14] presented their solution for detecting sentence boundaries in Noisy text in the financial domain but their solution was limited to detecting sentences and not the lists and items contained within these documents. Our work improves upon this shortcoming to identify the lists and list items inside the noisy PDF documents along with the identification of sentences which can be used by any NLP system in their preprocessing step to get state of the art performance. 3 Task Definition The goal for the FinSBD-2 2020 Shared Task [15] is to extract well segmented sentences identifying them by their sta"
2020.finnlp-1.12,N16-1174,0,0.0594122,"= tanh (Ww ht + bw ) (3) αt = (b) Convert these POS tags into their one hot vectors. exp (ut T uw ) Σt αt ht v = Σt αt ht . (c) Pass the previous 7 one hot POS encodings into a forward directional LSTM [17] with the last timestep corresponding to the token just before the potential split token being classified. (4) (5) Here We represents the embedding matrix, xt represents the embedded word, ht represents the hidden state of the LSTM encoder at each timestep t, ut is a word level context vector, αt is the attention weight to given to each word in the input sentence and v is the final sentence [20] vector which captures the information for that sentence. This is followed by a linear layer to classify if the input sentence is a well formed sentence or not. This sentence classifier has been used by us in two ways, namely : (d) Pass the next 7 one hot POS encodings into a backward directional LSTM with the last timestep corresponding to the token just after the potential split token being classified. (e) Concatenate the final hidden states of these two LSTM encoders and pass a linear layer over it to classify if the 77 Document (1) Overcome the shortcomings of the token classifier. (2) Ide"
2020.icon-main.1,D14-1162,0,0.0820899,"Missing"
2020.icon-main.1,E12-2021,0,0.0979068,"Missing"
2020.icon-main.1,C18-1182,0,0.0180063,"020. ©2020 NLP Association of India (NLPAI) reactions given in the patent. 1.1 intellectual property rights or the innovative part of the patent granted resides in the examples contained in the Detailed Description section. This section will be analyzed thoroughly for any novel synthetic route to be non-infringing on existing intellectual property rights. Therefore in the next section, we present the WEAVE4 patents corpus, which focuses exclusively on synthetic procedures in the Examples section. Related work There is a large body of chemical and biomedical NER literature. We refer readers to Yadav and Bethard (2018) and Huang et al. (2020) for a comprehensive survey. We include a summary of the publicly available datasets as follows: Chapati corpus (Grego et al., 2009) is a manually annotated set of 40 patents with 11,162 annotations. The chemical named entities identified were mapped to the Chemical Entities of Biological Interest (ChEBI) database. BioSemantics corpus (Akhondi et al., 2014) is a manually annotated set of patents. This corpus has two sets: First, a harmonized set of 47 patents with 36,537 annotations, and the second set of 198 patents with 400,125 annotations. Besides chemical entity men"
2020.icon-main.1,S18-2021,0,0.0142358,"embeddings had a dictionary size of 6,828,514 and were used for all experiments. 4.2 Model and Hyper-parameters Figure 2 presents the architecture of the NER model proposed by Yadav et al. (2018). The model features Character Bi-LSTM, Word features, Word Bi-LSTM, and Word CRF layer for generating BIO tags for the named entities. The above model was used as is with minor modifications in hyperparameters. The word embeddings size of 200-d was used, train embeddings was set to false, and batch size was set to 25. All other parameters were set to the default values given in the model proposed by Yadav et al. (2018). 4.3 The results validate the linguistic structure of the title and abstract of a patent is very different from that of the Examples section. Hence, when combined with the CHEMDNER-patents corpus, the WEAVE corpus are complementary; without losing precision, we have an increase in the recall of the NER model. This also supports our assertion of the need for a focused dataset covering the Examples section of patents. The combined corpus can perform very close to the state-of-the-art results in chemical NER. This combination also gives us many high-quality annotations 199,763 (100,129 WEAVE + 9"
2020.icon-main.1,W19-4007,0,0.0189945,"in detail and highlight some specific challenges in annotating synthetic chemical procedures with chemical named entities. We make the corpus available to the community to promote further research and development of downstream NLP systems applications. We also provide baseline results for the NER model to the community to improve on. 1 Introduction There is a renewed interest in academia and industry to access the information regarding chemical and chemical reactions currently available in unstructured raw text form in journal publications and patents (Coley et al., 2017; Segler et al., 2018; Mysore et al., 2019) using machine learning. Also, several chemical NER datasets exist. With increasing demand in automated chemical synthesis design and planning novel chemical reactions, 1 https://chemu-patent-ie.github. io/resources/Annotation_Guidelines_ CLEF2020_ChEMU_task1.pdf 1 Proceedings of the 17th International Conference on Natural Language Processing, pages 1–9 Patna, India, December 18 - 21, 2020. ©2020 NLP Association of India (NLPAI) reactions given in the patent. 1.1 intellectual property rights or the innovative part of the patent granted resides in the examples contained in the Detailed Descrip"
2020.icon-main.13,N16-1000,0,0.222736,"Missing"
2020.icon-main.13,A92-1018,0,0.846023,"Missing"
2020.icon-main.13,Y09-1014,0,0.0889024,"Missing"
2020.icon-main.13,W12-5211,0,0.0111336,"d 104 1. Character N-Grams: Character N-Grams are proven to be efficient in the task of classification of text and are language-independent (Majumder et al., 2002). They are helpful when there are misspellings in the text (Cavnar et al., 1994; Huffman, 1995; Lodhi et al., 2002). Group of chars can help in capturing the semantic information. Character N-Grams are especially helpful in cases like code mixed language where there is free use of words, which vary significantly from the standard Kannada-English words. 2. Word N-Grams: Bag of words has been a staple for languages other than English (Jahangir et al., 2012) in tasks like NER and POS. Thus, we use adjacent words as a feature vector to train our model as our word N-Grams. These are also called contextual features. We used Word N-Grams of size 3 in the paper. 3. Common Symbols: It is observed that currency symbols, brackets like ‘(,’ ‘[,’ etc. And other symbols are followed by numeric or some mention, are present in the corpus which direct to symbol tag under Universal POS. Hence, the presence of these symbols is a good indicator of the words before or after them for being a ‘SYM’ tag in POS tagging. 4. Numbers in String: In social media, we see pe"
2020.icon-main.13,N16-1030,0,0.0199155,", among others. When it comes to POS tagging, it has been proven to be better than the tree-based models. 5.2 LSTM Long Short Term Memory (LSTM) is a special kind of RNN architecture that is well suited for classification and making predictions based on time series data. LSTMs are capable of capturing only past information. In order to overcome this limitation Bidirectional LSTMs are proposed where two LSTM networks run in forward and backward directions capturing the context in either directions. 5.3 LSTM-CRF The Bi-LSTM-CRF is a combination of bidirectional LSTM and CRF (Huang et al., 2015; Lample et al., 2016). The Bi-LSTM model can be combined with CRF to enhance recognition accuracy. This combined model of Bi-LSTM-CRF inherits the ability to learn past and future context features from the Bi-LSTM model and use sentence-level tags to predict possible tags using the CRF layer. Bi-LSTM-CRF has been proved to be a powerful 3 Features https://en.wikipedia.org/wiki/Conditionalr andomf ield 104 1. Character N-Grams: Character N-Grams are proven to be efficient in the task of classification of text and are language-independent (Majumder et al., 2002). They are helpful when there are misspellings in the t"
2020.icon-main.13,W16-5811,0,0.0379297,"Missing"
2020.icon-main.13,N13-1039,0,0.0608601,"Missing"
2020.icon-main.13,petrov-etal-2012-universal,0,0.115391,"Missing"
2020.icon-main.13,N16-1159,1,0.901772,"Missing"
2020.icon-main.13,P06-2100,1,0.747825,"Missing"
2020.icon-main.13,P19-2025,1,0.731123,"Missing"
2020.icon-main.13,N03-1033,0,0.092494,"Missing"
2020.icon-main.13,N03-1000,0,0.0720768,"Missing"
2020.icon-main.13,W17-4422,0,0.06079,"Missing"
2020.icon-main.13,D14-1105,0,0.0530081,"Missing"
2020.icon-main.21,Q17-1010,0,0.0264158,"(19) Similar to previous section, uij ∀i, j ∈ [1..H] can then be concatenated horizontally into a single vec0 0 tor u and this u can be used to get the relevance score. 4 Experimental Setup This section describes our datasets, how training and testing were performed and our implementation details. 4.1 Dataset and Learning rule logP (P + |hQ, P + , P − i)) hQ,P + ,P − i ∈ S n = n − i + 1. We also add sentinel vector dφ (Merity et al., 2016) to Uij to allow the query to not attend to any particular clause in the passage: t=m−j+1 X X (21) Hyperparameters In all our experiments, we use FastText (Bojanowski et al., 2017) word embeddings of dimension 300. These FastText embeddings are trained on all queries and passages from the training set, we freeze these embeddings during the training. All the other parameters of the model are initialized using an uniform distribution U (−0.01, 0.01). The number of filters in convolution layers is set to 300. We only experiment with uni and bi-grams i.e, H = 2. We use the BiLSTMs with 2 layers and hidden sizes of 512 with dropout of 0.2 (Srivastava et al., 2014) between the layers. ADAM optimizer (Kingma and Ba, 2014) with initial learning rate of 0.001, β1 = 0.9, β2 = 0.9"
2020.icon-main.21,D16-1058,0,0.0396848,"ated question answering systems (QA), etc., The typical answer extraction process in these systems consists of two main phases. The first phase is ranking passages from the collection that most likely contain the answers. The second phase is extracting answers from these passages. The performance of first phase significantly impact the performance of extracting answers and the performance of the overall system. Thus it is important for a QA system to effectively rank passages. Attention mechanisms have shown tremendous improvements in the deep learning based NLP models (Bahdanau et al., 2014; Wang et al., 2016; Yang et al., 2016; Lu et al., 2016; Vaswani et al., 2017). Attention allows the model to dynamically focus only on certain parts of the input that helps in performing the task at hand effectively. Coattentions (Xiong et al., 2016) are class of attention mechanisms which can be applied on text matching problems. They proved to be highly effective as they enables the learning to attend based on computing word level affinity scores between two texts thus helping in effectively deciding the relevance between them. This paper builds on previous work on coattention mechanism (Alaparthi, 2019) (we"
2020.icon-main.21,N16-1174,0,0.0561987,"ring systems (QA), etc., The typical answer extraction process in these systems consists of two main phases. The first phase is ranking passages from the collection that most likely contain the answers. The second phase is extracting answers from these passages. The performance of first phase significantly impact the performance of extracting answers and the performance of the overall system. Thus it is important for a QA system to effectively rank passages. Attention mechanisms have shown tremendous improvements in the deep learning based NLP models (Bahdanau et al., 2014; Wang et al., 2016; Yang et al., 2016; Lu et al., 2016; Vaswani et al., 2017). Attention allows the model to dynamically focus only on certain parts of the input that helps in performing the task at hand effectively. Coattentions (Xiong et al., 2016) are class of attention mechanisms which can be applied on text matching problems. They proved to be highly effective as they enables the learning to attend based on computing word level affinity scores between two texts thus helping in effectively deciding the relevance between them. This paper builds on previous work on coattention mechanism (Alaparthi, 2019) (we call it as naive co"
2020.icon-main.21,N16-1000,0,0.211311,"Missing"
2020.isa-1.10,P11-2023,0,0.0369714,"zero-copula and participial constructions, so as to make it less ambiguous for annotators. 1 2. Related Work In this section, we introduce some of the work done in event detection in low resource and morphologically rich languages, with a focus on TimeML event extraction, or event representation in Indian languages. TimeML was introduced by Pustejovsky et al. (2003) as a mechanism of recognizing, annotating, classifying and representing events in text for the purpose of question answering. TimeML has been used in event detection across languages such as Italian (Caselli et al., 2011), French (Bittar et al., 2011), Romanian (For˘ascu and Tufis¸, 2012), and Spanish (Saurı, 2010). Of course, corpora annotated with TimeML events have often been done alongside the detection of other temporal information such as time expressions, temporal links and other notions. For languages which have syntactic structures that vary significantly from English, event detection is used as an introductory task and the definition of an event is modified to be true to the syntactic structure of the language. Examples of this include event detection in Turkish (Seker and Diri, 2010), Hindi (Goud et al., 2019), Hungarian (Subecz"
2020.isa-1.10,W11-0418,0,0.0307992,"has been modified to adapt the zero-copula and participial constructions, so as to make it less ambiguous for annotators. 1 2. Related Work In this section, we introduce some of the work done in event detection in low resource and morphologically rich languages, with a focus on TimeML event extraction, or event representation in Indian languages. TimeML was introduced by Pustejovsky et al. (2003) as a mechanism of recognizing, annotating, classifying and representing events in text for the purpose of question answering. TimeML has been used in event detection across languages such as Italian (Caselli et al., 2011), French (Bittar et al., 2011), Romanian (For˘ascu and Tufis¸, 2012), and Spanish (Saurı, 2010). Of course, corpora annotated with TimeML events have often been done alongside the detection of other temporal information such as time expressions, temporal links and other notions. For languages which have syntactic structures that vary significantly from English, event detection is used as an introductory task and the definition of an event is modified to be true to the syntactic structure of the language. Examples of this include event detection in Turkish (Seker and Diri, 2010), Hindi (Goud et"
2020.isa-1.10,forascu-tufis-2012-romanian,0,0.0550548,"Missing"
2020.isa-1.10,P08-1030,0,0.0563699,"oogle.com/drive/folders/ 11ZXpP4mQcDcM91SKHiSNEtWi_mAkXku7 https://www.ethnologue.com/language/kan 88 (2018) has provided a forum dedicated to social media event extraction for Indian languages. Deep learning methods have also been used for a few Indian languages such as Hindi, Tamil and Malayalam (Kuila and Sarkar, 2017). However, these events are based on the ACE definition and analysis of events, which does not consider all event predicates (Ahn, 2006), and views event analysis solely as a task in semantic prediction, without the explicit demarcation and analysis of the surrounding syntax (Ji and Grishman, 2008). 3. Since Kannada has only one verb per sentence, relative clauses are converted into adjectival constructions, which describe the verb in the relative clause as a description of the subject of the main verb. Therefore, the sentence ”Arjun came to town and went to the festival” can not be translated into Kannada directly. There is no possible mechanism to represent this sentence, other than the inclusion of an adverbial clause to the coordinating verb that occurs semantically prior to the main verb (i.e. is meant to take place before the main verb). This implies a general notion of sequential"
2020.isa-1.10,W15-0809,0,0.0173445,"te the representation of events. We begin by considering the notion of a TimeML event. According to Pustejovsky et al. (2003) and Saur´ı et al. (2006), TimeML defines an event as a cover term for situations that happen or occur, as well as predicates in which something obtains or holds true. Adopting Goud et al. (2019)’s definition, we consider an event mention as the textual span expected to provide complete information about an event, such as tense, aspect, modality and negation. We also consider the event nugget to be the semantically meaningful unit that expresses the event in a sentence (Mitamura et al., 2015). Kannada is a free word order, morphologically rich language. However, by convention, verbs usually occur at the end of the sentence. Passive voice is rare. The subject often occurs in nominative case, the object in dative. There are a few primary notions of Kannada syntax which are crucial to event annotation. These include: • Kannada employs tenseless negative forms (Lindblom, 2014). Negative forms are analytically represented by a single functional negative term. While there are no semantically negative words in Kannada, a single functional negative form is morphaffixed onto the finite ver"
2020.isa-1.10,J12-2001,0,0.0367656,"marking time, duration, telicity, durativity as well as case relations (Pustejovsky, 1991). Therefore, the first step in the generalized understanding of events in a morphologically rich language is the isolation of inflections that provide tense, aspect and modality information. The heterogeneity of the markers provides the various possible inflections (and irregular constructions if any) in which an event can take place. While in most language tense and aspect are fairly rigorous (Giorgi and Pianesi, 1997), the modality of verbs and verbal predicates need to be analyzed on a granular level. Morante and Sporleder (2012) presents a thorough study into annotation and corpus linguistics into the role of modality and negation as extra-propositional aspects. Indeed, in event annotation, both negation and modality play a role in the complete description of an event. However, languages vary in their representation of modal verbs and negative polarity, and therefore, development of guidelines for these event features becomes a language specific problem. Indeed, while the guidelines developed and challenges faced in Section 4. are to be seen, if development in event detection takes place in Telugu, Tamil and other Dr"
2020.isa-1.10,E12-2021,0,0.128366,"Missing"
2020.isa-1.2,pustejovsky-etal-2010-iso,0,0.0343886,"tions such as question answering and summarization systems. The detection of events, states, temporal expressions and their relations provides a rich source of temporal information, and acts as the representation of real world information in text. This has two-fold implications, first, that the representation mechanism depends on the syntactic and semantic properties of the language, and second, that in order to create systems that use this information, large amounts of annotated data are a prerequisite. An attempt towards solving the issue of disparate representations was made by ISO-TimeML (Pustejovsky et al., 2010), by developing an international standard based on the earlier, highly popular event annotation framework known as TimeML (Pustejovsky et al., 2003a). ISO-TimeML is an inter-operable semantic framework for linguistic annotation of temporal expressions such as events (e.g. occurrences and happenings) and time expressions (e.g. mentions of days, dates and times). The international standard had been created such that the annotation framework could be applied across languages extensively. The issue of training data for large systems was solved by creating large annotated corpora based on the preva"
2020.isa-1.2,W11-3603,0,0.0164698,"news articles and the remaining 50 were short fiction stories. We collected these news articles from Navbharat Times3 , a national Hindi daily newspaper with over 2 million copies circulated nationwide. The distribution of these scraped articles can be found in Table 2. The short stories are by Premchand, who is a renowned Hindi author4 . The addition of these articles will allow the models trained on the Hindi TimeBank to be more reliable in detecting events, states, and temporal expressions in Hindi text. For these 200 articles, they were first tokenized using a freely available tokenizer (Reddy and Sharoff, 2011) 5 and then the identification of both events and states were done by 4 annotators in batches of 50 articles over 4 rounds. Large inter-annotator disparity was found between annotators for reporting verbs with no participating entity, due to which those constructions were removed from the purview of event and state annotation. 5.2. Description Word Identity Part-of-Speech Bi-gram and tri-gram features Beginning Of Sentence Current Word is part of a TIMEX tag Table 1: CRF Features This resulted in a corpus of 1000 articles with event and state phrase boundaries identified and classified. 5.3. T"
2020.isa-1.2,E12-2021,0,0.118374,"Missing"
2020.isa-1.2,C12-1179,0,0.0220076,"ta-TimeBank is as much as possible compliant with the TIDES TIMEX22 annotation. In the Romanian (Forascu and Tufis, , 2012) and Spanish (Saurı and Badia, 2012) TimeBanks, the authors opted to indicated whether an EVENT is a state (with the ‘class’ attribute having the value ‘STATE’), instead of using the attribute ‘type’ to indicate if the EVENT is a state, a process or a transition. The Portuguese TimeBank (Costa and Branco, 2012) uses the same guidelines as the English TimeBank, and use a combination of the Portuguese OpenWordNet and temopral-aware systems. Finally, in the Persian TimeBank (Yaghoobzadeh et al., 2012), gerund phrases, known as “esm-e masdar”, must always be annotated as events, even when they represent generic events. Furthermore, the authors also consider objective deverbal adjectives in PersTimeML. Syntactically, Persian TimeBank differs from ISO-TimeML in the way that all the tokens part of an event are marked under the same event ID irrespective of whether they are consecutive or not. 3. 1. hai is Due to the lack of expletive subjects, the verb ”kahA” can not be attributed to any entity. 3.2. Time Expressions Time expressions are defined as a span of text which denote a specific time,"
2020.isa-1.2,J86-2003,0,0.0789342,"Missing"
2020.isa-1.2,P11-2023,0,0.0147949,"nsional understanding of events based on the change in the properties of entities, certain reporting verbs with sentential predicates are not considered events if they do not contain a participating entity. Hindi allows subject ellipsis constructions, therefore those verbs do not contain any entities, and are therefore not annotated. For example: Related Work TimeBanks have been introduced for multiple languages after English. These TimeBanks were developed after fundamental additions and modifications to ISO-TimeML guidelines for language specific syntactic properties. In the French TimeBank(Bittar et al., 2011), the authors propose that those verbs be tagged as modal since modality is expressed by fully inflected verbs. Furthermore, the authors also provide a way of capturing the difference between support verb constructions with a neutral aspectual value (mener une attaque (carry out an attack)) and those with an inchoative aspectual value. The Italian TimeBank(Caselli et al., 2011) focuses on the EVENT and TIMEX3 tag and modifies their properties to suit Italian. The main difference with regards to the EVENT tag is in the tag attribute list and attribute values. The TIMEX tag used in the Ita-TimeB"
2020.isa-1.2,W11-0418,0,0.0286188,"en introduced for multiple languages after English. These TimeBanks were developed after fundamental additions and modifications to ISO-TimeML guidelines for language specific syntactic properties. In the French TimeBank(Bittar et al., 2011), the authors propose that those verbs be tagged as modal since modality is expressed by fully inflected verbs. Furthermore, the authors also provide a way of capturing the difference between support verb constructions with a neutral aspectual value (mener une attaque (carry out an attack)) and those with an inchoative aspectual value. The Italian TimeBank(Caselli et al., 2011) focuses on the EVENT and TIMEX3 tag and modifies their properties to suit Italian. The main difference with regards to the EVENT tag is in the tag attribute list and attribute values. The TIMEX tag used in the Ita-TimeBank is as much as possible compliant with the TIDES TIMEX22 annotation. In the Romanian (Forascu and Tufis, , 2012) and Spanish (Saurı and Badia, 2012) TimeBanks, the authors opted to indicated whether an EVENT is a state (with the ‘class’ attribute having the value ‘STATE’), instead of using the attribute ‘type’ to indicate if the EVENT is a state, a process or a transition. T"
2020.isa-1.2,costa-branco-2012-timebankpt,0,0.0287583,"d TIMEX3 tag and modifies their properties to suit Italian. The main difference with regards to the EVENT tag is in the tag attribute list and attribute values. The TIMEX tag used in the Ita-TimeBank is as much as possible compliant with the TIDES TIMEX22 annotation. In the Romanian (Forascu and Tufis, , 2012) and Spanish (Saurı and Badia, 2012) TimeBanks, the authors opted to indicated whether an EVENT is a state (with the ‘class’ attribute having the value ‘STATE’), instead of using the attribute ‘type’ to indicate if the EVENT is a state, a process or a transition. The Portuguese TimeBank (Costa and Branco, 2012) uses the same guidelines as the English TimeBank, and use a combination of the Portuguese OpenWordNet and temopral-aware systems. Finally, in the Persian TimeBank (Yaghoobzadeh et al., 2012), gerund phrases, known as “esm-e masdar”, must always be annotated as events, even when they represent generic events. Furthermore, the authors also consider objective deverbal adjectives in PersTimeML. Syntactically, Persian TimeBank differs from ISO-TimeML in the way that all the tokens part of an event are marked under the same event ID irrespective of whether they are consecutive or not. 3. 1. hai is"
2020.isa-1.2,forascu-tufis-2012-romanian,0,0.0591803,"Missing"
2020.isa-1.2,W16-5706,0,0.0604124,"Missing"
2020.lrec-1.5,R19-1063,1,0.477105,"enon and the corpus prepared by (Nielsen, 2005) containing 1500 cases of VPE from parts of Wall Street Journal (WSJ), British National Corpus (BNC) and Brown Corpus. Coming to computational work on noun ellipsis, there is a rule based system that detects noun ellipsis using syntactic constraints on licensors of ellipsis and resolves them by matching Part-of-Speech (POS) tag similarity between the licensor of ellipsis and the modifier of the antecedent, and fine tunes these syntactic rules on a small curated dataset that contains 234 instances of noun ellipsis along with some negative samples (Khullar et al., 2019). Although this dataset curates examples of noun ellipsis from Universal Dependency (UD) treebank (Silveira et al., 2014) and ParCorFull: a Parallel Corpus Annotated with Full Coreference (Lapshinova-Koltunski et al., 2018) also, a majority of these examples are actually from linguistic textbooks which may not fully represent the real world occurrence of this phenomenon. There is another corpus called the GECCo (German-English Contrasts in Cohesion) Corpus (Menzel and Lapshinova-Koltunski, 2014) that contains manual annotations on nominal, verbal and clausal ellipsis presented as cohesive devi"
2020.lrec-1.5,L18-1065,0,0.0730468,"s, there is a rule based system that detects noun ellipsis using syntactic constraints on licensors of ellipsis and resolves them by matching Part-of-Speech (POS) tag similarity between the licensor of ellipsis and the modifier of the antecedent, and fine tunes these syntactic rules on a small curated dataset that contains 234 instances of noun ellipsis along with some negative samples (Khullar et al., 2019). Although this dataset curates examples of noun ellipsis from Universal Dependency (UD) treebank (Silveira et al., 2014) and ParCorFull: a Parallel Corpus Annotated with Full Coreference (Lapshinova-Koltunski et al., 2018) also, a majority of these examples are actually from linguistic textbooks which may not fully represent the real world occurrence of this phenomenon. There is another corpus called the GECCo (German-English Contrasts in Cohesion) Corpus (Menzel and Lapshinova-Koltunski, 2014) that contains manual annotations on nominal, verbal and clausal ellipsis presented as cohesive device in a total of fourteen written and spoken registers of English and German. The corpus is not publicly available and the number of annotated noun ellipses are not enough for training machine learning models. In this paper"
2020.lrec-1.5,H94-1020,0,0.24378,"dentity in the number feature with the ellipsis as they provide the lexico-grammatical content necessary for the resolution of the noun ellipsis. for example using auxiliary verbs and modals in case of VPE detection (McShane and Babkin, 2016). We use the CLAWS5 Part-of-Speech Tagger for English to semi-automatically add POS tags to every licensor of the noun ellipsis. Since most parsers give erroneous output sometimes for sentences containing ellipsis (Menzel, 2017), we verify the tags manually before annotation. We choose the CLAWS5 tagset over other tagsets such as the Penn Treebank tagset (Marcus et al., 1994) because it is more fine grained and lets us assign distinct tags for different grammatical categories of licensors that we wish to mark. For instance, articles, demonstrative determiners and general determiners are all represented by the DT (determiner) tag in the Penn Treebank tagset, but CLAWS5 tagset has separate tags for these. This distinction is potentially very useful for the noun ellipsis detection and resolution as we show in the section on corpus summary. (b) We select the maximal antecedent boundary. For example, in (14), the elided noun can be resolved from just the antecedent lov"
2020.lrec-1.5,2016.lilt-13.1,0,0.479162,"ing towards the dialogue. We write e inside square brackets to mark the ellipsis site, enclose the elliptical noun phrase inside square bracket with the subscript NP, and write the antecedent in bold font. We use the same convention to present examples of ellipsis from the movie dialogues throughout this paper. 34 approach to generated patterns for VPE resolution (Hardt, 1998), the domain independent detection and resolution of VPE using machine learning methods (Nielsen, 2003), automatically parsed text (Nielsen, 2004), sentence trimming methods (McShane et al., 2015), linguistic principles (McShane and Babkin, 2016), improved parsing techniques that encode elided material dependencies for reconstruction of sentences containing gapping (Schuster et al., 2018), etc. More recently, complex Neural Networks like Transformers and Multilayer Perceptrons (MLP) have been used to achieve promising results on both VPE detection and resolution tasks (Zhang et al., 2019). This has been possible because of the availability of linguistic resources on VPE such as the annotated corpus for the analysis of VPE in English (Bos and Spenader, 2011) with 487 cases of VPE plus related phenomenon and the corpus prepared by (Niel"
2020.lrec-1.5,L18-1556,0,0.0247544,"heme that does not modify the original corpus text. Using these annotations, we present statistical insights on noun ellipsis and baseline results on the detection and resolution tasks by training a simple classifier. This corpus will open up further avenues for computational work on noun ellipsis. 3. In pro-drop languages, the subject of the main sentence can also be dropped. However, in English, the subject, for instance, John in (2) needs to be overtly present. Subject ellipsis has been studied in detail in Chinese (Yeh and Chen, 2019a; Yeh and Chen, 2019b) and Japanese (Iida et al., 2007; Asao et al., 2018; Chen, 2016). There is some evidence of the phenomenon being used to achieve certain interactional functions in ordinary conversational settings by English speakers. For example, English speakers sometimes delete subjects in informal speech or conversational settings as in (3), although the sentence is ungrammatical (Oh, 2005). 3. ? ∅ Told you so. The second related phenomenon is one-anaphora, in which the elided noun is replaced by a phonologically overt pro form inside the noun phrase. There have been studies on one-anaphora in English, for instance a data-driven investigation of one-anapho"
2020.lrec-1.5,W11-0609,0,0.0309333,"Missing"
2020.lrec-1.5,D16-1179,0,0.0486813,"Missing"
2020.lrec-1.5,P92-1002,0,0.712761,"), in cognitive sciences (Kim et al., 2019) and language acquisition studies (Hyams et al., 2017; Lindenbergh et al., 2015; Goksun et al., 2010; Wijnen et al., 2003). In recent years, ellipsis resolution has been identified as an important Natural Language Processing (NLP) task for improving accuracy of information retrieval, event extraction, dialogue systems, etc (Hansen and Sogaard, 2019; Dean et al., 2016). One of the earliest computational approaches on ellipsis resolution involved the detection of Verb Phrase Ellipsis (VPE) instances in the Penn Treebank using a syntactic pattern match (Hardt, 1992). Most of the work on ellipsis resolution since then has focused on VPE and related phenomenon such as gapping, sluicing and doso anaphora, for instance, a transformation learning-based 1. 〈 L3315 m2 Jordy 〉 Do you have coffee? 〈 L3316 m2 Daphne 〉 In the kitchen 〈 L3316 m2 Jordy 〉 I will make [NP some [e]] for us. The first string in the angular brackets denotes the unique label given to a single turn in the dialogue in the corpus, m2 is the movie number (starting from m0) and Jordy is the name of the character contributing towards the dialogue. We write e inside square brackets to mark the el"
2020.lrec-1.5,P97-1051,0,0.871182,"sion (Chen, 2016), ellipsis resolution is deemed as a hard task for Natural Language Processing (NLP) systems. A type of ellipsis is noun ellipsis, where the head noun inside a Noun Phrase (NP) is elliptically omitted. For example, in the following conversation taken from the third movie (m2) of the Cornell Dialogs Dataset (DanescuNiculescu-Mizil and Lee, 2011), the word coffee is elided in the third turn of the dialogue: 2. Previous Work Ellipsis has been discussed fairly well in theoretical linguistics literature (Halliday and Hasan, 1976; Dalrymple et al., 1991; Lobeck, 1995; Lappin, 1996; Hobbs and Kehler, 1997; Hardt, 1999; Johnson, 2001; Merchant, 2004; Frazier, 2008; Chung et al., 2010; Merchant, 2010; Rouveret, 2012; Gunther, 2011; van Craenenbroeck and Merchant, 2013; Park, 2017), in cognitive sciences (Kim et al., 2019) and language acquisition studies (Hyams et al., 2017; Lindenbergh et al., 2015; Goksun et al., 2010; Wijnen et al., 2003). In recent years, ellipsis resolution has been identified as an important Natural Language Processing (NLP) task for improving accuracy of information retrieval, event extraction, dialogue systems, etc (Hansen and Sogaard, 2019; Dean et al., 2016). One of th"
2020.lrec-1.5,C04-1157,0,0.754401,"2 is the movie number (starting from m0) and Jordy is the name of the character contributing towards the dialogue. We write e inside square brackets to mark the ellipsis site, enclose the elliptical noun phrase inside square bracket with the subscript NP, and write the antecedent in bold font. We use the same convention to present examples of ellipsis from the movie dialogues throughout this paper. 34 approach to generated patterns for VPE resolution (Hardt, 1998), the domain independent detection and resolution of VPE using machine learning methods (Nielsen, 2003), automatically parsed text (Nielsen, 2004), sentence trimming methods (McShane et al., 2015), linguistic principles (McShane and Babkin, 2016), improved parsing techniques that encode elided material dependencies for reconstruction of sentences containing gapping (Schuster et al., 2018), etc. More recently, complex Neural Networks like Transformers and Multilayer Perceptrons (MLP) have been used to achieve promising results on both VPE detection and resolution tasks (Zhang et al., 2019). This has been possible because of the availability of linguistic resources on VPE such as the annotated corpus for the analysis of VPE in English (Bo"
2020.lrec-1.5,W16-0701,0,0.205505,"Missing"
2020.lrec-1.5,N18-1105,0,0.0944885,"the subscript NP, and write the antecedent in bold font. We use the same convention to present examples of ellipsis from the movie dialogues throughout this paper. 34 approach to generated patterns for VPE resolution (Hardt, 1998), the domain independent detection and resolution of VPE using machine learning methods (Nielsen, 2003), automatically parsed text (Nielsen, 2004), sentence trimming methods (McShane et al., 2015), linguistic principles (McShane and Babkin, 2016), improved parsing techniques that encode elided material dependencies for reconstruction of sentences containing gapping (Schuster et al., 2018), etc. More recently, complex Neural Networks like Transformers and Multilayer Perceptrons (MLP) have been used to achieve promising results on both VPE detection and resolution tasks (Zhang et al., 2019). This has been possible because of the availability of linguistic resources on VPE such as the annotated corpus for the analysis of VPE in English (Bos and Spenader, 2011) with 487 cases of VPE plus related phenomenon and the corpus prepared by (Nielsen, 2005) containing 1500 cases of VPE from parts of Wall Street Journal (WSJ), British National Corpus (BNC) and Brown Corpus. Coming to comput"
2020.lrec-1.5,silveira-etal-2014-gold,0,0.0501489,"Missing"
2020.lrec-1.5,E12-2021,0,0.0183633,"Missing"
2020.lrec-1.5,O01-1011,0,0.138251,"Missing"
2020.repl4nlp-1.4,N18-1062,0,0.0177164,"hat of w ˆ2 , then w ˆ2 ⊆ w ˆ1 . We find that feature inclusion is closely linked to hyponymy, which we will show in 5.3. 27 Fˆ french english france german spanish british ˆ B isles colonial subcontinent cinema boer canadians ˆ Fˆ  B communaut aise langue monet dictionnaire gascon ~ F french english france german spanish british ~ B scottish american thatcherism netherlands hillier cukcs ~ −B ~ F ranjit privatised tardis molloy isaacs raj We would like to point out that while most methods of introducing asymmetric similarity measures in word2vec account for both the focus and context vector Asr et al. (2018) and provide the asymmetry by querying on this combination of focus and context representations of each word. Our representation, on the other hand, uses only the focus representations (which are a part of the word representations used for downstream task as well as any other intrinsic evaluation), and still provides an innately asymmetric notion of similarity. Table 3: An example of feature difference, along with a possible word2vec analogue (vector difference). French is represented by F and British by B. We see here that set difference capture french words from the dataset, while there does"
2020.repl4nlp-1.4,P16-1187,0,0.0199499,"sitionality of word embeddings. Mikolov et al. (2013b) claims that word embeddings in vector spaces possess additive compositionality, i.e. by vector addition, semantic phrases such as compounds can be well represented. We claim that our representation in fact captures the semantics of phrases by performing a literal combination of the features of the head and modifier word, therefore providing a more robust representation of phrases. We use the English nominal compound phrases from Ramisch et al. (2016). An initial set of experiments on nominal compounds using word2vec have been done before (Cordeiro et al., 2016), where it 3 Our Representation 0.4117 0.4081 0.4912 0.4803 0.4549 0.4091 was shown to be a fairly difficult task for modern non-contextual word embeddings. In order to analyse nominal compounds, we adjust our similarity metric to account for asymmetry in the similarity between the head-word and the modifier, and vice versa. We report performance on two metrics, the Spearman correlation (Spearman, 1987) and Pearson correlation (Pearson, 1920). The results are shown in table 11. The difference in scores for the Pearson and Spearman rank correlation show that word2vec at higher dimensions better"
2020.repl4nlp-1.4,W16-1605,0,0.012158,"r representation of words to create a compositional distributional model of meaning. By providing a categorytheoretic framework, the model creates an inherently compositional structure based on distributional word representations. However, they showed Herbelot and Vecchi (2015) refers to a notion of general formal semantics inferred from a distributional representation by creating relevant ontology based on the existing distribution. This mapping is therefore from a standard distributional model to a set-theoretic model, where dimensions are predicates and weights are generalised quantifiers. Emerson and Copestake (2016, 2017) developed functional distributional semantics, which is a probabilistic framework based on model theory. The framework relies on differentiating and learning entities and predicates and their relations, on which Bayesian inference is performed. This representation is inherently compositional, context dependent representation. Background: Fuzzy Sets and Fuzzy Logic In this section, we provide a basic background of fuzzy sets including some fuzzy set operations, reinterpreting sets as tuples in a universe of finite elements and showing some set operations. We also cover the computation o"
2020.repl4nlp-1.4,W17-6806,0,0.0393134,"Missing"
2020.repl4nlp-1.4,P14-2049,0,0.0197564,"ty is computed using the notion of dot product, analogy and compositionality use vector addition. However, distributional representations of words over vector spaces have an inherent lack of interpretablity (Goldberg and Levy, 2014). Furthermore, due to the symmetric nature of the vector space operations for similarity and analogy, which are far from human similarity judgements (Tversky, 1977). Other word representations tried to provide asymmetric notions of similarity in a noncontextualized setting, including Gaussian embeddings (Vilnis and McCallum, 2014) and word similarity by dependency (Gawron, 2014). However, these models could not account for the inherent compositionality of word embeddings (Mikolov et al., 2013b). Moreover, while work has been done on providing entailment for vector space models by entirely reinterpreting word2vec as an entailment based semantic model (Henderson and Popa, 2016), it requires an external notion of compositionality. Finally, word2vec and GloVe, as such, are meaning conflation deficient, meaning that a single word with all its possible meanings is represented by a single vector (Camacho-Collados and Pilehvar, 2018). Sense representation models in noncontex"
2020.repl4nlp-1.4,D15-1003,0,0.0256751,"t adopted from the vector space directly, it presents a unique perspective of entailment chains for reasoning tasks. Their analysis of inference using fuzzy representations provides interpretability in reasoning tasks. Grefenstette (2013) presents a tenosrial calculus for word embeddings, which is based on compositional operators which uses vector representation of words to create a compositional distributional model of meaning. By providing a categorytheoretic framework, the model creates an inherently compositional structure based on distributional word representations. However, they showed Herbelot and Vecchi (2015) refers to a notion of general formal semantics inferred from a distributional representation by creating relevant ontology based on the existing distribution. This mapping is therefore from a standard distributional model to a set-theoretic model, where dimensions are predicates and weights are generalised quantifiers. Emerson and Copestake (2016, 2017) developed functional distributional semantics, which is a probabilistic framework based on model theory. The framework relies on differentiating and learning entities and predicates and their relations, on which Bayesian inference is performed"
2020.repl4nlp-1.4,P16-2026,0,0.0240678,"per feature (dimension) across all the words. Therefore, increasing the dimensionFinally, we evaluate the compositionality of word embeddings. Mikolov et al. (2013b) claims that word embeddings in vector spaces possess additive compositionality, i.e. by vector addition, semantic phrases such as compounds can be well represented. We claim that our representation in fact captures the semantics of phrases by performing a literal combination of the features of the head and modifier word, therefore providing a more robust representation of phrases. We use the English nominal compound phrases from Ramisch et al. (2016). An initial set of experiments on nominal compounds using word2vec have been done before (Cordeiro et al., 2016), where it 3 Our Representation 0.4117 0.4081 0.4912 0.4803 0.4549 0.4091 was shown to be a fairly difficult task for modern non-contextual word embeddings. In order to analyse nominal compounds, we adjust our similarity metric to account for asymmetry in the similarity between the head-word and the modifier, and vice versa. We report performance on two metrics, the Spearman correlation (Spearman, 1987) and Pearson correlation (Pearson, 1920). The results are shown in table 11. The"
2020.repl4nlp-1.4,J15-4004,0,0.040589,"s.). For the first five, only overall accuracy is shown as overall accuracy is the same as semantic accuracy (as there is no syntactic accuracy measure). For all the others, we present, syntactic, semantic and overall accuracy as well. We see here that we outperform word2vec on every single metric. Similarity and Analogy Similarity and analogy are the most popular intrinsic evaluation mechanisms for word representations (Mikolov et al., 2013a). Therefore, to evaluate our representations, the first tasks we show are similarity and analogy. For similarity computations, we use the SimLex corpus (Hill et al., 2015) for training and testing at different dimensions For word analogy, we use the MSR Word Relatedness Test (Mikolov et al., 2013c). We compare it to the vector representation of words for different dimensions. 5.1.1 20 50 100 200 Category Experiments and Results 5.1 word2vec Table 8: Similarity scores on the SimLex-999 dataset (Hill et al., 2015), for various dimension sizes (Dims.). The scores are provided according to the Spearman Correlation to incorporate higher precision. are clipped at (0, 1] so that the fuzzy representation is consistent. Analogical reasoning is based on the common featur"
2020.repl4nlp-1.4,P99-1004,0,0.540324,"erations. The code can be found at https://github. com/AlokDebnath/fuzzy_embeddings. The code also has a working command line interface where users can perform qualitative assessments on the set theoretic operations, similarity, analogy and compositionality which are discussed in the paper. that in this framework, quantifiers could not be expressed. 2 3 Related Work The representation of words using logical paradigms such as fuzzy logic, tensorial representations and other probabilistic approaches have been attempted before. In this section, we uncover some of these representations in detail. Lee (1999) introduced measures of distributional similarity to improve the probability estimation for unseen occurrences. The measure of similarity of distributional word clusters was based on multiple measures including Euclidian distance, cosine distance, Jaccard’s Coefficient, and asymmetric measures like α-skew divergence. Bergmair (2011) used a fuzzy set theoretic view of features associated with word representations. While these features were not adopted from the vector space directly, it presents a unique perspective of entailment chains for reasoning tasks. Their analysis of inference using fuzz"
2020.repl4nlp-1.4,D15-1200,0,0.0255591,"del (Henderson and Popa, 2016), it requires an external notion of compositionality. Finally, word2vec and GloVe, as such, are meaning conflation deficient, meaning that a single word with all its possible meanings is represented by a single vector (Camacho-Collados and Pilehvar, 2018). Sense representation models in noncontextualized representations such as multi-sense skip gram, by performing joint clustering for local word neighbourhood. However, these sense representations are conditioned on non-disambiguated senses in the context and require additional conditioning on the intended senses (Li and Jurafsky, 2015). In this paper, we aim to answer the question: Can a single word representation mechanism account for lexical similarity and analogy, compositionality, lexical entailment and be used to detect and resolve polysemy? We find that by performing column-wise normalization of word vectors trained using the word2vec skip-gram negative sampling In this paper, we provide an alternate perspective on word representations, by reinterpreting the dimensions of the vector space of a word embedding as a collection of features. In this reinterpretation, every component of the word vector is normalized against"
2020.repl4nlp-1.4,N13-1090,0,0.794713,"stributional representations of words over vector spaces have an inherent lack of interpretablity (Goldberg and Levy, 2014). Furthermore, due to the symmetric nature of the vector space operations for similarity and analogy, which are far from human similarity judgements (Tversky, 1977). Other word representations tried to provide asymmetric notions of similarity in a noncontextualized setting, including Gaussian embeddings (Vilnis and McCallum, 2014) and word similarity by dependency (Gawron, 2014). However, these models could not account for the inherent compositionality of word embeddings (Mikolov et al., 2013b). Moreover, while work has been done on providing entailment for vector space models by entirely reinterpreting word2vec as an entailment based semantic model (Henderson and Popa, 2016), it requires an external notion of compositionality. Finally, word2vec and GloVe, as such, are meaning conflation deficient, meaning that a single word with all its possible meanings is represented by a single vector (Camacho-Collados and Pilehvar, 2018). Sense representation models in noncontextualized representations such as multi-sense skip gram, by performing joint clustering for local word neighbourhood."
2020.repl4nlp-1.4,D14-1162,0,0.100182,"us benchmarks, and explore some of the unique properties including function word detection, detection of polysemous words, and some insight into the interpretability provided by set theoretic operations. 1 Introduction Word embedding is one of the most crucial facets of Natural Language Processing (NLP) research. Most non-contextualized word representations aim to provide a distributional view of lexical semantics, known popularly by the adage ”a word is known by the company it keeps” (Firth, 1957). Popular implementations of word embeddings such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) aim to represent words as embeddings in a vector space. These embeddings are trained to be oriented such that vectors with higher similarities have higher dot products when normalized. Some of the most common methods of intrinsic evaluation of word embeddings include similarity, 24 Proceedings of the 5th Workshop on Representation Learning for NLP (RepL4NLP-2020), pages 24–33 c July 9, 2020. 2020 Association for Computational Linguistics regime, we can indeed represent all the above characteristics in a single representation. We interpret a column wise normalized word representation. We now t"
2020.semeval-1.157,C16-1234,1,0.831821,"ude both the 2D structure or pixels in an image, or the 1D structure of words in a sentence, paragraph, or document. It is therefore, a great starting point to work on processing memes and other images/videos of this sort. It makes sense that the results received when comparing the accuracy of word-level and characterlevel features are similar as the data is mostly concise monolingual text(the given memes predominantly used English). Character-level features generally tend to show promising improvement when the data is bilingual/code-mixed. Systems like sub-word level compositions with LSTMs (Prabhu et al., 2016) to capture sentiment at morpheme level are effective to a certain degree, but are limited by their dependence on abundant data that is beyond which was provided for the task. Given that the amount of data is scarce, such deep learning models or use of transformers would fall short as they require an abundance of data to train accurately. Hyperparameter tuning worked best for unigram or at most bigram parameters. This may have to do with the length of the input text. Models that perform better when the given text input is short in length would perform better when analysing memes due to the inh"
2020.trac-1.3,L18-1226,0,0.31732,"s work is the first attempt to gain an extensive and fine-grained understanding of patterns of aggression and figurative language use when voicing opinion. We present a Hindi-English code-mixed dataset of opinion on the politico-social issue of ‘2016 India banknote demonetisation‘ and annotate it across multiple dimensions such as aggression, hate speech, emotion arousal and figurative language usage (such as sarcasm/irony, metaphors/similes, puns/word-play). Keywords: Social Media, Stance, Opinion, Aggression, Hate Speech, Figurative Language, Emotion 1. Introduction prestige” (also cited by Kumar et al. (2018)). Baron and Richardson (2004) identified some characteristics of aggression as : There has been an explosion in terms of the amount of data generated by users online. Social media and online forums encourage users to share their thoughts with the world resulting in a vast resource of opinion-rich data. This has garnered a lot of attention from the research community as it allows for analyzing the interactions between users as well as their usage of informal language in depth. • Form of behaviour rather than an emotion, motive or attitude. • Visible intention to hurt or harm (may not be physic"
2020.trac-1.3,S16-1064,0,0.0153172,"and is overtly aggressive to both the members of AAP and their leader Arvind Kejriwal. Its author suggests that the opposition leader should die and proceeds to verbally abuse him. 2. Related Work User generated data from social media forums like Twitter has attracted a lot of attention from the research community. Mohammad et al. (2017) and Krejzl et al. (2017) analyzed stance in tweets and online discussion forums respectively. The task of stance detection on tweets at SemEval 2016 (Mohammad et al., 2016) led to targeted interest in the area with contributions from Augenstein et al. (2016), Liu et al. (2016) etc. Aggression and offensive language was the focus of a SemEval 2019 task (Zampieri et al., 2019b) and some of the works on aggression identification are Kumar et al. (2018), Zampieri et al. (2019a). Closely related is detecting hate speech in social media which has been explored by Malmasi and Zampieri (2017), Schmidt and Wiegand (2017), Davidson et al. (2017), Badjatiya et al. (2017) among others. 4. Tweet: ’what if .. Modi Ji says Mitron ,,, kal raat ko zyada ho gayi thi ,,. Kuch nahi badla he.. #NoteBandi’ Translation: ’What if Modi says that he had too much to drink last night and noth"
2020.trac-1.3,malmasi-zampieri-2017-detecting,0,0.070464,"h community. Mohammad et al. (2017) and Krejzl et al. (2017) analyzed stance in tweets and online discussion forums respectively. The task of stance detection on tweets at SemEval 2016 (Mohammad et al., 2016) led to targeted interest in the area with contributions from Augenstein et al. (2016), Liu et al. (2016) etc. Aggression and offensive language was the focus of a SemEval 2019 task (Zampieri et al., 2019b) and some of the works on aggression identification are Kumar et al. (2018), Zampieri et al. (2019a). Closely related is detecting hate speech in social media which has been explored by Malmasi and Zampieri (2017), Schmidt and Wiegand (2017), Davidson et al. (2017), Badjatiya et al. (2017) among others. 4. Tweet: ’what if .. Modi Ji says Mitron ,,, kal raat ko zyada ho gayi thi ,,. Kuch nahi badla he.. #NoteBandi’ Translation: ’What if Modi says that he had too much to drink last night and nothing has really changed. #Demonetisation.’ Tweet 4 makes a sarcastic joke about how Prime Minister Modi might have been joking and hung over while making this sudden announcement. Despite its humorous take, this tweet is non-aggressive and neutral in stance. Domains of verbal aggression, abuse, hate have till now"
2020.trac-1.3,S17-2005,0,0.0322191,"Missing"
2020.trac-1.3,S16-1003,0,0.0332392,"Honorific refering to Narendra Modi (Prime Minister of India) Tweet 3 supports the decision of demonetisation and is overtly aggressive to both the members of AAP and their leader Arvind Kejriwal. Its author suggests that the opposition leader should die and proceeds to verbally abuse him. 2. Related Work User generated data from social media forums like Twitter has attracted a lot of attention from the research community. Mohammad et al. (2017) and Krejzl et al. (2017) analyzed stance in tweets and online discussion forums respectively. The task of stance detection on tweets at SemEval 2016 (Mohammad et al., 2016) led to targeted interest in the area with contributions from Augenstein et al. (2016), Liu et al. (2016) etc. Aggression and offensive language was the focus of a SemEval 2019 task (Zampieri et al., 2019b) and some of the works on aggression identification are Kumar et al. (2018), Zampieri et al. (2019a). Closely related is detecting hate speech in social media which has been explored by Malmasi and Zampieri (2017), Schmidt and Wiegand (2017), Davidson et al. (2017), Badjatiya et al. (2017) among others. 4. Tweet: ’what if .. Modi Ji says Mitron ,,, kal raat ko zyada ho gayi thi ,,. Kuch nahi"
2020.trac-1.3,P18-1017,0,0.0245133,"sions - valence (polarity of emotion) and arousal (intensity of emotion) (Russell and Barrett, 1999). Despite that, many works on emotion classification in text have generally used directly annotated 6 emotion categories (happy, sad, anger, fear, disgust, surprise) instead of first annotating arousal and valence separately before mapping them into emotion categories. We restricted the scope for this project to analyze only for emotion arousal level as emotion valence level is analogous to sentiment. For emotion arousal level, Bradley and Lang (1999) averaged annotations on a 9 point scale and Mohammad (2018) used a Best-Worst scale to obtain fine-grained scores. Similar to the SemEval 2017 task (Rosenthal et al., 2019) for sentiment analysis on Twitter, we use a 5-point ordinal scale (Very Low, Low, Neutral, High, Very High) for emotion arousal level. 4. 9. Tweet: ’kabhi kabhi sher ka shikar karne ke liye bhed (aam janta) ko chara banana padta hai. notebandi’ Data Statistics and Analysis Table 1 presents the tweet level average statistics on the corpus. The dataset tweets contain majorly Hindi language 16 Task Stance Aggression Hate Speech Sarcasm / Irony / Rhetorical Questions Puns / Word-play M"
2020.trac-1.3,D16-1084,0,0.0270748,"Missing"
2020.trac-1.3,W18-1105,1,0.914213,"th its nuances as well as the highly polarizing nature of opinions on the topic. Gafaranga (2007) describes code-mixing as use of linguistic units from different languages in a single utterance or sentence and code-switching as the co-occurrence of speech extracts belonging to two different grammatical systems. Majority of user generated data on social media is code-mixed and consequently, so is our dataset. Code mixed datasets for Hindi-English tweets have been previously created for humor (Khandelwal et al., 2018), sarcasm (Swami et al., 2018a), aggression (Kumar et al., 2018), hate speech (Bohra et al., 2018) and emotion (Vijay et al., 2018). 3. Gloss: ”kale dhan”: black money, ”Arvind Kejriwal”: Leader of opposition political party AAP, ”Notebandi”: demonetisation Tweet 5 was originally classified as a neutral stance. We feel that cases like above can be confidently annotated as favourable to the issue (i.e. favourable to demonetisation). The author rhetorically and sarcastically questions the opinion, intentions and reasons of those against the issue (in this case leader of opposition party). This tweet is also an example of what we consider covert aggression. For aggression annotation, we follo"
2020.trac-1.3,S15-2080,0,0.0238147,"usage of ’Notebandi’ (demonetisation) with ’Nasbandi’ (castration) as shown in the earlier examples, was the most common word-play seen. A third figurative language category of metaphors (and occasionally similes) can also be clearly observed in our corpus. Metaphor identification has been typically treated as a token level or phrase level tagging task (Shutova and Teufel, 2010). To be consistent we other figurative language categories used in this work, we annotated metaphors at the tweet level which was also the annotation level for SemEval 2015 task on figurative language in Twitter data (Ghosh et al., 2015). The following tweet is an example of metaphor usage - 10. Tweet: ’ab itni taklif hai to atmadaah kyo nahi kar lete notebandi k khilf. Delhi walo ko bhi mukti milegi tumse’ Translation: ’If you have such a huge issue with it, why don’t you perform a self-immolation? The people of Delhi would also get freedom from you’ In tweet 10, the author is referring to Arvind Kejriwal who is the leader of opposition party AAP and also the Chief Minister of Delhi (capital of India). The author suggests Kejriwal should kill himself to free the residents of Delhi. In the process of supporting the decision o"
2020.trac-1.3,W17-1101,0,0.0140973,"(2017) and Krejzl et al. (2017) analyzed stance in tweets and online discussion forums respectively. The task of stance detection on tweets at SemEval 2016 (Mohammad et al., 2016) led to targeted interest in the area with contributions from Augenstein et al. (2016), Liu et al. (2016) etc. Aggression and offensive language was the focus of a SemEval 2019 task (Zampieri et al., 2019b) and some of the works on aggression identification are Kumar et al. (2018), Zampieri et al. (2019a). Closely related is detecting hate speech in social media which has been explored by Malmasi and Zampieri (2017), Schmidt and Wiegand (2017), Davidson et al. (2017), Badjatiya et al. (2017) among others. 4. Tweet: ’what if .. Modi Ji says Mitron ,,, kal raat ko zyada ho gayi thi ,,. Kuch nahi badla he.. #NoteBandi’ Translation: ’What if Modi says that he had too much to drink last night and nothing has really changed. #Demonetisation.’ Tweet 4 makes a sarcastic joke about how Prime Minister Modi might have been joking and hung over while making this sudden announcement. Despite its humorous take, this tweet is non-aggressive and neutral in stance. Domains of verbal aggression, abuse, hate have till now been studied in isolation fr"
2020.trac-1.3,shutova-teufel-2010-metaphor,0,0.0414019,"y of figurative language. Similarly, puns and word-play are merged into a single category of figurative language as well and the annotation guidelines were based on the SemEval 2017 task of detecting english puns (Miller et al., 2017). Rhyming usage of ’Notebandi’ (demonetisation) with ’Nasbandi’ (castration) as shown in the earlier examples, was the most common word-play seen. A third figurative language category of metaphors (and occasionally similes) can also be clearly observed in our corpus. Metaphor identification has been typically treated as a token level or phrase level tagging task (Shutova and Teufel, 2010). To be consistent we other figurative language categories used in this work, we annotated metaphors at the tweet level which was also the annotation level for SemEval 2015 task on figurative language in Twitter data (Ghosh et al., 2015). The following tweet is an example of metaphor usage - 10. Tweet: ’ab itni taklif hai to atmadaah kyo nahi kar lete notebandi k khilf. Delhi walo ko bhi mukti milegi tumse’ Translation: ’If you have such a huge issue with it, why don’t you perform a self-immolation? The people of Delhi would also get freedom from you’ In tweet 10, the author is referring to Ar"
2020.trac-1.3,N18-4018,1,0.829201,"hly polarizing nature of opinions on the topic. Gafaranga (2007) describes code-mixing as use of linguistic units from different languages in a single utterance or sentence and code-switching as the co-occurrence of speech extracts belonging to two different grammatical systems. Majority of user generated data on social media is code-mixed and consequently, so is our dataset. Code mixed datasets for Hindi-English tweets have been previously created for humor (Khandelwal et al., 2018), sarcasm (Swami et al., 2018a), aggression (Kumar et al., 2018), hate speech (Bohra et al., 2018) and emotion (Vijay et al., 2018). 3. Gloss: ”kale dhan”: black money, ”Arvind Kejriwal”: Leader of opposition political party AAP, ”Notebandi”: demonetisation Tweet 5 was originally classified as a neutral stance. We feel that cases like above can be confidently annotated as favourable to the issue (i.e. favourable to demonetisation). The author rhetorically and sarcastically questions the opinion, intentions and reasons of those against the issue (in this case leader of opposition party). This tweet is also an example of what we consider covert aggression. For aggression annotation, we follow the guidelines by Kumar et al."
2020.trac-1.3,P14-2084,0,0.031203,"e language. As aggression levels are highly predictive of offensive language but not of hate speech category, we used a binary classification speech. However annotators faced difficulty in differentiating over a personal attack full of hatred than a community being targeted. An example : Non-Aggressive (NAG) Refers to texts which are not lying in the above two categories. 8. Tweet: ’kya Aam aadmi ke liye NoteBandi ka Faisla Shi hai?’ Translation: ’Is the decision of demonetisation in the favour of common man?’ Prior works regarding sarcasm and irony detection on social media data like Reddit (Wallace et al., 2014) and Twitter (Bamman and Smith, 2015) have shown that context is essential in understanding sarcasm. Therefore, most social media datasets of sarcasm are self-annotated i.e. hashtag specific twitter scraping like #sarcasm and #notsarcasm. As we are re-annotating a previously scraped dataset which was not self-annotated through specific hashtags, we rely on the domain knowledge of context expert annotators on the Indian socio-political scenario and focus issue of demonetisation. This however is not a drawback because in a dataset like ours , rich with strongly opinionated tweets, annotating sar"
2020.trac-1.3,N19-1144,0,0.0123603,"or suggests that the opposition leader should die and proceeds to verbally abuse him. 2. Related Work User generated data from social media forums like Twitter has attracted a lot of attention from the research community. Mohammad et al. (2017) and Krejzl et al. (2017) analyzed stance in tweets and online discussion forums respectively. The task of stance detection on tweets at SemEval 2016 (Mohammad et al., 2016) led to targeted interest in the area with contributions from Augenstein et al. (2016), Liu et al. (2016) etc. Aggression and offensive language was the focus of a SemEval 2019 task (Zampieri et al., 2019b) and some of the works on aggression identification are Kumar et al. (2018), Zampieri et al. (2019a). Closely related is detecting hate speech in social media which has been explored by Malmasi and Zampieri (2017), Schmidt and Wiegand (2017), Davidson et al. (2017), Badjatiya et al. (2017) among others. 4. Tweet: ’what if .. Modi Ji says Mitron ,,, kal raat ko zyada ho gayi thi ,,. Kuch nahi badla he.. #NoteBandi’ Translation: ’What if Modi says that he had too much to drink last night and nothing has really changed. #Demonetisation.’ Tweet 4 makes a sarcastic joke about how Prime Minister M"
2020.trac-1.3,S19-2010,0,0.0136442,"or suggests that the opposition leader should die and proceeds to verbally abuse him. 2. Related Work User generated data from social media forums like Twitter has attracted a lot of attention from the research community. Mohammad et al. (2017) and Krejzl et al. (2017) analyzed stance in tweets and online discussion forums respectively. The task of stance detection on tweets at SemEval 2016 (Mohammad et al., 2016) led to targeted interest in the area with contributions from Augenstein et al. (2016), Liu et al. (2016) etc. Aggression and offensive language was the focus of a SemEval 2019 task (Zampieri et al., 2019b) and some of the works on aggression identification are Kumar et al. (2018), Zampieri et al. (2019a). Closely related is detecting hate speech in social media which has been explored by Malmasi and Zampieri (2017), Schmidt and Wiegand (2017), Davidson et al. (2017), Badjatiya et al. (2017) among others. 4. Tweet: ’what if .. Modi Ji says Mitron ,,, kal raat ko zyada ho gayi thi ,,. Kuch nahi badla he.. #NoteBandi’ Translation: ’What if Modi says that he had too much to drink last night and nothing has really changed. #Demonetisation.’ Tweet 4 makes a sarcastic joke about how Prime Minister M"
2020.wildre-1.8,I08-2099,1,0.531578,"1: Telugu treebank stats The treebank is annotated using Paninian dependency grammar(Bharati et al., 1995). The paninian dependency relations are created around the notion of karakas, various participants in an action. These dependency relations are syntacto-semantic in nature. There are 40 different dependency labels specified in the panianian dependency grammar. These relations are hierarchical and certain relations can be under-specified in cases where a finer analysis is not required or when in certain cases the decision making is more difficult for the annotators(Bharati et al., 2009b). Begum et al. (2008) describe the guidelines for annotating dependency relations for Indian languages using paninian dependencies. The treebank is annotated with part-of-speech tags and morphological information like root, gender, number, person, TAM, vibhakti or case markers etc at word level. The dependency relations are annotated at chunk level. The treebank is made available in SSF format(Bharati et al., 2007). An example is shown in Figure 1. The dependency tree for the sentence is shown in Figure 2. In the example sentence, the intra-chunk dependencies, i.e dependency labels for cAlA (many) and I (this) are"
2020.wildre-1.8,J95-3006,0,0.875998,"rouped together and a lexicon consisting of words and symbols. Dependency grammars on the other hand model the syntactic relationship between the words of a sentence directly using headdependent relations. Dependency grammars are useful in modeling free word order languages. Indian languages are primarily free word order languages. There are few different dependency formalisms that have been developed for different languages. In recent years, Universal dependencies(Nivre et al., 2016) have been developed to arrive at a common dependency formalism for all languages. Paninian dependency grammar(Bharati et al., 1995) is specifically developed for Indian languages which are morphologically rich and free word order languages. Case markers and postpositions play crucial roles in these languages and word order is considered only at a surface level when required. Most Indian languages are also low resource languages. ICON-2009 and 2010 tools contests made available the initial dependency treebanks for Hindi, Telugu and Bangla. These treebanks are small in size and are annotated using the Paninian dependency grammar. Further efforts are being taken to build dependency annotated treebanks for Indian languages. H"
2020.wildre-1.8,W09-3036,1,0.755365,"Missing"
2020.wildre-1.8,W06-2920,0,0.167668,"speech tags, at morphological level with root, gender, number, person, TAM, vibhakti and case features and the dependency relations are annotated at a chunk level. The dependency relations within a chunk are left unannotated. Intra-chunk dependency annotation has been done on Hindi(Kosaraju et al., 2012) and Urdu(Bhat, 2017) treebanks previously. Annotating intra-chunk dependencies leads to a complete parse tree for every sentence in the treebank. Having completely annotated parse trees is essential for building robust end to end dependency parsers or making the treebanks available in CoNLL (Buchholz and Marsi, 2006) format and thereby making use of readily available parsers. In this paper, we extend one of those approaches for the Telugu treebank to annotate intra-chunk dependency relations. Telugu is a highly inflected morphologically rich language and has a few constructions like classifiers etc that do not occur in Hindi which makes the expansion task challenging. The fully expanded Telugu treebank is made publicly available 1 . The part-of-speech and chunk annotation of the Telugu treebank is done following the Anncorra (Bharati et al., 2009b) tagset developed for Indian languages. In the recent year"
2020.wildre-1.8,W12-3607,1,0.805763,"eebank is made publicly available. Keywords: Dependency Treebank, Intra-chunk dependencies, Low resource Language, Telugu 1. Introduction the Indian Language Treebanking project. These treebanks are annotated in Shakti Standard Format(SSF)(Bharati et al., 2007). Each sentence is annotated at word level with part of speech tags, at morphological level with root, gender, number, person, TAM, vibhakti and case features and the dependency relations are annotated at a chunk level. The dependency relations within a chunk are left unannotated. Intra-chunk dependency annotation has been done on Hindi(Kosaraju et al., 2012) and Urdu(Bhat, 2017) treebanks previously. Annotating intra-chunk dependencies leads to a complete parse tree for every sentence in the treebank. Having completely annotated parse trees is essential for building robust end to end dependency parsers or making the treebanks available in CoNLL (Buchholz and Marsi, 2006) format and thereby making use of readily available parsers. In this paper, we extend one of those approaches for the Telugu treebank to annotate intra-chunk dependency relations. Telugu is a highly inflected morphologically rich language and has a few constructions like classifie"
2020.wildre-1.8,J93-2004,0,0.0769178,"d across all Indian Languages. This tagset is commonly referred to as the BIS 2 (Bureau of Indian standards) tagset. All the latest annotation of part of speech tagging of Indian languages is done using the BIS tagset. In this paper, we convert the existing Telugu treebank from Anncorra to BIS standard. BIS tagset is a fine grained hierarchical tagset Treebanks play a crucial role in developing parsers as well as investigating other linguistic phenomena. Which is why there has been a targeted effort to create treebanks in several languages. Some such notable efforts include the Penn treebank (Marcus et al., 1993), the Prague Dependency treebank (Hajiˇcov´a, 1998). A treebank is annotated with a grammar. The grammars used for annotating treebanks can be broadly categorized into two types, Context Free Grammars and dependency grammars. A Context Free Grammar consists of a set of rules that determine how the words and symbols of a language can be grouped together and a lexicon consisting of words and symbols. Dependency grammars on the other hand model the syntactic relationship between the words of a sentence directly using headdependent relations. Dependency grammars are useful in modeling free word or"
2020.wildre-1.8,nivre-etal-2006-maltparser,0,0.136164,"ls Symbols are separated into symbols and punctuations. Question words They are separated into pronoun question words and demonstrative question words in BIS tagset. Demonstrative question words are always followed by a noun. While resolving question words (WQ), if the word 41 Figure 4: Intra-chunk dependency annotation in SSF format. intf Intensifiers (RP INTF) can modify both adjectives and adverbs. So we replace the jjmod intf with intf and use the same dependency label when an intensifier modifies an adverb or adjective. marked based on these rules. In the statistical approach Malt Parser(Nivre et al., 2006) is used to identify the intrachunk dependencies. A model is trained on a few manually annotated chunks with Malt parser and the same model is used to predict the intra-chunk dependencies for the rest of the treebank. nmod adj lwg psp lwg neg lwg vaux lwg rp lwg uh lwg cont pof redup pof cn pof cv jjmod intf rsym adjectives modifying nouns or pronouns post-positions negation verb auxiliaries particles interjection continuation reduplication compound nouns compound verbs adjectival intensifier symbols nmod wq This dependency relation is used when question words modify nouns inside a chunk. adv"
2020.wildre-1.8,L16-1262,0,0.0749111,"Missing"
2020.wildre-1.8,W04-0308,0,0.0484501,"reebank publicly available to facilitate further research. 6. In this paper, we follow the approach proposed by Bhat (2017) that makes use of a Context Free Grammar (CFG) and a shift-reduce parser for automatically annotating intrachunk dependencies. We use the treebank expander code made available by Bhat (2017) 3 and write the Context Free Grammar for Telugu. The Context Free Grammar is generated using the POS tags and creates a mapping between head and child POS tags and dependency labels. The intra-chunk annotation is done using a shift-reduce parser which internally uses the Arc-Standard(Nivre, 2004) transition system. The parser predicts a sequence of transitions starting from an initial configuration to a terminal configuration, and annotate the chunk dependencies in the process. A configuration consists of a stack, a buffer, and a set of dependency arcs. In the initial configuration, the stack is empty, buffer contains all the words in the chunk and intra-chunk dependencies are empty. In the terminal configuration, buffer is empty and stack contains only one element, the chunk head, and the chunk sub-tree is given by the set of dependency arcs. The next transition is predicted based on"
2020.wmt-1.55,W11-2123,0,0.0455131,"Basic tokenization denoted as BasicTok in Table 1 which make use of IndicNLP toolkit. • For Hindi to Marathi,we simply joined the subwords in text translated. • BPE which tokenize words into subword and is denoted as BPE. • Tokenization using Morfessor, which is denoted as Morfes. 2.3 Machine Translation Model We made use of Moses toolkit (Koehn et al., 2007) to build statistical models trained with tokenized bitext. We also use GIZA++ (Och and Ney, 2003) to find alignments between parallel text and grow-diag-final-and method (Koehn et al., 2003) to extract aligned phrases. And utilize KenLM (Heafield, 2011) to train a trigram model with kneser ney smoothing on monolingual corpus of 1 http://anoopkunchukuttan.github.io/indic nlp library/ Using back-translation to augment training data Due to time constraint we translated some part of Hindi monolingual corpus (AuthenticHindi ) to Marathi (SyntheticM arathi ). We used beam search with default setting in Moses for this translation. We used already trained LM from Section 2.3 to learn average LM score of BPE tokenized Marathi monolingual corpus. SyntheticM arathi is than pruned (SyntheticPrunedM arathi ) by keeping backtranslated sentences which have"
2020.wmt-1.55,P17-4012,0,0.0379114,"combinations, for Hindi to Marathi Systems with BPE tokenization on both Hindi and Marathi, 1. Original training text + Synthetic Marathi and Authentic Hindi + Synthetic Hindi and Authentic Marathi 2. Original training text + Synthetic Pruned Marathi and Authentic Pruned Hindi + Synthetic Pruned Hindi and Authentic Pruned Marathi 453 All these dataset combinations were used to train following methods to build MT models with respective default configurations available in respective toolkits, • SMT model using Moses toolkit (Koehn et al., 2007) • NMT model with attention using Opennmt toolkit (Klein et al., 2017) • NMT model with attention and copy attention (See et al., 2017) using Opennmt toolkit, to make use of similarity between Hindi Marathi language pair 4 Result learning to align and translate. arXiv:1409.0473. To submit two best systems out of 12 in each direction as directed by shared task, we did two evaluations. Firstly, we compared our system outputs to output of another publicly available translation model. Second, we went through some random outputs of all system outputs. We found that in most systems synthetic-authentic dataset which was not pruned with LM scores along with original tra"
2020.wmt-1.55,P07-2045,0,0.0463162,"er translating monolingual corpus, we did the following post processing based on direction of translation, • In case of Marathi to Hindi translation, in post processing we remove ’+’ delimiter. This is due to Marathi being morphological richer than Hindi. • Basic tokenization denoted as BasicTok in Table 1 which make use of IndicNLP toolkit. • For Hindi to Marathi,we simply joined the subwords in text translated. • BPE which tokenize words into subword and is denoted as BPE. • Tokenization using Morfessor, which is denoted as Morfes. 2.3 Machine Translation Model We made use of Moses toolkit (Koehn et al., 2007) to build statistical models trained with tokenized bitext. We also use GIZA++ (Och and Ney, 2003) to find alignments between parallel text and grow-diag-final-and method (Koehn et al., 2003) to extract aligned phrases. And utilize KenLM (Heafield, 2011) to train a trigram model with kneser ney smoothing on monolingual corpus of 1 http://anoopkunchukuttan.github.io/indic nlp library/ Using back-translation to augment training data Due to time constraint we translated some part of Hindi monolingual corpus (AuthenticHindi ) to Marathi (SyntheticM arathi ). We used beam search with default settin"
2020.wmt-1.55,W17-3204,0,0.0116314,"al age communication, translation between similar language is a justifiable requirement. But there is a scarcity of good quality bitext for many language pairs, as is the case of Hindi Marathi. Hence, we used characteristics displayed by similar languages (in this case Hindi and Marathi) like similar form of spelling, pronunciation etc. Following Kunchukuttan and Bhattacharyya (2017) and Kunchukuttan et al. (2014b) we made use of byte pair encoding (Sennrich et al., 2016b) and morfessor toolkit (Virpioja et al., 2013) respectively as part of preprocessing step before training. Using cues from Koehn and Knowles (2017) and looking at the size of training data provided, we use statistical method to build initial models. To further salvage similarity between this language pair we made use of backtranslation (Sennrich et al., 2016a) to generate more synthetic data for further training using both neural and statistical methods. For this shared task we developed 12 translation systems in each direction (Hindi ⇐⇒ Marathi). To rank systems, we went through some test instances subjectively and also compared our BLEU scores with another Translation system. And chose top 2 systems in both direction using both subject"
2020.wmt-1.55,N03-1017,0,0.219744,"with training data in various settings to build translation models. We also report configuration of the submitted systems and results produced by them. 1 Introduction Machine Translation systems are models which aim to translate text from one language into another. There are multiple ways of building such a model (Rule Based, Data driven, Hybrid etc.). In this system description paper, we use data driven techniques to build MT systems. As the name suggests, data driven MT systems make use of parallel sentences (i.e. xth sentence in two languages have same meaning). We make use of statistical (Koehn et al., 2003) and neural (Bahdanau et al., 2014) methods to build systems for Hindi Marathi pair. Hindi Marathi language pair comes under purview of similar languages. Similar Languages are languages which exhibit lexical and structural similarities (Kunchukuttan et al., 2014a). This can be due to common ancestry or being in close proximity for long time. In current digital age communication, translation between similar language is a justifiable requirement. But there is a scarcity of good quality bitext for many language pairs, as is the case of Hindi Marathi. Hence, we used characteristics displayed by s"
2020.wmt-1.55,W17-4102,0,0.0185012,"ir comes under purview of similar languages. Similar Languages are languages which exhibit lexical and structural similarities (Kunchukuttan et al., 2014a). This can be due to common ancestry or being in close proximity for long time. In current digital age communication, translation between similar language is a justifiable requirement. But there is a scarcity of good quality bitext for many language pairs, as is the case of Hindi Marathi. Hence, we used characteristics displayed by similar languages (in this case Hindi and Marathi) like similar form of spelling, pronunciation etc. Following Kunchukuttan and Bhattacharyya (2017) and Kunchukuttan et al. (2014b) we made use of byte pair encoding (Sennrich et al., 2016b) and morfessor toolkit (Virpioja et al., 2013) respectively as part of preprocessing step before training. Using cues from Koehn and Knowles (2017) and looking at the size of training data provided, we use statistical method to build initial models. To further salvage similarity between this language pair we made use of backtranslation (Sennrich et al., 2016a) to generate more synthetic data for further training using both neural and statistical methods. For this shared task we developed 12 translation s"
2020.wmt-1.55,kunchukuttan-etal-2014-shata,0,0.0270444,"another. There are multiple ways of building such a model (Rule Based, Data driven, Hybrid etc.). In this system description paper, we use data driven techniques to build MT systems. As the name suggests, data driven MT systems make use of parallel sentences (i.e. xth sentence in two languages have same meaning). We make use of statistical (Koehn et al., 2003) and neural (Bahdanau et al., 2014) methods to build systems for Hindi Marathi pair. Hindi Marathi language pair comes under purview of similar languages. Similar Languages are languages which exhibit lexical and structural similarities (Kunchukuttan et al., 2014a). This can be due to common ancestry or being in close proximity for long time. In current digital age communication, translation between similar language is a justifiable requirement. But there is a scarcity of good quality bitext for many language pairs, as is the case of Hindi Marathi. Hence, we used characteristics displayed by similar languages (in this case Hindi and Marathi) like similar form of spelling, pronunciation etc. Following Kunchukuttan and Bhattacharyya (2017) and Kunchukuttan et al. (2014b) we made use of byte pair encoding (Sennrich et al., 2016b) and morfessor toolkit (V"
2020.wmt-1.55,P16-1009,0,0.156129,"ructural similarities (Kunchukuttan et al., 2014a). This can be due to common ancestry or being in close proximity for long time. In current digital age communication, translation between similar language is a justifiable requirement. But there is a scarcity of good quality bitext for many language pairs, as is the case of Hindi Marathi. Hence, we used characteristics displayed by similar languages (in this case Hindi and Marathi) like similar form of spelling, pronunciation etc. Following Kunchukuttan and Bhattacharyya (2017) and Kunchukuttan et al. (2014b) we made use of byte pair encoding (Sennrich et al., 2016b) and morfessor toolkit (Virpioja et al., 2013) respectively as part of preprocessing step before training. Using cues from Koehn and Knowles (2017) and looking at the size of training data provided, we use statistical method to build initial models. To further salvage similarity between this language pair we made use of backtranslation (Sennrich et al., 2016a) to generate more synthetic data for further training using both neural and statistical methods. For this shared task we developed 12 translation systems in each direction (Hindi ⇐⇒ Marathi). To rank systems, we went through some test i"
2020.wmt-1.55,P16-1162,0,0.12248,"ructural similarities (Kunchukuttan et al., 2014a). This can be due to common ancestry or being in close proximity for long time. In current digital age communication, translation between similar language is a justifiable requirement. But there is a scarcity of good quality bitext for many language pairs, as is the case of Hindi Marathi. Hence, we used characteristics displayed by similar languages (in this case Hindi and Marathi) like similar form of spelling, pronunciation etc. Following Kunchukuttan and Bhattacharyya (2017) and Kunchukuttan et al. (2014b) we made use of byte pair encoding (Sennrich et al., 2016b) and morfessor toolkit (Virpioja et al., 2013) respectively as part of preprocessing step before training. Using cues from Koehn and Knowles (2017) and looking at the size of training data provided, we use statistical method to build initial models. To further salvage similarity between this language pair we made use of backtranslation (Sennrich et al., 2016a) to generate more synthetic data for further training using both neural and statistical methods. For this shared task we developed 12 translation systems in each direction (Hindi ⇐⇒ Marathi). To rank systems, we went through some test i"
2021.calcs-1.3,W18-3817,1,0.925598,"stic constraints on code-switching. Code-mixing and code-switching are similar terms that slightly differ technically, but they are often used interchangeably by the research community. We will also be using them interchangeably in our paper. In this paper, we work with English-Hindi codemixed data. English-Hindi code-mixed language often called Hinglish is very common in India because of a large number of bilingual speakers who often use English in their professional lives while using Hindi in their personal lives. An example of an English-Hindi code-mixed sentence from a dataset released by Dhar et al. (2018) is shown below: Code-mixing is a common phenomenon in multilingual societies around the world and is especially common in social media texts. Traditional NLP systems, usually trained on monolingual corpora, do not perform well on code-mixed texts. Training specialized models for code-switched texts is difficult due to the lack of large-scale datasets. Translating code-mixed data into standard languages like English could improve performance on various code-mixed tasks since we can use transfer learning from state-of-the-art English models for processing the translated data. This paper focuses"
2021.calcs-1.3,D15-1075,0,0.301544,"esis&quot; and classifying the hypothesis as true (entailment), false (contradiction), or undetermined (neutral). It is arguably one of the most fundamental tasks in natural language understanding. Wang et al. (2018) and Yin et al. (2019) suggest that various NLP tasks can be reduced to Natural Language Inference, which makes it an even more valuable task to solve. Natural Language Inference for English texts has been an active area of research. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, large-scale pre-trained models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-to-human performance. Although NLI on English data has seen many advances, there has been little work on NLI for code-mixed data. Khanuja et al. (2020a) release the first NLI dataset for code-mixed languages. It consists of conversations from Hindi movies (Bollywood) as premises. Chakravarthy et al. (2020) compare the effectiveness of various approaches on the dataset. 3 Tr"
2021.calcs-1.3,2020.wnut-1.22,0,0.0861568,"extual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, large-scale pre-trained models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-to-human performance. Although NLI on English data has seen many advances, there has been little work on NLI for code-mixed data. Khanuja et al. (2020a) release the first NLI dataset for code-mixed languages. It consists of conversations from Hindi movies (Bollywood) as premises. Chakravarthy et al. (2020) compare the effectiveness of various approaches on the dataset. 3 Translating Code-Mixed Text In this section, we describe our proposed model, which uses mBART (Liu et al., 2020) to translate code-mixed texts to English. 3.1 mBART We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder. It has been pre-trained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages extracted from Common Crawl1 (Wenzek et al., 2020; Conneau et al., 2020). Both English and Hindi are part of the pre-training corpus with 55,608 million token"
2021.calcs-1.3,W18-3204,0,0.043113,"Missing"
2021.calcs-1.3,N19-1409,0,0.0438219,"Missing"
2021.calcs-1.3,2020.acl-main.747,0,0.0389165,"nguages. It consists of conversations from Hindi movies (Bollywood) as premises. Chakravarthy et al. (2020) compare the effectiveness of various approaches on the dataset. 3 Translating Code-Mixed Text In this section, we describe our proposed model, which uses mBART (Liu et al., 2020) to translate code-mixed texts to English. 3.1 mBART We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder. It has been pre-trained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages extracted from Common Crawl1 (Wenzek et al., 2020; Conneau et al., 2020). Both English and Hindi are part of the pre-training corpus with 55,608 million tokens (300.8 GB) and 1,715 million tokens (20.2 GB), respectively. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. 3.2 Data Preparation We use the datasets released by Dhar et al. (2018) and Srivastava and Singh (2020), the statistics of the datasets are provided in the Table 1. Since both the datasets contain Hindi words in Roman script, we use the CSNLI"
2021.calcs-1.3,D13-1176,0,0.0311493,"Missing"
2021.calcs-1.3,2020.calcs-1.2,0,0.155441,"badi dikkatein chhoti lagti hain. • Gloss: [My brother always told me] that [in retrospect], big problems small seem are. • Translation: My brother always told me that, in retrospect, big problems seem to be small. Although there is a large population globally that communicates using code-mixed languages, annotated datasets remain scarce even when the monolingual constituent languages have large-scale datasets. Recent work suggests that multilingual models trained on several monolingual datasets perform well with zero-shot cross-lingual transfer in code-switched settings (Patwa et al., 2020; Khanuja et al., 2020b). However, Khanuja et al. (2020b) conclude that their model had varying performance across tasks and especially struggled with NLI and sentiment analysis tasks. Another challenge with code-mixed language research is that, Introduction In the last decade, social media has become a significant part of the lives of a large population in the world. Unlike previously popular communication platforms, online messaging is very informal, and in recent years, it has led to an increase in the usage of emojis, slang, and even a hybrid form of language, code-mixed language. Code-mixed language is a mixtu"
2021.calcs-1.3,2020.emnlp-demos.2,0,0.0297831,"Missing"
2021.calcs-1.3,2020.acl-main.329,0,0.0977018,"badi dikkatein chhoti lagti hain. • Gloss: [My brother always told me] that [in retrospect], big problems small seem are. • Translation: My brother always told me that, in retrospect, big problems seem to be small. Although there is a large population globally that communicates using code-mixed languages, annotated datasets remain scarce even when the monolingual constituent languages have large-scale datasets. Recent work suggests that multilingual models trained on several monolingual datasets perform well with zero-shot cross-lingual transfer in code-switched settings (Patwa et al., 2020; Khanuja et al., 2020b). However, Khanuja et al. (2020b) conclude that their model had varying performance across tasks and especially struggled with NLI and sentiment analysis tasks. Another challenge with code-mixed language research is that, Introduction In the last decade, social media has become a significant part of the lives of a large population in the world. Unlike previously popular communication platforms, online messaging is very informal, and in recent years, it has led to an increase in the usage of emojis, slang, and even a hybrid form of language, code-mixed language. Code-mixed language is a mixtu"
2021.calcs-1.3,2020.acl-main.441,0,0.0203031,"out, 0.1 attention dropout and polynomial decay learning rate scheduling. We validate the models every 8000 steps and select the best checkpoint based on the lowest validation loss. To train our systems efficiently, we prune mBART’s vocabulary by removing the tokens which are not present in any of the datasets mentioned in the previous section. We compare the following 3 strategies for finetuning mBART: 3.4 Datasets Model 4 5 https://github.com/pytorch/fairseq 18 https://github.com/mjpost/sacrebleu https://www.nltk.org/ Dataset(Number of samples) Model Architecture (1) (Liu et al., 2019) (2) (Nie et al., 2020) (3) (Nie et al., 2020) (4) (Nie et al., 2020) (5) (He et al., 2021) RoBERTa large RoBERTa large XLNet large ALBERT xxlarge DeBERTa large SNLI (570k) MultiNLI (433k) FEVER-NLI (250k) ANLI(R1,R2,R3) (170k) #Parameters ∼355M ∼355M ∼340M ∼223M ∼390M Table 3: The pre-trained checkpoints we use along with their architecture, number of parameters and finetuning datasets. # of sentences # of entailed sentences # of contradictory sentences # of tokens # of Hindi tokens # of English tokens # of ‘Other’ tokens Train Set Dev Set Test Set 1,392 696 696 123,366 75,865 19,952 27,549 400 200 200 33,932 20,83"
2021.calcs-1.3,N19-4009,0,0.0220083,"∗ 10−5 , followed by further fine-tuning on on the merged dataset with parallel English-Hindi code-mixed sentences for 10,000 steps with 2,500 warm-up steps and a learning rate of 10−5 . 4 Results Code-Mixed Sequence-level Classification In this section, we describe our approach for codemixed sequence-level classification tasks using our We use BLEU scores as the metric for comparing our systems, the scores are computed using the 3 Dhar et al. (2018) Table 2: BLEU scores of our systems on the test sets of the two datasets. We use the implementation of mBART available in the fairseq library3 (Ott et al., 2019). We finetune on 4 Nvidia GeForce RTX 2080 Ti GPUs with an effective batch size of 1024 tokens per GPU. We use the Adam optimizer ( = 10−6 , β1 = 0.9, β2 = 0.98) (Kingma and Ba, 2015) with 0.2 label smoothing, 0.3 dropout, 0.1 attention dropout and polynomial decay learning rate scheduling. We validate the models every 8000 steps and select the best checkpoint based on the lowest validation loss. To train our systems efficiently, we prune mBART’s vocabulary by removing the tokens which are not present in any of the datasets mentioned in the previous section. We compare the following 3 strateg"
2021.calcs-1.3,L18-1548,0,0.0390462,"e text into positive, negative, or neutral classes. It has several applications such as customer feedback, marketing, and social media monitoring. There has been extensive research on sentiment analysis of English texts with various shared tasks and datasets. Sentiment analysis for code-mixed texts is an essential task due to the widespread usage of 1 2 17 https://commoncrawl.org/ https://github.com/irshadbhat/csnli train:validation:test split. We merge the training and validation sets of the two datasets and use the merged datasets for all our experiments. We also use the dataset released by Kunchukuttan et al. (2018) which contains parallel sentences for English and Hindi. We use the training set, which contains 1,609,682 sentences, for training our systems. 3.3 mBART-hien mBART-cm mBART-hien-cm Srivastava and Singh (2020) 17.2 30.5 31.7 16.7 31.6 33.0 Sabse bakwaas was pin ball Optimization Transliteration and Normalization सबसे बकवास was pin ball Translation The worst was the pin ball Fine-tuned Classification Model Negative Figure 1: The working of our pipeline for the task of code-mixed Natural Language Inference is demonstrated on an example (with minor edits) from the dataset (the details of the dat"
2021.calcs-1.3,2020.acl-main.703,0,0.0177774,"ances, there has been little work on NLI for code-mixed data. Khanuja et al. (2020a) release the first NLI dataset for code-mixed languages. It consists of conversations from Hindi movies (Bollywood) as premises. Chakravarthy et al. (2020) compare the effectiveness of various approaches on the dataset. 3 Translating Code-Mixed Text In this section, we describe our proposed model, which uses mBART (Liu et al., 2020) to translate code-mixed texts to English. 3.1 mBART We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder. It has been pre-trained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages extracted from Common Crawl1 (Wenzek et al., 2020; Conneau et al., 2020). Both English and Hindi are part of the pre-training corpus with 55,608 million tokens (300.8 GB) and 1,715 million tokens (20.2 GB), respectively. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. 3.2 Data Preparation We use the datasets released by Dhar et al. (2018) and Srivastava and Singh (2020), the"
2021.calcs-1.3,2020.tacl-1.47,0,0.100775,"ons, differing conventions of transcription, or simple idiosyncrasy (Roark et al., 2020). This makes it challenging to prepare reliable datasets to train robust deep learning models. Most of the existing datasets focus on a few language pairs and have been prepared by several shared task organizers. To address these issues, we propose translating the code-mixed data to English (a high-resource language) and applying powerful models trained on English data to perform sequence-level classification tasks on the translated data. To translate the code-mixed data to English, we propose using mBART (Liu et al., 2020), a pre-trained multilingual sequence-to-sequence model. We experiment with our pipeline on two EnglishHindi code-mixed sequence classification tasks of the GLUECoS (Khanuja et al., 2020b) benchmark - Natural Language Inference and Sentiment Analysis. We achieve state-of-the-art performance in both tasks. The code for our proposed system is available at https://github.com/ devanshg27/cm_translatify. The main contributions of our work are as follows: The rest of the paper is organized as follows. We discuss prior work related to code-mixed language processing and also discuss work related to ma"
2021.calcs-1.3,W16-5805,0,0.0148964,"ainst past work. We conclude with a direction for future work and highlight our main findings. 2 Related Work Code-mixing occurs when a speaker uses words belonging to different languages interleaved with each other in the same conversation. With the rise of social media and messaging platforms, there has been a significant increase in code-mixed language usage. Several shared tasks have been conducted as a part of code-switching workshops (Diab et al., 2014, 2016; Aguilar et al., 2018b) which were held in notable conferences. These tasks include language identification (Solorio et al., 2014; Molina et al., 2016), named entity recognition (Aguilar et al., 2018a; Rao and Devi, 2016), information retrieval (Roy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2018), Partof-speech tagging (Jamatia et al., 2016), sentiment analysis (Patra et al., 2018; Patwa et al., 2020), and question answering (Chandu et al., 2018). Although these tasks have helped progress codeswitching language research, most tasks require building specialized systems for the specific task and language pair due to the limited dataset sizes. Recently, large pre-trained multilingual models have been used for"
2021.calcs-1.3,2020.semeval-1.100,0,0.428028,"me ki in retrospect, badi dikkatein chhoti lagti hain. • Gloss: [My brother always told me] that [in retrospect], big problems small seem are. • Translation: My brother always told me that, in retrospect, big problems seem to be small. Although there is a large population globally that communicates using code-mixed languages, annotated datasets remain scarce even when the monolingual constituent languages have large-scale datasets. Recent work suggests that multilingual models trained on several monolingual datasets perform well with zero-shot cross-lingual transfer in code-switched settings (Patwa et al., 2020; Khanuja et al., 2020b). However, Khanuja et al. (2020b) conclude that their model had varying performance across tasks and especially struggled with NLI and sentiment analysis tasks. Another challenge with code-mixed language research is that, Introduction In the last decade, social media has become a significant part of the lives of a large population in the world. Unlike previously popular communication platforms, online messaging is very informal, and in recent years, it has led to an increase in the usage of emojis, slang, and even a hybrid form of language, code-mixed language. Code-mix"
2021.calcs-1.3,2020.wnut-1.7,0,0.115155,"ng the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages extracted from Common Crawl1 (Wenzek et al., 2020; Conneau et al., 2020). Both English and Hindi are part of the pre-training corpus with 55,608 million tokens (300.8 GB) and 1,715 million tokens (20.2 GB), respectively. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. 3.2 Data Preparation We use the datasets released by Dhar et al. (2018) and Srivastava and Singh (2020), the statistics of the datasets are provided in the Table 1. Since both the datasets contain Hindi words in Roman script, we use the CSNLI library2 (Bhat et al., 2017, 2018) as a preprocessing step. It transliterates the Hindi words to Devanagari and also performs text normalization. We split the datasets into an 8:1:1 Sentiment Analysis is the task of understanding the sentiment expressed in the text and classifying the text into positive, negative, or neutral classes. It has several applications such as customer feedback, marketing, and social media monitoring. There has been extensive rese"
2021.calcs-1.3,W18-6319,0,0.0127846,"0 Sabse bakwaas was pin ball Optimization Transliteration and Normalization सबसे बकवास was pin ball Translation The worst was the pin ball Fine-tuned Classification Model Negative Figure 1: The working of our pipeline for the task of code-mixed Natural Language Inference is demonstrated on an example (with minor edits) from the dataset (the details of the dataset are discussed later). • mBART-cm: We fine-tune mBART on the merged dataset with parallel English-Hindi code-mixed sentences. We fine-tune for 20,000 steps with 2,500 warm-up steps and a learning rate of 3 ∗ 10−5 . SacreBLEU library4 (Post, 2018) after tokenization using the TweetTokenizer available with the NLTK library5 (Bird et al., 2009). The scores of our systems are shown in Table 2. We find that mBART-hien which was only fine-tuned for HindiEnglish translation, performs considerably worse than the other models, showing that fine-tuning on English-Hindi code-mixed data improves the performance substantially. We also find that mBARThien-cm has the best performance among the systems we consider. It uses transfer learning from Hindi to English translation to improve HinglishEnglish translation. • mBART-hien: We fine-tune mBART on t"
2021.calcs-1.3,N18-1074,0,0.0276032,"Missing"
2021.calcs-1.3,2020.lrec-1.294,0,0.0231605,"articles or books written in codemixed languages. Instead, most research uses informal sources such as social media texts or messages, which are usually challenging to obtain. Also, most of the data is written in the Roman script, and Hindi words are transliterated informally without any standard rules. Instead, individuals generally provide a rough phonetic transcription of the intended word, which can vary from individual to individual due to any number of factors, including regional or dialectal differences in pronunciations, differing conventions of transcription, or simple idiosyncrasy (Roark et al., 2020). This makes it challenging to prepare reliable datasets to train robust deep learning models. Most of the existing datasets focus on a few language pairs and have been prepared by several shared task organizers. To address these issues, we propose translating the code-mixed data to English (a high-resource language) and applying powerful models trained on English data to perform sequence-level classification tasks on the translated data. To translate the code-mixed data to English, we propose using mBART (Liu et al., 2020), a pre-trained multilingual sequence-to-sequence model. We experiment"
2021.calcs-1.3,S17-2088,0,0.0324507,"Missing"
2021.calcs-1.3,W18-5446,0,0.0645951,"Missing"
2021.calcs-1.3,2020.lrec-1.494,0,0.0405523,"set for code-mixed languages. It consists of conversations from Hindi movies (Bollywood) as premises. Chakravarthy et al. (2020) compare the effectiveness of various approaches on the dataset. 3 Translating Code-Mixed Text In this section, we describe our proposed model, which uses mBART (Liu et al., 2020) to translate code-mixed texts to English. 3.1 mBART We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder. It has been pre-trained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages extracted from Common Crawl1 (Wenzek et al., 2020; Conneau et al., 2020). Both English and Hindi are part of the pre-training corpus with 55,608 million tokens (300.8 GB) and 1,715 million tokens (20.2 GB), respectively. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. 3.2 Data Preparation We use the datasets released by Dhar et al. (2018) and Srivastava and Singh (2020), the statistics of the datasets are provided in the Table 1. Since both the datasets contain Hindi words in Roman s"
2021.calcs-1.3,N18-1101,0,0.0256334,"heckpoints we use along with their architecture, number of parameters and finetuning datasets. # of sentences # of entailed sentences # of contradictory sentences # of tokens # of Hindi tokens # of English tokens # of ‘Other’ tokens Train Set Dev Set Test Set 1,392 696 696 123,366 75,865 19,952 27,549 400 200 200 33,932 20,837 5,457 7,638 447 224 223 40,072 24,413 6,624 9,035 guage Inference for English texts. We use publicly available checkpoints for each model, which have been fine-tuned for Natural Language Inference on various English datasets such as SNLI (Bowman et al., 2015), MultiNLI (Williams et al., 2018), FEVER-NLI (Nie et al., 2019), ANLI (R1, R2, R3) (Nie et al., 2020). We fine-tune the checkpoints further on the code-mixed data translated to English. The details about the checkpoints we use are shown in Table 3. Table 4: The statistics of the Natural Language Inference dataset. We use the language tokens predicted by the CSNLI library. 4.1.3 Optimization For the implementation of our systems, we use the HuggingFace Transformers library6 (Wolf et al., 2020) and the AdamW optimizer ( = 10−8 , β1 = 0.9, β2 = 0.999, wd = 0.01) available in PyTorch7 (Paszke et al., 2019) with a learning rate o"
2021.calcs-1.3,W14-3907,0,0.029734,"how its performance against past work. We conclude with a direction for future work and highlight our main findings. 2 Related Work Code-mixing occurs when a speaker uses words belonging to different languages interleaved with each other in the same conversation. With the rise of social media and messaging platforms, there has been a significant increase in code-mixed language usage. Several shared tasks have been conducted as a part of code-switching workshops (Diab et al., 2014, 2016; Aguilar et al., 2018b) which were held in notable conferences. These tasks include language identification (Solorio et al., 2014; Molina et al., 2016), named entity recognition (Aguilar et al., 2018a; Rao and Devi, 2016), information retrieval (Roy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2018), Partof-speech tagging (Jamatia et al., 2016), sentiment analysis (Patra et al., 2018; Patwa et al., 2020), and question answering (Chandu et al., 2018). Although these tasks have helped progress codeswitching language research, most tasks require building specialized systems for the specific task and language pair due to the limited dataset sizes. Recently, large pre-trained multilingual mod"
2021.calcs-1.3,D19-1404,0,0.0144338,"has been some work related to code-mixed sentiment analysis with a few shared tasks (Patra et al., 2018; Patwa et al., 2020). The participants of the task organized by Patwa et al. (2020) explored various approaches such as pretrained language models, RNN, CNN, and word embeddings. Natural Language Inference is the task of determining if the given “premise&quot; supports a given “hypothesis&quot; and classifying the hypothesis as true (entailment), false (contradiction), or undetermined (neutral). It is arguably one of the most fundamental tasks in natural language understanding. Wang et al. (2018) and Yin et al. (2019) suggest that various NLP tasks can be reduced to Natural Language Inference, which makes it an even more valuable task to solve. Natural Language Inference for English texts has been an active area of research. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, large-scale pre-trained models (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achi"
2021.calcs-1.7,W18-3817,1,0.773859,"tistical or rule-based approaches. Kalchbrenner and Blunsom (2013) first proposed a DNN model for translation, following which transformerbased approaches (Vaswani et al., 2017) have taken the stage. Some approaches utilize multilingual pre-training (Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020); however, these works focus only on monolingual language pairs. Although a large number of multilingual speakers in a highly populous country like India use English-Hindi code-mixed language, only a few studies (Srivastava and Singh, 2020; Singh and Solorio, 2018; Dhar et al., 2018) have attempted the problem. Enabling translation systems in the following pair can bridge the communication gap between several people and further improve the state of globalization in the world. • We propose a normalized BLEU score metric to better account for the spelling variations in the code-mixed sentences. • Along with BLEU scores, we analyze the code-mixing quality of the reference translations along with the generated outputs and propose that for assessing code-mixed translations, measures of code-mixing should be part of evaluation and analysis. The rest of the paper is organized as"
2021.calcs-1.7,E17-2052,1,0.846562,"e describe our proposed systems for the task, which use mBART (Liu et al., 2020) to translate English to Hinglish. 3.1 3.3 We transliterate the Hindi words in our predicted translations from Devanagari to Roman. We use the following methods to transliterate a given Devanagari token (we use the first method which provides us with the transliteration): Data Preparation We use the dataset provided by the task organizers for our systems, the statistics of the datasets are provided in Table 1. Since the target sentences in the dataset contain Hindi words in Roman script, we use the CSNLI library4 (Bhat et al., 2017, 2018) as a preprocessing step. It transliterates the Hindi words to Devanagari and also performs text normalization. We use the provided train:validation:test split, which is in the ratio 8:1:1. 3.2 1. When we transliterate the Hindi words in the target sentences from Roman to Devanagari (as discussed in Section 3.1), we store the most frequent Roman transliteration for each Hindi word in the train set. If the current Devanagari token’s transliteration is available, we use it directly. 2. We use the publicly available Dakshina Dataset (Roark et al., 2020) which has 25,000 Hindi words in Deva"
2021.calcs-1.7,N19-1409,0,0.0157185,"f software to translate text from one language to another. In the current state of globalization, translation systems have widespread applications and are consequently an active area of research. Neural machine translation has gained popularity only in the last decade, while earlier works focused on statistical or rule-based approaches. Kalchbrenner and Blunsom (2013) first proposed a DNN model for translation, following which transformerbased approaches (Vaswani et al., 2017) have taken the stage. Some approaches utilize multilingual pre-training (Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020); however, these works focus only on monolingual language pairs. Although a large number of multilingual speakers in a highly populous country like India use English-Hindi code-mixed language, only a few studies (Srivastava and Singh, 2020; Singh and Solorio, 2018; Dhar et al., 2018) have attempted the problem. Enabling translation systems in the following pair can bridge the communication gap between several people and further improve the state of globalization in the world. • We propose a normalized BLEU score metric to better account for the spelling variations in the cod"
2021.calcs-1.7,N18-1090,1,0.887888,"Missing"
2021.calcs-1.7,L16-1292,0,0.116023,"enerates a higher number of Hindi tokens while being low in code-mixing quality, and makes lesser grammatical errors. A more extensive and finegrained analysis of these errors will undoubtedly help improve the models’ characterization, and we leave it for future improvements. 5.2 Code Mixing Quality of generated translations In the code-mixed machine translation setting, it is essential to observe the quality of the code-mixing in the generated translations. While BLEU scores indicate how close we are to the target translation in terms of n-gram overlap, a measure like CodeMixing Index (CMI) (Gambäck and Das, 2016) provides us means to assess if the generated output is a mix of two languages or not. Relying on just the BLEU score for assessing translations can misrepresent the quality of translations, as models could generate monolingual outputs and still have a basic BLEU score due to n-gram overlap. If a measure of code mixing intensity, like CMI, is also part of the evaluation regime, we would be able to assess the code mixing quality of generated outputs as well. Figure 2 shows us that the distribution of CMI for outputs generated by our various models (mBART-en and mBART-hien) for both validation a"
2021.calcs-1.7,2020.findings-emnlp.206,0,0.462066,"ighlight our main findings. 2 Synthetic code-mixed data generation is a plausible option to build resources for code-mixed language research and is a very similar task to translation. While translation focuses on retaining the meaning of the source sentence, generation is a simpler task requiring focus only on the quality of the synthetic data generated. Pratapa et al. (2018) started by exploring linguistic theories to generate code-mixed data. Later works attempt the problem using several approaches including Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework (Gupta et al., 2020), pointer-generator networks (Winata et al., 2019), and a two-level Background Code-mixing occurs when a speaker switches between two or more languages in the context of the same conversation. It has become popular in multilingual societies with the rise of social media applications and messaging platforms. In attempts to progress the field of code-mixed data, several code-switching workshops (Diab et al., 2014, 2016; Aguilar et al., 2018b) have been organized in notable conferences. Most of the workshops include shared tasks on various of the lan3 https://github.com/devanshg27/cm_ translation"
2021.calcs-1.7,W18-3204,0,0.0459404,"Missing"
2021.calcs-1.7,D13-1176,0,0.0269824,"the effectiveness of fine-tuning mBART to translate to code-mixed sentences by utilizing the Hindi pre-training of the model in Devanagri script. We further explore the effectiveness of using parallel sentences as input. Machine Translation refers to the use of software to translate text from one language to another. In the current state of globalization, translation systems have widespread applications and are consequently an active area of research. Neural machine translation has gained popularity only in the last decade, while earlier works focused on statistical or rule-based approaches. Kalchbrenner and Blunsom (2013) first proposed a DNN model for translation, following which transformerbased approaches (Vaswani et al., 2017) have taken the stage. Some approaches utilize multilingual pre-training (Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020); however, these works focus only on monolingual language pairs. Although a large number of multilingual speakers in a highly populous country like India use English-Hindi code-mixed language, only a few studies (Srivastava and Singh, 2020; Singh and Solorio, 2018; Dhar et al., 2018) have attempted the problem. Enabling translatio"
2021.calcs-1.7,L18-1548,0,0.118508,". Model We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder pretrained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages including English and Hindi. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. To train our systems efficiently, we prune mBART’s vocabulary by removing the tokens which are not present in the provided dataset or the dataset released by Kunchukuttan et al. (2018) which contains 1,612,709 parallel sentences for English and Hindi. We compare the following two strategies for finetuning mBART: 4 Post-Processing 3. We use the indic-trans library6 (Bhat et al., 2015) to transliterate the token from Devanagari to Roman. 4 Experimental Setup 4.1 Implementation We use the implementation of mBART available in the fairseq library7 (Ott et al., 2019). We finetune on 4 Nvidia GeForce RTX 2080 Ti GPUs 5 https://cloud.google.com/translate https://github.com/libindic/ indic-trans 7 https://github.com/pytorch/fairseq 6 https://github.com/irshadbhat/csnli 49 Model mBAR"
2021.calcs-1.7,W18-6319,0,0.0121729,"Hindi words that have multiple roman spellings. Thus, even if the model is generating the correct Devanagari token, the BLEU scores will be understated due to the spelling variation in the transliterated reference sentence. By back-transliterating Hindi tokens to Devanagari, BLEUnormalized score thus provides a better representation of translation quality. Evaluation Metrics We use the following two evaluation metrics for comparing our systems: 1. BLEU: The BLEU score (Papineni et al., 2002) is the official metric used in the leader board. We calculate the score using the SacreBLEU library8 (Post, 2018) after lowercasing and tokenization using the TweetTokenizer available with the NLTK library9 (Bird et al., 2009). 2. BLEUnormalized : Instead of calculating the BLEU scores on the texts where the Hindi words are transliterated to Roman, we calculate the score on texts where Hindi words are in Devanagari and English words in Roman. We transliterate the target sentences using the CSNLI library and we use the outputs of our system before performing the post-processing (Section 3.3). We again use the SacreBLEU library after lowercasing and tokenization using the TweetTokenizer available with the"
2021.calcs-1.7,2020.acl-main.703,0,0.033273,"we store the most frequent Roman transliteration for each Hindi word in the train set. If the current Devanagari token’s transliteration is available, we use it directly. 2. We use the publicly available Dakshina Dataset (Roark et al., 2020) which has 25,000 Hindi words in Devanagari script along with their attested romanizations. If the current Devanagari token is available in the dataset, we use the transliteration with the maximum number of attestations from the dataset. Model We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder pretrained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages including English and Hindi. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each and a model dimension of 1024 on 16 heads resulting in ∼680 million parameters. To train our systems efficiently, we prune mBART’s vocabulary by removing the tokens which are not present in the provided dataset or the dataset released by Kunchukuttan et al. (2018) which contains 1,612,709 parallel sentences for English and Hindi. We compare the following two strategies for fine"
2021.calcs-1.7,P18-1143,0,0.0116738,"ystems and compare the performances of our approaches. We discuss the amount of codemixing in the translations predicted by our systems and discuss some issues present in the provided dataset. We conclude with a direction for future work and highlight our main findings. 2 Synthetic code-mixed data generation is a plausible option to build resources for code-mixed language research and is a very similar task to translation. While translation focuses on retaining the meaning of the source sentence, generation is a simpler task requiring focus only on the quality of the synthetic data generated. Pratapa et al. (2018) started by exploring linguistic theories to generate code-mixed data. Later works attempt the problem using several approaches including Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework (Gupta et al., 2020), pointer-generator networks (Winata et al., 2019), and a two-level Background Code-mixing occurs when a speaker switches between two or more languages in the context of the same conversation. It has become popular in multilingual societies with the rise of social media applications and messaging platforms. In attempts to progress the field of code-mixed da"
2021.calcs-1.7,2020.tacl-1.47,0,0.126967,"te text from one language to another. In the current state of globalization, translation systems have widespread applications and are consequently an active area of research. Neural machine translation has gained popularity only in the last decade, while earlier works focused on statistical or rule-based approaches. Kalchbrenner and Blunsom (2013) first proposed a DNN model for translation, following which transformerbased approaches (Vaswani et al., 2017) have taken the stage. Some approaches utilize multilingual pre-training (Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020); however, these works focus only on monolingual language pairs. Although a large number of multilingual speakers in a highly populous country like India use English-Hindi code-mixed language, only a few studies (Srivastava and Singh, 2020; Singh and Solorio, 2018; Dhar et al., 2018) have attempted the problem. Enabling translation systems in the following pair can bridge the communication gap between several people and further improve the state of globalization in the world. • We propose a normalized BLEU score metric to better account for the spelling variations in the code-mixed sentences."
2021.calcs-1.7,2021.eacl-demos.24,0,0.403189,"e encoder and decoding Hinglish sentences. For feeding the data to the encoder, we concatenate the Hindi translations, followed by a separator token ‘##’, followed by the English sentence. We use the Google NMT system5 (Wu et al., 2016) to translate the English source sentences to Hindi. We again use beam search with a beam size of 5 for decoding. Table 1: The statistics of the dataset. We use the language tags predicted by the CSNLI library4 . Since the target sentences of the test set are not public, we do not provide its statistics. variational autoencoder (Samanta et al., 2019). Recently, Rizvi et al. (2021) released a tool to generate code-mixed data using parallel sentences as input. 3 System Overview In this section, we describe our proposed systems for the task, which use mBART (Liu et al., 2020) to translate English to Hinglish. 3.1 3.3 We transliterate the Hindi words in our predicted translations from Devanagari to Roman. We use the following methods to transliterate a given Devanagari token (we use the first method which provides us with the transliteration): Data Preparation We use the dataset provided by the task organizers for our systems, the statistics of the datasets are provided in"
2021.calcs-1.7,W16-5805,0,0.0606434,"to Linguistic Code-Switching, pages 47–55 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_007 target sentences from Roman script to Devanagri script to utilize its pre-training. We further translate the English input to Hindi using pre-existing models and show improvements in the translation using parallel sentences as input to the mBART model. The code for our systems, along with error analysis, is public3 . The main contributions of our work are as follows: guage understanding tasks like language identification (Solorio et al., 2014; Molina et al., 2016), NER (Aguilar et al., 2018a; Rao and Devi, 2016), IR (Roy et al., 2013; Banerjee et al., 2018), PoS tagging (Jamatia et al., 2016), sentiment analysis (Patra et al., 2018; Patwa et al., 2020), and question answering (Chandu et al., 2018). Although these workshops have gained traction, the field lacks standard datasets to build robust systems. The small size of the datasets is a major factor that limits the scope of code-mixed systems. • We explore the effectiveness of fine-tuning mBART to translate to code-mixed sentences by utilizing the Hindi pre-training of the model in Devanagri script. W"
2021.calcs-1.7,2020.lrec-1.294,0,0.042285,"man script, we use the CSNLI library4 (Bhat et al., 2017, 2018) as a preprocessing step. It transliterates the Hindi words to Devanagari and also performs text normalization. We use the provided train:validation:test split, which is in the ratio 8:1:1. 3.2 1. When we transliterate the Hindi words in the target sentences from Roman to Devanagari (as discussed in Section 3.1), we store the most frequent Roman transliteration for each Hindi word in the train set. If the current Devanagari token’s transliteration is available, we use it directly. 2. We use the publicly available Dakshina Dataset (Roark et al., 2020) which has 25,000 Hindi words in Devanagari script along with their attested romanizations. If the current Devanagari token is available in the dataset, we use the transliteration with the maximum number of attestations from the dataset. Model We fine-tune mBART, which is a multilingual sequence-to-sequence denoising auto-encoder pretrained using the BART (Lewis et al., 2020) objective on large-scale monolingual corpora of 25 languages including English and Hindi. It uses a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 encoder and decoder layers each an"
2021.calcs-1.7,N19-4009,0,0.016418,"on 16 heads resulting in ∼680 million parameters. To train our systems efficiently, we prune mBART’s vocabulary by removing the tokens which are not present in the provided dataset or the dataset released by Kunchukuttan et al. (2018) which contains 1,612,709 parallel sentences for English and Hindi. We compare the following two strategies for finetuning mBART: 4 Post-Processing 3. We use the indic-trans library6 (Bhat et al., 2015) to transliterate the token from Devanagari to Roman. 4 Experimental Setup 4.1 Implementation We use the implementation of mBART available in the fairseq library7 (Ott et al., 2019). We finetune on 4 Nvidia GeForce RTX 2080 Ti GPUs 5 https://cloud.google.com/translate https://github.com/libindic/ indic-trans 7 https://github.com/pytorch/fairseq 6 https://github.com/irshadbhat/csnli 49 Model mBART-en mBART-hien Validation Set Test Set BLEU BLEUnormalized BLEU BLEUnormalized 15.3 14.6 18.9 20.2 12.22 11.86 − − Table 2: Performance of our systems on the validation set and test set of the dataset. Since the target sentences of the test set are not public, we do not calculate the scores ourselves. We report the BLEU scores of our systems on the test set from the official lead"
2021.calcs-1.7,P02-1040,0,0.111615,"re 1 shows the extent of this phenomena in the dataset released as part of this shared task, and it is evident that there are Hindi words that have multiple roman spellings. Thus, even if the model is generating the correct Devanagari token, the BLEU scores will be understated due to the spelling variation in the transliterated reference sentence. By back-transliterating Hindi tokens to Devanagari, BLEUnormalized score thus provides a better representation of translation quality. Evaluation Metrics We use the following two evaluation metrics for comparing our systems: 1. BLEU: The BLEU score (Papineni et al., 2002) is the official metric used in the leader board. We calculate the score using the SacreBLEU library8 (Post, 2018) after lowercasing and tokenization using the TweetTokenizer available with the NLTK library9 (Bird et al., 2009). 2. BLEUnormalized : Instead of calculating the BLEU scores on the texts where the Hindi words are transliterated to Roman, we calculate the score on texts where Hindi words are in Devanagari and English words in Roman. We transliterate the target sentences using the CSNLI library and we use the outputs of our system before performing the post-processing (Section 3.3)."
2021.calcs-1.7,W14-3907,0,0.0745822,"putational Approaches to Linguistic Code-Switching, pages 47–55 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_007 target sentences from Roman script to Devanagri script to utilize its pre-training. We further translate the English input to Hindi using pre-existing models and show improvements in the translation using parallel sentences as input to the mBART model. The code for our systems, along with error analysis, is public3 . The main contributions of our work are as follows: guage understanding tasks like language identification (Solorio et al., 2014; Molina et al., 2016), NER (Aguilar et al., 2018a; Rao and Devi, 2016), IR (Roy et al., 2013; Banerjee et al., 2018), PoS tagging (Jamatia et al., 2016), sentiment analysis (Patra et al., 2018; Patwa et al., 2020), and question answering (Chandu et al., 2018). Although these workshops have gained traction, the field lacks standard datasets to build robust systems. The small size of the datasets is a major factor that limits the scope of code-mixed systems. • We explore the effectiveness of fine-tuning mBART to translate to code-mixed sentences by utilizing the Hindi pre-training of the model"
2021.calcs-1.7,2020.wnut-1.7,0,0.158609,"n the last decade, while earlier works focused on statistical or rule-based approaches. Kalchbrenner and Blunsom (2013) first proposed a DNN model for translation, following which transformerbased approaches (Vaswani et al., 2017) have taken the stage. Some approaches utilize multilingual pre-training (Song et al., 2019; Conneau and Lample, 2019; Edunov et al., 2019; Liu et al., 2020); however, these works focus only on monolingual language pairs. Although a large number of multilingual speakers in a highly populous country like India use English-Hindi code-mixed language, only a few studies (Srivastava and Singh, 2020; Singh and Solorio, 2018; Dhar et al., 2018) have attempted the problem. Enabling translation systems in the following pair can bridge the communication gap between several people and further improve the state of globalization in the world. • We propose a normalized BLEU score metric to better account for the spelling variations in the code-mixed sentences. • Along with BLEU scores, we analyze the code-mixing quality of the reference translations along with the generated outputs and propose that for assessing code-mixed translations, measures of code-mixing should be part of evaluation and an"
2021.calcs-1.7,K19-1026,0,0.0235774,"d data generation is a plausible option to build resources for code-mixed language research and is a very similar task to translation. While translation focuses on retaining the meaning of the source sentence, generation is a simpler task requiring focus only on the quality of the synthetic data generated. Pratapa et al. (2018) started by exploring linguistic theories to generate code-mixed data. Later works attempt the problem using several approaches including Generative Adversarial Networks (Chang et al., 2019), an encoder-decoder framework (Gupta et al., 2020), pointer-generator networks (Winata et al., 2019), and a two-level Background Code-mixing occurs when a speaker switches between two or more languages in the context of the same conversation. It has become popular in multilingual societies with the rise of social media applications and messaging platforms. In attempts to progress the field of code-mixed data, several code-switching workshops (Diab et al., 2014, 2016; Aguilar et al., 2018b) have been organized in notable conferences. Most of the workshops include shared tasks on various of the lan3 https://github.com/devanshg27/cm_ translation 48 # of sentences # of tokens in source sentences"
2021.ecnlp-1.17,L16-1429,0,0.28229,"Pontiki et al., 2014), 2015 (Nakov et al., 2015), 2016 (Pontiki et al., 2016), and as a part of the overall task of sentiment analysis on Twitter in SemEval 2017 (Rosenthal et al., 2017). These tasks have garnered a lot of attention in various languages including Arabic, Chinese, Dutch, French, Russian, Spanish and Turkish. Each monolingual dataset consisted of one or two domains with each language having anywhere between 4,000 to 9,000 sentences overall (including the train and test split). For Indian languages, there has been some work in developing a dataset for aspect extraction in Hindi (Akhtar et al., 2016) and Telugu (Regatte et al., 2020). Limited work has been done on improving the state of AE and ABSA in Hindi beyond the development of a singular dataset, namely Akhtar et al. (2016). Existing evaluations show that existing sequence tagging models (both general and specific to AE) have performed very poorly on this dataset when their performance is compared to English AE as well as in similar sequence tagging tasks in Hindi such as named entity recognition (NER) and event detection. In this paper, we thoroughly analyze the existing dataset for AE in Hindi and explain the reason for the poor m"
2021.ecnlp-1.17,P19-1630,0,0.0265035,"rk being done in fine-graining downstream NLP tasks. One common method of fine-grained analysis is the use of aspect information. An aspect term is an entity of interest which identifies a unique aspect of a predefined topic or domain (Pontiki et al., 2014). For example, in the restaurant domain, service and seasoning are aspects. While aspect extraction (AE) has been often seen as a subtask of fine grained aspect-based sentiment analysis (ABSA), recent advances in literature have established it as an independent task which can be used in other downstream tasks as well, such as summarization (Frermann and Klementiev, 2019) and topic-specific information retrieval such as opinion mining (Asghar et al., 2019). Aspect extraction (as a subtask of aspect-based sentiment analysis) datasets and models have been developed for multiple languages. ABSA has been a shared task in SemEval 2014 (Pontiki et al., 2014), 2015 (Nakov et al., 2015), 2016 (Pontiki et al., 2016), and as a part of the overall task of sentiment analysis on Twitter in SemEval 2017 (Rosenthal et al., 2017). These tasks have garnered a lot of attention in various languages including Arabic, Chinese, Dutch, French, Russian, Spanish and Turkish. Each mono"
2021.ecnlp-1.17,L18-1550,0,0.0220077,"layers and a fully connected layer with softmax for label prediction (Xu et al., 2018). 3 https://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html • Seq2Seq4ATE: This model is a sequence-tosequence model for aspect terms extraction. The model uses a BiGRU encoder and a position aware attention variant of gated unit networks as a decoder with softmax for label prediction (Ma et al., 2019). For consistency, in all the above mentioned models, we use the FastText embeddings for word as well as character embeddings for both English and Hindi (Bojanowski et al., 2017; Mikolov et al., 2018; Grave et al., 2018). For the English dataset, we use the Pontiki et al. (2014) train-test split (3045 training to 800 test sentences and 2000 training to 676 test sentences in the ‘Laptop’ and ‘Restaurant’ domains respectively). For the Hindi dataset, we use a train-test split of 4062 train to 1355 test sentences based on Akhtar et al. (2020). For the LSTM based models, we use 128 unit LSTM layers, with a hidden size of 1024, and a dropout of 0.4 over 50 epochs. For the CNN based model, we use a 128 filter network with a kernel size of 5 and hidden embeddings of size 100 and dropout of 0.4 over 50 epochs. We fin"
2021.ecnlp-1.17,P04-1077,0,0.0113394,"annotator agreement score which is computed as follows: P¯ − P¯e 1 − P¯e (5) k X 1 Pi = nij (nij − 1) n(n − 1) j=1    k X 1  n2ij  − n = n(n − 1) P¯e = where β = κ= pj j=1 We then calculate Pi , the degree of agreement with the ith annotator as: the stringent translation/transliteration guidelines, lack of extensive vocabulary in the descriptions and less number of words per sentence, the ROUGE-L metric is a decent approximation of the translation quality provided by the annotators. The ROUGE-L metric also accounts for the relative free-word order nature and constituent rearrangement (Lin and Och, 2004). ROUGE-L is the comparison of the longest common subsequence between two translated phrases. Given the translations X of length m and Y of length n, the ROUGE-L score is given by: LCS(X, Y ) = m LCS(X, Y ) = n (1 + β 2 )Rlcs Plcs = Rlcs + β 2 Plcs k X (4) where P − Pe is the actual degree of agreement achieved and 1 − Pe is the degree of agreement above chance. Given N tokens to be annotated and n annotators, with k categories to annotate the data. We first calculate the proportion of annotations in The Fleiss’ Kappa scores of the aspect-aware, aspect-blind and overall translations are provid"
2021.ecnlp-1.17,D15-1168,0,0.0268528,"lingual models. The monolingual models are trained and tested on the individual language datasets while the multilingual models involve the use of transfer learning from the SemEval-2014 dataset to the dataset we have created. 3.1 Monolingual Aspect Extraction We evaluate our dataset against the existing Hindi dataset and the SemEval 2014 dataset using the following baselines: • CRF: We use a conditional random field with basic features3 such as word form and POS tag. • BiLSTM: We use a vanilla BiLSTM as a baseline model for aspect extraction as it is an established baseline in seq2seq tasks (Liu et al., 2015). • BiLSTM-CRF: We use a BiLSTM to encode the input sentence and a conditional random field for the sequence labeling. This is a commonly used baseline for sequence tagging tasks (Huang et al., 2015). We also use the following neural models for our analysis: • BiLSTM-CNN-CRF: The state-of-the-art in neural named entity recognition. The architecture uses both character and word level features in a CNN and BiLSTM respectively, and using a CRF for sequence labeling tasks (Reimers and Gurevych, 2017). We use a slightly modified version where word embeddings are generated by concatenating character"
2021.ecnlp-1.17,P19-1344,0,0.0114074,"Prabhu et al. (2019) for event detection in Hindi. • DeCNN: The commonly adopted model for aspect extraction specifically, this model uses a combination of general and domain based embeddings in multiple convolutional layers and a fully connected layer with softmax for label prediction (Xu et al., 2018). 3 https://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html • Seq2Seq4ATE: This model is a sequence-tosequence model for aspect terms extraction. The model uses a BiGRU encoder and a position aware attention variant of gated unit networks as a decoder with softmax for label prediction (Ma et al., 2019). For consistency, in all the above mentioned models, we use the FastText embeddings for word as well as character embeddings for both English and Hindi (Bojanowski et al., 2017; Mikolov et al., 2018; Grave et al., 2018). For the English dataset, we use the Pontiki et al. (2014) train-test split (3045 training to 800 test sentences and 2000 training to 676 test sentences in the ‘Laptop’ and ‘Restaurant’ domains respectively). For the Hindi dataset, we use a train-test split of 4062 train to 1355 test sentences based on Akhtar et al. (2020). For the LSTM based models, we use 128 unit LSTM layer"
2021.ecnlp-1.17,L18-1008,0,0.0292551,"multiple convolutional layers and a fully connected layer with softmax for label prediction (Xu et al., 2018). 3 https://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html • Seq2Seq4ATE: This model is a sequence-tosequence model for aspect terms extraction. The model uses a BiGRU encoder and a position aware attention variant of gated unit networks as a decoder with softmax for label prediction (Ma et al., 2019). For consistency, in all the above mentioned models, we use the FastText embeddings for word as well as character embeddings for both English and Hindi (Bojanowski et al., 2017; Mikolov et al., 2018; Grave et al., 2018). For the English dataset, we use the Pontiki et al. (2014) train-test split (3045 training to 800 test sentences and 2000 training to 676 test sentences in the ‘Laptop’ and ‘Restaurant’ domains respectively). For the Hindi dataset, we use a train-test split of 4062 train to 1355 test sentences based on Akhtar et al. (2020). For the LSTM based models, we use 128 unit LSTM layers, with a hidden size of 1024, and a dropout of 0.4 over 50 epochs. For the CNN based model, we use a 128 filter network with a kernel size of 5 and hidden embeddings of size 100 and dropout of 0.4 o"
2021.ecnlp-1.17,Q17-1010,0,0.0152855,"main based embeddings in multiple convolutional layers and a fully connected layer with softmax for label prediction (Xu et al., 2018). 3 https://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html • Seq2Seq4ATE: This model is a sequence-tosequence model for aspect terms extraction. The model uses a BiGRU encoder and a position aware attention variant of gated unit networks as a decoder with softmax for label prediction (Ma et al., 2019). For consistency, in all the above mentioned models, we use the FastText embeddings for word as well as character embeddings for both English and Hindi (Bojanowski et al., 2017; Mikolov et al., 2018; Grave et al., 2018). For the English dataset, we use the Pontiki et al. (2014) train-test split (3045 training to 800 test sentences and 2000 training to 676 test sentences in the ‘Laptop’ and ‘Restaurant’ domains respectively). For the Hindi dataset, we use a train-test split of 4062 train to 1355 test sentences based on Akhtar et al. (2020). For the LSTM based models, we use 128 unit LSTM layers, with a hidden size of 1024, and a dropout of 0.4 over 50 epochs. For the CNN based model, we use a 128 filter network with a kernel size of 5 and hidden embeddings of size 10"
2021.ecnlp-1.17,P19-1493,0,0.0241026,"ing Parallel Data As mentioned in section 2, the corpus we have developed aims to be a parallel corpus, which allows us to use language invariant, transfer learning based models for aspect extraction in Hindi. We use the BERT mutilingual sentence embeddings (Devlin et al., 2018) as the sentence representations for the English and Hindi on the (a) BiLSTM, (b) BiLSTM-CNN-CRF and (c) the Seq2Seq4ATE models, mentioned in Section 3.1. The BERT multilingual embeddings have been used for a variety of tasks in Hindi including machine comprehension (Gupta and Khade, 2020) and named entity recognition (Pires et al., 2019), among other sequence labeling tasks. Pires et al. (2019) showcases the model efficacy in using monolingual corpora for zero-shot code-mixed tasks as well, which would be useful for our corpus. 146 Model Akhtar et al. (2016) Pontiki et al. (2014) Our Dataset 54.97 61.01 62.61 47.07 54.77 50.26 73.03 77.67 78.86 67.08 68.35 68.61 Baselines CRF BiLSTM BILSTM-CRF 22.08 20.71 34.71 Neural SoTA Models BiLSTM-CNN-CRF DeCNN Seq2Seq4ATE 36.56 38.21 35.04 Table 3: F1 scores of established models on the monolingual aspect extraction task. Training Model Baseline BiLSTM BiLSTM-CNN-CRF Seq2Seq4ATE 41.06"
2021.ecnlp-1.17,S16-1002,0,0.490323,"ks have garnered a lot of attention in various languages including Arabic, Chinese, Dutch, French, Russian, Spanish and Turkish. Each monolingual dataset consisted of one or two domains with each language having anywhere between 4,000 to 9,000 sentences overall (including the train and test split). For Indian languages, there has been some work in developing a dataset for aspect extraction in Hindi (Akhtar et al., 2016) and Telugu (Regatte et al., 2020). Limited work has been done on improving the state of AE and ABSA in Hindi beyond the development of a singular dataset, namely Akhtar et al. (2016). Existing evaluations show that existing sequence tagging models (both general and specific to AE) have performed very poorly on this dataset when their performance is compared to English AE as well as in similar sequence tagging tasks in Hindi such as named entity recognition (NER) and event detection. In this paper, we thoroughly analyze the existing dataset for AE in Hindi and explain the reason for the poor model performance. We then propose the creation of a parallel corpus, by manually translating the SemEval-2014 ABSA corpus (Pontiki et al., 2014). We provide detailed guidelines and ch"
2021.ecnlp-1.17,S14-2004,0,0.282398,"set using state-of-the-art neural aspect extraction models in both monolingual and multilingual settings and show that the models perform far better on our corpus than on the existing Hindi dataset. With this, we establish our corpus as the gold-standard aspect extraction dataset in Hindi. 1 Introduction Recent literature has seen an increase in the amount of work being done in fine-graining downstream NLP tasks. One common method of fine-grained analysis is the use of aspect information. An aspect term is an entity of interest which identifies a unique aspect of a predefined topic or domain (Pontiki et al., 2014). For example, in the restaurant domain, service and seasoning are aspects. While aspect extraction (AE) has been often seen as a subtask of fine grained aspect-based sentiment analysis (ABSA), recent advances in literature have established it as an independent task which can be used in other downstream tasks as well, such as summarization (Frermann and Klementiev, 2019) and topic-specific information retrieval such as opinion mining (Asghar et al., 2019). Aspect extraction (as a subtask of aspect-based sentiment analysis) datasets and models have been developed for multiple languages. ABSA ha"
2021.ecnlp-1.17,2020.lrec-1.617,0,0.0307522,"v et al., 2015), 2016 (Pontiki et al., 2016), and as a part of the overall task of sentiment analysis on Twitter in SemEval 2017 (Rosenthal et al., 2017). These tasks have garnered a lot of attention in various languages including Arabic, Chinese, Dutch, French, Russian, Spanish and Turkish. Each monolingual dataset consisted of one or two domains with each language having anywhere between 4,000 to 9,000 sentences overall (including the train and test split). For Indian languages, there has been some work in developing a dataset for aspect extraction in Hindi (Akhtar et al., 2016) and Telugu (Regatte et al., 2020). Limited work has been done on improving the state of AE and ABSA in Hindi beyond the development of a singular dataset, namely Akhtar et al. (2016). Existing evaluations show that existing sequence tagging models (both general and specific to AE) have performed very poorly on this dataset when their performance is compared to English AE as well as in similar sequence tagging tasks in Hindi such as named entity recognition (NER) and event detection. In this paper, we thoroughly analyze the existing dataset for AE in Hindi and explain the reason for the poor model performance. We then propose"
2021.ecnlp-1.17,D17-1035,0,0.0177321,"vanilla BiLSTM as a baseline model for aspect extraction as it is an established baseline in seq2seq tasks (Liu et al., 2015). • BiLSTM-CRF: We use a BiLSTM to encode the input sentence and a conditional random field for the sequence labeling. This is a commonly used baseline for sequence tagging tasks (Huang et al., 2015). We also use the following neural models for our analysis: • BiLSTM-CNN-CRF: The state-of-the-art in neural named entity recognition. The architecture uses both character and word level features in a CNN and BiLSTM respectively, and using a CRF for sequence labeling tasks (Reimers and Gurevych, 2017). We use a slightly modified version where word embeddings are generated by concatenating character embeddings, as done by Prabhu et al. (2019) for event detection in Hindi. • DeCNN: The commonly adopted model for aspect extraction specifically, this model uses a combination of general and domain based embeddings in multiple convolutional layers and a fully connected layer with softmax for label prediction (Xu et al., 2018). 3 https://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html • Seq2Seq4ATE: This model is a sequence-tosequence model for aspect terms extraction. The model uses a B"
2021.ecnlp-1.17,S17-2088,0,0.0576105,"Missing"
2021.ecnlp-1.17,P18-2094,0,0.0177429,"d entity recognition. The architecture uses both character and word level features in a CNN and BiLSTM respectively, and using a CRF for sequence labeling tasks (Reimers and Gurevych, 2017). We use a slightly modified version where word embeddings are generated by concatenating character embeddings, as done by Prabhu et al. (2019) for event detection in Hindi. • DeCNN: The commonly adopted model for aspect extraction specifically, this model uses a combination of general and domain based embeddings in multiple convolutional layers and a fully connected layer with softmax for label prediction (Xu et al., 2018). 3 https://sklearn-crfsuite.readthedocs. io/en/latest/tutorial.html • Seq2Seq4ATE: This model is a sequence-tosequence model for aspect terms extraction. The model uses a BiGRU encoder and a position aware attention variant of gated unit networks as a decoder with softmax for label prediction (Ma et al., 2019). For consistency, in all the above mentioned models, we use the FastText embeddings for word as well as character embeddings for both English and Hindi (Bojanowski et al., 2017; Mikolov et al., 2018; Grave et al., 2018). For the English dataset, we use the Pontiki et al. (2014) train-te"
2021.gem-1.14,N19-4010,0,0.0627586,"Missing"
2021.gem-1.14,2020.acl-main.424,0,0.057314,"& syntactic complexity as attributes to gauge the transformations between complex and simple sentence pairs. We then represent each of these attributes as numerical measures, which are then added to our data. We show that this provides a considerable improvement over as-is Transformer approaches. 2.1 Background Sentence Simplification Past approaches towards sentence simplification have dealt with it as a monolingual machine translation(MT) task (specifically Seq2Seq MT (Sutskever et al., 2014)). This meant training MT architectures over complex-simple sentence pairs, either aligned manually (Alva-Manchego et al., 2020; Xu et al., 2016) or automatically (Zhu et al., 2010; Wubben et al., 2012) using large complex-simple repository pairs such as the English Wikipedia and the Simple English Wikipedia. Some implementations also utilize reinforcement learning (Zhang and Lapata, 2017) over the MT task, with automated metrics such as SARI (Xu et al., 2016), information preservation, and grammatical fluency constituting the training reward. 2.2 Controllable Text Generation A recent approach towards sentence simplification involves using control tokens during machine translation (Martin et al., 2020). For simplifica"
2021.gem-1.14,W14-1215,0,0.01538,"generated simplifications according to user desired attributes. Additionally, we show that NER-tagging the training data before use helps stabilize the effect of the control tokens and significantly improves the overall performance of the system. We also employ pretrained embeddings to reduce data sparsity and allow the model to produce more generalizable outputs. 1 2 Introduction Sentence simplification aims at reducing the linguistic complexity of a given text, while preserving all the relevant details of the initial text. This is particularly useful for people with cognitive disabilities (Evans et al., 2014), as well as for second language learners and people with low-literacy levels (Watanabe et al., 2009). Text and Sentence simplification also play an important role within NLP. Simplification has been utilized as a preprocessing step in larger NLP pipelines, which can greatly aid learning by reducing vocabulary and regularizing of syntax. In our model, we use control tokens to tune a Seq2Seq Transformer model (Vaswani et al., 2017) for sentence simplification. We take character length compression, extent of paraphrase, and lexical & syntactic complexity as attributes to gauge the transformation"
2021.gem-1.14,W18-2706,0,0.0293861,"nguistics Control Attribute Amount of compression Paraphrasing Lexical complexity Syntactic complexity Control Measure Compression ratio Levenshtein similarity Avg. third-quartile of log-ranks Max dependency tree depth Control Token <NbChars x.xx&gt; <LevSim x.xx&gt; <WordRank x.xx&gt; <DepTreeDepth x.xx&gt; Table 1: Control Tokens used for Modelling encodes and enforces changes in certain attributes of the text. Similar approaches for controlling generated text have been explored in other domains: Filippova (2020) uses control tokens to estimate and control the amount of hallucination in generated text, Fan et al. (2018) explored pre-pending control tokens to the input text for summarization, providing control over the length of the output, and customizing text generation for different sources. Our model makes use of control tokens similar to Martin et al. (2020) to tailor the generated simplifications according to the extent of changes in the following attributes: character length, extent of paraphrasing, and lexical & syntactic complexity. These attributes are represented by their respective numerical measures (see 3.1), and then pre-pended to the complex sentences using in specific formats (Table 1). Along"
2021.gem-1.14,2020.findings-emnlp.76,0,0.071207,"Missing"
2021.gem-1.14,P02-1040,0,0.115653,"duct@1.” ”entrance to tsinghua is very very difficult.” ”the entrance to tsinghua is very very simple .” Table 4: Sample outputs of the baseline(T) and SimpleNER models on the TurkCorpus-testset 10 human-annotated simplifications for each of the 2359 source sentences, whereas TurCorpus provides 8. Apart from lower-casing all three splits of the data, the data pairs of the trainset with token length lower than 3 were removed, and sentence pairs with compression ratio (len(target)/len(source)) beyond the bounds [0.2, 1.5] were omitted. 4.3 Evaluation Metrics Our model is evaluated on both BLEU (Papineni et al., 2002) and SARI (Xu et al., 2016). But as Martin et al. (2020) points out, BLEU favours directly replicating the source sentence because of a high N-Gram similarity between the source and target sentences in most sentence simplification datasets. Therefore we only use SARI to rate and compare the models. We also make use of SARI to choose the best performing checkpoints on the validation sets of each of the tracks for evaluation on their respective test sets. 4.4 Training All models were trained on 4 Nvidia GeForce GTX 1080 Ti GPUs with 64 GB of vRAM. Training was carried out for 20 epochs, and took"
2021.gem-1.14,P12-1107,0,0.0688979,"Missing"
2021.gem-1.14,Q16-1029,0,0.372055,"tributes to gauge the transformations between complex and simple sentence pairs. We then represent each of these attributes as numerical measures, which are then added to our data. We show that this provides a considerable improvement over as-is Transformer approaches. 2.1 Background Sentence Simplification Past approaches towards sentence simplification have dealt with it as a monolingual machine translation(MT) task (specifically Seq2Seq MT (Sutskever et al., 2014)). This meant training MT architectures over complex-simple sentence pairs, either aligned manually (Alva-Manchego et al., 2020; Xu et al., 2016) or automatically (Zhu et al., 2010; Wubben et al., 2012) using large complex-simple repository pairs such as the English Wikipedia and the Simple English Wikipedia. Some implementations also utilize reinforcement learning (Zhang and Lapata, 2017) over the MT task, with automated metrics such as SARI (Xu et al., 2016), information preservation, and grammatical fluency constituting the training reward. 2.2 Controllable Text Generation A recent approach towards sentence simplification involves using control tokens during machine translation (Martin et al., 2020). For simplification, it 1 https:/"
2021.gem-1.14,2020.acl-main.577,0,0.0282777,"WordRank 0.75&gt; oxygen is a chemical element with symbol o and atomic number 8 . Prediction: It has the chemical symbol o . It has the atomic number 8 . Here, the proper noun ”Oxygen” is replaced by the pronoun ”it”. Although the model follows the requirement of bringing down the word rank of the sentence and remains grammatically sound, it doesn’t help with the simplification. To address the issue of data sparsity as well that of unwanted NE-replacement, we propose NER mapping the data before training, and replacing the NE-tokens back after generation. We make use of the Ontonotes NER tagger (Yu et al., 2020) in the Flair toolkit (Akbik et al., 2019). We identify named entities in the complex halves of all three of the data splits and replace them with one of 18 tags (from the NER tagger) with a unique index (Table 2). NER replacement for simplification was previously explored by Zhang and Lapata (2017), but consisted of fewer classes. The large number of tags allow for a fine division between different named-entity types, which helps the model to encode the contexts of each of the types better while still reducing the NE-vocabulary size substantially. The tagged data is then used for training and"
2021.gem-1.14,D17-1062,0,0.256686,"as-is Transformer approaches. 2.1 Background Sentence Simplification Past approaches towards sentence simplification have dealt with it as a monolingual machine translation(MT) task (specifically Seq2Seq MT (Sutskever et al., 2014)). This meant training MT architectures over complex-simple sentence pairs, either aligned manually (Alva-Manchego et al., 2020; Xu et al., 2016) or automatically (Zhu et al., 2010; Wubben et al., 2012) using large complex-simple repository pairs such as the English Wikipedia and the Simple English Wikipedia. Some implementations also utilize reinforcement learning (Zhang and Lapata, 2017) over the MT task, with automated metrics such as SARI (Xu et al., 2016), information preservation, and grammatical fluency constituting the training reward. 2.2 Controllable Text Generation A recent approach towards sentence simplification involves using control tokens during machine translation (Martin et al., 2020). For simplification, it 1 https://github.com/kvadityasrivatsa/ gem_2021_simplification_task 155 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 155–160 August 5–6, 2021. ©2021 Association for Computational Linguistics Cont"
2021.gem-1.14,C10-1152,0,0.046509,"ns between complex and simple sentence pairs. We then represent each of these attributes as numerical measures, which are then added to our data. We show that this provides a considerable improvement over as-is Transformer approaches. 2.1 Background Sentence Simplification Past approaches towards sentence simplification have dealt with it as a monolingual machine translation(MT) task (specifically Seq2Seq MT (Sutskever et al., 2014)). This meant training MT architectures over complex-simple sentence pairs, either aligned manually (Alva-Manchego et al., 2020; Xu et al., 2016) or automatically (Zhu et al., 2010; Wubben et al., 2012) using large complex-simple repository pairs such as the English Wikipedia and the Simple English Wikipedia. Some implementations also utilize reinforcement learning (Zhang and Lapata, 2017) over the MT task, with automated metrics such as SARI (Xu et al., 2016), information preservation, and grammatical fluency constituting the training reward. 2.2 Controllable Text Generation A recent approach towards sentence simplification involves using control tokens during machine translation (Martin et al., 2020). For simplification, it 1 https://github.com/kvadityasrivatsa/ gem_2"
2021.gem-1.14,N19-4009,0,0.0179726,"ter generalizability). 3.3 Pre-Trained Embeddings The vocabulary of a model trained on a corpus like WikiLarge is quite small, which prevents the model from predicting better fitting tokens. To address this, we use FastText’s pre-trained embeddings (Bojanowski et al., 2016) (dimensionality: 300) as input embeddings for our model. The embeddings significantly boost the vocabulary size of usable content words for the model. 4 Experimental Setup 4.1 Architecture Our architecture is a Transformer Model (Vaswani et al., 2017), and we make use of the Transformer Seq2Seq implementation from FairSeq (Ott et al., 2019). To understand the impact of each of the proposed methods, we train a total of four models: • T: Vanilla Transformer (Vaswani et al., 2017), with control tokens, used as a baseline model. • T+Pre: Transformer trained with FastText’s pretrained embeddings. • T+NER: Transformer trained on NER mapped data. • SimpleNER (T+Pre+NER): Transformer trained on NER mapped data with FastText’s pretrained embeddings. For ease of comparison, all four models were trained with an input embedding dimensionality of 300, fully connected layers with a dimensionality of 2048, 6 layers and 6 attention heads on bot"
2021.mtsummit-loresmt.12,P07-2045,0,0.0934271,"en methods to create translation system. In data driven methods - statistical (Koehn et al., 2003) and neural methods (Bahdanau et al., 2014) have been employed to build decent MT systems in resource setting like English ⇐⇒ French. In LoResMT shared task (Ojha et al., 2021) we are dealing with low resource setting for English, Marathi pair. According to Koehn and Knowles (2017), compared to statistical methods neural methods have a drawback when used in low resource setting. Hence, for this shared task we are using only phrase based statistical models to build translation models using Moses1 (Koehn et al., 2007). Marathi is morphologically richer, agglutinative language when compared to English. Also, former follows SOV as canonical syntactic structure while latter follows SVO. Level of difference in morphological richness and syntactic divergence between the two languages suggests to look for methods which can help to address them to certain extent in phrase based statistical models. Since we are in low resource setting, to address data sparsity problem, we use various tokenization schemes, e.g. BPE (Sennrich et al., 2016b), morfessor (Virpioja et al., 2013). Combinations of these tokenization schem"
2021.mtsummit-loresmt.12,W17-3204,0,0.017514,"and results produced by them. 1 Introduction Machine Translation systems are systems which translate from source language to target. There are multiple ways of creating such a system - rule based, data driven, hybrid etc. We are using data driven methods to create translation system. In data driven methods - statistical (Koehn et al., 2003) and neural methods (Bahdanau et al., 2014) have been employed to build decent MT systems in resource setting like English ⇐⇒ French. In LoResMT shared task (Ojha et al., 2021) we are dealing with low resource setting for English, Marathi pair. According to Koehn and Knowles (2017), compared to statistical methods neural methods have a drawback when used in low resource setting. Hence, for this shared task we are using only phrase based statistical models to build translation models using Moses1 (Koehn et al., 2007). Marathi is morphologically richer, agglutinative language when compared to English. Also, former follows SOV as canonical syntactic structure while latter follows SVO. Level of difference in morphological richness and syntactic divergence between the two languages suggests to look for methods which can help to address them to certain extent in phrase based"
2021.mtsummit-loresmt.12,N03-1017,0,0.147925,"data and further train augmented dataset to create more statistical models. Also, we reorder English to match Marathi syntax to further train another set of baseline and data augmented models using various tokenization schemes. We report configuration of the submitted systems and results produced by them. 1 Introduction Machine Translation systems are systems which translate from source language to target. There are multiple ways of creating such a system - rule based, data driven, hybrid etc. We are using data driven methods to create translation system. In data driven methods - statistical (Koehn et al., 2003) and neural methods (Bahdanau et al., 2014) have been employed to build decent MT systems in resource setting like English ⇐⇒ French. In LoResMT shared task (Ojha et al., 2021) we are dealing with low resource setting for English, Marathi pair. According to Koehn and Knowles (2017), compared to statistical methods neural methods have a drawback when used in low resource setting. Hence, for this shared task we are using only phrase based statistical models to build translation models using Moses1 (Koehn et al., 2007). Marathi is morphologically richer, agglutinative language when compared to En"
2021.mtsummit-loresmt.12,kunchukuttan-etal-2014-shata,0,0.0246036,"tems. We elevate the amount of learning, the reordering model of SMT has to do, by making use of rule based reordering system 1 http://statmt.org/moses/ Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 124 Dataset Language Monolingual training dev Baseline and Reordered Baseline English Marathi 34891 40972 20651 500 Augmented and Reordered Augmented English Marathi 56789 57569 59146 500 Table 1: Data statistics, number of sentences for each set of experiments (Patel et al., 2013), (Kunchukuttan et al., 2014) to reorder English to match Marathi syntax. With this we build another set of baseline systems for reordered English, Marathi pair. Like in baseline systems, mentioned above, here also we make use of various tokenization schemes. After comparing these schemes, we create synthetic dataset using back translation to augment reordered English, Marathi pair. Subsequent sections give more detailed overview of the systems developed. 2 SMT Systems We use SMT model to make initial baseline systems using various tokenization schemes. We further make use of rule based reordering model to create another"
2021.mtsummit-loresmt.12,P03-1021,0,0.118319,"edings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 125 2.2 Translation Models We made use of Moses toolkit (Koehn et al., 2007) to build statistical models trained with various tokenized bitext pairs. We also use GIZA++ (Och and Ney, 2003) to find alignments between parallel text and grow-diag-final-and method (Koehn et al., 2003) to extract aligned phrases. And utilize KenLM (Heafield, 2011) to train a trigram model with kneser ney smoothing on monolingual corpus of both languages. MERT (Och, 2003) is used for tuning the trained models. We also trained a reordering system for Reordered English to English so that we can have Reordered English as pseudo-pivot language. 2.2.1 Transliteration Module Since we are building systems in low resource setting, its entirely possible to get unknown words while translating. To see if we can also counter unknown words in this resource constrained environments, we also made a small transliteration system. First a phrase based model was trained on English Marathi bitext using Moses(Koehn et al., 2007) with max phrase length5 set to 1 to find tokens with"
2021.mtsummit-loresmt.12,J03-1002,0,0.0305048,"ized using BPE into subword. • Morf: text tokenized using morfessor. 2 https://anoopkunchukuttan.github.io/indic_nlp_library/ 3 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/ tokenizer.perl 4 https://www.cfilt.iitb.ac.in/static/download.html Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 125 2.2 Translation Models We made use of Moses toolkit (Koehn et al., 2007) to build statistical models trained with various tokenized bitext pairs. We also use GIZA++ (Och and Ney, 2003) to find alignments between parallel text and grow-diag-final-and method (Koehn et al., 2003) to extract aligned phrases. And utilize KenLM (Heafield, 2011) to train a trigram model with kneser ney smoothing on monolingual corpus of both languages. MERT (Och, 2003) is used for tuning the trained models. We also trained a reordering system for Reordered English to English so that we can have Reordered English as pseudo-pivot language. 2.2.1 Transliteration Module Since we are building systems in low resource setting, its entirely possible to get unknown words while translating. To see if we can"
2021.mtsummit-loresmt.12,W13-2807,0,0.0172452,"thod to build more systems. We elevate the amount of learning, the reordering model of SMT has to do, by making use of rule based reordering system 1 http://statmt.org/moses/ Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 124 Dataset Language Monolingual training dev Baseline and Reordered Baseline English Marathi 34891 40972 20651 500 Augmented and Reordered Augmented English Marathi 56789 57569 59146 500 Table 1: Data statistics, number of sentences for each set of experiments (Patel et al., 2013), (Kunchukuttan et al., 2014) to reorder English to match Marathi syntax. With this we build another set of baseline systems for reordered English, Marathi pair. Like in baseline systems, mentioned above, here also we make use of various tokenization schemes. After comparing these schemes, we create synthetic dataset using back translation to augment reordered English, Marathi pair. Subsequent sections give more detailed overview of the systems developed. 2 SMT Systems We use SMT model to make initial baseline systems using various tokenization schemes. We further make use of rule based reorde"
2021.mtsummit-loresmt.12,W18-6319,0,0.0117318,"nsliteration as translation problem). Since transliteration system is trained on very small corpus and hence prone to error, for each output from SMT translation system we give two outputs. One in which we made use of transliteration system for unknown words and another one in which we did not. 2.2.2 Performance on Dev sets We used dev set to evaluate above mentioned models and all models which are described later on. Outputs were post processed according to the tokenization scheme of respective target language in each model, and then detokenized. After evaluating all systems using sacrebleu (Post, 2018), Table 2 lists the result on dev sets for baseline systems trained in English to Marathi Direction and Table 3 lists the result of systems trained on Marathi to English direction. If we Tokenization Scheme Baseline SMT unk EnTok MrTok EnBPE MrBPE EnMorf MrMorf EnTok MrBPE EnTok MrMorf EnBPE MrTok EnBPE MrMorf EnMorf MrTok EnMorf MrBPE 58.2 50.1 41.3 49.3 47.4 54.3 46.0 51.4 44.3 transliterate 58.1 50.1 41.3 49.1 47.3 54.3 46.0 51.3 44.2 Augment SMT unk 57.6 52.0 43.6 50.7 47.3 54.4 48.0 50.9 45.9 transliterate 57.5 51.9 43.6 50.6 47.2 54.4 48.0 50.9 45.8 Baseline Reordered SMT transliunk tera"
2021.mtsummit-loresmt.12,P16-1009,0,0.490389,"g only phrase based statistical models to build translation models using Moses1 (Koehn et al., 2007). Marathi is morphologically richer, agglutinative language when compared to English. Also, former follows SOV as canonical syntactic structure while latter follows SVO. Level of difference in morphological richness and syntactic divergence between the two languages suggests to look for methods which can help to address them to certain extent in phrase based statistical models. Since we are in low resource setting, to address data sparsity problem, we use various tokenization schemes, e.g. BPE (Sennrich et al., 2016b), morfessor (Virpioja et al., 2013). Combinations of these tokenization schemes are used with SMT based method to create a baseline systems. After checking the optimal tokenization scheme, we use that scheme to augment training data with synthetic dataset using back translation (Sennrich et al., 2016a). As was the case in baseline systems, augmented dataset goes through prepossessing with various tokenization schemes and SMT method to build more systems. We elevate the amount of learning, the reordering model of SMT has to do, by making use of rule based reordering system 1 http://statmt.org"
2021.mtsummit-loresmt.12,P16-1162,0,0.421526,"g only phrase based statistical models to build translation models using Moses1 (Koehn et al., 2007). Marathi is morphologically richer, agglutinative language when compared to English. Also, former follows SOV as canonical syntactic structure while latter follows SVO. Level of difference in morphological richness and syntactic divergence between the two languages suggests to look for methods which can help to address them to certain extent in phrase based statistical models. Since we are in low resource setting, to address data sparsity problem, we use various tokenization schemes, e.g. BPE (Sennrich et al., 2016b), morfessor (Virpioja et al., 2013). Combinations of these tokenization schemes are used with SMT based method to create a baseline systems. After checking the optimal tokenization scheme, we use that scheme to augment training data with synthetic dataset using back translation (Sennrich et al., 2016a). As was the case in baseline systems, augmented dataset goes through prepossessing with various tokenization schemes and SMT method to build more systems. We elevate the amount of learning, the reordering model of SMT has to do, by making use of rule based reordering system 1 http://statmt.org"
2021.sdp-1.9,2020.emnlp-main.750,0,0.0987672,"Missing"
2021.sdp-1.9,N18-2097,0,0.028158,"etraining. The best model is selected on the basis of validation ROUGE scores for one-line summaries on the validation set. This is done to select the model with the best &quot;extreme&quot; summarization capability. When evaluating on Pubmed, the number of sentences extracted is set to 6, as reported in (Zhong et al., 2020). For fine-tuning on S CI T LDRA as well as S CI T LDR-AIC, the batch size is set to 100 and the number of extracted sentences to form the final summary is 1. Experimental Setup Summarization Datasets We evaluate the models on two scientific summarization benchmark datasets— Pubmed (Cohan et al., 2018) and S CI T LDR (Cachola et al., 2020). We use the CNN/DM (Hermann et al., 2015) dataset for intermediate pretraining. S CI T LDR. S CI T LDR is a curated corpus containing computer science articles, with each article having one or more reference T LDR’s or one-sentence summaries. The inputs could either be abstractonly (S CI T LDR-A) or the abstract, introduction and conclusion sections of the article (S CI T LDRAIC). We present results for both settings and use the splits specified in (Cachola et al., 2020). Evaluation Metrics. The S CI T LDR tasks have multiple reference summaries for each"
2021.sdp-1.9,2020.emnlp-main.748,0,0.0342064,"in NLP and its benefits on the 6 tasks of classification. The benefits of this in summarization has been shown by Yu et al. (2021) where in they finetune BART (Lewis et al., 2020) on XSUM (Narayan et al., 2018) and show its results on low resource domain adaptation benchmark for summarization . We show the effects of intermediate pretraining in the context of scientific document summarization. Scientific Summarization Cachola et al. (2020) introduce the S CI T LDR task and benchmark a variety of summarization models such as MatchSum and BERTSUM on the task. Impressive results were reported by Pilault et al. (2020), Zaheer et al. (2020) on scientific datasets like Pubmed, arXiv using compute-intensive transformer based models. We report results on Pubmed and S CI T LDR where our models use significantly less compute and achieve superior results on S CI T LDR over BERTSUM and MatchSum. • Intermediate pretraining using labeled summarization datasets (even when containing articles that are very different in domain from scientific articles) is very beneficial to lowresource target tasks like S CI T LDR. We also derive additional benefits by filtering the intermediate pretraining data to only retain a subset"
2021.sdp-1.9,2020.acl-main.703,0,0.184753,".iitb.ac.in 73 Proceedings of the Second Workshop on Scholarly Document Processing, pages 73–82 June 10, 2021. ©2021 Association for Computational Linguistics entific summarization benchmark, S CI T LDR (Cachola et al., 2020). We also make the following key observations: ing summarization on scientific documents is an overlooked area. Intermediate Pretraining Howard and Ruder (2018) first introduced the idea of intermediate pretraining in NLP and its benefits on the 6 tasks of classification. The benefits of this in summarization has been shown by Yu et al. (2021) where in they finetune BART (Lewis et al., 2020) on XSUM (Narayan et al., 2018) and show its results on low resource domain adaptation benchmark for summarization . We show the effects of intermediate pretraining in the context of scientific document summarization. Scientific Summarization Cachola et al. (2020) introduce the S CI T LDR task and benchmark a variety of summarization models such as MatchSum and BERTSUM on the task. Impressive results were reported by Pilault et al. (2020), Zaheer et al. (2020) on scientific datasets like Pubmed, arXiv using compute-intensive transformer based models. We report results on Pubmed and S CI T LDR"
2021.sdp-1.9,2020.emnlp-main.635,0,0.0401932,"Missing"
2021.semeval-1.180,2020.acl-main.398,0,0.0784367,"Missing"
2021.semeval-1.180,D15-1075,0,0.0437525,"candidates. 2. This is for four main reasons. Figure 1: An example from the SEM-TAB-FACTS dataset: Table A1 From 10262.xml along with its caption and legend. Some example statements of each class associated with this table are also shown. The highlighted cells are the relevant cells for entailed statement 2. 2 Background Verifying if the given textual evidence supports a given statement is a fundamental natural language processing problem. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, largescale pre-trained models (Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-tohuman performance. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verificatio"
2021.semeval-1.180,N19-1423,0,0.0467806,"its caption and legend. Some example statements of each class associated with this table are also shown. The highlighted cells are the relevant cells for entailed statement 2. 2 Background Verifying if the given textual evidence supports a given statement is a fundamental natural language processing problem. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, largescale pre-trained models (Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-tohuman performance. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical for"
2021.semeval-1.180,2020.findings-emnlp.27,0,0.0808074,"Missing"
2021.semeval-1.180,P17-1167,0,0.0342222,"Missing"
2021.semeval-1.180,P15-1142,0,0.116567,"for TAB FACT are semantic parsing approaches similar to LPA. Table-BERT encodes the linearized tables and statements using BERT-based models and directly pre1 https://www.wikipedia.org/ dicts the entailment decision. Zhang et al. (2020) inject table structural information into the mask of the self-attention layer of BERT-based models, which helps the model learn better table representations. TAPAS (Herzig et al., 2020) extends BERT’s architecture to capture the tabular structure, and it showed competitive performance on various table question answering datasets: SQA (Iyyer et al., 2017), WTQ (Pasupat and Liang, 2015) and WikiSQL (Zhong et al., 2017). Eisenschlos et al. (2020) add an intermediate pre-training step before the fine-tuning step to TAPAS and show that it achieves state-of-the-art results on TAB FACT and SQA (Iyyer et al., 2017). Their model is still 8 points behind human performance on TAB FACT since tabular fact verification involves table understanding and complex reasoning. While TAB FACT also focuses on fact verification using tables as evidence, it focuses on tables from Wikipedia, whereas SemEval-2021 Task 9 (SEM-TAB-FACTS) instead focuses on tables from scientific articles and has a sub"
2021.semeval-1.180,N18-1202,0,0.0135032,"nd. Some example statements of each class associated with this table are also shown. The highlighted cells are the relevant cells for entailed statement 2. 2 Background Verifying if the given textual evidence supports a given statement is a fundamental natural language processing problem. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, largescale pre-trained models (Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-tohuman performance. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical forms) and executes the"
2021.semeval-1.180,2020.coling-main.466,0,0.0182565,"NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical forms) and executes the programs against the table to predict the entailment decision. Most of the current models (Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) for TAB FACT are semantic parsing approaches similar to LPA. Table-BERT encodes the linearized tables and statements using BERT-based models and directly pre1 https://www.wikipedia.org/ dicts the entailment decision. Zhang et al. (2020) inject table structural information into the mask of the self-attention layer of BERT-based models, which helps the model learn better table representations. TAPAS (Herzig et al., 2020) extends BERT’s architecture to capture the tabular structure, and it showed competitive performance on various table question answering datasets: SQA (Iyyer"
2021.semeval-1.180,P17-2034,0,0.0234547,"tailed statement 2. 2 Background Verifying if the given textual evidence supports a given statement is a fundamental natural language processing problem. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, largescale pre-trained models (Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-tohuman performance. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical forms) and executes the programs against the table to predict the entailment decision. Most of the current models (Zhong et al., 2020; Shi et al., 2020; Yang e"
2021.semeval-1.180,P19-1644,0,0.0186232,"nd Verifying if the given textual evidence supports a given statement is a fundamental natural language processing problem. It has been extensively studied under different tasks such as RTE (Recognizing Textual Entailment) (Dagan et al., 2006), NLI (Natural Language Inference) (Bowman et al., 2015), FEVER (Fact Extraction and VERification) (Thorne et al., 2018). In recent years, largescale pre-trained models (Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019; Liu et al., 2019) have dominated these tasks and have achieved close-tohuman performance. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical forms) and executes the programs against the table to predict the entailment decision. Most of the current models (Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) for TAB FACT are"
2021.semeval-1.180,N18-1074,0,0.0451055,"Missing"
2021.semeval-1.180,2020.acl-main.539,0,0.0171085,"ohuman performance. NLVR (Suhr et al., 2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical forms) and executes the programs against the table to predict the entailment decision. Most of the current models (Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) for TAB FACT are semantic parsing approaches similar to LPA. Table-BERT encodes the linearized tables and statements using BERT-based models and directly pre1 https://www.wikipedia.org/ dicts the entailment decision. Zhang et al. (2020) inject table structural information into the mask of the self-attention layer of BERT-based models, which helps the model learn better table representations. TAPAS (Herzig et al., 2020) extends BERT’s architecture to capture the tabular structure, and it showed competitive performance on various table question answering da"
2021.semeval-1.180,2021.semeval-1.39,0,0.0372299,"cell selection followed by aggregation), we use the TAPAS architecture previously used for table question-answering and fine-tune it to select the relevant cells. We also evaluate how different fine-tuning strategies can improve TAPAS’ performance on evidence finding. Introduction There has been extensive work on verifying if a given textual context supports a given statement. Even though tables are also widely used to convey information, especially in scientific texts, there has been comparatively less work on verifying if a given table supports a statement. To this end, SemEval 2021 Task 9 (Wang et al., 2021) focuses on statement verification and evidence finding for tables from scientific articles in the English language. The task is divided into two subtasks - A and B. The aim of subtask A is to classify whether a given ∗ The authors have contributed equally. Our systems achieve an F1-micro score of 67.34 in subtask A and 72.89 in subtask A if the unknown statements are not considered while calculating the metrics (however, classifying entailed/refuted statements as unknown is still penalized). Our submitted system achieves an F1 score of 62.95 in subtask B. During the post-evaluation phase, we"
2021.semeval-1.180,2020.emnlp-main.628,0,0.0206923,"2017) and NLVR2 (Suhr et al., 2019) focus on verifying a statement given an image as evidence. TAB FACT (Chen et al., 2020) focuses on verifying a statement given a table from Wikipedia1 as evidence. Along with releasing TAB FACT, Chen et al. (2020) also discuss two promising approaches for tabular fact verification, Latent Program Algorithm(LPA) and Table-BERT. LPA is a semantic parsing approach that parses statements into programs (logical forms) and executes the programs against the table to predict the entailment decision. Most of the current models (Zhong et al., 2020; Shi et al., 2020; Yang et al., 2020) for TAB FACT are semantic parsing approaches similar to LPA. Table-BERT encodes the linearized tables and statements using BERT-based models and directly pre1 https://www.wikipedia.org/ dicts the entailment decision. Zhang et al. (2020) inject table structural information into the mask of the self-attention layer of BERT-based models, which helps the model learn better table representations. TAPAS (Herzig et al., 2020) extends BERT’s architecture to capture the tabular structure, and it showed competitive performance on various table question answering datasets: SQA (Iyyer et al., 2017), WTQ"
2021.sigdial-1.17,D19-5409,0,0.025987,"). However, we see that they fail to have regard of sentence-level dependencies leading to fragmented segmentation (Takanobu et al., 2018). Various supervised methods approached this task as a classifiAnnotation Framework We use the human-transcribed conversations from the NXT-format Switchboard corpus (Calhoun et al., 2010) in our task. In this dataset, participants are given a topic prompt and were asked to converse with each other for around ten minutes. This dataset was chosen for annotation, amongst others, as some did not have enough turns to observe a topic shift [(Lowe et al., 2015), (Gliwa et al., 2019)] or had fixed topics of conversation [(McCowan et al., 2005), (Janin et al., 2003)] neither of which were favourable for us to model an off-topic shift detection for open-domain conversations. In Switchboard, we observe the freedom with which the participants drift from the given topic prompt, leading to different off-topic threads in the conversation and several statements by the users to steer the conversation back to the original topic. To model this property, we annotated the dataset, into three labels - major, minor and off-topic tags. Dialogues are inherently hierarchical in structure,"
2021.sigdial-1.17,C04-1110,0,0.0587423,"Missing"
2021.sigdial-1.17,J97-1003,0,0.556452,"e in solving a user’s problem (Horvitz, 1999), by detecting the major topic of the conversation and steering the user towards it in case of a diversion. Related work A good conversation is one which focuses on a balance between staying on topic and changing it in an interactive multi-turn conversation system (See et al., 2019). Detection of what constitutes as ontopic can be viewed as segmentation of conversation into relevant and irrelevant of the conversation (Stewart et al., 2006). Earlier work in segmenting conversations into topics expected a high lexical cohesion within a topic segment (Hearst, 1997). However, we see that they fail to have regard of sentence-level dependencies leading to fragmented segmentation (Takanobu et al., 2018). Various supervised methods approached this task as a classifiAnnotation Framework We use the human-transcribed conversations from the NXT-format Switchboard corpus (Calhoun et al., 2010) in our task. In this dataset, participants are given a topic prompt and were asked to converse with each other for around ten minutes. This dataset was chosen for annotation, amongst others, as some did not have enough turns to observe a topic shift [(Lowe et al., 2015), (G"
2021.sigdial-1.17,L18-1016,0,0.0247785,"go back to the major topic of the conversation, when it detects a topic shift from it. Results We fine-tune BERT by taking a pre-trained model, adding an additional untrained classifier layer and training this new model for our task. This is done because pre-trained Transformer model weights 164 Setup The major bottleneck in generating a SI response is the detection of MT in an open-domain conversation. Since there are no predefined topics at hand, we see that one manner of MT detection could be using word importance scores which are scored using a bidirectional LSTM in the range of 0 to 5. (Kafle and Huenerfauth, 2018) Major Topic Detection Our assumption in this case study was that the set of words with word importance scores > 4, in the first K turns of the conversation, contain the major topic in them. We test our assumption using the human-annotated major topics of the conversation. We evaluate the extracted Bag of Words (BoW) and the annotated data using cosine similarity score. After sampling for values of K ranging from 0 to 40, we see that the major topic is detected best when K = 15. References MT  A:      B: So, do you fish? B:   and so we go there for the small lake, uh,  just outside of"
2021.sigdial-1.17,W17-5503,0,0.0590187,"Missing"
2021.sigdial-1.17,W06-1303,0,0.0886099,"Conversational systems have become a part and parcel of our everyday life and virtual assistants like Amazon’s Alexa1 , Google Home2 or Apple’s Siri 3 are soon becoming conventional household items (Terzopoulos and Satratzemi, 2020). Most of the conversational systems were built with the primary goal of accessing information, completing tasks, or executing transactions. However, recent conversational agents are transitioning towards a novel hybrid of both task-oriented and a non-task-oriented systems (Akasaki and Kaji, 2017) from the earlier models that resembled factual information systems (Leuski et al., 2006). But with this transition, they 1 https://developer.amazon.com/en-US/alexa https://assistant.google.com/ 3 https://www.apple.com/siri/ 2 In this work, we observe how a human-human open-domain conversation with an initial topic to begin with, handles topic drift and its rectification in a conversation. We work on the Switchboard dataset (Godfrey et al., 1992) and annotate 74 conversations with ‘major’, ‘minor’ and ‘offtopic’ tags (Section 4). A key result of our finding was that most of the topic shift detection models [(Takanobu et al., 2018), (Wang and Goutte, 2018), (Stewart et al., 2006)]"
2021.sigdial-1.17,W15-4640,0,0.0273969,"segment (Hearst, 1997). However, we see that they fail to have regard of sentence-level dependencies leading to fragmented segmentation (Takanobu et al., 2018). Various supervised methods approached this task as a classifiAnnotation Framework We use the human-transcribed conversations from the NXT-format Switchboard corpus (Calhoun et al., 2010) in our task. In this dataset, participants are given a topic prompt and were asked to converse with each other for around ten minutes. This dataset was chosen for annotation, amongst others, as some did not have enough turns to observe a topic shift [(Lowe et al., 2015), (Gliwa et al., 2019)] or had fixed topics of conversation [(McCowan et al., 2005), (Janin et al., 2003)] neither of which were favourable for us to model an off-topic shift detection for open-domain conversations. In Switchboard, we observe the freedom with which the participants drift from the given topic prompt, leading to different off-topic threads in the conversation and several statements by the users to steer the conversation back to the original topic. To model this property, we annotated the dataset, into three labels - major, minor and off-topic tags. Dialogues are inherently hiera"
2021.sigdial-1.17,W96-0204,0,0.534835,"in the dataset. • Off-topic (OT) - The utterances that are part of a complete digression of the topic at hand were tagged as off-topic. Each conversation could encompass multiple instances of Off Topic clusters. Figure 1: Image (left) shows the t-SNE representation of MT vs MiT vs OT classes whereas the (right) shows the t-SNE representation of MT vs rest classes. A conversational speech is not as structured as written text; it consists of overlaps of turns between the participants and interruptions. That is why each turn is divided into an utterance consisting of a single independent clause (Meteer and Iyer, 1996). This also helps us in narrowing down each utterance to have a single topic of discussion and thus a single tag to belong to. For our ease of annotation, we have considered incomplete sentence as complete sentences and annotated accordingly. We have also made a conscious decision to drop one word sentences. 4.1 Cohen’s kappa score or the inter evaluator agreement is 0.64 for our annotation, which indicates reliability. We had observed that the major issue for disagreement lie in whether to tag a conversation as minor or off-topic. In cases of confusion, annotators were advised to tag the turn"
2021.sigdial-1.17,J97-1005,0,0.0723797,"r of which were favourable for us to model an off-topic shift detection for open-domain conversations. In Switchboard, we observe the freedom with which the participants drift from the given topic prompt, leading to different off-topic threads in the conversation and several statements by the users to steer the conversation back to the original topic. To model this property, we annotated the dataset, into three labels - major, minor and off-topic tags. Dialogues are inherently hierarchical in structure, but we see that human annotators cannot definitively agree on a hierarchical segmentation (Passonneau and Litman, 1997). Thus we adopt a flat model of annotation where a strong shift from the original topic of conversation is annotated as off-topic and a subsidiary shift is labelled as minor topic. 162 • Major Topic (MT) - The utterances which belong to the topic with which the conversaTopic tag Major Topic Minor Topic off-topic tion commenced with and is largely talked about were tagged as major topic. Each conversation has a solitary Major topic. • Minor Topic (MiT) - The utterances that are part of a sub-topic, which was a natural digression from the major topic but lies in the semantic space of the major t"
2021.sigdial-1.17,N19-1170,0,0.0239848,"es are bucketed into the same. In an unbounded natural conversation, specifying the topic set in advance is not a feasible task. Our proposed topic segmentation would help us introduce a system initiative module by figuring out when to give refinement or guidance and how to best contribute in solving a user’s problem (Horvitz, 1999), by detecting the major topic of the conversation and steering the user towards it in case of a diversion. Related work A good conversation is one which focuses on a balance between staying on topic and changing it in an interactive multi-turn conversation system (See et al., 2019). Detection of what constitutes as ontopic can be viewed as segmentation of conversation into relevant and irrelevant of the conversation (Stewart et al., 2006). Earlier work in segmenting conversations into topics expected a high lexical cohesion within a topic segment (Hearst, 1997). However, we see that they fail to have regard of sentence-level dependencies leading to fragmented segmentation (Takanobu et al., 2018). Various supervised methods approached this task as a classifiAnnotation Framework We use the human-transcribed conversations from the NXT-format Switchboard corpus (Calhoun et"
2021.sigdial-1.17,W06-3402,0,0.0613217,"tion would help us introduce a system initiative module by figuring out when to give refinement or guidance and how to best contribute in solving a user’s problem (Horvitz, 1999), by detecting the major topic of the conversation and steering the user towards it in case of a diversion. Related work A good conversation is one which focuses on a balance between staying on topic and changing it in an interactive multi-turn conversation system (See et al., 2019). Detection of what constitutes as ontopic can be viewed as segmentation of conversation into relevant and irrelevant of the conversation (Stewart et al., 2006). Earlier work in segmenting conversations into topics expected a high lexical cohesion within a topic segment (Hearst, 1997). However, we see that they fail to have regard of sentence-level dependencies leading to fragmented segmentation (Takanobu et al., 2018). Various supervised methods approached this task as a classifiAnnotation Framework We use the human-transcribed conversations from the NXT-format Switchboard corpus (Calhoun et al., 2010) in our task. In this dataset, participants are given a topic prompt and were asked to converse with each other for around ten minutes. This dataset w"
2021.sigdial-1.17,C18-1212,0,0.0424652,"Missing"
C16-1135,D14-1002,0,0.0347768,"related to question and answer. We calculate the similarity between two GCD concepts using WikiMiner. The similarity between question and answer represented by GCD concepts is calculated as in Equation 3 where we use GCD concepts instead of TagMe concepts. 3.2.3 Clickthrough Features Sent2Vec Similarity: Sent2Vec maps a pair of short texts to a pair of feature vectors in a continuous, low-dimensional space. Sent2Vec performs the mapping using the Deep Structured Semantic Model (DSSM) built using Clickthrough data (Huang et al., 2013), or the DSSM with convolutional-pooling structure (CDSSM) (Gao et al., 2014; Shen et al., 2014). We map the question and answer to vectors using both DSSM and CDSSM. We compute the Sent2Vec DSSM similarity between the question and answer as the cosine similarity between the vectors of question and answer obtained by using Sent2Vec performing the mapping of vectors using DSSM. Similarly by using CDSSM instead of DSSM we also compute the Sent2Vec CDSSM similarity between the question and answer. Paragraph2vec Similarity: Paragraph2Vec(Le and Mikolov, 2014) allows to model vectors for text of any arbitrary length. It learns continuous distributed vector representations"
C16-1135,S15-2035,0,0.352111,"e answers for given a question. However, popularity based signals (votes, ratings) are often prone to spam This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1429 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1429–1440, Osaka, Japan, December 11-17 2016. due to users who may artificially inflate their ratings, votes with the help of other users whom they know. To overcome the above problems, recent approaches (Tran et al., 2015; Hou et al., 2015; Nicosia et al., 2015; Yi et al., 2015; Wang et al., 2009; Zhou et al., 2015; Severyn and Moschitti, 2015; Yu et al., 2014; Filice et al., 2016; Barr´on-Cede˜no et al., 2016) have focused on automatically ranking answers for a given question based on their quality. The problem of answer quality prediction is defined as follows: Given a question Q and its set of community answers C = {A1 , A2 , . . . , An }, rate the answers corresponding to their quality. The cQA tasks of SemEval-2015 (Task A) (Nakov et al., 2015) and SemEval-2016 (Task A) (Nakov et al., 2016) provide a universal benchmark da"
C16-1135,D14-1162,0,0.0820342,"t features independent of the position in the sentence to create (sub-)sentence representations. CNN consists of sentence matrix and multiple convolutional, pooling and non-linearity layers as in Figure 1 . Sentence Matrix: The input to the sentence matrix is a vector of words from the sentence (question/answer) s = [w1 , w2 , ....w|s |]. We build the sentence matrix by mapping each word wi in the sentence to its corresponding word embedding in d dimensions. Word embeddings represent similar words by similar vectors and, thus, identify synonyms and other important context words. We use GLoVE (Pennington et al., 2014) based embeddings of 300 dimensions to map the words in the question and answer. We limit the size of the sentence upto certain threshold. We ignore the words in the sentence after a certain threshold if the length of the sentence is greater than the threshold and pad zeros upto the threshold if the length of the sentence is less than the threshold. The sentence matrix is given as input to the convolutional layer. Convolution: Convolution is an operation where the feature map (input sentence matrix) and the convolution filter mix together to form a transformed feature map. The convolutional la"
C16-1135,spitkovsky-chang-2012-cross,0,0.0192142,"Cross-Lingual Dictionary(GCD) based similarity feature. The GCD based similarity between two Named Entities is computed as the ratio of number of wikipedia documents in which these two named entities co-occur in the top k retrieved documents when queried on GCD. Similarity between question and answer represented by Named Entities is calculated as in Equation 3 where we use Named Entities instead of TagMe concepts and GCD based similarity feature instead of WikiMiner to calculate the similarity between two Named Entities. 3.2.2 AnchorText based Features: Google Cross-Lingual Dictionary (GCD) (Spitkovsky and Chang, 2012) is a string to concept mapping on the vast link structure of the web, created using anchor text from various pages across the web. A concept is an individual Wikipedia document. The text strings are the anchor texts that refer to these concepts. Thus, each anchor text string represents a concept. 1434 We extract common and proper nouns from the question and answer using Stanford CoreNLP POS Tagger (Toutanova et al., 2003) and query them individually on GCD anchor texts to get top ten unique concepts related to question and answer. We calculate the similarity between two GCD concepts using Wik"
C16-1135,N03-1033,0,0.0639838,"e of the similarity between pairs of TagMe concepts (one each from the question and the answer) as in Equation 3 Pn Pm i=1 j=1 sim(ci , cj ) qasim = (3) nm where qasim is the similarity between question and answer based on TagMe Similarity n, m are the number of TagMe concepts in the question and answer respectively, ci , cj are the ith and j th TagMe concepts in the question and answer respectively, sim(ci , cj ) is the similarity between ci and cj calculated using WikiMiner. Named Entities Similarity: We extract Named Entities from the question and answer, using Stanford CoreNLP NER Tagger (Toutanova et al., 2003) and compute the similarity between two Named Entities using a Google Cross-Lingual Dictionary(GCD) based similarity feature. The GCD based similarity between two Named Entities is computed as the ratio of number of wikipedia documents in which these two named entities co-occur in the top k retrieved documents when queried on GCD. Similarity between question and answer represented by Named Entities is calculated as in Equation 3 where we use Named Entities instead of TagMe concepts and GCD based similarity feature instead of WikiMiner to calculate the similarity between two Named Entities. 3.2"
C16-1135,S15-2038,0,0.276869,"for ranking multiple answers for given a question. However, popularity based signals (votes, ratings) are often prone to spam This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1429 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1429–1440, Osaka, Japan, December 11-17 2016. due to users who may artificially inflate their ratings, votes with the help of other users whom they know. To overcome the above problems, recent approaches (Tran et al., 2015; Hou et al., 2015; Nicosia et al., 2015; Yi et al., 2015; Wang et al., 2009; Zhou et al., 2015; Severyn and Moschitti, 2015; Yu et al., 2014; Filice et al., 2016; Barr´on-Cede˜no et al., 2016) have focused on automatically ranking answers for a given question based on their quality. The problem of answer quality prediction is defined as follows: Given a question Q and its set of community answers C = {A1 , A2 , . . . , An }, rate the answers corresponding to their quality. The cQA tasks of SemEval-2015 (Task A) (Nakov et al., 2015) and SemEval-2016 (Task A) (Nakov et al., 2016) provide a univ"
C16-1135,N16-1152,0,0.0467742,"Missing"
C16-1135,C10-1131,0,0.0188736,"en researched a lot in the IR community. (Jeon et al., 2006) employ nontextual features such as clicks, print counts, copy counts etc. to predict the quality of an answer in a cQA forum. (Liu et al., 2008) investigate a slightly related problem i.e. predicting whether an asker would be satisfied with the answers provided so far to the given question. (Burel et al., 2012) have used a combination of content, user and thread related features for predicting answer quality. (Dalip et al., 2013) propose a learning to rank approach for AQP using eight different groups of features. (Yao et al., 2013; Wang and Manning, 2010) used CRF models with extracted features for AQP. (Li et al., 2015) studied the various factors such as shorter length, authors reputation which lead to a high answer quality rating as rated by peers. More recently, (Tran et al., 2015) made use of topic models, word vectors and other hand crafted rules to train a SVM classifier for AQP. (Hou et al., 2015) made use of statistics like avg. word length of a sentence (question or answer), sentence length with other hand-crafted features to train an ensemble of classifiers for AQP. (Wang et al., 2009) use Bayesian logistic regression and link predi"
C16-1135,P15-2116,0,0.158577,"Missing"
C16-1135,N13-1106,0,0.0552276,"Missing"
C16-1135,S15-2042,0,0.0243171,"popularity based signals (votes, ratings) are often prone to spam This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1429 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1429–1440, Osaka, Japan, December 11-17 2016. due to users who may artificially inflate their ratings, votes with the help of other users whom they know. To overcome the above problems, recent approaches (Tran et al., 2015; Hou et al., 2015; Nicosia et al., 2015; Yi et al., 2015; Wang et al., 2009; Zhou et al., 2015; Severyn and Moschitti, 2015; Yu et al., 2014; Filice et al., 2016; Barr´on-Cede˜no et al., 2016) have focused on automatically ranking answers for a given question based on their quality. The problem of answer quality prediction is defined as follows: Given a question Q and its set of community answers C = {A1 , A2 , . . . , An }, rate the answers corresponding to their quality. The cQA tasks of SemEval-2015 (Task A) (Nakov et al., 2015) and SemEval-2016 (Task A) (Nakov et al., 2016) provide a universal benchmark dataset for evaluating research on this p"
C16-1135,S15-2037,0,0.359963,"tings) are often prone to spam This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 1429 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1429–1440, Osaka, Japan, December 11-17 2016. due to users who may artificially inflate their ratings, votes with the help of other users whom they know. To overcome the above problems, recent approaches (Tran et al., 2015; Hou et al., 2015; Nicosia et al., 2015; Yi et al., 2015; Wang et al., 2009; Zhou et al., 2015; Severyn and Moschitti, 2015; Yu et al., 2014; Filice et al., 2016; Barr´on-Cede˜no et al., 2016) have focused on automatically ranking answers for a given question based on their quality. The problem of answer quality prediction is defined as follows: Given a question Q and its set of community answers C = {A1 , A2 , . . . , An }, rate the answers corresponding to their quality. The cQA tasks of SemEval-2015 (Task A) (Nakov et al., 2015) and SemEval-2016 (Task A) (Nakov et al., 2016) provide a universal benchmark dataset for evaluating research on this problem. In SemEval-2015, the answers a"
C16-1135,S16-1138,0,\N,Missing
C16-1234,bakliwal-etal-2012-hindi,1,0.717687,"ion (Feldman, 2013; Liu, 2012; Pang and Lee, 2008) where the aim is to identify whether a given sentence or document is (usually) positive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan,"
C16-1234,W14-3902,0,0.259828,"nglish speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Reco"
C16-1234,P07-1056,0,0.0156888,"tive, negative or neutral. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan, December 11-17 2016. Sentence variations Trailer dhannnsu hai bhai Dhannnsu trailer hai bhai Bhai trailer dhannnsu hai Bhai dhannnsu"
C16-1234,W14-3908,0,0.026059,"g interest owing to the rising amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tas"
C16-1234,W10-3208,0,0.0476344,"al. Due to availability of large-scale monolingual corpora, resources and widespread use of the language, English has attracted the most attention. Seminal work in sentiment analysis of Hindi text was done by Joshi et al. (2010) in which the authors built three step fallback model based on classification, machine translation and sentiment lexicons. They also observed that their system performed best with unigram features without stemming. Bakliwal et al. (2012) generated a sentiment lexicon for Hindi and validated the results on translated form of Amazon Product Dataset Blitzer et al. (2007). Das and Bandyopadhyay (2010) created Hindi SentiWordNet, a sentiment lexicon for Hindi. ∗ * indicates these authors contributed equally to this work. † Corresponding Author This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 2482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2482–2491, Osaka, Japan, December 11-17 2016. Sentence variations Trailer dhannnsu hai bhai Dhannnsu trailer hai bhai Bhai trailer dhannnsu hai Bhai dhannnsu trailer hai Table 1: Illustra"
C16-1234,W15-3904,0,0.0635959,"Missing"
C16-1234,esuli-sebastiani-2006-sentiwordnet,0,0.0460907,"ng data into 80-20 split to get the final training, validation and testing data. As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence. The architecture of the proposed system (Subword-LSTM) is described in Figure 2. We compare it with a character-level LSTM (Char-LSTM) following the same architecture without the convolutional and maxpooling layers. We use Adamax (Kingma and Ba, 2014) (a variant of Adam based on infinity norm) optimizer to train this setup in an end-to-end fashion using batch size of 128. We use very simplistic architectures because of the constraint on the size of"
C16-1234,N13-1090,0,0.0191484,"esome, brother. Sentiment Polarity Positive Negative Positive Table 4: Examples of Hi-En Code Mixed Comments from the dataset. Our dataset and code is freely available for download 2 to encourage further exploration in this domain. 2 https://github.com/DrImpossible/Sub-word-LSTM 2484 3 Learning Compositionality Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations. 3.1 Word-level models Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to"
C16-1234,D13-1170,0,0.0727234,"kers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar˜aes, 2015). LSTMs have been observed to outperform baselines for language modelling (Kim et al., 2015) and classification (Zhou et al., 2015). In a recent work, (Bojanowski et al., 2016) proposed a skip-gram based"
C16-1234,W14-3907,0,0.0805565,"ted final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging and Named Entity Recognition (dos Santos and Guimar˜aes, 2015). LSTMs have been observed to outperform baselines"
C16-1234,J11-2001,0,0.0210151,"t − C + B = Bat” lacks any linguistic basis. But, groups of characters may serve semantic functions. This is illustrated by U n + Holy = U nholy or Cat + s = Cats which is semantically interpretable by a human. Since sub-word level representations can generate meaningful lexical representations and individually carry semantic weight, we believe that sub-word level representations consisting composition of characters might allow generation of new lexical structures and serve as better linguistic units than characters. 3.3 Sub-word level representations Lexicon based approaches for the SA task (Taboada et al., 2011; Sharma et al., 2015) perform a dictionary look up to obtain an individual score for words in a given sentence and combine these scores to get the sentiment polarity of a sentence. We however want to use intermediate sub-word feature representations learned by the filters during convolution operation. Unlike traditional approaches that add sentiment scores of individual words, we propagate relevant information with LSTM and compute final sentiment of the sentence as illustrated in Figure 1. Hypothesis: We propose that incorporating sub-word level representations into the design of our models"
C16-1234,W13-3512,0,0.0154115,"Table 4: Examples of Hi-En Code Mixed Comments from the dataset. Our dataset and code is freely available for download 2 to encourage further exploration in this domain. 2 https://github.com/DrImpossible/Sub-word-LSTM 2484 3 Learning Compositionality Our target is to perform sentiment analysis on the above presented dataset. Most commonly used statistical approaches learn word-level feature representations. We start our exploration for suitable algorithms from models having word-based representations. 3.1 Word-level models Word2Vec(Mikolov et al., 2013) and Word-level RNNs (Word-RNNs) (thang Luong et al., 2013) have substantially contributed to development of new representations and their applications in NLP such as in Summarization (Cao et al., 2015) and Machine Translation (Cho et al., 2014). They are theoretically sound since language consists of inherently arbitrary mappings between ideas and words. Eg: The words person(English) and insaan(Hindi) do not share any priors in their construction and neither do their constructions have any relationship with the semantic concept of a person. Hence, popular approaches consider lexical units to be independent entities. However, operating on the lexical"
C16-1234,D14-1105,0,0.0461119,"ing amount of non-English speaking users. Sharma et al. (2015) segregated Hindi and English words and calculated final sentiment score by lexicon lookup in respective sentient dictionaries. Hindi-English (Hi-En) code mixing allows ease-of-communication among speakers by providing a much wider variety of phrases and expressions. A common form of code mixing is called as romanization 1 , which refers to the conversion of writing from a different writing system to the Roman script. But this freedom makes the task for developing NLP tools more difficult, highlighted by (Chittaranjan et al., 2014; Vyas et al., 2014; Barman et al., 2014). Initiatives have been taken by shared tasks (Sequiera et al., 2015; Solorio et al., 2014), however they do not cover the requirements for a sentiment analysis system. Deep learning based approaches (Zhang and LeCun, 2015; Socher et al., 2013) have been demonstrated to solve various NLP tasks. We believe these can provide solution to code-mixed and romanized text from various demographics in India, as similar trends are followed in many other Indian languages too. dos Santos and Zadrozny (2014) demonstrated applicability of character models for NLP tasks like POS tagging"
C16-1234,P12-2018,0,0.0555154,"ches: Hashtags, User Mentions, Emoticons etc. may not exist in the data. 2486 Figure 1: Illustration of the proposed methodology Figure 3: Training accuracy and loss variation. Figure 2: Schematic overview of the architecture. 4.2 Experimental Setup Our dataset is divided into 3 splits- Training, validation and testing. We first divide the data into randomized 80-20 train test split, then further randomly divide the training data into 80-20 split to get the final training, validation and testing data. As the problem is relatively new, we compare state of the art sentiment analysis techniques (Wang and Manning, 2012; Pang and Lee, 2008) which are generalizable to our dataset. We also compare the results with system proposed by Sharma et al. (2015) on our dataset. As their system is not available publicly, we implemented it using language identification and transliteration using the tools provided by Bhat et al. (2015) for Hi-En Code Mixed data. The polarity of thus obtained tokens is computed from SentiWordNet (Esuli and Sebastiani, 2006) and Hindi SentiWordNet (Das and Bandyopadhyay, 2010) to obtain the polarity of words, which are then voted to get final polarity of the sentence. The architecture of th"
C18-1133,E17-2017,0,0.0816236,"users’ tweets are completely ignored. Hence, the collected data is not a representative of the larger sample. 2. Tweets that do not originate from devices with geolocation features like smartphones are also completely excluded. Curated content or in-depth political discussions are not necessarily tweeted from a smartphone. 1 http://trec.nist.gov/data/tweets Similarly, (Scheffler, 2014) have created a German twitter corpus using Twitter APIs. (Cui et al., 2011) analyze emotion tokens, including emotion symbols (e.g. emoticons) for sentiment analysis and emotion analysis of Twitter snapshots. (Barbieri et al., 2017) established a sentiment analysis architecture for Twitter and released a data set for the same in English. They released the data set of roughly 500K tweets for English with emojis as labels. On a similar track, our work focuses on resource-poor languages. Here, the architecture addresses sentiment analysis as a supervised multi class classification problem where each English tweet is “annotated” with its emoji. Hence their data consists of tweets with exactly a single emoji which later is treated as the sentiment label of the tweet, while the tweet itself is striped of the emoji. We continue"
C18-1133,baroni-bernardini-2004-bootcat,0,0.0434207,"oblog shared task, and the Rovereto n-gram corpus (Herdağdelen, 2013). In this method, a considerably small fraction of tweets, over a time period are collected. However, the way in which Twitter makes these set of tweets is unclear, inducing a possible bias. Even after these restrictions, the corpora that exist are predominantly in English, hence extracting twitter for resource-scarce languages is necessary. In their attempt to construct language specific corpora, some approaches choose certain sites to crawl based on results from using medium frequency terms of the language as search terms (Baroni and Bernardini, 2004; Schäfer and Bildhauer, 2012). Following a similar approach, we employ the top most frequent words of the required language as keywords. (Rehbein et al., 2013) proposes an efficient approach to collect German tweets using geolocation features with language filter. However, the data encounters certain biases: 1. Only a fraction of users have GPS access while tweeting. These tweets are included whereas other kinds of users’ tweets are completely ignored. Hence, the collected data is not a representative of the larger sample. 2. Tweets that do not originate from devices with geolocation features"
C18-1133,W10-0513,0,0.0180547,"k 1 2. Distribution of the derivatives of data, e.g.; sharing n-gram counts, instead of the original tweets (Herdağdelen, 2013). However, the second method loses important information about the data such as word order and limits the analyses of experiments. For example, in (Herdağdelen, 2013), the analysis in section 4 would require more than just n-gram counts since the model deals with tweets per day of the week. Hence, primary metadata regarding the tweets is required. Twitter provides a streaming API under a public “gardenhouse” setting to build corpora. For example, the Edinburgh corpus (Petrovic et al., 2010), the Tweets2011 corpus from the TREC microblog shared task, and the Rovereto n-gram corpus (Herdağdelen, 2013). In this method, a considerably small fraction of tweets, over a time period are collected. However, the way in which Twitter makes these set of tweets is unclear, inducing a possible bias. Even after these restrictions, the corpora that exist are predominantly in English, hence extracting twitter for resource-scarce languages is necessary. In their attempt to construct language specific corpora, some approaches choose certain sites to crawl based on results from using medium frequen"
C18-1133,schafer-bildhauer-2012-building,0,0.016772,"vereto n-gram corpus (Herdağdelen, 2013). In this method, a considerably small fraction of tweets, over a time period are collected. However, the way in which Twitter makes these set of tweets is unclear, inducing a possible bias. Even after these restrictions, the corpora that exist are predominantly in English, hence extracting twitter for resource-scarce languages is necessary. In their attempt to construct language specific corpora, some approaches choose certain sites to crawl based on results from using medium frequency terms of the language as search terms (Baroni and Bernardini, 2004; Schäfer and Bildhauer, 2012). Following a similar approach, we employ the top most frequent words of the required language as keywords. (Rehbein et al., 2013) proposes an efficient approach to collect German tweets using geolocation features with language filter. However, the data encounters certain biases: 1. Only a fraction of users have GPS access while tweeting. These tweets are included whereas other kinds of users’ tweets are completely ignored. Hence, the collected data is not a representative of the larger sample. 2. Tweets that do not originate from devices with geolocation features like smartphones are also com"
C18-1133,scheffler-2014-german,0,0.0624123,"to collect German tweets using geolocation features with language filter. However, the data encounters certain biases: 1. Only a fraction of users have GPS access while tweeting. These tweets are included whereas other kinds of users’ tweets are completely ignored. Hence, the collected data is not a representative of the larger sample. 2. Tweets that do not originate from devices with geolocation features like smartphones are also completely excluded. Curated content or in-depth political discussions are not necessarily tweeted from a smartphone. 1 http://trec.nist.gov/data/tweets Similarly, (Scheffler, 2014) have created a German twitter corpus using Twitter APIs. (Cui et al., 2011) analyze emotion tokens, including emotion symbols (e.g. emoticons) for sentiment analysis and emotion analysis of Twitter snapshots. (Barbieri et al., 2017) established a sentiment analysis architecture for Twitter and released a data set for the same in English. They released the data set of roughly 500K tweets for English with emojis as labels. On a similar track, our work focuses on resource-poor languages. Here, the architecture addresses sentiment analysis as a supervised multi class classification problem where"
D17-1028,Q15-1016,0,0.0313516,", st&gt; <null , er&gt; Word4 studied goes schools reduces rates shows strictest harsher Cosine 0.89 1.0 0.88 0.91 0.86 0.83 1.0 0.91 Table 3: Example results of transformation operators for regular transformations. Word1 joined turned learn support Word2 joins turns learned supported Word3 became said build see Operator <ed , s&gt; <ed , s&gt; <null , ed&gt; <null , ed&gt; Word4 becomes says built saw Cosine 0.68 0.74 0.80 0.72 Table 4: Example results of transformation operators for irregular transformations. In table 2, GN denotes the scores of GoogleNews word embeddings on the test set. SGNSL and Glove-L (Levy et al., 2015) denote the results of Skip-gram with negative sampling and Glove word embeddings respectively, both trained on large datasets. SG denotes the scores of our word2vec trained model (on 1B tokens). “w/ M” implies that we have used matrix arithmetic (along with CosSum/CosMul as backup) for word analogy answering questions. Our model uses Figure 2: Prediction WorkFlow Word1 reach recognize Word2 reached recognizes Word3 go be Operator <null , ed&gt; <null , s&gt; Word4 went is Cosine 0.80 0.70 Table 5: Example results of transformation operators for complete change of word form. “CosSum” and “CosMul” as"
D17-1028,W14-1618,0,0.0300323,"table 1. Word1 decides reach ask Word2 decided reaches asks Word3 studies go reduce Operator <s , d&gt; <null , es&gt; <null , s&gt; Word4 studied goes reduces Cosine 0.89 1.0 0.91 Table 1: Some example results of transformation operators. While testing, we extract the lexical transition using the first two words of the analogy question. For example, for pairs like <reach, reached&gt;, < walk, walked&gt;, we are able to extract that they follow <null, ed&gt; rule. But, for <go, went&gt;, we are not able to find any transformation operator after lexical analysis, and for such cases, we fall back on CosSum/CosMul (Levy et al., 2014) approaches as our backup. Mikolov et al. showed that relations between words are reflected to a large extent in the offsets between their vector embeddings (queen - king = woman - man), and thus the vector of the hidden word b∗ will be similar to the vector b − a + a∗ , suggesting that the analogy question can be solved by optimizing: arg max (sim(b∗ , b − a + a∗ )) ∗ b ∈V (2) where V is the vocabulary and sim is a similarity measure. Specifically, they used the cosine similarity measure, defined as: cos(u, v) = u.v ||u ||. ||v|| (3) resulting in: arg max (cos(b∗ , b − a + a∗ )) ∗ b ∈V (4) Eq"
D17-1028,W13-3512,0,0.0345708,", we carefully use only those word pairs which are of high frequency (as they have better trained embeddings). We lower bound the frequency of both words of pair at thetaf req = 1000. We could have relied on an external morph analyzer such as Morfessor (Creutz and Lagus, 2007) to extract candidate rules and word pairs, but we wished to keep the approach completely unsupervised. Figure 1: Training WorkFlow 3.2 Learning Transformation Matrices Previous works that handle morphology using vector space representations involved complex neural network architectures such as recursive neural networks (Luong et al., 2013) and log-bilinear models (Botha and Blunsom, 2014). Both the referred works treat morph-analysis as a pre-processing step using Morfessor (Creutz and Lagus, 2007). In contrast, we propose a simple yet effective linear approach to learn the representations of transformations without depending on external segmentation tools. Suppose we get “N” highly frequent word pairs following the same regularity(transition rule). For our experiments, the lower bound of “N” is set at 50. Dimensions of word embedding of a word in our model is “D”. Using first word of our “N” chosen word pairs, we create a matr"
D17-1028,N13-1090,0,0.361509,"vents, .... as run is to ?”. A transformation operator aims to be a unified transition function for different forms of the same transition. Learning a representation of this operator would allow us to capture the semantic changes associated with the transition. As word embeddings for rare and out-of-vocabulary words are poorly trained or not trained at all, learning this operator will be beneficial to reducing the sparsity in corpus. The idea of projection learning has been applied to a multitude of tasks such as in the learning of cross lingual mappings for translation of English to Spanish (Mikolov et al., 2013b) and for unsupervised mapping between vector spaces (Akhtar et al., 2017a). Our approach has its basis on the same lines but with a different formulation and end goal to learn morphological rules rather than semantic associations and translational constraints. In summary, we present a new method to harness morphological regularities present in high dimensional word embeddings and learn its representation in the form of a matrix. Using this method, we present state of the art results on MSR word analogy dataset. This paper is structured as follows. We first discuss the corpus used for trainin"
D17-1028,N15-1186,0,0.0601174,"Missing"
D17-1028,W16-2520,0,0.040821,"Missing"
D17-1028,D15-1243,0,0.0612479,"Missing"
D17-1028,P10-1040,0,0.114051,"Missing"
D19-5401,P18-1138,0,0.0175381,"s such as (Weston et al., 2015) sets up a variety of tasks for inferring and answering the question. (Bordes and Weston, 2016) improves on the memory networks and handles out-of-vocabulary (OOV) words by inserting special words into the vocabulary for each knowledge base entity types. These systems are dependent on templates or special heuristics to reproduce facts. We demonstrate through our baseline model that generating template-like sentences from factual input can be achieved with limited success. Recent works on KB-based end-to-end QA systems such as (Yin et al., 2015; He et al., 2017a; Liu et al., 2018a) generate full-length answers with neural pointer networks(G¨ulc¸ehre et al., 2016; Vinyals et al., 2015; He et al., 2017b) after retrieving facts from a knowledge base (KB). Dialogue systems such as (Liu et al., 2018b; Lian et al., 2019) extract information from knowledge 3 Data Since there is no available dataset for the task, we used the standard machine comprehension datasets such as SQuAD (Rajpurkar et al., 2016) and HarvestingQA (Du and Cardie, 2018) to create auto-annotated data. This provide us with questions and factoid answers which we use as input to our system. For the ground-tru"
D19-5401,D15-1166,0,0.0393223,"Missing"
D19-5401,P18-1177,0,0.0859471,"ctual input can be achieved with limited success. Recent works on KB-based end-to-end QA systems such as (Yin et al., 2015; He et al., 2017a; Liu et al., 2018a) generate full-length answers with neural pointer networks(G¨ulc¸ehre et al., 2016; Vinyals et al., 2015; He et al., 2017b) after retrieving facts from a knowledge base (KB). Dialogue systems such as (Liu et al., 2018b; Lian et al., 2019) extract information from knowledge 3 Data Since there is no available dataset for the task, we used the standard machine comprehension datasets such as SQuAD (Rajpurkar et al., 2016) and HarvestingQA (Du and Cardie, 2018) to create auto-annotated data. This provide us with questions and factoid answers which we use as input to our system. For the ground-truth, we automatically extract full-length answers from the passages of these datasets by applying certain heuristics (explained in section 3.1). We extract ∼300,000 samples (question, factoid answer, full-length answer) from SQuAD and HarvestingQA. Additionally, we have manually annotated 15000 samples from SQuAD of which 2500 are used for development, 2500 for testing and we augment the rest 2 Question : what is the name of the term that is used in the unite"
D19-5401,D16-1264,0,0.0292177,"generating template-like sentences from factual input can be achieved with limited success. Recent works on KB-based end-to-end QA systems such as (Yin et al., 2015; He et al., 2017a; Liu et al., 2018a) generate full-length answers with neural pointer networks(G¨ulc¸ehre et al., 2016; Vinyals et al., 2015; He et al., 2017b) after retrieving facts from a knowledge base (KB). Dialogue systems such as (Liu et al., 2018b; Lian et al., 2019) extract information from knowledge 3 Data Since there is no available dataset for the task, we used the standard machine comprehension datasets such as SQuAD (Rajpurkar et al., 2016) and HarvestingQA (Du and Cardie, 2018) to create auto-annotated data. This provide us with questions and factoid answers which we use as input to our system. For the ground-truth, we automatically extract full-length answers from the passages of these datasets by applying certain heuristics (explained in section 3.1). We extract ∼300,000 samples (question, factoid answer, full-length answer) from SQuAD and HarvestingQA. Additionally, we have manually annotated 15000 samples from SQuAD of which 2500 are used for development, 2500 for testing and we augment the rest 2 Question : what is the nam"
D19-5401,N18-1017,0,0.0173657,"s, dates, etc. Modern factoid QA systems which use machinecomprehension datasets, predict the answer span from relevant documents using encoder-decoder architectures with co-attention. Conversely, knowledge-base (KB) oriented QA systems retrieve relevant facts using structured queries or neural representation of the question. Formulating the retrieved factoid answer into a full-length 1 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 1–9 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics bases to formulate a response. Systems such as (Fu and Feng, 2018) uses KB based key-value memory after extracting information from documents or external KBs. However, these systems are restricted to only information modeled by the KB or slot-value memory. Our system, is generic and can be used with any knowledge source, structured such as a knowledge base or free form such as machine-comprehension dataset. Since our system doesn’t use any additional relational information as modelled in a KB, it is invariant to the type of dataset. The pointer generator network, introduced in (See et al., 2017), is a generative summarization model that can copy out-of-vocab"
D19-5401,P17-1099,0,0.7141,"al Linguistics bases to formulate a response. Systems such as (Fu and Feng, 2018) uses KB based key-value memory after extracting information from documents or external KBs. However, these systems are restricted to only information modeled by the KB or slot-value memory. Our system, is generic and can be used with any knowledge source, structured such as a knowledge base or free form such as machine-comprehension dataset. Since our system doesn’t use any additional relational information as modelled in a KB, it is invariant to the type of dataset. The pointer generator network, introduced in (See et al., 2017), is a generative summarization model that can copy out-of-vocabulary (OOV) words from a source sequence. Our work is inspired from the ability of this network to accurately reproduce information from source. To the best of our knowledge, there is no existing QA data-set which addresses the task directly. However, Knowledge-based QA dataset such as (Yin et al., 2015) creates a knowledgebase from Chinese websites and extracts questionanswer pairs from Chinese communityQA webpage. The system built over this dataset, is able to generate natural answers to simple questions. The recently released C"
D19-5401,D16-1166,0,0.0195176,"actual factoid answer in a post-processing step. The masking in the data copes with the named entities and other OOV words in the dataset. Training Dataset Synthetic-only Augmented Acc 83.4 92.8 Table 6: Accuracy Scores(in the range of 0-100) for the various models 90 80 70 60 50 We have also performed cross-dataset evaluation on a knowledge base dataset(Freebase) and a machine comprehension dataset(NewsQA) to test the generalization capability of our system. We randomly selected 900 samples, comprising of question and object-names(factoid answers), from the test samples provided by SimpleQA(Golub and He, 2016) which were extracted from the KB dataset Freebase(Bollacker et al., 2008). We also randomly extract 500 test samples, questions and factoid answers, from the machine comprehension NewsQA(Trischler et al., 2017) dataset. The system predictions were compared with the manually annotated ground-truth full-length answers for these samples. Basline 2-Encoder Pointer Gen on auto-generated data 2-Encoder Pointer Gen on augmented data 40 0 10000 20000 30000 40000 50000 60000 70000 Figure 4: Validation Accuracy 6 Results As shown in table 4, 5, 6 and 7, augmenting the manually annotated data with the a"
D19-5401,P16-1014,0,0.0575915,"Missing"
D19-5401,P17-1019,0,0.100461,"Rule based systems such as (Weston et al., 2015) sets up a variety of tasks for inferring and answering the question. (Bordes and Weston, 2016) improves on the memory networks and handles out-of-vocabulary (OOV) words by inserting special words into the vocabulary for each knowledge base entity types. These systems are dependent on templates or special heuristics to reproduce facts. We demonstrate through our baseline model that generating template-like sentences from factual input can be achieved with limited success. Recent works on KB-based end-to-end QA systems such as (Yin et al., 2015; He et al., 2017a; Liu et al., 2018a) generate full-length answers with neural pointer networks(G¨ulc¸ehre et al., 2016; Vinyals et al., 2015; He et al., 2017b) after retrieving facts from a knowledge base (KB). Dialogue systems such as (Liu et al., 2018b; Lian et al., 2019) extract information from knowledge 3 Data Since there is no available dataset for the task, we used the standard machine comprehension datasets such as SQuAD (Rajpurkar et al., 2016) and HarvestingQA (Du and Cardie, 2018) to create auto-annotated data. This provide us with questions and factoid answers which we use as input to our system."
D19-5401,W17-2623,0,0.0267514,"curacy Scores(in the range of 0-100) for the various models 90 80 70 60 50 We have also performed cross-dataset evaluation on a knowledge base dataset(Freebase) and a machine comprehension dataset(NewsQA) to test the generalization capability of our system. We randomly selected 900 samples, comprising of question and object-names(factoid answers), from the test samples provided by SimpleQA(Golub and He, 2016) which were extracted from the KB dataset Freebase(Bollacker et al., 2008). We also randomly extract 500 test samples, questions and factoid answers, from the machine comprehension NewsQA(Trischler et al., 2017) dataset. The system predictions were compared with the manually annotated ground-truth full-length answers for these samples. Basline 2-Encoder Pointer Gen on auto-generated data 2-Encoder Pointer Gen on augmented data 40 0 10000 20000 30000 40000 50000 60000 70000 Figure 4: Validation Accuracy 6 Results As shown in table 4, 5, 6 and 7, augmenting the manually annotated data with the auto-generated data for training leads to significant improvements for the 2-encoder pointer generator network. From our best assumption, this is not only due to cleaner samples in the manually annotated data whi"
D19-5401,P17-4012,0,0.0142369,"f the two encoders as h0T = hnQ + hm A 5 For all our experiments, we used a 6GB 1060TX Nvidia GPU. We trained the system on batch size of 32, dropout rate of 0.5, RNN size of 512 and decay steps 10000. Since, our dataset is small, we shared the vocabulary between source and target. We used pre-trained GloVe embeddings (300 dimension) to initialize both the encoder and decoder words. Since our manually created samples are less, we oversampled the manually annotated data 3 times to mitigate any bias introduced by the synthetic dataset. We have built our system over the OpenNMT-pytorch code base(Klein et al., 2017). We have tested our models independently on both the manual dataset and autocreated dataset. We have used 2500 samples of the manually annotated SQuAD data set and 3284 samples of the auto-generated dataset to evaluate the models’ performance. These samples were selected randomly from the respective datasets. To evaluate the effectiveness of the manual data samples, we have compared the performance of our 2-encoder pointer-generator network trained on the auto-generated data and on the whole augmented dataset, containing both the manual and auto-generated data. For this comparison, training o"
D19-5511,P18-1031,0,0.0319774,"based model to classify the algorithm used in a programming problem using the C++ code. Our model tries to accomplish this task by using the natural language problem description. Gulwani et al. (2017) is a comprehensive treatise on program synthesis. Document classification The problem of classifying a programming word problem in natural language is similar to the task of document classification. The state-of-the-art approach currently for single label classification is to use a hierarchical attention network based model (Yang et al., 2016). This model is improved by using transfer learning (Howard and Ruder, 2018). when trained on the problem statement, whereas the other classes perform much better on the format and constraints. For each class except greedy, we see an additive trend – the accuracy is improved by combining both these features. Refer to figure 2 for more details. Multilabel partial problem results We also tabulate the classifier accuracies on the CFML20 dataset by training it only on the format and constraints, and the problem statement. Even here, we observe similar trends as the multiclass partial problem experiments. We find that classifiers are more accurate when trained only on the"
D19-5511,P16-1195,0,0.0274007,"roblems (Matsuzaki et al., 2017), (Hopkins et al., 2017). Wang et al. (2017), and Mehta et al. (2017) have built deep neural network based solvers for math word problems. Program synthesis Work related to the task of converting natural language description to code comes under the research areas of program synthesis and natural language understanding. This work is still in its nascent stage. Zhong et al. (2017) worked on generating SQL queries automatically from natural language descriptions. Lin et al. (2017) worked on automatically generating bash commands from natural language descriptions. Iyer et al. (2016) worked on summarizing source code. Sudha et al. (2017) use a CNN based model to classify the algorithm used in a programming problem using the C++ code. Our model tries to accomplish this task by using the natural language problem description. Gulwani et al. (2017) is a comprehensive treatise on program synthesis. Document classification The problem of classifying a programming word problem in natural language is similar to the task of document classification. The state-of-the-art approach currently for single label classification is to use a hierarchical attention network based model (Yang e"
D19-5511,D14-1181,0,0.0121619,"Missing"
D19-5511,N16-1063,0,0.0336676,"Missing"
D19-5511,P14-1026,0,0.0304131,".41 30.70 34.70 31.37 30.38 37.29 30.29 42.7 Table 4: Classification Accuracy for multi-label classification. TWE stands for trainable word embeddings initialized with a normal distribution. Note that all results were obtained on 10-fold cross validation. CNN Random refers to a CNN trained on a random labelling of the dataset. classification and program synthesis. Math word problem solving In the recent years, many models have been built to solve different kinds of math word problems. Some models solve only arithmetic problems (Hosseini et al., 2014), while others solve algebra word problems (Kushman et al., 2014). There are some recent solvers which solve a wide range preuniversity level math word problems (Matsuzaki et al., 2017), (Hopkins et al., 2017). Wang et al. (2017), and Mehta et al. (2017) have built deep neural network based solvers for math word problems. Program synthesis Work related to the task of converting natural language description to code comes under the research areas of program synthesis and natural language understanding. This work is still in its nascent stage. Zhong et al. (2017) worked on generating SQL queries automatically from natural language descriptions. Lin et al. (201"
D19-5511,D16-1076,0,0.041563,"Missing"
D19-5511,D17-1083,0,0.0271972,"Missing"
D19-5511,P17-1195,0,0.0134403,"ds for trainable word embeddings initialized with a normal distribution. Note that all results were obtained on 10-fold cross validation. CNN Random refers to a CNN trained on a random labelling of the dataset. classification and program synthesis. Math word problem solving In the recent years, many models have been built to solve different kinds of math word problems. Some models solve only arithmetic problems (Hosseini et al., 2014), while others solve algebra word problems (Kushman et al., 2014). There are some recent solvers which solve a wide range preuniversity level math word problems (Matsuzaki et al., 2017), (Hopkins et al., 2017). Wang et al. (2017), and Mehta et al. (2017) have built deep neural network based solvers for math word problems. Program synthesis Work related to the task of converting natural language description to code comes under the research areas of program synthesis and natural language understanding. This work is still in its nascent stage. Zhong et al. (2017) worked on generating SQL queries automatically from natural language descriptions. Lin et al. (2017) worked on automatically generating bash commands from natural language descriptions. Iyer et al. (2016) worked on sum"
D19-5511,D14-1058,0,0.0242647,"07 29.67 34.93 38.55 38.12 38.44 42.75 37.56 51.8 F1 macro 4.02 23.41 30.70 34.70 31.37 30.38 37.29 30.29 42.7 Table 4: Classification Accuracy for multi-label classification. TWE stands for trainable word embeddings initialized with a normal distribution. Note that all results were obtained on 10-fold cross validation. CNN Random refers to a CNN trained on a random labelling of the dataset. classification and program synthesis. Math word problem solving In the recent years, many models have been built to solve different kinds of math word problems. Some models solve only arithmetic problems (Hosseini et al., 2014), while others solve algebra word problems (Kushman et al., 2014). There are some recent solvers which solve a wide range preuniversity level math word problems (Matsuzaki et al., 2017), (Hopkins et al., 2017). Wang et al. (2017), and Mehta et al. (2017) have built deep neural network based solvers for math word problems. Program synthesis Work related to the task of converting natural language description to code comes under the research areas of program synthesis and natural language understanding. This work is still in its nascent stage. Zhong et al. (2017) worked on generating SQL queries"
D19-5511,I17-3017,1,0.855759,"Note that all results were obtained on 10-fold cross validation. CNN Random refers to a CNN trained on a random labelling of the dataset. classification and program synthesis. Math word problem solving In the recent years, many models have been built to solve different kinds of math word problems. Some models solve only arithmetic problems (Hosseini et al., 2014), while others solve algebra word problems (Kushman et al., 2014). There are some recent solvers which solve a wide range preuniversity level math word problems (Matsuzaki et al., 2017), (Hopkins et al., 2017). Wang et al. (2017), and Mehta et al. (2017) have built deep neural network based solvers for math word problems. Program synthesis Work related to the task of converting natural language description to code comes under the research areas of program synthesis and natural language understanding. This work is still in its nascent stage. Zhong et al. (2017) worked on generating SQL queries automatically from natural language descriptions. Lin et al. (2017) worked on automatically generating bash commands from natural language descriptions. Iyer et al. (2016) worked on summarizing source code. Sudha et al. (2017) use a CNN based model to cl"
D19-5511,D14-1162,0,0.0979144,"elines use uni87 grams and bigrams as features. We also try applying TF-IDF to these features. Multi-layer Perceptron (MLP) An MLP is a class of artificial neural network that uses backpropagation for training in a supervised setting (Rumelhart et al., 1986). MLP-based models are standard for text classification baselines (Glorot et al., 2011). Convolutional Neural Network (CNN) We also train a Convolutional Neural Network (CNN) based model, similar to the one used by Kim (2014) in their paper, to classify the problems. We use the model both with and without pre-trained GloVe word-embeddings (Pennington et al., 2014). CNN ensemble Hansen and Salamon (1990) introduce neural network ensemble learning, in which many neural networks are trained and their predictions combined. These neural network systems show greater generalization ability and predictive power. We train five CNN networks and combine their predictions using the majority voting system. 4.2 sum through a sigmoid function and consider the labels (tags) with activation greater than 0.5. 5 Experiment setup All hyperparameter tuning experiments were performed with 10-fold cross validation. For the nonneural network-based methods, we first vectorize"
D19-5511,P12-2018,0,0.054331,"or that a problem is of math type). Whereas, for solution category PWPs, a deeper understanding of the problem is required. The classes belong to problem and solution categories for CFML20 are mentioned in the supplementary material. 4 Classification Models To test the compatibility of our problem with text classification paradigm, we apply to it some standard text classification models from recent literature. 4.1 Multiclass Classification To approximate the optimal tagging function f1∗ (see section 2) we use the following models. Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM) Wang and Manning (2012) proposed several simple and effective baselines for text classification. An MNB is a naive Bayes classifier for multinomial models. An SVM is a discriminative hyperplane-based classifier (Hearst et al., 1998). These baselines use uni87 grams and bigrams as features. We also try applying TF-IDF to these features. Multi-layer Perceptron (MLP) An MLP is a class of artificial neural network that uses backpropagation for training in a supervised setting (Rumelhart et al., 1986). MLP-based models are standard for text classification baselines (Glorot et al., 2011). Convolutional Neural Network (CNN"
D19-5511,D17-1088,0,0.0496391,"Missing"
D19-5511,N16-1174,0,0.0252933,"(2016) worked on summarizing source code. Sudha et al. (2017) use a CNN based model to classify the algorithm used in a programming problem using the C++ code. Our model tries to accomplish this task by using the natural language problem description. Gulwani et al. (2017) is a comprehensive treatise on program synthesis. Document classification The problem of classifying a programming word problem in natural language is similar to the task of document classification. The state-of-the-art approach currently for single label classification is to use a hierarchical attention network based model (Yang et al., 2016). This model is improved by using transfer learning (Howard and Ruder, 2018). when trained on the problem statement, whereas the other classes perform much better on the format and constraints. For each class except greedy, we see an additive trend – the accuracy is improved by combining both these features. Refer to figure 2 for more details. Multilabel partial problem results We also tabulate the classifier accuracies on the CFML20 dataset by training it only on the format and constraints, and the problem statement. Even here, we observe similar trends as the multiclass partial problem exper"
E17-1061,P04-1015,0,0.189212,"ed conditions are listed in multi-lines with indentation. wik denotes english wiktionary. similar to that used in Song et al. (2014), listed in Table 7, to generate a candidate set of inflections. An averaged perceptron classifier (Collins, 2002) is trained for each lemma. For distinguishing between singular and plural candidate verb forms, the feature templates in Table 8 are used. ~ a) where θ~ is the model parameter vector and Φ(C, denotes a feature vector consisting of configuration and action components. Given a set of labeled training examples, the averaged perceptron with early update (Collins and Roark, 2004) is used. 4 Morphological Generation The last step is to inflate the lemmas in the sentence. There are three POS categories, including nouns, verbs and articles, for which we need to generate morphological forms. We use Wiktionary2 as a basis and write a small set of rules 2 candidates ← T OP -K(agenda) agenda ← ∅ Rules for be attr[‘partic’] == ‘pres’ → being attr[‘partic’] == ‘past’ → been attr[‘tense’] == ‘past’ sbj.attr[‘num’] == ‘sg’ → was sbj.attr[‘num’] == ‘pl’ → were other → [was,were] attr[‘tense’] == ‘pres’ sbj.attr[‘num’] == ‘sg’ → is sbj.attr[‘num’] == ‘pl’ → are other → [am,is,are]"
E17-1061,W02-1001,0,0.489305,"015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD – modifiers of index; LABEL-MOD – dependency labels of modifiers; BAG – set. eraged perceptron classifier (Collins, 2002) to predict function words, which is consistent with the joint model. Baseline We build a baseline following the pipeline in Figure 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string. 3.1 3.2 Linearization The next step is linearizing the graph, which we solve using a novel transition-based algorithm. 3.2.1 Transition-Based Tree Linearization Liu et al. (2015) introduce a transition-based model for tree"
E17-1061,E14-1028,0,0.0468592,"Missing"
E17-1061,J05-3002,0,0.0757316,"g Morph (b) Deep graph Morphological String with Syntactic tree (c) Figure 1: Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in g"
E17-1061,P08-1022,0,0.114531,"Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization. While they built a pipeline system, we show that joint models can be used to overcome limitations of the pipeline approach giving the best results. Joint models for NLP have shown effectiveness in recent years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using HPSG and CCG (Carroll and Oepen, 2005; Velldal and Oepen, 2006; Espinosa et al., 2008; White and Rajkumar, 2009; White, 2006; Carroll et al., 1999). Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based (Nivre, 2008) method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in Fig 1c, as compared with the pipelined baseline in Fig 1a. For a directly comparable baseline, we construct a pipeline system of fu"
E17-1061,W11-2832,0,0.301324,"n. Standard evaluations show that: 1. Our joint model for deep input surface realisation achieves significantly better scores over its pipeline counterpart. 2. We achieve the best results reported on the task. Our system scores 1 BLEU point better over Bohnet et al. (2011) without using any external resources. We make the source code available at https://github.com/SUTDNLP/ ZGen/releases/tag/v0.3. mas for each word being connected by semantic labels (Banarescu et al., 2013; Melˇcuk, 2015). In contrast to shallow syntactic trees, function words in surface forms are not included in deep graphs (Belz et al., 2011). Deep inputs can more commonly occur as input of NLG systems where entities and content words are available, and one has to generate a grammatical sentence using them with only provision for inflections of words and introduction of function words. Such usecases include summarization, dialog generation etc. 2 A pipeline of deep input linearization is shown in Figure 1a. Generation involves predicting the correct word order, deciding inflections and also filling in function words at the appropriate positions. The worst-case complexity is n! for permuting n words, 2n for function word prediction"
E17-1061,N09-1037,0,0.0611765,"Missing"
E17-1061,C10-1012,0,0.12653,"ext theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic"
E17-1061,P98-1116,0,0.36896,"ve Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic linearization: linearizing the unordered syntactic tree; 3. morphological generation: generating the"
E17-1061,W11-2835,0,0.525424,"iting student at Singapore University of Technology and Design. 643 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 643–654, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics joint system with regard to the use of information. Standard evaluations show that: 1. Our joint model for deep input surface realisation achieves significantly better scores over its pipeline counterpart. 2. We achieve the best results reported on the task. Our system scores 1 BLEU point better over Bohnet et al. (2011) without using any external resources. We make the source code available at https://github.com/SUTDNLP/ ZGen/releases/tag/v0.3. mas for each word being connected by semantic labels (Banarescu et al., 2013; Melˇcuk, 2015). In contrast to shallow syntactic trees, function words in surface forms are not included in deep graphs (Belz et al., 2011). Deep inputs can more commonly occur as input of NLG systems where entities and content words are available, and one has to generate a grammatical sentence using them with only provision for inflections of words and introduction of function words. Such u"
E17-1061,P14-1038,0,0.0184628,"ld of n; Functions: WORD – word at index; POS – part-of-speech at index. A1 INF C-A1 to have VC increase A1 price Figure 3: Equivalent shallow graph for Figure 2. Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark, 2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD – modifiers of index; LABEL-MOD – dependency labels of modifiers; BAG – set. eraged perceptron classifier (Collins, 2002) to predict function words, which is consistent with the joint model. Baseline We build a baseline following the pipeline in Figure 1a. Three stages a"
E17-1061,I05-1015,0,0.0399126,"s for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization. While they built a pipeline system, we show that joint models can be used to overcome limitations of the pipeline approach giving the best results. Joint models for NLP have shown effectiveness in recent years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using HPSG and CCG (Carroll and Oepen, 2005; Velldal and Oepen, 2006; Espinosa et al., 2008; White and Rajkumar, 2009; White, 2006; Carroll et al., 1999). Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based (Nivre, 2008) method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in Fig 1c, as compared with the pipelined baseline in Fig 1a. For a directly comparabl"
E17-1061,D15-1043,1,0.826413,"ed. Related Work Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 1988). Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison. Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the ta"
E17-1061,P07-1002,0,0.0325333,"reported so far. 1 Shallow graph String Morph (a) Sem graph Synt tree String Morph (b) Deep graph Morphological String with Syntactic tree (c) Figure 1: Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words"
E17-1061,N15-1012,1,0.88711,"M decoder is defined. Related Work Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 1988). Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison. Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-"
E17-1061,D14-1082,0,0.0455874,"aseline following the pipeline in Figure 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string. 3.1 3.2 Linearization The next step is linearizing the graph, which we solve using a novel transition-based algorithm. 3.2.1 Transition-Based Tree Linearization Liu et al. (2015) introduce a transition-based model for tree linearization. The approach extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where state consists of stack to hold partially built outputs and a queue to hold input sequence of words. In case of linearization, the input is a set of words. Liu et al. therefore use a set to hold the input instead of a queue. State is represented by a tuple (σ, ρ, A), where σ is stack to store partial derivations, ρ is set of input words and A is the set of dependency relations that have been built. There are three transition actions: • S HIFT-Word-POS – shifts Word from ρ, assigns POS to it and pushes it to top of stack as S0 ; • L EFTA RC-LABEL – constructs dependency LABEL arc S1 ←−−"
E17-1061,meyers-etal-2004-annotating,0,0.0318694,"A RC, I NSERT and I DLE actions to handle function words. If node i has a child node in C, which is not shifted, we predict S PLITA RC and I NSERT. If i is sibling to j, we predict I NSERT. If both the stack and buffer are empty, we predict I DLE. Pseudocode for G ETP OSSIBLE ACTIONS for the joint method is shown in Algorithm 5. 5 Experiments 5.1 Dataset We work on the deep dataset from the Surface Realisation Shared Task (Belz et al., 2011)4 . Sentences are represented as sets of unordered nodes with labeled semantic edges between them. Semantic representation is obtained by merging Nombank (Meyers et al., 2004), Propbank (Palmer et al., 2005) and syntactic dependencies. Edge labeling follows PropBank annotation scheme such as {A0, A1, ... An}. The nodes are annotated with lemma and where appropriate number, tense and participle features. Function words including 3 For example in Figure 2, price is the subject of be and if be is in present tense and price is in plural form, the inflections {am, is, was, were} are impossible and are is the correct inflection for be. We therefore generate transition action as S HIFT-are. 4 649 http://www.nltg.brighton.ac.uk/research/sr-task/ Input (unordered lemma-form"
E17-1061,E09-1097,0,0.203043,"paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by their lemmas. The function word to and comma are absent in graph. independent steps 1. syntactic generation: generating an unordered and lemma-formed syntactic tree from a semantic graph, introducing function words; 2. syntactic linearization: lin"
E17-1061,C04-1010,0,0.0524741,"l. Baseline We build a baseline following the pipeline in Figure 1a. Three stages are involved: 1. prediction of function words, inserting the predicted function words in the deep graph, resulting in a shallow graph; 2. linearizing the shallow graph; 3. generating the inflection for each lemma in the string. 3.1 3.2 Linearization The next step is linearizing the graph, which we solve using a novel transition-based algorithm. 3.2.1 Transition-Based Tree Linearization Liu et al. (2015) introduce a transition-based model for tree linearization. The approach extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where state consists of stack to hold partially built outputs and a queue to hold input sequence of words. In case of linearization, the input is a set of words. Liu et al. therefore use a set to hold the input instead of a queue. State is represented by a tuple (σ, ρ, A), where σ is stack to store partial derivations, ρ is set of input words and A is the set of dependency relations that have been built. There are three transition actions: • S HIFT-Word-POS – shifts Word from ρ, assigns POS to it and pushes it to top of stack as S0 ; • L EFTA RC-LABEL – constructs de"
E17-1061,J08-4003,0,0.161272,"years. Though having to tackle increased search space, they overcome issues with error propagation in pipelined models. Joint models have been explored for grammar-based approaches to surface realisation using HPSG and CCG (Carroll and Oepen, 2005; Velldal and Oepen, 2006; Espinosa et al., 2008; White and Rajkumar, 2009; White, 2006; Carroll et al., 1999). Pipelined systems suffer from the problem of error propagation. In addition, because the steps are independent of each other, information available in a later stage is not made use of in the earlier stages. We introduce a transition-based (Nivre, 2008) method for joint deep input surface realisation integrating linearization, function word prediction and morphological generation. The model is shown in Fig 1c, as compared with the pipelined baseline in Fig 1a. For a directly comparable baseline, we construct a pipeline system of function words prediction, linearization and morphological generation similar to the pipeline of Bohnet et al. (2011), but with the following difference. Our baseline pipeline system makes function word prediction for a deep input graph, whereas Bohnet et al. (2011) have a preprocessing step to construct a syntactic"
E17-1061,D09-1043,0,0.114694,"Missing"
E17-1061,J05-1004,0,0.104494,"to handle function words. If node i has a child node in C, which is not shifted, we predict S PLITA RC and I NSERT. If i is sibling to j, we predict I NSERT. If both the stack and buffer are empty, we predict I DLE. Pseudocode for G ETP OSSIBLE ACTIONS for the joint method is shown in Algorithm 5. 5 Experiments 5.1 Dataset We work on the deep dataset from the Surface Realisation Shared Task (Belz et al., 2011)4 . Sentences are represented as sets of unordered nodes with labeled semantic edges between them. Semantic representation is obtained by merging Nombank (Meyers et al., 2004), Propbank (Palmer et al., 2005) and syntactic dependencies. Edge labeling follows PropBank annotation scheme such as {A0, A1, ... An}. The nodes are annotated with lemma and where appropriate number, tense and participle features. Function words including 3 For example in Figure 2, price is the subject of be and if be is in present tense and price is in plural form, the inflections {am, is, was, were} are impossible and are is the correct inflection for be. We therefore generate transition action as S HIFT-are. 4 649 http://www.nltg.brighton.ac.uk/research/sr-task/ Input (unordered lemma-formed graph): Sem ID PID Lemma SROO"
E17-1061,P02-1040,0,0.0990305,"old tree i.e mismatch between edges of gold tree and input graph. After excluding these instances, we have 34.3k training instances. We also exclude 800 training instances where the function words to and that have more than one child, and around 100 training instances where function words’ parent and child nodes are not connected by an arc in the deep graph. The above cases are deemed annotation mistakes. We thus train on a final subset of 33.4k training instances. The development set comprises 1034 instances and the test set comprises 2398 instances. Evaluation is done using the BLEU metric (Papineni et al., 2002). if i ∈ S IBLING(j) then P ROCESS S IBLING (i, j) 15 num=pl num=sg num=sg partic=past Lexeme are meanwhile . starts housing september thought Table 10: Deep type training instance from Surface Realisation Shared Task 2011. Sem – semantic label, ID – unique ID of node within graph, PID – the ID of the parent, Attr – Attributes such as partic (participle), tense or number, Lexeme – lexeme which is resolved using wiktionary and rules in Table 7. if {j → i} ∈ C∧ A.L EFT C HILD(j) is N IL then T ← T ∪ (R IGHTA RC) if i ∈ D ESCENDANT(j) then P ROCESS D ESCENDANT (i, j) 11 Attr tense=pres 6 that com"
E17-1061,D10-1082,1,0.808929,"P VC Features for predicting function words including to infinitive, that complementizer WORD(n); POS(n); WORD(c) , . Table 1: Feature templates for the prediction of function words- to infinitive and that complementizer. Indices on the surface string: n – word index; c – child of n; Functions: WORD – word at index; POS – part-of-speech at index. A1 INF C-A1 to have VC increase A1 price Figure 3: Equivalent shallow graph for Figure 2. Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark, 2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD –"
E17-1061,N16-1058,1,0.72801,"ted work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 1988). Belz et al. (2011) organized a shared task on both shallow and deep linearization according to meaning-text theory, which provides a standard benchmark for system comparison. Song et al. (2014) achieved the best results for the task of shallow-syntactic linearization. Using SVM models with rich features, Bohnet et al. (2011) achieved state-of-art results on the task of deep realization. Wh"
E17-1061,J11-1005,1,0.826628,"IGHTA RC or L EFTA RC action; 2. i is descendant of j. In this case the parents of i (such that they are descendants of j) and siblings of i through such parents are shifted. 3. i is sibling of j. In this case, parents of i and their descendants are shifted such that A remains consistent. Because the input is a graph, more than one of the above configuration can occur simultaneously. More detailed discussion related to G ET P OSSIBLE ACTIONS is given in Appendix A. 3.2.5 Search and Learning We follow Puduppully et al. (2016) and Liu et al. (2015), applying the learning and search framework of Zhang and Clark (2011). Pseudocode is shown in Algorithm 4. It performs beam search holding k best states in an agenda at each incremental step. At the start of decoding, agenda holds the initial state. At a step, for each state in the 1 Here Lcls represents set of arc labels of child nodes (of word to shift L) shifted on the stack, Lclns represents set of arc labels of child nodes not shifted on the stack, Lcps the POS set of shifted child nodes, Lcpns the POS set of unshifted child nodes, Lsls the set of arc labels of shifted siblings, Lslns the set of arc labels of unshifted siblings, Lsps the POS set of shifted"
E17-1061,J15-3005,1,0.889421,"ep input representation, following the pipeline of Figure 1b (with input as deep graph instead of semantic graph). They construct a syntactic tree from deep input graph followed by function word prediction, linearization and morphological generation. A rich set of features are used at each stage of the pipeline and for each adjacent pair of stages, an SVM decoder is defined. Related Work Related work can be broadly summarized into three areas: abstract word ordering, applications of meaning-text theory and joint modelling of NLP tasks. In abstract word ordering (Wan et al., 2009; Zhang, 2013; Zhang and Clark, 2015), De Gispert et al. (2014) compose phrases over individual words and permute the phrases to achieve linearization. Schmaltz et al. (2016) show that strong surface-level language models are more effective than models trained with syntactic information for the task of linearization. Transitionbased techniques have also been explored (Liu et al., 2015; Liu and Zhang, 2015; Puduppully et al., 2016). To our knowledge, we are the first to use transition-based techniques for deep input linearization. There has been work done in the area of sentence linearization using meaning-text theory (Melˇcuk, 19"
E17-1061,D15-1211,1,0.853166,", . Table 1: Feature templates for the prediction of function words- to infinitive and that complementizer. Indices on the surface string: n – word index; c – child of n; Functions: WORD – word at index; POS – part-of-speech at index. A1 INF C-A1 to have VC increase A1 price Figure 3: Equivalent shallow graph for Figure 2. Features for predicting count of comma WORD(n); POS(n) BAG(WORD-MOD(n)) BAG(LABEL-MOD(n)) Joint models have been proposed for word segmentation and POS-tagging (Zhang and Clark, 2010), POS-tagging and syntactic chunking (Sutton et al., 2007), segmentation and normalization (Qian et al., 2015), syntactic linearization and morphologization (Song et al., 2014), parsing and NER (Finkel and Manning, 2009), entity and relation extraction (Li and Ji, 2014) and so on. We propose a first joint model for deep realization, integrating linearization, function word prediction and morphological generation. 3 Table 2: Feature templates for the comma prediction system. Indices on the surface string: n – word index; Functions: WORD – word at index; POS – part-of-speech at index; WORD-MOD – modifiers of index; LABEL-MOD – dependency labels of modifiers; BAG – set. eraged perceptron classifier (Coll"
E17-1061,D14-1021,1,0.855668,"graph String Morph (a) Sem graph Synt tree String Morph (b) Deep graph Morphological String with Syntactic tree (c) Figure 1: Linearization pipelines (a) NLG pipeline with deep input graph, (b) pipeline based on the meaning text theory, (c) this paper. be ADV SBJ VC AM-TMP P . think A1 meanwhile Introduction C-A1 have Natural language generation (NLG) (Reiter and Dale, 1997; White, 2004) aims to synthesize natural language text given input syntactic, semantic or logical representations. It has been shown useful in various tasks in NLP, including machine translation (Chang and Toutanova, 2007; Zhang et al., 2014), abstractive summarization (Barzilay and McKeown, 2005) and grammatical error correction (Lee and Seneff, 2006). A line of traditional methods treat the problem as a pipeline of several independent steps (Bohnet et al., 2010; Wan et al., 2009; Bangalore et al., 2000; H. Oh and I. Rudnicky, 2000; Langkilde and Knight, 1998). For example, shown in Figure 1b, a pipeline based on the meaning text theory (MTT) (Melˇcuk, 1988) splits NLG into three VC increase A1 price Figure 2: Sample deep graph for the sentence: meanwhile, prices are thought to have increased. Note that words are replaced by thei"
E17-1061,D16-1255,0,0.0281194,"Missing"
E17-1061,P13-1043,1,0.841874,". The rules in Table 7 are used to prune impossible inflections.3 Table 9 shows the transition actions to linearize the graph in Figure 2. These newly introduced transition actions result in variability in the number of transition actions. With function word prediction, the number of transition actions for a bag of n words is not necessarily 2n-1. For example, considering an I NSERT, S PLITA RC-to or S PLITA RC-that action post each S HIFT action, the maximum number of possible actions is 5n-1. This variance in the number of actions can impact the linear separability of state items. Following Zhu et al. (2013), we use I DLE actions as a form of padding method, which results in completed state items being further expanded up to 5n-1 steps. The joint model uses the same perceptron training al4.2 Obtaining Possible Transition Actions Given a Configuration Given a state s = ([σ|j i], ρ, A) and an input graph C, the possible transition actions include as a subset the transition actions in Algorithm 1 for shallow graph linearization. In addition, for each lemma being shifted, we enumerate its inflections and create S HIFT transition actions for each inflection. Further, we predict S PLITA RC, I NSERT and"
E17-2052,Q16-1031,0,0.0528961,"Missing"
E17-2052,P15-1119,0,0.0296859,"e-trained word representations while for the non-lexical features, we use randomly initialized embeddings within a range of −0.25 to +0.25.9 We use Hindi and English monolingual corpora to learn the distributed representation of the lexical units. The English monolingual data contains around 280M sentences, while the Hindi data is comparatively smaller and contains around 40M sentences. The word representations are learned using Skip-gram model with negative sampling which is implemented in word2vec toolkit (Mikolov et al., 2013). For multilingual models, we use robust projection algorithm of Guo et al. (2015) to induce bilingual representations 8 In our experiments we fixed these to be {-0.25,0.25} for Hindi and {0.25,-0.25 } for English 9 Dimensionality of input units in POS and parsing models: 80 for words, 20 for POS tags, 2 for language tags and 20 for affixes. POS Models We train POS tagging models using a similar neural network architecture as dis327 Data-set CMd CMt HINt ENGt Monolingual UAS LAS 60.77 49.24 60.05 48.52 93.29 90.60 85.12 82.86 Gold (POS + language tag) Interpolated Multilingual Multipassf UAS LAS UAS LAS UAS LAS 74.62 64.11 75.77 65.32 69.37 58.83 74.40 63.65 74.16 64.11 68."
E17-2052,C16-1234,1,0.876885,"Missing"
E17-2052,D14-1082,0,0.0106639,"OS data sets. In the Multilingual approach, we train a single model on combined data sets of the languages in the codemixed data. We concatenate an additional 1x2 vector8 in the input layer of the neural network representing the language tag of the current word. Table 2 gives the POS tagging accuracies of the two models. Count 29 1290 1460 167 376 3322 Table 1: Language Identification results on code-mixed development set and test set. 5 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Chen and Manning, 2014). The models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B"
E17-2052,W02-1001,0,0.119048,"Missing"
E17-2052,N15-3017,0,0.0320652,"able 1. Normalization and Transliteration We model the problem of both normalization and backtransliteration of (noisy) Romanized Hindi words as a single transliteration problem. Our goal is to learn a mapping for both standard and nonstandard Romanized Hindi word forms to their respective standard forms in Devanagari. For this purpose, we use the structured perceptron of Collins (Collins, 2002) which optimizes a given loss function over the entire observation sequence. For training the model, we use the transliteration pairs (87,520) from the Libindic transliteration project6 and Brahmi-Net (Kunchukuttan et al., 2015) and augmented them with noisy transliteration pairs (63,554) which are synthetically generated by dropping non-initial vowels and replacing consonants based on their phonological proximity. We use Giza++ (Och and Ney, 2003) to character align the transliteration pairs for training. At inference time, our transliteration model would predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering an overall sentential context. Contracted word forms in social media content are quite often ambiguous and can repres"
E17-2052,N16-1159,1,0.896121,"Missing"
E17-2052,D08-1102,0,0.0693574,"languages or language varieties in a single utterance1 . The phenomenon is mostly prevalent in spoken language and in informal settings on social media such as in news groups, blogs, chat forums etc. Computational modeling of code-mixed data, particularly from social media, is presumed to be more challenging than monolingual data due to various factors. The main contributing factors are non-adherence to a standard grammar, spelling variations and/or back-transliteration. It has been generally observed that traditional NLP techniques perform miserably when processing code-mixed language data (Solorio and Liu, 2008b; Vyas et al., 2014; C¸etino˘glu et al., 2016). 2 Parsing Strategies We explore three different parsing strategies to parse code-mixed data and evaluate their performance on a manually annotated evaluation set. These strategies are distinguished by the way they use pre-existing treebanks for parsing code-mixed data. 1 For brevity, we will not differentiate between intra- and inter-sentential mixing of languages and use the terms codemixing and code-switching interchangeably throughout the paper. • Monolingual: The monolingual method uses two separate models trained from the respective 324 Pro"
E17-2052,D08-1110,0,0.71943,"languages or language varieties in a single utterance1 . The phenomenon is mostly prevalent in spoken language and in informal settings on social media such as in news groups, blogs, chat forums etc. Computational modeling of code-mixed data, particularly from social media, is presumed to be more challenging than monolingual data due to various factors. The main contributing factors are non-adherence to a standard grammar, spelling variations and/or back-transliteration. It has been generally observed that traditional NLP techniques perform miserably when processing code-mixed language data (Solorio and Liu, 2008b; Vyas et al., 2014; C¸etino˘glu et al., 2016). 2 Parsing Strategies We explore three different parsing strategies to parse code-mixed data and evaluate their performance on a manually annotated evaluation set. These strategies are distinguished by the way they use pre-existing treebanks for parsing code-mixed data. 1 For brevity, we will not differentiate between intra- and inter-sentential mixing of languages and use the terms codemixing and code-switching interchangeably throughout the paper. • Monolingual: The monolingual method uses two separate models trained from the respective 324 Pro"
E17-2052,W14-3907,0,0.136667,"Missing"
E17-2052,L16-1680,0,0.0326726,"Missing"
E17-2052,D14-1105,0,0.460363,"arieties in a single utterance1 . The phenomenon is mostly prevalent in spoken language and in informal settings on social media such as in news groups, blogs, chat forums etc. Computational modeling of code-mixed data, particularly from social media, is presumed to be more challenging than monolingual data due to various factors. The main contributing factors are non-adherence to a standard grammar, spelling variations and/or back-transliteration. It has been generally observed that traditional NLP techniques perform miserably when processing code-mixed language data (Solorio and Liu, 2008b; Vyas et al., 2014; C¸etino˘glu et al., 2016). 2 Parsing Strategies We explore three different parsing strategies to parse code-mixed data and evaluate their performance on a manually annotated evaluation set. These strategies are distinguished by the way they use pre-existing treebanks for parsing code-mixed data. 1 For brevity, we will not differentiate between intra- and inter-sentential mixing of languages and use the terms codemixing and code-switching interchangeably throughout the paper. • Monolingual: The monolingual method uses two separate models trained from the respective 324 Proceedings of the 15th"
E17-2052,W03-3017,0,0.115112,"1460 167 376 3322 Table 1: Language Identification results on code-mixed development set and test set. 5 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Chen and Manning, 2014). The models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): 1) Shift, 2) Left-Arc, 3) Right-Arc, and 4) Reduce."
E17-2052,J08-4003,0,0.0469972,"2 gives the POS tagging accuracies of the two models. Count 29 1290 1460 167 376 3322 Table 1: Language Identification results on code-mixed development set and test set. 5 Experimental Setup The parsing experiments reported in this paper are conducted using a non-linear neural networkbased transition system which is similar to (Chen and Manning, 2014). The models are trained on Universal Dependency Treebanks of Hindi and English released under version 1.4 of Universal Dependencies (Nivre et al., 2016). Parsing Models Our parsing model is based on transition-based dependency parsing paradigm (Nivre, 2008). Particularly, we use an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of tr"
E17-2052,J03-1002,0,0.00619803,"nonstandard Romanized Hindi word forms to their respective standard forms in Devanagari. For this purpose, we use the structured perceptron of Collins (Collins, 2002) which optimizes a given loss function over the entire observation sequence. For training the model, we use the transliteration pairs (87,520) from the Libindic transliteration project6 and Brahmi-Net (Kunchukuttan et al., 2015) and augmented them with noisy transliteration pairs (63,554) which are synthetically generated by dropping non-initial vowels and replacing consonants based on their phonological proximity. We use Giza++ (Och and Ney, 2003) to character align the transliteration pairs for training. At inference time, our transliteration model would predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering an overall sentential context. Contracted word forms in social media content are quite often ambiguous and can represent different standard word forms such as ‘pt’ may refer to ‘put’, ‘pit’, ‘pat’, ‘pot’ and ‘pet’. To resolve this ambiguity, we extract n-best transliterations from the transliteration model using beam-search decoding. The b"
E17-2052,D16-1121,0,0.0236397,"Missing"
I17-2020,P15-2001,0,0.0417675,"Missing"
I17-2020,N15-1184,0,0.27527,"of English and test them on variety of word similarity tasks. Then we demonstrate the utility of our method by creating improved embeddings for Urdu and Telugu languages using Hindi WordNet, beating the previously established baseline for Urdu. 1 A disadvantage of learning word embeddings only from text corpus is that valuable knowledge contained in knowledge resources like WordNet (Miller, 1995) is not used. Numerous methods have been proposed to incorporate knowledge from external resources into word embeddings for their refinement (Xu et al., 2014; Bian et al., 2014; Mrksic et al., 2016). (Faruqui et al., 2015) introduced retrofitting as a light graph based technique that improves learned word embeddings. In this work we introduce a method to improve word embeddings of one language (target language) using knowledge resources from some other similar language (source language). To accomplish this, we represent both languages in the same vector space (bilingual embeddings) and obtain translations of source language’s resources. Then we use these translations to improve the embeddings of the target language by using retrofitting, leveraging the information contained in bilingual space to adjust retrofit"
I17-2020,E14-1049,0,0.031846,"simultaneously represented in the same vector space. The model is trained such that word embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. A number (Kiela et al., 2015) demonstrated the advantage of specializing word embeddings for either similarity or relatedness, which we also incorporate. Our method is also independent of the way bilingual embeddings were obtained. An added advantage of using bilingual embeddings is that they are better than monolingual counterparts due to incorporating multilingual evidence (Faruqui and Dyer, 2014; Mrkˇsi´c et al., 2017). 116 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 116–121, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 2.1 Background word vectors are close to their original vectors as well as vectors of related words. The function to be minimized to accomplish this objective is: Bilingual Embeddings Various methods have been proposed to generate bilingual embeddings. One class of methods learn mappings to transform words from one monolingual model to another, using some form of dictionary (Mikolov et al., 2013b; Fa"
I17-2020,N13-1092,0,0.110032,"Missing"
I17-2020,I05-1067,0,0.0419009,"results (compared to other sources like WordNet). To specialize embeddings for relatedness, we use University of South Florida Free Association Norms (Nelson et al., 2004) as indicated by (Kiela et al., 2015). For Hindi as source language, we use Hindi WordNet (Bhattacharyya et al., 2010). Whenever the size of resource is big enough, we first inject word embeddings with half of the dataset (random selection) followed by full length dataset to demonstrate the sequential gain in performance. Multilingual WS353 and SimLex999 datasets are by (Leviant and Reichart, 2015). We also use German RG65 (Gurevych, 2005), French RG65 (Joubarne and Inkpen, 2011) and Spanish MC30, RG65 (Hassan and Mihalcea, 2009; CamachoCollados et al., 2015). For Indian languages we use datasets provided by (Akhtar et al., 2017). 5 5.1 Table 3 shows the result of retrofitting translation invariant bilingual embeddings of four European languages for similarity using English Paraphrase Database. For each language we set η as 0.70 and f ilter as 2. The embeddings are evaluated specifically on datasets measuring similarity. All embeddings are 40 dimensional. To show that our method is effective, the embeddings are first fitted wit"
I17-2020,D14-1162,0,0.0866403,"Then we use these translations to improve the embeddings of the target language by using retrofitting, leveraging the information contained in bilingual space to adjust retrofitting process and handle noise. We also show why a dictionary based translation would be ineffective for this problem and how to handle situations where vocabulary of target embeddings is too big or too small compared to size of resource. Introduction Recently fast and scalable methods to generate dense vector space models have become very popular following the works of (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). These methods take large amounts of text corpus to generate real valued vector representation for words (word embeddings) which carry many semantic properties. Mikolov et al. (2013b) extended this model to two languages by introducing bilingual embeddings where word embeddings for two languages are simultaneously represented in the same vector space. The model is trained such that word embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. A number (Kiela et al., 2015) demonstrated the advantage of specializing word"
I17-2020,D09-1124,0,0.0341776,"elatedness, we use University of South Florida Free Association Norms (Nelson et al., 2004) as indicated by (Kiela et al., 2015). For Hindi as source language, we use Hindi WordNet (Bhattacharyya et al., 2010). Whenever the size of resource is big enough, we first inject word embeddings with half of the dataset (random selection) followed by full length dataset to demonstrate the sequential gain in performance. Multilingual WS353 and SimLex999 datasets are by (Leviant and Reichart, 2015). We also use German RG65 (Gurevych, 2005), French RG65 (Joubarne and Inkpen, 2011) and Spanish MC30, RG65 (Hassan and Mihalcea, 2009; CamachoCollados et al., 2015). For Indian languages we use datasets provided by (Akhtar et al., 2017). 5 5.1 Table 3 shows the result of retrofitting translation invariant bilingual embeddings of four European languages for similarity using English Paraphrase Database. For each language we set η as 0.70 and f ilter as 2. The embeddings are evaluated specifically on datasets measuring similarity. All embeddings are 40 dimensional. To show that our method is effective, the embeddings are first fitted with only half of the database followed by fitting with full length database. Table 3 also con"
I17-2020,P15-2118,0,0.0170588,"ods learn mappings to transform words from one monolingual model to another, using some form of dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another class of methods jointly optimize monolingual and cross-lingual objectives using word aligned parallel corpus (Klementiev et al., 2012; Zou et al., 2013) or sentence aligned parallel corpus (Chandar A P et al., 2014; Hermann and Blunsom, 2014). Also there are other methods which use monolingual data and a smaller set of sentence aligned parallel corpus (Coulmance et al., 2016) and those which use non-parallel document aligned data (Vulic and Moens, 2015). We experiment with translation invariant bilingual embeddings by (Gardner et al., 2015). We also experiment with method proposed by (Artetxe et al., 2016) where they learn a linear transform between two monolingual embeddings with monolingual invariance preserved. They use a small bilingual dictionary to accomplish this task. These methods are useful in our situation because they preserve the quality of original monolingual embeddings and do not require parallel text (beneficial in case of Indian languages). 2.2 Φ(Q) = n X &quot; 2 αi kqi − qˆi k + i=1 X # βij kqi − qj k 2 (i,j)∈E The iterative u"
I17-2020,P14-1006,0,0.0201107,"inal vectors as well as vectors of related words. The function to be minimized to accomplish this objective is: Bilingual Embeddings Various methods have been proposed to generate bilingual embeddings. One class of methods learn mappings to transform words from one monolingual model to another, using some form of dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another class of methods jointly optimize monolingual and cross-lingual objectives using word aligned parallel corpus (Klementiev et al., 2012; Zou et al., 2013) or sentence aligned parallel corpus (Chandar A P et al., 2014; Hermann and Blunsom, 2014). Also there are other methods which use monolingual data and a smaller set of sentence aligned parallel corpus (Coulmance et al., 2016) and those which use non-parallel document aligned data (Vulic and Moens, 2015). We experiment with translation invariant bilingual embeddings by (Gardner et al., 2015). We also experiment with method proposed by (Artetxe et al., 2016) where they learn a linear transform between two monolingual embeddings with monolingual invariance preserved. They use a small bilingual dictionary to accomplish this task. These methods are useful in our situation because they"
I17-2020,D15-1242,0,0.107229,"ert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014). These methods take large amounts of text corpus to generate real valued vector representation for words (word embeddings) which carry many semantic properties. Mikolov et al. (2013b) extended this model to two languages by introducing bilingual embeddings where word embeddings for two languages are simultaneously represented in the same vector space. The model is trained such that word embeddings capture not only semantic information of monolingual words, but also semantic relationships across different languages. A number (Kiela et al., 2015) demonstrated the advantage of specializing word embeddings for either similarity or relatedness, which we also incorporate. Our method is also independent of the way bilingual embeddings were obtained. An added advantage of using bilingual embeddings is that they are better than monolingual counterparts due to incorporating multilingual evidence (Faruqui and Dyer, 2014; Mrkˇsi´c et al., 2017). 116 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 116–121, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 2.1 Background word vectors are"
I17-2020,D13-1141,0,0.021863,"December 1, 2017 2017 AFNLP 2 2.1 Background word vectors are close to their original vectors as well as vectors of related words. The function to be minimized to accomplish this objective is: Bilingual Embeddings Various methods have been proposed to generate bilingual embeddings. One class of methods learn mappings to transform words from one monolingual model to another, using some form of dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another class of methods jointly optimize monolingual and cross-lingual objectives using word aligned parallel corpus (Klementiev et al., 2012; Zou et al., 2013) or sentence aligned parallel corpus (Chandar A P et al., 2014; Hermann and Blunsom, 2014). Also there are other methods which use monolingual data and a smaller set of sentence aligned parallel corpus (Coulmance et al., 2016) and those which use non-parallel document aligned data (Vulic and Moens, 2015). We experiment with translation invariant bilingual embeddings by (Gardner et al., 2015). We also experiment with method proposed by (Artetxe et al., 2016) where they learn a linear transform between two monolingual embeddings with monolingual invariance preserved. They use a small bilingual d"
I17-2020,C12-1089,0,0.0394939,"i, Taiwan, November 27 – December 1, 2017 2017 AFNLP 2 2.1 Background word vectors are close to their original vectors as well as vectors of related words. The function to be minimized to accomplish this objective is: Bilingual Embeddings Various methods have been proposed to generate bilingual embeddings. One class of methods learn mappings to transform words from one monolingual model to another, using some form of dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another class of methods jointly optimize monolingual and cross-lingual objectives using word aligned parallel corpus (Klementiev et al., 2012; Zou et al., 2013) or sentence aligned parallel corpus (Chandar A P et al., 2014; Hermann and Blunsom, 2014). Also there are other methods which use monolingual data and a smaller set of sentence aligned parallel corpus (Coulmance et al., 2016) and those which use non-parallel document aligned data (Vulic and Moens, 2015). We experiment with translation invariant bilingual embeddings by (Gardner et al., 2015). We also experiment with method proposed by (Artetxe et al., 2016) where they learn a linear transform between two monolingual embeddings with monolingual invariance preserved. They use"
I17-2020,N16-1018,0,0.0289147,"Missing"
I17-3017,Q15-1042,0,0.0755737,"Missing"
I17-3017,Q15-1001,0,\N,Missing
I17-3017,D14-1058,0,\N,Missing
I17-3017,N16-1136,0,\N,Missing
I17-3017,P16-1202,0,\N,Missing
L18-1193,W14-3902,0,0.199138,"of automatic text classification in the field of computational linguistics over the past decade. Within this domain, one problem that has drawn the attention of many researchers is automatic humor detection in texts. In depth semantic understanding of the text is required to detect humor which makes the problem difficult to automate. With increase in the number of social media users, many multilingual speakers often interchange between languages while posting on social media which is called code-mixing. It introduces some challenges in the field of linguistic analysis of social media content (Barman et al., 2014), like spelling variations and non-grammatical structures in a sentence. Past researches include detecting puns in texts (Kao et al., 2016) and humor in one-lines (Mihalcea et al., 2010) in a single language, but with the tremendous amount of code-mixed data available online, there is a need to develop techniques which detects humor in code-mixed tweets. In this paper, we analyze the task of humor detection in texts and describe a freely available corpus containing English-Hindi code-mixed tweets annotated with humorous(H) or non-humorous(N) tags. We also tagged the words in the tweets with La"
L18-1193,W16-3915,0,0.219477,"Missing"
L18-1193,N12-2012,0,0.0770421,"Missing"
N16-1058,W11-2832,0,0.621016,"Missing"
N16-1058,C10-1009,0,0.0716788,"Missing"
N16-1058,C10-1012,0,0.487455,"k is the sparsity of S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search ("
N16-1058,D14-1082,0,0.0749744,"significantly better accuracies. It gives the best results for all standard benchmarks, being over thirty times faster than Zhang (2013). The new feature structures can be applied to other transition-based systems also. 2 Transition-based linearization Liu et al. (2015) uses a transition-based model for word ordering, building output sentences using a sequence of state transitions. Instead of scoring output syntax trees directly, it scores the transition action sequence for structural disambiguation. Liu et al.’s transition system extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where a state consists of a stack to hold partially built outputs. Transition-based parsers use a queue to maintain input word sequences. However, for word ordering, the input is a set without order. Accordingly, Liu et al. uses a set to maintain the input. The transition actions are: • S HIFT-Word-POS, which removes Word from the set, assigns POS to it and pushes it onto the stack as the top word S0 ; • L EFTA RC-LABEL, which removes the second top of stack S1 and builds a dependency arc LABEL S1 ←−−−−− S0 ; • R IGHTA RC-LABEL, which removes the top of stack S0 and builds a dependency arc L"
N16-1058,E14-1028,0,0.222646,"Missing"
N16-1058,P07-1041,0,0.0805036,"S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) fo"
N16-1058,P09-1091,0,0.193752,"ther than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In pra"
N16-1058,D15-1043,1,0.905874,"Missing"
N16-1058,N15-1012,1,0.745927,"translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output quality. Recently, Liu et al. (2015) proposed a transition-based model to address this issue, which uses a sequence of state transitions to build the output. The system of Liu et al. (2015) achieves significant speed improvements without sacrificing accuracies when working with unlabeled dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scor"
N16-1058,C04-1010,0,0.108144,"l. (2015), but achieves significantly better accuracies. It gives the best results for all standard benchmarks, being over thirty times faster than Zhang (2013). The new feature structures can be applied to other transition-based systems also. 2 Transition-based linearization Liu et al. (2015) uses a transition-based model for word ordering, building output sentences using a sequence of state transitions. Instead of scoring output syntax trees directly, it scores the transition action sequence for structural disambiguation. Liu et al.’s transition system extends from transition-based parsers (Nivre and Scholz, 2004; Chen and Manning, 2014), where a state consists of a stack to hold partially built outputs. Transition-based parsers use a queue to maintain input word sequences. However, for word ordering, the input is a set without order. Accordingly, Liu et al. uses a set to maintain the input. The transition actions are: • S HIFT-Word-POS, which removes Word from the set, assigns POS to it and pushes it onto the stack as the top word S0 ; • L EFTA RC-LABEL, which removes the second top of stack S1 and builds a dependency arc LABEL S1 ←−−−−− S0 ; • R IGHTA RC-LABEL, which removes the top of stack S0 and"
N16-1058,E09-1097,0,0.0746629,"-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output qualit"
N16-1058,D09-1043,0,0.0574509,"; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to the use of time-constrained best-first-search (White and Rajkumar, 2009) for decoding. In practice, the system can take seconds to order a bag of words in order to obtain reasonable output quality. Recently, Liu et al. (2015) proposed a transition-based model to address this issue, which uses a sequence of state transitions to build the output. The system of Liu et al. (2015) achieves significant speed improvements without sacrificing accuracies when working with unlabeled dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can"
N16-1058,J11-1005,1,0.936958,"ed dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scores the output transition sequence by summing the scores of each transition action. Transition actions are treated as an atomic output component in each feature instance. This works effectively for most structured prediction tasks, including parsing (Zhang and Clark, 2011a). For word ordering, however, transition actions are significantly more complex and sparse compared 488 Proceedings of NAACL-HLT 2016, pages 488–493, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics with parsing, which limits the power of the traditional feature model. We instead break down complex actions into smaller components, merging some components into configuration features which reduces sparsity in the output action and allows flexible lookahead features to be defined according to the next action to be applied. On the other hand, this change"
N16-1058,D11-1106,1,0.954058,"ed dependency trees. With labeled dependency trees as input constraints, however, the system of Liu et al. (2015) gives much lower accuracies compared with Zhang (2013). While the low accuracy can be attributed to heavy pruning, we show that it can be mitigated by modifying the feature structure of the standard transitionbased framework, which scores the output transition sequence by summing the scores of each transition action. Transition actions are treated as an atomic output component in each feature instance. This works effectively for most structured prediction tasks, including parsing (Zhang and Clark, 2011a). For word ordering, however, transition actions are significantly more complex and sparse compared 488 Proceedings of NAACL-HLT 2016, pages 488–493, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics with parsing, which limits the power of the traditional feature model. We instead break down complex actions into smaller components, merging some components into configuration features which reduces sparsity in the output action and allows flexible lookahead features to be defined according to the next action to be applied. On the other hand, this change"
N16-1058,J15-3005,1,0.868168,"hat the main cause for the performance bottleneck is the sparsity of S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-based feature structure, which reduces feature sparsity and allows lookahead features at a small cost to decoding efficiency. Our model gives the best reported accuracies on all benchmarks, yet still being over 30 times faster compared with best-first-search. 1 Introduction Word ordering is the abstract language modeling task of making a grammatical sentence by ordering a bag of words (White, 2004; Zhang and Clark, 2015; De Gispert et al., 2014; Bohnet et al., 2010; Filippova and Strube, 2007; He et al., 2009), which is practically relevant to text-to-text applications such as summarization (Wann et al., 2009) and machine translation (Blackwood et al., 2010). Zhang (2013) built a discriminative word ordering model, which takes a bag of words, together with optional POS and dependency arcs on a subset of input words, and ∗ Part of the work was done when the author was a visiting student at Singapore University of Technology and Design. One limitation of Zhang (2013) is relatively low time efficiency, due to t"
N16-1058,E12-1075,1,0.932157,"Missing"
N16-1058,D14-1021,1,0.885954,"rmation Technology, Hyderabad (IIIT Hyderabad) ‡Singapore University of Technology and Design ratish.surendran@research.iiit.ac.in yue zhang@sutd.edu.sg m.shrivastava@iiit.ac.in Abstract yields a sentence together with its dependency parse tree that conforms to input syntactic constraints. The system is flexible with respect to input constraints, performing abstract word ordering when no constraints are given, but gives increasingly confined outputs when more POS and dependency relations are specified. It has been applied to syntactic linearization (Song et al., 2014) and machine translation (Zhang et al., 2014). It has been shown that transition-based methods can be used for syntactic word ordering and tree linearization, achieving significantly faster speed compared with traditional best-first methods. State-of-the-art transitionbased models give competitive results on abstract word ordering and unlabeled tree linearization, but significantly worse results on labeled tree linearization. We demonstrate that the main cause for the performance bottleneck is the sparsity of S HIFT transition actions rather than heavy pruning. To address this issue, we propose a modification to the standard transition-b"
N16-1159,W14-3914,0,0.126279,"Missing"
N16-1159,W14-3902,0,0.286734,"alizer. The POS tagger uses the output of the normalizer to assign each word a POS tag. Finally, the Shallow Parser assigns a chunk label with boundary. The functionality and performance of each module is described in greater detail in the following subsections. 4.1 While language identification at the document level is a well-established task (McNamee, 2005), identifying language in social media posts has certain challenges associated to it. Spelling errors, phonetic typing, use of transliterated alphabets and abbreviations combined with code-mixing make this problem interesting. Similar to (Barman et al., 2014), we performed two experiments treating language identification as a three class (‘hi’, ‘en’, ‘rest’) classification problem. The feature set comprised of BNC: normalized frequency of the word in British National Corpus (BNC)3 . LEXNORM: binary feature indicating presence of the word in the lexical normalization dataset released by Han et al. (2011). HINDI DICT: binary feature indicating presence of the word in a dictionary of 30,823 transliterated Hindi words as released by Gupta (2012). NGRAM: word n-grams. AFFIXES: prefixes and suffixes of the word. Using these features and introducing a co"
N16-1159,P11-2008,0,0.0243967,"Missing"
N16-1159,gupta-etal-2012-mining,0,0.0191516,"d below. Both subnormalizers generated normalized candidates which were then ranked, as explained later in this subsection. 1. Noisy Channel Framework: A generative model was trained to produce noisy (unnormalized) tokens from a given normalized word. Using the model’s confidence score and the probability of the normalized word in the background corpus, n-best normalizations were chosen. First, we obtained character alignments between noisy Hindi words in Roman script (Hr ) to normalized Hindi wordsformat(Hw ) using GIZA++ (Och and Ney, 2003) on 30,823 Hindi word pairs of the form (Hw - Hr ) (Gupta et al., 2012). Next, a CRF classifier was trained over these alignments, enabling it to convert a character sequence from Roman to Devanagari using learnt letter transformations. Using this model, noisy Hr words were created for Hw words obtained from a dictionary of 1,17,789 Hindi words (Biemann et al., 2007). Finally, using the formula below, we computed the most probable Hw for a given Hr . Hw = argmaxHwi p(Hwi |Hr ) = argmaxHwi p(Hr |Hwi )p(Hwi ) where p(Hwi ) is the probability of word Hwi in the background corpus. 1343 Accuracy 69.27 70.44 72.61 73.18 73.55 75.07 Table 4: Feature Ablation for POS Tag"
N16-1159,P11-1038,0,0.0207668,"Missing"
N16-1159,D12-1039,0,0.0215036,"Missing"
N16-1159,R15-1033,0,0.122373,"Missing"
N16-1159,W01-0706,0,0.0975929,"ual speakers. Two other annotators reviewed and cleaned it. To measure interannotator agreement, another annotator read the guidelines and annotated 25 sentences (334 tokens) from scratch. The inter-annotator agreement calculated using Cohen’s κ (Cohen, 1960) came out to be 0.97, 0.83 and 0.89 for language identification, POS tagging and shallow parsing respectively. 4 Shallow Parsing Pipeline Shallow parsing is the task of identifying and segmenting text into syntactically correlated word groups (Abney, 1992; Harris, 1957). Shallow parsing is a viable alternative to full parsing as shown by (Li and Roth, 2001). Our shallow parsing pipeline is composed of four main modules, as shown in Figure 1. These modules, in the order of their usage, are Language Identification, Normalization, POS Tagger and Shallow Parser. Our pipeline takes a raw utterance in Roman script as input on which each module runs sequentially. Twokenizer2 (Owoputi et al., 2013) which 2 performs well on Hindi-English CSMT (Jamatia et al., 2015) was used to tokenize the utterance into words. The Language Identification module assigns each token a language label. Based on the language label assigned, the Normalizer runs the Hindi norma"
N16-1159,P12-1109,0,0.0291547,"Missing"
N16-1159,J03-1002,0,0.0453549,"i and other for English/Rest, had two subnormalizers each, as described below. Both subnormalizers generated normalized candidates which were then ranked, as explained later in this subsection. 1. Noisy Channel Framework: A generative model was trained to produce noisy (unnormalized) tokens from a given normalized word. Using the model’s confidence score and the probability of the normalized word in the background corpus, n-best normalizations were chosen. First, we obtained character alignments between noisy Hindi words in Roman script (Hr ) to normalized Hindi wordsformat(Hw ) using GIZA++ (Och and Ney, 2003) on 30,823 Hindi word pairs of the form (Hw - Hr ) (Gupta et al., 2012). Next, a CRF classifier was trained over these alignments, enabling it to convert a character sequence from Roman to Devanagari using learnt letter transformations. Using this model, noisy Hr words were created for Hw words obtained from a dictionary of 1,17,789 Hindi words (Biemann et al., 2007). Finally, using the formula below, we computed the most probable Hw for a given Hr . Hw = argmaxHwi p(Hwi |Hr ) = argmaxHwi p(Hr |Hwi )p(Hwi ) where p(Hwi ) is the probability of word Hwi in the background corpus. 1343 Accuracy 69"
N16-1159,N13-1039,0,0.0428979,"Missing"
N16-1159,petrov-etal-2012-universal,0,0.0251756,"Missing"
N16-1159,D08-1110,0,0.0483601,"Missing"
N16-1159,D14-1105,0,0.227709,"xt analysis of Hindi English CSMT. The pipeline is accessible at 1 . 1 2 Introduction Multilingual speakers tend to exhibit code-mixing and code-switching in their use of language on social media platforms. Code-Mixing is the embedding of linguistic units such as phrases, words or morphemes of one language into an utterance of another language whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systems (Gumperz., 1982). Here we use code-mixing to refer to both the scenarios. Hindi-English bilingual speakers produce huge amounts of CSMT. Vyas et al. (2014) noted that the complexity in analyzing CSMT stems from nonadherence to a formal grammar, spelling variations, lack of annotated data, inherent conversational nature of the text and of course, code-mixing. Therefore, there is a need to create datasets and Natural 1 http://bit.ly/csmt-parser-api Background Bali et al. (2014) gathered data from Facebook generated by English-Hindi bilingual users which on analysis, showed a significant amount of codemixing. Barman et al. (2014) investigated language identification at word level on Bengali-HindiEnglish CSMT. They annotated a corpus with more than"
N18-1090,E17-2052,1,0.811923,"uages making it much more diverse than any monolingual corpora (C¸etino˘glu et al., 2016). As the current computational models fail to cater to the complexities of CS data, there is often a need for dedicated techniques tailored to its specific characteristics. Given the peculiar nature of CS data, it has been widely studied in linguistics literature (Poplack, 1980; Gumperz, 1982; Myers-Scotton, 1995), and more recently, there has been a surge in studies concerning CS data in NLP as well (Solorio and Liu, 2008a,a; Vyas et al., 2014; Sharma et al., 2016; Rudra et al., 2016; Joshi et al., 2016; Bhat et al., 2017; Chandu et al., 2017; Rijhwani et al., Code-switching is a phenomenon of mixing grammatical structures of two or more languages under varied social constraints. The code-switching data differ so radically from the benchmark corpora used in NLP community that the application of standard technologies to these data degrades their performance sharply. Unlike standard corpora, these data often need to go through additional processes such as language identification, normalization and/or back-transliteration for their efficient processing. In this paper, we investigate these indispensable processes"
N18-1090,W16-5801,0,0.100771,"Missing"
N18-1090,K17-3002,0,0.0614427,"Missing"
N18-1090,D16-1250,0,0.0176928,"ta and a parsing model which leverages both data would significantly improve parsing performance. While a parsing model trained on our limited CS data might not be enough to accurately parse the individual grammatical fragments of Hindi and English, the preexisting Hindi and 992 English monolingual data contains around 280M sentences, while the Hindi data is comparatively smaller and contains around 40M sentences. The word representations are learned using Skip-gram model with negative sampling which is implemented in word2vec toolkit (Mikolov et al., 2013). We use the projection algorithm of Artetxe et al. (2016) to transform the Hindi and English monolingual embeddings into same semantic space using a bilingual lexicon (∼63,000 entries). The bilingual lexicon is extracted from ILCI and Bojar Hindi-English parallel corpora (Jha, 2010; Bojar et al., 2014). For normalization models, we use 32-dimensional character embeddings uniformly initialized within a range of [−0.1, +0.1]. model trained on augmented Hindi and English data. For tagging, we augment the input layer of the CS tagger with the MLP layer of the source tagger. For transferring parsing knowledge, hidden representations from the parser speci"
N18-1090,P11-2008,0,0.261694,"Missing"
N18-1090,N15-3017,0,0.0451594,"back-transliteration problems as a general sequence to sequence learning problem. In general, our goal is to learn a mapping for non-standard English and Romanized Hindi word forms to standard forms in their respective scripts. In case of Hindi, we address the problem of normalization and back-transliteration of Romanized Hindi words using a single model. We use the attention-based encoder-decoder model of Luong (Luong et al., 2015) with global attention for learning. For Hindi, we train the model on the transliteration pairs (87,520) from the Libindic transliteration project4 and BrahmiNet (Kunchukuttan et al., 2015) which are further augmented with noisy transliteration pairs (1,75,668) for normalization. Similarly, for normalization of noisy English words, we train the model on noisy word forms (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for 3 4 At inference time, our normalization models will predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering a"
N18-1090,D15-1166,0,0.0167549,"rate but similar character-level models for normalization-cum-transliteration of noisy Romanized Hindi words and normalization of noisy English words. We treat both normalization and back-transliteration problems as a general sequence to sequence learning problem. In general, our goal is to learn a mapping for non-standard English and Romanized Hindi word forms to standard forms in their respective scripts. In case of Hindi, we address the problem of normalization and back-transliteration of Romanized Hindi words using a single model. We use the attention-based encoder-decoder model of Luong (Luong et al., 2015) with global attention for learning. For Hindi, we train the model on the transliteration pairs (87,520) from the Libindic transliteration project4 and BrahmiNet (Kunchukuttan et al., 2015) which are further augmented with noisy transliteration pairs (1,75,668) for normalization. Similarly, for normalization of noisy English words, we train the model on noisy word forms (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for 3 4 At infere"
N18-1090,C12-1059,0,0.0188223,"set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): Shift, Left-Arc, Right-Arc, and Reduce. We use the training by exploration method of Goldberg and Nivre (2012) for decoding a tran991 Tagger network: The input layer of the tagger encodes each input word in a sentence by concatenating a pre-trained word embedding with its character embedding given by a character Bi-LSTM. In the feature layer, the concatenated word and character representations are passed through two stacked Bi-LSTMs to generate a sequence of hidden representations which encode the contextual information spread across the sentence. The first Bi-LSTM is shared with the parser network while the other is specific to the tagger. Finally, output layer uses the feed-forward neural network wi"
N18-1090,P13-2121,0,0.0301872,"m ‘pt’ can expand to different standard word forms such as ‘put’, ‘pit’, ‘pat’, ‘pot’ and ‘pet’. The choice of word selection will solely depend on the sentential context. To select contextually relevant forms, we use exact search over n-best normalizations from the respective models extracted using beam-search decoding. The best word sequence is selected using the Viterbi decoding over bn word sequences scored by a trigram language model. b is the size of beam-width and n is the sentence length. The language models are trained on the monolingual data of Hindi and English using KenLM toolkit (Heafield et al., 2013). For each word, we extract five best normalizations (b=5). Decoding the best word sequence is a nontrivial problem for CS data due to lack of normalized and back-transliterated CS data for training a language model. One obvious solution is to apply decoding on individual language fragments in a CS sentence (Dutta et al., 2015). One major probhttp://ltrc.iiit.ac.in/icon2015/ https://github.com/libindic/indic-trans 989 English Decoding Raw Tweet Top 3 Normalizations Hindi Decoding Top 2 Dictionary Equivalents Top 3 Transliterations Best Yar year yarn yard friend buddy buddy ययर cn can con cano"
N18-1090,R15-1033,0,0.178929,"Missing"
N18-1090,W03-3017,0,0.110613,"s trained by minimizing a joint negative log-likelihood loss for both tasks. Unlike Zhang and Weiss (2016), we compute the gradients of the log-loss function simultaneously for each training instance. While the parser network is updated given the parsing loss only, the tagger network is updated with respect to both tagging and parsing losses. Both tagger and parser networks comprise of an input layer, a feature layer, and an output layer as shown in Figure 4. Following Zhang and Weiss (2016), we refer to this model as stack-prop. Our parsing models are based on an arc-eager transition system (Nivre, 2003). The arc-eager system defines a set of configurations for a sentence w1 ,...,wn , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w1 ,...,wn ] and A = ∅ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions (t): Shift, Left-Arc, Right-Arc, and Reduce. We use the t"
N18-1090,P05-1013,0,0.186498,"Missing"
N18-1090,N13-1039,0,0.116777,"Missing"
N18-1090,C16-1234,1,0.879292,"Missing"
N18-1090,P17-1180,0,0.17978,"Missing"
N18-1090,Q16-1023,0,0.0925677,"Missing"
N18-1090,P17-4012,0,0.0633061,"Missing"
N18-1090,N16-1159,1,0.928777,"Missing"
N18-1090,W14-3907,0,0.26126,"Missing"
N18-1090,P16-1147,0,0.0840875,"Missing"
N18-1090,D08-1102,0,0.0634004,"Missing"
N18-1090,D08-1110,0,0.233261,"Missing"
N18-1090,D14-1105,0,0.184706,"Missing"
N18-1090,P17-1159,0,0.0329441,"… Linear Hidden … Hidden ℎ11 ℎ21 ℎ11 ℎ21 … … x2 … x1 Hidden Hidden ℎn1 ℎn1 xn Hidden Hin-Eng POS hidden layer x1 x2 … Hin-Eng Base Model POS output layer POS feature layer POS Input layer xn Figure 6: Neural Stacking-based parsing architecture for incorporating monolingual syntactic knowledge. English grammar As we discussed above, we adapted featurelevel neural stacking (Zhang and Weiss, 2016; Chen et al., 2016) for joint learning of POS tagging and parsing. Similarly, we also adapt this stacking approach for incorporating the monolingual syntactic knowledge into the base CS model. Recently, Wang et al. (2017) used neural stacking for injecting syntactic knowledge of English into a graph-based Singlish parser which lead to significant improvements in parsing performance. Unlike Wang et al. (2017), our base stacked models will allow us to transfer the POS tagging knowledge as well along the parse tree knowledge. As shown in Figure 6, we transfer both POS tagging and parsing information from the source Mixed grammar dis rat ki barish alwayz scares me . This night of rain always scares me . Figure 5: Code-switching tweet showing grammatical fragments from Hindi and English. 4.3 Linear Hidden Hidden Fe"
N18-4018,C10-1021,0,0.0671211,"s.” Introduction Micro-blogging sites like Twitter and Facebook encourage users to express their daily thoughts in real time, which often result in millions of emotional statements being posted online, everyday. Identification and analysis of emotions in social-media texts are of great significance in understanding the trends, reviews, events and human behaviour. Emotion prediction aims to identify fine-grained emotions, i.e., Happy, Anger, Fear, Sadness, Surprise, Disgust, if any present in the text. Previous research related to this task has mainly been focused only on the monolingual text (Chen et al., 2010; Alm et al., 2005) due to the availability of large-scale monolingual resources. However, usage of code mixed language in online posts is very common, especially in multilingual societies like India, for expressing one’s emotions The above examples contain both English and Hindi texts. T1 expresses fear through Hindi phrase “dar lagta hai mujhe”, happiness is expressed in T2 through a Hindi-English mixed phrase “jeetne mein successful ho hi gayi”, while in T3, surprise is expressed through English phrase “This is a big surprise”. Since very few resources are available for HindiEnglish code-mi"
N18-4018,Y10-1013,0,0.0631201,"Missing"
N18-4018,H05-1073,0,0.0971082,"cro-blogging sites like Twitter and Facebook encourage users to express their daily thoughts in real time, which often result in millions of emotional statements being posted online, everyday. Identification and analysis of emotions in social-media texts are of great significance in understanding the trends, reviews, events and human behaviour. Emotion prediction aims to identify fine-grained emotions, i.e., Happy, Anger, Fear, Sadness, Surprise, Disgust, if any present in the text. Previous research related to this task has mainly been focused only on the monolingual text (Chen et al., 2010; Alm et al., 2005) due to the availability of large-scale monolingual resources. However, usage of code mixed language in online posts is very common, especially in multilingual societies like India, for expressing one’s emotions The above examples contain both English and Hindi texts. T1 expresses fear through Hindi phrase “dar lagta hai mujhe”, happiness is expressed in T2 through a Hindi-English mixed phrase “jeetne mein successful ho hi gayi”, while in T3, surprise is expressed through English phrase “This is a big surprise”. Since very few resources are available for HindiEnglish code-mixed text, in this p"
N18-4018,C16-1234,1,0.924759,"Missing"
N18-4018,W14-3914,0,0.158259,"Missing"
N18-4018,W15-3116,0,0.0179198,"eets are the ones which comprise only of hashtags or urls. Also, tweets in which language other than Hindi or English is used were also considered as noisy and hence removed from the corpus. Furthermore, all those tweets which were written either in pure English or pure Hindi language were removed, and thus, keeping only the code-mixed tweets. In the annotation phase, we further removed all those tweets which were not expressing any emotion. 1 129 https://pypi.python.org/pypi/twitterscraper/0.2.7 “mujhe fear hai”, it is thus essential to annotate the data with four kinds of causal situations (Lee and Wang, 2015), i.e. Hindi, English, Mixed and Both. Next, we further discuss these situations in detail. Hindi means the emotion of the given post is solely expressed through Hindi text. In the example, T4 happiness is expressed through Hindi text. T4 : “Bahut badiya, ab sab okay hai surgical strike ke baad.” Translation : “Very good, now everything is okay after the surgical strike.” English means the emotion of the given post is solely expressed through English text. T5 is an example that expresses surprise through English text. Figure 1: Annotated Instance for tweet “@sachin rt sab cheezo ke bare main t"
N18-4018,W14-3902,0,0.211968,"Missing"
N18-4018,W12-2103,0,0.0504122,"tensifiers (I): Users often tend to use intensifiers for laying emphasis on sentiment and emotion. For example in the following codemixed text, “Wo kisi se baat nahi karega because he is too sad”, Translation : “He will not talk to anyone because he is too sad”. “too” is used to emphasize on the sadness of the boy. A list of English intensifiers was taken from wikipedia4 . For creating the list of Hindi intensifiers, English intensifiers were 2. Word N-Grams (W) : Bag of word features have been widely used to capture emotion in a text (Purver and Battersby, 2012) and in detecting hate speech (Warner and Hirschberg, 2012). Thus we use word n-grams, where n varies from 1 to 3 as a feature to train our classification models. 3 4 132 https://en.wikipedia.org/wiki/List of emoticons https://en.wikipedia.org/wiki/Intensifier Class Happiness Sadness Anger Weight 4 2 1 Feature Eliminated None Emoticons Char N-Grams Word N-Grams Repetitive Characters Punctuation Marks Upper Case Words Intensifiers Negation Words Lexicon Table 4: Weights assigned to classes transliterated to Hindi. Also Hindi words found in the corpus which are usually used as intensifiers were incorporated in the list. We count the number of intensifie"
N18-4018,N12-1071,0,0.0488868,"Missing"
N18-4018,C10-1136,0,0.0619829,"Missing"
N18-4018,W10-0204,0,0.118027,"Missing"
N18-4018,E12-1049,0,0.0247253,"as one of the features, where n varies from 1 to 3. 7. Intensifiers (I): Users often tend to use intensifiers for laying emphasis on sentiment and emotion. For example in the following codemixed text, “Wo kisi se baat nahi karega because he is too sad”, Translation : “He will not talk to anyone because he is too sad”. “too” is used to emphasize on the sadness of the boy. A list of English intensifiers was taken from wikipedia4 . For creating the list of Hindi intensifiers, English intensifiers were 2. Word N-Grams (W) : Bag of word features have been widely used to capture emotion in a text (Purver and Battersby, 2012) and in detecting hate speech (Warner and Hirschberg, 2012). Thus we use word n-grams, where n varies from 1 to 3 as a feature to train our classification models. 3 4 132 https://en.wikipedia.org/wiki/List of emoticons https://en.wikipedia.org/wiki/Intensifier Class Happiness Sadness Anger Weight 4 2 1 Feature Eliminated None Emoticons Char N-Grams Word N-Grams Repetitive Characters Punctuation Marks Upper Case Words Intensifiers Negation Words Lexicon Table 4: Weights assigned to classes transliterated to Hindi. Also Hindi words found in the corpus which are usually used as intensifiers were"
N18-4018,N16-1159,1,0.884569,"Missing"
N18-4018,D14-1105,0,0.0787909,"Missing"
N18-4018,C16-1153,0,0.0607669,"Missing"
N19-3017,W10-1411,0,0.0147634,"+ forms of lagnaa (to begin) (Spencer et al., 2005; Chakrabarti et al., 2008). 2.1 Hindi follows a general SOV word order (more specifically, S-IO-DO-V) (Seddah et al., 2010).1 In the default word order, Hindi is a head-final with a relatively free word order (Patil et al., 2008). However, constituents are often ""mixed up"", which may be done for focus or emphasis, but this is not always the case (Butt and King, 1996; Kidwai, 2000). We take an example of the sentence: raam ne sitaa ko kitaab dii Ram erg. Sita dat. book gave-fem. ""Ram gave the book to Sita"" to explain the possible word orders (Ambati et al., 2010). Table 1: Case/Role Marking in Hindi aforementioned restrictions presented for Sanskrit, and present two other novel methods for representing restricted word order alternation, which can be applied to other languages as well, evidenced by the examples of prepositional phrases in English. 2 Restricted Word Order Alternation Properties of Hindi Syntax • Default word order (S-IO-DO-V) used above This section details the syntactic properties of Hindi, which include postpositional case marking (karaka and sambandha markers) in noun phrases, lexicalized tense and aspect markers in verb phrases, and"
N19-3017,P93-1015,0,0.582178,"here are similar in to a few of the simple sentences of the Hindi treebank (Bhatt et al., 2009). 1. I go to school. →n mein I π The type assignment seems to imply that in the genitive case marker is the headword of the noun phrase raam kaa, which is not the case. Genitives in Hindi are gender marked, and they agree with the gender of the following noun phrase, which is preserved by the current type assignment. The type assignment is reflective of the Paninian framework of modifier-modified relationship, in which the genitive case marker reflects the modification of the following noun phrase (Bharati and Sangal, 1993). The VP consists of a verb, an optional auxiliary (denoted by α) and a tense marker (denoted by τ ), in that order of occurrence. Verb transitivity does affect the verb typing, but only in the sentence. All statement verbs are given the type s. Verbs in Hindi are unique for their tense marking system, which include a gender-marked past tense marker, a gender-neutral present tense marker, and a suffixed future tense marker. Given the semantic nature of karaka markers in Hindi, verb transitivity raises ambiguous cases. For example, the sentence Ram ne seb ko khaaya and Ram ne seb khaaya both tr"
N19-3017,W09-3036,0,0.020706,"t karaka and sambandh markers (for the genitive case) respectively. The subscript on κ denotes the case of the noun that precedes it (refer Table 2). A simple example of the genitive case interaction, for the noun phrase ""Ram’s brother"" (raam kaa bhai), is as: (n)(nr ρ)(ρr n) subject and accusative case marker on the direct object (Palmer et al., 2009). 4.2 Examples of Hindi Sentences Given the set of basic types {π, s, p, o, n, κi , ρ, α, τ }, simple sentences can be typed in Hindi as follows. The toy examples chosen here are similar in to a few of the simple sentences of the Hindi treebank (Bhatt et al., 2009). 1. I go to school. →n mein I π The type assignment seems to imply that in the genitive case marker is the headword of the noun phrase raam kaa, which is not the case. Genitives in Hindi are gender marked, and they agree with the gender of the following noun phrase, which is preserved by the current type assignment. The type assignment is reflective of the Paninian framework of modifier-modified relationship, in which the genitive case marker reflects the modification of the following noun phrase (Bharati and Sangal, 1993). The VP consists of a verb, an optional auxiliary (denoted by α) and a"
N19-3017,J07-3004,0,0.0641587,"der alternation. The replicability of these methods is explained in the representation of adverbs and prepositional phrases in English. 1 Introduction Categorial grammars are one of the frameworks for the representation of syntactic structures of languages (Oehrle et al., 2012). A foundational problem in such formalisms, including the well established lexical formalism combinatory categorial grammars (CCG), is the representation of free word order in light of syntactic or semantic constraints presented by the language. Extensive resources following the different formalisms, such as CCG Banks (Hockenmaier and Steedman, 2007), have been developed. Development in pregroup calculus, however, has been more focused on developing formal constraints in the calculus for the representation of syntactic phenomena. In that vein, this paper aims at presenting the problem 125 Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 125–135 c Minneapolis, Minnesota, June 3 - 5, 2019. 2017 Association for Computational Linguistics karaka karta vibhakti φ ne karam ko karan sampradan apadaan adhikaran se ko, ke liye se me, par Equivalent Ca"
P06-2100,A92-1018,0,0.891061,"Missing"
P06-2100,J95-4004,0,0.114244,"Missing"
P06-2100,H92-1023,0,0.811288,"Missing"
P06-2100,C94-1027,0,0.0553839,"Missing"
P06-2100,P97-1032,0,0.0153778,"foreign words, derivationally morphed words, spelling variations and other unknown words) (Manning and Schutze, 2002). For English there are many POS taggers, employing machine learning techniques In this scenario, POS tagging of highly inflectional languages presents an interesting case study. Morphologically rich languages are characterized by a large number of morphemes in a single word, where morpheme boundaries are difficult to detect because they are fused together. They are typically free-word ordered, which causes fixed-context systems to be hardly adequate for statistical approaches (Samuelsson and Voutilainen, 1997). Morphology-based POS tagging of some languages like Turkish (Oflazer and Kuruoz, 1994), Arabic (Guiassa, 2006), Czech (Hajic et al., 2001), Modern Greek (Orphanos et al., 1999) and Hungarian (Megyesi, 1999) has been tried out using a combination of hand-crafted rules and statistical learning. These systems use large amount of corpora along with morphological analysis to POS tag the texts. It may be noted that a purely rule-based or a purely stochastic approach will not be effective for such 779 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 779–786, c Sydney, July"
P06-2100,W01-0512,0,\N,Missing
P06-2100,W99-0633,0,\N,Missing
P06-2100,A94-1024,0,\N,Missing
P06-2100,P01-1035,0,\N,Missing
P16-1036,I11-1031,0,0.178506,"(Robertson et al., 1994) and Language modeling for Information Retrieval (LM IR) (Zhai and Lafferty, 2004) score the similarity based on the weights of the matching text terms between the questions. 2. Translation Models: Learning word or phrase level translation models from question-answer pairs in parallel corpora of same language (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The similarity function between questions is then defined as the probability of translating a given question into another. 3. Topic Models: Learning topic models from question-answer pairs (Ji et al., 2012; Cai et al., 2011; Zhang et al., 2014). Here, the similarity between questions, is defined in the latent topic space discovered by the topic model. 4. Deep Learning Based Approaches: Deep Learning based models like (Zhou et al., 2016),(Qiu and Huang, 2015), (Das et al., 2016) use variants of neural network architectures to model question-question pair similarity. • The knowledge and expertise of the answerers and askers usually differ in a cQA forum. The askers, who are novices or nonexperts, usually use less technical terminology whereas the answerers, who are typically experts, are more likely to use terms w"
P16-1036,P11-1066,0,0.0996972,"rain SCQA which is usually hard to obtain in large numbers. Hence, SCQA overcomes this limitation by leveraging Question-Answer pairs (Q, A) from the cQA archives. This also has additional advantages such as: models like BM 25 (Robertson et al., 1994) and Language modeling for Information Retrieval (LM IR) (Zhai and Lafferty, 2004) score the similarity based on the weights of the matching text terms between the questions. 2. Translation Models: Learning word or phrase level translation models from question-answer pairs in parallel corpora of same language (Jeon et al., 2005; Xue et al., 2008; Zhou et al., 2011). The similarity function between questions is then defined as the probability of translating a given question into another. 3. Topic Models: Learning topic models from question-answer pairs (Ji et al., 2012; Cai et al., 2011; Zhang et al., 2014). Here, the similarity between questions, is defined in the latent topic space discovered by the topic model. 4. Deep Learning Based Approaches: Deep Learning based models like (Zhou et al., 2016),(Qiu and Huang, 2015), (Das et al., 2016) use variants of neural network architectures to model question-question pair similarity. • The knowledge and expert"
P16-1036,P08-1081,0,0.0304395,"label. The gradient of the loss function with respect to the weights and biases shared by the sub-networks, is computed using back-propagation. Stochastic Gradient Descent method is used to update the parameters of the sub-networks. Siamese Neural Network Siamese Neural Networks (shown in Figure 1) were introduced by Bromley et al. (1993) to solve the problem of signature verification. Later, Chopra et al. (2005) used the architecture with discriminative loss function for face verification. Recently these networks are used extensively to enhance the quality of visual search (Liu et al., 2008; Ding et al., 2008). Let, F (X) be the family of functions with set of parameters W . F (X) is assumed to be differentiable with respect to W . Siamese network seeks a value of the parameter W such that the symmetric similarity metric is small if X1 and X2 belong to the same category, and large if they belong to different categories. The scalar energy function S(Q, A) that measures the semantic relatedness between question answer pair (Q,A) can be defined as: S(Q, A) = kF (Q) − F (A)k Architecture of SCQA 3.1 Inputs to SCQA The size of training data used is in millions, thus representing every word with one hot"
P16-1036,P15-1025,0,0.0120946,"uage model to it. Zhou et al. (2011) used phrase based translation model where they considered question answer pairs as parallel corpus. However, Zhang et al. (2014) stated that questions and answers cannot be considered parallel because they are heterogeneous in lexical level and in terms of user behaviors. To overcome these vulnerabilities topic modeling was introduced by (Ji et al., 2012; Cai et al., 2011; Zhang et al., 2014). The approach assumes that questions and answers share some common latent topics. These techniques match questions not only on a term level but also on a topic level. Zhou et al. (2015) used a fisher kernel to model the fixed size representation of the variable length questions. The model enhances the embedding of the questions with the metadata “category” involved with them. Zhang et al. (2016) learnt representations of words and question categories simultaneously and incorporated the learnt representations into traditional language models. Following the recent trends, deep learning is also employed to solve this problem. Qiu et al. (2015) introduced convolutional neural tensor network (CNTN), which combines sentence modeling and semantic matching. CNTN transforms the word"
P16-1036,C08-1063,0,0.00985565,"e measure and the label. The gradient of the loss function with respect to the weights and biases shared by the sub-networks, is computed using back-propagation. Stochastic Gradient Descent method is used to update the parameters of the sub-networks. Siamese Neural Network Siamese Neural Networks (shown in Figure 1) were introduced by Bromley et al. (1993) to solve the problem of signature verification. Later, Chopra et al. (2005) used the architecture with discriminative loss function for face verification. Recently these networks are used extensively to enhance the quality of visual search (Liu et al., 2008; Ding et al., 2008). Let, F (X) be the family of functions with set of parameters W . F (X) is assumed to be differentiable with respect to W . Siamese network seeks a value of the parameter W such that the symmetric similarity metric is small if X1 and X2 belong to the same category, and large if they belong to different categories. The scalar energy function S(Q, A) that measures the semantic relatedness between question answer pair (Q,A) can be defined as: S(Q, A) = kF (Q) − F (A)k Architecture of SCQA 3.1 Inputs to SCQA The size of training data used is in millions, thus representing ever"
P18-3017,P13-1138,0,0.0482769,"Missing"
P18-3022,H05-1103,0,0.0604791,"., 2017); (Yuan et al., 2017); (Du et al.), etc. to generate questions. In the current paper, we fetch relative pronouns and relative adverbs from complex English sentences and use dependency-based rules, grounded in linguistic theory of relative clause syntactic structure, to generate multiple relevant questions. The work follows in the tradition of question writing algorithm (Finn, 1975) and transformation Introduction Asking questions from learners is said to facilitate interest and learning (Chi, 1994), to recognize problem learning areas (Tenenberg and Murphy, 2005) to assess vocabulary (Brown et al., 2005) and reading comprehension (Mitkov, 2006); (Kunichika et al., 2004), to provide writing support (Liu et al., 2012), to support inquiry needs (Ali et al., 2010), etc. Manual generation of questions from a text for creating practice exercises, tests, quizzes, etc. has consumed labor and time of academicians and instructors since forever, and with the invent of a large body of educational material available online, there is a growing need to make this task scalable. Along with that, in the recent times, there is an increased demand to cre153 Proceedings of ACL 2018, Student Research Workshop, pag"
P18-3022,J15-1001,0,0.29367,"ovel in the Automatic Question Generation task. 1 2 Related Work Previous work on Automatic QG has focused on generating questions using question templates (Liu et al., 2012); (Mostow and Chen, 2009); (Sneiders, 2002), transformation rules based largely on case grammars (Finn, 1975), general-purpose, transformation rules (Heilman and Smith, 2009), tree manipulation rules (Heilman, 2011); (Ali et al., 2010); (Gates, 2008), discourse cues (Agarwal et al., 2011), queries (Lin, 2008), various scopes (Mannem et al., 2010), dependency parse information (Mazidi and Nielsen, 2015), topic information (Chali and Hasan, 2015), ontologies (Alsubait et al., 2015), etc. More recent approaches also apply neural methods (Subramanian et al., 2017); (Zhou et al., 2017); (Yuan et al., 2017); (Du et al.), etc. to generate questions. In the current paper, we fetch relative pronouns and relative adverbs from complex English sentences and use dependency-based rules, grounded in linguistic theory of relative clause syntactic structure, to generate multiple relevant questions. The work follows in the tradition of question writing algorithm (Finn, 1975) and transformation Introduction Asking questions from learners is said to fa"
P18-3022,D15-1162,0,0.0140299,"orever, and with the invent of a large body of educational material available online, there is a growing need to make this task scalable. Along with that, in the recent times, there is an increased demand to cre153 Proceedings of ACL 2018, Student Research Workshop, pages 153–158 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics over other QG systems. rules based approach (Heilman and Smith, 2009). However, while Finn’s work was based largely around case grammars (Fillmore, 1968), our system exploits dependency parse information using the Spacy parser (Honnibal and Johnson, 2015), which provides us with a better internal structure of complex sentences to work with. The generalpurpose transformation rules in Heilman’s system do not work well on sentences with a highly complex structure, as we show in the section on comparison and evaluation. Although no other stateof-the art system focuses specifically on QG from relative pronouns and relative adverbs, a more recent Minimal Recursion semantics-based QG system (Yao et al., 2012) has a sub part that deals with sentences with a relative clause, but less comprehensively. We differ from their system in that, for one, we do"
P18-3022,W03-0203,0,0.226861,"Missing"
P19-2025,P18-3008,0,0.0491713,"Missing"
P19-2025,P05-1045,0,0.033851,"57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 183–189 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2 Background and Related work 2,16,800 tweets using the python twitter API and after the extensive cleaning we are left with 3968 code-mixed Telugu-English Tweets. The corpus will be made available online soon. The following explains the mapping of tokens with their respective tags. There has been a significant amount of research done in Named Entity Recognition(NER) of resource rich languages Finkel et al. (2005), English Sarkar (2015), German Tjong Kim Sang and De Meulder (2003), French Azpeitia et al. (2014) and Spanish Zea et al. (2016) while the same is not true for code-mixed Indian languages. The FIRE(Forum for Information Retrieval and Extraction)2 tasks have shed light on NER in Indian languages as well as code-mixed data. The following are some works in code-mixed Indian languages. Bhargava et al. (2016) proposed an algorithm which uses a hybrid approach of a dictionary cum supervised classification approach for identifying entities in Code Mixed Text of Indian Languages such as Hindi- Englis"
P19-2025,W18-2405,1,0.885828,"Missing"
P19-2025,W03-0419,0,0.228649,"Missing"
P19-2025,W16-2705,0,0.0359341,"Missing"
P19-2025,W12-5211,0,0.379412,"m for not to be a named entity. 5.2 Results and Discussion Table 3 shows the results of the CRF model with ‘l2sgd’(Stochastic Gradient Descent with L2 regularization term) algorithm for 100 iterations. The c2 value corresponds to the ‘L2 regression’ which is used to restrict our estimation of w*. Experiments using the algorithms ‘ap’(Averaged Perceptron) and ‘pa’(Passive Aggressive) yielded almost similar F1-scores of 0.96. Table 5 shows 2. Word N-Grams: We use word N-Grams, where we used the previous and the next word as a feature vector to train our model which serve as contextual features. Jahangir et al. (2012) 186 Tag B-Loc I-Loc B-Org I-Org B-Per I-Per OTHER weighted avg Precision 0.958 0.867 0.802 0.385 0.908 0.715 0.974 0.963 Recall 0.890 0.619 0.600 0.100 0.832 0.617 0.992 0.966 Feature Char N-Grams Word NGrams Capitalization Mentions, Hashtags Numbers in String Previous Word tag Common Symbols F1-score 0.922 0.722 0.687 0.159 0.869 0.663 0.983 0.964 Table 3: CRF Model with ‘c2=0.1’ and ‘l2sgd’ algo. Tag B-Org I-Per B-Per I-Loc OTHER B-Loc I-Org weighted avg Precision 0.55 0.43 0.76 0.50 0.98 0.83 0.09 0.94 Recall 0.61 0.50 0.76 0.59 0.97 0.84 0.13 0.94 F1-score 0.58 0.47 0.76 0.54 0.97 0.84 0."
P19-2052,C16-1234,1,0.849279,"en attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentiment space (Choudhary et al., 2018). 3 3.1 Generating Subword Representations Word embeddings are now commonplace but are generally trained for one language. They are not ideal for code-mixed data given the transcription of one script into another, and spelling variations in social media data. As a single character does not inherently provide any semantic information that can be used for our purpose, we dismiss characterlevel feature representations as a possible choice of embedd"
P19-2052,E17-2068,0,0.0624558,"Missing"
P19-2052,S13-2053,0,0.121952,"Missing"
P19-2052,N16-1159,1,0.847281,"hybrid framework to exploit lexicons (polarized and emotional words) as well as different word embedding approaches in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements (Araque et al., 2017). Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentime"
P19-2052,Q17-1010,0,0.0494581,"the neural network framework of our model. Specific Encoder ui = tanh(Wi ki + bi ) (6) • Extended words: Number of words with multiple contiguous repeating characters. • Aggregate positive and negative sentiments: Using SentiWordNet (Esuli and Sebastiani, 2006) for every word bar articles and conjunctions, and combining the sentiment polarity values into net positive aggregate and net negative aggregate features. • Repeated exclamations and other punctuation: Number of sets of two or more contiguous punctuation. • Exclamation at end of sentence: Boolean value. • Monolingual Sentence Vectors: Bojanowski et al. (2017)’s method is used to train word vectors for Hindi words in the code-mixed sentences. 373 t Representation of the Collective Sentiment of the Sentence cmr s = cmr c = h t Representation of the Speciﬁc Sentiment of the Sentence ∑ α ik i i=1 α1 ← h1 ← h2 ← h3 ← ht → h1 → h2 → h3 → ht sw 3 sw t α2 α3 Attention Weights αt ← k1 ← k2 ← k3 ← kt → → → → sw 1 sw 2 sw 3 sw t BiLSTM Layer BiLSTM Layer sw 1 Sub-word Representations sw 2 k1 k2 k3 kt Sub-word Representations (B) Speciﬁc Encoder Models the speciﬁc sentiments of the sentence (A) Collective Encoder Models the overall sentiment of the sentence F"
P19-2052,W14-3908,0,0.0241977,"in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements (Araque et al., 2017). Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using contrastive learning with Siamese networks to map code-mixed and standard language text to a common sentiment space (Choudhary et al., 2018). 3 3.1 Generating Subword Representations Word embeddings are now commonplace but are ge"
P19-2052,D13-1170,0,0.00807254,"Missing"
P19-2052,esuli-sebastiani-2006-sentiwordnet,0,0.0102381,"king inspiration from (Yang et al., 2016), we quantify the significance of a subword by measuring the similarity of an additional hidden representation ui of each sub-word against a sub-word level context vector X. Then, a normalized significance weight αi is obtained. (3) exp(uTi X) αi = P exp(uTi X) (4) Feature Network We also use linguistic features to augment the neural network framework of our model. Specific Encoder ui = tanh(Wi ki + bi ) (6) • Extended words: Number of words with multiple contiguous repeating characters. • Aggregate positive and negative sentiments: Using SentiWordNet (Esuli and Sebastiani, 2006) for every word bar articles and conjunctions, and combining the sentiment polarity values into net positive aggregate and net negative aggregate features. • Repeated exclamations and other punctuation: Number of sets of two or more contiguous punctuation. • Exclamation at end of sentence: Boolean value. • Monolingual Sentence Vectors: Bojanowski et al. (2017)’s method is used to train word vectors for Hindi words in the code-mixed sentences. 373 t Representation of the Collective Sentiment of the Sentence cmr s = cmr c = h t Representation of the Speciﬁc Sentiment of the Sentence ∑ α ik i i=1"
P19-2052,D14-1105,0,0.0622259,"using SVMs. Keeping in mind a lack of computational resources, Giatsoglou et al. (2017) came up with a hybrid framework to exploit lexicons (polarized and emotional words) as well as different word embedding approaches in a polarity classification model. Ensembling simple word embedding based models with surfaceform classifiers has also yielded slight improvements (Araque et al., 2017). Extending standard NLP tasks to code-mixed data has presented peculiar challenges. Methods for POS tagging of code-mixed data obtained from online social media such as Facebook and Twitter has been attempted (Vyas et al., 2014) . Shallow parsing of code-mixed data curated from social media has also been tried (Sharma et al., 2016). Work has also been done to support word level identification of languages in code-mixed text (Chittaranjan et al., 2014). Sharma et al. (2015) tried an approach based on lexicon lookup for text normalization and sentiment analysis of code-mixed data. Pravalika et al. (2017) used a lexicon lookup approach to perform domain specific sentiment analysis. Other attempts include using sub-word level compositions with LSTMs to capture sentiment at morpheme level (Joshi et al., 2016), or using co"
P19-2052,P12-2018,0,0.125109,"Missing"
P19-2052,N16-1174,0,0.0423438,"ilar to the collective encoder, in that it takes as input a subword representation of the sentence and is built over LSTMs, with one caveat - an affixed attention mechanism. This allows for selection of subwords which contribute the most towards the sentiment of the input text. It can be seen in Figure 2(B). Identifying which subwords play a significant role in deciding the sentiment is crucial. The specific encoder generates a context vector cmrs to this end. We first concatenate the forward and backward states to obtain a combined annotation (k1 , k2 , . . . , kt ). Taking inspiration from (Yang et al., 2016), we quantify the significance of a subword by measuring the similarity of an additional hidden representation ui of each sub-word against a sub-word level context vector X. Then, a normalized significance weight αi is obtained. (3) exp(uTi X) αi = P exp(uTi X) (4) Feature Network We also use linguistic features to augment the neural network framework of our model. Specific Encoder ui = tanh(Wi ki + bi ) (6) • Extended words: Number of words with multiple contiguous repeating characters. • Aggregate positive and negative sentiments: Using SentiWordNet (Esuli and Sebastiani, 2006) for every wor"
R19-1063,J97-4002,0,0.34274,"Missing"
R19-1063,P97-1051,0,0.882972,"Missing"
R19-1063,L18-1556,0,0.118017,"dencies (Schuster et al., 2018), etc. There are no known systems that handle NPE detection and resolution in English. However, on a related linguistic phenomenon called oneanaphora or one-substitution, in which the elided noun is replaced by an overt pro-form, there is a thorough data-driven investigation (Gardiner, 2003) and machine-learning methods that use heuristics proposed in this study (Ng et al., 2005). Another phenomenon similar to NPE is zeroanaphora, which has been thoroughly studied in some pro drop languages such as Chinese (Yeh and Chen, 2019a,b) and Japanese (Iida et al., 2007; Asao et al., 2018; Chen, 2016). Zero-anaphora does not occur in English, although there is some evidence of the phenomenon being used to achieve certain interactional functions in ordinary conversational settings by English speakers (Oh, 2005). There are also proposed heuristics for determining antecedents of pronominal words (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). In the No. 1. Syntactic Category Can License NPE Cardinal Numbers 2. Ordinal Numbers 3. Demonstrative Determiners (Plural) Quantifiers (Not all) Superlative Adjectives Noun Possessives Pronoun Possessives Interrogative Determiners 4. 5"
R19-1063,D15-1162,0,0.190048,"Missing"
R19-1063,C96-1021,0,0.447898,"that use heuristics proposed in this study (Ng et al., 2005). Another phenomenon similar to NPE is zeroanaphora, which has been thoroughly studied in some pro drop languages such as Chinese (Yeh and Chen, 2019a,b) and Japanese (Iida et al., 2007; Asao et al., 2018; Chen, 2016). Zero-anaphora does not occur in English, although there is some evidence of the phenomenon being used to achieve certain interactional functions in ordinary conversational settings by English speakers (Oh, 2005). There are also proposed heuristics for determining antecedents of pronominal words (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). In the No. 1. Syntactic Category Can License NPE Cardinal Numbers 2. Ordinal Numbers 3. Demonstrative Determiners (Plural) Quantifiers (Not all) Superlative Adjectives Noun Possessives Pronoun Possessives Interrogative Determiners 4. 5. 6. 7. 8. 1. 2. 3. 4. Cannot License NPE Adjectives Demonstrative Determiners (Singular) Articles Quantifiers (Not all) 3 Task Description Resolution of ellipsis comprises two tasks - detection of the elided material and antecedent selection. In some cases, reference resolution might also be necessary (Liu et al., 2016; Nielsen, 2003). For example, in (3), the"
R19-1063,J94-4002,0,0.205557,"achine-learning methods that use heuristics proposed in this study (Ng et al., 2005). Another phenomenon similar to NPE is zeroanaphora, which has been thoroughly studied in some pro drop languages such as Chinese (Yeh and Chen, 2019a,b) and Japanese (Iida et al., 2007; Asao et al., 2018; Chen, 2016). Zero-anaphora does not occur in English, although there is some evidence of the phenomenon being used to achieve certain interactional functions in ordinary conversational settings by English speakers (Oh, 2005). There are also proposed heuristics for determining antecedents of pronominal words (Lappin and Leass, 1994; Kennedy and Boguraev, 1996). In the No. 1. Syntactic Category Can License NPE Cardinal Numbers 2. Ordinal Numbers 3. Demonstrative Determiners (Plural) Quantifiers (Not all) Superlative Adjectives Noun Possessives Pronoun Possessives Interrogative Determiners 4. 5. 6. 7. 8. 1. 2. 3. 4. Cannot License NPE Adjectives Demonstrative Determiners (Singular) Articles Quantifiers (Not all) 3 Task Description Resolution of ellipsis comprises two tasks - detection of the elided material and antecedent selection. In some cases, reference resolution might also be necessary (Liu et al., 2016; Nielsen, 20"
R19-1063,N18-1105,0,0.397181,"Missing"
R19-1063,L18-1065,0,0.115075,"testset has 76 positive and 132 negative samples. bank (Silveira et al., 2014) for English that contains example sentences for different types of ellipsis such as VPE, NPE, etc. The UD treebank marks NPE by raising the dependents of the elided noun to the position of head in cases where the dependents are overtly marked. Through a simple search for noun dependents that are given the status of noun heads, we get a total of 146 cases of NPE in 120 sentences from the UD treebank. There is another comparatively small corpus called the ParCorFull: a Parallel Corpus Annotated with Full Coreference (Lapshinova-Koltunski et al., 2018) that is dedicated to anaphora. This corpus targets anaphora, but deals partly with NPE cases as well, marking them with a nom-ellipsis tag. A simple search for this tag gives us 5 sentences containing 5 NPE cases. We also pick a total of 80 sentences containing 83 cases of NPE from linguistic textbooks on ellipsis (Lobeck, 1995; Saito et al., 2008; Menzel, 2017; Kim et al., 2019; Corver and van Koppen, 2011) to cover even the infrequently occurring cases. Finally, we randomly pick 132 sentences that do not contain NPE from UD treebank, ParCOrFull and the same linguistic textbooks. Some of the"
R19-1063,silveira-etal-2014-gold,0,0.139368,"Missing"
R19-1063,2016.lilt-13.1,0,0.304652,"meaning of the elided part from the antecedent, which can be present in the linguistic context as in (1) or has to be retrieved from real world knowledge as in (2), where Mary’s actually means Mary’s place. Previous Work 534 Proceedings of Recent Advances in Natural Language Processing, pages 534–540, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_063 present paper, we do not deal with one-anaphora, zero anaphora or pronominals, and restrict our focus to NPE. of VPE using machine learning (Nielsen, 2003) and parsed text (Nielsen, 2004), using linguistic principles (McShane and Babkin, 2016), with sentence trimming methods (McShane et al., 2015), reconstruction of sentences with gapping using improved parsing techniques that encode elided material dependencies (Schuster et al., 2018), etc. There are no known systems that handle NPE detection and resolution in English. However, on a related linguistic phenomenon called oneanaphora or one-substitution, in which the elided noun is replaced by an overt pro-form, there is a thorough data-driven investigation (Gardiner, 2003) and machine-learning methods that use heuristics proposed in this study (Ng et al., 2005). Another phenomenon s"
R19-1063,C04-1157,0,0.560414,"n only be understood when we reconstruct the meaning of the elided part from the antecedent, which can be present in the linguistic context as in (1) or has to be retrieved from real world knowledge as in (2), where Mary’s actually means Mary’s place. Previous Work 534 Proceedings of Recent Advances in Natural Language Processing, pages 534–540, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_063 present paper, we do not deal with one-anaphora, zero anaphora or pronominals, and restrict our focus to NPE. of VPE using machine learning (Nielsen, 2003) and parsed text (Nielsen, 2004), using linguistic principles (McShane and Babkin, 2016), with sentence trimming methods (McShane et al., 2015), reconstruction of sentences with gapping using improved parsing techniques that encode elided material dependencies (Schuster et al., 2018), etc. There are no known systems that handle NPE detection and resolution in English. However, on a related linguistic phenomenon called oneanaphora or one-substitution, in which the elided noun is replaced by an overt pro-form, there is a thorough data-driven investigation (Gardiner, 2003) and machine-learning methods that use heuristics propos"
S19-2009,W17-3007,0,0.0228899,"Missing"
S19-2009,S19-2007,0,0.0867251,"considers the sub-word information. • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. 4 Dataset The data collection methods used to compile the dataset used in HatEval is described in (Basile et al., 2019). We did not use any external datasets to augment the data for training our models. 5 Results and Analysis The official test set results scored on CodaLab have been presented below in Table 2. • Deep Contextualized Word Representations (ELMo) (Peters et al., 2018) use language models to get the embeddings for individual words. The entire sentence or paragraph is taken into consideration while calculating these embedding representations. ELMo uses a pre-trained bi-directional LSTM language model. For the input supplied, the ELMo architecture extracts the hidden state of each layer. A weighted s"
S19-2009,W17-3013,0,0.0617531,"Missing"
S19-2009,W18-4401,0,0.0370188,"Missing"
S19-2009,malmasi-zampieri-2017-detecting,0,0.120451,"chine learning classifiers for detection (Dinakar et al., 2011; Waseem and Hovy, 2016; Nobata et al., 2016). Deep learning methods for hate speech detection were used by Badjatiya et al. (2017) wherein the authors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and"
S19-2009,D14-1162,0,0.0955796,"ds. 3.1 Word Embeddings Word embeddings have been widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While w"
S19-2009,N18-1202,0,0.250349,"widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for"
S19-2009,W18-1209,0,0.0219416,"task organizers. Using each of the sentence embeddings we have mentioned above, we seek to evaluate how each of them performs when the vector representations are supplied for classification with various off-theshelf machine learning algorithms. For each of the evaluation tasks, we perform experiments using each of the sentence embeddings mentioned above and show our classification performance on the dev set given by the task organizers. considerably closes the gap to state-of-theart methods mono-lingually and substantially outperforms many complex techniques crosslingually. • Lexical Vectors (Salle and Villavicencio, 2018) is another word embedding similar to fastText with slightly modified objective. FastText (Bojanowski et al., 2016) is another word embedding model which incorporates character n-grams into the skipgram model of Word2Vec and considers the sub-word information. • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of"
S19-2009,W17-1101,0,0.0324117,"e early works related to hate speech detection employed the use of features like bag of words, word and character n-grams with relatively off-the-shelf machine learning classifiers for detection (Dinakar et al., 2011; Waseem and Hovy, 2016; Nobata et al., 2016). Deep learning methods for hate speech detection were used by Badjatiya et al. (2017) wherein the authors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlig"
S19-2009,W17-3003,0,0.0165797,"tures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Introduction Microblogging platforms like Twitter provide channels to exchange ideas using short messages called tweets. While such a platform can be used for constructive ideas, a small group of peopl"
S19-2009,W17-3012,0,0.0450659,"hors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity versus hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Introduction Microblogging platforms like Twitter provide channels to exchange ideas using short messages called tweets. While such a"
S19-2009,N16-2013,0,0.0529118,"sentence embeddings for transforming the input and SVM (with RBF kernel) for classification, scored first position (among 68) in the leaderboard on the test set for Subtask A in English language. In this paper we provide a detailed description of the approach, as well as the results obtained in the task. 1 2 In this section we briefly describe other work in this area. A few of the early works related to hate speech detection employed the use of features like bag of words, word and character n-grams with relatively off-the-shelf machine learning classifiers for detection (Dinakar et al., 2011; Waseem and Hovy, 2016; Nobata et al., 2016). Deep learning methods for hate speech detection were used by Badjatiya et al. (2017) wherein the authors experimented with a combination of multiple deep learning architectures along with randomly initialized word embeddings learned by Long Short Term Memory (LSTM) models. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al.,"
S19-2109,W18-4401,0,0.0553136,"Missing"
S19-2109,malmasi-zampieri-2017-detecting,0,0.0316093,"y which can be misused to target offensive comments to targeted parties. Users may engage in generating offensive content on social media which may show aggressive behaviour and may also include hate speech. As a result, it is imperative for social media platforms to invest heavily in creating solutions which can identify offensive language and to prevent such behaviour on social media. Using computational methods to identify offense, aggression and hate speech in user generated content has been gaining attention in the recent years as evidenced in (Waseem et al., 2017; Davidson et al., 2017; Malmasi and Zampieri, 2017; Kumar et al., 2018) and workshops such as Abusive Language Workshop (ALW) 1 and Work1 Related Work 3 Methodology 3.1 Word Embeddings Word embeddings have been widely used in modern Natural Language Processing applications as 2 https://sites.google.com/view/trac1 http://ta-cos.org/ 4 https://sites.google.com/site/alw2018 5 https://sites.google.com/view/trac1 3 https://sites.google.com/view/alw2018 611 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 611–616 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics s"
S19-2109,D18-2029,0,0.0233732,"ing techniques. A common approach of obtaining a sentence representation using word embeddings is by the simple and na¨ıve way of using the simple arithmetic mean of all the embeddings of the words present in the sentence. Smooth inverse frequency, which uses weighted averages and modifies it using Singular Value Decomposition (SVD), has been a strong contender as a baseline over traditional averaging technique (Arora et al., 2016). Other sentence embedding techniques include pmeans (R¨uckl´e et al., 2018), InferSent (Conneau et al., 2017), SkipThought (Kiros et al., 2015), Universal Encoder (Cer et al., 2018). We formulate each of the sub-tasks of OffensEval as a text classification task. In this paper, we evaluate various pre-trained sentence embeddings for identifying the offense, hate and aggres• The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length"
S19-2109,D14-1162,0,0.107946,"ub-tasks as defined in this task. In the following, we discuss various popular sentence embedding methods in brief. they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 • InferSent (Conneau et al."
S19-2109,N18-1202,0,0.379919,", we discuss various popular sentence embedding methods in brief. they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 • InferSent (Conneau et al., 2017) is a set of embeddings proposed by Fac"
S19-2109,D17-1070,0,0.146605,"et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 • InferSent (Conneau et al., 2017) is a set of embeddings proposed by Facebook. InferSent embeddings have been trained using the popular language inference corpus. Given two sentences the model is trained to infer whether they are a contradiction, a neutral pairing, or an entailment. The output is an embedding of 4096 dimensions. • Concatenated Power Mean Word Embedding (R¨uckl´e et al., 2018) generalizes the concept of average word embeddings to power mean word embeddings. The concatenation of different types of power mean word embeddings considerably closes the gap to state-of-theart methods mono-lingually and substantially"
S19-2109,W18-1209,0,0.0222762,"the popular language inference corpus. Given two sentences the model is trained to infer whether they are a contradiction, a neutral pairing, or an entailment. The output is an embedding of 4096 dimensions. • Concatenated Power Mean Word Embedding (R¨uckl´e et al., 2018) generalizes the concept of average word embeddings to power mean word embeddings. The concatenation of different types of power mean word embeddings considerably closes the gap to state-of-theart methods mono-lingually and substantially outperforms many complex techniques crosslingually. Sentence Embeddings • Lexical Vectors (Salle and Villavicencio, 2018) is another word embedding similar to fastText with slightly modified objective. FastText (Bojanowski et al., 2016) is another word embedding model which incorporates character n-grams into the skipgram model of Word2Vec and considers the sub-word information. While word embeddings can produce representations for words which can capture the linguistic properties and the semantics of the words, the idea of representing sentences as vectors is an important and open research problem (Conneau et al., 2017). Finding a universal representation of a sentence which works with a variety of downstream t"
S19-2109,W17-1101,0,0.0647868,"Missing"
S19-2109,W17-3007,0,0.0251414,"Missing"
S19-2109,W17-3003,0,0.0303558,"al leaderboard. 2 In this section we briefly describe other work in this area. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018; Badjatiya et al., 2017). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity vs. hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Online Safety (TA-COS) 3 , Abusive Language Workshop 4 , and TRAC 5 . Related shared tasks include GermEval (Wiegand et al., 2018) and TRAC (Kumar et al., 2018). Through the paper we provide a detailed de"
S19-2109,W17-3012,0,0.0544584,"uses SVM (RBF kernel) for training, scored third position on the official leaderboard. 2 In this section we briefly describe other work in this area. Papers published in the last two years include the surveys by (Schmidt and Wiegand, 2017) and (Fortuna and Nunes, 2018), the paper by (Davidson et al., 2017) which presented the Hate Speech Detection dataset used in (Malmasi and Zampieri, 2017) and a few other recent papers such as (ElSherief et al., 2018; Gamb¨ack and Sikdar, 2017; Zhang et al., 2018; Badjatiya et al., 2017). A proposal of typology of abusive language sub-tasks is presented in (Waseem et al., 2017). For studies on languages other than English see (Su et al., 2017) on Chinese and (Fiˇser et al., 2017) on Slovene. Finally, for recent discussion on identifying profanity vs. hate speech see (Malmasi and Zampieri, 2018). This work highlighted the challenges of distinguishing between profanity, and threatening language which may not actually contain profane language. Some of the similar and related previous workshops are Text Analytics for Cybersecurity and Online Safety (TA-COS) 3 , Abusive Language Workshop 4 , and TRAC 5 . Related shared tasks include GermEval (Wiegand et al., 2018) and TR"
S19-2109,W17-3013,0,0.0399184,"Missing"
S19-2109,N19-1144,0,0.0419401,"f each layer. A weighted sum is computed of the hidden states to obtain an embedding for each sentence. Using each of the sentence embeddings we have mentioned above, we seek to evaluate how each of them performs when the vector representations are supplied for classification with various off-theshelf machine learning algorithms. For each of the evaluation tasks, we perform experiments using each of the sentence embeddings mentioned above and show our classification performance on the dev set given by the task organizers. 4 Figure 1: Distribution of label combinations in the data (taken from (Zampieri et al., 2019)) posts as offensive (OFF) vs not (NOT). Subtask B (Categorization of Offensive Language) deals with categorization of offense as: targeted (TIN) and untargeted (INT). Sub-task C (Offensive Language Target Identification) categorizes the targets of insults and threats as individual (IND), group (GRP), and other (OTH). The overall dataset across the three sub-tasks consists of 14100 posts. Fig. 1 (reproduced from (Zampieri et al., 2019)) shows dataset size details. Dataset The data collection methods used to compile the dataset used in OffensEval is described in (Zampieri et al., 2019). Sub-tas"
S19-2203,S16-1130,0,0.0294083,"is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic"
S19-2203,D18-2029,0,0.0348251,"the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for words which can capture the linguistic properties and the semantics of the words, the idea of representing sentences as vectors is an important and open research problem (Conneau et al., 2017). Finding a universal representation of a sentence which works with a variety of downstream tasks 1161 • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. is the major goal of many sentence embedding techniques. A common approach of obtaining a sentence representation using word embeddings is by the simple and na¨ıve way of using the simple"
S19-2203,D17-1070,0,0.123195,"d embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for words which can capture the linguistic properties and the semantics of the words, the idea of representing sentences as vectors is an important and open research problem (Conneau et al., 2017). Finding a universal representation of a sentence which works with a variety of downstream tasks 1161 • The Universal Sentence Encoder (Cer et al., 2018) encodes text into high dimensional vectors. The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector. is the major goal of many sentenc"
S19-2203,S17-2045,0,0.0168754,"ed (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features such as Bag of Words (BoW) (FrancoSalvador et al., 2016); Bag of vectors (BoV) (Mohtarami et al., 2016); Lexical features (for example, Cosine"
S19-2203,S16-1126,0,0.0313487,"uestions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features such as Bag of Words (BoW) (FrancoSalvador et al., 2016); Bag of vectors (BoV) (Mohtarami et al., 2016); Lexical features (for example, Cosine Similarity, Word Overlap, Noun Overlap, N-gram Overlap, Longest Common Substring/Subsequence, Keyword and Named Entity features etc.) (Franco-Salvador et al., 2016; Mohtarami et al., 2016; Nandi et al., 2017); Semantic features (for example, Distributed representations of text, Knowledge Graphs, Distributed word alignments, Word Cluster Similarity, etc.) (Franco-Salvador et al., 2016); Word Embedding Features (like Word2vec, GloVe etc.) (Wang and Poupart, 2016; Mohtarami et al., 2016; Nandi et al., 2017); and Metadata-based features (Mohtarami et al., 2016; Mihaylova et al., 2016; Xie Methodology and Data The data supplied by organizers is used for the task at hand. Specifically, for sub-task A, the subject and body for each question is provided by the"
S19-2203,S19-2149,0,0.0683836,"Missing"
S19-2203,S16-1128,0,0.176937,"erform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al.,"
S19-2203,S17-2009,0,0.0878099,"shelf machine learning algorithms for the multi-class prediction problem. The approach is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016)"
S19-2203,D14-1162,0,0.0840945,"ly. 3.1 Word Embeddings Word embeddings have been widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While w"
S19-2203,S17-2055,0,0.0640604,"Missing"
S19-2203,S16-1133,0,0.103328,"on problem. The approach is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and"
S19-2203,S17-2047,0,0.0230645,"ng algorithms for the multi-class prediction problem. The approach is described in Section 3 where we describe our methodology in detail. et al., 2017). In this work, we seek to evaluate pre-trained sentence embeddings and how they perform across comprehension of questions in the community QA tasks. We now describe the methodology and data in the following section. 3 2 Related Work For classification tasks like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in t"
S19-2203,S17-2052,0,0.0310138,"like question similarity across community QA forums, machine learning classification algorithms like Support Vector Maˇ chines (SVMs) have been used (Saina et al., 2017; Nandi et al., 2017; Xie et al., 2017; Mihaylova et al., 2016; Wang and Poupart, 2016; Balchev et al., 2016). Recently, advances in deep neural network architectures have also led to the use ˇ of Convolutional Neural Networks (CNNs) (Saina et al., 2017; Mohtarami et al., 2016) which perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features"
S19-2203,P11-1066,0,0.0355737,"perform reasonably well for selection of the correct answer amongst cQA formus. Algorithms and methods for answer selection also include works by (Zhang et al., 2017) which use a Long-Short Term Memory (LSTM) model for answer selection. Similarly, LSTMs for answer selection are also used by (Feng et al., 2017; Mohtarami et al., 2016). Other works in the space include use of Random Forests (Wang and Poupart, 2016); topic models to match the questions at both the term level and topic level (Zhang et al., 2014). There have also been works on translation based retrieval models (Jeon et al., 2005; Zhou et al., 2011); XgBoost (Feng et al., 2017) and Feedforward Neural Networks (NN) (Wang and Poupart, 2016). All of the above related works on cQA used the features such as Bag of Words (BoW) (FrancoSalvador et al., 2016); Bag of vectors (BoV) (Mohtarami et al., 2016); Lexical features (for example, Cosine Similarity, Word Overlap, Noun Overlap, N-gram Overlap, Longest Common Substring/Subsequence, Keyword and Named Entity features etc.) (Franco-Salvador et al., 2016; Mohtarami et al., 2016; Nandi et al., 2017); Semantic features (for example, Distributed representations of text, Knowledge Graphs, Distributed"
S19-2203,N18-1202,0,0.0658627,"widely used in modern Natural Language Processing applications as they provide vector representation of words. They capture the semantic properties of words and the linguistic relationship between them. These word embeddings have improved the performance of many downstream tasks across many domains like text classification, machine comprehension etc. (Camacho-Collados and Pilehvar, 2018). Multiple ways of generating word embeddings exist, such as Neural Probabilistic Language Model (Bengio et al., 2003), Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and more recently ELMo (Peters et al., 2018). These word embeddings rely on the distributional linguistic hypothesis. They differ in the way they capture the meaning of the words or the way they are trained. Each word embedding captures a different set of semantic attributes which may or may not be captured by other word embeddings. In general, it is difficult to predict the relative performance of these word embeddings on downstream tasks. The choice of which word embeddings should be used for a given downstream task depends on experimentation and evaluation. 3.2 Sentence Embeddings While word embeddings can produce representations for"
S19-2203,W18-1209,0,0.0254185,"been trained using the popular language inference corpus. Given two sentences the model is trained to infer whether they are a contradiction, a neutral pairing, or an entailment. The output is an embedding of 4096 dimensions. • Concatenated Power Mean Word Embedding (R¨uckl´e et al., 2018) generalizes the concept of average word embeddings to power mean word embeddings. The concatenation of different types of power mean word embeddings considerably closes the gap to state-of-theart methods mono-lingually and substantially outperforms many complex techniques crosslingually. • Lexical Vectors (Salle and Villavicencio, 2018) is another word embedding similar to fastText with slightly modified objective. FastText (Bojanowski et al., 2016) is another word embedding model which incorporates character n-grams into the skipgram model of Word2Vec and considers the sub-word information. • Deep Contextualized Word Representations (ELMo) (Peters et al., 2018) use language models to get the embeddings for individual words. The entire sentence or paragraph is taken into consideration while calculating these embedding representations. ELMo uses a pre-trained bi-directional LSTM language model. For the input supplied, the ELM"
W14-0126,N13-1088,1,0.885924,"Missing"
W14-0126,N03-1032,0,0.0718943,"Missing"
W14-0126,C12-3033,1,\N,Missing
W14-5126,W04-2215,0,0.0323745,"ously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP tasks such as POS tagging and chunking, but not much work has gone in the direction of corpora generation aid for Machine Translation (MT). The few similar works that we did find are noted below. PolyPhraZ (Hajlaoui and Boitet, 2004) is one such tool which helps in visualizing, editing and evaluating MT systems on parallel corpora. CasualConc (Imao, 2008) is a parallel concordancer which generates keyword in context concordance lines, word clusters, collocation analysis, and word counts. MemoQ (Kilgray, 2006) and Trados (SDL, 2007) are also Computer Aided Translation (CAT) D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 162–166, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen syst"
W14-5126,P07-2045,0,0.00501907,"to ensure time and cost effectiveness of the process. Traditional method of parallel corpus creation 162 involves manual translation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text"
W14-5126,J03-1002,0,0.00655842,"slation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP ta"
W14-5126,E03-1016,0,0.0455033,"Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen systems which are commercially available with features like Translation memory, and Term Extraction. Wordfast is CAT system having just one free version “WordFast Anywhere”. We studied and used the system, but found the interface less intuitive, and hard to use. “WordFast Anywhere” also has an integrated MT system which provides translations via Microsoft Bing and an integrated MT system. Another system we came across is a web based text corpora development system (Yablonsky, 2003) that focuses on the development of UML-specifications, architecture and implementations of DBMS tools. None of the above mentioned systems provide a word alignment visualization, which can be corrected manually, and saved to provide perfect phrase tables later. 3 Parallel Corpora Management System Parallel Corpora Management System (PaCMan) (Figure: 1) is a platform-independent web-based workbench for managing all the processes involved in the generation of good quality parallel corpora. Along with covering the procedural / managerial aspects of the parallel corpora generation process, this t"
W14-5126,E03-1063,0,\N,Missing
W14-5126,kunchukuttan-etal-2014-shata,1,\N,Missing
W16-6320,D14-1162,0,0.0849362,"Missing"
W16-6320,C96-1071,0,0.152981,"ervations of testing on both the datasets. • In Section 5 We give our conclusions from the experiments and also describe methods to extend our approach to other languages. 2 Related Work NER task has been extensively studied in the literature. Previous approaches in NER can be roughly classified into Rule based approaches and learning based approaches. Rule based approaches include the system developed by Ralph Grishman in 1995 which used a large dictionary of Named Entities (R. Grishman et al., 1995). Another model was built for NER using large lists of names of people, location etc. in 1996(Wakao et al., 1996). A huge disadvantage of these systems is that155 a huge list needed to be made and the output for any entity not seen before could not be determined. They lacked in discovering new named entities, not present in the dictionary available and also cases where the word appeared in the dictionary but was not a named entity. This is an even bigger problem for indian languages which would frequently be agglutinative in nature hence creation of dictionaries would be rendered impossible. People either used feature learning based approaches using Hand-crafted features like Capitalization etc. They gav"
W16-6320,P01-1041,0,0.0649605,"Missing"
W16-6320,P09-1116,0,0.0570156,"Missing"
W16-6320,D15-1104,0,0.0204478,"et al., 2001). Many attempts have been made to combine the above two approaches to achieve better performance. An example of this is (Srihari et al., 2000) who use a combination of both handcrafted rules along with HMM and ME. More recent approaches for Indian language and Hindi NER are based on CRFs and include (Das et al., 2013) and (Sharnagat et al., 2013). The recent RNN based approaches for NER include ones by (Lample et al., 2016). Also, there are many approaches which combine NER with other tasks like (Collobert et al., 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al., 2015) (combining Entity Linking and NER) which have produced state-ofthe-art results on English datasets. 3 Proposed Approach Owing to the recent success in deep learning frameworks, we sought to apply the techniques to Indian language data like Hindi. But, the main challenge in these approaches is to learn inspite of the scarcity of labelled data, one of the core problems of adapting deep-learning approaches to this domain. We propose to leverage the vast amount of unlabelled data available in this domain. The recurrent neural networks RNN trained generally have Figure 1: Our pipeline is illustrat"
W16-6320,A00-1034,0,0.0609723,"lassifier like Support Vector Machine (SVM)(Takeuchi et al., 2002), Naive Bayes (NB) or Maximum Entropy (ME) classifiers. Some posed this problem as a sequence labelling problem terming the context is very important in determining the entities. Then, the handcrafted series were used in sequences using Machine learning methods such as Hidden Markov Models (HMM)(Bikel et al., 1997), Conditional Random Field (CRF) (Das et al., 2013) and Decision Trees (DT)(Isozaki et al., 2001). Many attempts have been made to combine the above two approaches to achieve better performance. An example of this is (Srihari et al., 2000) who use a combination of both handcrafted rules along with HMM and ME. More recent approaches for Indian language and Hindi NER are based on CRFs and include (Das et al., 2013) and (Sharnagat et al., 2013). The recent RNN based approaches for NER include ones by (Lample et al., 2016). Also, there are many approaches which combine NER with other tasks like (Collobert et al., 2011) (POS Tagging and NER along with Chunking and SRL tasks) and (Luo et al., 2015) (combining Entity Linking and NER) which have produced state-ofthe-art results on English datasets. 3 Proposed Approach Owing to the rece"
W17-0811,P13-1045,0,0.0204246,"oken Indian languages worldwide after Hindi and Bengali. For the construction of these datasets, our approach relies on translation and re-annotation of word similarity datasets of English. We also present baseline scores for word representation models using state-of-the-art techniques for Urdu, Telugu and Marathi by evaluating them on newly created word similarity datasets. 1 Introduction Word representations are being increasingly popular in various areas of natural language processing like dependency parsing (Bansal et al., 2014), named entity recognition (Miller et al., 2004) and parsing (Socher et al., 2013). Word similarity task is one of the most popular benchmark for the evaluation of word representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2005), Machine Translation Evaluation (Lavie and Denkowski, 2009), Question Answering (Mohler et al., 2011), and Lexical Substitution (Diana and Navigli, 2009). Word Similarity task is a computationally efficient method to evaluate the quality of word vectors. It relies on finding correlation between human assigned semantic similarity (between words) and corresponding word vectors. We have used 91 Procee"
W17-0811,jawaid-etal-2014-tagged,0,0.0301766,"these datasets in section 5. Finally, we analyze and explain the results in section 6 and finish this paper with how we plan to extend our work in section 7. 2 Datasets For all the models trained in this paper, we have used the Skip-gram, CBOW (Mikolov et al., 2013a) and FastText (Bojanowski et al., 2016) algorithms. The dimensionality has been fixed at 300 with a minimum count of 5 along with negative sampling. As training set of Marathi, we use the monolingual corpus created by IIT-Bombay. This data contains 27 million tokens. For Urdu, we use the untagged corpus released by Jawaid et. al. (2014) containing 95 million tokens. For Telugu, we use Telugu wikidump available at https://archive.org/details/ tewiki-20150305 having 11 million tokens. For testing, we use the newly created datasets. The word similarity datatsets for Urdu, Marathi, Telugu, Punjabi, Gujarati and Tamil contain 100, 104, 111, 143, 163 and 97 word pairs respectively. For rest of the paper, we have calculated the Spearman ρ (multiplied by 100) between human assigned similarity and cosine similarity of our word embeddings for the word-pairs. For any word which was is not found, we assign it a zero vector. In order to"
W17-0811,P15-2001,0,0.0358663,"Missing"
W17-0811,N04-1043,0,0.0802272,"arati. These languages are most spoken Indian languages worldwide after Hindi and Bengali. For the construction of these datasets, our approach relies on translation and re-annotation of word similarity datasets of English. We also present baseline scores for word representation models using state-of-the-art techniques for Urdu, Telugu and Marathi by evaluating them on newly created word similarity datasets. 1 Introduction Word representations are being increasingly popular in various areas of natural language processing like dependency parsing (Bansal et al., 2014), named entity recognition (Miller et al., 2004) and parsing (Socher et al., 2013). Word similarity task is one of the most popular benchmark for the evaluation of word representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2005), Machine Translation Evaluation (Lavie and Denkowski, 2009), Question Answering (Mohler et al., 2011), and Lexical Substitution (Diana and Navigli, 2009). Word Similarity task is a computationally efficient method to evaluate the quality of word vectors. It relies on finding correlation between human assigned semantic similarity (between words) and corresponding wo"
W17-0811,P05-3019,0,0.0102832,"lso present baseline scores for word representation models using state-of-the-art techniques for Urdu, Telugu and Marathi by evaluating them on newly created word similarity datasets. 1 Introduction Word representations are being increasingly popular in various areas of natural language processing like dependency parsing (Bansal et al., 2014), named entity recognition (Miller et al., 2004) and parsing (Socher et al., 2013). Word similarity task is one of the most popular benchmark for the evaluation of word representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2005), Machine Translation Evaluation (Lavie and Denkowski, 2009), Question Answering (Mohler et al., 2011), and Lexical Substitution (Diana and Navigli, 2009). Word Similarity task is a computationally efficient method to evaluate the quality of word vectors. It relies on finding correlation between human assigned semantic similarity (between words) and corresponding word vectors. We have used 91 Proceedings of the 11th Linguistic Annotation Workshop, pages 91–94, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics German and Portuguese reported IAA (Inter Annotator Ag"
W17-0811,N13-1090,0,0.0368388,"nnotator Agreement) of 0.81 and 0.71 respectively, no IAA was calculated for French. For Spanish and Farsi, inter annotator agreement of 0.83 and 0.88 respectively was reported. Our datasets were created using RG-65 and WordSim-353 as base, and their respective IAA(s) are mentioned later in the paper. experimental results of various models evaluated on these datasets in section 5. Finally, we analyze and explain the results in section 6 and finish this paper with how we plan to extend our work in section 7. 2 Datasets For all the models trained in this paper, we have used the Skip-gram, CBOW (Mikolov et al., 2013a) and FastText (Bojanowski et al., 2016) algorithms. The dimensionality has been fixed at 300 with a minimum count of 5 along with negative sampling. As training set of Marathi, we use the monolingual corpus created by IIT-Bombay. This data contains 27 million tokens. For Urdu, we use the untagged corpus released by Jawaid et. al. (2014) containing 95 million tokens. For Telugu, we use Telugu wikidump available at https://archive.org/details/ tewiki-20150305 having 11 million tokens. For testing, we use the newly created datasets. The word similarity datatsets for Urdu, Marathi, Telugu, Punja"
W17-0811,W06-1104,0,0.0628401,"Missing"
W17-0811,P11-1076,0,0.0253144,"ugu and Marathi by evaluating them on newly created word similarity datasets. 1 Introduction Word representations are being increasingly popular in various areas of natural language processing like dependency parsing (Bansal et al., 2014), named entity recognition (Miller et al., 2004) and parsing (Socher et al., 2013). Word similarity task is one of the most popular benchmark for the evaluation of word representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2005), Machine Translation Evaluation (Lavie and Denkowski, 2009), Question Answering (Mohler et al., 2011), and Lexical Substitution (Diana and Navigli, 2009). Word Similarity task is a computationally efficient method to evaluate the quality of word vectors. It relies on finding correlation between human assigned semantic similarity (between words) and corresponding word vectors. We have used 91 Proceedings of the 11th Linguistic Annotation Workshop, pages 91–94, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics German and Portuguese reported IAA (Inter Annotator Agreement) of 0.81 and 0.71 respectively, no IAA was calculated for French. For Spanish and Farsi, inter"
W17-0811,P14-2131,0,0.0321417,"– Urdu, Telugu, Marathi, Punjabi, Tamil and Gujarati. These languages are most spoken Indian languages worldwide after Hindi and Bengali. For the construction of these datasets, our approach relies on translation and re-annotation of word similarity datasets of English. We also present baseline scores for word representation models using state-of-the-art techniques for Urdu, Telugu and Marathi by evaluating them on newly created word similarity datasets. 1 Introduction Word representations are being increasingly popular in various areas of natural language processing like dependency parsing (Bansal et al., 2014), named entity recognition (Miller et al., 2004) and parsing (Socher et al., 2013). Word similarity task is one of the most popular benchmark for the evaluation of word representations. Applications of word similarity range from Word Sense Disambiguation (Patwardhan et al., 2005), Machine Translation Evaluation (Lavie and Denkowski, 2009), Question Answering (Mohler et al., 2011), and Lexical Substitution (Diana and Navigli, 2009). Word Similarity task is a computationally efficient method to evaluate the quality of word vectors. It relies on finding correlation between human assigned semantic"
W17-0811,S14-2003,0,\N,Missing
W17-7526,D14-1162,0,0.0954806,"Missing"
W17-7526,P08-1028,0,0.0300481,"cture (Siamese LSTM, to be precise) that accepts two inputs, one being a word while another a phrase. The energy function in the Siamese network measures the similarity between these two input units. In the course of training the network, baseline word embeddings (Section 5.2) are modified and phrase embeddings are generated. We describe the model in detail in further sections. 2 Related Work There has been a significant development in phrase embeddings after the word2vec breakthrough by Mikolov et al. in 2013. Earlier, word vectors were combined with some functions to create phrase vectors. (Mitchell and Lapata, 2008) developed systems with predefined composition operators. In their work, they created datasets of similarity for adjective-noun, noun-noun and verb-object biS Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 205–211, c Kolkata, India. December 2017. 2016 NLP Association of India (NLPAI) gram units. They found the simple additive and multiplicative function to be quite effective. However, these simple functions ignored word orientation in phrases and their interaction. To make these compositions robust in order to handle complex st"
W17-7526,Q15-1017,0,0.0182116,"is method is useful for learning short phrase representations with good quality, it does not generalize well to relatively longer and rare occurring phrases in the dataset. In 2011, Socher et al. used a recursive neural network to learn representations for multi-word phrases. In particular, they used an unsupervised autoencoder and their model performed well on the sentiment classification task but not so well on phrase similarity related problems. The primary reason for this was the low-dimensional representations (upto 50) they had used to reduce the computational complexity. More recently (Yu and Dredze, 2015), the idea of learning composition functions based on phrase structure and context was proposed to compose phrase embeddings using baseline word embeddings. The composition model developed by Yu and Dredze used Feature-rich Compositional Transformations (FCT) from words to phrases in which the summation weights were defined by the linguistic features of component words such as POS tags, head words and so on. 3 Methodology We develop our model with the objective of training a system to predict similarity between a word and a phrase leveraging a similarity dataset. In our work, unlike the previo"
W17-7526,C08-1063,0,0.0175465,"mon abstract transformation for both the word and the phrase. While training our system over a large dataset containing input-output pairs as in Table 1, the model learns weights with which we build phrase embeddings and fine-tune baseline word embeddings such that both are embedded in the same space. 3.1 Siamese Neural Network For the task of signature verification, Siamese Neural Networks were first proposed by Bromley et al. in 1993. After that, the architecture has been used in several works of similarity and discrimation such as for face verification (Chopra et al., 2005), visual search (Liu et al., 2008), sentence similarity (Mueller and Thyagarajan, 2016), similar question retrieval in Q/A (Das et al., 2016), etc. Suppose Out(X) is a set of functions which has a set of parameters W. In Figure 1, input A is first given to the network. Then another input B is fed and a similarity function gets the outputs of these A and B, i.e. Out(A) and Out(B). Siamese network learns a value of W such that the similarity metric is small if Inp. A (first input) and Inp. B (second input) are similar and large if they are not. The similarity function can be defined as: S(Inp.A, Inp.B) = ||Out(A) − Out(B) ||(1)"
W17-7526,P16-1036,1,0.839095,"t containing input-output pairs as in Table 1, the model learns weights with which we build phrase embeddings and fine-tune baseline word embeddings such that both are embedded in the same space. 3.1 Siamese Neural Network For the task of signature verification, Siamese Neural Networks were first proposed by Bromley et al. in 1993. After that, the architecture has been used in several works of similarity and discrimation such as for face verification (Chopra et al., 2005), visual search (Liu et al., 2008), sentence similarity (Mueller and Thyagarajan, 2016), similar question retrieval in Q/A (Das et al., 2016), etc. Suppose Out(X) is a set of functions which has a set of parameters W. In Figure 1, input A is first given to the network. Then another input B is fed and a similarity function gets the outputs of these A and B, i.e. Out(A) and Out(B). Siamese network learns a value of W such that the similarity metric is small if Inp. A (first input) and Inp. B (second input) are similar and large if they are not. The similarity function can be defined as: S(Inp.A, Inp.B) = ||Out(A) − Out(B) ||(1) The use of LSTM network helps our system to learn the context of constituent words in a phrase. 4.2 We use"
W17-7526,N13-1092,0,0.108704,"Missing"
W17-7526,D12-1110,0,0.0605305,"y created datasets of similarity for adjective-noun, noun-noun and verb-object biS Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 205–211, c Kolkata, India. December 2017. 2016 NLP Association of India (NLPAI) gram units. They found the simple additive and multiplicative function to be quite effective. However, these simple functions ignored word orientation in phrases and their interaction. To make these compositions robust in order to handle complex structures in sentences, Matrix composition functions (Zanzotto et al., 2010; Socher et al., 2012) and Tensor composition functions (Bride et al., 2015) were proposed. In 2013, Mikolov et al. generated phrase representation using the same method used for word representation in word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b). High-frequency multi-word units such as New York was embedded along with the words by taking them as single token, or pseudo-words, i.e. New York. Though this method is useful for learning short phrase representations with good quality, it does not generalize well to relatively longer and rare occurring phrases in the dataset. In 2011, Socher et al. used a recu"
W17-7526,P13-1045,0,0.0486403,"n Section 6.1. We report the results (accuracy scores) of our system along with existing benchmark methods in Table 7. RAE is the recursive auto-encoder model developed in (Socher et al., 2011) wheras FCT-LM and FCTJoint are the Feature Rich Compositional Transformation Models proposed by (Yu and Dredze, 3 https://www.cs.york.ac.uk/ semeval-2013 2015) with Language Modeling and Joint Traning (Language Modeling and Task-Specific) objective respectively for updating the embeddings. We also report our results with the Recursive Neural Network (ReNN) based model developed by (Socher et al., 2011; Socher et al., 2013) and obtain comparable results with the state-of-the-art system developed for generating phrase representations evaluated on this task. Model RAE FCT-LM FCT-Joint ReNN Our System SemEval2013 Test 51.75 67.22 70.64 72.22 72.14 Table 7: Performance on the SemEval2013 5(a) Semantic Similarity Task We see that the Siamese network based model outperforms the RAE by significant margin. However, the ReNN still has the best performance. Since the method proposed in this work is primarily dependant on the dataset containing wordparaphrase pairs, the larger this data size, the better quality embeddings"
W17-7526,P15-1028,0,0.0382046,"Missing"
W17-7526,C10-1142,0,0.0302711,"ors. In their work, they created datasets of similarity for adjective-noun, noun-noun and verb-object biS Bandyopadhyay, D S Sharma and R Sangal. Proc. of the 14th Intl. Conference on Natural Language Processing, pages 205–211, c Kolkata, India. December 2017. 2016 NLP Association of India (NLPAI) gram units. They found the simple additive and multiplicative function to be quite effective. However, these simple functions ignored word orientation in phrases and their interaction. To make these compositions robust in order to handle complex structures in sentences, Matrix composition functions (Zanzotto et al., 2010; Socher et al., 2012) and Tensor composition functions (Bride et al., 2015) were proposed. In 2013, Mikolov et al. generated phrase representation using the same method used for word representation in word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b). High-frequency multi-word units such as New York was embedded along with the words by taking them as single token, or pseudo-words, i.e. New York. Though this method is useful for learning short phrase representations with good quality, it does not generalize well to relatively longer and rare occurring phrases in the dataset. In 2011, Soc"
W17-7526,D11-1014,0,0.0585216,"well Table 6: Nearest phrases for a given phrase 6.3 Semantic Similarity Task We use the embeddings derived by our system to evaluate the phrasal semantic similarity task of SemEval20133 and compare our results with that of the existing systems. The task of SemEval2013 5(a) is to determine if a word-phrase pair are semantically similar (True for similar and False for dissimilar) which is notably as same as the experiment in Section 6.1. We report the results (accuracy scores) of our system along with existing benchmark methods in Table 7. RAE is the recursive auto-encoder model developed in (Socher et al., 2011) wheras FCT-LM and FCTJoint are the Feature Rich Compositional Transformation Models proposed by (Yu and Dredze, 3 https://www.cs.york.ac.uk/ semeval-2013 2015) with Language Modeling and Joint Traning (Language Modeling and Task-Specific) objective respectively for updating the embeddings. We also report our results with the Recursive Neural Network (ReNN) based model developed by (Socher et al., 2011; Socher et al., 2013) and obtain comparable results with the state-of-the-art system developed for generating phrase representations evaluated on this task. Model RAE FCT-LM FCT-Joint ReNN Our S"
W17-7533,W06-1908,0,0.0803067,"s in the dialog. In order to add the query prediction for information retrieval (through API calls), we train a memory network. We also handle cases requiring updation of API calls and querying for additional information. Using the combination of sequence to sequence learning and memory network, we successfully create an end to end dialog system for Telugu. 1 Introduction There have been few attempts to create dialog systems for Telugu, which are mostly rule-based systems using ad-hoc user interactions to test the system rather than over a set of prepared test dialogs (Sravanthi et al., 2015; Reddy and Bandyopadhyay, 2006). This is primarily due to a lack of dialog data as Telugu is a low-resource language. Wen et al. (2016) proclaim that the greatest bottleneck for statistical approaches to dialog system development is the collection of appropriate data which is especially true for task oriented dialog systems; that for task-oriented dialog systems, indomain data is essential. Dialog models using neural networks are able to leverage the large amounts of data to learn meaningful representations for natural language 265 and generation strategies, and require only a minimal amount of domain knowledge and handcraf"
W17-7533,D11-1054,0,0.0449147,"nce learning and memory network, we successfully create an end-to-end dialog system for the tourist domain in Telugu. After discussing Related Work in Section 2, we outline the tasks our system must perform in Section 3, then we discuss the motivation behind our system pipeline in Section 4, Section 5 describes dialog data creation strategy, followed by sequence-to-sequence model for producing system responses in Section 6, Section 7 deals with the memory network layer for generating API calls, finally followed by the conclusion and future work in Sections 8 and 9 respectively. 2 Related Work Ritter et al. (2011) first proposed using generative probabilistic models to model conversations from micro-blogging websites, treating the response generation problem as a statistical machine translation problem, where the post is to be translated into a response. They find that generating responses is a harder problem than language translation due to the wide range of possible responses and a lack of alignment between the source and the response. Shang et al. (2015) extend the work by using recurrent neural networks, which they show outperform retrieval-based and SMT based methods for generating responses to a"
W18-1105,C16-1234,1,0.916593,"Missing"
W18-1105,W14-3914,0,0.163794,"Missing"
W18-1105,malmasi-zampieri-2017-detecting,0,0.0394319,"computational linguistics. Popularity of opinion-rich online resources like review forums and microblogging sites has encouraged users to express and convey their thoughts all across the world in real time. This often results in users posting offensive and abusive content online using hateful speech. These may be directed towards an individual or community to show their dissent. Detecting hate speech is thus important for lawmakers and social media platforms to discourage occurence of any wrongful activities. Previous research related to this task has mainly been focused on monolingual texts (Malmasi and Zampieri, 2017; Schmidt and Wiegand, 2017; T3 : “Jisne bhi Nirbhaya ka rape kiya should be bloody hanged till death.” Translation : “Whoever raped Nirbhaya, should be bloody hanged till death.” It can be observed that T1 and T3 contain hate speech, while T2 is an instance of normal speech. To the best of our knowledge, currently there are no online code-mixed resources available for detecting hate speech. We believe that our initial efforts in constructing a Hindi-English code-mixed dataset for hate speech detection will prove to be extremely valuable for linguists working in this domain. The structure of t"
W18-1105,W14-3902,0,0.0298398,"Missing"
W18-1105,N12-1071,0,0.0767182,"Missing"
W18-1105,E12-1049,0,0.066549,"Missing"
W18-1105,W17-1101,0,0.0380938,"opularity of opinion-rich online resources like review forums and microblogging sites has encouraged users to express and convey their thoughts all across the world in real time. This often results in users posting offensive and abusive content online using hateful speech. These may be directed towards an individual or community to show their dissent. Detecting hate speech is thus important for lawmakers and social media platforms to discourage occurence of any wrongful activities. Previous research related to this task has mainly been focused on monolingual texts (Malmasi and Zampieri, 2017; Schmidt and Wiegand, 2017; T3 : “Jisne bhi Nirbhaya ka rape kiya should be bloody hanged till death.” Translation : “Whoever raped Nirbhaya, should be bloody hanged till death.” It can be observed that T1 and T3 contain hate speech, while T2 is an instance of normal speech. To the best of our knowledge, currently there are no online code-mixed resources available for detecting hate speech. We believe that our initial efforts in constructing a Hindi-English code-mixed dataset for hate speech detection will prove to be extremely valuable for linguists working in this domain. The structure of the paper is as follows. In"
W18-1105,N16-1159,1,0.895526,"Missing"
W18-1105,D14-1105,0,0.191048,"Missing"
W18-1105,W12-2103,0,0.179039,"Missing"
W18-2405,W14-3914,0,0.0878615,"Missing"
W18-2405,W14-3902,0,0.387885,"Missing"
W18-2405,W17-4422,0,0.041171,"Missing"
W18-2405,I08-5008,0,0.0446852,"Missing"
W18-2405,N16-1159,1,0.927984,"Missing"
W18-2405,A00-1034,0,0.337299,"Missing"
W18-2405,W03-0419,0,0.269287,"Missing"
W18-2405,W12-5211,0,0.477989,"Missing"
W18-2405,D14-1105,0,0.476276,"Missing"
W18-2405,W16-2705,0,0.154649,"Missing"
W18-2405,W12-5603,0,0.0749798,"Missing"
W18-2405,I08-5014,0,0.0607714,"Missing"
W18-3205,P17-1042,0,0.0197008,"ome work in CM sentiment analysis (Joshi et al., 2016). Raghavi et al. (2015) demonstrate question type classification for CM questions and Raghavi et al. (2017) also demonstrate a CM factoid QA system that searches for the lexically translated CM question using Google Search on a small dataset of 100 CM questions. To the best of our knowledge, there has been no work on building an end-to-end CM QA system over a KB. 4 Our System: CMQA Bilingual Embeddings Recent work has shown that it is possible to obtain bilingual embeddings using only a minimal set of parallel lexicons (Smith et al., 2017; Artetxe et al., 2017; Ammar et al., 2016; Luong et al., 2015; P et al., 2014) or without any parallel lexicons (Zhang et al., 2017; Conneau et al., 2017). Our approach, can use these bilingual embeddings and supervised corpus for a resource-rich language, to enable CM applications for resourcepoor languages. In this section, we describe our system which consists of two components: (1) the Candidate Generation module for finding relevant candidates and (2) a Candidate Re-ranking model, for getting the top answer from the list of candidate answers. 4.1 Candidate Generation Any freebase tuple (specifically, the obje"
W18-3205,P16-1036,1,0.923037,"s (Ren et al., 2010), have also explored using lexical translations for this task. Recently, Ture et al. (Ture and Boschee, 2016) proposed models that combine different translation settings. There have been some efforts (Pouran Ben Veyseh, 2016; Hakimov et al., 2017; Chen et al., 2017b) to attempt cross-lingual question answering over knowledge bases. 2 41 http://lucene.apache.org/solr/ Figure 1: TSHCNN Architecture to the matching. This is a limiting factor in candidate generation for CM questions. 4.2 spired by the success of neural models in various image and text tasks (Vo and Hays, 2016; Das et al., 2016). Our network is a TripletSiamese Hybrid Convolutional neural network (TSHCNN), see figure 1. Vo and Hays (2016) show that classification-siamese hybrid and triplet networks work well on image similarity tasks. Our hybrid model can jointly extract and exchange information from the question and tuple inputs. Candidate Re-ranking We use Convolutional Neural Networks (CNNs) to learn the semantic representation for input text (Kim, 2014; Hu et al., 2015; Lai et al., 2015; Cho et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015). CNNs learn globally word order invariant features and at the sa"
W18-3205,dey-fung-2014-hindi,0,0.0171455,"ormulated to finding the correct subject ˆs and predicate pˆ that question q refers to and which characterise the set of triples in K that contains the answer to q. Consider the example, given question ”Which city in Canada did Ian Tyson originated from?”, the Freebase subject entity m.041ftf representing the Canadian artist Ian Tyson and the relation fb:music/artist/origin, can answer it. CodeMixing and CodeSwitching Codemixing and code-switching has recently gathered much attention from researchers (Bhat et al., 2018; Rijhwani et al., 2017; Raghavi et al., 2015, 2017; Banerjee et al., 2016; Dey and Fung, 2014; Bhat et al., 2017). CM research is mostly confined towards developing parsers and other language pipeline primitives (Bhat et al., 2018, 2017). There has been some work in CM sentiment analysis (Joshi et al., 2016). Raghavi et al. (2015) demonstrate question type classification for CM questions and Raghavi et al. (2017) also demonstrate a CM factoid QA system that searches for the lexically translated CM question using Google Search on a small dataset of 100 CM questions. To the best of our knowledge, there has been no work on building an end-to-end CM QA system over a KB. 4 Our System: CMQA"
W18-3205,W16-5801,0,0.0442191,"Missing"
W18-3205,D16-1166,0,0.0678943,"Missing"
W18-3205,P17-1171,0,0.14191,"rds to English and an existing Google like-search engine to get answers, which we do not require. The rest of the paper is structured as follows: We survey related work in Section 2 and describe the task description in Section 3. We explain our system in Section 4. We describe experiments in Section 5 and provide a detailed analysis and discussion in Section 6 and conclude in Section 7. 2 Related Work Question Answering and Knowledge Bases Question answering is a well studied problem over knowledge bases (KBs) (Lukovnikov et al., 2017; Yin et al., 2016; Fader et al., 2014) and in open domain (Chen et al., 2017a; Hermann et al., 2015). Learning to rank approaches have also been applied to QA successfully (Agarwal et al., 2012; Bordes et al., 2014). Many earlier works (Ture and Jojic, 2017; Yu et al., 2017; Yin et al., 2016) which tackle SimpleQuestions divide the task into two steps: mention detection and relation http://dbpedia.org/ 40 3 Task Description prediction, whereas we jointly do both using our model. Lukovnikov et al. (2017) is more similar to our approach wherein they train a neural network in an end-to-end manner. The SimpleQuestions task presented by Bordes et al. (2015) can be defined"
W18-3205,D17-1083,0,0.0418211,"Missing"
W18-3205,W17-2617,0,0.0199526,"on the English corpora, i.e. trained using English word vectors only. When the input is say, a French sentence, they use French word vectors. Bilingual embeddings try and project both the English and French word vectors in the same semantic space, but these vectors are not perfectly aligned and might lead to errors in the networks’ prediction. We propose to obtain the average of the nearest k-english-word-vectors for the given french word and use it as the embedding for the French word. For k=1, this reduces to a bilingual lexical dictionary using bilingual embeddings (Vulic and Moens, 2015; Madhyastha and España-Bonet, 2017). Since the bilingual embeddings are not perfectly aligned, Smith et al. (2017) show4 that precision@k increases as k increases (e.g. for Hindi P@1 is 0.39, P@3 is 0.58 and P@10 is 0.63), when we obtain French (or any other language) translations for an English word. Thus, we conduct experiments with varying values of k and report the best results for the optimal k. Our experiments confirm the efficacy of KNBET. Further, we believe this KNBET can be used to improve the performance of any multilingual system that uses bilingual embeddings. 4.2.2 We use the SimpleQuestions (Bordes et al., 2015)"
W18-3205,N15-1011,0,0.0154255,"spired by the success of neural models in various image and text tasks (Vo and Hays, 2016; Das et al., 2016). Our network is a TripletSiamese Hybrid Convolutional neural network (TSHCNN), see figure 1. Vo and Hays (2016) show that classification-siamese hybrid and triplet networks work well on image similarity tasks. Our hybrid model can jointly extract and exchange information from the question and tuple inputs. Candidate Re-ranking We use Convolutional Neural Networks (CNNs) to learn the semantic representation for input text (Kim, 2014; Hu et al., 2015; Lai et al., 2015; Cho et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015). CNNs learn globally word order invariant features and at the same time pick order in short phrases. This ability of CNNs is important since different languages3 have different word orders. Retrieving a semantically similar answer to a given question can be modelled as a classification problem with a large number of classes. Here, each answer is a potential class and the number of questions per class is small (Could be zero, one or more than one. Since we match only the subject and predicate, there could be multiple questions having a common subject and predicate combinat"
W18-3205,I17-4024,0,0.0264528,"h better feature representations. We discuss this in the results section. As shown in figure 1, questions and candidate tuples are provided to our system. Our experiments vary in the input questions (English and CM variations of questions), but the candidates (tuples or answers) are always in monolingual English. Thus our final answer is always in English. 3 English is SVO whereas Hindi is free word order. SVO means Subject Verb Object. 42 4.2.1 5 Experiments K-Nearest Bilingual Embedding Transformation (KNBET) 5.1 Dataset The standard approach given bilingual (say English-French) embeddings (Plank, 2017; Da San Martino et al., 2017; Klementiev et al., 2012) has been to use the English word vector corresponding to the English word and the French word vector for the French word. Also, the network is trained only on the English corpora, i.e. trained using English word vectors only. When the input is say, a French sentence, they use French word vectors. Bilingual embeddings try and project both the English and French word vectors in the same semantic space, but these vectors are not perfectly aligned and might lead to errors in the networks’ prediction. We propose to obtain the average of the ne"
W18-3205,C16-1234,1,0.85338,"n Canada did Ian Tyson originated from?”, the Freebase subject entity m.041ftf representing the Canadian artist Ian Tyson and the relation fb:music/artist/origin, can answer it. CodeMixing and CodeSwitching Codemixing and code-switching has recently gathered much attention from researchers (Bhat et al., 2018; Rijhwani et al., 2017; Raghavi et al., 2015, 2017; Banerjee et al., 2016; Dey and Fung, 2014; Bhat et al., 2017). CM research is mostly confined towards developing parsers and other language pipeline primitives (Bhat et al., 2018, 2017). There has been some work in CM sentiment analysis (Joshi et al., 2016). Raghavi et al. (2015) demonstrate question type classification for CM questions and Raghavi et al. (2017) also demonstrate a CM factoid QA system that searches for the lexically translated CM question using Google Search on a small dataset of 100 CM questions. To the best of our knowledge, there has been no work on building an end-to-end CM QA system over a KB. 4 Our System: CMQA Bilingual Embeddings Recent work has shown that it is possible to obtain bilingual embeddings using only a minimal set of parallel lexicons (Smith et al., 2017; Artetxe et al., 2017; Ammar et al., 2016; Luong et al."
W18-3205,W16-1403,0,0.0225592,"nglish KB), any nonEnglish word in the query does not contribute Cross-lingual Question Answering Closely related is the problem of cross-lingual QA. There have been various approaches (Ahn et al., 2004; Lin and Kuo, 2010; Ren et al., 2010; Ture and Boschee, 2016) to cross-lingual QA. Some approaches (Lin and Kuo, 2010) rely on translating the entire question. Others (Ren et al., 2010), have also explored using lexical translations for this task. Recently, Ture et al. (Ture and Boschee, 2016) proposed models that combine different translation settings. There have been some efforts (Pouran Ben Veyseh, 2016; Hakimov et al., 2017; Chen et al., 2017b) to attempt cross-lingual question answering over knowledge bases. 2 41 http://lucene.apache.org/solr/ Figure 1: TSHCNN Architecture to the matching. This is a limiting factor in candidate generation for CM questions. 4.2 spired by the success of neural models in various image and text tasks (Vo and Hays, 2016; Das et al., 2016). Our network is a TripletSiamese Hybrid Convolutional neural network (TSHCNN), see figure 1. Vo and Hays (2016) show that classification-siamese hybrid and triplet networks work well on image similarity tasks. Our hybrid model"
W18-3205,P17-1147,0,0.0641711,"Missing"
W18-3205,D14-1181,0,0.00258804,"a limiting factor in candidate generation for CM questions. 4.2 spired by the success of neural models in various image and text tasks (Vo and Hays, 2016; Das et al., 2016). Our network is a TripletSiamese Hybrid Convolutional neural network (TSHCNN), see figure 1. Vo and Hays (2016) show that classification-siamese hybrid and triplet networks work well on image similarity tasks. Our hybrid model can jointly extract and exchange information from the question and tuple inputs. Candidate Re-ranking We use Convolutional Neural Networks (CNNs) to learn the semantic representation for input text (Kim, 2014; Hu et al., 2015; Lai et al., 2015; Cho et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015). CNNs learn globally word order invariant features and at the same time pick order in short phrases. This ability of CNNs is important since different languages3 have different word orders. Retrieving a semantically similar answer to a given question can be modelled as a classification problem with a large number of classes. Here, each answer is a potential class and the number of questions per class is small (Could be zero, one or more than one. Since we match only the subject and predicate, th"
W18-3205,D16-1264,0,0.151982,"Missing"
W18-3205,C12-1089,0,0.0196772,"ss this in the results section. As shown in figure 1, questions and candidate tuples are provided to our system. Our experiments vary in the input questions (English and CM variations of questions), but the candidates (tuples or answers) are always in monolingual English. Thus our final answer is always in English. 3 English is SVO whereas Hindi is free word order. SVO means Subject Verb Object. 42 4.2.1 5 Experiments K-Nearest Bilingual Embedding Transformation (KNBET) 5.1 Dataset The standard approach given bilingual (say English-French) embeddings (Plank, 2017; Da San Martino et al., 2017; Klementiev et al., 2012) has been to use the English word vector corresponding to the English word and the French word vector for the French word. Also, the network is trained only on the English corpora, i.e. trained using English word vectors only. When the input is say, a French sentence, they use French word vectors. Bilingual embeddings try and project both the English and French word vectors in the same semantic space, but these vectors are not perfectly aligned and might lead to errors in the networks’ prediction. We propose to obtain the average of the nearest k-english-word-vectors for the given french word"
W18-3205,P17-1180,0,0.0160915,"K such that oˆ is the correct answer for question q. This task can be reformulated to finding the correct subject ˆs and predicate pˆ that question q refers to and which characterise the set of triples in K that contains the answer to q. Consider the example, given question ”Which city in Canada did Ian Tyson originated from?”, the Freebase subject entity m.041ftf representing the Canadian artist Ian Tyson and the relation fb:music/artist/origin, can answer it. CodeMixing and CodeSwitching Codemixing and code-switching has recently gathered much attention from researchers (Bhat et al., 2018; Rijhwani et al., 2017; Raghavi et al., 2015, 2017; Banerjee et al., 2016; Dey and Fung, 2014; Bhat et al., 2017). CM research is mostly confined towards developing parsers and other language pipeline primitives (Bhat et al., 2018, 2017). There has been some work in CM sentiment analysis (Joshi et al., 2016). Raghavi et al. (2015) demonstrate question type classification for CM questions and Raghavi et al. (2017) also demonstrate a CM factoid QA system that searches for the lexically translated CM question using Google Search on a small dataset of 100 CM questions. To the best of our knowledge, there has been no wo"
W18-3205,W15-1521,0,0.0464099,"Missing"
W18-3205,D16-1055,0,0.023378,"e use Solr to index all our freebase tuples (FB2M) and then query for the top-k relevant candidates given the question as a query. We use BM25 as the scoring metric to rank results. Since we index freebase tuples which are in English (translating the entire KB would require a very large amount of effort and we restrict ourselves to using only the provided English KB), any nonEnglish word in the query does not contribute Cross-lingual Question Answering Closely related is the problem of cross-lingual QA. There have been various approaches (Ahn et al., 2004; Lin and Kuo, 2010; Ren et al., 2010; Ture and Boschee, 2016) to cross-lingual QA. Some approaches (Lin and Kuo, 2010) rely on translating the entire question. Others (Ren et al., 2010), have also explored using lexical translations for this task. Recently, Ture et al. (Ture and Boschee, 2016) proposed models that combine different translation settings. There have been some efforts (Pouran Ben Veyseh, 2016; Hakimov et al., 2017; Chen et al., 2017b) to attempt cross-lingual question answering over knowledge bases. 2 41 http://lucene.apache.org/solr/ Figure 1: TSHCNN Architecture to the matching. This is a limiting factor in candidate generation for CM qu"
W18-3205,D17-1307,0,0.021533,"n 2 and describe the task description in Section 3. We explain our system in Section 4. We describe experiments in Section 5 and provide a detailed analysis and discussion in Section 6 and conclude in Section 7. 2 Related Work Question Answering and Knowledge Bases Question answering is a well studied problem over knowledge bases (KBs) (Lukovnikov et al., 2017; Yin et al., 2016; Fader et al., 2014) and in open domain (Chen et al., 2017a; Hermann et al., 2015). Learning to rank approaches have also been applied to QA successfully (Agarwal et al., 2012; Bordes et al., 2014). Many earlier works (Ture and Jojic, 2017; Yu et al., 2017; Yin et al., 2016) which tackle SimpleQuestions divide the task into two steps: mention detection and relation http://dbpedia.org/ 40 3 Task Description prediction, whereas we jointly do both using our model. Lukovnikov et al. (2017) is more similar to our approach wherein they train a neural network in an end-to-end manner. The SimpleQuestions task presented by Bordes et al. (2015) can be defined as follows. Let K = {(si , pi , oi )} be a knowledge base represented as a set of tuples, where si represents a subject entity, pi a predicate (also referred as relation), and oi an"
W18-3205,P15-2118,0,0.0225636,"Missing"
W18-3205,D14-1105,0,0.138141,"Missing"
W18-3205,C16-1164,0,0.148148,"stics is challenging not just because of having multiple languages with different semantics but also because of the different word order of source language and CM, making it difficult to extract essential features from the input text. We base our work on the premise that humans can answer CM questions easily provided they understand the languages used in the question. They require no additional training in the form of CM questions to comprehend a CM question. So, one way to tackle CM questions is to translate them into a single language and use monolingual QA systems (Lukovnikov et al., 2017; Yin et al., 2016; Fader et al., 2014). Machine Translation systems perform poorly on CM sentences. The only other viable option is lexical translation (word by word translation). Lexical translation requires language identification, which Bhat et al. (2018) show to be solved. We show that our model trained on both English and Hindi can perform better on CM question directly than its lexical translation. This removes the need to obtain a large bilingual mapping of words for lexical translation. Also, such a sizeable bilingual mapping may be hard to obtain for low-resource languages. Knowledge Bases (KBs) like"
W18-3205,P17-1053,0,0.0246055,"ask description in Section 3. We explain our system in Section 4. We describe experiments in Section 5 and provide a detailed analysis and discussion in Section 6 and conclude in Section 7. 2 Related Work Question Answering and Knowledge Bases Question answering is a well studied problem over knowledge bases (KBs) (Lukovnikov et al., 2017; Yin et al., 2016; Fader et al., 2014) and in open domain (Chen et al., 2017a; Hermann et al., 2015). Learning to rank approaches have also been applied to QA successfully (Agarwal et al., 2012; Bordes et al., 2014). Many earlier works (Ture and Jojic, 2017; Yu et al., 2017; Yin et al., 2016) which tackle SimpleQuestions divide the task into two steps: mention detection and relation http://dbpedia.org/ 40 3 Task Description prediction, whereas we jointly do both using our model. Lukovnikov et al. (2017) is more similar to our approach wherein they train a neural network in an end-to-end manner. The SimpleQuestions task presented by Bordes et al. (2015) can be defined as follows. Let K = {(si , pi , oi )} be a knowledge base represented as a set of tuples, where si represents a subject entity, pi a predicate (also referred as relation), and oi an object entity. T"
W18-3205,P17-1179,0,0.0155371,"cation for CM questions and Raghavi et al. (2017) also demonstrate a CM factoid QA system that searches for the lexically translated CM question using Google Search on a small dataset of 100 CM questions. To the best of our knowledge, there has been no work on building an end-to-end CM QA system over a KB. 4 Our System: CMQA Bilingual Embeddings Recent work has shown that it is possible to obtain bilingual embeddings using only a minimal set of parallel lexicons (Smith et al., 2017; Artetxe et al., 2017; Ammar et al., 2016; Luong et al., 2015; P et al., 2014) or without any parallel lexicons (Zhang et al., 2017; Conneau et al., 2017). Our approach, can use these bilingual embeddings and supervised corpus for a resource-rich language, to enable CM applications for resourcepoor languages. In this section, we describe our system which consists of two components: (1) the Candidate Generation module for finding relevant candidates and (2) a Candidate Re-ranking model, for getting the top answer from the list of candidate answers. 4.1 Candidate Generation Any freebase tuple (specifically, the object in a tuple is the answer to the question) can be an answer to our question. We use an efficient (non-deep l"
W18-3810,N10-1133,0,0.0275766,"ted Work Automatic text summarization was first attempted in the middle of the 20th century (Luhn, 1958). Since then it has been applied to several domains and corpora, such as news articles (Lee et al., 2005), scientific articles (Teufel and Moens, 2002), and blogs (Hu et al., 2007). News articles have been the focus of summarization systems for a long time because of the vast practical applications. In fact, most datasets available today are built from news corpora. However, a comparative study has shown that a single summarization technique does not perform equally well across all domains (Ceylan et al., 2010). Therefore, separate systems have to be built to deal with the domain of fiction and its nuances, and news corpora based datasets are not sufficient to train and evaluate the same. There has been research on short fiction summarization (Kazantseva and Szpakowicz, 2010), fairy tales (Lloret and Palomar, 2009) and whole books (Mihalcea and Ceylan, 2007). But the aforementioned work in short fiction summarization had a different objective - helping a reader decide whether one would be interested in reading the complete story. Hence it contains just enough information to help the reader decide bu"
W18-3810,A00-1043,0,0.136065,"cannot be applied to the domain of fiction. Grefenstette (1998) proposed the use of sentence shortening to generate telegraphic texts that would help a blind reader skim a page (using text-to-speech). He provided eight levels of telegraphic reduction. The first (and the most drastic) generated a stream of all the proper nouns in the text. The second generated all nouns present in the subject or object position. The third, in addition, included the head verbs. The least drastic reduction generated all subjects, head verbs, objects, subclauses, prepositions and dependent noun heads. Since then Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. Intuitively it appears that sentence-shortening can allow more important information to be included in a summary. However, Lin (2003) showed that statistical sentence-shortening approaches like Knight and Marcu (2000) resulted in significantly worse content selection. He concluded that pure syntax-based compression does not improve overall summarizer performance, even though it performs well at the sentence le"
W18-3810,J10-1003,0,0.0270264,"nd blogs (Hu et al., 2007). News articles have been the focus of summarization systems for a long time because of the vast practical applications. In fact, most datasets available today are built from news corpora. However, a comparative study has shown that a single summarization technique does not perform equally well across all domains (Ceylan et al., 2010). Therefore, separate systems have to be built to deal with the domain of fiction and its nuances, and news corpora based datasets are not sufficient to train and evaluate the same. There has been research on short fiction summarization (Kazantseva and Szpakowicz, 2010), fairy tales (Lloret and Palomar, 2009) and whole books (Mihalcea and Ceylan, 2007). But the aforementioned work in short fiction summarization had a different objective - helping a reader decide whether one would be interested in reading the complete story. Hence it contains just enough information to help the reader decide but does not reveal the entire plot. However, our dataset aims to summarize the entire plot. This is useful to learn plot structures and story organization. Turney (2000) and the KEA algorithm by Witten et al. (1999) have attacked the problem of key-phrase extraction. But"
W18-3810,W03-1101,0,0.107214,"r nouns in the text. The second generated all nouns present in the subject or object position. The third, in addition, included the head verbs. The least drastic reduction generated all subjects, head verbs, objects, subclauses, prepositions and dependent noun heads. Since then Jing (2000), Riezler et al. (2003) and Knight and Marcu (2000) have explored statistical models for sentence shortening that, in addition, aim at ensuring grammaticality of the shortened sentences. Intuitively it appears that sentence-shortening can allow more important information to be included in a summary. However, Lin (2003) showed that statistical sentence-shortening approaches like Knight and Marcu (2000) resulted in significantly worse content selection. He concluded that pure syntax-based compression does not improve overall summarizer performance, even though it performs well at the sentence level. Which is why there is a need for semantically aware techniques. Our dataset provides the tools to evaluate and, in the future, maybe even train such algorithms. 3 Data Construction 3.1 Collection and Preprocessing We collected English short stories containing 300 to 1100 words, available in the public domain1 . 20"
W18-3810,D07-1040,0,0.0449856,"a long time because of the vast practical applications. In fact, most datasets available today are built from news corpora. However, a comparative study has shown that a single summarization technique does not perform equally well across all domains (Ceylan et al., 2010). Therefore, separate systems have to be built to deal with the domain of fiction and its nuances, and news corpora based datasets are not sufficient to train and evaluate the same. There has been research on short fiction summarization (Kazantseva and Szpakowicz, 2010), fairy tales (Lloret and Palomar, 2009) and whole books (Mihalcea and Ceylan, 2007). But the aforementioned work in short fiction summarization had a different objective - helping a reader decide whether one would be interested in reading the complete story. Hence it contains just enough information to help the reader decide but does not reveal the entire plot. However, our dataset aims to summarize the entire plot. This is useful to learn plot structures and story organization. Turney (2000) and the KEA algorithm by Witten et al. (1999) have attacked the problem of key-phrase extraction. But the key-phrases extracted by them do not form a cohesive summary and just try to li"
W18-3810,J02-4002,0,0.436654,"the same, annotate a gold corpus of 200 English short stories. 1 Introduction The purpose of summarization is to capture all the useful information from the source text in as few words as possible. Extractive summarization involves identifying parts of the text which are important. Such summaries are usually generated by ranking all the source sentences, according to some heuristic or metric, and then selecting the top sentences as the summary. Most extractive summarization systems have been developed for domains such as newswire articles (Lee et al., 2005), encyclopedic and scientific texts (Teufel and Moens, 2002). They work well in such domains because these texts revolve around a central theme and the information is often enforced by reiteration across several sentences. However, fictional narratives do not talk about a single topic. They describe a sequence of events and often contain dialogue. Information is not repeated and each sentence contributes to developing the plot further. Hence, selecting a subset of such sentences does not accurately capture the story. In this paper, we focus on telegraphic summarization. Telegraphic summary does not contain whole sentences, instead, shorter segments are"
W18-3817,W14-3914,0,0.041047,"Missing"
W18-3817,W14-3902,0,0.34643,"Missing"
W18-3817,W14-3908,0,0.0238321,"Missing"
W18-3817,W14-5152,0,0.276936,"Missing"
W18-3817,R15-1033,0,0.0276102,"Missing"
W18-3817,C16-1234,1,0.854663,"ased. We provide a deeper look into the code-mixed translation process and demonstrate the impact along with benchmark dataset. 3 3.1 Corpus creation Analysis We started with the dataset created and released by Gupta et al. (2016). They collected 1,446 sentences from social media, and performed language identification and word normalization on these sentences. We also obtained 771 sentences from the dataset released as part of the ICON 2017 tool contest on POStagging for code-mixed social media text, created by collection of Whatsapp chat messages. Additionally, we use the dataset released by Joshi et al. (2016) for the task of sentiment analysis of code-mixed content, which contains 3,879 code-mixed sentences. For our study, we removed annotations such as sentiment labels, POS tags, etc. from the obtained datasets, and only used raw sentences for the task of corpus creation. For the augmentation pipeline, we also make use of the language identifiers, wherever available in the dataset samples. The 6,096 code-mixed sentences contain a total of 63,913 tokens. Of these tokens, 37,673 are Hindi words and 16,182 are English words. The rest of the tokens were marked as “Rest”. “Rest” would mean that these"
W18-3817,P07-2045,0,0.00547639,"anguage. This is to ensure the maximal meaningful translation with a single call to the Em-Ma MT engine. This results in a code-mixed sentence that follows the syntax of the Matrix language and also has the majority of its words in this Matrix language. 4.4 Translation into Target Language Now that the code-mixed sentence has been translated into the matrix language, it can be directly translated into the Target language using the Ma-Tgt MT engine., 5 Evaluation and Results In order to evaluate our methods of augmentation, we consider the following existing machine translation systems: Moses (Koehn et al., 2007), Google’s Neural Machine Translation System (NMTS) (Wu et al., 2016), Bing Translator. For training a translation model for Moses, we used the English-Hindi parallel corpus released by Kunchukuttan et al. (2017). This dataset consists of 1,492,827 parallel monolingual English and monolingual Hindi sentences. The Hindi sentences were in the Devanagari script, and required pre-processing for use with our code-mixed dataset, which is entirely in Roman script. We compare the output translations of these MT systems for code-mixed data, with and without the augmentation by our system. For accuracy"
W18-3817,D13-1084,0,0.0249765,"Missing"
W18-3817,N16-1159,1,0.877247,"Missing"
W18-3817,D14-1105,0,0.268695,"Missing"
W18-4413,N16-2013,0,0.102364,"c, severe toxic etc as labels. The Kaggle data has around 150k Tweets out of which 16k are toxic, which is around twice the hate speech present in the collected data. But it has classified the degree of hatred into only two distinct classes, severe toxic and toxic, without any granularity like we present in our spectrum classification. To the best of our knowledge, we are the first ones to work on conceptualizing, identifying and classifying ontological classes of harmful speech based on understanding degree of harmful content, intent of the speaker and how it affects people on social media. (Hovy and Waseem, 2016) provided a Hate Speech dataset and the respective annotation procedure in which an initial manual search was conducted in Twitter in order to collect common slurs and terms pertaining to religious, sexual, gender, and ethnic minorities. The main researcher of the article, together with a gender studies student, manually annotated the dataset of 16,918 tweets in categories as racist, sexist and neither of the two. Another article (Chang et al., 2016) describes a dataset where messages are classified in the general class abusive language, and within the subclasses hate speech, derogatory and pr"
W18-5106,W14-3914,0,0.0810623,"that majority of the work in this domain has been done in English (Del Bosque and Garza, 2014) and a few more languages (Alfina et al.), (Mubarak et al., 2017), (Tarasova, 2016), but we know that social media abuse, bullying or aggression is independent of demography or language. With the advancement of new language keypads and social media websites supporting many new languages brings with itself the negative side of social media to those languages too. Hence, there is a need to address this problem and many others (Singh et al., 2018) for low resourced languages or say informal languages. (Bali et al., 2014) performed analysis of data from Facebook posts generated by EnglishHindi bilingual users. Analysis depicted that significant amount of code-mixing was present in the posts. (Vyas et al., 2014) formalized the problem, created a POS tag annotated Hindi-English code-mixed corpus and reported the challenges and problems in the Hindi-English code-mixed text. They also performed experiments on language identification, transliteration, normalization and POS tagging of the dataset. (Sharma et al., 2016) addressed the problem of shallow parsing of Hindi-English code-mixed social media text and develop"
W18-5106,W17-3001,0,0.0262912,"nt neural networks (RNN) are a family of neural networks that operate on sequential data. They take 46 Tag CAG NAG OAG Avg / total Precision 0.54 0.70 0.60 0.59 Recall 0.68 0.31 0.59 0.57 F1-score 0.60 0.43 0.59 0.56 Tag CAG NAG OAG avg / total Table 7: SVM Model with L2 penalty Tag CAG NAG OAG avg / total Precision 0.54 0.74 0.52 0.41 Recall 0.51 0.79 0.53 0.42 Recall 0.62 0.83 0.69 0.57 F1-score 0.63 0.83 0.69 0.58 Table 9: LSTM model F1-score 0.51 0.75 0.52 0.39 ing’), number of emoticons (emoticons and exclamation marks can be associated with more aggressive forms of online communication (Clarke and Grieve, 2017)), presence and repetition of punctuation, URLs, phone numbers, etc. The median value for URLs for “bully”, “spam”, “aggressive”, and normal users is 1, 1, 0.9, and 0.6, respectively. The maximum number of URLs between users also varies: for the bully and aggressive users it is 1.17 and 2 respectively, while for spam and normal users it is 2.38 and 1.38. Thus, normal users tend to post fewer URLs than others. Also aggressive and bully users have a propensity to use more hashtags within their tweets, as they try to disseminate their attacking message to more individuals or groups (Chatzakou et"
W18-5106,N16-1159,1,0.849599,"problem and many others (Singh et al., 2018) for low resourced languages or say informal languages. (Bali et al., 2014) performed analysis of data from Facebook posts generated by EnglishHindi bilingual users. Analysis depicted that significant amount of code-mixing was present in the posts. (Vyas et al., 2014) formalized the problem, created a POS tag annotated Hindi-English code-mixed corpus and reported the challenges and problems in the Hindi-English code-mixed text. They also performed experiments on language identification, transliteration, normalization and POS tagging of the dataset. (Sharma et al., 2016) addressed the problem of shallow parsing of Hindi-English code-mixed social media text and developed a system for Hindi-English code-mixed • News websites/organizations like NDTV, ABP News, Zee News, etc. • Web-based forums/portals like Firstost, The Logical Indian, etc. • Political Parties/groups like INC, BJP, etc. • Students’ organisations/groups like SFI, JNUSU, AISA, etc. • Support and opposition groups built around incidents in last 2 years in Indian Universities of higher education like Rohith Vemula’s suicide in HCU, February 9, 2016, incident in JNU, etc. For Twitter, the data was co"
W18-5106,W18-2405,1,0.812066,"(Razavi et al., 2010) (Watanabe et al., 2018). The first thing to observe is that majority of the work in this domain has been done in English (Del Bosque and Garza, 2014) and a few more languages (Alfina et al.), (Mubarak et al., 2017), (Tarasova, 2016), but we know that social media abuse, bullying or aggression is independent of demography or language. With the advancement of new language keypads and social media websites supporting many new languages brings with itself the negative side of social media to those languages too. Hence, there is a need to address this problem and many others (Singh et al., 2018) for low resourced languages or say informal languages. (Bali et al., 2014) performed analysis of data from Facebook posts generated by EnglishHindi bilingual users. Analysis depicted that significant amount of code-mixing was present in the posts. (Vyas et al., 2014) formalized the problem, created a POS tag annotated Hindi-English code-mixed corpus and reported the challenges and problems in the Hindi-English code-mixed text. They also performed experiments on language identification, transliteration, normalization and POS tagging of the dataset. (Sharma et al., 2016) addressed the problem o"
W18-5106,D14-1105,0,0.0633054,"hat social media abuse, bullying or aggression is independent of demography or language. With the advancement of new language keypads and social media websites supporting many new languages brings with itself the negative side of social media to those languages too. Hence, there is a need to address this problem and many others (Singh et al., 2018) for low resourced languages or say informal languages. (Bali et al., 2014) performed analysis of data from Facebook posts generated by EnglishHindi bilingual users. Analysis depicted that significant amount of code-mixing was present in the posts. (Vyas et al., 2014) formalized the problem, created a POS tag annotated Hindi-English code-mixed corpus and reported the challenges and problems in the Hindi-English code-mixed text. They also performed experiments on language identification, transliteration, normalization and POS tagging of the dataset. (Sharma et al., 2016) addressed the problem of shallow parsing of Hindi-English code-mixed social media text and developed a system for Hindi-English code-mixed • News websites/organizations like NDTV, ABP News, Zee News, etc. • Web-based forums/portals like Firstost, The Logical Indian, etc. • Political Parties"
W18-5106,N16-1030,0,0.114329,"Missing"
W18-5106,W17-3008,0,0.052377,"cted to be discussed more among the Indians (and in Hindi) for the reason of the presence of Code-Mixed text. While collecting data from Facebook more than 40 pages were identified and crawled. It included pages of the below-mentioned types: There have been several studies on computational methods to detect abusive/aggressive language published on social media in the last few years (Razavi et al., 2010) (Watanabe et al., 2018). The first thing to observe is that majority of the work in this domain has been done in English (Del Bosque and Garza, 2014) and a few more languages (Alfina et al.), (Mubarak et al., 2017), (Tarasova, 2016), but we know that social media abuse, bullying or aggression is independent of demography or language. With the advancement of new language keypads and social media websites supporting many new languages brings with itself the negative side of social media to those languages too. Hence, there is a need to address this problem and many others (Singh et al., 2018) for low resourced languages or say informal languages. (Bali et al., 2014) performed analysis of data from Facebook posts generated by EnglishHindi bilingual users. Analysis depicted that significant amount of code-m"
W18-5504,D16-1166,0,0.200115,"Missing"
W18-5504,D14-1181,0,0.00616659,"The fully connected layers also share weights. This weight sharing guarantees that the question and its relevant answer are nearer to each other in the semantics space and irrelevant answers to it are far away. It also reduces the required number of parameters to be learned. We provide additional inputs to our network which is the concatenation of both the input question and tuple. This additional input is motivated by the need to learn features for both the question and tuple. Candidate Re-ranking We use Convolutional Neural Networks (CNN) to learn the semantic representation for input text (Kim, 2014; Hu et al., 2015; Zhang et al., 2015). CNNs learn globally word order invariant features and at the same time pick the order in short phrases. Thus, CNNs are ideal for a QA task since different users may paraphrase the same question in different ways. Siamese networks have shown promising results in distance-based learning methods (Bromley et al., 1993; Chopra et al., 2005; Das et al., 2016) and they possess the capability to learn a similarity metric between questions and answers. Our candidate re-ranking module is motivated by the success of neural models in various image and text tasks (Vo"
W18-5504,P17-1171,0,0.0525979,"Missing"
W18-5504,D18-1051,0,0.212278,"Missing"
W18-5504,D17-1307,0,0.0816125,"Missing"
W18-5504,P16-1036,1,0.82681,"is additional input is motivated by the need to learn features for both the question and tuple. Candidate Re-ranking We use Convolutional Neural Networks (CNN) to learn the semantic representation for input text (Kim, 2014; Hu et al., 2015; Zhang et al., 2015). CNNs learn globally word order invariant features and at the same time pick the order in short phrases. Thus, CNNs are ideal for a QA task since different users may paraphrase the same question in different ways. Siamese networks have shown promising results in distance-based learning methods (Bromley et al., 1993; Chopra et al., 2005; Das et al., 2016) and they possess the capability to learn a similarity metric between questions and answers. Our candidate re-ranking module is motivated by the success of neural models in various image and text tasks (Vo and Hays, 2016; Das et al., 2 2.2.1 Loss Function We use the distance based logistic triplet loss (Vo and Hays, 2016), which Vo and Hays (2016) report exhibits better performance in image similarity tasks. Considering Spos / Sneg as the score obtained by the question+positive tuple / question+negative tuple, respectively and L as the logistic triplet loss, we have: L = loge (1 + e(Sneg −Spos"
W18-5504,P17-1053,0,0.0863009,"cy for English Questions Model Acc. Memory NN Bordes et al. (2015) 62.7 Attn. LSTM Golub and He (2016) 70.9 GRU Lukovnikov et al. (2017) 71.2 BiLSTM & BiGRU Mohammed et al. (2017) 3.1 Generating negative samples 74.9 In our experiments, we observe that the negative sample generation method has a significant influCNN & Attn. CNN & ence on the results. We develop a custom negative 76.4 BiLSTM-CRF Yin et al. (2016) sample generation method that generates negative samples similar to the actual answer and helps furHR-BiLSTM & CNN & 77.0 ther increase the discriminatory ability of our netBiLSTM-CRF Yu et al. (2017) work. BiLSTM-CRF & BiLSTM 78.1 We generate 10 negative samples for each trainPetrochuk and Zettlemoyer (2018) ing sample. We use the approach in Bordes et al. Candidate Generation (Ours) 68.4 (2014) to generate 5 of these 10 negative samSolr & TSHCNN (Ours) 80.0 ples. These candidates are samples picked at random and then corrupted following Bordes et al. (2014). Essentially, Given (q, t) ∈ D, Bordes Table 3: Candidate generation results: Recall of top-k et al. (2014) create a corrupted triple t´ with the folanswer candidates. lowing method: pick another random triple t´ from K 1 2 5 10 50 10"
Y18-1016,D13-1160,0,0.0303024,"ed ??? • you mean the dpkg-reconfigure command ? where is it stuck at ? if it is indeed stuck • has anybody tried connecting your phone and PC via bluetooth ? Did you get it working ? 3.2 Open domain QA datasets We use three open domain QA datasets, namely SQuAD, WikiMovies and WebQuestion to build our artificial compound question corpus. The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a reading comprehension dataset. It comprises of over 100,000 questions based on Wikipedia articles, the corresponding answer is a segment of text from the related relevant passage. (Berant et al., 2013) developed the WebQuestion dataset to answer questions from the Freebase knowledge base, by crawling questions using Google Suggest API. The answers for these questions were then obtained using Amazon Mechanical Turk. WikiMovies (Miller et al., 2016) originally created from OMDb and MovieLens databases contains 96k question-answer pairs in the movie domain. Following are few question samples from the above datasets. • Which prize did Frederick Buechner create? • who did the philippines gain independence from? 140 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong,"
Y18-1016,P17-1171,0,0.0121722,"rformance of the DrQA system when it encounters compound questions which suggests that this approach is vital for real-time human-chatbot interaction. 1 Introduction Traditional question answering systems retrieve information from a knowledge-base in accordance with what is being asked in a user utterance. Questions in these systems are queried in a single question format, such that there is only one question per utterance. However, most of these systems suffer in question-answering accuracy, especially when speakers embed multiple questions within the same utterance. QA systems like DrQA by (Chen et al., 2017) do not perform well in cases when the user utterance contains more than one question. The performance of such systems is generally suboptimal, because the answers are generated through the assumption that exactly one question is embedded Manish Shrivastava LTRC, KCIS IIIT-Hyderabad m.shrivastava@iiit.ac.in within one complete utterance. In other words, the entire utterance is processed as a single question. We propose a front end for question answering systems that detects question spans within the utterance, especially when multiple questions are compounded together by the user. We report ac"
Y18-1016,Y18-1000,0,0.264748,"Missing"
Y18-1016,N16-1030,0,0.0246945,"What decade did herbicides become common ? and how many are believed to have been uprooted by this unrest ? Tag: B-Q I-Q I-Q I-Q I-Q I-Q I-Q O B-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q 4.2 • Question: What is professional wrestling ? Tag: B-Q I-Q I-Q I-Q I-Q The Bidirectional LSTM (BiLSTM) (Graves and Schmidhuber, 2005) is capable of capturing the forward and backward dependencies in a sentence and Conditional Random Field (CRF) (Lafferty et al., 2001) models the whole sentence to generate question span prediction tags. The word embeddings are generated using the procedure explained in (Lample et al., 2016). As per their algorithm, we concatenate the last states of forward and backward pass of a character-level Bidirectional LSTM network trained over the vocabulary. This vector is further concatenated to a pre-trained GloVe (Pennington et • Question: On what day did airborne radar help intercept and destroy enemy aircraft for the first time and what will IBM use to analyze weather and make predictions ? Tag: B-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q O B-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q I-Q Table 2 gives the statistics of train, dev and test sets for datasets. Multiple"
Y18-1016,W15-4640,0,0.0202315,"l patterns within community QA data to classify questions in Yahoo! Answers dataset. These described techniques do not detect question boundary but, only classify a text as question or not. 3 Data We use four datasets, one of which is a dialog corpus and the remaining are open domain QA datasets. Ubuntu dialogue corpus is used to understand the patterns of asking multiple questions within a single utterance when in conversation with another human. We build an artificial corpus using open domain QA datasets - SQUAD, Wiki Movies and Web Questions Ubuntu Dialogue Corpus The Ubuntu Dialog Corpus (Lowe et al., 2015) is an archive of two-person conversations extracted from the Ubuntu chat log. It contains around 1 million multi-turn dialogues, which consists over 7 million utterances, composing 100 million words. We extract only those utterances which contain question marks (‘?’). We assume that question spans occur in all of these extracted utterances. Table 1 gives the total number of extracted utterances, which will be used as training data for our experiments. Here are a few instances of questions found in Ubuntu dialogue corpus. • how to acces a file with a path if i get permission denied ??? • you m"
Y18-1016,D16-1147,0,0.0157365,"namely SQuAD, WikiMovies and WebQuestion to build our artificial compound question corpus. The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a reading comprehension dataset. It comprises of over 100,000 questions based on Wikipedia articles, the corresponding answer is a segment of text from the related relevant passage. (Berant et al., 2013) developed the WebQuestion dataset to answer questions from the Freebase knowledge base, by crawling questions using Google Suggest API. The answers for these questions were then obtained using Amazon Mechanical Turk. WikiMovies (Miller et al., 2016) originally created from OMDb and MovieLens databases contains 96k question-answer pairs in the movie domain. Following are few question samples from the above datasets. • Which prize did Frederick Buechner create? • who did the philippines gain independence from? 140 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 • What movies can be described with chris noonan? 4 Approach Our approach comprises of understanding the natural question combinations that occur in the Ubuntu dialogue corpus and build a mode"
Y18-1016,D14-1162,0,0.0805359,"Missing"
Y18-1016,D16-1264,0,0.0362313,"f extracted utterances, which will be used as training data for our experiments. Here are a few instances of questions found in Ubuntu dialogue corpus. • how to acces a file with a path if i get permission denied ??? • you mean the dpkg-reconfigure command ? where is it stuck at ? if it is indeed stuck • has anybody tried connecting your phone and PC via bluetooth ? Did you get it working ? 3.2 Open domain QA datasets We use three open domain QA datasets, namely SQuAD, WikiMovies and WebQuestion to build our artificial compound question corpus. The Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) is a reading comprehension dataset. It comprises of over 100,000 questions based on Wikipedia articles, the corresponding answer is a segment of text from the related relevant passage. (Berant et al., 2013) developed the WebQuestion dataset to answer questions from the Freebase knowledge base, by crawling questions using Google Suggest API. The answers for these questions were then obtained using Amazon Mechanical Turk. WikiMovies (Miller et al., 2016) originally created from OMDb and MovieLens databases contains 96k question-answer pairs in the movie domain. Following are few question sample"
Y18-1016,J00-3003,0,0.568634,"Missing"
Y18-1016,C10-1130,0,0.00901224,") for a better response. We specifically deal with questions embedded within Ubuntu chat logs. Although there has not been an attempt to discover several questions compounded together in a single utterance, there have been two such works to identify questions within tweets. Li et al. (2011) claim theirs to be the first such work and they employ rulebased as well as support vector machines to classify tweets containing questions. Dent and Paul (2011) proposed another technique based on comprehensive linguistic parsing of tweets and then classifying them as questions. In the study conducted by (Wang and Chua, 2010) to mine syntactic and sequential patterns within community QA data to classify questions in Yahoo! Answers dataset. These described techniques do not detect question boundary but, only classify a text as question or not. 3 Data We use four datasets, one of which is a dialog corpus and the remaining are open domain QA datasets. Ubuntu dialogue corpus is used to understand the patterns of asking multiple questions within a single utterance when in conversation with another human. We build an artificial corpus using open domain QA datasets - SQUAD, Wiki Movies and Web Questions Ubuntu Dialogue C"
Y18-1017,P16-1046,0,0.0897154,"Missing"
Y18-1017,W14-4012,0,0.11606,"Missing"
Y18-1017,D15-1166,0,0.134039,"Missing"
Y18-1017,K16-1028,0,0.0580607,"Missing"
Y18-1017,N18-1158,0,0.0489893,"Missing"
Y18-1017,D14-1162,0,0.0798147,"Missing"
Y18-1017,P17-1099,0,0.0511054,"Missing"
