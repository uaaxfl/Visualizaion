2021.repl4nlp-1.10,Inductively Representing Out-of-Knowledge-Graph Entities by Optimal Estimation Under Translational Assumptions,2021,-1,-1,7,1,2470,damai dai,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"Conventional Knowledge Graph Completion (KGC) assumes that all test entities appear during training. However, in real-world scenarios, Knowledge Graphs (KG) evolve fast with out-of-knowledge-graph (OOKG) entities added frequently, and we need to efficiently represent these entities. Most existing Knowledge Graph Embedding (KGE) methods cannot represent OOKG entities without costly retraining on the whole KG. To enhance efficiency, we propose a simple and effective method that inductively represents OOKG entities by their optimal estimation under translational assumptions. Moreover, given pretrained embeddings of the in-knowledge-graph (IKG) entities, our method even needs no additional learning. Experimental results on two KGC tasks with OOKG entities show that our method outperforms the previous methods by a large margin with higher efficiency."
2021.naacl-main.437,"Decompose, Fuse and Generate: A Formation-Informed Method for {C}hinese Definition Generation",2021,-1,-1,6,0,2471,hua zheng,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word ({``}æ¡è±{''}, peach-blossom) is formed by formation components ({``}æ¡{''}, peach; {``}è±{''}, flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset, and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust."
2021.findings-acl.47,{SIRE}: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction,2021,-1,-1,3,1,7610,shuang zeng,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.227,Joint Multi-Decoder Framework with Hierarchical Pointer Network for Frame Semantic Parsing,2021,-1,-1,3,0,8062,xudong chen,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.749,Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning,2021,-1,-1,5,0.925926,10136,runxin xu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins."
2021.ccl-1.8,åºäºåç¼ç å¨çå»å­¦ææ¬ä¸­æåè¯({C}hinese word segmentation of medical text based on dual-encoder),2021,-1,-1,2,0,11692,yuan zong,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}ä¸­æåè¯æ¯èªç¶è¯­è¨å¤çé¢åçåºç¡å·¥ä½,ç¶èåäººçå»å­¦ææ¬åè¯å·¥ä½é½åªæ¯ç´æ¥å¥ç¨éç¨åè¯çæ¹æ³,èå»å­¦ææ¬å¤ä¸ç¨æ¯è¯­çç¹ç¹è®©åè¯ç³»ç»éè¦å¯¹å»å­¦ä¸ç¨æ¯è¯­åå»å­¦ææ¬ä¸­çéå»å­¦æ¯è¯­ææ¬æä¾ä¸åçåè¯ç²åº¦ãæ¬ææåºäºåç¼ç å¨å»å­¦ææ¬ä¸­æåè¯æ¨¡å,å©ç¨è¾
å©ç¼ç å¨ä¸ºå»å­¦ä¸ææ¯è¯­æä¾ç²ç²åº¦è¡¨ç¤ºãæ¨¡åå°éè¦ç²ç²åº¦åè¯çå»å­¦ä¸ç¨æ¯è¯­åéè¦éç¨åè¯ç²åº¦çææ¬åå¼,å¨æåå»å­¦ä¸ç¨æ¯è¯­çåè¯è½åçåæ¶æå¤§éåº¦å°é¿å
äºå
¶ç²ç²åº¦å¯¹äºå»å­¦ææ¬ä¸­éç¨ææ¬åè¯çå¹²æ°ã{''}"
2021.acl-long.274,Document-level Event Extraction via Heterogeneous Graph-based Interaction Model with a Tracker,2021,-1,-1,4,0.925926,10136,runxin xu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Document-level event extraction aims to recognize event information from a whole piece of article. Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model. In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges. For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions. For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events. Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1. Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document."
2020.lrec-1.846,{H}ypo{NLI}: Exploring the Artificial Patterns of Hypothesis-only Bias in Natural Language Inference,2020,2,0,3,1,2474,tianyu liu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Many recent studies have shown that for models trained on datasets for natural language inference (NLI), it is possible to make correct predictions by merely looking at the hypothesis while completely ignoring the premise. In this work, we manage to derive adversarial examples in terms of the hypothesis-only bias and explore eligible ways to mitigate such bias. Specifically, we extract various phrases from the hypotheses (artificial patterns) in the training sets, and show that they have been strong indicators to the specific labels. We then figure out {`}hard{'} and {`}easy{'} instances from the original test sets whose labels are opposite to or consistent with those indications. We also set up baselines including both pretrained models (BERT, RoBerta, XLNet) and competitive non-pretrained models (InferSent, DAM, ESIM). Apart from the benchmark and baselines, we also investigate two debiasing approaches which exploit the artificial pattern modeling to mitigate such hypothesis-only bias: down-sampling and adversarial training. We believe those methods can be treated as competitive baselines in NLI debiasing tasks."
2020.emnlp-main.32,A Spectral Method for Unsupervised Multi-Document Summarization,2020,-1,-1,2,1,20100,kexiang wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents. In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact. Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster. The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation. According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact. The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems."
2020.emnlp-main.127,Double Graph Based Reasoning for Document-level Relation Extraction,2020,-1,-1,3,1,7610,shuang zeng,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Document-level relation extraction aims to extract relations among entities within a document. Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs. In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs. GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG). The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities. Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities. Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art. Our code is available at https://github.com/PKUnlp-icler/GAIN."
2020.emnlp-main.657,Discriminatively-{T}uned {G}enerative {C}lassifiers for {R}obust {N}atural {L}anguage {I}nference,2020,-1,-1,3,0,4022,xiaoan ding,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"While discriminative neural network classifiers are generally preferred, recent work has shown advantages of generative classifiers in term of data efficiency and robustness. In this paper, we focus on natural language inference (NLI). We propose GenNLI, a generative classifier for NLI tasks, and empirically characterize its performance by comparing it to five baselines, including discriminative models and large-scale pretrained language representation models like BERT. We explore training objectives for discriminative fine-tuning of our generative classifiers, showing improvements over log loss fine-tuning from prior work (Lewis and Fan, 2019). In particular, we find strong results with a simple unbounded modification to log loss, which we call the {``}infinilog loss{''}. Our experiments show that GenNLI outperforms both discriminative and pretrained baselines across several challenging NLI experimental settings, including small training sets, imbalanced label distributions, and label noise."
2020.conll-1.48,An Empirical Study on Model-agnostic Debiasing Strategies for Robust Natural Language Inference,2020,-1,-1,4,1,2474,tianyu liu,Proceedings of the 24th Conference on Computational Natural Language Learning,0,"The prior work on natural language inference (NLI) debiasing mainly targets at one or few known biases while not necessarily making the models more robust. In this paper, we focus on the model-agnostic debiasing strategies and explore how to (or is it possible to) make the NLI models robust to multiple distinct adversarial attacks while keeping or even strengthening the models{'} generalization power. We firstly benchmark prevailing neural NLI models including pretrained ones on various adversarial datasets. We then try to combat distinct known biases by modifying a mixture of experts (MoE) ensemble method and show that it{'}s nontrivial to mitigate multiple NLI biases at the same time, and that model-level ensemble method outperforms MoE ensemble method. We also perform data augmentation including text swap, word substitution and paraphrase and prove its efficiency in combating various (though not all) adversarial attacks at the same time. Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models."
2020.coling-main.500,An Anchor-Based Automatic Evaluation Metric for Document Summarization,2020,-1,-1,3,1,20100,kexiang wang,Proceedings of the 28th International Conference on Computational Linguistics,0,"The widespread adoption of reference-based automatic evaluation metrics such as ROUGE has promoted the development of document summarization. In this paper, we consider a new protocol for designing reference-based metrics that require the endorsement of source document(s). Following protocol, we propose an anchored ROUGE metric fixing each summary particle on source document, which bases the computation on more solid ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here."
2020.ccl-1.52,é¢åå»å­¦ææ¬å¤ççå»å­¦å®ä½æ æ³¨è§è(Medical Entity Annotation Standard for Medical Text Processing),2020,-1,-1,3,0,4157,huan zhang,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"éçæºæ
§å»ççæ®å,å©ç¨èªç¶è¯­è¨å¤çææ¯è¯å«å»å­¦ä¿¡æ¯çéæ±æ¥çå¢é¿ãç®å,éå¯¹å»å­¦å®ä½èè¨,å»å­¦å
±äº«è¯­æåºä»å¤äºç©ºç½ç¶æ,è¿å¯¹å»å­¦ææ¬ä¿¡æ¯å¤çåé¡¹ä»»å¡çè¿å±é æäºå·¨å¤§é»åãå¦ä½å¤æ­ä¸åçå»å­¦å®ä½ç±»å«?å¦ä½çå®ä¸åå®ä½é´çæ¶µçèå´?è¿äºé®é¢å¯¼è´ç¼ºä¹ç±»ä¼¼éç¨åºæ¯çå¤§è§æ¨¡è§èæ æ³¨çå»å­¦ææ¬æ°æ®ãéå¯¹ä¸è¿°é®é¢,è¯¥æåèäºUMLSä¸­å®ä¹çè¯­ä¹ç±»å,æåºé¢åå»å­¦ææ¬ä¿¡æ¯å¤ççå»å­¦å®ä½æ æ³¨è§è,æ¶µçäºç¾ç
ãä¸´åºè¡¨ç°ãå»çç¨åºãå»çè®¾å¤ç­9ç§å»å­¦å®ä½,ä»¥ååºäºè§èæå»ºå»å­¦å®ä½æ æ³¨è¯­æåºãè¯¥æç»¼è¿°äºæ æ³¨è§èçæè¿°ä½ç³»ãåç±»ååãæ··æ·å¤çãè¯­ææ æ³¨è¿ç¨ä»¥åå»å­¦å®ä½èªå¨æ æ³¨åºçº¿å®éªç­ç¸å
³é®é¢,å¸æè½ä¸ºå»å­¦å®ä½è¯­æåºçæå»ºæä¾å¯åèçæ æ³¨è§è,ä»¥åä¸ºå»å­¦å®ä½è¯å«æä¾è¯­ææ¯æã"
W19-1302,A Soft Label Strategy for Target-Level Sentiment Classification,2019,0,0,4,0,3724,da yin,"Proceedings of the Tenth Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"In this paper, we propose a soft label approach to target-level sentiment classification task, in which a history-based soft labeling model is proposed to measure the possibility of a context word as an opinion word. We also apply a convolution layer to extract local active features, and introduce positional weights to take relative distance information into consideration. In addition, we obtain more informative target representation by training with context tokens together to make deeper interaction between target and context tokens. We conduct experiments on SemEval 2014 datasets and the experimental results show that our approach significantly outperforms previous models and gives state-of-the-art results on these datasets."
P19-1194,Towards Fine-grained Text Sentiment Transfer,2019,0,3,6,1,2472,fuli luo,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we focus on the task of fine-grained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from the conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation."
P19-1600,Towards Comprehensive Description Generation from Factual Attribute-value Tables,2019,0,2,5,1,2474,tianyu liu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"The comprehensive descriptions for factual attribute-value tables, which should be accurate, informative and loyal, can be very helpful for end users to understand the structured data in this form. However previous neural generators might suffer from key attributes missing, less informative and groundless information problems, which impede the generation of high-quality comprehensive descriptions for tables. To relieve these problems, we first propose force attention (FA) method to encourage the generator to pay more attention to the uncovered attributes to avoid potential key attributes missing. Furthermore, we propose reinforcement learning for information richness to generate more informative as well as more loyal descriptions for tables. In our experiments, we utilize the widely used WIKIBIO dataset as a benchmark. Besides, we create WB-filter based on WIKIBIO to test our model in the simulated user-oriented scenarios, in which the generated descriptions should accord with particular user interests. Experimental results show that our model outperforms the state-of-the-art baselines on both automatic and human evaluation."
P19-1603,Learning to Control the Fine-grained Sentiment for Story Ending Generation,2019,0,3,5,1,2472,fuli luo,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Automatic story ending generation is an interesting and challenging task in natural language generation. Previous studies are mainly limited to generate coherent, reasonable and diversified story endings, and few works focus on controlling the sentiment of story endings. This paper focuses on generating a story ending which meets the given fine-grained sentiment intensity. There are two major challenges to this task. First is the lack of story corpus which has fine-grained sentiment labels. Second is the difficulty of explicitly controlling sentiment intensity when generating endings. Therefore, we propose a generic and novel framework which consists of a sentiment analyzer and a sentimental generator, respectively addressing the two challenges. The sentiment analyzer adopts a series of methods to acquire sentiment intensities of the story dataset. The sentimental generator introduces the sentiment intensity into decoder via a Gaussian Kernel Layer to control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better."
D19-1336,Pun-{GAN}: Generative Adversarial Network for Pun Generation,2019,0,1,5,1,2472,fuli luo,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"In this paper, we focus on the task of generating a pun sentence given a pair of word senses. A major challenge for pun generation is the lack of large-scale pun corpus to guide supervised learning. To remedy this, we propose an adversarial generative network for pun generation (Pun-GAN). It consists of a generator to produce pun sentences, and a discriminator to distinguish between the generated pun sentences and the real sentences with specific word senses. The output of the discriminator is then used as a reward to train the generator via reinforcement learning, encouraging it to produce pun sentences which can support two word senses simultaneously. Experiments show that the proposed Pun-GAN can generate sentences that are more ambiguous and diverse in both automatic and human evaluation."
P18-1230,Incorporating Glosses into Neural Word Sense Disambiguation,2018,19,5,4,1,2472,fuli luo,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets."
L18-1079,{E}vent{W}iki: A Knowledge Base of Major Events,2018,-1,-1,3,1,3790,tao ge,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1170,Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention,2018,0,10,6,1,2472,fuli luo,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"The goal of Word Sense Disambiguation (WSD) is to identify the correct meaning of a word in the particular context. Traditional supervised methods only use labeled data (context), while missing rich lexical knowledge such as the gloss which defines the meaning of a word sense. Recent studies have shown that incorporating glosses into neural networks for WSD has made significant improvement. However, the previous models usually build the context representation and gloss representation separately. In this paper, we find that the learning for the context and gloss representation can benefit from each other. Gloss can help to highlight the important words in the context, thus building a better context representation. Context can also help to locate the key words in the gloss of the correct word sense. Therefore, we introduce a co-attention mechanism to generate co-dependent representations for the context and gloss. Furthermore, in order to capture both word-level and sentence-level information, we extend the attention mechanism in a hierarchical fashion. Experimental results show that our model achieves the state-of-the-art results on several standard English all-words WSD test datasets."
D18-1271,Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition,2018,0,1,5,1,3790,tao ge,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes to study fine-grained coordinated cross-lingual text stream alignment through a novel information network decipherment paradigm. We use Burst Information Networks as media to represent text streams and present a simple yet effective network decipherment algorithm with diverse clues to decipher the networks for accurate text stream alignment. Experiments on Chinese-English news streams show our approach not only outperforms previous approaches on bilingual lexicon extraction from coordinated text streams but also can harvest high-quality alignments from large amounts of streaming data for endless language knowledge mining, which makes it promising to be a new paradigm for automatic language knowledge acquisition."
D18-1311,Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data,2018,0,9,2,1,4078,wenhui wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Pre-trained word embeddings and language model have been shown useful in a lot of tasks. However, both of them cannot directly capture word connections in a sentence, which is important for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly capture word connections from unlabeled data by a word ordering model with self-attention mechanism. Experiments show that these implicit word connections do improve our parsing model. Furthermore, by combining with a pre-trained language model, our model gets state-of-the-art performance on the English PTB dataset, achieving 96.35{\%} UAS and 95.25{\%} LAS."
W17-4305,Syntax Aware {LSTM} model for Semantic Role Labeling,2017,17,4,3,0,31688,feng qian,Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing,0,"In Semantic Role Labeling (SRL) task, the tree structured dependency relation is rich in syntax information, but it is not well handled by existing models. In this paper, we propose Syntax Aware Long Short Time Memory (SA-LSTM). The structure of SA-LSTM changes according to dependency structure of each sentence, so that SA-LSTM can model the whole tree structure of dependency relation in an architecture engineering way. Experiments demonstrate that on Chinese Proposition Bank (CPB) 1.0, SA-LSTM improves F1 by 2.06{\%} than ordinary bi-LSTM with feature engineered dependency relation information, and gives state-of-the-art F1 of 79.92{\%}. On English CoNLL 2005 dataset, SA-LSTM brings improvement (2.1{\%}) to bi-LSTM model and also brings slight improvement (0.3{\%}) when added to the state-of-the-art model."
P17-1018,Gated Self-Matching Networks for Reading Comprehension and Question Answering,2017,22,249,4,1,4078,wenhui wang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we present the gated self-matching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3{\%} on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9{\%}. At the time of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model."
P17-1189,A Progressive Learning Approach to {C}hinese {SRL} Using Heterogeneous Data,2017,15,1,3,0,26846,qiaolin xia,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Previous studies on Chinese semantic role labeling (SRL) have concentrated on a single semantically annotated corpus. But the training data of single corpus is often limited. Whereas the other existing semantically annotated corpora for Chinese SRL are scattered across different annotation frameworks. But still, Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In this paper, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that our model outperforms state-of-the-art methods."
D17-1020,Affinity-Preserving Random Walk for Multi-Document Summarization,2017,0,0,4,1,20100,kexiang wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Multi-document summarization provides users with a short text that summarizes the information in a set of related documents. This paper introduces affinity-preserving random walk to the summarization task, which preserves the affinity relations of sentences by an absorbing random walk model. Meanwhile, we put forward adjustable affinity-preserving random walk to enforce the diversity constraint of summarization in the random walk process. The ROUGE evaluations on DUC 2003 topic-focused summarization task and DUC 2004 generic summarization task show the good performance of our method, which has the best ROUGE-2 recall among the graph-based ranking methods."
D17-1189,A Soft-label Method for Noise-tolerant Distantly Supervised Relation Extraction,2017,9,39,3,1,2474,tianyu liu,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Distant-supervised relation extraction inevitably suffers from wrong labeling problems because it heuristically labels relational facts with knowledge bases. Previous sentence level denoise models don{'}t achieve satisfying performances because they use hard labels which are determined by distant supervision and immutable during training. To this end, we introduce an entity-pair level denoise method which exploits semantic information from correctly labeled entity pairs to correct wrong labels dynamically during training. We propose a joint score function which combines the relational scores based on the entity-pair representation and the confidence of the hard label to obtain a new label, namely a soft label, for certain entity pair. During training, soft labels instead of hard labels serve as gold labels. Experiments on the benchmark dataset show that our method dramatically reduces noisy instances and outperforms other state-of-the-art systems."
P16-1116,{RBPB}: Regularization-Based Pattern Balancing Method for Event Extraction,2016,32,5,5,1,7754,lei sha,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Event extraction is a particularly challenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classification), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization-Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents."
P16-1218,Graph-based Dependency Parsing with Bidirectional {LSTM},2016,25,69,2,1,4078,wenhui wang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we propose a neural network model for graph-based dependency parsing which utilizes Bidirectional LSTM (BLSTM) to capture richer contextual information instead of using high-order factorization, and enable our model to use much fewer features than previous work. In addition, we propose an effective way to learn sentence segment embedding on sentence-level based on an extra forward LSTM network. Although our model uses only first-order factorization, experiments on English Peen Treebank and Chinese Penn Treebank show that our model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models."
N16-1049,Joint Learning Templates and Slots for Event Schema Induction,2016,1,0,3,1,7754,lei sha,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work."
D16-1035,Discourse Parsing with Attention-based Hierarchical Neural Networks,2016,19,23,3,0,3420,qi li,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1075,News Stream Summarization using Burst Information Networks,2016,33,5,3,1,3790,tao ge,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"This paper studies summarizing key information from news streams. We propose simple yet effective models to solve the problem based on a novel and promising representation of text streams xe2x80x90 Burst Information Networks (BINets). A BINet can be aware of redundant information, allows global analysis of a text stream, and can be efficiently built and dynamically updated, which perfectly fits the demands of text stream summarization. Extensive experiments show that the BINet-based approaches are not only efficient and can be used in a real-time online summarization setting, but also can generate high-quality summaries, outperforming the state-of-the-art approach."
D16-1212,Capturing Argument Relationship for {C}hinese Semantic Role Labeling,2016,8,5,3,1,7754,lei sha,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1260,Encoding Temporal Information for Time-Aware Link Prediction,2016,20,18,6,1,30123,tingsong jiang,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1161,Towards Time-Aware Knowledge Graph Completion,2016,27,15,5,1,30123,tingsong jiang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Knowledge graph (KG) completion adds new facts to a KG by making inferences from existing facts. Most existing methods ignore the time information and only learn from time-unknown fact triples. In dynamic environments that evolve over time, it is important and challenging for knowledge graph completion models to take into account the temporal aspects of facts. In this paper, we present a novel time-aware knowledge graph completion model that is able to predict links in a KG using both the existing facts and the temporal information of the facts. To incorporate the happening time of facts, we propose a time-aware KG embedding model using temporal order information among facts. To incorporate the valid time of facts, we propose a joint time-aware inference model based on Integer Linear Programming (ILP) using temporal consistencyinformationasconstraints. Wefurtherintegratetwomodelstomakefulluseofglobal temporal information. We empirically evaluate our models on time-aware KG completion task. Experimental results show that our time-aware models achieve the state-of-the-art on temporal facts consistently."
C16-1270,Reading and Thinking: Re-read {LSTM} Unit for Textual Entailment Recognition,2016,16,32,2,1,7754,lei sha,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences (premise and hypothesis) and send them together into a multi-layer perceptron to get their entailment type, or use LSTM-RNN to link two sentences together while using attention mechanic to enhance the model{'}s ability. In this paper, we propose to use the re-read mechanic, which means to read the premise again and again while reading the hypothesis. After read the premise again, the model can get a better understanding of the premise, which can also affect the understanding of the hypothesis. On the contrary, a better understanding of the hypothesis can also affect the understanding of the premise. With the alternative re-read process, the model can {``}think{''} of a better decision of entailment type. We designed a new LSTM unit called re-read LSTM (rLSTM) to implement this {``}thinking{''} process. Experiments show that we achieve results better than current state-of-the-art equivalents."
C16-1309,Event Detection with Burst Information Networks,2016,19,6,3,1,3790,tao ge,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Retrospective event detection is an important task for discovering previously unidentified events in a text stream. In this paper, we propose two fast centroid-aware event detection models based on a novel text stream representation {--} Burst Information Networks (BINets) for addressing the challenge. The BINets are time-aware, efficient and can be easily analyzed for identifying key information (centroids). These advantages allow the BINet-based approaches to achieve the state-of-the-art performance on multiple datasets, demonstrating the efficacy of BINets for the task of event detection."
P15-2110,One Tense per Scene: Predicting Tense in {C}hinese Conversations,2015,11,0,3,1,3790,tao ge,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We study the problem of predicting tense in Chinese conversations. The unique challenges include: (1) Chinese verbs do not have explicit lexical or grammatical forms to indicate tense; (2) Tense information is often implicitly hidden outside of the target sentence. To tackle these challenges, we first propose a set of novel sentence-level (local) features using rich linguistic resources and then propose a new hypothesis of xe2x80x9cOne tense per scenexe2x80x9d to incorporate scene-level (global) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark."
P15-1031,An Effective Neural Network Model for Graph-based Dependency Parsing,2015,18,34,3,1,37483,wenzhe pei,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Most existing graph-based parsing models rely on millions of hand-crafted features, which limits their generalization ability and slows down the parsing speed. In this paper, we propose a general and effective Neural Network model for graph-based dependency parsing. Our model can automatically learn high-order feature combinations using only atomic features by exploiting a novel activation function tanhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers."
P15-1056,Bring you to the past: Automatic Generation of Topically Relevant Event Chronicles,2015,32,13,5,1,3790,tao ge,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"An event chronicle provides people with an easy and fast access to learn the past. In this paper, we propose the first novel approach to automatically generate a topically relevant event chronicle during a certain period given a reference chronicle during another period. Our approach consists of two core components xe2x80x93 a timeaware hierarchical Bayesian model for event detection, and a learning-to-rank model to select the salient events to construct the final chronicle. Experimental results demonstrate our approach is promising to tackle this new problem."
D15-1099,Multi-label Text Categorization with Joint Learning Predictions-as-Features Method,2015,13,11,4,0,5316,li li,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Multi-label text categorization is a type of text categorization, where each document is assigned to one or more categories. Recently, a series of methods have been developed, which train a classifier for each label, organize the classifiers in a partially ordered structure and take predictions produced by the former classifiers as the latter classifiersxe2x80x99 features. These predictions-asfeatures style methods model high order label dependencies and obtain high performance. Nevertheless, the predictionsas-features methods suffer a drawback. When training a classifier for one label, the predictions-as-features methods can model dependencies between former labels and the current label, but they canxe2x80x99t model dependencies between the current label and the latter labels. To address this problem, we propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. We conduct experiments using real-world textual data sets, and these experiments illustrate the predictions-as-features models trained by our algorithm outperform the original models."
D15-1185,Recognizing Textual Entailment Using Probabilistic Inference,2015,18,4,3,1,7754,lei sha,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Recognizing Text Entailment (RTE) plays an important role in NLP applications including question answering, information retrieval, etc. In recent work, some research explore xe2x80x9cdeepxe2x80x9d expressions such as discourse commitments or strict logic for representing the text. However, these expressions suffer from the limitation of inference inconvenience or translation loss. To overcome the limitations, in this paper, we propose to use the predicate-argument structures to represent the discourse commitments extracted from text. At the same time, with the help of the YAGO knowledge, we borrow the distant supervision technique to mine the implicit facts from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the confidence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work."
D15-1186,{C}hinese Semantic Role Labeling with Bidirectional Recurrent Neural Networks,2015,20,21,3,0,1699,zhen wang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Traditional approaches to Chinese Semantic Role Labeling (SRL) almost heavily rely on feature engineering. Even worse, the long-range dependencies in a sentence can hardly be modeled by these methods. In this paper, we introduce bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to capture bidirectional and long-range dependencies in a sentence with minimal feature engineering. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance."
D15-1289,{ERSOM}: A Structural Ontology Matching Approach Using Automatically Learned Entity Representation,2015,39,9,3,0,37854,chuncheng xiang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"As a key representation model of knowledge, ontology has been widely used in a lot of NLP related tasks, such as semantic parsing, information extraction and text mining etc. In this paper, we study the task of ontology matching, which concentrates on finding semantically related entities between different ontologies that describe the same domain, to solve the semantic heterogeneity problem. Previous works exploit different kinds of descriptions of an entity in ontology directly and separately to find the correspondences without considering the higher level correlations between the descriptions. Besides, the structural information of ontology havenxe2x80x99t been utilized adequately for ontology matching. We propose in this paper an ontology matching approach, named ERSOM, which mainly includes an unsupervised representation learning method based on the deep neural networks to learn the general representation of the entities and an iterative similarity propagation method that takes advantage of more abundant structure information of the ontology to discover more mappings. The experimental results on the datasets from Ontology Alignment Evaluation Initiative (OAEI1) show that ERSOM achieves a competitive performance compared to the state-of-the-art ontology matching systems. The OAEI is an international initiative organizing annual campaigns for evaluating ontology matching systems. All of the ontologies provided by OAEI are described in OWL-DL language, and like most of the other participates our ERSOM also manages the OWL ontology in its current version. OAEI: http://oaei.ontologymatching.org/"
S14-2102,{SSMT}:A Machine Translation Evaluation View To Paragraph-to-Sentence Semantic Similarity,2014,10,2,2,0,25742,pingping huang,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper presents the system SSMT measuring the semantic similarity between a paragraph and a sentence submitted to the SemEval 2014 task3: Cross-level Semantic Similarity. The special difficulty of this task is the length disparity between the two semantic comparison texts. We adapt several machine translation evaluation metrics for features to cope with this difficulty, then train a regression model for the semantic similarity prediction. This system is straightforward in intuition and easy in implementation. Our best run gets 0.808 in Pearson correlation. METEORderived features are the most effective ones in our experiment."
P14-1028,Max-Margin Tensor Neural Network for {C}hinese Word Segmentation,2014,33,108,3,1,37483,wenzhe pei,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensorbased transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Furthermore, a new tensor factorization approach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks."
D14-1092,A Joint Model for Unsupervised {C}hinese Word Segmentation,2014,20,7,2,0,40119,miaohong chen,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In this paper, we propose a joint model for unsupervised Chinese word segmentation (CWS). Inspired by the xe2x80x9cproducts of expertsxe2x80x9d idea, our joint model firstly combines two generative models, which are word-based hierarchical Dirichlet process model and character-based hidden Markov model, by simply multiplying their probabilities together. Gibbs sampling is used for model inference. In order to further combine the strength of goodness-based model, we then integrated nVBE into our joint model by using it to initializing the Gibbs sampler. We conduct our experiments on PKU and MSRA datasets provided by the second SIGHAN bakeoff. Test results on these two datasets show that the joint model achieves much better results than all of its component models. Statistical significance tests also show that it is significantly better than stateof-the-art systems, achieving the highest F-scores. Finally, analysis indicates that compared with nVBE and HDP, the joint model has a stronger ability to solve both combinational and overlapping ambiguities in Chinese word segmentation."
C14-1035,Inducing Word Sense with Automatically Learned Hidden Concepts,2014,23,5,1,1,2476,baobao chang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Word Sense Induction (WSI) aims to automatically induce meanings of a polysemous word from unlabeled corpora. In this paper, we first propose a novel Bayesian parametric model to WSI. Unlike previous work, our research introduces a layer of hidden concepts and view senses as mixtures of concepts. We believe that concepts generalize the contexts, allowing the model to measure the sense similarity at a more general level. The Zipfxe2x80x99s law of meaning is used as a way of pre-setting the sense number for the parametric model. We further extend the parametric model to non-parametric model which not only simplifies the problem of model selection but also brings improved performance. We test our model on the benchmark datasets released by Semeval-2010 and Semeval-2007. The test results show that our model outperforms state-of-theart systems."
W13-4413,A Maximum Entropy Approach to {C}hinese Spelling Check,2013,7,9,2,0,40681,dongxu han,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"Spelling check identifies incorrect writing words in documents. For the reason of input methods, Chinese spelling check is much different from English and it is still a challenging work. For the past decade years, most of the methods in detecting errors in documents are lexicon-based or probability-based, and much progress are made. In this paper, we propose a new method in Chinese spelling check by using maximum entropy (ME). Experiment shows that by importing a large raw corpus, maximum entropy can build a well-trained model to detect spelling errors in Chinese documents."
I13-1181,Feature-based Neural Language Model and {C}hinese Word Segmentation,2013,10,15,3,0,30595,mairgup mansur,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper we introduce a feature-based neural language model, which is trained to estimate the probability of an element given its previous context features. In this way our feature-based language model can learn representation for more sophisticated features. We introduced the deep neural architecture into the Chinese Word Segmentation task. We got a significant improvement on segmenting performance by sharing the pre-learned representation of character features. The experimental result shows that, while using the same feature sets, our neural segmentation model has a better segmenting performance than CRF-based segmentation model."
D13-1001,Event-Based Time Label Propagation for Automatic Dating of News Articles,2013,12,9,2,1,3790,tao ge,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small."
C12-1098,Update Summarization using a Multi-level Hierarchical {D}irichlet Process Model,2012,31,14,5,0,6713,jiwei li,Proceedings of {COLING} 2012,0,"Update summarization is a new challenge which combines salience ranking with novelty detection. Previous researches usually convert novelty detection to the problem of redundancy removal or salience re-ranking, and seldom explore the birth, splitting, merging and death of aspects for a given topic. In this paper, we borrow the idea of evolutionary clustering and propose a three-level HDP model named h-uHDP, which reveals the diversity and commonality between aspects discovered from two different epochs (i.e. epoch history and epoch update). Specifically, we strengthen modeling the sentence level in the h-uHDP model to adapt to the sentence extraction based framework. Automatic and manual evaluations on TAC data demonstrate the effectiveness of our update summarization algorithm, especially from the novelty criterion."
W10-4136,{C}hinese word segmentation model using bootstrapping,2010,1,0,1,1,2476,baobao chang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
D10-1077,Enhancing Domain Portability of {C}hinese Segmentation Model Using Chi-Square Statistics and Bootstrapping,2010,9,8,1,1,2476,baobao chang,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Almost all Chinese language processing tasks involve word segmentation of the language input as their first steps, thus robust and reliable segmentation techniques are always required to make sure those tasks well-performed. In recent years, machine learning and sequence labeling models such as Conditional Random Fields (CRFs) are often used in segmenting Chinese texts. Compared with traditional lexicon-driven models, machine learned models achieve higher F-measure scores. But machine learned models heavily depend on training materials. Although they can effectively process texts from the same domain as the training texts, they perform relatively poorly when texts from new domains are to be processed. In this paper, we propose to use X2 statistics when training an SVM-HMM based segmentation model to improve its ability to recall OOV words and then use bootstrapping strategies to maintain its ability to recall IV words. Experiments show the approach proposed in this paper enhances the domain portability of the Chinese word segmentation model and prevents drastic decline in performance when processing texts across domains."
D08-1034,Improving {C}hinese Semantic Role Classification with Hierarchical Feature Selection Strategy,2008,23,15,2,0,48691,weiwei ding,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"In recent years, with the development of Chinese semantically annotated corpus, such as Chinese Proposition Bank and Normalization Bank, the Chinese semantic role labeling (SRL) task has been boosted. Similar to English, the Chinese SRL can be divided into two tasks: semantic role identification (SRI) and classification (SRC). Many features were introduced into these tasks and promising results were achieved. In this paper, we mainly focus on the second task: SRC. After exploiting the linguistic discrepancy between numbered arguments and ARGMs, we built a semantic role classifier based on a hierarchical feature selection strategy. Different from the previous SRC systems, we divided SRC into three sub tasks in sequence and trained models for each sub task. Under the hierarchical architecture, each argument should first be determined whether it is a numbered argument or an ARGM, and then be classified into fine-gained categories. Finally, we integrated the idea of exploiting argument interdependence into our system and further improved the performance. With the novel method, the classification precision of our system is 94.68%, which outperforms the strong baseline significantly. It is also the state-of-the-art on Chinese SRC."
I05-1088,Extracting Terminologically Relevant Collocations in the Translation of {C}hinese Monograph,2005,9,1,2,0,51078,byeongkwu kang,Second International Joint Conference on Natural Language Processing: Full Papers,0,"This paper suggests a methodology which is aimed to extract the terminologically relevant collocations for translation purposes. Our basic idea is to use a hybrid method which combines the statistical method and linguistic rules. The extraction system used in our work operated at three steps: (1) Tokenization and POS tagging of the corpus; (2) Extraction of multi-word units using statistical measure; (3) Linguistic filtering to make use of syntactic patterns and stop-word list. As a result, hybrid method using linguistic filters proved to be a suitable method for selecting terminological collocations, it has considerably improved the precision of the extraction which is much higher than that of purely statistical method. In our test, hybrid method combining xe2x80x9cLog-likelihood ratioxe2x80x9d and xe2x80x9clinguistic rulesxe2x80x9d had the best performance in the extraction. We believe that terminological collocations and phrases extracted in this way, could be used effectively either to supplement existing terminological collections or to be used in addition to traditional reference works."
Y04-1030,{C}hinese-{E}nglish Parallel Corpus Construction and its Application,2004,2,6,1,1,2476,baobao chang,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"Chinese-English parallel corpora are key resources for Chinese-English cross-language information processing, Chinese-English bilingual lexicography, Chinese-English language research and teaching. But so far large-scale Chinese-English corpus is still unavailable yet, given the difficulties and the intensive labours required. In this paper, our work towards building a large-scale Chinese-English parallel corpus is presented. We elaborate on the collection, annotation and mark-up of the parallel Chinese-English texts and the workflow that we used to construct the corpus. In addition, we also present our work toward building tools for constructing and using the corpus easily for different purposes. Among these tools, a parallel concordance tool developed by us is examined in detail. Several applications of the corpus being conducted are also introduced briefly in the paper."
W03-1722,{C}hinese Word Segmentation at Peking University,2003,1,10,3,0,38116,huiming duan,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Word segmentation is the first step in Chinese information processing, and the performance of the segmenter, therefore, has a direct and great influence on the processing steps that follow. Different segmenters will give different results when handling issues like word boundary. And we will present in this paper that there is no need for an absolute definition of word boundary for all segmenters, and that different results of segmentation shall be acceptable if they can help to reach a correct syntactic analysis in the end."
W02-1801,Extraction of Translation Unit from {C}hinese-{E}nglish Parallel Corpora,2002,3,11,1,1,2476,baobao chang,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,More and more researchers have recognized the potential value of the parallel corpus in the research on Machine Translation and Machine Aided Translation. This paper examines how Chinese English translation units could be extracted from parallel corpus. An iterative algorithm based on degree of word association is proposed to identify the multiword units for Chinese and English. Then the Chinese-English Translation Equivalent Pairs are extracted from the parallel corpus. We also made comparison between different statistical association measurement in this paper.
