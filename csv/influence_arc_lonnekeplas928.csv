2020.lrec-1.626,W18-5102,0,0.0258496,"Missing"
2020.lrec-1.626,N13-1037,0,0.0155689,"or one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed scheme in yielding better interannotator agreement results is established more concretely. In this regard, a line of future work, briefly discussed in Section 3., is to further validate the multi-level annotation schemes. Specifically, reliability needs to be estimated on the basis of larger and more diverse samples. Furthermore, we have already identified the question of whether, having conducted a micro-analysis of user-gene"
2020.lrec-1.626,C12-2029,0,0.0160199,"comments have to be annotated to find a considerable number of hate speech instances” (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data. Clearly, this does not mean that we will not have additional challenges to face before we end up with a dataset that could potentially be used for training and testing pur5094 poses. For one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed"
2020.lrec-1.626,W14-3901,0,0.0131572,"iderable number of hate speech instances” (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data. Clearly, this does not mean that we will not have additional challenges to face before we end up with a dataset that could potentially be used for training and testing pur5094 poses. For one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed scheme in yielding better interannotator ag"
2020.lrec-1.626,gao-huang-2017-detecting,0,0.0132488,"tions, “overt prejudicial bias has been transformed into subtle and increasingly covert expressions” (Leets, 2003, p.146). Therefore, while clearly indicative of a negative attitude, the use of derogatory terms, as in (9), cannot account for all instances of hate speech: (8) [username] sure NO! The majority of Maltese people are against ” ILLEGALS”! Do not mix racism with illegals please! (9) That’s because we’re not simply importing destitute people. We’re importing a discredited, disheveled and destructive culture. 5091 In this respect, it is often acknowledged (Warner and Hirschberg, 2012; Gao and Huang, 2017; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; Sanguinetti et al., 2018; Watanabe et al., 2018) that discourse context plays a crucial role in evaluating remarks, as there are several indirect strategies for expressing discriminatory hatred, which our in-depth analysis enabled us to additionally unearth. The most pertinent example of this is the use of metaphor, which has for long been emphasised as a popular strategy for communicating discrimination, since it typically involves “mappings from a conceptual ‘source domain’ to a ‘target domain’ with resulting conceptual ‘blends’ that help"
2020.lrec-1.626,W19-3512,0,0.0119109,"parable annotation tasks. Discussing this issue within the more general context of detecting abusive language, Waseem et al. (2017) identify two sources for the resulting annotation confusion: (a) the existence of abusive language directed towards some generalised outgroup, as opposed to a specific individual; and (b) the implicitness with which an abusive attitude can often be communicated. Despite targeting abusive language in general, the resulting two-fold typology has been taken to apply to the more particular discussion of hate speech too (ElSherief et al., 2018; MacAvaney et al., 2019; Mulki et al., 2019; Rizos et al., 2019), rendering the lack of an explicit insult/threat towards an individual target in terms of their membership to a protected group more difficult to classify as hate speech than a remark that explicitly incites to discriminatory hatred. This much seems to be further corroborated by a recent study by Salminen et al. (2019) which revealed that, when evaluating the hatefulness of online comments on a scale of 1 (not hateful at all) to 4 (very hateful), annotators agree more on the two extremes than in the middle ground. Quite justifiably, this suggests that it is easier to clas"
2020.lrec-1.626,W14-5904,0,0.0119121,"tated to find a considerable number of hate speech instances” (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data. Clearly, this does not mean that we will not have additional challenges to face before we end up with a dataset that could potentially be used for training and testing pur5094 poses. For one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed scheme in yielding"
2020.lrec-1.626,L18-1443,0,0.158396,"ot hate speech, or neither offensive nor hate speech. Along similar lines, Zampieri et al. (2019) introduce an explicitly hierarchical annotation scheme that requested annotators to code tweets on three consecutive levels: (a) on whether they contain offensive language; (b) on whether the insult/threat is targeted to some individual or group; and (c) on whether the target is an individual, a group or another type of entity (e.g. an organization or event). Finally, an annotation framework which, like the one proposed here, takes into account the intricate nature of hate speech was developed by Sanguinetti et al. (2018). Here, annotators were asked to not only provide a binary classification of Italian tweets as hate speech or not, but also to grade their intensity on a scale from 0 to 4, and indicate whether each tweet contains ironical statements or stereotypical representations as well as how it fares in terms of aggressiveness and offensiveness. Despite the apparently increasing interest in the area and the development of all the more sophisticated annotation methodologies, a major cause for concern when it comes to annotations used for model training and testing is that reliability scores are consistent"
2020.lrec-1.626,W17-1101,0,0.264416,"it is an instance of hate speech or not. This makes the problem a case of binary classification (±hate speech), which in turn makes it amenable to treatment using a variety of classification methods. These supervised learning techniques require pre-labelled training data, consisting of manually annotated positive and negative examples of the class(es) to be identified, to learn a model which, given a new instance, can predict the label with some degree of probability. In this setting, the most common features traditionally used for hate speech classifiers are lexical and grammatical features (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), with more recent approaches making use of neural network models relying on word embeddings (Badjatiya et al., 2017; Agrawal and Awekar, 2018). In this paper, we concentrate on the particular issues that one has to take into consideration when annotating Web 2.0 data for hate speech. The unique angle of our perspective is that it is informed by data-driven research in the field of Critical Discourse Analysis (CDA), a strand of applied linguistics that has for long dealt with the ways in which language is used to express ideologically-charged attitudes, especially in"
2020.lrec-1.626,W12-2103,0,0.686826,"model. Even so, previous studies “remain fairly vague when it comes to the annotation guidelines their annotators were given for their work” (Schmidt and Wiegand, 2017, p.8). A review of the relevant literature reveals that the majority of previous attempts to annotate Web 2.0 data for hate speech involves simple binary classification into hate speech and non-hate speech (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; Nobata et al., 2016). There are of course notable exceptions where the annotation scheme involved was more or less hierarchical in nature. For example, in Warner and Hirschberg (2012), annotators were tasked with classifying texts on the basis of whether they constitute hate speech or not, but were additionally asked to specify the target of said speech in the interest of distinguishing between seven different domains of hatred (e.g. sexism, xenophobia, homophobia, etc). Then, acknowledging that hate speech is a subtype of the more general category of offensive language and can thus often be conflated with it, Davidson et al. (2017) asked annotators to label tweets in terms of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speec"
2020.lrec-1.626,N16-2013,0,0.0265639,"is strikingly evident when one takes into account that most of the studies reviewed in NLP state-of-the-art reports on hate speech detection (including our discussion so far) formulate the question at hand using different – albeit interrelated – terms, which apart from hate speech variously include harmful speech, offensive and abusive language, verbal attacks, hostility or even cyber-bullying, among others. In this vein, as Waseem et al. (2017, p.78) observe and exemplify, this “lack of consensus has resulted in contradictory annotation guidelines – some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017).” The apparent confusion as to how to adequately define hate speech is of course not exclusive to NLP research, but extends to social scientific (Gagliardone et al., 2015) and even legal treatments of the theme (Bleich, 2014; Sellars, 2016). Given this general confusion, it is certainly no surprise that annotators, especially ones who do not have domainspecific knowledge on the matter, will be prone to disagreeing as to how to classify some text in the relevant task, as opposed to other comparable"
2020.lrec-1.626,W17-3012,0,0.0140427,"e taken to incite to discriminatory hatred, hate speech is now often used as an umbrella label for all sorts of hateful/insulting/abusive content (Brown, 2017). This much is strikingly evident when one takes into account that most of the studies reviewed in NLP state-of-the-art reports on hate speech detection (including our discussion so far) formulate the question at hand using different – albeit interrelated – terms, which apart from hate speech variously include harmful speech, offensive and abusive language, verbal attacks, hostility or even cyber-bullying, among others. In this vein, as Waseem et al. (2017, p.78) observe and exemplify, this “lack of consensus has resulted in contradictory annotation guidelines – some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017).” The apparent confusion as to how to adequately define hate speech is of course not exclusive to NLP research, but extends to social scientific (Gagliardone et al., 2015) and even legal treatments of the theme (Bleich, 2014; Sellars, 2016). Given this general confusion, it is certainly no surprise that annotators, especially o"
2020.lrec-1.626,W16-5618,0,0.276041,"ntensity on a scale from 0 to 4, and indicate whether each tweet contains ironical statements or stereotypical representations as well as how it fares in terms of aggressiveness and offensiveness. Despite the apparently increasing interest in the area and the development of all the more sophisticated annotation methodologies, a major cause for concern when it comes to annotations used for model training and testing is that reliability scores are consistently found to be low pretty much across the board (Warner and Hirschberg, 2012; Nobata et al., 2016; Ross et al., 2016; Tulkens et al., 2016; Waseem, 2016; Bretschneider and Peters, 2017; Schmidt and Wiegand, 2017; de Gibert et al., 2018). This effectively suggests that, even in the presence of more detailed guidelines (Ross et al., 2016; Malmasi and Zampieri, 2018; 2 Samples of this corpus can be made available upon request. A full release is expected in future, pending licensing agreements with the donors of the MaNeCo data, which will need to cover sensitive data such as comments written by online users, but deleted by the moderators of the newspaper portal. Sanguinetti et al., 2018), annotators often fail to develop an intersubjective under"
2020.lrec-1.626,N19-1144,0,0.0148192,"with classifying texts on the basis of whether they constitute hate speech or not, but were additionally asked to specify the target of said speech in the interest of distinguishing between seven different domains of hatred (e.g. sexism, xenophobia, homophobia, etc). Then, acknowledging that hate speech is a subtype of the more general category of offensive language and can thus often be conflated with it, Davidson et al. (2017) asked annotators to label tweets in terms of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. Along similar lines, Zampieri et al. (2019) introduce an explicitly hierarchical annotation scheme that requested annotators to code tweets on three consecutive levels: (a) on whether they contain offensive language; (b) on whether the insult/threat is targeted to some individual or group; and (c) on whether the target is an individual, a group or another type of entity (e.g. an organization or event). Finally, an annotation framework which, like the one proposed here, takes into account the intricate nature of hate speech was developed by Sanguinetti et al. (2018). Here, annotators were asked to not only provide a binary classificatio"
2020.lrec-1.784,W17-1304,1,0.827867,"unsupported. As a matter of fact, digital support for the Maltese language has improved drastically since the publication of the METANET white paper (and this is likely true for a number of other languages covered by the white paper series). To take some examples, large annotated corpora for Maltese are now available, with accompanying tools for segmentation and labelling, including tokenisation and partˇ epl¨o, 2013).2 . Advances of-speech annotation (Gatt and C´ have been made in the development of electronic lexicons (Camilleri, 2013) and in automatic morphological analysis and labelling (Borg and Gatt, 2017; Ravishankar et al., 2017) as well as dependency parsing (Tiedemann and van der Plas, 2016; Zammit, 2018). Advances in speech technology for Maltese have however been comparatively limited. While there have been successful attempts to build speech synthesis systems using concatenative techniques (Micallef, 1997; Borg et al., 2011), no tools currently exist for Automatic Speech Recognition (ASR). This is partly due to a substantial data bottleneck where resources for speech engineering are concerned. 1.2. Aims of the present paper The present paper addresses this gap, presenting a new corpus f"
2020.lt4hala-1.11,Q16-1006,0,0.0296489,"Missing"
2020.lt4hala-1.11,W11-1511,0,0.202802,"ed topic amongst the experts. Over the past 100 years or so, various researchers have applied a gamut of statistical analysis techniques. Many of these were used to find evidence that either supported or rejected the hoax hypothesis. Apart from Rugg, Montemurro, and Schinner, other researchers have used computational techniques to analyse, decipher, interpret, and to try to ultimately understand the manuscript. In Mary D’Imperio’s highly-cited book (D’Imperio, 1978), The Voynich Manuscript: An Elegant Enigma, she collected, analysed, and curated most of the research available up to that time. Reddy and Knight (2011) investigated the VM’s linguistic characteristics using a combination of statistical techniques and probabilistic models at page, paragraph, word and character levels. They found, inter alia, that VM characters within words were relatively more predictable than for English, Arabic, and Pinyin. Additional character-level analysis was performed by Landini (2001) and Zandbergen (2020) exploring topics such as entropy and spectral analysis of the text. In 2015, McInnes and Wang (2015) published a comprehensive report on the application of statistical methods and data mining techniques that they us"
2021.blackboxnlp-1.15,2020.acl-main.747,0,0.094691,"Missing"
2021.blackboxnlp-1.15,2020.blackboxnlp-1.5,0,0.0304814,"on, models like mBERT also have language-neutral representations, which cut across linguistic distinctions and enable the model to handle aspects of meaning language-independently. This also allows the model to be fine-tuned on a monolingual labelled data set and achieve good results in other languages, a process known as cross-lingual zero-shot learning (Pires et al., 2019; Libovick´y et al., 2020; Conneau et al., 2018; Hu et al., 2020). These results have motivated researchers to try and disentangle the language-specific and languageneutral components of mBERT (e.g. Libovick´y et al., 2020; Gonen et al., 2020). This background provides the motivation for the work presented in this paper. We focus on the relationship between language-specific and languageneutral representations in mBERT. However, our main goal is to study the impact of fine-tuning on the balance between these two types of representations. More specifically, we measure the Recent work has shown evidence that the knowledge acquired by multilingual BERT (mBERT) has two components: a languagespecific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks – POS tagging an"
2021.blackboxnlp-1.15,2021.naacl-main.282,0,0.0536609,"Missing"
2021.blackboxnlp-1.15,2020.emnlp-main.363,0,0.0387014,"Missing"
2021.blackboxnlp-1.15,2020.findings-emnlp.150,0,0.0314188,"Missing"
2021.blackboxnlp-1.15,N18-1101,0,0.0173193,"form cross-lingual zero-shot learning, we fine-tune mBERT on English only and evalu215 guages (Conneau et al., 2018). label classifier UDPOS For POS tagging, we use data from the Universal Dependencies Treebank (UDPOS; Marneffe et al., 2020) v2.7, using the train/dev/test splits provided. A validation set is randomly sampled from the training set. Since we are interested in cross-lingual zero-shot learning, we removed all non-English data from the train/val splits. mBERT language classifier Figure 1: The basic model architecture. XNLI For NLI, we use the monolingual English MultiNLI data set (Williams et al., 2018) as a training set, and the Cross-lingual Natural Language Inference data (XNLI; Conneau et al., 2018) for the development set and test set. Again, a validation set is randomly sampled from the training set. the test data from the target task (which is also labelled by language). Unless otherwise specified, gradients from the language classifier are not propagated to the pretrained mBERT model. Thus, the mBERT model parameters are only fine-tuned on the target task data set whilst the language classifier is finetuned in isolation. This allows us to monitor how much language-specific informatio"
2021.blackboxnlp-1.15,2020.aacl-main.56,0,0.0690634,"Missing"
2021.blackboxnlp-1.15,D19-1077,0,0.021936,"a line of earlier work which sought to achieve transferable multilingual representations using recurrent networkbased methods (e.g. Artetxe et al., 2019, inter alia), as well as work on developing multilingual embedding representations (Ruder et al., 2017). The considerable capacity of these multilingual models and their success in cross-lingual tasks has motivated a lot of research into the nature of the representations learned during pretraining. On the one hand, there is a significant amount of research suggesting that models such as mBERT acquire robust language-specific representations (Wu and Dredze, 2019; Libovick´y et al., 2020; Choenni and Shutova, 2020). On the other hand, it has been suggested that in addition to language-specific information, models like mBERT also have language-neutral representations, which cut across linguistic distinctions and enable the model to handle aspects of meaning language-independently. This also allows the model to be fine-tuned on a monolingual labelled data set and achieve good results in other languages, a process known as cross-lingual zero-shot learning (Pires et al., 2019; Libovick´y et al., 2020; Conneau et al., 2018; Hu et al., 2020). These results"
2021.blackboxnlp-1.15,P19-1493,0,0.318385,"ing that models such as mBERT acquire robust language-specific representations (Wu and Dredze, 2019; Libovick´y et al., 2020; Choenni and Shutova, 2020). On the other hand, it has been suggested that in addition to language-specific information, models like mBERT also have language-neutral representations, which cut across linguistic distinctions and enable the model to handle aspects of meaning language-independently. This also allows the model to be fine-tuned on a monolingual labelled data set and achieve good results in other languages, a process known as cross-lingual zero-shot learning (Pires et al., 2019; Libovick´y et al., 2020; Conneau et al., 2018; Hu et al., 2020). These results have motivated researchers to try and disentangle the language-specific and languageneutral components of mBERT (e.g. Libovick´y et al., 2020; Gonen et al., 2020). This background provides the motivation for the work presented in this paper. We focus on the relationship between language-specific and languageneutral representations in mBERT. However, our main goal is to study the impact of fine-tuning on the balance between these two types of representations. More specifically, we measure the Recent work has shown"
2021.blackboxnlp-1.15,2020.coling-main.105,0,0.0843735,"Missing"
2021.blackboxnlp-1.15,D07-1043,0,0.223036,"Missing"
C14-1099,P11-1135,0,0.374187,"Missing"
C14-1099,A92-1029,0,0.289951,"Missing"
C14-1099,W09-3707,0,0.0157553,"s English leaves the compound relation (i.e., the semantic relation between two components, e.g., N2 made of N1 as in iron door) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1047 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1047–1058, Dublin, Ireland, August 23-29 2014. covert, in French we find prepositions that correlate with the relation type (Girju, 2007; Celli and Nissim, 2009). Chocolate cake, cake made of chocolate, is translated with gateau au chocolat, whereas wedding cake, cake made for a wedding, is gateau de marriage. The first aim of this study is to extract a large database of compounds and their translations in context from a parallel corpus. This database will serve multiple purposes. For example, it will be used to study compounding across different languages, and we will exploit the cross-lingual variation for compound processing. In the second part of this paper, we will show a case study of how the extracted database can be used for analysing the stru"
C14-1099,I08-1053,0,0.015878,"tract both the NCs and their context. While the aforementioned work serves as resource for improving machine translation (MT) systems, we focus on NC research and how multilingual evidence can help analysing and interpreting English NCs. This multilingual perspective on a considerable number of languages has been adopted as well by Macherey et al., (2011), who present a multilingual language-independent approach to compound splitting. Moreover, they learned morphological operations on compounding automatically. Here, Macherey et al., (2011) extract training instances using a method related to Garera and Yarowsky (2008): select a single word f in a language l translated to several English words ei . If there is a translation for each ei to a word gi that shows a (partial) substring match with f , (f ; e1 , . . . , en ; g1 , . . . , gn ) is extracted. While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract NC instances with a set of four closed compounding languages. This token-based perspective has the advantage that we can process English NCs for which there is no literal translation to the target language (e.g., health insurance aligned to Krankenvers"
C14-1099,P07-1072,0,0.0759501,"ample, whereas English leaves the compound relation (i.e., the semantic relation between two components, e.g., N2 made of N1 as in iron door) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1047 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1047–1058, Dublin, Ireland, August 23-29 2014. covert, in French we find prepositions that correlate with the relation type (Girju, 2007; Celli and Nissim, 2009). Chocolate cake, cake made of chocolate, is translated with gateau au chocolat, whereas wedding cake, cake made for a wedding, is gateau de marriage. The first aim of this study is to extract a large database of compounds and their translations in context from a parallel corpus. This database will serve multiple purposes. For example, it will be used to study compounding across different languages, and we will exploit the cross-lingual variation for compound processing. In the second part of this paper, we will show a case study of how the extracted database can be us"
C14-1099,S13-2025,0,0.0224035,"Missing"
C14-1099,P06-2064,0,0.0336891,"Missing"
C14-1099,P11-1140,0,0.0158845,"es. Previous work on multilingual extraction include Morin and Daille (2010) and Weller and Heid (2012). These type-based approaches focus on bilingual terminology extraction using comparable corpora. Our token-based extraction method includes 10 languages and we extract both the NCs and their context. While the aforementioned work serves as resource for improving machine translation (MT) systems, we focus on NC research and how multilingual evidence can help analysing and interpreting English NCs. This multilingual perspective on a considerable number of languages has been adopted as well by Macherey et al., (2011), who present a multilingual language-independent approach to compound splitting. Moreover, they learned morphological operations on compounding automatically. Here, Macherey et al., (2011) extract training instances using a method related to Garera and Yarowsky (2008): select a single word f in a language l translated to several English words ei . If there is a translation for each ei to a word gi that shows a (partial) substring match with f , (f ; e1 , . . . , en ; g1 , . . . , gn ) is extracted. While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we"
C14-1099,J93-2004,0,0.0462718,"red with the geometric mean of the components’ frequencies in the parallel corpus. The highest-scored segmentation (possibly with no split point) is used. 3.2 Preselection of English noun compounds using PoS patterns As a basis for the extraction of English NCs, we use a set of possible English PoS sequences that can constitute an NC. These PoS patterns account for the various ways of composing English NCs and for the inseparability property as described in Section 2.1. Table 1 lists all plausible PoS patterns for bipartite and tripartite NCs with some examples (cf. the Penn Treebank tag set (Marcus et al., 1993)). For all examples in Table 1, we found translations to a closed compound in German, which satisfies our initial definition described in Section 2.2, e.g., overall recovery rate has been translated to Gesamtr¨uckforderungsquote. Although the larger the number of components, the sparser the number of (correct) extractions, we create a regular expression for PoS patterns that cover English NCs with n components (where 2 ≤ n ≤ 10). This regular expression combines all possible combinations of observed NC types. In the next step, we will filter noise, that occurs mostly in longer word sequences."
C14-1099,W05-0603,0,0.33391,"Missing"
C14-1099,J03-1002,0,0.00382252,"k data using the MATE5 tagger. The sentence alignment provided by OPUS is restricted to language pairs. As we need a sentence representation that is parallel in all 10 languages, we apply the OPUS sentence aligner (with English as pivot) on our language set and extract a total of 884,164 parallel sentence representations. The word alignment information provided by OPUS was also based on language pairs. This means, the sentence-wise token indices has to be adapted to our updated sentence representation (which is different due to a larger language set). In OPUS, the word alignment tool GIZA ++ (Och and Ney, 2003) has been used with the symmetrisation heuristics (grow-diag-final-and (Koehn et al., 2007)). 4.2 Evaluation procedure and scoring In order to compare the added value in terms of recall and precision of each closed compound restrictor (i.e., CCR(1) to CCR(4)), we randomly select 50 accepted and 50 rejected English word sequences for each restrictor. We rate the correctness of acceptance and rejection and compute precision and recall as given in (2) and (3). F-Score is defined as harmonic mean of precision and recall. P recision = Recall = accepted ∩ correct accepted accepted ∩ correct (accepte"
C14-1099,C10-1100,0,0.0956568,")). In cross-lingual annotation transfer (Yarowsky and Ngai, 2001; Pad´o, 2007; Van der Plas et al., 2011) human annotations are transferred from one language to the other in parallel data. In this paper, we use the structural differences between languages as found in parallel corpora to generate annotations on the target language and do not rely on annotations on the source language. Bracketing methods for both three-noun compounds and complete base NPs have been designed both supervised and unsupervised. Vadas and Curran (2007) used a supervised bracketing method on manually annotated data. Pitler et al. (2010) used the data from Vadas and Curran (2007) for a parser applicable on base NPs of any length including coordinations. Their supervised classifier exploited webscale N-grams. Although supervised methods outperform unsupervised methods by far, the need for annotated data is a drawback of supervised approaches. Bergsma et al. (2011) used crosslingual data as additional supervision to make the need for manual annotations less pressing. Unsupervised methods use N-gram statistics (Marcus, 1980; Lauer, 1995; Nakov and Hearst, 2005) or semantic information (Kim and Baldwin, 2013). 7 Conclusion In thi"
C14-1099,C92-4201,0,0.56568,"Missing"
C14-1099,W03-1803,0,0.0321297,"Missing"
C14-1099,tiedemann-2012-parallel,0,0.0266507,"uroparl corpus comprises 21 European languages, the amount of common data they cover is rather small. This means, the more languages we use, the smaller the amount of common data. In order to get a good trade-off between cross-lingual coverage and language variation exploitation, we decided on a set of 10 languages: English, the closed compounding languages Danish, Dutch, German and Swedish, as well as Greek and the Romance languages French, Italian, Portuguese and Spanish. Instead of preprocessing the parallel corpus on our own, we exploit the already preprocessed Europarl resource of OPUS3 (Tiedemann, 2012). This preprocessed resource is PoS tagged using TreeTagger (Schmid, 1995) for English, Dutch, German, French, Italian and Spanish and the Hunpos4 tagger for Danish, Portuguese and Swedish. We additionally tagged the Greek data using the MATE5 tagger. The sentence alignment provided by OPUS is restricted to language pairs. As we need a sentence representation that is parallel in all 10 languages, we apply the OPUS sentence aligner (with English as pivot) on our language set and extract a total of 884,164 parallel sentence representations. The word alignment information provided by OPUS was als"
C14-1099,weller-heid-2012-analyzing,0,0.0801834,"mance; † indicates significantly higher than the LEFT baseline Table 6 shows the results of our system compared to the LEFT class baseline. The first result is that both inferred label and word alignment information for phrase pattern outperform the LEFT class baseline significantly7 . Bracketing with word alignment information for phrase pattern outperforms bracketing based on the inferred labels. 6 Related Work Our methods for extracting and structuring English NCs rely on the spelling of various aligned languages. Previous work on multilingual extraction include Morin and Daille (2010) and Weller and Heid (2012). These type-based approaches focus on bilingual terminology extraction using comparable corpora. Our token-based extraction method includes 10 languages and we extract both the NCs and their context. While the aforementioned work serves as resource for improving machine translation (MT) systems, we focus on NC research and how multilingual evidence can help analysing and interpreting English NCs. This multilingual perspective on a considerable number of languages has been adopted as well by Macherey et al., (2011), who present a multilingual language-independent approach to compound splitting"
C14-1099,N01-1026,0,0.143996,"ds ei . If there is a translation for each ei to a word gi that shows a (partial) substring match with f , (f ; e1 , . . . , en ; g1 , . . . , gn ) is extracted. While Macherey et al., (2011) extract training instances type-based in a bilingual setting, we directly extract NC instances with a set of four closed compounding languages. This token-based perspective has the advantage that we can process English NCs for which there is no literal translation to the target language (e.g., health insurance aligned to Krankenversicherung (lit. invalid insurance)). In cross-lingual annotation transfer (Yarowsky and Ngai, 2001; Pad´o, 2007; Van der Plas et al., 2011) human annotations are transferred from one language to the other in parallel data. In this paper, we use the structural differences between languages as found in parallel corpora to generate annotations on the target language and do not rely on annotations on the source language. Bracketing methods for both three-noun compounds and complete base NPs have been designed both supervised and unsupervised. Vadas and Curran (2007) used a supervised bracketing method on manually annotated data. Pitler et al. (2010) used the data from Vadas and Curran (2007) f"
C14-1099,C00-2137,0,0.0772822,"Missing"
C14-1099,P07-2045,0,\N,Missing
C14-1121,W12-4201,1,0.858221,"mation was retained during training and not only for aligned ones, in contrast to direct transfer. We expect to augment the recall when using global estimates and hope that the effect on precision is not too negative. Learning For each French verb (v) in the lexicon built as described in Section 3, we want to be able to identify its correct predicate label in a new context by choosing one among its candidate labels (L) retained from the training corpus. A feature vector is built for each candidate label Li (1 ≤ i ≤ |L|) found for the verb v in the lexicon, following the procedure described in Apidianaki et al. (2012). For each candidate label, we extract the content word co-occurrences of the verb v in the French sentences where it translates an English verb tagged with this label in the training corpus. The retained French words constitute the features of the vector built for that label. Let N be the number of features retained for each label Li of the verb v from the corresponding French contexts. Each feature Fj (1 ≤ j ≤ N ) receives a total weight with the label (tw(Fj , Li )) which is learned from the data and defined as the product of the feature’s global weight (gw(Fj )) and its local weight with t"
C14-1121,E09-1010,1,0.835832,"its sense. For example, “give.01” stands for the first sense of the verb give. As the predicate label contains a lot of lexical information, putting the correct English predicate label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the candidate senses are the words’ translations in other languages and WSD aims at predicting semantically correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual transfer of predicate labels is that we do not search for correct translations of French words but for the most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense). The global predicate labelling method consists of a learning step and a labelling step. During learning, we compute estimates for annotation transfer on the basis of the word alignments between English and French predicates over the entire parallel training corpus. At labelling time, we label French verbs with English predica"
C14-1121,2009.jeptalnrecital-long.4,0,0.11988,"Missing"
C14-1121,D07-1007,0,0.0257633,"n the English verb and its sense. For example, “give.01” stands for the first sense of the verb give. As the predicate label contains a lot of lexical information, putting the correct English predicate label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the candidate senses are the words’ translations in other languages and WSD aims at predicting semantically correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual transfer of predicate labels is that we do not search for correct translations of French words but for the most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense). The global predicate labelling method consists of a learning step and a labelling step. During learning, we compute estimates for annotation transfer on the basis of the word alignments between English and French predicates over the entire parallel training corpus. At labelling time, we label French verbs w"
C14-1121,P11-1061,0,0.0277476,"The two global methods proposed in this paper are presented in Section 5. We report and discuss our results in Section 6, before concluding. 2 Related work Transferring annotation from one language to another in order to train monolingual tools for new languages was first introduced by Yarowsky and Ngai (2001). In their approach, token-level part-of-speech (PoS) and noun phrase bracketing information was projected across word-aligned bitext and this partial annotation served to estimate the parameters of a model that generalized from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed 1 Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010) or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider a"
C14-1121,J94-4004,0,0.0387831,"the previous section and exploit them for role labelling. For a given predicate, diathesis alternations are the major source of variation in propositions. They give rise to different syntactic structures, while the semantic roles remain stable. For example, the sentence “I gave the book to Jean” is syntactically different from “I gave Jean the book”, but semantic roles on the three arguments stay the same. We will show in a feasibility study that the effect of diathesis alternations on the correlation between syntax and semantics is limited. In a cross-lingual setting, structural divergences (Dorr, 1994) are expected to reduce the correlation between syntax and semantics. An example is the difference in syntactic structure between the sentences “Tu me manques” vs. “I miss you”, which are translations of each other, however the semantic roles are the same across languages. As our global method is not restricted to alignments at labelling time, we are able to classify all given arguments3 and not just those that are aligned in a parallel corpus. In this way, we believe that the negative effect of structural divergences and diathesis alternations is limited. Moreover, we show how mild supervisio"
C14-1121,W06-1601,0,0.029663,"us section. P|CF | j=1 tw(CFj , Li ) assoc score(Vi , C) = (4) |CF | The label that receives the highest association score with the new context is returned and serves to annotate the corresponding French verb. 5.2 Global cross-lingual role labelling For role labelling, we adopt a different strategy. Whereas predicate labels include a lot of lexical information, role labels do not. However, for role labels there is another source of information that helps to define global estimates: the correlation between syntax and semantics. Previous work in monolingual unsupervised semantic role induction (Grenager and Manning, 2006; Lang and Lapata, 2010; Lang and Lapata, 2011) showed that mapping rules that assign semantic roles to arguments of a verb based on the syntactic functions of these arguments, represent a baseline that is very hard to beat. This strong correlation between syntactic labels and semantic role labels in the PropBank annotation has been shown in detail by Merlo and Van der Plas (2009). In contrast to previous work on monolingual unsupervised semantic role induction, we add the predicate label as a predictor. The core arguments of the verb, that are the numbered labels in PropBank, are known to be"
C14-1121,W08-2122,0,0.052169,"Missing"
C14-1121,2005.mtsummit-papers.11,0,0.0276186,"nd lexical) to adapt a source model to a targetlanguage model. The ideas behind their cross-lingual model adaptation resemble the ideas behind our global method for semantic role labelling. However, in contrast to their work we do not consider the predicate labelling as given because, as manual annotations show (van der Plas et al., 2010), this task is not trivial. We first build a tailored global model for cross-lingual predicate labelling and then use the predicted predicate labels for semantic role labelling. 3 Data In our experiments, we use the English-French part of the Europarl corpus (Koehn, 2005). The dataset is tokenised and lowercased and only sentence pairs corresponding to a one-to-one sentence alignment with lengths ranging from one to 40 tokens on both French and English sides are considered. Furthermore, because translation shifts are known to pose problems for the automatic projection of semantic roles across languages (Pad´o, 2007), we select only those parallel sentences in Europarl that are direct 1281 translations from English to French or vice versa. In the end, we have a parallel corpus of 276-thousand sentence pairs. The English part of the parallel corpus is annotated"
C14-1121,P13-1117,0,0.355914,"across multiple examples. In their work, transfer of predicate labels and semantic role labels is done in one step. The model needs an aggressive filter to compensate for missing annotations on the predicate level after direct transfer. This filter successively leads to drops in performance for the role labellings. Here, we build two separate global models that complement direct transfer instead of relying on it. The same emphasis on learning is found in cross-lingual model transfer where source language models are adapted to work on the target language directly. For semantic role labelling, Kozhevnikov and Titov (2013) use shared feature representations (syntactic and lexical) to adapt a source model to a targetlanguage model. The ideas behind their cross-lingual model adaptation resemble the ideas behind our global method for semantic role labelling. However, in contrast to their work we do not consider the predicate labelling as given because, as manual annotations show (van der Plas et al., 2010), this task is not trivial. We first build a tailored global model for cross-lingual predicate labelling and then use the predicted predicate labels for semantic role labelling. 3 Data In our experiments, we use"
C14-1121,N10-1137,0,0.0716458,"o estimate the parameters of a model that generalized from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed 1 Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010) or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider argument identification as given. 1280 EN: English FR: French Parallel corpus direct transfer FR EN semantic annotations (predicates + roles) FR annotation transfer PoS tags semantic annotations (predicates + roles) Parallel corpus global predicate labelling EN semantic annotations (predicates) FR PoS tags learning model for predicate labelling FR labelling semantic annotations (predicates) FR Parallel corpus EN syntactic annotations EN learning model for role labelling FR syntac"
C14-1121,P11-1112,0,0.113379,"ed from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distributions. A different approach to cross-lingual PoS tagging is proposed 1 Most unsupervised approaches consider argument identification as a separate task that is omitted (Lang and Lapata, 2010) or performed heuristically (Lang and Lapata, 2011). We focus on semantic role labelling in this paper and consider argument identification as given. 1280 EN: English FR: French Parallel corpus direct transfer FR EN semantic annotations (predicates + roles) FR annotation transfer PoS tags semantic annotations (predicates + roles) Parallel corpus global predicate labelling EN semantic annotations (predicates) FR PoS tags learning model for predicate labelling FR labelling semantic annotations (predicates) FR Parallel corpus EN syntactic annotations EN learning model for role labelling FR syntactic annotations FR model transfer OR global semanti"
C14-1121,J93-2004,0,0.0488719,"Missing"
C14-1121,P09-1033,1,0.929936,"Missing"
C14-1121,P03-1058,0,0.0207565,"ate labels contain the English verb and its sense. For example, “give.01” stands for the first sense of the verb give. As the predicate label contains a lot of lexical information, putting the correct English predicate label on a French verb is very close to Word Sense Disambiguation (WSD), the task of automatically identifying the meaning of words in context (Navigli, 2009). In the cross-lingual variant of this task, the candidate senses are the words’ translations in other languages and WSD aims at predicting semantically correct translations for words in context (Resnik and Yarowsky, 2000; Ng et al., 2003; Carpuat and Wu, 2007; Apidianaki, 2009). The main difference between cross-lingual WSD and our cross-lingual transfer of predicate labels is that we do not search for correct translations of French words but for the most appropriate predicate labels in context (i.e. verbs disambiguated with a predicate sense). The global predicate labelling method consists of a learning step and a labelling step. During learning, we compute estimates for annotation transfer on the basis of the word alignments between English and French predicates over the entire parallel training corpus. At labelling time, w"
C14-1121,J03-1002,0,0.0104729,"Missing"
C14-1121,J05-1004,0,0.919839,", Ireland, August 23-29 2014. or constituents in those sentences. We will refer to these traditional methods as direct transfer because the semantic annotations are transferred directly from token to token. Although direct transfer methods are straightforward and easy to implement, they are vulnerable to missing or incorrect alignments which lead to missing and erroneous annotations in the target language. Consequently, non-literal translations and translation shifts present major problems for these methods. In this paper we propose a global approach to the cross-lingual transfer of PropBank (Palmer et al., 2005) semantic annotations that aggregates information at the corpus level and, as a consequence, is more robust to non-literal translations and alignment errors. Our global approach involves two steps: in the learning step, two global models are learned on the basis of role and predicate annotations in the source language (English). In the labelling step, these models assign labels to verbs and their arguments in the target language (French) without consulting any parallel data. Contrary to previous work, we build separate models for the transfer of semantic role and predicate annotation because p"
C14-1121,J08-2005,0,0.0479713,"to A1 if neither the dependency label nor the predicate has been seen in training. To treat the R-suffix, which takes care of anaphoric arguments, we use the following simple rule: for the monolingual setting all arguments with PoS-tags “WDT”, “WP”, and “WRB” receive the R-suffix. In the cross-lingual setting, we translate the PoS tags to the single French PoS tag “PROREL”. We do not treat the C-prefix, which takes care of discontinuous arguments, because there were only a few examples. We do not accept duplicate semantic roles, a constraint that leads to valid role configurations in general (Punyakanok et al., 2008). We expect the more prominent semantic roles, such as A0 and A1, to appear earlier in the sentence than semantic roles with higher numbers. We therefore attribute semantic roles of a predicate from left to right. 5.3 Combining direct and global cross-lingual transfer Direct transfer methods generally have low recall, we however expect them to be more precise than the global methods. In our combined method, we use the annotations assigned by direct transfer as the backbone and fill missing labels by the global methods. The annotations from direct transfer restrict the possible roles the global"
C14-1121,Q13-1001,0,0.0823028,"Missing"
C14-1121,W07-2218,0,0.0603147,"Missing"
C14-1121,W10-1814,1,0.935459,"Missing"
C14-1121,N01-1026,0,0.0849576,"our method adapts relatively easily to other language pairs without requiring semantic lexicons in the target language. In the next section, we present related work on cross-lingual annotation transfer. In Section 3 we present the data used in our experiments and in Section 4 we briefly discuss direct transfer. The two global methods proposed in this paper are presented in Section 5. We report and discuss our results in Section 6, before concluding. 2 Related work Transferring annotation from one language to another in order to train monolingual tools for new languages was first introduced by Yarowsky and Ngai (2001). In their approach, token-level part-of-speech (PoS) and noun phrase bracketing information was projected across word-aligned bitext and this partial annotation served to estimate the parameters of a model that generalized from the noisy projection in a robust way. In more recent work, Das and Petrov (2011) propose a graph-based framework for projecting syntactic information across languages. They create type-level tag dictionaries by aggregating over projected token-level information extracted from bi-text and use label propagation on a similarity graph to smooth and expand the label distrib"
C14-1121,W09-1201,0,\N,Missing
F14-1005,D07-1007,0,0.0574797,"Missing"
F14-1005,P11-1061,0,0.135028,"Missing"
F14-1005,W09-1201,0,0.0598514,"Missing"
F14-1005,W08-2122,0,0.0476428,"Missing"
F14-1005,2005.mtsummit-papers.11,0,0.0814041,"Missing"
F14-1005,P13-1117,0,0.0330466,"Missing"
F14-1005,J93-2004,0,0.0487423,"Missing"
F14-1005,P03-1058,0,0.0649263,"Missing"
F14-1005,J03-1002,0,0.00610544,"Missing"
F14-1005,J05-1004,0,0.260464,"Missing"
F14-1005,Q13-1001,0,0.106742,"Missing"
F14-1005,W10-1814,1,0.907122,"Missing"
F14-1005,N01-1026,0,0.334581,"Missing"
F14-1005,E09-1010,1,\N,Missing
F14-1005,P11-2052,1,\N,Missing
F14-1005,W12-4201,1,\N,Missing
I05-7011,alfonseca-manandhar-2002-improving,0,\N,Missing
I05-7011,W02-0908,0,\N,Missing
I05-7011,P90-1034,0,\N,Missing
I05-7011,P94-1019,0,\N,Missing
I05-7011,P89-1010,0,\N,Missing
I05-7011,P98-2127,0,\N,Missing
I05-7011,C98-2122,0,\N,Missing
I05-7011,N04-1041,0,\N,Missing
I05-7011,P99-1004,0,\N,Missing
I05-7011,P03-1001,0,\N,Missing
I13-1104,C10-1011,0,0.015182,"that specifies the set of candidate terms for all languages) – in our case, we choose German since it expresses term boundaries very directly in its linguistic forms (capitalized nouns, single word compounding). German terms are defined as a capitalized token with at least 4 letters. For each unordered language pair {Lterm ,Li }, we define each term in Li as a sequence of tokens that are aligned to a term in Lterm . In Figure 3, “liquid phase hydrogenation” is defined as term since it is aligned to “Fl¨ussigphasenhydrierung” . For reducing errors due to poor word alignment4 , we apply M ATE (Bohnet, 2010) part of speech (PoS) tagger on phrases in languages other PROCESS : A method or event that results in a change of state (e.g., stretching, molding process, redundancy control, . . . ). TECHNICAL QUALITY : A basic or essential attribute which is measurable or shared by all members of a group (e.g., power consumption, piston speed, light reflection index, . . . ). The sources for the English seed sets have been WordNet lexicographer classes (Ciaramita and Johnson, 2003) and Wikipedia6 word lists. For each semantic class and language we induced lexicons of 2000 terms. For each lexicon we evaluat"
I13-1104,C10-2010,0,0.0309838,"pendent we restrict our demonstration of its potential to two languages: German and English. The parallel corpus. We use patent data distributed by the European Patent Office (EPO3 ) between 1998 and 2008. Most European patents provide their claims (the part of a patent defining the scope of protection) in German, English and French. We constructed a German-English parallel corpus out of 177,317 patent documents. Creation of Moses phrase table. For each unordered language pair, we create a M OSES (Koehn et al., 2007) phrase table in several steps. We first apply sentence alignment (G ARGANTUA Braune and Fraser (2010)), then word alignment (MG IZA ++ Gao and Vogel (2008)) to the data. And finally, we apply the statistical machine translation tool M OSES to the parallel word-aligned data. The resulting data structure is a phrase table of word-aligned phrases in two languages as shown in Figure 3, where the third line indicates the word alignment. English (JJ|VBG|NN)* NN (IN NN+)? Figure 4: PoS pattern for term filtering Patterns are extracted from the phrase tables as well. For each phrase in Lterm and Li we use the remaining tokens surrounding each term as bootstrapping pattern associated with this term (e"
I13-1104,W03-1022,0,0.0270711,"ion” is defined as term since it is aligned to “Fl¨ussigphasenhydrierung” . For reducing errors due to poor word alignment4 , we apply M ATE (Bohnet, 2010) part of speech (PoS) tagger on phrases in languages other PROCESS : A method or event that results in a change of state (e.g., stretching, molding process, redundancy control, . . . ). TECHNICAL QUALITY : A basic or essential attribute which is measurable or shared by all members of a group (e.g., power consumption, piston speed, light reflection index, . . . ). The sources for the English seed sets have been WordNet lexicographer classes (Ciaramita and Johnson, 2003) and Wikipedia6 word lists. For each semantic class and language we induced lexicons of 2000 terms. For each lexicon we evaluated a sample of 200 terms. Two annotators first rated 50 terms for each language and class as TRUE or FALSE. Then they discussed disagreements. Afterwards, they rated the remaining terms in each lexicon sample. We achieved a total inter-annotator agreement of κ = .701 (Cohen’s 3 www.epo.org Since our corpus is not large enough for perfect word alignment, it can be supported by a part of speech tagger. To keep the process completely language-independent, this step may al"
I13-1104,P91-1017,0,0.306455,"Curran et al., 2007). For instance, the ambiguity found in female names such as Iris and Rose may cause the induced terms to drift into flower names (McIntosh and Curran, 2009). Examples from the patent domain, that we are focusing on in this work, are PROCESSES that may drift into the semantic class of OBJECTS when terms such as energy storage and spring coupling are induced. Previous work has used the cross-lingual correspondence between variations in linguistic structure and variations in ambiguity as a form of naturally occurring supervision in unsupervised learning for a number of tasks (Dagan et al., 1991; 844 International Joint Conference on Natural Language Processing, pages 844–848, Nagoya, Japan, 14-18 October 2013. where Pi is the number of patterns containing termi . Finally Basilisk adds the t (originally 5) highest-ranked terms to the lexicon (line 8) and process repeats. ble method we are able to induce lexicons for any language given a parallel corpus. We do not need seed lists for all languages, which are often sparse. Translating1 the English seed list automatically results in high-quality lexicons for all other languages. Finally, many pattern-based lexicon bootstrapping methods"
I13-1104,W08-0509,0,0.0190449,"two languages: German and English. The parallel corpus. We use patent data distributed by the European Patent Office (EPO3 ) between 1998 and 2008. Most European patents provide their claims (the part of a patent defining the scope of protection) in German, English and French. We constructed a German-English parallel corpus out of 177,317 patent documents. Creation of Moses phrase table. For each unordered language pair, we create a M OSES (Koehn et al., 2007) phrase table in several steps. We first apply sentence alignment (G ARGANTUA Braune and Fraser (2010)), then word alignment (MG IZA ++ Gao and Vogel (2008)) to the data. And finally, we apply the statistical machine translation tool M OSES to the parallel word-aligned data. The resulting data structure is a phrase table of word-aligned phrases in two languages as shown in Figure 3, where the third line indicates the word alignment. English (JJ|VBG|NN)* NN (IN NN+)? Figure 4: PoS pattern for term filtering Patterns are extracted from the phrase tables as well. For each phrase in Lterm and Li we use the remaining tokens surrounding each term as bootstrapping pattern associated with this term (e.g., “Verfahren zur selektiven <X&gt;” is defined as patt"
I13-1104,W09-1703,0,0.152481,"xts. Languages are not isomorphic: ambiguous terms and contexts are frequently language-specific. In our example above, the English term energy storage is ambiguous, however, in German, each reading has its own translation. Energy storage is translated with Energiespeicher in the OBJECT reading and Energiespeicherung in the PROCESS reading. Our multilingual ensemble lexicon bootstrapping system is inspired by Basilisk (Thelen and Riloff, 2002). Previous work has addressed semantic drift in Basilisk by conflict resolution between several classes (Thelen and Riloff, 2002), by using web queries (Igo and Riloff, 2009) and by combining Basilisk in an ensemble with an SVM tagger and a coreference resolution system (Qadir and Riloff, 2012). These approaches are monolingual. Instead we use a multilingual ensemble method where the induced lexicons of several languages constrain each other. Apart from addressing semantic drift, the multilingual setting we propose has several other advantages. First, one language may leave implicit what another expresses directly in linguistic forms. In German, common nouns are capitalized and compound nouns are written as one word. We propagate German noun information via word a"
I13-1104,P07-2045,0,0.00261452,"ing are stored in a dictionary DICTterm↔i . Although our method is multilingual and language-independent we restrict our demonstration of its potential to two languages: German and English. The parallel corpus. We use patent data distributed by the European Patent Office (EPO3 ) between 1998 and 2008. Most European patents provide their claims (the part of a patent defining the scope of protection) in German, English and French. We constructed a German-English parallel corpus out of 177,317 patent documents. Creation of Moses phrase table. For each unordered language pair, we create a M OSES (Koehn et al., 2007) phrase table in several steps. We first apply sentence alignment (G ARGANTUA Braune and Fraser (2010)), then word alignment (MG IZA ++ Gao and Vogel (2008)) to the data. And finally, we apply the statistical machine translation tool M OSES to the parallel word-aligned data. The resulting data structure is a phrase table of word-aligned phrases in two languages as shown in Figure 3, where the third line indicates the word alignment. English (JJ|VBG|NN)* NN (IN NN+)? Figure 4: PoS pattern for term filtering Patterns are extracted from the phrase tables as well. For each phrase in Lterm and Li w"
I13-1104,S10-1003,0,0.098618,"Missing"
I13-1104,P09-1045,0,0.0257162,"source for many natural language processing (NLP) tasks like information extraction or anaphora resolution. Methods for automatically bootstrapping semantic lexicons given a seed list often struggle with lexicon accuracy decrease in higher iterations depending on corpus size (Igo and Riloff, 2009). One reason for this is semantic drift, which occurs when erroneous terms and/or contexts are introduced into and then dominate the iterative process (Curran et al., 2007). For instance, the ambiguity found in female names such as Iris and Rose may cause the induced terms to drift into flower names (McIntosh and Curran, 2009). Examples from the patent domain, that we are focusing on in this work, are PROCESSES that may drift into the semantic class of OBJECTS when terms such as energy storage and spring coupling are induced. Previous work has used the cross-lingual correspondence between variations in linguistic structure and variations in ambiguity as a form of naturally occurring supervision in unsupervised learning for a number of tasks (Dagan et al., 1991; 844 International Joint Conference on Natural Language Processing, pages 844–848, Nagoya, Japan, 14-18 October 2013. where Pi is the number of patterns cont"
I13-1104,S12-1028,0,0.0211001,"the English term energy storage is ambiguous, however, in German, each reading has its own translation. Energy storage is translated with Energiespeicher in the OBJECT reading and Energiespeicherung in the PROCESS reading. Our multilingual ensemble lexicon bootstrapping system is inspired by Basilisk (Thelen and Riloff, 2002). Previous work has addressed semantic drift in Basilisk by conflict resolution between several classes (Thelen and Riloff, 2002), by using web queries (Igo and Riloff, 2009) and by combining Basilisk in an ensemble with an SVM tagger and a coreference resolution system (Qadir and Riloff, 2012). These approaches are monolingual. Instead we use a multilingual ensemble method where the induced lexicons of several languages constrain each other. Apart from addressing semantic drift, the multilingual setting we propose has several other advantages. First, one language may leave implicit what another expresses directly in linguistic forms. In German, common nouns are capitalized and compound nouns are written as one word. We propagate German noun information via word alignment to English and thereby learn both single words as well as most multiword expressions (MWEs) without the need for"
I13-1104,W02-1028,0,0.368538,"e preeminently suitable to remedy problems related to semantic drift in iterative bootstrapping, where lexical and structural ambiguity give rise to erroneous terms and/or contexts. Languages are not isomorphic: ambiguous terms and contexts are frequently language-specific. In our example above, the English term energy storage is ambiguous, however, in German, each reading has its own translation. Energy storage is translated with Energiespeicher in the OBJECT reading and Energiespeicherung in the PROCESS reading. Our multilingual ensemble lexicon bootstrapping system is inspired by Basilisk (Thelen and Riloff, 2002). Previous work has addressed semantic drift in Basilisk by conflict resolution between several classes (Thelen and Riloff, 2002), by using web queries (Igo and Riloff, 2009) and by combining Basilisk in an ensemble with an SVM tagger and a coreference resolution system (Qadir and Riloff, 2012). These approaches are monolingual. Instead we use a multilingual ensemble method where the induced lexicons of several languages constrain each other. Apart from addressing semantic drift, the multilingual setting we propose has several other advantages. First, one language may leave implicit what anoth"
I13-1104,W10-3304,1,0.89472,"Missing"
I13-1104,W09-2413,0,\N,Missing
I13-1188,C10-1011,0,0.0363546,"terns we use. Experimental setup and results are presented in Section 4. In Section 5, we discuss and analyze these results. The last two sections describe related work and conclusions, respectively. 2 Data, task description and evaluation methodology Data. We use the patent data distributed by the European Patent Office1 (EPO) as our corpus. We extract the description (the main part of a patent) from 561,676 English patents filed between 1998 and 2008 and perform sentence splitting and tokenization using Treetagger (Schmid, 1994) and lemmatization and part-of-speech (POS) tagging using MATE (Bohnet, 2010). Sentences up to a size of 100 tokens extracted from a sample of 25,000 patents are parsed by M ATE. The resulting EPO corpus consists of roughly 4.6 billion tokens. Task description. The task we address is semantic tagging of patents. The research reported here was conducted as part of a project on computational linguistics analysis of patent text. We want to be able to support functionalities like color-coding entities of a particular semantic class for quick perusal; or searching for entities in a particular semantic class. Our longterm goal is to support semantic tagging for a large varie"
I13-1188,P99-1016,0,0.328772,"lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and Charniak (1998); Caraballo (1999); Goyal et al. (2010)). Most of this previous work uses a pairwise perspective (i.e., a focus on whether two words cooccur in a coordination). However, we use coordinations in a Basilisk approach, for which patterns contain several terms in general. We therefore do not split up coordinations in pairs but keep the complete coordination intact. Coordinations in technical domains frequently contain more than 2 elements. We argue that bootstrapping methods, known to be particularly sensitive to the ambiguity of terms and contexts and prone to semantic drift, benefit from the strong semantic cohere"
I13-1188,W09-2201,0,0.0256643,"Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This restriction to coordinat"
I13-1188,W03-0415,0,0.0235698,"ce in named entity recognition (NER). Work on automatic extraction of gazetteers for NER includes (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327 our approach because it uses knowledge bases like Wikipedia or is only applicable to traditional named entities (NEs). Traditional NEs like person are capitalized. Substances are not. Our work also differs in its focus on coordinations and technical text. Coordinations have been frequently used in work on lexical acquisition. Caraballo (1999) builds a hierarchy of coordinated nouns and their hypernyms. Cederberg and Widdows (2003) use coordinations to estimate the semantic relatedness of nouns. Widdows and Dorow (2002) and Qiu et al. (2011) cluster nouns and evaluate the semantic homogeneity of the clusters. Etzioni et al. (2005) use Hearst patterns to bootstrap lexicons. They also consider coordinations when selecting candidates. This previous work on coordinations is unsupervised and not focused on learning a particular semantic class that is defined by a seed set. Roark and Charniak (1998) use a variety of syntactic constructions, including coordinations, for bootstrapping. Our approach is different in that we do no"
I13-1188,W03-1022,0,0.0430402,"ernym relation such as “other products” in “copolymers, polyisobutene and other products”. We treat the conjuncts that survive filtering as an unordered set, i.e., we ignore their order in the text. The set is discarded and not used by BasiliskC if it only contains one element. 4 Experimental setup and results Experimental setup. We evaluate performance of the two algorithms Basilisk-G and Basilisk-C introduced above. We run experiments on EPO (Section 2) with the goal of learning the classes SUBSTANCE and DISEASE. Our seed set (Figure 1, line 1) consists of the 4223 substances distributed by Ciaramita and Johnson (2003) as part of SuperSenseTagger and 239 diseases extracted from Simple English Wikipedia5 . For Basilisk-C, we extracted 9.7 million unique coordinations, out of a total of 25 million. For Basilisk-G, we found 1.6 billion unique context patterns. In order to be able to run experiments quickly, we introduce frequency thresholds for MWEs, patterns and MWE-pattern combinations. We only consider MWEs and patterns that occur at least θ1 = 10 times and MWE-pattern combinations that occur at least θ2 = 3 times in EPO. These thresholds are unlikely to diminish lexicon quality since many rare instances of"
I13-1188,P07-1030,0,0.0191319,"ly promising resource for lexical bootstrapping in technical domains like patents. However, as exemplified by our experiments with Wikipedia, Basilisk-C shows similar performance on other domains, given that the members of the semantic class appear often in coordinations. 6 Related work We have chosen a semisupervised approach to lexical bootstrapping here since it is reasonable to expect that in the type of application scenario we have in mind resources are available to create a seed set. There are also completely unsupervised approaches to lexical bootstrapping (e.g., Lin and Pantel (2002); Davidov et al. (2007); Van Durme and Pas¸ca (2008); Dalvi et al. (2012)), but they usually cannot match the quality of approaches like ours that use human input such as a seed set. The bootstrapping approach we have adopted here starts with a seed set and then iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and some of which use syntact"
I13-1188,W03-0425,0,0.0302442,"e 1 www.epo.org for the patent domain and a large proportion of patents contain substances. A disease is an abnormal condition that affects the body of an organism. We selected disease as a clearly nontechnical category to be able to investigate potential differences of lexical bootstrapping algorithms for categories with very different properties. Gazetteers are crucial for good performance in machine-learning-based semantic tagging (Ratinov and Roth, 2009), e.g., the best performing systems for recognition of person, location and organization named entities all use gazetteer features (e.g., Florian et al. (2003)). It is in this context that we address the task of bootstrapping lexicons from corpora: for most semantic classes of interest in the patent domain high-coverage lexicons are not available. Evaluation methodology. Since our primary task is semantic tagging, we evaluate the quality of the bootstrapped lexicon directly on this task, i.e., on the task of tagging members of the semantic class in text – rather than evaluating the lexicon in a type-based evaluation as a set of terms without context as most previous work has done. A tagging-based evaluation directly measures what we need for our app"
I13-1188,D10-1008,0,0.0505988,"Missing"
I13-1188,P10-1029,0,0.0147699,"ients) based on Basilisk, that learns from coordinated verbs. This work is focused on verbs with the same patient polarity in binary coordinations extracted from a web corpus. Our approach is based on coordinations of any size from a large patent corpus and focuses on semantic lexicon induction. One distinguishing characteristic of our work is the patent domain. Other work on technical or scientific domains includes press releases of pharmaceutical companies (Phillips and Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude"
I13-1188,W02-1017,0,0.13351,"annot match the quality of approaches like ours that use human input such as a seed set. The bootstrapping approach we have adopted here starts with a seed set and then iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and some of which use syntactic analysis (e.g., Roark and Charniak (1998); Riloff and Jones (1999); Phillips and Riloff (2002)). Our approach does not make use of syntactic analysis but relies on POS patterns. Some recent work attempts to improve Basilisk’s accuracy. Igo and Riloff (2009) enhance precision by checking candidate terms using web queries. Qadir and Riloff (2012) combine Basilisk in an ensemble with an SVM tagger and a coreference resolution system. Our focus is learning technical terminology from very large corpora using coordinations, but any work that improves the accuracy of basic Basilisk could also be beneficial in our setting. Gazetteers are crucial for good performance in named entity recognition"
I13-1188,S12-1028,0,0.110734,". For Basilisk-C, we extracted 9.7 million unique coordinations, out of a total of 25 million. For Basilisk-G, we found 1.6 billion unique context patterns. In order to be able to run experiments quickly, we introduce frequency thresholds for MWEs, patterns and MWE-pattern combinations. We only consider MWEs and patterns that occur at least θ1 = 10 times and MWE-pattern combinations that occur at least θ2 = 3 times in EPO. These thresholds are unlikely to diminish lexicon quality since many rare instances of MWEs are due to OCR errors or failures of our RE-based recognition of NPs (see also (Qadir and Riloff, 2012)). Using the thresholds θ1 and θ2 , 4 This version of Basilisk uses the same RE to detect NPs as Basilisk-G. 5 simple.wikipedia.org/wiki/List of diseases there were 3.2 million unique MWEs, 56 million unique patterns and 121 million unique MWEpattern combinations. This is the raw data we run Basilisk-G on. As discussed in Section 2, our evaluation methodology directly evaluates the semantic lexicon on the task of interest: semantic tagging of patents. The tagging method we use is simple lexicon lookup. While tagging MWEs we exploit the compositional structure of entities by merging adjacent or"
I13-1188,W09-1119,0,0.0106988,"us on the semantic classes SUBSTANCE and DISEASE. A substance is a particular kind of physical matter with uniform properties. Substances are of obvious relevance 1 www.epo.org for the patent domain and a large proportion of patents contain substances. A disease is an abnormal condition that affects the body of an organism. We selected disease as a clearly nontechnical category to be able to investigate potential differences of lexical bootstrapping algorithms for categories with very different properties. Gazetteers are crucial for good performance in machine-learning-based semantic tagging (Ratinov and Roth, 2009), e.g., the best performing systems for recognition of person, location and organization named entities all use gazetteer features (e.g., Florian et al. (2003)). It is in this context that we address the task of bootstrapping lexicons from corpora: for most semantic classes of interest in the patent domain high-coverage lexicons are not available. Evaluation methodology. Since our primary task is semantic tagging, we evaluate the quality of the bootstrapped lexicon directly on this task, i.e., on the task of tagging members of the semantic class in text – rather than evaluating the lexicon in"
I13-1188,W97-0313,0,0.932796,"as}@ims.uni-stuttgart.de Abstract We address the task of bootstrapping a semantic lexicon from a list of seed terms and a large corpus. By restricting to a small subset of semantically strong patterns, i.e., coordinations, we improve results significantly. We show that the restriction to coordinations has several additional benefits, such as improved extraction of multiword expressions, and the possibility to scale up previous efforts. 1 Introduction High-quality semantic lexicons are needed for many natural language processing (NLP) tasks like information extraction and discourse processing (Riloff and Shepherd, 1997). Building such lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and C"
I13-1188,P98-2182,0,0.915716,"herd, 1997). Building such lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and Charniak (1998); Caraballo (1999); Goyal et al. (2010)). Most of this previous work uses a pairwise perspective (i.e., a focus on whether two words cooccur in a coordination). However, we use coordinations in a Basilisk approach, for which patterns contain several terms in general. We therefore do not split up coordinations in pairs but keep the complete coordination intact. Coordinations in technical domains frequently contain more than 2 elements. We argue that bootstrapping methods, known to be particularly sensitive to the ambiguity of terms and contexts and prone to semantic drift, benefit from the stro"
I13-1188,W02-1028,0,0.629108,"ons, we improve results significantly. We show that the restriction to coordinations has several additional benefits, such as improved extraction of multiword expressions, and the possibility to scale up previous efforts. 1 Introduction High-quality semantic lexicons are needed for many natural language processing (NLP) tasks like information extraction and discourse processing (Riloff and Shepherd, 1997). Building such lexicons manually is costly and time-consuming. Automatic lexicon construction is therefore an important task and much prior work has addressed it. This paper adopts Basilisk (Thelen and Riloff, 2002) as its basic approach, a system that uses lexico-syntactic patterns for bootstrapping. We have adapted Basilisk to our setting in several ways. Whereas the original Basilisk covers a wide variety of lexico-syntactic patterns, we restrict ourselves to a specific type of patterns, i.e., coordinations. Coordinations have been exploited in lexical acquisition before (e.g., Roark and Charniak (1998); Caraballo (1999); Goyal et al. (2010)). Most of this previous work uses a pairwise perspective (i.e., a focus on whether two words cooccur in a coordination). However, we use coordinations in a Basili"
I13-1188,W06-2809,0,0.133124,"Missing"
I13-1188,C02-1114,0,0.0470596,"des (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327 our approach because it uses knowledge bases like Wikipedia or is only applicable to traditional named entities (NEs). Traditional NEs like person are capitalized. Substances are not. Our work also differs in its focus on coordinations and technical text. Coordinations have been frequently used in work on lexical acquisition. Caraballo (1999) builds a hierarchy of coordinated nouns and their hypernyms. Cederberg and Widdows (2003) use coordinations to estimate the semantic relatedness of nouns. Widdows and Dorow (2002) and Qiu et al. (2011) cluster nouns and evaluate the semantic homogeneity of the clusters. Etzioni et al. (2005) use Hearst patterns to bootstrap lexicons. They also consider coordinations when selecting candidates. This previous work on coordinations is unsupervised and not focused on learning a particular semantic class that is defined by a seed set. Roark and Charniak (1998) use a variety of syntactic constructions, including coordinations, for bootstrapping. Our approach is different in that we do not require parsing and that we cover MWEs in general, not just heads or compounds with a co"
I13-1188,C02-1154,0,0.0384348,"ical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This restriction to coordinations, a relation that is known to impose strong semantic coherence upon its members and as such a possible remedy for semantic drift, leads to significant improvements for the task of semantic tagging, compared to an unrestricted version of Basilisk. We further extended original Basilis"
I13-1188,C00-2137,0,0.0228439,"Missing"
I13-1188,W09-1703,0,0.0655695,"iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and some of which use syntactic analysis (e.g., Roark and Charniak (1998); Riloff and Jones (1999); Phillips and Riloff (2002)). Our approach does not make use of syntactic analysis but relies on POS patterns. Some recent work attempts to improve Basilisk’s accuracy. Igo and Riloff (2009) enhance precision by checking candidate terms using web queries. Qadir and Riloff (2012) combine Basilisk in an ensemble with an SVM tagger and a coreference resolution system. Our focus is learning technical terminology from very large corpora using coordinations, but any work that improves the accuracy of basic Basilisk could also be beneficial in our setting. Gazetteers are crucial for good performance in named entity recognition (NER). Work on automatic extraction of gazetteers for NER includes (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327"
I13-1188,D07-1073,0,0.0141213,"nt work attempts to improve Basilisk’s accuracy. Igo and Riloff (2009) enhance precision by checking candidate terms using web queries. Qadir and Riloff (2012) combine Basilisk in an ensemble with an SVM tagger and a coreference resolution system. Our focus is learning technical terminology from very large corpora using coordinations, but any work that improves the accuracy of basic Basilisk could also be beneficial in our setting. Gazetteers are crucial for good performance in named entity recognition (NER). Work on automatic extraction of gazetteers for NER includes (Toral and Mu˜noz, 2006; Kazama and Torisawa, 2007). Most of this work is complementary to 1327 our approach because it uses knowledge bases like Wikipedia or is only applicable to traditional named entities (NEs). Traditional NEs like person are capitalized. Substances are not. Our work also differs in its focus on coordinations and technical text. Coordinations have been frequently used in work on lexical acquisition. Caraballo (1999) builds a hierarchy of coordinated nouns and their hypernyms. Cederberg and Widdows (2003) use coordinations to estimate the semantic relatedness of nouns. Widdows and Dorow (2002) and Qiu et al. (2011) cluster"
I13-1188,P08-1119,0,0.0186558,"companies (Phillips and Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This r"
I13-1188,C02-1144,0,0.017373,"seem to be a particularly promising resource for lexical bootstrapping in technical domains like patents. However, as exemplified by our experiments with Wikipedia, Basilisk-C shows similar performance on other domains, given that the members of the semantic class appear often in coordinations. 6 Related work We have chosen a semisupervised approach to lexical bootstrapping here since it is reasonable to expect that in the type of application scenario we have in mind resources are available to create a seed set. There are also completely unsupervised approaches to lexical bootstrapping (e.g., Lin and Pantel (2002); Davidov et al. (2007); Van Durme and Pas¸ca (2008); Dalvi et al. (2012)), but they usually cannot match the quality of approaches like ours that use human input such as a seed set. The bootstrapping approach we have adopted here starts with a seed set and then iteratively extends the lexicon by adding the highest-confidence MWEs in each iteration. Basilisk (Thelen and Riloff, 2002) is perhaps the best known bootstrapping method of this type, but there exists a large literature on similar methods, some of which exploit lexical co-occurrence statistics (e.g., Riloff and Shepherd (1997)) and so"
I13-1188,U08-1013,0,0.0194942,"omplex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in other lexical bootstrapping work on technical domains have been an order of magnitude smaller than ours. We showed that using only coordinations remedies the problem of semantic drift. Other work on semantic drift includes Yangarber et al. (2002); Curran et al. (2007); McIntosh and Curran (2008); McIntosh and Curran (2009). 7 Conclusion In this paper, we presented Basilisk-C. The method is inspired by original Basilisk but adapts it to large corpora of technical text by restricting it to one type of patterns: coordinations. This restriction to coordinations, a relation that is known to impose strong semantic coherence upon its members and as such a possible remedy for semantic drift, leads to significant improvements for the task of semantic tagging, compared to an unrestricted version of Basilisk. We further extended original Basilisk to include MWEs, as these are predominant in tec"
I13-1188,P09-1045,0,0.0195536,"patient polarity verbs (i.e., verbs that impart positive or negative states on their patients) based on Basilisk, that learns from coordinated verbs. This work is focused on verbs with the same patient polarity in binary coordinations extracted from a web corpus. Our approach is based on coordinations of any size from a large patent corpus and focuses on semantic lexicon induction. One distinguishing characteristic of our work is the patent domain. Other work on technical or scientific domains includes press releases of pharmaceutical companies (Phillips and Riloff, 2002), medline abstracts (McIntosh and Curran, 2009), message board posts from the Veterinary Information Network (Huang and Riloff, 2010) and texts from ProMed and PubMed (Igo and Riloff, 2009; Qadir and Riloff, 2012). Patents can be argued to be particularly difficult technical text due to long sentences, legalese and complex NP syntax. To the best of our knowledge, our experiments are also the largest semantic bootstrapping experiments on technical text to date. While there has been much work on experiments on large web corpora and other general text (e.g., Kozareva et al. (2008); Carlson et al. (2009); Bakalov et al. (2011)), the corpora in"
J17-4005,E89-1001,0,0.360581,"lable during parsing. Some studies have shown the great interest of performing MWE identification during syntactic parsing. We henceforth also use the term “joint” to refer to such approaches. Joint Grammar-Based Approaches. In a grammar-based parser, MWE identification is often integrated in the grammar. MWEs are generally found in a lexical resource, and parsers embody mechanisms to link MWE entries to grammar rules, as in Abeillé (1995) for TAG, Attia (2006) for LFG, and Copestake et al. (2002) and Villavicencio et al. (2007) for HPSG. For instance, in the TAG paradigm, Abeillé and Schabes (1989) link MWE lexical entries to tree rules that are anchored by multiple components present in the lexical entry. This mechanism has been integrated in the XTAG project that aims to construct a lexicalized TAG for English (XTAG 2001). In practice, rule-based parsers can also use MWE identification as a cue to locally select the best syntactic analysis: for instance, Wehrli (2014) applies heuristics favoring MWE analyses. Where statistical grammar-based parsers are trained from a reference treebank, MWEs must be annotated within the treebank. Typically, each MWE is annotated with a specific subtre"
J17-4005,W03-1812,0,0.035785,"s consider the items cat, dog, hot dog, and sandwich. We would expect that dog is similar to cat, dog is not similar to hot dog, and hot dog is similar to sandwich. Semantic similarity methods differ mainly in how they represent word and MWE senses, how they combine senses, and how they measure similarity. Word and MWE senses can be modeled using entries of semantic lexicons like WordNet synsets (McCarthy, Venkatapathy, and Joshi 2007). However, most discovery methods use distributional models (or word embeddings) instead, where senses are represented as vectors of co-occurring context words (Baldwin et al. 2003; Korkontzelos 2011). The creation of such vectors in distributional models has several parameters that affect the performance of MWE discovery, such as the number of vector dimensions and the type of context window (Cordeiro et al. 2016). The evaluation of discovery methods based on distributional similarity can use dedicated test sets (Reddy, McCarthy, and Manandhar 2011; Farahmand and Henderson 2016) or use handbuilt resources such as WordNet (Baldwin et al. 2003). Because methods based on distributional semantics use contextual information to represent meaning, they are closely related to"
J17-4005,2013.mtsummit-wmwumttt.5,1,0.923746,"BMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language texts such as parse trees (Wehrli et al. 2009) or intermediate representation languages like minimal recursion semantics (Oepen et al. 2004), a semantico-syntactic abstraction language (Monti et al. 2011; Barreiro et al. 2013). Transfer rules handle MWE variability and discontiguity (Forcada et al. 2011) and are manually defined or automatically learned from parallel corpora (Haugereid and Bond 2011). Discontiguous or variable MWEs represent an important source of translation errors. These methods have the advantage of handling discontiguous or variable MWEs with the help of rules for RBMT or by completing word alignments dynamically in SMT. 5.3 Evaluation of MWE-Aware MT The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted (Monti et al. 2012; Ramisch, Besacier, and"
J17-4005,E06-1042,0,0.0535441,"res. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard features. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs. Their classifier considers an expression as literal if its presence in the sentence does not have a negative impact on cohesion, defined as the similarity between co-occurring words. For instance, play with fire reinforces cohesion"
J17-4005,H91-1060,0,0.0613814,"Survey of a parser generating the n-best parses (including MWE identification) and showed significant improvement in MWE identification accuracy. 4.3 Evaluation of MWE-Aware Parsing Evaluating a syntactic parser generally consists of comparing the output to reference (gold standard) parses from a manually labeled treebank. In the case of constituency parsing, a constituent is treated as correct if there exists a constituent in the gold standard parse with the same labels and starting and ending points. These parsers are traditionally evaluated through precision, recall, and F-measure metrics (Black et al. 1991; Sekine and Collins 1997). In standard dependency parsing with the single-head constraint,23 the number of dependencies produced by the parser should be equal to the number of total dependencies in the gold-standard parse tree. Common metrics to evaluate these parsers include the percentage of tokens with correct head, called unlabeled attachment score (UAS), and the percentage of tokens with correct head and dependency label, called labeled attachment score (LAS) (Buchholz and Marsi 2006; Nilsson, Riedel, and Yuret 2007). The evaluation of identification and discovery has been discussed in p"
J17-4005,W06-1620,0,0.021192,"Missing"
J17-4005,bonial-etal-2014-propbank,0,0.0382152,"Missing"
J17-4005,W12-5108,0,0.0604679,"Missing"
J17-4005,bouamor-etal-2012-identifying,0,0.0607539,"Missing"
J17-4005,D09-1049,0,0.0657262,"Missing"
J17-4005,W05-1501,0,0.0707877,"Missing"
J17-4005,C96-2182,0,0.467758,"Missing"
J17-4005,P98-1030,0,0.271352,"search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a single token (e.g., by the way → by_the_way). MWE identification prior to parsing has been implemented both in statistical (Cafferkey, Hogan, and van Genabith 2007; Korkontzelos and Manandhar 2010; Constant, Sigogne, and Watrin 2012; de Lhoneux 2015) and rule-based parsers (Brun 1998; Mamede et al. 2012). This orchestration type has the advantage of simplicity and empirical efficiency. For instance, Cafferkey, Hogan, and van Genabith (2007) show that pre-identifying multiword named entities and prepropositional MWEs improves parsing accuracy in the constituent framework. The best system of the track on MWE-aware dependency parsing in the SPMRL 2013 shared task (Seddah et al. 2013) was the only one that included deterministic pre-identification (Constant, Candito, and Seddah 2013). Limitations. The pre-identification approach suffers from limitations. First, in this scenar"
J17-4005,W06-2920,0,0.0923239,"lose one’s mind’),16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-distance discontiguity. Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. Hashimoto, Sato, and Utsuro (2006) encode in a lexicon detailed properties of 100 Japanese verb-noun ambiguous idioms such as voice, adnominal modifications, modality, and selectional restrictions. Then, they only classify as idioms those occurrences that match the constraints in a dependency-parsed test set. More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup (Carpuat and Diab 2010; Ghoneim and Diab 2013). The lookup relies on a maximum forwar"
J17-4005,W13-1003,0,0.0193097,"e-to-many alignments to solve translation asymmetries (Melamed 1997; Carpuat and Diab 2010; Okita 2012). Word alignment completion is based on simple word alignment and on MWE identification tools, designed for specific MWE categories (Tan and Pal [2014] for multiword named entities; Bouamor, Semmar, and Zweigenbaum [2012b] and Okita and Way [2011] for terms; Ramisch, Villavicencio, and Boitet [2010] for general MWEs). Alternatively, MWE identification and alignment is performed using bilingual lexical resources, with translation alongside an n-gram language model to help with disambiguation (Bungum et al. 2013). The resulting many-to-many word alignment is used to retrain the system in order to build a new phrase table. As a consequence, the phrase table takes into account MWEs and their translations. Alternatively, bilingual dictionaries of MWEs are added as additional training data to the parallel corpus (Babych and Hartley 2010; Tan and Pal 2014). Modifying the phrase table. Usually, a bilingual list of MWEs and their equivalents is dynamically extracted from the simple word alignment using specific MWE discovery tools (Bouamor, Semmar, and Zweigenbaum 2012b; Kordoni and Simova 2012; Pal, Naskar,"
J17-4005,calzolari-etal-2002-towards,0,0.0359057,"n aim of this survey is to shed light on how MWEs are handled in NLP applications. More particularly, it tries to clarify the nature of interactions between MWE processing and downstream applications such as MWE-aware parsing and MT. There is no shortage of proposed approaches for MWE processing and MWE-aware NLP applications. In fact, it is the emergence of approaches in the absence of guiding principles that motivates this article. There have been other surveys and reviews about MWEs with different scopes. Some concentrate primarily on their linguistic characteristics (Mel’ˇcuk et al. 1999; Calzolari et al. 2002; Sag et al. 2002; Wray 2002). Although this is a valid area of linguistic research, it is not of primary interest to researchers who are addressing the design of computational solutions to the spectrum of problems that MWEs bring into focus. Others are bibliographical reviews/state-of-the-art overviews done in the context of Ph.D. theses (Evert 2005; Pecina 2008) or book chapters (Manning and Schütze 1999; McKeown and Radev 1999; Baldwin and Kim 2010; Seretan 2011; Ramisch 2015), with a narrow scope focusing only on a specific part of MWE processing. In these studies, the subject area is rele"
J17-4005,W11-3806,0,0.348073,"lve word reordering: John gave it up → John gave_up it. In addition, when MWE components are concatenated into a single token, their internal syntactic structure is lost, whereas it may be required for the semantic processing of semi-compositional MWEs. However, this can be performed a posteriori, for instance, by applying simple rules based on POS patterns (Candito and Constant 2014). Then, the retokenization increases data sparsity that negatively affects parsing performance, because the vocabulary size increases whereas the total amount of training ˙ data is the same. Eryigit, ˘ Ilbay, and Can (2011) showed that the concatenation operation of different MWE categories has different impacts on parsing performance for Turkish. Whereas retokenization of multiword named entities and numerical expressions 865 Computational Linguistics Volume 43, Number 4 improved dependency parsing performance, retokenization of light-verb constructions harmed it. Another disadvantage is that pre-identification is deterministic, so the syntactic parser cannot recover from MWE identification errors. A sentence like He walked by and large tractors passed him cannot be analyzed correctly if by and large is pre-ana"
J17-4005,W16-1809,0,0.299487,"syntactic permutations by reordering words inside the MWE, combining frequencies using an entropy measure. Artificially generated variants can be transformed into features for supervised discovery methods, as we will see in Section 2.2.4 (Lapata and Lascarides 2003; Ramisch et al. 2008a). Methods based on variant generation and/or lookup were used to discover several MWE categories, such as English verb-particle constructions (McCarthy, Keller, and Carroll 2003; Ramisch et al. 2008b), English verb-noun idioms (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007), English noun compounds (Farahmand and Henderson 2016), and German noun-verb and noun-PP idioms (Weller and Heid 2010). Such methods often require external lexicons or grammars describing possible variants, like synonym lists or local reorderings (e.g., Noun1 Noun2 → Noun2 of Noun1 ). Synonyms or related words in substitution methods can come from thesauri like a WordNet and VerbNet (Pearce 2001; Ramisch et al. 2008b). Related words can be found in automatically compiled thesauri built using distributional vectors (Riedl and Biemann 2015; Farahmand and Henderson 2016). When compared with association measures, most of these methods are hard to gen"
J17-4005,J09-1005,0,0.306126,"Missing"
J17-4005,E06-1043,0,0.235234,"lose one’s mind’),16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-distance discontiguity. Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. Hashimoto, Sato, and Utsuro (2006) encode in a lexicon detailed properties of 100 Japanese verb-noun ambiguous idioms such as voice, adnominal modifications, modality, and selectional restrictions. Then, they only classify as idioms those occurrences that match the constraints in a dependency-parsed test set. More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup (Carpuat and Diab 2010; Ghoneim and Diab 2013). The lookup relies on a maximum forwar"
J17-4005,N09-1037,0,0.183694,"introduced by preposition de (of, from). Discussion. Joint approaches are of great interest for MWEs having syntactic vari˙ ability. In particular, Eryigit, ˘ Ilbay, and Can (2011) and Vincze, Zsibrita, and Nagy (2013) state that a joint approach using a dependency parser is very successful for the identification of light-verb constructions in Turkish and in Hungarian, respectively. Nonetheless, such approaches have the inconvenience of complicating the parsing stage through an increase in the size of the label sets. For instance, the literature shows 21 Also of note is the work of Finkel and Manning (2009), which is limited to named entity recognition and constituent parsing: They jointly performed both tasks using a parser based on conditional random fields, combining features specific to both tasks. The experimental results showed that the accuracy of both tasks increased. 867 Computational Linguistics Volume 43, Number 4 mixed results for non-compositional open compounds for which a pre-identification approach is sometimes more accurate than a joint one (Constant and Nivre 2016). The right balance therefore has to be found. An interesting way to deal with this issue is to combine the before"
J17-4005,W11-0805,0,0.163378,"Missing"
J17-4005,S12-1017,0,0.0264325,"zaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard feat"
J17-4005,W10-1734,0,0.0311993,"word Schwiegereltern (parentsin-law) or non-lexicalized, that is, the individual words keep their meanings when combined, for instance, the German neologism Helikoptereltern (helicopter parents). They are usually translated into several target language words. Their meaning might be more or less compositional. MT systems fail to correctly translate these compounds because of their low frequencies and their variability. Moreover, non-compositional compounds have unpredictable meaning. Splitting strategies can be applied to cut the compounds into subsequent words to improve translation quality (Fritzinger and Fraser 2010; Stymne, Cancedda, and Ahrenberg 2013). Splitting is done by identifying component words in the corpus or by prefix and suffix identification together with distributional semantics (Weller et al. 2014) or by using a morphosyntactic tagger and parser (Cap et al. 2014). Oversplitting can also be a problem: Splitting non-compositional compounds may generate erroneous translations. Some methods aim to distinguish between compositional and non-compositional compounds and split only the compositional ones (Weller et al. 2014). A postprocessing step is required to merge components back into compound"
J17-4005,N06-2011,0,0.0164964,"Missing"
J17-4005,I13-1168,0,0.0605546,"Missing"
J17-4005,D11-1067,0,0.084647,"Missing"
J17-4005,J13-1009,0,0.0702486,"Missing"
J17-4005,D08-1104,0,0.0324534,"instead focus on detecting which of these are true MWEs. We discuss both supervised and unsupervised classifiers. Uchiyama, Baldwin, and Ishizaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses"
J17-4005,P06-2046,0,0.0188686,"Missing"
J17-4005,W11-0814,0,0.0259244,"Missing"
J17-4005,W10-1761,0,0.0300946,"Missing"
J17-4005,W09-2905,0,0.0632833,"Missing"
J17-4005,L16-1629,0,0.0216352,"Missing"
J17-4005,N15-2005,1,0.837894,"Missing"
J17-4005,D13-1176,0,0.0140378,"Chiang 2007; Hoang and Koehn 2010), or linguistic annotation layers in factorbased SMT (Koehn and Hoang 2007)). Phrase-based SMT and its variants build phrase tables—that is, a list of source fragments (words, phrases, subtrees), their translations, and their translation probabilities 870 Constant et al. MWE Processing: A Survey that take into account word sequences, not only simple words. In principle, therefore, such systems can naturally handle contiguous MWEs. Whether they can handle them correctly in all cases is, of course, a separate question. More recently, neural machine translation (Kalchbrenner and Blunsom 2013; Cho et al. 2014) proposes alternative methods to compute translation probabilities, by using recurrent neural networks to model the translation task. Most neural translation systems use an encoder–decoder architecture. The input sentence is encoded into a fixedlength or variable–length vector and then one or more decoders use this representation to obtain the target sentence. The probability of the translation of one word is computed on the basis of the translation probabilities of previous words. An attention model is frequently used to represent larger contexts for the translated words and"
J17-4005,W06-1203,0,0.045459,"ndicates any item whose part of speech is not prefixed by V; [lemma!=there] indicates any item whose lemma is not the string there. Every lexical item occurs once by default. In case of potential multiple occurrences, it is followed by a repeat feature (between curly brackets) indicating the number of times it can occur as a range (here, [pos!∼/V.*/] can occur between 0 and 5 times). 19 http://intex.univ-fcomte.fr, http://macaon.lif.univ-mrs.fr, http://www.nooj4nlp.net, http://alpage.inria.fr/~sagot/sxpipe.html, http://www-igm.univ-mlv.fr/~unitex/. 858 Constant et al. MWE Processing: A Survey Katz and Giesbrecht (2006), detecting idiomatic verb-noun expressions in German, assume that the context of an idiomatic MWE differs from the contexts of its literal uses. Given two distributional vectors representing literal and idiomatic instances, a test instance is classified according to its similarity to the respective vectors. Cook, Fazly, and Stevenson (2007) propose a similar method based on canonical forms learned automatically from large corpora. Once a canonical form is recognized, distributional vectors for canonical and non-canonical forms are learned and then an instance is classified as idiomatic if it"
J17-4005,J03-3005,0,0.0173438,"s take into account their contingency table. Examples of such measures are χ2 and the more robust likelihood ratio (Dunning 1993). Pedersen (1996) suggests using Fisher’s exact test in automatic MWE discovery, and this measure is implemented among others in the Text:NSP package.13 Another measure for MWE discovery is the average and standard deviation of the distance between words, implemented in Xtract (Smadja 1993). Because these measures are based on frequency counts, there have been some studies to use Web hits as an alternative to corpus counts, in order to avoid low-frequency estimates (Keller and Lapata 2003; Ramisch et al. 2008a). Although association measures work quite well for two-word expressions, they are hard to generalize to arbitrary n-word MWE candidates. One simple approach is to merge two-word MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, ’to make a blunder’) can be modeled as the verb faire (to make) combined with the compound faux_pas (blunder), which had been merged due to high association in a previous pass (Seretan 2011). The LocalMaxs algorithm finds optimal MWE boundaries by recursive"
J17-4005,2010.eamt-1.27,0,0.0204193,"Missing"
J17-4005,D07-1091,0,0.0396832,"Missing"
J17-4005,N03-1017,0,0.058843,"Missing"
J17-4005,kordoni-simova-2014-multiword,0,0.0556978,"Missing"
J17-4005,N10-1089,0,0.0109352,"ation measures. 4.2.2 Identification Before Parsing. When MWE identification is performed before parsing, the search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a single token (e.g., by the way → by_the_way). MWE identification prior to parsing has been implemented both in statistical (Cafferkey, Hogan, and van Genabith 2007; Korkontzelos and Manandhar 2010; Constant, Sigogne, and Watrin 2012; de Lhoneux 2015) and rule-based parsers (Brun 1998; Mamede et al. 2012). This orchestration type has the advantage of simplicity and empirical efficiency. For instance, Cafferkey, Hogan, and van Genabith (2007) show that pre-identifying multiword named entities and prepropositional MWEs improves parsing accuracy in the constituent framework. The best system of the track on MWE-aware dependency parsing in the SPMRL 2013 shared task (Seddah et al. 2013) was the only one that included deterministic pre-identification (Constant, Candito, and Seddah 2013). Limi"
J17-4005,2005.mtsummit-posters.11,0,0.058389,"ble takes into account MWEs and their translations. Alternatively, bilingual dictionaries of MWEs are added as additional training data to the parallel corpus (Babych and Hartley 2010; Tan and Pal 2014). Modifying the phrase table. Usually, a bilingual list of MWEs and their equivalents is dynamically extracted from the simple word alignment using specific MWE discovery tools (Bouamor, Semmar, and Zweigenbaum 2012b; Kordoni and Simova 2012; Pal, Naskar, and Bandyopadhyay 2013). Then, the phrase table is completed with the bilingual lists of MWEs and the probabilities are modified accordingly (Lambert and Banchs 2005) or added into a new phrase table with the probability set to 1 (Ren et al. 2009). An alternate strategy consists of adding new features in the phrase table, such as the number of MWEs present in the bilingual aligned phrases (Carpuat and Diab 2010) or the property that the parallel phrase contains a bilingual MWE (Ren et al. 2009). In this way, the translation quality is improved for certain specific MWE categories or languages (Costa-Jussà, Daudaravicius, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word tr"
J17-4005,E03-1073,0,0.14049,"Missing"
J17-4005,C14-1177,0,0.156618,"Missing"
J17-4005,D13-1116,0,0.0259179,"Missing"
J17-4005,W16-1810,0,0.157295,"Missing"
J17-4005,L16-1364,1,0.862298,"Missing"
J17-4005,J93-2004,0,0.0584036,"Missing"
J17-4005,W10-3713,0,0.193741,"lustration, Baldwin (2005) proposes different morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. In particular, he shows the effect of using the outputs of a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined via a classifier. It experimentally appears that the ensemble method significantly outperforms the individual performances. As for individual scores, the use of shallow syntactic information like chunks tends to be prevalent. It is also possible to use pattern-free approaches like Martens and Vandeghinste (2010) and Sangati and van Cranenburgh (2015), who propose discovery methods not dedicated to a specific MWE category but based on recurring tree fragments and association measures. 4.2.2 Identification Before Parsing. When MWE identification is performed before parsing, the search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a singl"
J17-4005,D10-1004,0,0.0384566,"Missing"
J17-4005,W03-1810,0,0.0220181,"Missing"
J17-4005,D07-1039,0,0.0853842,"Missing"
J17-4005,H05-1066,0,0.0219995,"Missing"
J17-4005,W97-0311,0,0.0813059,"hrase (Carpuat and Diab 2010). Several observed approaches are: (1) changing the training data dynamically (word alignment or the parallel corpus) to take into account MWEs and then retraining the system; (2) modifying the phrase table directly by including information about MWEs and their translations. In both strategies, the use of MWE identification and discovery tools is essential to improve the quality of the translation. Modifying training data. A frequent strategy completes simple word alignment with many-to-many, many-to-one, or one-to-many alignments to solve translation asymmetries (Melamed 1997; Carpuat and Diab 2010; Okita 2012). Word alignment completion is based on simple word alignment and on MWE identification tools, designed for specific MWE categories (Tan and Pal [2014] for multiword named entities; Bouamor, Semmar, and Zweigenbaum [2012b] and Okita and Way [2011] for terms; Ramisch, Villavicencio, and Boitet [2010] for general MWEs). Alternatively, MWE identification and alignment is performed using bilingual lexical resources, with translation alongside an n-gram language model to help with disambiguation (Bungum et al. 2013). The resulting many-to-many word alignment is u"
J17-4005,P12-1082,0,0.0585527,"Missing"
J17-4005,2011.freeopmt-1.4,1,0.821234,"decoding phase and helping disambiguation. More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process 876 Constant et al. MWE Processing: A Survey (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs. RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language"
J17-4005,2010.amta-srw.2,0,0.00843731,"dding new features in the phrase table, such as the number of MWEs present in the bilingual aligned phrases (Carpuat and Diab 2010) or the property that the parallel phrase contains a bilingual MWE (Ren et al. 2009). In this way, the translation quality is improved for certain specific MWE categories or languages (Costa-Jussà, Daudaravicius, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word translation during the decoding phase and helping disambiguation. More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process 876 Constant et al. MWE Processing: A Survey (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntac"
J17-4005,W14-0803,0,0.0495089,"Missing"
J17-4005,P15-1108,1,0.945817,"key role in syntax may be ambiguous (e.g., up to). For instance, in John looked up to the sky, the sequence up to should not be identified as a multiword preposition. If so, it would prevent the right analysis: (John) ((looked) (up to the sky)) instead of (John) ((looked) (up) (to the sky)). Conversely, combining MWE identification and parsing can help resolve such ambiguities, yielding both better identification and parsing models. Multiword function words such as complex prepositions, conjunctions, and adverbials (up to, now that, by the way) can be disambiguated by their syntactic context (Nasr et al. 2015). For example, the sequence de la in French can be either a compositional sequence (preposition de + determiner la), or a complex partitive determiner, as shown in the following examples and their corresponding syntactic analyses in Figure 5: la voiture (1) Je parle de I talk about the car (2) Je mange de la soupe I eat some soup MWE-aware parsing is a natural way to solve this ambiguity. The intransitive verb parle (talk) selects the preposition de (about), whereas mange (eat) requires a noun phrase as its object. Furthermore, one of the main challenges of parsing in general is attachment amb"
J17-4005,J14-2001,0,0.0292739,"Missing"
J17-4005,P00-1056,0,0.382568,"Missing"
J17-4005,2004.tmi-1.2,0,0.198054,"account fine linguistic descriptions (Sennrich and Haddow 2016). Neural machine translation obtains impressive improvements of the evaluation scores such as BLEU (Wu et al. 2016). Rule-based machine translation (RBMT) uses large lexicons and explicit rules describing the syntactic and semantic constraints on both the source and the target language. Transfer rules are used to map source language structures to target language ones and to identify the right translation. These rules are based on formal grammars or intermediate language-independent structures (such as minimal recursion semantics [Oepen et al. 2004]) capable of generating correct translation equivalents. Finally, example-based machine translation (EBMT) is based mainly on examples in the form of large translation memories (large collections of source/target sentence pairs) but also uses rules to acquire new linguistic knowledge dynamically. EBMT is based on a translation by analogy approach, where at run time translations are obtained by looking up and using examples stored in translation memories. The translation process is organized in three stages: (i) matching of input sentences with translations previously stored, (ii) retrieval of"
J17-4005,W04-0409,0,0.240092,"r verbal inflection paradigm. The inflection process may be based on finite-state transducers as in Silberztein (1997), possibly augmented with a unification mechanism for handling agreement between the MWE components (Savary 2009). These approaches are extremely precise, but costly. The manual assignment of inflection rules may be eased by tools like Leximir for predicting inflection classes (Krstev et al. 2013). Another approach comprises two processing stages: morphological analysis of simple words followed by a composition of regular rules to identify MWEs, as in Oflazer, Çetinoglu, ˘ and Say (2004) for Turkish. Breidt, Segond, and Valetto (1996) design regular rules that handle morphological variations and restrictions like the French idiom perdre ADV* :la :tˆete (lit. lose ADV* :the :head, ’to lose one’s mind’),16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-di"
J17-4005,W13-2814,0,0.0239032,"Missing"
J17-4005,P02-1040,0,0.118219,"source of translation errors. These methods have the advantage of handling discontiguous or variable MWEs with the help of rules for RBMT or by completing word alignments dynamically in SMT. 5.3 Evaluation of MWE-Aware MT The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted (Monti et al. 2012; Ramisch, Besacier, and Kobzar 2013; Barreiro et al. 2014), because of a lack of shared assessment methodologies, benchmarking resources, and annotation guidelines. With reference to the assessment methodologies, automatic evaluation metrics such as BLEU (Papineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010; Stymne, Cancedda, and Ahrenberg 2013; Salehi et al. 2015)"
J17-4005,pearce-2002-comparative,0,0.0450507,"s by recursively including left and right context words, stopping when the association decreases (da Silva et al. 1999).14 A similar approach, using a lexical tightness measure, was proposed to segment Chinese MWEs (Ren et al. 2009). Association measures can be adapted according to the morphosyntactic nature of lexical elements. Hoang, Kim, and Kan (2009) propose new measures where very frequent words such as prepositions are weighted differently from regular tokens. Comparisons between different association measures have been published, but to date no single best measure has been identified (Pearce 2002; Evert 2005; Pecina 2008; Ramisch, De Araujo, and Villavicencio 2012). 2.2.2 Substitution and Insertion. A French kiss cannot be referred to as a kiss that is French, a kiss from France, or a French smack, unlike non-MWE combinations like French painter and passionate kiss. Because of their non-compositionality, MWEs exhibit nonsubstitutability, that is, limited morphosyntactic and semantic variability. Thus, the replacement or modification of individual words of an MWE often results in unpredictable meaning shifts or invalid combinations. This property is the basis of discovery methods based"
J17-4005,P15-1031,0,0.0418764,"Missing"
J17-4005,racz-etal-2014-4fx,0,0.0382637,"Missing"
J17-4005,2013.mtsummit-wmwumttt.8,0,0.0747715,"Missing"
J17-4005,W12-3301,1,0.804561,"Missing"
J17-4005,ramisch-etal-2010-mwetoolkit,1,0.876983,"Missing"
J17-4005,W08-2107,1,0.876823,"or a French smack, unlike non-MWE combinations like French painter and passionate kiss. Because of their non-compositionality, MWEs exhibit nonsubstitutability, that is, limited morphosyntactic and semantic variability. Thus, the replacement or modification of individual words of an MWE often results in unpredictable meaning shifts or invalid combinations. This property is the basis of discovery methods based on substitution and insertion (including permutation, syntactic alternations, etc.). 12 For more details on association measures, see http://www.collocations.de, Evert (2005), and Pecina (2008). 13 http://search.cpan.org/dist/Text-NSP/. 14 http://research.variancia.com/multiwords2/. 850 Constant et al. MWE Processing: A Survey Pearce’s (2001) early synonym substitution method replaces parts of the MWE by synonyms obtained from WordNet, and then obtains frequencies for the artificially generated MWE variants from external sources. Instead of using variant frequencies directly, it is possible to estimate an MWE candidate’s frequency using a weighted sum of variant corpus frequencies (Lapata and Lascarides 2003) or Web-based frequencies (Keller and Lapata 2003). A similar approach is u"
J17-4005,W95-0107,0,0.118037,"ing the insertion of variable components. The overall impact of discontiguity is language-dependent. For example, separable verb-particle constructions, frequent in Germanic languages, are almost non-existent in Romance languages. 855 Computational Linguistics Volume 43, Number 4 Overlaps. A discontiguous MWE can have other nested MWEs in between its components, like dirty word as the direct object of look up. This is especially problematic for systems that use IOB encoding, as exemplified in row 3, which addresses the segmentation problem with tags B, for begin, I for inside, O, for outside (Ramshaw and Marcus 1995). Often, nesting demands multi-level tags, otherwise different segments could be mixed up. For instance, word and up would have subsequent I tags, although they are not part of the same MWE. In the example, outer MWEs use capital IOB tags and inner MWEs use lowercase iob tags, following the tagging scheme proposed by Schneider et al. (2014a). Nesting is a particular case of overlap, whereby MWEs can also share tokens in a sentence. For instance, the verb-particle construction to let out can be contained in the idiom to let the cat out of the bag. If the MWE tagger cannot output more than one M"
J17-4005,I11-1024,0,0.0169712,"Missing"
J17-4005,W09-2907,0,0.0591534,"MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, ’to make a blunder’) can be modeled as the verb faire (to make) combined with the compound faux_pas (blunder), which had been merged due to high association in a previous pass (Seretan 2011). The LocalMaxs algorithm finds optimal MWE boundaries by recursively including left and right context words, stopping when the association decreases (da Silva et al. 1999).14 A similar approach, using a lexical tightness measure, was proposed to segment Chinese MWEs (Ren et al. 2009). Association measures can be adapted according to the morphosyntactic nature of lexical elements. Hoang, Kim, and Kan (2009) propose new measures where very frequent words such as prepositions are weighted differently from regular tokens. Comparisons between different association measures have been published, but to date no single best measure has been identified (Pearce 2002; Evert 2005; Pecina 2008; Ramisch, De Araujo, and Villavicencio 2012). 2.2.2 Substitution and Insertion. A French kiss cannot be referred to as a kiss that is French, a kiss from France, or a French smack, unlike non-MWE"
J17-4005,C92-2065,0,0.422926,"s comprises two phases: an analysis phase, generating a set of possible syntactic trees, followed by a disambiguation phase based on heuristics (Boullier and Sagot 2005; Wehrli 2014) or statistical models (Riezler et al. 2002; Villemonte De La Clergerie 2013). In the second (mainstream) strategy, the grammar is accompanied by a statistical model. For instance, parsers based on generative-models assign probabilities to rules of an underlying grammatical formalism, as in probabilistic context-free grammars (PCFGs) (Charniak and Johnson 2005), tree-substitution grammars (Green et al. 2011), TAG (Resnik 1992), and LFG (Cahill 2004). The parsing algorithms generally rely on dynamic programming. They usually include one pass, but two-pass processes also exist. For instance, Charniak and Johnson (2005) successfully propose applying a discriminative reranker to the n-best parses produced by a generative PCFG-based parser. Grammarless parsing is performed without any underlying grammar and is based on discriminative approaches. It uses machine learning techniques only, mainly (not exclusively) in the dependency framework. The different parsing algorithms vary from local search approaches, such as trans"
J17-4005,D15-1290,0,0.181527,"h verb-noun idioms (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007), English noun compounds (Farahmand and Henderson 2016), and German noun-verb and noun-PP idioms (Weller and Heid 2010). Such methods often require external lexicons or grammars describing possible variants, like synonym lists or local reorderings (e.g., Noun1 Noun2 → Noun2 of Noun1 ). Synonyms or related words in substitution methods can come from thesauri like a WordNet and VerbNet (Pearce 2001; Ramisch et al. 2008b). Related words can be found in automatically compiled thesauri built using distributional vectors (Riedl and Biemann 2015; Farahmand and Henderson 2016). When compared with association measures, most of these methods are hard to generalize, as they model specific limitations that depend on the language and MWE category. 2.2.3 Semantic Similarity. Models based on semantics account for the fact that many MWE categories are partly or fully non-compositional. Because the meaning of the parts does not add up to the meaning of the whole, there should be little similarity between the computational-semantic representation of MWEs and of words that constitute them. For instance, let us consider the items cat, dog, hot do"
J17-4005,W16-1816,0,0.155422,"Missing"
J17-4005,P02-1035,0,0.175342,"Missing"
J17-4005,W15-0908,1,0.841812,"Missing"
J17-4005,L16-1368,0,0.056784,"Missing"
J17-4005,N15-1099,0,0.0118336,"pineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010; Stymne, Cancedda, and Ahrenberg 2013; Salehi et al. 2015). Extrinsic evaluations in MT have also been performed, mainly for SMT. For instance, Carpuat and Diab (2010) conducted a pilot study for a task-oriented evaluation of MWE translation in SMT, whereas Bouamor, Semmar, and Zweigenbaum (2012a) consider SMT as an extrinsic evaluation of the usefulness of automatically discovered MWEs and explore strategies for integrating them in a SMT system, aiming at a more thorough error analysis of MWE translation. Another important drawback in this field is represented by the fact that parallel corpora annotated with MWEs, which are important and necessary g"
J17-4005,W15-0909,0,0.0305586,"pineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010; Stymne, Cancedda, and Ahrenberg 2013; Salehi et al. 2015). Extrinsic evaluations in MT have also been performed, mainly for SMT. For instance, Carpuat and Diab (2010) conducted a pilot study for a task-oriented evaluation of MWE translation in SMT, whereas Bouamor, Semmar, and Zweigenbaum (2012a) consider SMT as an extrinsic evaluation of the usefulness of automatically discovered MWEs and explore strategies for integrating them in a SMT system, aiming at a more thorough error analysis of MWE translation. Another important drawback in this field is represented by the fact that parallel corpora annotated with MWEs, which are important and necessary g"
J17-4005,W14-0806,0,0.0203644,"Missing"
J17-4005,W15-0902,0,0.0491953,"Missing"
J17-4005,Q14-1016,0,0.0745576,"Missing"
J17-4005,S16-1084,0,0.39778,"al. 2012). For example, whereas the syntactic structure within light-verb constructions such as to make a decision is annotated with special MWE relations in the Prague Treebank, they are annotated as regular verb–object pairs in the Penn Treebank. Although, at the time of writing this survey, there is still not a universal standard for MWE annotation, one of the main goals of the PARSEME network is to develop annotation guidelines for MWE representation in both constituency and dependency treebanks. For an up-to-date status of the current annotations for different languages, see Rosén et al. (2015, 2016). Appendix A provides a complementary list of resources and tools for MWE processing. Identification relies on lexical resources that can be either the fruit of discovery or hand-built. Both parsing and MT rely on lexical resources as well, either through a separate identification step or by using them internally. For example, MWE lexicons are important for MT within preprocessing, postprocessing, and translation phases of different paradigms: They are mainly used to delimit MWEs, replacing them by either a single token, a sense identifier, or by a translation equivalent before alignment takes"
J17-4005,schneider-etal-2014-comprehensive,0,0.0968038,"Missing"
J17-4005,W14-0821,0,0.0606462,"Missing"
J17-4005,W13-4917,0,0.0380886,"Missing"
J17-4005,W16-2209,0,0.016354,"fixedlength or variable–length vector and then one or more decoders use this representation to obtain the target sentence. The probability of the translation of one word is computed on the basis of the translation probabilities of previous words. An attention model is frequently used to represent larger contexts for the translated words and sentences. Indeed, attention models represent source word and larger-context words (using a dot product of vectors or multilayer perceptrons) to generate a target word. Few neural machine translation systems take into account fine linguistic descriptions (Sennrich and Haddow 2016). Neural machine translation obtains impressive improvements of the evaluation scores such as BLEU (Wu et al. 2016). Rule-based machine translation (RBMT) uses large lexicons and explicit rules describing the syntactic and semantic constraints on both the source and the target language. Transfer rules are used to map source language structures to target language ones and to identify the right translation. These rules are based on formal grammars or intermediate language-independent structures (such as minimal recursion semantics [Oepen et al. 2004]) capable of generating correct translation eq"
J17-4005,W13-1021,0,0.0692869,"nd Baldwin (2006), whose work’s main purpose is to acquire new tagged lexical items for two head-driven phrase structure grammars (ERG for English and JACY for Japanese), propose a supertagging approach based on conditional random fields to assign lexical types to the tokens of an input sequence using a pseudolikelihood method that accommodates large tag sets. The proposed approach only enables the identification of contiguous MWEs. This work is very close to methods for joint (contiguous) MWE identification and POS tagging based on linear conditional random fields (Constant and Sigogne 2011; Shigeto et al. 2013). Their tagging scheme concatenates lexical segmentation information (B and I tags for the IOB tag set) with the POS tag of the lexical unit to which the current token belongs. Constant and Sigogne (2011) trained and evaluated their models on the French Treebank, and Shigeto et al. (2013) worked on a modified version of the Penn Treebank onto which complex function words from Wiktionary were projected. Some MWE taggers concentrate on the identification task only, using an IOB-like annotation scheme (Vincze, Nagy, and Berend 2011; Constant, Sigogne, and Watrin 2012; Schneider et al. 2014a). Vin"
J17-4005,W09-2906,0,0.0284012,"llel corpora. Here, a check is carried out to assess whether the MWE candidate could be literally translated from a bilingual dictionary. Similarly, Rondon, Caseli, and Ramisch (2015) model translatability as the probability of translating the words of the MWE from Portuguese into English and then back to the same Portuguese word. Monolingual discovery. For monolingual discovery we consider the possibility of using translation asymmetries identified in parallel corpora to compile lists of potential MWE candidates in one specific language without using precomputed word alignments. For example, Sinha (2009) discover Hindi by compiling a list of Hindi light verbs and then looking at mismatches (indicating the use of a verb in a more idiomatic sense) in meaning in the corresponding English counterpart, given a list of literal translations of Hindi light verbs into English. This approach has also been extended to comparable corpora (texts from the same domain, genre, or type that are not in a translation relation). Morin and Daille (2010) collect bilingual terminology from comparable corpora with the help of a bilingual dictionary. This method applies a compositional word-for-word translation for a"
J17-4005,J93-1007,0,0.0567457,"ng evidence to reject the independence null hypothesis, that is, the candidate words are not independent and probably form an MWE. More sophisticated test statistics for two-word MWE candidates take into account their contingency table. Examples of such measures are χ2 and the more robust likelihood ratio (Dunning 1993). Pedersen (1996) suggests using Fisher’s exact test in automatic MWE discovery, and this measure is implemented among others in the Text:NSP package.13 Another measure for MWE discovery is the average and standard deviation of the distance between words, implemented in Xtract (Smadja 1993). Because these measures are based on frequency counts, there have been some studies to use Web hits as an alternative to corpus counts, in order to avoid low-frequency estimates (Keller and Lapata 2003; Ramisch et al. 2008a). Although association measures work quite well for two-word expressions, they are hard to generalize to arbitrary n-word MWE candidates. One simple approach is to merge two-word MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, ’to make a blunder’) can be modeled as the verb faire ("
J17-4005,E09-1086,0,0.0174536,"Missing"
J17-4005,W04-0401,0,0.137462,"Missing"
J17-4005,J13-4009,0,0.0958839,"Missing"
J17-4005,W14-3323,0,0.128234,"tive, that is, new multiword named entities and terms are constantly being created, so it is difficult to have complete and updated lexical resources for use during the translation process. For term identification, bilingual or multilingual term glossaries might be applied to transform terms into an intermediate representation (words concatenated by underscore). But when these resources are missing, for new domains or for under-resourced languages, multiword named entities and multiword terms can be annotated as a single token with the help of specific techniques for named entity recognition (Tan and Pal 2014), or term extraction (Bouamor, Semmar, and Zweigenbaum 2012b) designed for monolingual, parallel, or comparable data (Morin and Daille 2010). Closed compounds, obtained by concatenating several lexemes with any parts of speech, are typical of Germanic languages and represent another difficult task for MT. This category of expressions can be lexicalized, that is, they belong to the lexicon of a language as a single meaning unit, such as the German word Schwiegereltern (parentsin-law) or non-lexicalized, that is, the individual words keep their meanings when combined, for instance, the German ne"
J17-4005,J14-2007,0,0.0636085,"lingual MWE lists, as presented in Section 2. Bouamor, Semmar, and Zweigenbaum (2012b) use an association measure to find the translations of each MWE in the target language counterpart without exploiting the alignment. Parsed bilingual data has also been used to filter word-aligned MWE candidates. Thus, Zarrieß and Kuhn (2009) propose a method for detecting verb-object 873 Computational Linguistics Volume 43, Number 4 MWEs in both source and target languages that are dependency-parsed, only retaining MWEs whose words are bilingually aligned and monolingually linked by syntactic dependencies. Tsvetkov and Wintner (2014) proposed supervised classifiers to distinguish MWEs from non-MWEs, using linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora. Here, a check is carried out to assess whether the MWE candidate could be literally translated from a bilingual dictionary. Similarly, Rondon, Caseli, and Ramisch (2015) model translatability as the probability of translating the words of the MWE from Portuguese into English and then back to the same Portuguese word. Monolingual discovery. For monolingual discovery we consider the possibility of usin"
J17-4005,W14-0817,0,0.0365037,"Missing"
J17-4005,D07-1110,1,0.793142,"Missing"
J17-4005,W13-5706,0,0.0378745,"Missing"
J17-4005,R11-1040,0,0.208692,"Missing"
J17-4005,vincze-2012-light,0,0.323696,". Progressively more refined information can approach the level of expressiveness found in treebanks. Examples of annotated corpora with MWE tags include Wiki50 (Vincze, Nagy, and Berend 2011), STREUSLE (Schneider et al. 2014b), and the PARSEME shared task corpora (Savary et al. 2017). Two or more corpora can also be set in correspondence. For example, parallel corpora in different languages include sentence-level alignment and are used to detect manyto-many, one-to-many, or many-to-one translations. An example of MWE-annotated parallel corpus is the English–Hungarian SzegedParallelFX corpus (Vincze 2012). Finally, treebanks are special corpora that include syntactic relations between nodes over text segments and are arguably the most valuable resources for data-driven parsing 845 Computational Linguistics Volume 43, Number 4 systems and syntax-aware MT systems. In the literature, there exist different opinions on whether syntactically regular but semantically idiomatic MWEs should be identified in syntactic treebanks. Although the Penn Treebank designers prefer not to annotate verbal MWEs (Marcus, Marcinkiewicz, and Santorini 1993), these are annotated in the Prague Treebank (Bejˇcek et al. 2"
J17-4005,I13-1024,0,0.24777,"Missing"
J17-4005,S13-1038,0,0.06584,"Missing"
J17-4005,W14-0804,0,0.196566,"hashi 1975), combinatory categorial grammar (Steedman 1987), lexical functional grammar (LFG) (Kaplan 1989), and headdriven phrase-structure grammar (HPSG) (Pollard and Sag 1994). Grammars may also be composed of sets of finite-state rules that are incrementally applied (Joshi and Hopeli 1996; Ait-Mokhtar, Chanod, and Roux 2002). Two different strategies are generally used to handle ambiguity. In the first strategy, the process comprises two phases: an analysis phase, generating a set of possible syntactic trees, followed by a disambiguation phase based on heuristics (Boullier and Sagot 2005; Wehrli 2014) or statistical models (Riezler et al. 2002; Villemonte De La Clergerie 2013). In the second (mainstream) strategy, the grammar is accompanied by a statistical model. For instance, parsers based on generative-models assign probabilities to rules of an underlying grammatical formalism, as in probabilistic context-free grammars (PCFGs) (Charniak and Johnson 2005), tree-substitution grammars (Green et al. 2011), TAG (Resnik 1992), and LFG (Cahill 2004). The parsing algorithms generally rely on dynamic programming. They usually include one pass, but two-pass processes also exist. For instance, Cha"
J17-4005,W10-3705,0,0.104896,"Missing"
J17-4005,2009.eamt-1.18,0,0.0171667,"es and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs. RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language texts such as parse trees (Wehrli et al. 2009) or intermediate representation languages like minimal recursion semantics (Oepen et al. 2004), a semantico-syntactic abstraction language (Monti et al. 2011; Barreiro et al. 2013). Transfer rules handle MWE variability and discontiguity (Forcada et al. 2011) and are manually defined or automatically learned from parallel corpora (Haugereid and B"
J17-4005,2011.mtsummit-papers.45,0,0.0143103,"us, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word translation during the decoding phase and helping disambiguation. More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process 876 Constant et al. MWE Processing: A Survey (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs. RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using"
J17-4005,W14-5709,0,0.135338,"ally translated into several target language words. Their meaning might be more or less compositional. MT systems fail to correctly translate these compounds because of their low frequencies and their variability. Moreover, non-compositional compounds have unpredictable meaning. Splitting strategies can be applied to cut the compounds into subsequent words to improve translation quality (Fritzinger and Fraser 2010; Stymne, Cancedda, and Ahrenberg 2013). Splitting is done by identifying component words in the corpus or by prefix and suffix identification together with distributional semantics (Weller et al. 2014) or by using a morphosyntactic tagger and parser (Cap et al. 2014). Oversplitting can also be a problem: Splitting non-compositional compounds may generate erroneous translations. Some methods aim to distinguish between compositional and non-compositional compounds and split only the compositional ones (Weller et al. 2014). A postprocessing step is required to merge components back into compounds once a translation is generated using a system trained on split compounds. Some methods replace the compounds by paraphrases (Ullman and Nivre 2014) before translating them. 875 Computational Linguist"
J17-4005,weller-heid-2010-extraction,0,0.104568,"Missing"
J17-4005,1983.tc-1.13,0,0.773002,"Missing"
J17-4005,D15-1201,0,0.0278442,"Missing"
J17-4005,W09-2904,0,0.0667944,"Missing"
J17-4005,W04-2407,0,\N,Missing
J17-4005,J90-1003,0,\N,Missing
J17-4005,copestake-etal-2002-multiword,0,\N,Missing
J17-4005,W04-0412,0,\N,Missing
J17-4005,W07-1106,0,\N,Missing
J17-4005,P05-1038,0,\N,Missing
J17-4005,C98-1030,0,\N,Missing
J17-4005,P11-1070,0,\N,Missing
J17-4005,P05-1022,0,\N,Missing
J17-4005,P15-1033,0,\N,Missing
J17-4005,D11-1077,0,\N,Missing
J17-4005,W13-4905,0,\N,Missing
J17-4005,P14-1070,0,\N,Missing
J17-4005,N10-1029,0,\N,Missing
J17-4005,D14-1179,0,\N,Missing
J17-4005,W11-0809,0,\N,Missing
J17-4005,2010.eamt-1.17,0,\N,Missing
J17-4005,W09-2903,0,\N,Missing
J17-4005,E14-1061,0,\N,Missing
J17-4005,P12-1022,0,\N,Missing
J17-4005,C12-1015,0,\N,Missing
J17-4005,W15-0903,0,\N,Missing
J17-4005,D14-1082,0,\N,Missing
J17-4005,barreiro-etal-2014-linguistic,1,\N,Missing
J17-4005,vincze-etal-2010-hungarian,0,\N,Missing
J17-4005,J07-2003,0,\N,Missing
J17-4005,N16-1127,0,\N,Missing
J17-4005,P16-1016,0,\N,Missing
J17-4005,P16-1187,1,\N,Missing
J17-4005,D07-1096,0,\N,Missing
J17-4005,W17-1704,1,\N,Missing
J17-4005,D14-1108,0,\N,Missing
L18-1525,P15-2017,0,0.0326605,"ni et al., 2011) and (Mitchell et al., 2012) are early examples of such systems. The state of the art in image description makes use of deep learning approaches, usually relying on a neural language model to generate descriptions based on image analysis conducted via a pre-trained convolutional network (Vinyals et al., 2015, Mao et al., 2015, Xu et al., 2015, Rennie et al., 2016). While these systems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a significant amount of descriptions that can be found as-is in the training set (Devlin et al., 2015, Tanti et al., 2018). This suggests that the datasets on 3324 which they are trained are very repetitive and lack diversity. State of the art image captioning requires large datasets for training and testing. While such datasets do exist for scene descriptions, no data is currently available for the face description task, despite the existence of annotated image datasets. In the following section we describe how we addressed this lacuna, initiating an ongoing crowd-sourcing exercise to create a large dataset of face descriptions, paired with images which are annotated with physical features."
L18-1525,D13-1128,0,0.0226852,"n a preliminary version of the corpus, focussing on how it was collected and evaluated.1 2. Background Automatic image description research can rely on a wide range of image-description datasets. Such datasets consist of images depicting various objects and actions, and associated descriptions, typically collected through crowdsourcing. The descriptions verbalise the objects and events or relations shown in the images with different degrees of granularity. For example, the most widely-used image captioning datsets, such as Flickr8k (Hodosh et al., 2013), Flickr30K (Young et al., 2014), VLT2K (Elliott and Keller, 2013), and MS COCO (Lin et al., 2014), contain images of familiar scenes, and the descriptions are restricted to the ‘concrete conceptual’ level (Hodosh et al., 2013), mentioning what is visible, while minimising inferences that can be drawn from the visual information. Other datasets are somewhat more specialised. For example, the CaltechUCSD Birds and Oxford Flowers-102 contain fine-grained 1 The corpus will shortly be released to the public. The current version is available upon request. visual descriptions of images of birds and flowers respectively (Reed et al., 2016). Some datasets also conta"
L18-1525,W16-3210,0,0.0352719,"Missing"
L18-1525,P12-1038,0,0.0304027,"n generation are based either on caption retrieval or direct generation (Bernardi et al., 2016). In the generation-by-retrieval approach, human authored descriptions for similar images are stored in a database of image-description pairs. Given an input image that is to be described, the database is queried to find the most similar images to the input image and the descriptions of these images are returned. The descriptions are then either copied directly (which assumes that descriptions can be reused as-is with similar images) or synthesized from extracted phrases. (Ordonez et al., 2011) and (Kuznetsova et al., 2012) are examples of retrieval in visual space; other approaches rely on retrieval in multimodal space (Hodosh et al., 2013, Socher et al., 2014). On the other hand, direct generation attempts to generate novel descriptions using natural language generation techniques. Traditionally, this was achieved by using computer vision (CV) detectors which are applied to generate a list of image content (e.g objects and their attributes, spatial relationships, and actions). These are fed into a classical NLG pipeline that produces a textual description, verbalising the salient aspects of the image. (Kulkarn"
L18-1525,E12-1076,0,0.0415758,"ieval in visual space; other approaches rely on retrieval in multimodal space (Hodosh et al., 2013, Socher et al., 2014). On the other hand, direct generation attempts to generate novel descriptions using natural language generation techniques. Traditionally, this was achieved by using computer vision (CV) detectors which are applied to generate a list of image content (e.g objects and their attributes, spatial relationships, and actions). These are fed into a classical NLG pipeline that produces a textual description, verbalising the salient aspects of the image. (Kulkarni et al., 2011) and (Mitchell et al., 2012) are early examples of such systems. The state of the art in image description makes use of deep learning approaches, usually relying on a neural language model to generate descriptions based on image analysis conducted via a pre-trained convolutional network (Vinyals et al., 2015, Mao et al., 2015, Xu et al., 2015, Rennie et al., 2016). While these systems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a significant amount of descriptions that can be found as-is in the training set (Devlin et al., 2015, Tanti et al., 2018). Th"
L18-1525,Q14-1006,0,0.11024,"scriptions. Here we report on a preliminary version of the corpus, focussing on how it was collected and evaluated.1 2. Background Automatic image description research can rely on a wide range of image-description datasets. Such datasets consist of images depicting various objects and actions, and associated descriptions, typically collected through crowdsourcing. The descriptions verbalise the objects and events or relations shown in the images with different degrees of granularity. For example, the most widely-used image captioning datsets, such as Flickr8k (Hodosh et al., 2013), Flickr30K (Young et al., 2014), VLT2K (Elliott and Keller, 2013), and MS COCO (Lin et al., 2014), contain images of familiar scenes, and the descriptions are restricted to the ‘concrete conceptual’ level (Hodosh et al., 2013), mentioning what is visible, while minimising inferences that can be drawn from the visual information. Other datasets are somewhat more specialised. For example, the CaltechUCSD Birds and Oxford Flowers-102 contain fine-grained 1 The corpus will shortly be released to the public. The current version is available upon request. visual descriptions of images of birds and flowers respectively (Reed et al"
N09-2032,N01-1016,0,0.0396628,"NAACL HLT 2009: Short Papers, pages 125–128, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings."
N09-2032,N03-1014,1,0.87695,"ith and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse hist"
N09-2032,E06-2009,1,0.81171,"etween two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings. Current linguistic theory provides several approaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern´andez, 2006). Following the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. T"
N09-2032,J93-2004,0,0.0347848,"Missing"
N09-2032,W08-2101,1,0.855716,"ated NSUs except for VPs and modifiers. 5 augmented model: One with and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent"
N09-2032,J05-1004,0,0.0274982,"ems. 3 Because NSUs can be interpreted only in context, the same NSU can correspond to several syntactic categories: South for example, can be an noun, an adverb, or an adjective. In case of ambiguity, we divided the score up for the several possible tags. This accounts for the fractional counts. Category NP JJ PP NN VP # Occ. 19.0 12.7 12.0 11.7 11.0 Perc. 15.2 10.1 9.6 9.3 8.8 Category RB DT CD Total frag. Full sents # Occ. 1.7 1.0 1.0 70.0 55.0 Perc. 1.3 0.8 0.8 56.0 44.0 Table 1: Distribution of types of NSUs and full sentences in the TownInfo development set. merged with PropBank labels (Palmer et al., 2005). We included all the sentences from this dataset in our artificial corpus, giving us 39,832 full sentences. In accordance with the target distribution we added 50,699 NSUs extracted from the same dataset. We sampled NSUs according to the distribution given in Table 1. After the extraction we added a root FRAG node to the extracted NSUs4 and we capitalised the first letter of each NSU to form an utterance. There are two additional pre-processing steps. First, for some types of NSUs maximal projections are added. For example, in the subset from the target source we saw many occurrences of nouns"
N09-2032,P07-1080,1,0.883131,"he target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse history which are useful for making"
N09-2032,D07-1096,0,\N,Missing
N15-2005,C10-3009,0,0.0201397,"Missing"
N15-2005,W14-0816,0,0.128358,"a predicate is referred to as a ‘roleset’ because it lists all required and possible semantic roles for the predicate used in a specific sense. The 12K rolesets in PB describe mostly single word predicates, to a great part leaving aside multiword expressions (MWEs). Complex predicates (CPs), ‘predicates which are multi-headed: they are composed of more than one grammatical element’ (Ramisch, 2012), are most relevant in the context of SRL. Light verb constructions (LVCs), e.g. take care, and verb particle constructions (VPCs), e.g. watch out, are the most frequently occurring types of CPs. As Bonial et al. (2014) stated ‘PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages’. For example the predicates in the CPs take a hard line, take time and many others are all annotated with a sense of take, meaning acquire, come to have, chose, bring with you from somewhere. This results in a loss of semantic information in the PB annotations. This is especially critical because CPs are a frequent phenomenon. The Wiki50 corpus (Vincze et al., 2011), which provides a full coverage MWE annotation, counts 814 occurrences of LV"
N15-2005,W11-0812,0,0.0567675,"Missing"
N15-2005,W10-1810,0,0.1315,"split up into its two components that are each assigned their own roleset. This annotation ignores the semantic unity of the CP and is unable to capture its single meaning of being concerned with or caring for something. 1 We show an excerpt of the original sentence found in the currently available version of PB (Proposition Bank I). 34 W HO? W HO? W HAT ? Frank takes take.01 OF W HAT ? care of business care.01 Figure 1: Current PB representation of the CP take care W HO? OF W HAT ? Frank takes care of business (take+care).01 Figure 2: Improved representation of the CP take care adopted from (Hwang et al., 2010; Duran et al., 2011) In contrast to this, Hwang et al. (2010) suggest a new annotation scheme for LVCs that assigns the argument structure of the LVC independently from the argument structure of its components. First, the arguments of the light verb and true predicate are annotated with roles regarding their relationship to the combination of the light verb and true predicate. Then, the light verb and predicate lemmas are joined into a single predicate. The result of this process is shown in Figure 2. Duran et al. (2011) discuss the analysis of Brazilian Portuguese CPs. Similarly to Hwang et"
N15-2005,N03-1017,0,0.0062322,"This results in a similarity ranking of PB roleset groups for each CP, from which we select the roleset with the highest cosine value as alias. 4 4.1 Experiments Tools and Data We processed the English section of the Europarl corpus (Koehn, 2005) (about 2 million sentences) with the MATE tools (Bj¨orkelund et al., 2010) to obtain lemmas, part-of-speech (POS) tags, dependency structures and semantic role labels. These annotations are used to find occurrences of the CPs and words assigned with PB rolesets in the English part. The word alignments produced with the grow-diagfinal-and-heuristics (Koehn et al., 2003) provided by the OPUS project (Tiedemann, 2012) are then used to find their alignments to all other 20 languages in the corpus and exploited as features in the distributional model. 4.2 Evaluation Framework Human Annotation. In order to evaluate our system, we set up an annotation effort loosely following the guidelines provided by Bonial et al. (2014). We selected 50 LVCs and 50 VPCs from the Wiki50 corpus (Vincze et al., 2011) divided equally over two frequency groups: Half of the expressions occur only once in the Wiki50 corpus (low-frequency 36 subgroup) and the other half occur at least t"
N15-2005,2005.mtsummit-papers.11,0,0.015287,"istinct PB roleset groups. take care care.01 think.01 ter cuidado (es) 3 4 0 achten (de) 3 3 2 prendre soin (fr) 5 7 1 penser a (fr) 0 1 6 Figure 6: Toy example co-occurrence matrix Finally, we measure the similarity between CPs and roleset groups using the cosine similarity because it worked best in previous experiments for finding synonyms (Van der Plas, 2008). This results in a similarity ranking of PB roleset groups for each CP, from which we select the roleset with the highest cosine value as alias. 4 4.1 Experiments Tools and Data We processed the English section of the Europarl corpus (Koehn, 2005) (about 2 million sentences) with the MATE tools (Bj¨orkelund et al., 2010) to obtain lemmas, part-of-speech (POS) tags, dependency structures and semantic role labels. These annotations are used to find occurrences of the CPs and words assigned with PB rolesets in the English part. The word alignments produced with the grow-diagfinal-and-heuristics (Koehn et al., 2003) provided by the OPUS project (Tiedemann, 2012) are then used to find their alignments to all other 20 languages in the corpus and exploited as features in the distributional model. 4.2 Evaluation Framework Human Annotation. In"
N15-2005,J05-1004,0,0.241113,"Missing"
N15-2005,W12-3311,0,0.0150286,"most widely used resources for training SRL systems. It provides senses of (mostly verbal) predicates with their typical semantic arguments annotated in a corpus and accompanied by a lexical resource. The sense of a predicate is referred to as a ‘roleset’ because it lists all required and possible semantic roles for the predicate used in a specific sense. The 12K rolesets in PB describe mostly single word predicates, to a great part leaving aside multiword expressions (MWEs). Complex predicates (CPs), ‘predicates which are multi-headed: they are composed of more than one grammatical element’ (Ramisch, 2012), are most relevant in the context of SRL. Light verb constructions (LVCs), e.g. take care, and verb particle constructions (VPCs), e.g. watch out, are the most frequently occurring types of CPs. As Bonial et al. (2014) stated ‘PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages’. For example the predicates in the CPs take a hard line, take time and many others are all annotated with a sense of take, meaning acquire, come to have, chose, bring with you from somewhere. This results in a loss of semantic"
N15-2005,D07-1002,0,0.251373,"Missing"
N15-2005,tiedemann-2012-parallel,0,0.016517,"t groups for each CP, from which we select the roleset with the highest cosine value as alias. 4 4.1 Experiments Tools and Data We processed the English section of the Europarl corpus (Koehn, 2005) (about 2 million sentences) with the MATE tools (Bj¨orkelund et al., 2010) to obtain lemmas, part-of-speech (POS) tags, dependency structures and semantic role labels. These annotations are used to find occurrences of the CPs and words assigned with PB rolesets in the English part. The word alignments produced with the grow-diagfinal-and-heuristics (Koehn et al., 2003) provided by the OPUS project (Tiedemann, 2012) are then used to find their alignments to all other 20 languages in the corpus and exploited as features in the distributional model. 4.2 Evaluation Framework Human Annotation. In order to evaluate our system, we set up an annotation effort loosely following the guidelines provided by Bonial et al. (2014). We selected 50 LVCs and 50 VPCs from the Wiki50 corpus (Vincze et al., 2011) divided equally over two frequency groups: Half of the expressions occur only once in the Wiki50 corpus (low-frequency 36 subgroup) and the other half occur at least twice (high-frequency subgroup). All occurrences"
N15-2005,P06-2111,1,0.740812,"Missing"
N15-2005,R11-1040,0,0.559724,"s (VPCs), e.g. watch out, are the most frequently occurring types of CPs. As Bonial et al. (2014) stated ‘PB has previously treated language as if it were purely compositional, and has therefore lumped the majority of MWEs in with lexical verb usages’. For example the predicates in the CPs take a hard line, take time and many others are all annotated with a sense of take, meaning acquire, come to have, chose, bring with you from somewhere. This results in a loss of semantic information in the PB annotations. This is especially critical because CPs are a frequent phenomenon. The Wiki50 corpus (Vincze et al., 2011), which provides a full coverage MWE annotation, counts 814 occurrences of LVCs and VPCs in 4350 sentences. This makes for one CP in every fifth sentence. Recently, Bonial et al. (2014) have introduced an approach to improve the handling of MWEs in PB while keeping annotation costs low. The process is called aliasing. Instead of creating new frames for CPs, human annotators map them to existing PB rolesets which encompass the same semantic and argument structure. For example, the CP give (a) talk could be mapped to the alias lecture.01. While this 33 Proceedings of NAACL-HLT 2015 Student Resea"
N15-2005,P98-1013,0,\N,Missing
N15-2005,C98-1013,0,\N,Missing
N16-1078,W13-5620,0,0.0351115,"Missing"
N16-1078,eiselen-puttkammer-2014-developing,0,0.049246,"Missing"
N16-1078,escartin-2014-chasing,0,0.0250702,"Missing"
N16-1078,W10-1734,0,0.884816,"dergoes both the en-truncation and the s-suffixation). In this paper, we present a language-independent, unsupervised compound splitter that normalizes constituent forms by tolerantly retrieving candidate lemmas using an N gram index and weighting string differences with inflectional information derived from lemmatized corpora. Most previous work on compound splitting includes language-specific knowledge such as large 644 Proceedings of NAACL-HLT 2016, pages 644–653, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics lexicons and morphological analyzers (Fritzinger and Fraser, 2010) or hand-crafted lists of linking elements and rules for modeling morphological transitions (Koehn and Knight, 2003; Stymne, 2008; Weller and Heid, 2012), which makes the approaches language-dependent. Macherey et al., (2011) were the first to overcome this limitation by learning morphological compounding operations automatically by retrieving compounds and their constituents from parallel corpora including English as support language. We would like to take this one step further by avoiding the usage of parallel data, which are known to be sparse and frequently domain-specific, while Bretschne"
N16-1078,R11-1058,0,0.389946,"Missing"
N16-1078,E03-1076,0,0.405366,"pound splitter that normalizes constituent forms by tolerantly retrieving candidate lemmas using an N gram index and weighting string differences with inflectional information derived from lemmatized corpora. Most previous work on compound splitting includes language-specific knowledge such as large 644 Proceedings of NAACL-HLT 2016, pages 644–653, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics lexicons and morphological analyzers (Fritzinger and Fraser, 2010) or hand-crafted lists of linking elements and rules for modeling morphological transitions (Koehn and Knight, 2003; Stymne, 2008; Weller and Heid, 2012), which makes the approaches language-dependent. Macherey et al., (2011) were the first to overcome this limitation by learning morphological compounding operations automatically by retrieving compounds and their constituents from parallel corpora including English as support language. We would like to take this one step further by avoiding the usage of parallel data, which are known to be sparse and frequently domain-specific, while Bretschneider and Zillner (2015) showed that compounding morphology varies between different domains. Instead, we exploit le"
N16-1078,P11-1140,0,0.630891,"dex and weighting string differences with inflectional information derived from lemmatized corpora. Most previous work on compound splitting includes language-specific knowledge such as large 644 Proceedings of NAACL-HLT 2016, pages 644–653, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics lexicons and morphological analyzers (Fritzinger and Fraser, 2010) or hand-crafted lists of linking elements and rules for modeling morphological transitions (Koehn and Knight, 2003; Stymne, 2008; Weller and Heid, 2012), which makes the approaches language-dependent. Macherey et al., (2011) were the first to overcome this limitation by learning morphological compounding operations automatically by retrieving compounds and their constituents from parallel corpora including English as support language. We would like to take this one step further by avoiding the usage of parallel data, which are known to be sparse and frequently domain-specific, while Bretschneider and Zillner (2015) showed that compounding morphology varies between different domains. Instead, we exploit lemmatized corpora and use word inflection as an approximation to compounding morphology. This way, we are able"
N16-1078,C00-2162,0,0.193309,"arallel corpora including a support language which creates open compounds and has only little inflection, such as English. We take this one step further by avoiding the dependence on such parallel corpora, known to be sparse, and by approximating compounding morphology with word inflection learned from monolingual preprocessed data. Linguistically based splitters are usually relying on a lexical database or a set of linguistic rules. While these splitters outperform statistical approaches (Escart´ın, 2014), they are designed for a specific language and thus less applicable to other languages. Nießen and Ney (2000) used the morpho-syntactic analyzer GERTWOL (Mariikka Haapalainen and Ari Majorin, 1995) for splitting compounds. Schmid (2004) developed the morphological analyzer SMOR, that enumerates linguistically motivated compound splits. Fritzinger and Fraser (2010) combined SMOR with Koehn and Knight’s (2003) statistical approach and outperformed both individual methods. Weller and Heid (2012) extended the splitter of Koehn and Knight (2003) with a list of PoS-tagged lemmas and a handcrafted set of morphological transition rules. While our approach similarly exploits lemma and PoS information, we avoi"
N16-1078,schmid-etal-2004-smor,0,0.430073,"ep further by avoiding the dependence on such parallel corpora, known to be sparse, and by approximating compounding morphology with word inflection learned from monolingual preprocessed data. Linguistically based splitters are usually relying on a lexical database or a set of linguistic rules. While these splitters outperform statistical approaches (Escart´ın, 2014), they are designed for a specific language and thus less applicable to other languages. Nießen and Ney (2000) used the morpho-syntactic analyzer GERTWOL (Mariikka Haapalainen and Ari Majorin, 1995) for splitting compounds. Schmid (2004) developed the morphological analyzer SMOR, that enumerates linguistically motivated compound splits. Fritzinger and Fraser (2010) combined SMOR with Koehn and Knight’s (2003) statistical approach and outperformed both individual methods. Weller and Heid (2012) extended the splitter of Koehn and Knight (2003) with a list of PoS-tagged lemmas and a handcrafted set of morphological transition rules. While our approach similarly exploits lemma and PoS information, we avoid the manual input of transition rules. 3 Theoretical preliminaries The splitting architecture, data structure, features and ev"
N16-1078,W14-5703,0,0.101371,"Missing"
N16-1078,weller-heid-2012-analyzing,0,0.567167,"uent forms by tolerantly retrieving candidate lemmas using an N gram index and weighting string differences with inflectional information derived from lemmatized corpora. Most previous work on compound splitting includes language-specific knowledge such as large 644 Proceedings of NAACL-HLT 2016, pages 644–653, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics lexicons and morphological analyzers (Fritzinger and Fraser, 2010) or hand-crafted lists of linking elements and rules for modeling morphological transitions (Koehn and Knight, 2003; Stymne, 2008; Weller and Heid, 2012), which makes the approaches language-dependent. Macherey et al., (2011) were the first to overcome this limitation by learning morphological compounding operations automatically by retrieving compounds and their constituents from parallel corpora including English as support language. We would like to take this one step further by avoiding the usage of parallel data, which are known to be sparse and frequently domain-specific, while Bretschneider and Zillner (2015) showed that compounding morphology varies between different domains. Instead, we exploit lemmatized corpora and use word inflecti"
N16-1078,C00-2137,0,0.304273,"Missing"
P06-2111,P98-2127,0,0.913685,"eke van der Plas & J¨org Tiedemann Alfa-Informatica University of Groningen P.O. Box 716 9700 AS Groningen The Netherlands {vdplas,tiedeman}@let.rug.nl Abstract that similar words share similar contexts. Systems based on distributional similarity provide ranked lists of semantically related words according to the similarity of their contexts. Synonyms are expected to be among the highest ranks followed by (co)hyponyms and hypernyms, since the highest degree of semantic relatedness next to identity is synonymy. However, this is not always the case. Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. They use grammatical relations1 to determine the context of a target word. We will refer to such systems as monolingual syntax-based systems. These systems have proven to be quite successful at finding semantically related words. However, they do not make a clear distinction between synonyms on the one hand and related words such as antonyms, (co)hyponyms, hypernyms etc. on the other hand. In this paper we have defined context in a multilingual setting. In particular, translations of"
P06-2111,P01-1008,0,0.0804512,"condly, the authors use a parallel corpus that is bilingual whereas we use a multilingual corpus containing 11 languages in total. The authors show that the bilingual method outperforms the monolingual methods. However a combination of different methods leads to the best performance. 3 Methodology 2 Related Work 3.1 Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Several authors have used a monolingual parallel corpus to find paraphrases (Ibrahim et al. (2003), Barzilay and McKeown (2001)). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al., 1991) and separation of senses (Dyvik, 1998). The latter work derives relations such as synonymy and hyponymy from the separated senses by applying the method of semantic mirrors. Turney (2001) reports on an PMI and IR driven approach that acquires data by querying a Web search engine. He evaluates on the TOEFL test in which the system has to select the synonym among 4 candidates. Lin et al. (2003) try to tackle the problem of identifying syno"
P06-2111,tiedemann-nygaard-2004-opus,1,0.859148,"Missing"
P06-2111,J93-2003,0,0.0171928,"Missing"
P06-2111,P89-1010,0,0.168852,"bout their semantic similarity than the fact that two nouns both occur as object of squeeze. To account for this intuition, the frequency of occurrence in a vector can be replaced by a weighted score. The weighted score is an indication of the amount of information carried by that particular combination of a noun and its feature. We believe that this type of weighting is beneficial for calculating similarity between word alignment vectors as well. Word alignments that are shared by many different words are most probably mismatches. For this experiment we used Pointwise Mutual Information (I) (Church and Hanks, 1989). I(W, f ) = log 4 Evaluation Framework P (W, f ) P (W )P (f ) In the following, we describe the data used and measures applied. The evaluation method that is most suitable for testing with multiple settings is one that uses an available resource for synonyms as a gold standard. In our experiments we apply automatic evaluation using an existing hand-crafted synonym database, Dutch EuroWordnet (EWN, Vossen (1998)). In EWN, one synset consists of several synonyms which represent a single sense. Polysemous words occur in several synsets. We have combined for each target word the EWN synsets in wh"
P06-2111,W02-0908,0,0.416741,"ibutional Similarity Lonneke van der Plas & J¨org Tiedemann Alfa-Informatica University of Groningen P.O. Box 716 9700 AS Groningen The Netherlands {vdplas,tiedeman}@let.rug.nl Abstract that similar words share similar contexts. Systems based on distributional similarity provide ranked lists of semantically related words according to the similarity of their contexts. Synonyms are expected to be among the highest ranks followed by (co)hyponyms and hypernyms, since the highest degree of semantic relatedness next to identity is synonymy. However, this is not always the case. Several researchers (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)) have used large monolingual corpora to extract distributionally similar words. They use grammatical relations1 to determine the context of a target word. We will refer to such systems as monolingual syntax-based systems. These systems have proven to be quite successful at finding semantically related words. However, they do not make a clear distinction between synonyms on the one hand and related words such as antonyms, (co)hyponyms, hypernyms etc. on the other hand. In this paper we have defined context in a multilingual setting. In particular, tra"
P06-2111,P91-1017,0,0.0492041,"method outperforms the monolingual methods. However a combination of different methods leads to the best performance. 3 Methodology 2 Related Work 3.1 Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Several authors have used a monolingual parallel corpus to find paraphrases (Ibrahim et al. (2003), Barzilay and McKeown (2001)). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al., 1991) and separation of senses (Dyvik, 1998). The latter work derives relations such as synonymy and hyponymy from the separated senses by applying the method of semantic mirrors. Turney (2001) reports on an PMI and IR driven approach that acquires data by querying a Web search engine. He evaluates on the TOEFL test in which the system has to select the synonym among 4 candidates. Lin et al. (2003) try to tackle the problem of identifying synonyms among distributionally related words in two ways: Firstly, by looking at the overlap in translations of semantically similar words in multiple bilingual"
P06-2111,W03-1610,0,0.641533,"Missing"
P06-2111,W03-1608,0,0.0224368,"d in the dictionary. Secondly, the authors use a parallel corpus that is bilingual whereas we use a multilingual corpus containing 11 languages in total. The authors show that the bilingual method outperforms the monolingual methods. However a combination of different methods leads to the best performance. 3 Methodology 2 Related Work 3.1 Monolingual syntax-based distributional similarity is used in many proposals to find semantically related words (Curran and Moens (2002), Lin (1998), van der Plas and Bouma (2005)). Several authors have used a monolingual parallel corpus to find paraphrases (Ibrahim et al. (2003), Barzilay and McKeown (2001)). However, bilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as target word selection (Dagan et al., 1991) and separation of senses (Dyvik, 1998). The latter work derives relations such as synonymy and hyponymy from the separated senses by applying the method of semantic mirrors. Turney (2001) reports on an PMI and IR driven approach that acquires data by querying a Web search engine. He evaluates on the TOEFL test in which the system has to select the synonym among 4 candidates. Lin et al. (2003) try to tackle th"
P06-2111,kilgarriff-yallop-2000-whats,0,0.0157336,"rds. In this paper we report on our findings trying to automatically acquire synonyms for Dutch using two different resources, a large monolingual corpus and a multilingual parallel corpus including 11 languages. A common approach to the automatic extraction of semantically related words is to use distributional similarity. The basic idea behind this is 1 One can define the context of a word in a non-syntactic monolingual way, that is as the document in which it occurs or the n words surrounding it. From experiments we have done and also building on the observations made by other researchers (Kilgarriff and Yallop, 2000) we can state that this approach generates a type of semantic similarity that is of a looser kind, an associative kind,for example doctor and disease. These words are typically not good candidates for synonymy. 866 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 866–873, c Sydney, July 2006. 2006 Association for Computational Linguistics imated by automatic word alignment. We will refer to this approach as multilingual alignmentbased approaches. We expect that these translations will give us synonyms and less semantically related words, because translations typically"
P06-2111,J90-1003,0,\N,Missing
P06-2111,C98-2122,0,\N,Missing
P09-1033,W08-2121,0,0.0685227,"Missing"
P09-1033,P98-1013,0,0.0647586,"a set of semantic roles that can apply to any argument of any verb, to provide an unambiguous identifier of the grammatical roles of the participants in the event described by the sentence (Dowty, 1991). Starting from the first proposals (Gruber, 1965; Fillmore, 1968; Jackendoff, 1972), several approaches have been put forth, ranging from a combination of very few roles to lists of very fine-grained specificity. (See Levin and Rappaport Hovav (2005) for an exhaustive review). In NLP, several proposals have been put forth in recent years and adopted in the annotation of large samples of text (Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Loper et al., 2007). The annotated PropBank corpus, and therefore implicitly its role labels inventory, has been largely adopted in NLP because of its exhaustiveness and because it is coupled with syntactic annotation, properties that make it very attractive for the automatic learning of these roles and their further applications to NLP tasks. However, the labelling choices made by PropBank have recently come under scrutiny (Zapirain et al., 2008; Loper et al., 2007; Yi et al., 2007). The annotation of PropBank labels has been conceived in a two-tiered fash"
P09-1033,J08-2002,0,0.0434203,"Missing"
P09-1033,N07-1069,0,0.522263,"in a two-tiered fashion. A first tier assigns abstract labels such as ARG0 or ARG1, while a separate annotation records the secondtier, verb-sense specific meaning of these labels. Labels ARG0 or ARG1 are assigned to the most prominent argument in the sentence (ARG1 for unaccusative verbs and ARG0 for all other verbs). The other labels are assigned in the order of prominence. So, while the same high-level labels are used across verbs, they could have different meanings for different verb senses. Researchers have usually concentrated on the high-level annotation, but as indicated in Yi et al. (2007), there is reason to think that these labels do not generalise across verbs, nor to unseen verbs or to novel verb Semantic role labels are the representation of the grammatically relevant aspects of a sentence meaning. Capturing the nature and the number of semantic roles in a sentence is therefore fundamental to correctly describing the interface between grammar and meaning. In this paper, we compare two annotation schemes, PropBank and VerbNet, in a task-independent, general way, analysing how well they fare in capturing the linguistic generalisations that are known to hold for semantic role"
P09-1033,P08-1063,0,0.183745,"Missing"
P09-1033,J93-2004,0,0.0461431,"Missing"
P09-1033,W08-2101,1,0.902183,"Missing"
P09-1033,J01-3003,1,0.863967,"refore fundamental to correctly describe the interface between grammar and meaning, and it is of paramount importance for all natural language processing (NLP) applications that attempt to extract meaning representations from analysed text, such as questionanswering systems or even machine translation. 288 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 288–296, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP ing. Because the well-attested strong correlation between syntactic structure and semantic role labels (Levin and Rappaport Hovav, 2005; Merlo and Stevenson, 2001) could intervene as a confounding factor in this analysis, we expressly limit our investigation to data analyses and statistical measures that do not exploit syntactic properties or parsing techniques. The conclusions reached this way are not task-specific and are therefore widely applicable. To preview, based on results in section 3, we conclude that PropBank is easier to learn, but VerbNet is more informative in general, it generalises better to new role instances and its labels are more strongly correlated to specific verbs. In section 4, we show that VerbNet labels provide finergrained spe"
P09-1033,J05-1004,0,0.867541,"oles that can apply to any argument of any verb, to provide an unambiguous identifier of the grammatical roles of the participants in the event described by the sentence (Dowty, 1991). Starting from the first proposals (Gruber, 1965; Fillmore, 1968; Jackendoff, 1972), several approaches have been put forth, ranging from a combination of very few roles to lists of very fine-grained specificity. (See Levin and Rappaport Hovav (2005) for an exhaustive review). In NLP, several proposals have been put forth in recent years and adopted in the annotation of large samples of text (Baker et al., 1998; Palmer et al., 2005; Kipper, 2005; Loper et al., 2007). The annotated PropBank corpus, and therefore implicitly its role labels inventory, has been largely adopted in NLP because of its exhaustiveness and because it is coupled with syntactic annotation, properties that make it very attractive for the automatic learning of these roles and their further applications to NLP tasks. However, the labelling choices made by PropBank have recently come under scrutiny (Zapirain et al., 2008; Loper et al., 2007; Yi et al., 2007). The annotation of PropBank labels has been conceived in a two-tiered fashion. A first tier ass"
P09-1033,C98-1013,0,\N,Missing
P11-2052,2009.jeptalnrecital-long.4,0,0.0459134,"Missing"
P11-2052,J94-4004,0,0.462313,"es sentences with missing predicate labels based on PoS-information in the French sentence. 2.1 Learning joint syntactic-semantic structures We know from previous work that there is a strong correlation between syntax and semantics (Merlo and van der Plas, 2009), and that this correlation has been successfully applied for the unsupervised induction of semantic roles (Lang and Lapata, 2010). However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argumentstructure divergences (Dorr, 1994). For example, the English verb like and the French verb plaire do not share correlations between syntax and semantics. The verb like takes an A0 subject and an A1 direct object, whereas the verb plaire licences an A1 subject and an A0 indirect object. We therefore transfer semantic roles crosslingually based only on lexical alignments and add syntactic information after transfer. In Figure 1, we see that cross-lingual transfer takes place at the semantic level, a level that is more abstract and known to port relatively well across languages, while the correlations with syntax, that are known"
P11-2052,2007.tmi-papers.10,0,0.0368477,"points, respectively, lower than the upper bound from manual annotations. 1 Introduction As data-driven techniques tackle more and more complex natural language processing tasks, it becomes increasingly unfeasible to use complete, accurate, hand-annotated data on a large scale for training models in all languages. One approach to addressing this problem is to develop methods that automatically generate annotated data by transferring annotations in parallel corpora from languages for which this information is available to languages for which these data are not available (Yarowsky et al., 2001; Fung et al., 2007; Pad´o and Lapata, 2009). Previous work on the cross-lingual transfer of semantic annotations (Pad´o, 2007; Basili et al., 2009) In this paper, we generate high-quality broadcoverage semantic annotations using an automatic approach that does not rely on a semantic ontology for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic crosslingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; La"
P11-2052,W08-2122,1,0.893548,"Missing"
P11-2052,P02-1050,0,0.0491108,"Missing"
P11-2052,P06-2057,0,0.0855077,"Missing"
P11-2052,N10-1137,0,0.306482,"07; Pad´o and Lapata, 2009). Previous work on the cross-lingual transfer of semantic annotations (Pad´o, 2007; Basili et al., 2009) In this paper, we generate high-quality broadcoverage semantic annotations using an automatic approach that does not rely on a semantic ontology for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic crosslingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; Lang and Lapata, 2010) have led us to make use of the available syntactic annotations on the target language. We use the semantic annotations resulting from cross-lingual transfer combined with syntactic annotations to train a joint syntactic-semantic parser for the target language, which, in turn, re-annotates the corpus (See Figure 1). We show that the semantic annotations produced by this parser are of higher quality than the data on which it was trained. Given our goal of producing broad-coverage annotations in a setting based on an aligned corpus, our choices of formal representation and of labelling scheme di"
P11-2052,J93-2004,0,0.0416764,"c roles across languages (Pad´o, 2007), we select only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned parallel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill´e et al., 2003) is a treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency conversion of the French Treebank into dependency format provided to us by Candito and Crabb´e and described in Candito et al. (2009). The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsection 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data described in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the semantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sentences into test and development set containing 500 sentences each. 4 Results W"
P11-2052,P09-1033,1,0.722461,"Missing"
P11-2052,W07-1513,0,0.10955,"Missing"
P11-2052,J03-1002,0,0.00759072,"Missing"
P11-2052,2007.jeptalnrecital-long.25,0,0.0744954,"Missing"
P11-2052,J05-1004,0,0.697041,"only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned parallel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill´e et al., 2003) is a treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency conversion of the French Treebank into dependency format provided to us by Candito and Crabb´e and described in Candito et al. (2009). The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsection 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data described in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the semantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sentences into test and development set containing 500 sentences each. 4 Results We evaluate our methods for automatic annotation ge"
P11-2052,W07-2218,1,0.874923,"Missing"
P11-2052,W10-1814,1,0.905206,"Missing"
P11-2052,2009.eamt-1.30,0,0.0516121,"Missing"
P11-2052,N09-2004,0,0.156095,"Missing"
P11-2052,H01-1035,0,0.610552,"Missing"
P17-2010,W10-1734,0,0.10424,"ompound splitting with extrinsic evaluation methods mostly focuses on statistical machine translation (SMT) (e.g., Nießen and Ney (2000), Koehn and Knight (2003)). Some other external tasks such as information retrieval (Kraaij and Pohlmann, 1998) or speech recognition (Larson et al., 2000) have been shown to benefit from prior compound splitting, yet these works have not compared the extrinsic performance of different compound splitting methods. Interestingly, the performance found in intrinsic evaluations does not automatically propagate to performance in downstream evaluations as shown in (Fritzinger and Fraser, 2010) for SMT, where oversplit compounds are simply learned as phrases (Dyer, 2009; Weller et al., 2014). Oversplitting is an example of a feature that might not be measured in intrinsic evaluations, because some available gold standards contain positive examples only (Ziering and van der Plas, 2016). It is highly relevant to increase the number of extrinsic tasks for the evaluation of compound splitting to be able to evaluate features that intrinsic evaluations and known extrinsic evaluations ignore. In this paper we investigate the suitability of Recognizing Textual Entailment (RTE) for the task"
P17-2010,D15-1075,0,0.0224194,"tuents of closed compounds hinders the matching of words from T in H1. Conversely, compound splitting also helps to detect 2 Note that we need to apply lemmatization prior to determining the lexical matches between T and H. 3 The compound splitters are designed to split compounds with any content word as head, i.e., noun compounds (Hunde|h¨utte ‘doghouse’), verb compounds (eis|laufen ‘to ice-skate’) and adjective compounds (hunde|m¨ude ‘dogtired’) and disregard constructions with a functional modifier (as in the particle verb auf|stehen ‘to stand up’). 1 We did not opt for neural RTE systems (Bowman et al., 2015), albeit state-of-the-art, in this first study because of the opacity of the models and the inclusion of phrase-level information, which will make interpretation of the effect harder. 59 System Acc INIT manual splitting ? ZvdP2016 FF2010 ? WH2012 64.13 67.88 66.63 67.38 66.00 Entailment P R F1 62.50 74.57 68.00 65.08 80.20 71.85 64.55 77.02 70.23 65.48 76.53 70.58 63.73 77.75 70.04 Non-entailment P R F1 66.67 53.20 59.18 72.64 54.99 62.59 69.87 55.75 62.02 70.19 57.80 63.39 69.77 53.71 60.69 Table 1: Results on RTE performance without (INIT) and with prior compound splitting. ?: significant di"
P17-2010,N09-1046,0,0.0145381,"lation (SMT) (e.g., Nießen and Ney (2000), Koehn and Knight (2003)). Some other external tasks such as information retrieval (Kraaij and Pohlmann, 1998) or speech recognition (Larson et al., 2000) have been shown to benefit from prior compound splitting, yet these works have not compared the extrinsic performance of different compound splitting methods. Interestingly, the performance found in intrinsic evaluations does not automatically propagate to performance in downstream evaluations as shown in (Fritzinger and Fraser, 2010) for SMT, where oversplit compounds are simply learned as phrases (Dyer, 2009; Weller et al., 2014). Oversplitting is an example of a feature that might not be measured in intrinsic evaluations, because some available gold standards contain positive examples only (Ziering and van der Plas, 2016). It is highly relevant to increase the number of extrinsic tasks for the evaluation of compound splitting to be able to evaluate features that intrinsic evaluations and known extrinsic evaluations ignore. In this paper we investigate the suitability of Recognizing Textual Entailment (RTE) for the task of compound splitting, inspired by the fact that previous work in RTE underli"
P17-2010,W07-1401,0,0.209332,"Missing"
P17-2010,W97-0802,0,0.655136,"Missing"
P17-2010,henrich-hinrichs-2010-gernedit,0,0.0720987,"Missing"
P17-2010,E03-1076,0,0.404862,"ormation of a oneword unit composing several lexemes, is a common linguistic phenomenon in several languages such as German, Dutch, Greek, and Finnish. The goal of compound splitting is to obtain the constituents of a compound to increase its semantic transparency. For example, for the German compound Apfelsaft ‘apple1 juice2 ’ the desired output of a compound splitter is Apfel1 Saft2 . Intrinsic evaluation of compound splitting measures the correctness of the determined split point (Riedl and Biemann, 2016) and the resulting lemmas by means of precision, recall, F1 -score and accuracy (e.g., Koehn and Knight (2003)). In extrinsic evaluation setups, compound splitting is applied to the input data of an external natural language processing (NLP) task that benefits from split compounds. As closed compounding introduces semantic opaqueness and vastly increases the vocabulary size of a language, many NLP tasks T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon. H: Yoko Ono is John Lennon’s widow. 58 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 58–63 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computati"
P17-2010,P13-1118,0,0.0252093,"Missing"
P17-2010,C14-1099,1,0.906103,"Missing"
P17-2010,N16-1078,1,0.899217,"n of the effect harder. 59 System Acc INIT manual splitting ? ZvdP2016 FF2010 ? WH2012 64.13 67.88 66.63 67.38 66.00 Entailment P R F1 62.50 74.57 68.00 65.08 80.20 71.85 64.55 77.02 70.23 65.48 76.53 70.58 63.73 77.75 70.04 Non-entailment P R F1 66.67 53.20 59.18 72.64 54.99 62.59 69.87 55.75 62.02 70.19 57.80 63.39 69.77 53.71 60.69 Table 1: Results on RTE performance without (INIT) and with prior compound splitting. ?: significant difference of the performance in comparison to INIT ZvdP2016 Finally, the method using least language-specific knowledge was proposed by Ziering and van der Plas (2016). Instead of using a morphological analyzer or manually compiling a hand-crafted list of rules, they recursively generate all possible binary splits by learning constituent transformations from regular inflection derived from a monolingual lemmatized corpus, e.g., the s-suffix in the case of a genitive marker is often used as linking morpheme. The recursion stops if a non-splitting (atomic) analysis is ranked highest. Additionally, to provide an upper bound, we manually split development and test data. 3.2 icon (Zeller et al., 2013)) in future work. We use TreeTagger (Schmid, 1995) as integrat"
P17-2010,P14-5008,0,0.0225561,"Missing"
P17-2010,C00-2162,0,0.117175,"Missing"
P17-2010,S15-1022,0,0.023753,"Missing"
P17-2010,N16-1075,0,0.0224912,"nguistics University of Malta, Malta Lonneke.vanderPlas @um.edu.mt Introduction Closed compounding, i.e., the formation of a oneword unit composing several lexemes, is a common linguistic phenomenon in several languages such as German, Dutch, Greek, and Finnish. The goal of compound splitting is to obtain the constituents of a compound to increase its semantic transparency. For example, for the German compound Apfelsaft ‘apple1 juice2 ’ the desired output of a compound splitter is Apfel1 Saft2 . Intrinsic evaluation of compound splitting measures the correctness of the determined split point (Riedl and Biemann, 2016) and the resulting lemmas by means of precision, recall, F1 -score and accuracy (e.g., Koehn and Knight (2003)). In extrinsic evaluation setups, compound splitting is applied to the input data of an external natural language processing (NLP) task that benefits from split compounds. As closed compounding introduces semantic opaqueness and vastly increases the vocabulary size of a language, many NLP tasks T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon. H: Yoko Ono is John Lennon’s widow. 58 Proceedings of the 55th Annual Meeting of the Association for Computational Linguis"
P17-2010,schmid-etal-2004-smor,0,0.0115065,"dent as we do not use any language-specific parameters. However, in the present work we test it on the most prominent closed-compounding language, German (Ziering and van der Plas, 2014). We inspect the impact of three different types of automatic compound splitting3 methods that follow a generate-and-rank principle, where the candidate splits are ranked according to the geometric mean of the constituents’ frequencies in a given training corpus (Koehn and Knight, 2003). FF2010 The compound splitter by Fritzinger and Fraser (2010) relies on the output of the German morphological analyzer SMOR (Schmid et al., 2004) to generate several plausible compound splits (e.g., due to word sense ambiguity). WH2012 As an alternative method, we use the statistical approach presented in Weller and Heid (2012) for German compound splitting. Instead of using the knowledge-rich SMOR, it includes an extensive list of hand-crafted transformation rules that allows to map constituents to corpus lemmas (e.g., by truncating linking morphemes) to generate all possible splits with up to four constituents per compound. Moreover, misleading lemmas are removed from the training corpus using hand-crafted filters. Relevance of Compo"
P17-2010,W14-5709,0,\N,Missing
P17-2010,weller-heid-2012-analyzing,0,\N,Missing
R15-1094,N01-1026,0,0.140555,"Missing"
R15-1094,P10-1131,0,0.0280405,"Missing"
R15-1094,P11-1135,0,0.0227143,"ing (NLP) tasks like machine translation (MT) or information retrieval (IR) and indispensable for understanding the meaning of complex units. For example, while [natural language] processing means the (machine) processing of natural languages, natural [language processing] denotes the natural processing of (any) languages. As previous work has shown, multilingual data can help resolving various kinds of structural ambiguity such as prepositional phrase (PP) attachment (Schwartz et al., 2003; Fossum and Knight, 2008), subject/object distinction (Schwarck et al., 2010) or coordination ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefore chooses not to include semantic indeterminacy in his NP annotation of the Penn Treebank (Marcus et al., 1993). We believe t"
R15-1094,C00-2137,0,0.0428565,"Missing"
R15-1094,D08-1092,0,0.0302161,"omplex units. For example, while [natural language] processing means the (machine) processing of natural languages, natural [language processing] denotes the natural processing of (any) languages. As previous work has shown, multilingual data can help resolving various kinds of structural ambiguity such as prepositional phrase (PP) attachment (Schwartz et al., 2003; Fossum and Knight, 2008), subject/object distinction (Schwarck et al., 2010) or coordination ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefore chooses not to include semantic indeterminacy in his NP annotation of the Penn Treebank (Marcus et al., 1993). We believe that it is important to include semantic indeterminacy in NLP, e.g., an anaphora resolver needs to know the structural equivalence f"
R15-1094,C14-1099,1,0.872191,"Missing"
R15-1094,J82-3004,0,0.0924794,"paraphrase onderzeese gaspijpleiding met dubbele pijp (undersea {gas pipeline} with twin pipes). In the final step, we put all valid trees from all languages into a tree accumulation (TA) and rank them by frequency (i.e., trees being valid in most cases are ranked first). For example, for the semantically indeterminate air traffic control centres, FAST assigns the same top rank to the semantically equivalent structures as shown in Table 1. Figure 1: FAST algorithm We first create all possible binary trees for Ψ (line 1). The number of possible binary trees increases with the Catalan numbers (Church and Patil, 1982), e.g., 3NCs have two possible trees (i.e., left- or right-branched), 4NCs have five possible trees and kNCs have Ck−1 possible trees, where Cn is the n-th Catalan number as given in (1). Rank 1 1 2 Structure [ air traffic ] [ control centres ] [ [ air traffic ] control ] centres [ air [ traffic control ] ] centres TA 13 13 10 Table 1: FAST top-ranking for air traffic control centres In addition to a token-based setting, FAST can also be applied on expression types. In this case, we put all valid trees from all aligned languages of all instances of Ψ into the TA. (1) All tree nodes Ni in these"
R15-1094,W15-0112,1,0.877833,"Missing"
R15-1094,2008.amta-srw.2,0,0.0370267,"stic expressions (e.g., noun phrases (NPs)) is a fundamental component in many natural language processing (NLP) tasks like machine translation (MT) or information retrieval (IR) and indispensable for understanding the meaning of complex units. For example, while [natural language] processing means the (machine) processing of natural languages, natural [language processing] denotes the natural processing of (any) languages. As previous work has shown, multilingual data can help resolving various kinds of structural ambiguity such as prepositional phrase (PP) attachment (Schwartz et al., 2003; Fossum and Knight, 2008), subject/object distinction (Schwarck et al., 2010) or coordination ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefore chooses not to include"
R15-1094,P10-2034,0,0.0285631,"Missing"
R15-1094,J93-2004,0,0.0504443,"on ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefore chooses not to include semantic indeterminacy in his NP annotation of the Penn Treebank (Marcus et al., 1993). We believe that it is important to include semantic indeterminacy in NLP, e.g., an anaphora resolver needs to know the structural equivalence for finding all possible nested antecedents, e.g., both animal welfare and welfare standards. This work aims at capturing semantic indeterminacy within a structural analysis. We exploit cross-linguality for this task because structural variation for semantic indeterminacy is visible in particular across languages. In a monolingual approach, we expect less variation, due to conventional language use. As a result, parse forests resulting from monolingual"
R15-1094,N10-1113,0,0.0137414,"mental component in many natural language processing (NLP) tasks like machine translation (MT) or information retrieval (IR) and indispensable for understanding the meaning of complex units. For example, while [natural language] processing means the (machine) processing of natural languages, natural [language processing] denotes the natural processing of (any) languages. As previous work has shown, multilingual data can help resolving various kinds of structural ambiguity such as prepositional phrase (PP) attachment (Schwartz et al., 2003; Fossum and Knight, 2008), subject/object distinction (Schwarck et al., 2010) or coordination ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefore chooses not to include semantic indeterminacy in his NP annotation of the P"
R15-1094,2003.mtsummit-papers.44,0,0.0220555,"oduction Parsing linguistic expressions (e.g., noun phrases (NPs)) is a fundamental component in many natural language processing (NLP) tasks like machine translation (MT) or information retrieval (IR) and indispensable for understanding the meaning of complex units. For example, while [natural language] processing means the (machine) processing of natural languages, natural [language processing] denotes the natural processing of (any) languages. As previous work has shown, multilingual data can help resolving various kinds of structural ambiguity such as prepositional phrase (PP) attachment (Schwartz et al., 2003; Fossum and Knight, 2008), subject/object distinction (Schwarck et al., 2010) or coordination ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefo"
R15-1094,W04-3207,0,0.0362405,"anding the meaning of complex units. For example, while [natural language] processing means the (machine) processing of natural languages, natural [language processing] denotes the natural processing of (any) languages. As previous work has shown, multilingual data can help resolving various kinds of structural ambiguity such as prepositional phrase (PP) attachment (Schwartz et al., 2003; Fossum and Knight, 2008), subject/object distinction (Schwarck et al., 2010) or coordination ellipsis (Bergsma et al., 2011). Parallel sentences have been jointly parsed supported by word alignment features (Smith and Smith, 2004; Burkett and Klein, 2008). Yarowsky 739 Proceedings of Recent Advances in Natural Language Processing, pages 739–746, Hissar, Bulgaria, Sep 7–9 2015. semantic indeterminacy, to the best of our knowledge, no attempt has been made to include this phenomenon in syntactic analysis. Vadas (2009) argues that in most cases the intended structure is unambiguous1 and therefore chooses not to include semantic indeterminacy in his NP annotation of the Penn Treebank (Marcus et al., 1993). We believe that it is important to include semantic indeterminacy in NLP, e.g., an anaphora resolver needs to know th"
R15-1094,P09-1009,0,0.0434279,"Missing"
R15-1094,J93-1005,0,\N,Missing
R15-1094,tiedemann-2012-parallel,0,\N,Missing
van-der-plas-etal-2004-automatic,P03-1071,0,\N,Missing
W08-1807,N04-1041,0,0.0911589,"Missing"
W08-1807,W98-0705,0,0.112213,"Missing"
W08-1807,D07-1002,0,0.0831903,"Missing"
W08-1807,W08-1803,1,0.838431,"expansions. Since all synonyms are equally similar, we do not have similarity scores for them to be used in a threshold. The categorised named entities were not only used to expand named entities with the corre6 Results In Table 2 the MRR (Mean Reciprocal Rank) is given for the various expansion techniques. Scores are given for expanding the several syntactic categories, where possible. The baseline does not 5 We used MRR instead of other common evaluation measures because of its stronger correlation with the overall performance of our QA system than, for example, coverage and redundancy (see Tiedemann and Mur (2008)). 4 Note that the number of nouns from EWN is the result of subtracting the proper names. 53 SynCat Nouns Adj Verbs Proper All EWN 51.52 52.33 52.40 52.59 51.65 MRR Syntax Align 51.15 51.21 52.27 52.38 52.33 52.21 50.16 51.21 51.02 Proxi 51.38 51.71 52.62 53.94 53.36 offs to keep the co-occurrence matrix manageable. The larger impact of the proximity-based nearest neighbours is probably partly due to this decision. The cutoffs for the alignment-based and syntaxbased method have been determined after evaluations on EuroWordNet (Vossen, 1998) (see also van der Plas (2008)). The largest impact r"
W08-1807,P06-2111,1,0.872885,"Missing"
W08-1807,W07-1206,0,0.068301,"Missing"
W08-1807,2006.jeptalnrecital-invite.2,0,0.0460105,"Missing"
W08-1807,P02-1005,0,0.0751087,"Missing"
W08-1807,E03-1070,0,0.0275792,"Missing"
W09-1706,P89-1010,0,0.0341585,"is selectionally weak (Resnik, 1993) or a LIGHT verb. A verb such as voer ‘feed’ on the other hand occurs much less frequently, and only with a restricted set of nouns as direct object. Intuitively, the fact that two nouns both occur as subject of hebben tells us less about their semantic similarity than the fact that two nouns 48 both occur as the direct object of feed. The results of vector-based methods can be improved if we take into account the fact that not all combinations of a word and syntactic relation have the same information value. We have used POINTWISE MUTUAL INFORMATION (PMI, Church and Hanks (1989)) to account for the differences in information value between the several headwords and attributes. The more similar the co-occurrence vectors of any two headwords are, the more distributionally similar the headwords are. In order to compare the vectors of any two headwords, we need a similarity measure. In these experiments we have used a variant of Dice: Dice†, proposed by Curran and Moens (2002). It is defined as: P min(wgt(W 1, ∗r , ∗w0 ), wgt(W 2, ∗r , ∗w0 )) P Dice† = 2 wgt(W 1, ∗r , ∗w0 ) + wgt(W 2, ∗r , ∗w0 ) We describe the function using an extension of the notation used by Lin (1998"
W09-1706,W02-0908,0,0.0732218,"e syntactic contexts a word is found in. For example, the verbs that are in a object relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb drink have something in common: they are liquid. We will refer to words linked by a syntactic relation, such as drink -OBJbeer, as SYNTACTIC CO - OCCURRENCES. Syntactic co-occurrences have often been used in work on 45 lexical acquisition (Lin, 1998b; Dagan et al., 1999; Curran and Moens, 2002; Alfonseca and Manandhar, 2002). Distributional methods for automatic acquisition of semantically related words suffer from data sparseness. They generally perform less well on low-frequency words (Weeds and Weir, 2005; van der Plas, 2008). This is a pity because the available resources for semantically related words usually cover the frequent words rather well. It is for the low-frequency words that automatic methods would be most welcome. This paper tries to find a way to improve the performance on the words that are most wanted: the middle to very-low-frequency words. At the basis of the p"
W09-1706,P97-1067,0,0.0540793,"D - ORDER AFFINITIES.1 There exists a third-order affinity between words, if they share many second-order affinities. If pear and watermelon are similar and orange and watermelon are similar, then pear and orange have a third-order affinity. We will refer to traditional approaches that compute second-order affinities as second-order techniques. In this paper we will compare a secondorder technique with a third-order technique, a technique that computes third-order affinities. In addition we use a combined technique that combines both second-order and third-order techniques. 2 Previous work In Edmonds (1997) the term third-order is used to refer to a different concept. Firstly, we have to mention that the author is working in a proximitybased framework, that is, he is concerned with cooccurrences of words in text, not relations between words in syntactic dependencies. Secondly, the notion of higher-order co-occurrences refers to connectivity paths in networks, i.e. the network of relations between words co-occurring is augmented by connecting words that are connected by a path of length 2 (second-order co-occurrences) and paths 1 Grefenstette (1994) uses the term third-order affinities for a diff"
W09-1706,P99-1004,0,0.0242833,"antically related words, when using third-order affinity techniques. 4 Methodology We will now describe the methodology used to compute nearest neighbours (subsection 4.1). In subsection 4.2 we will describe how we have used these nearest neighbours as input to the third-order and 47 combined technique. 4.1 Syntax-based distributional similarity In this section we will describe the syntactic contexts selected, the data we used, and the measures and weights applied to retrieve nearest neighbours. 4.1.1 Syntactic context Most research has been done using a limited number of syntactic relations (Lee, 1999; Weeds, 2003). We use several syntactic relations: subject, object, adjective, coordination, apposition, and prepositional complement. In Figure 1 examples are given for these types of syntactic relations.2 Subj: Obj: Adj: Coord: Appo: Prep: De kat eet. ‘The cat eats.’ Ik voer de kat. ‘I feed the cat.’ De langharige kat loopt. ‘The long-haired cat walks.’ Jip and Janneke spelen. ‘Jip and Janneke are playing.’ De clown Bassie lacht. ‘The clown Bassie is laughing.’ Ik begin met mijn werk. ‘I start with my work.’ Figure 1: Types of syntactic relations extracted 4.1.2 Data collection Because we b"
W09-1706,P98-2127,0,0.801792,"ays. In this paper we look at the syntactic contexts a word is found in. For example, the verbs that are in a object relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb drink have something in common: they are liquid. We will refer to words linked by a syntactic relation, such as drink -OBJbeer, as SYNTACTIC CO - OCCURRENCES. Syntactic co-occurrences have often been used in work on 45 lexical acquisition (Lin, 1998b; Dagan et al., 1999; Curran and Moens, 2002; Alfonseca and Manandhar, 2002). Distributional methods for automatic acquisition of semantically related words suffer from data sparseness. They generally perform less well on low-frequency words (Weeds and Weir, 2005; van der Plas, 2008). This is a pity because the available resources for semantically related words usually cover the frequent words rather well. It is for the low-frequency words that automatic methods would be most welcome. This paper tries to find a way to improve the performance on the words that are most wanted: the middle to ve"
W09-1706,D08-1096,0,0.0352717,"Missing"
W09-1706,2006.jeptalnrecital-invite.2,0,0.0617257,"Missing"
W09-1706,J05-4002,0,0.0142589,"rmine the semantic relatedness of words. For instance, words that occur in a object relation with the verb drink have something in common: they are liquid. We will refer to words linked by a syntactic relation, such as drink -OBJbeer, as SYNTACTIC CO - OCCURRENCES. Syntactic co-occurrences have often been used in work on 45 lexical acquisition (Lin, 1998b; Dagan et al., 1999; Curran and Moens, 2002; Alfonseca and Manandhar, 2002). Distributional methods for automatic acquisition of semantically related words suffer from data sparseness. They generally perform less well on low-frequency words (Weeds and Weir, 2005; van der Plas, 2008). This is a pity because the available resources for semantically related words usually cover the frequent words rather well. It is for the low-frequency words that automatic methods would be most welcome. This paper tries to find a way to improve the performance on the words that are most wanted: the middle to very-low-frequency words. At the basis of the proposed technique lies the intuition that semantic similarity between concepts is transitive: if A is like B and B is like C → A is like C. As explained in the second paragraph of this section, the fact that both milk a"
W09-1706,P94-1019,0,0.0916688,"most researchers in the field of distributional methods we have little choice but to evaluate our work on the resource that we want to enrich. We want to be able to enrich Dutch EuroWordNet (EWN, Vossen (1998)), but at the same time we use it to evaluate on. Especially for Dutch there are not many resources to evaluate semantically related words available. For each word we collected its k nearest neighbours according to the system. For each pair of words4 (target word plus one of the nearest neighbours) we calculated the semantic similarity according to EWN. We used the Wu and Palmer measure (Wu and Palmer, 1994) applied to Dutch EWN for computing the semantic similarity between two words.5 The EWN similarity of a set of word pairs is defined as the average of the similarity between the pairs. The Wu and Palmer measure for computing the semantic similarity between two words (W1 and W2) in a word net, whose most specific common subsumer (lowest super-ordinate) is W3, is defined as follows: Sim(W1,W2) = 2(D3) D1 + D2 + 2(D3) We computed, D1 (D2) as the distance from W1 (W2) to the lowest common ancestor of W1 and W2, W3. D3 is the distance of that ancestor to the root node. Some words returned by the sy"
W09-1706,W02-1029,0,\N,Missing
W09-1706,C98-2122,0,\N,Missing
W09-1706,biemann-etal-2004-automatic,0,\N,Missing
W10-1814,C08-1085,0,0.0912828,"Missing"
W10-1814,J05-1004,0,0.329392,"to be useful in many applications and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language 113 Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 113–1"
W10-1814,W09-3007,0,0.0583874,"Missing"
W10-1814,W07-2218,0,0.175555,"Missing"
W10-1814,burchardt-etal-2006-salsa,0,0.0862519,"Missing"
W10-1814,2009.jeptalnrecital-long.4,0,0.311795,"Missing"
W10-1814,2005.mtsummit-papers.11,0,0.0341546,"succeeding or working out Annotation Procedure Annotators have access to PropBank frame files and guidelines adapted for the current task. The frame files provide verb-specific descriptions of all possible semantic roles and illustrate these roles with examples as shown for the verb paid in (1) and the verb senses of pay in Table 1. Annotators need to look up each verb in the frame files to be able to label it with the right verb sense and to be able to allocate the arguments consistently. 2.3 Corpus We selected the French sentences for the manual annotation from the parallel Europarl corpus (Koehn, 2005). Because translation shifts are known to pose problems for the automatic crosslingual transfer of semantic roles (Pad´o, 2007) and for machine translation (Ozdowska and Way, (1) [A0 The Latin American nation] has [REL−P AY.01 paid] [A1 very little] [A3 on its debt] [AM −T M P since early last year]. 114 2009), and these are more likely to appear in indirect translations, we decided to select only those parallel sentences, for which we can infer from the labels used in Europarl that they are direct translations from English to French, or vice versa. We selected 1040 sentences for annotation (4"
W10-1814,P09-1033,1,0.825514,"Missing"
W10-1814,W07-1513,0,0.356023,"e lost in translation. Yet, translation preserves enough meaning across language pairs to be useful in many applications and for many text genres. The belief that this layer of meaning which is preserved across languages can be formally represented and automatically calculated underlies methods that use parallel corpora for the automatic generation of semantic annotations through crosslingual transfer (Pad´o, 2007; Basili et al., 2009). A methodology similar in spirit — re-use of the existing resources in a different language — has also been applied in developing manually annotated resources. Monachesi et al. (2007) annotate Dutch sentences using the PropBank annotation scheme (Palmer et al., 2005), while Burchardt et al. (2009) use the FrameNet framework (Fillmore et al., 2003) to annotate a German corpus. Instead of building special lexicons containing the specific semantic information needed for the annotation for each language separately, which is a complex and time-consuming endeavour in itself, these approaches rely on the lexicons already developed for English. In this paper, we hypothesize that the level of abstraction that is necessary to develop a semantic lexicon/ontology for a single language"
W10-1814,2009.eamt-1.14,0,0.0299566,"Missing"
W10-3304,P05-1074,0,0.0424916,"irs that are output by the system and evaluated these on the test set described in Subsection 4.1. The method is composed of two main steps. In the ﬁrst step candidate terms are extracted from the corpus using a PoS ﬁlter, that is similar to the PoS ﬁlter we applied. In the second step pairs of candidate term variations are re-ranked on the basis of information from the Web. Phrasal patterns such as XorY are used to get synonym compatibility hits as opposed to XandY that points to non-synonymous terms. The second method we compare with is the phrase-based translation method ﬁrst introduced by Bannard and Callison-Burch (2005). Statistical word alignment can be used to measure the relation between source language items. Here, one makes use of the estimated translation likelihoods of phrases (p(f |e) and p(e|f )) that are used to build translation models in standard phrase-based statistical machine translation systems (Koehn et al., 2007). Bannard and Callison-Burch (2005) deﬁne the problem of paraphrasing as the following search problem: eˆ2 ≈ argmaxe2 :e2 =e1  f C fC p(fC |e1 )p(e2 |fC ) This is the approach that we also adapted for our comparison. The only difference in our implementation is that we applied a P"
W10-3304,J93-2003,0,0.0172852,"Missing"
W10-3304,D08-1021,0,0.148398,"relations such synonymy and hyponymy are detected based on intersection and inclusion among feature sets . Improving the syntax-based approach for synonym identiﬁcation using bilingual dictionaries has been discussed in Lin et al. (2003) and Wu and Zhou (2003). In the latter parallel corpora are also applied as a reference to assign translation likelihoods to candidates derived from the dictionary. Both of them are limited to single-word terms. Some researchers employ multilingual corpora for the automatic acquisition of paraphrases (Shimota and Sumita, 2002; Bannard and CallisonBurch, 2005; Callison-Burch, 2008). The last two are based on automatic word alignment as is our approach. Bannard and Callison-Burch (2005) use a method that is also rooted in phrase-based statistical machine translation. Translation probabilities provide a ranking of candidate paraphrases. These are reﬁned by taking contextual information into account in the form of a language model. The Europarl corpus (Koehn, 2005) is used. It has about 30 million words per language. 46 English phrases are selected as a test set for manual evaluation by two judges. When using automatic alignment, the precision reached without using context"
W10-3304,W02-0808,0,0.0358915,"ned to the English text and vice versa (dotted lines versus continuous lines). The alignment models produced are asymmetric. Several heuristics exist to combine directional word alignments which is usually called “symmetrization”. In order to cover multi-word terms standard phrase extraction techniques can be used to move from word alignment to linked phrases (see section 3.2 for more details). 2.2 DE Katze 17 2.3 Related work Multilingual parallel corpora have mostly been used for tasks related to word sense disambiguation such as separation of senses (Resnik and Yarowsky, 1997; Dyvik, 1998; Ide et al., 2002). However, taking sense separation as a basis, Dyvik (2002) derives relations such as synonymy and hyponymy by applying the method of semantic mirrors. The paper illustrates how the method works. First, different senses are identiﬁed on the basis of manual word translations in sentence-aligned Norwegian-English data (2,6 million words in total). Second, senses are grouped in semantic ﬁelds. Third, features are Measures for computing similarity Translational co-occurrence vectors are used to ﬁnd distributionally similar words. For ease of 30 assigned on the basis of inheritance. Lastly, semanti"
W10-3304,2005.mtsummit-papers.11,0,0.0495405,"Missing"
W10-3304,2006.jeptalnrecital-invite.2,0,0.035756,"Missing"
W10-3304,P98-2127,0,0.124418,"ann@lingfil.uu.se Abstract tions. These term variations could be used to enhance existing medical ontologies for the Dutch language. Our technique builds on the distributional hypothesis, the idea that semantically related words are distributed similarly over contexts (Harris, 1968). This is in line with the Firthian saying that, ’You shall know a word by the company it keeps.’ (Firth, 1957). In other words, you can grasp the meaning of a word by looking at its contexts. Context can be deﬁned in many ways. Previous work has been mainly concerned with the syntactic contexts a word is found in (Lin, 1998; Curran, 2003). For example, the verbs that are in a subject relation with a particular noun form a part of its context. In accordance with the Firthian tradition these contexts can be used to determine the semantic relatedness of words. For instance, words that occur in a object relation with the verb to drink have something in common: they are liquid. Other work has been concerned with the bagof-word context, where the context of a word are the words that are found in its proximity (Wilks et al., 1993; Sch¨utze, 1992). Yet another context, that is much less studied, is the translational con"
W10-3304,shimohata-sumita-2002-automatic,0,0.0470174,"Missing"
W10-3304,P06-2111,1,0.88343,"Missing"
W10-3304,W03-1610,0,\N,Missing
W10-3304,P07-2045,0,\N,Missing
W10-3304,C98-2122,0,\N,Missing
W10-3304,L08-1000,0,\N,Missing
W15-0112,W14-5708,0,0.021645,"Missing"
W15-0112,P11-1135,0,0.380137,"coordinations. Barker (1998) presents a bracketing method for k-partite NPs that reduces the task to three-word bracketings within a sliding window. One advantage of supervised approaches for this task is that kNCs are labeled in context so contextual features can be used in the learning framework. These are especially useful when dealing with ambiguous kNCs. The need for annotated data is a drawback of supervised approaches. Manual annotations are costly and time-consuming. To circumvent this need for annotated data, previous work has used cross-lingual supervision based on parallel corpora. Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996); Girju (2007); Sinha (2009); Tsvetkov and Wintner (2010); Ziering et al. (2013). 82 Proceedings of the 11th International Conference on Computational Semantics, pages 82–87, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistic"
W15-0112,P07-1072,0,0.027193,"for annotated data is a drawback of supervised approaches. Manual annotations are costly and time-consuming. To circumvent this need for annotated data, previous work has used cross-lingual supervision based on parallel corpora. Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996); Girju (2007); Sinha (2009); Tsvetkov and Wintner (2010); Ziering et al. (2013). 82 Proceedings of the 11th International Conference on Computational Semantics, pages 82–87, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics Ziering and Van der Plas (2014) propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed p"
W15-0112,N04-1016,0,0.0532201,"interpretation and of fundamental importance for many tasks in natural language processing such as machine translation. The correct French translation of luxury cattle truck depends on the internal structure. While [luxury cattle] truck is translated as camion pour b´etail de luxe, the preferred translation for luxury [cattle truck] is camion de luxe pour b´etail. Previous work on noun compound bracketing has shown that supervised beats unsupervised. The latter approaches use N-gram statistics or lexical patterns (Lauer, 1995; Nakov and Hearst, 2005; Barri`ere and M´enard, 2014), web counts (Lapata and Keller, 2004) or semantic relations (Kim and Baldwin, 2013) and evaluate on carefully selected evaluation data from encyclopedia (Lauer, 1995; Barri`ere and M´enard, 2014) or from general newspaper text (Kim and Baldwin, 2013). Vadas and Curran (2007a,b) manually annotated the Penn Treebank and showed that they improve over unsupervised results by a large margin. Pitler et al. (2010) used the data from Vadas and Curran (2007a) for a parser applicable on base noun phrases (NPs) of any length including coordinations. Barker (1998) presents a bracketing method for k-partite NPs that reduces the task to three-"
W15-0112,W05-0603,0,0.212229,"e, a proper structural analysis is a crucial part of noun compound interpretation and of fundamental importance for many tasks in natural language processing such as machine translation. The correct French translation of luxury cattle truck depends on the internal structure. While [luxury cattle] truck is translated as camion pour b´etail de luxe, the preferred translation for luxury [cattle truck] is camion de luxe pour b´etail. Previous work on noun compound bracketing has shown that supervised beats unsupervised. The latter approaches use N-gram statistics or lexical patterns (Lauer, 1995; Nakov and Hearst, 2005; Barri`ere and M´enard, 2014), web counts (Lapata and Keller, 2004) or semantic relations (Kim and Baldwin, 2013) and evaluate on carefully selected evaluation data from encyclopedia (Lauer, 1995; Barri`ere and M´enard, 2014) or from general newspaper text (Kim and Baldwin, 2013). Vadas and Curran (2007a,b) manually annotated the Penn Treebank and showed that they improve over unsupervised results by a large margin. Pitler et al. (2010) used the data from Vadas and Curran (2007a) for a parser applicable on base noun phrases (NPs) of any length including coordinations. Barker (1998) presents a"
W15-0112,C10-1100,0,0.016994,"work on noun compound bracketing has shown that supervised beats unsupervised. The latter approaches use N-gram statistics or lexical patterns (Lauer, 1995; Nakov and Hearst, 2005; Barri`ere and M´enard, 2014), web counts (Lapata and Keller, 2004) or semantic relations (Kim and Baldwin, 2013) and evaluate on carefully selected evaluation data from encyclopedia (Lauer, 1995; Barri`ere and M´enard, 2014) or from general newspaper text (Kim and Baldwin, 2013). Vadas and Curran (2007a,b) manually annotated the Penn Treebank and showed that they improve over unsupervised results by a large margin. Pitler et al. (2010) used the data from Vadas and Curran (2007a) for a parser applicable on base noun phrases (NPs) of any length including coordinations. Barker (1998) presents a bracketing method for k-partite NPs that reduces the task to three-word bracketings within a sliding window. One advantage of supervised approaches for this task is that kNCs are labeled in context so contextual features can be used in the learning framework. These are especially useful when dealing with ambiguous kNCs. The need for annotated data is a drawback of supervised approaches. Manual annotations are costly and time-consuming."
W15-0112,W09-2906,0,0.0208277,"data is a drawback of supervised approaches. Manual annotations are costly and time-consuming. To circumvent this need for annotated data, previous work has used cross-lingual supervision based on parallel corpora. Bergsma et al. (2011) made use of small amounts of annotated data on the target side and complement this with bilingual features from unlabeled bitext in a co-trained classifier for coordination disambiguation in complex NPs. Previous work on using cross-lingual data for the analysis of multi-word expressions (MWEs) of different types include Busa and Johnston (1996); Girju (2007); Sinha (2009); Tsvetkov and Wintner (2010); Ziering et al. (2013). 82 Proceedings of the 11th International Conference on Computational Semantics, pages 82–87, c London, UK, April 15-17 2015. 2015 Association for Computational Linguistics Ziering and Van der Plas (2014) propose an approach that refrains from using any human annotation. They use the fact, that languages differ in their preference for open or closed compounding (i.e., multiword vs. one-word compounds), for inducing the English bracketing of 3NCs. English open 3NCs like human rights abuses can be translated to partially closed phrases as in G"
W15-0112,tiedemann-2012-parallel,0,0.0319701,"Missing"
W15-0112,P07-1031,0,0.0246333,"s translated as camion pour b´etail de luxe, the preferred translation for luxury [cattle truck] is camion de luxe pour b´etail. Previous work on noun compound bracketing has shown that supervised beats unsupervised. The latter approaches use N-gram statistics or lexical patterns (Lauer, 1995; Nakov and Hearst, 2005; Barri`ere and M´enard, 2014), web counts (Lapata and Keller, 2004) or semantic relations (Kim and Baldwin, 2013) and evaluate on carefully selected evaluation data from encyclopedia (Lauer, 1995; Barri`ere and M´enard, 2014) or from general newspaper text (Kim and Baldwin, 2013). Vadas and Curran (2007a,b) manually annotated the Penn Treebank and showed that they improve over unsupervised results by a large margin. Pitler et al. (2010) used the data from Vadas and Curran (2007a) for a parser applicable on base noun phrases (NPs) of any length including coordinations. Barker (1998) presents a bracketing method for k-partite NPs that reduces the task to three-word bracketings within a sliding window. One advantage of supervised approaches for this task is that kNCs are labeled in context so contextual features can be used in the learning framework. These are especially useful when dealing wit"
W15-0112,C00-2137,0,0.155223,"Missing"
W15-0112,C14-1099,1,0.794449,"Missing"
W15-0112,I13-1104,1,0.883444,"Missing"
W15-0112,C10-2144,0,\N,Missing
W15-2514,W10-1410,0,0.0620296,"Missing"
W15-2514,N12-1005,0,0.19591,"ng a probabilistic neural network, motivated by previous work in the field of Statistical Language Modeling and Statistic Machine Translation. Specifically, the feature words are treated through a projection layer to become continuous vectors. This approach leads to a distributed representation of the words, that has shown to capture morphosyntactic and semantic information (Mikolov et al., 2013c; K¨oper et al., 2015). After that, the output of the network is a soft-max layer computing probabilities of the possible outputs, such as language models (Bengio et al., 2003), or translation models (Son et al., 2012). The input words can belong to one single language (language model case (Bengio et al., 2003)), or even two different languages (translation model case (Son et al., 2012)). More importantly, the size of projected vectors is much smaller than the vocabulary, aiming at a reduction of the data sparseness problem. We apply the concept in our system, by learning the probabilities of the pronouns given the word vectors in the input layer. In the works mentioned to motivate this structure, this projection layer is learned together with the neural network parameters (Schwenk, 2007; Mikolov et al., 20"
W15-2514,D13-1037,0,0.0738787,"(Bengio et al., 2003)), or even two different languages (translation model case (Son et al., 2012)). More importantly, the size of projected vectors is much smaller than the vocabulary, aiming at a reduction of the data sparseness problem. We apply the concept in our system, by learning the probabilities of the pronouns given the word vectors in the input layer. In the works mentioned to motivate this structure, this projection layer is learned together with the neural network parameters (Schwenk, 2007; Mikolov et al., 2010; Le et al., 2011). For the task of cross-lingual pronoun prediction, Hardmeier et al. (2013) also chose to learn the projection matrices and the neural network weights at the same OTHER Predicting pronouns across languages from a language with less variation to one with much more is a hard task that requires many different types of information, such as morpho-syntactic information as well as lexical semantics and coreference. We assumed that continuous word spaces fed into a multi-layer perceptron enriched with morphological tags and coreference resolution would be able to capture many of the linguistic regularities we found. Our results show that the model captures most of the lingu"
W15-2514,W15-2501,0,0.0711487,"Missing"
W15-2514,2005.mtsummit-papers.11,0,0.149152,"Missing"
W15-2514,W15-0105,0,0.0340881,"Missing"
W15-2514,J13-4004,0,0.219291,"olov et al., 2013a) to alleviate the training process. In contrast to English, the source language in this shared task, every noun in French has a grammatical gender. Pronouns agree in gender and number with their antecedents (or postcedents). As a consequence, in many cases in which the English translation contains the pronoun it, we need to choose between elle or il in French depending on the gender of the nouns the pronoun is referring to. In the example above the gender of the noun e´ nergie is feminine so we choose the pronoun elle. We included the Stanford Coreference Resolution system (Lee et al., 2013) in our model for this reason. Moreover, in an effort to compare the effectiveness of the word embeddings and handcrafted features for capturing morpho-syntactic information, we decided to use Morfette (Seddah et al., 2010) to supply information on gender and number for each French word explicitly. 2 2. Three words in front of and three words after the English pronoun. This will capture whether the passive is used, whether we find one of the verbs that are often found with expletive pronouns etc. 3. Two words in front of and three words after the French pronoun. This will capture whether we fi"
W15-2514,N13-1090,0,0.569558,"y different factors. By analysing the linguistic characteristics of this problem, we identified the factors contributing to the predictability of the pronouns, as described in Section 2. The dependencies are modeled by using a probabilistic neural network, motivated by previous work in the field of Statistical Language Modeling and Statistic Machine Translation. Specifically, the feature words are treated through a projection layer to become continuous vectors. This approach leads to a distributed representation of the words, that has shown to capture morphosyntactic and semantic information (Mikolov et al., 2013c; K¨oper et al., 2015). After that, the output of the network is a soft-max layer computing probabilities of the possible outputs, such as language models (Bengio et al., 2003), or translation models (Son et al., 2012). The input words can belong to one single language (language model case (Bengio et al., 2003)), or even two different languages (translation model case (Son et al., 2012)). More importantly, the size of projected vectors is much smaller than the vocabulary, aiming at a reduction of the data sparseness problem. We apply the concept in our system, by learning the probabilities of"
W16-1807,C00-2137,0,0.206765,"Missing"
W16-1807,W10-1734,0,0.314847,"neral-purpose statistical methods and necessitates automatic compound analysis as a principal part of many natural language processing tasks such as statistical machine translation (SMT). Therefore, previous work has tried to tackle the task of compound splitting (e.g., decomposing Eidotter to Ei ‘egg’ and Dotter ‘yolk’). Most compound splitters follow a generate-and-rank procedure. Firstly, all possible candidate splits are generated, e.g., Ei|dotter, Eid|otter, . . . , Eidott|er (Koehn and Knight, 2003) or a knowledge-rich morphological analyzer provides a set of plausible candidate splits (Fritzinger and Fraser, 2010). In a second step, the list of candidate splits is ranked according to statistical features such as constituent frequency (Stymne, 2008; Macherey et al., 2011; Weller and Heid, 2012) or frequency 50 Proceedings of the 12th Workshop on Multiword Expressions, pages 50–55, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics main. They disambiguated candidate splits using semantic relations from the ontology (e.g., Beckenbodenmuskel ‘pelvic floor muscle’ is binary split to Beckenboden |muskel using the part of relation). As back-off strategy, if the ontology looku"
W16-1807,N16-1078,1,0.887129,"or the disambiguation. Daiber et al., (2015) developed a compound splitter based on semantic analogy (e.g., bookshop is to shop as bookshelf is to shelf ). From word embeddings of compound and head word, they learned prototypical vectors representing the modification. During splitting, they determined the most suitable modifier by comparing the analogy to the prototypes. While Daiber et al., (2015) developed an autonomous splitter and focused on semantic analogy, we present a re-ranker that combines distributional similarity with additional splitting features. Very recently, Riedl and Biemann (2016) developed a semantic compound splitter that uses a pre-compiled distributional thesaurus for searching semantically similar substrings of a compound subject to decomposition. While their stand-alone method focuses on knowledge-lean split point determination, our approach improves splitters including the task of constituent normalization. Our contributions are as follows. We are the first to show that distributional semantics information as an additional feature helps in determining the best split among the candidate splits proposed by various compound splitters in an intrinsic evaluation. Mor"
W16-1807,R11-1058,0,0.465352,"candidate splits proposed by a splitter according to the DS scores only. 2 Evaluation setup While Weller at al., (2014) did not observe a difference in SMT performance between ranking candidate splits according to frequency and compositionality, we use an intrinsic evaluation measure actually revealing significant differences. We follow the evaluation approach of Ziering and Van der Plas (2016), who defined splitting accuracy3 in terms of determining the correct split point (SPAcc) and correctly normalizing the resulting constituents (NormAcc), and use the GermaNet4 gold standard developed by Henrich and Hinrichs (2011). We remove hyphenated compounds, which should be trivial splitting cases that do not need improvement by re-ranking. The final set comprises 51,230 compounds. In analogy to the distributional model of Weller et al., (2014), we adopt a setting whose parameters are tuned on a development set and prove best for compositionality (Schulte im Walde et al., 2013). It employs corpus-based co-occurrence information extracted from a window of 20 words to the left and 20 to the right of a target word. We restrict to the 20K most frequent nominal co-occurrents. 3.3 Inspected compound splitters 3 4 de.wik"
W16-1807,E03-1076,0,0.711372,"while the token frequency of individual compounds is low. This makes it hard to process closed compounds with general-purpose statistical methods and necessitates automatic compound analysis as a principal part of many natural language processing tasks such as statistical machine translation (SMT). Therefore, previous work has tried to tackle the task of compound splitting (e.g., decomposing Eidotter to Ei ‘egg’ and Dotter ‘yolk’). Most compound splitters follow a generate-and-rank procedure. Firstly, all possible candidate splits are generated, e.g., Ei|dotter, Eid|otter, . . . , Eidott|er (Koehn and Knight, 2003) or a knowledge-rich morphological analyzer provides a set of plausible candidate splits (Fritzinger and Fraser, 2010). In a second step, the list of candidate splits is ranked according to statistical features such as constituent frequency (Stymne, 2008; Macherey et al., 2011; Weller and Heid, 2012) or frequency 50 Proceedings of the 12th Workshop on Multiword Expressions, pages 50–55, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics main. They disambiguated candidate splits using semantic relations from the ontology (e.g., Beckenbodenmuskel ‘pelvic floor m"
W16-1807,P11-1140,0,0.0201006,"ne translation (SMT). Therefore, previous work has tried to tackle the task of compound splitting (e.g., decomposing Eidotter to Ei ‘egg’ and Dotter ‘yolk’). Most compound splitters follow a generate-and-rank procedure. Firstly, all possible candidate splits are generated, e.g., Ei|dotter, Eid|otter, . . . , Eidott|er (Koehn and Knight, 2003) or a knowledge-rich morphological analyzer provides a set of plausible candidate splits (Fritzinger and Fraser, 2010). In a second step, the list of candidate splits is ranked according to statistical features such as constituent frequency (Stymne, 2008; Macherey et al., 2011; Weller and Heid, 2012) or frequency 50 Proceedings of the 12th Workshop on Multiword Expressions, pages 50–55, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics main. They disambiguated candidate splits using semantic relations from the ontology (e.g., Beckenbodenmuskel ‘pelvic floor muscle’ is binary split to Beckenboden |muskel using the part of relation). As back-off strategy, if the ontology lookup fails, they used constituent frequency. We do not restrict to a certain domain and related ontology but use distributional semantics in combination with freq"
W16-1807,N16-1075,0,0.132111,"d split features for the disambiguation. Daiber et al., (2015) developed a compound splitter based on semantic analogy (e.g., bookshop is to shop as bookshelf is to shelf ). From word embeddings of compound and head word, they learned prototypical vectors representing the modification. During splitting, they determined the most suitable modifier by comparing the analogy to the prototypes. While Daiber et al., (2015) developed an autonomous splitter and focused on semantic analogy, we present a re-ranker that combines distributional similarity with additional splitting features. Very recently, Riedl and Biemann (2016) developed a semantic compound splitter that uses a pre-compiled distributional thesaurus for searching semantically similar substrings of a compound subject to decomposition. While their stand-alone method focuses on knowledge-lean split point determination, our approach improves splitters including the task of constituent normalization. Our contributions are as follows. We are the first to show that distributional semantics information as an additional feature helps in determining the best split among the candidate splits proposed by various compound splitters in an intrinsic evaluation. Mor"
W16-1807,schmid-etal-2004-smor,0,0.486211,"et al., (2014), we focus on true compounds and ignore non-split options. 51 3 7 7 3 Experiments 3.1 3.5 We inspect three different types of German compound splitters, ranging from knowledge-lean to knowledge-rich. Ziering and Van der Plas (2016) developed a corpus-based approach, where morphological operations are learned automatically from word inflection. Weller and Heid (2012) used a frequency-based approach with a list of PoS-tagged lemmas and an extensive handcrafted set of normalization rules. Fritzinger and Fraser (2010) combined the splitting output of the morphological analyzer SMOR (Schmid et al., 2004) with corpus frequencies. Data We use the German Wikipedia2 corpus comprising 665M words. We tokenize, lemmatize and PoStag using TreeTagger (Schmid, 1995). While we are aware of the fact that there are German corpora larger than Wikipedia which can boost the perfomance of distributional semantics methods, we decided to use the same corpora as used in previous work for the inspected compound splitters (Ziering and Van der Plas, 2016). By controlling for corpus size, we can contrast the differences in splitting performance with respect to information type (i.e., distributional similarity vs. fr"
W16-1807,S13-1038,1,0.901579,"Missing"
W16-1807,weller-heid-2012-analyzing,0,0.301311,"herefore, previous work has tried to tackle the task of compound splitting (e.g., decomposing Eidotter to Ei ‘egg’ and Dotter ‘yolk’). Most compound splitters follow a generate-and-rank procedure. Firstly, all possible candidate splits are generated, e.g., Ei|dotter, Eid|otter, . . . , Eidott|er (Koehn and Knight, 2003) or a knowledge-rich morphological analyzer provides a set of plausible candidate splits (Fritzinger and Fraser, 2010). In a second step, the list of candidate splits is ranked according to statistical features such as constituent frequency (Stymne, 2008; Macherey et al., 2011; Weller and Heid, 2012) or frequency 50 Proceedings of the 12th Workshop on Multiword Expressions, pages 50–55, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics main. They disambiguated candidate splits using semantic relations from the ontology (e.g., Beckenbodenmuskel ‘pelvic floor muscle’ is binary split to Beckenboden |muskel using the part of relation). As back-off strategy, if the ontology lookup fails, they used constituent frequency. We do not restrict to a certain domain and related ontology but use distributional semantics in combination with frequency-based split featur"
W16-1807,W14-5709,1,\N,Missing
W16-1807,I11-1024,0,\N,Missing
W16-3811,N09-2061,0,0.0185194,"Missing"
W16-3811,D10-1002,0,0.0222689,"Missing"
W16-3811,J02-3004,0,0.230701,"non-head in DCs, supporting Grimshaw’s approach. Moreover, the appearance of a by-phrase in DCs – which pace Borer (2013) is well attested in the corpus – seems to be harmful to our model, which shows us that the by-phrase test is not very telling for the structure and interpretation of DCs.3 2.2 Interpretation of DCs in natural language processing literature Research on deverbal compounds (referred to with the term nominalizations) in the NLP literature has focused on the task of predicting the underlying relation between deverbal heads and non-heads. Relation inventories range from 2-class (Lapata, 2002) to 3-class (Nicholson and Baldwin, 2006), and 13-class (Grover et al., 2005), where the 2-class inventory is restricted to the subject and direct object relations, the 3-class adds prepositional complements, and the 13-class further specifies the prepositional complement. Although we are performing the same task, our underlying aim is different. Instead of trying to reach state-of-the-art performance in the prediction task, we are interested in the contribution of a range of features based on linguistic literature, in particular, morphosyntactic features of the deverbal head. Features used in"
W16-3811,W12-3018,0,0.0333779,"Missing"
W16-3811,W06-1208,0,0.0351434,"g Grimshaw’s approach. Moreover, the appearance of a by-phrase in DCs – which pace Borer (2013) is well attested in the corpus – seems to be harmful to our model, which shows us that the by-phrase test is not very telling for the structure and interpretation of DCs.3 2.2 Interpretation of DCs in natural language processing literature Research on deverbal compounds (referred to with the term nominalizations) in the NLP literature has focused on the task of predicting the underlying relation between deverbal heads and non-heads. Relation inventories range from 2-class (Lapata, 2002) to 3-class (Nicholson and Baldwin, 2006), and 13-class (Grover et al., 2005), where the 2-class inventory is restricted to the subject and direct object relations, the 3-class adds prepositional complements, and the 13-class further specifies the prepositional complement. Although we are performing the same task, our underlying aim is different. Instead of trying to reach state-of-the-art performance in the prediction task, we are interested in the contribution of a range of features based on linguistic literature, in particular, morphosyntactic features of the deverbal head. Features used in the NLP literature mainly rely on occurr"
W17-5311,D15-1075,0,0.0167851,"etc. (referred to as matched examples) and other parts do not (referred to as mismatched examples). This paper presents Team LCT-MALTA’s submission to the shared task. In line with previous research, we obtain a single vector which is the Instead of the BiLSTM architecture, Tai et al. (2015) propose a tree-structured LSTM to capture the hierarchical structure of natural language sentences. Conneau et al. (2017) use BiLSTM with max pooling and achieve state-of-art results when testing their sentence representations on an NLI task based on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Lin et al. (2017) introduce a self-attention mechanism with multiple hops of attention on top of BiLSTM, where the different hops attend to different parts of the input sentence. Their approach represents sentence embeddings as 2-D matrices instead of vectors. 56 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 56–60, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 3 Our Approach dependencies across multiple words. As such, dependency parsing provides vital information on the sentence’s structure. Hence, we"
W17-5311,P17-1152,0,0.0404133,"distributed sentence representations, typically based on existing word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The baseline models at the shared task use GloVe vectors and present three approaches to obtaining sentence embeddings (Williams et al., 2017): a) taking the sum of the embeddings of all the words in the sentence (continous bag of words, CBOW); b) taking the average of the hidden state outputs of a bidirectional LSTM (BiLSTM; Hochreiter and Schmidhuber 1997) across all the words; and c) the Enhanced Sequential Inference Model (ESIM) by (Chen et al., 2017), which, however, relies on cross-sentence attention, which submissions to the shared task may not make use of. Introduction The RepEval 2017 Shared Task aims to evaluate fixed-length vector representations (or embeddings) of sentences on the basis of a natural language understanding task, viz. natural language inference (NLI), also known as recognising textual entailments. Given two sentences, the first being the premise and the second the hypothesis, the goal of NLI is to train a classifier to predict whether the relation of the hypothesis to the premise is one of entailment, contradiction o"
W17-5311,D17-1070,0,0.11498,"pus (see Williams et al. (2017) for details). Task participants are provided with both training and development datasets, where parts of the development data match the training data in terms of genre, topic etc. (referred to as matched examples) and other parts do not (referred to as mismatched examples). This paper presents Team LCT-MALTA’s submission to the shared task. In line with previous research, we obtain a single vector which is the Instead of the BiLSTM architecture, Tai et al. (2015) propose a tree-structured LSTM to capture the hierarchical structure of natural language sentences. Conneau et al. (2017) use BiLSTM with max pooling and achieve state-of-art results when testing their sentence representations on an NLI task based on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Lin et al. (2017) introduce a self-attention mechanism with multiple hops of attention on top of BiLSTM, where the different hops attend to different parts of the input sentence. Their approach represents sentence embeddings as 2-D matrices instead of vectors. 56 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 56–60, c Copenhagen, Denmark, Septembe"
W17-5311,D14-1181,0,0.00547889,"Missing"
W17-5311,D14-1162,0,0.0802714,"ared Task on natural language inference. Our system is a simple system based on a standard BiLSTM architecture, using as input GloVe word embeddings augmented with further linguistic information. We use max pooling on the BiLSTM outputs to obtain embeddings for sentences. On both the matched and the mismatched test sets, our system clearly beats the shared task’s BiLSTM baseline model. 1 2 Related Work Various works in recent years have dealt with the creation of distributed sentence representations, typically based on existing word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The baseline models at the shared task use GloVe vectors and present three approaches to obtaining sentence embeddings (Williams et al., 2017): a) taking the sum of the embeddings of all the words in the sentence (continous bag of words, CBOW); b) taking the average of the hidden state outputs of a bidirectional LSTM (BiLSTM; Hochreiter and Schmidhuber 1997) across all the words; and c) the Enhanced Sequential Inference Model (ESIM) by (Chen et al., 2017), which, however, relies on cross-sentence attention, which submissions to the shared task may not make use of. Introduction The RepEval 20"
W17-5311,L16-1680,0,0.0618536,"Missing"
W17-5311,P15-1150,0,0.219484,"g and test data for this 3-way classification task at RepEval 2017 are drawn from the Multi-Genre NLI, or MultiNLI corpus (see Williams et al. (2017) for details). Task participants are provided with both training and development datasets, where parts of the development data match the training data in terms of genre, topic etc. (referred to as matched examples) and other parts do not (referred to as mismatched examples). This paper presents Team LCT-MALTA’s submission to the shared task. In line with previous research, we obtain a single vector which is the Instead of the BiLSTM architecture, Tai et al. (2015) propose a tree-structured LSTM to capture the hierarchical structure of natural language sentences. Conneau et al. (2017) use BiLSTM with max pooling and achieve state-of-art results when testing their sentence representations on an NLI task based on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Lin et al. (2017) introduce a self-attention mechanism with multiple hops of attention on top of BiLSTM, where the different hops attend to different parts of the input sentence. Their approach represents sentence embeddings as 2-D matrices instead of vectors. 56 Procee"
W17-5311,P16-2022,0,\N,Missing
W19-4729,S16-2020,0,0.0349828,"Missing"
W19-4729,L16-1362,0,0.0378865,"Missing"
W19-4729,W19-5105,1,0.891798,"Missing"
W19-4729,S13-1038,0,0.0569087,"Missing"
W19-4729,C18-1117,0,0.0237223,"nstituents per year. We operate on 5-grams, which is the largest unit provided by Google Ngrams and use the words appearing in the 5-grams as both target words and context. We use the Part-of-Speech information already included in the Google Ngram corpus to extract noun-noun patterns. We then regard all other tokens in the 5-gram as context words and from this build up a semantic space repA common challenge in building semantic spaces on a diachronic scale is that when building the spaces for individual spans of time, the spaces need to be aligned later on in order to compare models (see e.g. Kutuzov et al., 2018, Section 3.3). We circumvent this problem by jointly learning the spaces for the target words. To do this, we take the sparse representations of the compounds and their constituents and jointly learn their dense representations using SVD. Similar to Hamilton et al. (2018) we also choose the dimensions of our embeddings to be 300. We carry out row normalization on the embeddings, in order to remove the bias of the frequency of the compounds and their constituents. We make use of six different semantic features that have been proposed in the literature to capture compositionality (Schulte im Wa"
W19-4729,I11-1024,0,0.244526,"its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vectorbased semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds. 1 2 Related Work From a synchronic perspective, Reddy et al. (2011), Schulte im Walde et al. (2013) and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations. However, Schulte im Walde et al. (2013) use German data and do not investigate diachronic changes. They report a Spearman’s ρ of 0.65 for predicting the compositionality of compounds based on the features of their semantic space and find that the modifiers mainly influence the compositionality of the whole compound, contrary to their expectation that the head should be the main source of influence. This is tru"
W19-4729,P16-1141,0,\N,Missing
W19-5105,D14-1004,0,0.0360214,"Missing"
W19-5105,P17-4008,0,0.0252929,"ttested in the recent corpus, based on judgments from independent human raters. 1 Comparatively little effort has been put into investigating the productive word formation process of compounding computationally. Although compounding is a rather challenging process to model as it involves concepts of compositionality and plausibility along with an intricate blend of semantic and syntactic processes, it is, in our view, one of the best starting points for modeling linguistic creativity. In contrast to relatively more studied topics in linguistics creativity, such as automatic poetry generation (Ghazvininejad et al., 2017), aesthetics are not involved. Moreover, compounding is limited to phrase level processes, as it involves a combination of known lexemes. In general, the creative power of language has been understudied in the field of natural language processing (NLP). The main focus is indeed on processing, as the name suggests. Creative thinking is a cognitive ability that fuels innovation. Therefore, the modelling and understanding of the underlying processes of novel concept creation is relevant. Ultimately, we aim to create tools that enhance peoples ability to interface more creatively with large data s"
W19-5105,P10-1045,0,0.0758513,"Missing"
W19-5105,P16-1141,0,0.0625765,"found as constituents of a compound or as simple standalone words (see Figure 1b). CompoundCentric: To the best of our knowledge, distributional models that are sensitive to the role a lexeme plays in a compound have not been tested before. Here we capture the distributional vectors of words based on their usage as constituents in a compound. So the word mill would have different representations, depending on its role in a compound. In Figure 1a, we show an example context that mill gets as a head, and an example context it gets as a modifier. 3.2.2 The Temporal Aspect Previous works such as Hamilton et al. (2016) have shown that meanings of certain words change over time. The same can be observed for compounds such as melting pot. The meaning of melting pot deviated from its original meaning (”A blast furnace”) to its current meaning, that of a society where people from many different cultures are living together, and assimilating into a cohesive whole. To test if time does impact our task, we envision two settings for our models: DecadeCentric: In this setting, we emphasise the temporal aspect by collecting counts of 32 Head context context context context context T he red pepper mill is on the shelf"
W19-5105,J03-3005,0,0.53943,"nding a candidate compound perform almost as well as features that are estimated on the basis of existing taxonomies such as WordNet. Although the semantic features they gathered from WordNet did not do very well, we believe our distributional semantic features are more fine-grained. The simple statistical features that did well in distinguishing rare compounds from nonce terms, would not be suitable in our scenario, where we try to generate novel, plausible compounds. We did however, follow their methodology for the automatic extraction of noun-noun compounds from corpora based on their PoS. Keller and Lapata (2003) obtain frequencies for unseen bigrams (including noun-noun bigrams) in corpora using Web search engines and show evidence of the reliability of the web counts for natural language processing, also by means of studying the correlation between the web counts and human plausibility judgments. The unseen bigrams are generated in a random fashion from corpus data, as the aim is not to generate plausible combinations, but to overcome data sparseness by providing counts from Web searches. ´ S´eaghdha (2010) uses topic models for seO lectional preference induction, and evaluates his models on the sam"
W19-5105,E03-1073,0,0.0690834,"tion with human plausibility ratings on a set of attested and corrupted compounds. However, their careful investigation of the different distributional semantic factors in their model have been very insightful for us and they inspired one of our systems. For example, they found that a higher relatedness between head and compound is associated with higher plausibility. And the similarity between the constituents is associated with a slightly higher plausibility as well. We used these features in one of our models as well. The related task of automatic novel compound detection was introduced by Lapata and Lascarides (2003). Their aim is to distinguish rare noun compounds from rare but nonce noun sequences. The biggest difference between their work and ours is that while they identify existing, albeit rare, and therefore possibly relatively novel compounds in corpora, we predict unseen, and therefore novel compounds, in an absolute sense. Still, the overlap between the tasks makes the work relevant. In their experiments, surface features, such as the frequency of the compound head/modifier, the likelihood of a word as a head/modifier, or the surface-grammatical context surrounding a candidate compound perform al"
W19-5105,petrov-etal-2012-universal,0,\N,Missing
