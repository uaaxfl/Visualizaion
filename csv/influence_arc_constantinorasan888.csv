2005.eamt-1.28,W03-2201,0,0.0797706,"Missing"
2014.tc-1.10,W14-3348,0,0.0453357,"Missing"
2014.tc-1.10,N13-1092,0,0.0151991,"ount of calculation involved in the task, we kept only those features which can be quickly calculated and proved the most useful for the original system. The system 86 Translating and The Computer 36 uses features based on surface form, parts of speech information, lemma, typed dependency parsing, named entities, paraphrasing, machine translation evaluation, and corpus pattern analysis (Hanks, 2013). Stanford CoreNLP3 toolkit (Manning et al., 2014) provides lemma, parts of speech (POS), named entities, and dependencies relations of words in each sentence. We used the PPDB paraphrase database (Ganitkevitch et al., 2013) to identify paraphrases. After extracting these features, we employed a support vector machine (SVM) in order to build a regression model to predict semantic similarity. The training dataset for the SVM is a set of 4934 parallel sentences of the SICK dataset (Marelli et al., 2014) annotated with similarity scores by humans. The SVM used an RBF kernel with C = 8 and γ = 0.125. More details about the method can be found in (Gupta et al., 2014). The trained SVM system works as a similarity calculator between any pair of sentences provided that the same feature values are available for this pair"
2014.tc-1.10,S14-2139,1,0.890722,"Missing"
2014.tc-1.10,P14-5010,0,0.00446235,"al., 2014) calculates the similarity and entailment between a pair of sentences. This system was adapted to measure the similarity between two TM segments. Given the amount of calculation involved in the task, we kept only those features which can be quickly calculated and proved the most useful for the original system. The system 86 Translating and The Computer 36 uses features based on surface form, parts of speech information, lemma, typed dependency parsing, named entities, paraphrasing, machine translation evaluation, and corpus pattern analysis (Hanks, 2013). Stanford CoreNLP3 toolkit (Manning et al., 2014) provides lemma, parts of speech (POS), named entities, and dependencies relations of words in each sentence. We used the PPDB paraphrase database (Ganitkevitch et al., 2013) to identify paraphrases. After extracting these features, we employed a support vector machine (SVM) in order to build a regression model to predict semantic similarity. The training dataset for the SVM is a set of 4934 parallel sentences of the SICK dataset (Marelli et al., 2014) annotated with similarity scores by humans. The SVM used an RBF kernel with C = 8 and γ = 0.125. More details about the method can be found in"
2014.tc-1.10,marelli-etal-2014-sick,0,0.0141921,"cy parsing, named entities, paraphrasing, machine translation evaluation, and corpus pattern analysis (Hanks, 2013). Stanford CoreNLP3 toolkit (Manning et al., 2014) provides lemma, parts of speech (POS), named entities, and dependencies relations of words in each sentence. We used the PPDB paraphrase database (Ganitkevitch et al., 2013) to identify paraphrases. After extracting these features, we employed a support vector machine (SVM) in order to build a regression model to predict semantic similarity. The training dataset for the SVM is a set of 4934 parallel sentences of the SICK dataset (Marelli et al., 2014) annotated with similarity scores by humans. The SVM used an RBF kernel with C = 8 and γ = 0.125. More details about the method can be found in (Gupta et al., 2014). The trained SVM system works as a similarity calculator between any pair of sentences provided that the same feature values are available for this pair of sentences. 3. Experiments and Results We carried out evaluations on two different sets. The test sets were generated by a random selection of segments from DGT-TM corpora (Steinberger et al., 2012). We used English as source and French as target. The target side (French) of the"
2014.tc-1.10,P02-1040,0,0.100894,"Missing"
2015.eamt-1.6,P02-1040,0,0.106419,"ch use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects"
2015.eamt-1.6,P06-1055,0,0.00717483,"t we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of G"
2015.eamt-1.6,1999.mtsummit-1.48,0,0.178112,"om scratch when an exact match is not available. However, this retrieval process is still limited to editdistance based measures operating on surface form c 2015 The authors. This article is licensed under a Creative  Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 35 Several researchers have used semantic or syntactic information in TMs, but their evaluations were shallow and most of the time limited to subjective evaluation carried out by the authors. This makes it hard to judge how much a semantically informed TM matching system can benefit a translator. Existing research (Planas and Furuse, 1999; Hod´asz and Pohl, 2005; Pekar and Mitkov, 2007; Mitkov, 2008) pointed out the need for similarity 1 http://www.omegat.org calculations in TMs beyond surface form comparisons. Both Planas and Furuse (1999) and Hodasz and Pohl (2005) proposed to use lemma and parts of speech along with surface form comparison. Hodasz and Pohl (2005) also extend the matching process to a sentence skeleton where noun phrases are either tagged by a translator or by a heuristic NP aligner developed for English-Hungarian translation. Planas and Furuse (1999) tested a prototype model on 50 sentences from the softwar"
2015.eamt-1.6,aziz-etal-2012-pet,0,0.0163664,"ificantly improves the retrieval results. We have also observed that there are different paraphrases used to bring about this improvement. In the interval [70, 85), 169 different paraphrases are used to retrieve 98 additional segments. To check the quality of the retrieved segments human evaluations are carried out. The sets’ distribution for human evaluation is given in the Table 3. The sets contain randomly selected segments from the additionally retrieved segments using paraphrasing which changed their top ranking.2 TH Set1 Set2 Total 4.1 Familiarisation with the Tool We used the PET tool (Aziz et al., 2012) for all our human experiments. However, settings were changed depending on the experiment. To familiarise translators with the PET tool we carried out a pilot experiment before the actual experiment with the Europarl corpus. This experiment was 100 117 16 13.67 9 24 14 100 2 5 7 [85, 100) 6 4 10 [70, 85) 6 7 13 Total 14 16 30 Table 3: Test Sets for Human Experiments 2 The sets are constructed so that a translator can post-edit a file in one sitting. There is no differentiation between the evaluations based on sets and all evaluations are carried out in both sets in a similar fashion with diff"
2015.eamt-1.6,2012.amta-papers.26,0,0.0678513,"M matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated using the machine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in p"
2015.eamt-1.6,W14-3348,0,0.0285348,"Missing"
2015.eamt-1.6,2006.amta-papers.25,0,0.165174,"Missing"
2015.eamt-1.6,N13-1092,0,0.0373631,"s better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing any named entities. Table 1 shows our corpus statistics. The translators involved in Segments Source words Target words TM 1565194 37824634 36267909 Test Set 9981 240916 230620 Table 1: Corpus Statistics our experiments were third year bachelor or masters translation students who were native speakers of German with English language level C1, in the age group of 21 to 40 years with a majority of female"
2015.eamt-1.6,R11-1014,0,0.0138269,"on metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into different types for efficient implementation based on the matching of the words between the source and corresponding"
2015.eamt-1.6,2014.eamt-1.2,1,0.60298,"Missing"
2015.eamt-1.6,2011.mtsummit-papers.37,0,0.166221,"ed was considered usable if less than half of the words required editing to match the input sentence. The authors concluded that the approach gives more usable results compared to Trados Workbench used as a baseline. Hodasz and Pohl (2005) claimed that their approach stores simplified patterns and hence makes it more probable to find a match in the TM. Pekar and Mitkov (2007) presented an approach based on syntactic transformation rules. On evaluation of the prototype model using a query sentence, the authors found that the syntactic rules help in retrieving better segments. Recently, work by Utiyama et al. (2011) and Gupta and Or˘asan (2014) presented approaches which use paraphrasing in TM matching and retrieval. Utiyama et al. (2011) proposed an approach using a finite state transducer. They evaluate the approach with one translator and find that paraphrasing is useful for TM both in terms of precision and recall of the retrieval process. However, their approach limits TM matching to exact matches only. Gupta and Or˘asan (2014) also use paraphrasing at the fuzzy match level and they report an improvement in retrieval and quality of retrieved segments. The quality of retrieved segments was evaluated"
2015.eamt-1.6,2005.mtsummit-papers.11,0,0.039766,"etter. 17 translators participated in this experiment. Finally, the decision of whether ‘ED is better’ or ‘PP is better’ is made on the basis of how many translators choose one over the other. 3.3 Subjective Evaluation with Three Options (SE3) This evaluation is similar to Evaluation SE2 except that we provided one more option to translators. Translators can choose among three options: A is better; B is better; or both are equal. 7 translators participated in this experiment. 4 Corpus, Tool and Translators expertise As a TM and test data, we have used EnglishGerman pairs of the Europarl V7.0 (Koehn, 2005) corpus with English as the source language and German as the target language. From this corpus we have filtered out segments of fewer than seven words and greater than 40 words, to create the TM and test datasets. Tokenization of the English data was done using the Berkeley Tokenizer (Petrov et al., 2006). We have used the lexical and phrasal paraphrases from the PPDB corpus (Ganitkevitch et al., 2013) of L size. In these experiments, we have not paraphrased any capitalised words (but we lowercase them for both baseline and paraphrasing similarities calculation). This is to avoid paraphrasing"
2015.eamt-1.6,2012.amta-wptp.2,0,0.0130456,"hine translation evaluation metric BLEU (Papineni et al., 2002). Simard and Fujita (2012) used different MT evaluation metrics for similarity calculation as well as for testing the quality of retrieval. For most of the metrics, the authors find that, the metric which is used in evaluation gives better score to itself (e.g. BLEU gives highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into dif"
2015.eamt-1.6,W14-0314,1,0.730688,"es highest score to matches retrieved using BLEU as similarity measure). Keystroke and post-editing time analysis are not new for TM and MT. Keystroke analysis has been used to judge translators’ productivity (Langlais and Lapalme, 2002; Whyman and Somers, 1999). 36 Koponen et al. (2012) suggested that post-editing time reflects the cognitive effort in post-editing the MT output. Sousa et al. (2011) evaluated different MT system performances against translating from scratch. Their study also concluded that subjective evaluations of MT system output correlate with the post-editing time needed. Zampieri and Vela (2014) used post-editing time to compare TM and MT translations. 3 Our Approach and Experiments We have used the approach presented in Gupta and Or˘asan (2014) to include paraphrasing in the TM matching and retrieval process. The approach classifies paraphrases into different types for efficient implementation based on the matching of the words between the source and corresponding paraphrase. Using this approach, the fuzzy match score between segments can be calculated in polynomial time despite the inclusion of paraphrases. The method uses dynamic programming along with greedy approximation. The me"
2015.eamt-1.6,2012.eamt-1.31,0,\N,Missing
2015.eamt-1.6,2012.tc-1.5,0,\N,Missing
2015.tc-1.3,W15-5202,0,0.0466142,"cant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memories. 2.3 Incorporation of Language Technology in Translation Memories Translation memories are among the most successfully used tools by professional translators. However, most of these tools rely on little language processing when they match and retrieve segments. Research carried out in the EXPERT project shows that even incorporation of simple language processing such as paraphrasing can help translators (Gupta and Or˘asan, 2014). Rather than expanding the segments stored in a translation memory with all the"
2015.tc-1.3,2014.tc-1.6,1,0.723752,"metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a). Resources like MyMemory2 contain large number of bi-segments that can be used in translation memories, but not all the bi-segments are true translations. For this reason, 2 https://mymemory.translated.net/ 19 Barbu (2015) proposed a method based on machine learning for cleaning existing translation memorie"
2015.tc-1.3,D14-1062,1,0.857312,"Missing"
2015.tc-1.3,C14-1182,1,0.796863,"Missing"
2015.tc-1.3,N15-1043,1,0.871728,"Missing"
2015.tc-1.3,2015.mtsummit-papers.22,1,0.879605,"Missing"
2015.tc-1.3,2014.eamt-1.2,0,0.0541452,"Missing"
2015.tc-1.3,W15-4905,1,0.824476,"Missing"
2015.tc-1.3,2014.amta-researchers.19,1,0.706099,"professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT, showing that for English-Chinese and English-French the proposed methods lead to better translations. Translation into morphological rich languages poses challenges to current methods in statistical machine translation. For this problem, Daiber and Sima’an (2015) propose a method which consists of two steps: first the source string is enriched with target morphological features and then fed into a translation model which takes care of reordering and lexical choice that 20 matches the provided morphologic"
2015.tc-1.3,W15-4907,1,0.836766,"by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015) investigate ways to collect and generate negative human feedback in various forms, including post-editing, and learn how to improve machine translation systems from this feedback, for example, by building word-level quality estimation models to mimic user feedback and introducing the predictions in SMT decoders. 2.5 Hybrid Approaches to Translation All the existing methods in MT have strengths and weaknesses and one of the most common ways to improve their performance is to combine them. Li et al. (2014) proposed a method for incorporating translation memories and linguistic knowledge in SMT,"
2015.tc-1.3,P02-1040,0,0.0939051,"uggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible"
2015.tc-1.3,W15-4107,0,0.0154388,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-wptp.4,0,0.0124165,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,2015.mtsummit-papers.11,0,0.0130963,"n flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently tested, compared and optimised based on user feedback. HandyCAT is an implementation of the proposed framework. The output of machine translation systems is usually evaluated using standard metrics such as BLEU (Papineni et al., 2002). However, these metrics are not necessarily that useful to translation companies. To this end, research is currently going on to develop a method that can predict the post-editing effort required by a given sentence (Béchara, 2015; Parra Escartín and Arcedillo, 2015a; Parra Escartín and Arcedillo, 2015b; Parra Escartín and Arcedillo, 2015c). 2.2 Data Collection and Preparation Given that the focus of the EXPERT project is on data-driven translation technologies, a significant amount of work is dedicated to collecting and preparing of relevant data. Costa et al. (2014) shows how it is possible to compile comparable corpora from the Internet using distributional similarity measures. This method is currently being integrated in a web-based application capable of semi-automatically compiling multilingual comparable and parallel corpora (Costa et al., 2015a)."
2015.tc-1.3,W15-5201,0,0.0277695,"largely due to the fact that in many cases the real needs of translators were not considered when designing these tools. To this end, a survey with professional translators was carried out in order to find out their views and requirements regarding various technologies, and their current work practices. Thanks to the help of the commercial partners in the project, the survey received 736 complete responses, from a total of over 1300 responses, which is more than in other similar surveys. A first analysis of the data is presented in (Zaretskaya et al., 2015) with more analyses underway. Parra Escartín (2015) carried out another study with professional translators in an attempt to find out “missing functionalities” of translation memories that could potentially improve their productivity. An interesting feature suggested was to generate segments on fly from fragments of previously translated segments. An implementation based on pattern matching showed that even such a simple approach can be potentially useful. Another way to address the needs of translators is to design flexible interfaces. Lewis et al. (2014) propose a framework in which new components of a user interface can be consistently test"
2015.tc-1.3,2014.eamt-1.21,1,0.710839,"em in translation memories and statistical machine translation. 2.4 The Human Translator in the Loop Post-editing is one of the most promising ways of integrating the output of machine translation methods in the workflows used by translation companies. Quality estimation methods are used to decide whether a sentence should be translated from scratch or it is good enough to be given to a post-editor. Most of the existing methods focus on estimating the quality of sentences, but in some cases it is necessary to estimate the quality of the translation of a whole document. The work carried out by Scarton and Specia (2014) in the EXPERT project focuses on document level quality estimation. Automatic post-editing provides an additional way to simplifying the work of professional translators. Pal (2015) shows how it is possible to apply Hierarchical Phrase Based Statistical Machine Translation to the task of monolingual Statistical Automatic Post-editing. Evaluation using standard MT metrics shows that automatically post-edited texts are better than the raw translations. In addition, an experiment with four professional translators reveals that the post-editing effort is also reduced. Logacheva and Specia (2015)"
2015.tc-1.3,W14-3323,0,0.0605642,"Missing"
2015.tc-1.3,2015.eamt-1.6,1,\N,Missing
2020.coling-main.445,2020.acl-main.747,0,0.253815,"Missing"
2020.coling-main.445,N19-1423,0,0.278988,"odel requires extensive predictor pre-training and relies on large parallel data and computational resources. In order to remove the dependency on large parallel data, which also entails the need for powerful computational resources, we propose to use crosslingual embeddings that are already fine-tuned to reflect properties between languages. We assume that by using them we will ease the burden of having complex neural network architectures. Over the last few years there has been significant work done in the area of crosslingual embeddings (Ruder et al., 2019). Since the introduction of BERT (Devlin et al., 2019), transformer models have been used successfully for various NLP tasks such as named entity recognition (Devlin et al., 2019), sentence classification (Sun et al., 2019), and question answering (Devlin et al., 2019), in many cases improving the state of the art. Most of the tasks were focused on English due to the fact that most of the pre-trained transformer models were trained on English data. Although there are several multilingual models like multilingual BERT (mBERT) (Devlin et al., 2019) and multilingual DistilBERT (mDistilBERT) (Sanh et al., 2019), researchers expressed some reservation"
2020.coling-main.445,N15-1124,0,0.030281,"4.2 Predicting DA Even though HTER has been typically used to assess quality in machine translations, the reliability of this metric for assessing the performance of quality estimation systems has been questioned by researchers (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2017), where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015). We used a recently created dataset to predict DA in machine translations which was released for the WMT 2020 quality estimation shared task 1 (Specia et al., 2020). The dataset is composed of data extracted from Wikipedia for six language pairs, consisting of high-resource English-German (En-De) and English-Chinese (En-Zh), medium-resource Romanian-English (Ro-En) and Estonian-English (EtEn), and low-resource Sinhala-English (Si-En) and Nepalese-English (Ne-En), as well as a Russian2 Language codes are available in ISO 639-1 Registration Authority Website Online - https://www.loc.gov/ standa"
2020.coling-main.445,C16-1294,0,0.022276,"English-Czech (En-Cs), English-German (EnDe), English-Russian (En-Ru), English-Latvian (En-Lv) and German-English (De-En). The texts are from a variety of domains and the translations were produced using both neural and statistical machine translation systems. More details about these datasets can be found in Table 1 and in (Specia et al., 2018; Fonseca et al., 2019). 4.2 Predicting DA Even though HTER has been typically used to assess quality in machine translations, the reliability of this metric for assessing the performance of quality estimation systems has been questioned by researchers (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2017), where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015). We used a recently created dataset to predict DA in machine translations which was released for the WMT 2020 quality estimation shared task 1 (Specia et al., 2020). The dataset is composed of data extracted from Wikipedia for si"
2020.coling-main.445,C18-1266,0,0.272386,"workflows as they have numerous potential uses. They can be employed to select the best translation when several translation engines are available or can inform the end user about the reliability of automatically translated content. In addition, QE systems can be used to decide whether a translation can be published as it is in a given context, or whether it requires human post-editing before publishing or even translation from scratch by a human (Kepler et al., 2019). The estimation of translation quality can be done at different levels: document level, sentence level and word/phrase level (Ive et al., 2018). In this research we focus on sentence-level quality estimation. As we discuss in Section 2, at present neural-based QE methods constitute the state of the art in quality estimation. However, these approaches are based on complex neural networks and require resourceintensive training. This resource-intensive nature of these deep-learning-based frameworks makes it expensive to have QE systems that work for several languages at the same time. Furthermore, these architectures require a large number of annotated instances for training, making the quality estimation task very difficult for low-res"
2020.coling-main.445,P19-3020,0,0.161523,"Missing"
2020.coling-main.445,W17-4763,0,0.0617344,"endent on linguistic processing and feature engineering to train traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013). Even though, they provided good results, these traditional approaches are no longer the state of the art. In recent years, neural-based QE systems have consistently topped the leader boards in WMT quality estimation shared tasks (Kepler et al., 2019). For example, the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and does not rely on feature engineering at all (Kim et al., 2017). POSTECH revolves around an encoder-decoder Recurrent Neural Network (RNN) (referred to as the ’predictor’), stacked with a bidirectional RNN (the ’estimator’) that produces quality estimates. In the predictor, an encoderdecoder RNN model predicts words based on their context representations and in the estimator step there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally i"
2020.coling-main.445,W15-3037,0,0.0289313,"ir context representations and in the estimator step there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally intensive (Ive et al., 2018). The POSTECH architecture was later re-implemented in deepQuest (Ive et al., 2018). OpenKiwi (Kepler et al., 2019) is another open-source QE framework developed by Unbabel. It implements four different neural network architectures QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016), Predictor-Estimator (Kim et al., 2017) and a stacked model of those architectures. Both the QUETCH and N U QE architectures have simple neural network models that do not rely on 1 The public GitHub repository is available on https://github.com/tharindudr/transquest and the official documentation is available on https://tharindudr.github.io/TransQuest. 5071 additional parallel data, but do not perform that well. The Predictor-Estimator model is similar to the POSTECH architecture and relies on additional parallel data. In OpenKiwi, the best performance for sente"
2020.coling-main.445,W16-2387,0,0.0155901,"n the estimator step there is a bidirectional RNN model to produce quality estimates for words, phrases and sentences based on representations from the predictor. To be effective, POSTECH requires extensive predictor pre-training, which means it depends on large parallel data and is computationally intensive (Ive et al., 2018). The POSTECH architecture was later re-implemented in deepQuest (Ive et al., 2018). OpenKiwi (Kepler et al., 2019) is another open-source QE framework developed by Unbabel. It implements four different neural network architectures QUETCH (Kreutzer et al., 2015), N U QE (Martins et al., 2016), Predictor-Estimator (Kim et al., 2017) and a stacked model of those architectures. Both the QUETCH and N U QE architectures have simple neural network models that do not rely on 1 The public GitHub repository is available on https://github.com/tharindudr/transquest and the official documentation is available on https://tharindudr.github.io/TransQuest. 5071 additional parallel data, but do not perform that well. The Predictor-Estimator model is similar to the POSTECH architecture and relies on additional parallel data. In OpenKiwi, the best performance for sentence-level quality estimation wa"
2020.coling-main.445,N19-4009,0,0.0355003,"entences. In that column NMT indicates Neural Machine Translation and SMT indicates Statistical Machine Translation. Competition shows the quality estimation competition in which the data was released and the last column indicates the number of instances the train, development and test dataset had in each language pair respectively. English (En-Ru) dataset which combines articles from Wikipedia and Reddit (Fomicheva et al., 2020). These datasets have been collected by translating sentences sampled from source-language articles using state-of-the-art NMT models built using the fairseq toolkit (Ott et al., 2019) and annotated with DA scores by professional translators. Each translation was rated with a score from 0-100 according to the perceived translation quality by at least three translators (Specia et al., 2020). The DA scores were standardised using the z-score. The quality estimation systems evaluated on these datasets have to predict the mean DA z-scores of test sentence pairs. Each language pair has 7,000 sentence pairs in the training set, 1,000 sentence pairs in the development set and another 1,000 sentence pairs in the testing set. 5 Evaluation and discussion This section presents the eva"
2020.coling-main.445,P19-1493,0,0.0205766,"arious NLP tasks such as named entity recognition (Devlin et al., 2019), sentence classification (Sun et al., 2019), and question answering (Devlin et al., 2019), in many cases improving the state of the art. Most of the tasks were focused on English due to the fact that most of the pre-trained transformer models were trained on English data. Although there are several multilingual models like multilingual BERT (mBERT) (Devlin et al., 2019) and multilingual DistilBERT (mDistilBERT) (Sanh et al., 2019), researchers expressed some reservations about their ability to represent all the languages (Pires et al., 2019). In addition, although mBERT and mDistilBERT showed some crosslingual characteristics, they do not perform well on crosslingual benchmarks (K et al., 2020). XLM-RoBERTa (XML-R) was released in November 2019 (Conneau et al., 2020) as an update to the XLM-100 model (Conneau and Lample, 2019). XLM-R takes a step back from XLM, eschewing XLM’s Translation Language Modeling (TLM) objective since it requires a dataset of parallel sentences, which can be difficult to acquire. Instead, XLM-R trains RoBERTa(Liu et al., 2019) on a huge, multilingual dataset at an enormous scale: unlabelled text in 104"
2020.coling-main.445,R19-1116,1,0.618898,"tmax layer that predicts the quality score of the translation. We used mean-squared-error loss as the objective function. Early experiments we carried out demonstrated that the CLS-strategy leads to better results than the other two strategies for this architecture. Therefore, we used the embedding of the [CLS] token as the input of a softmax layer. 2. SiameseTransQuest (STransQuest): The second approach proposed in this paper relies on the Siamese architecture depicted in Figure 1b which has shown promising results in monolingual semantic textual similarity tasks (Reimers and Gurevych, 2019; Ranasinghe et al., 2019). In this case, we feed the original text and the translation into two separate XLM-R transformer models. Similar to the previous architecture we used the same three pooling strategies for the outputs of the transformer models. We then calculated the cosine similarity between the two outputs of the pooling strategy. We used mean-squared-error loss as the objective function. In initial experiments we carried out with this architecture, the MEAN-strategy showed better results than the other two strategies. For this reason, we used the MEAN-strategy for our experiments. Therefore, cosine similari"
2020.coling-main.445,2020.wmt-1.122,1,0.89248,"inning solutions submitted to recent shared tasks on 15 different language pairs on different aspects of quality estimation. In fact, a tuned version of TransQuest was declared the winner for all 8 tasks of the direct This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 5070 Proceedings of the 28th International Conference on Computational Linguistics, pages 5070–5081 Barcelona, Spain (Online), December 8-13, 2020 assessment sentence level QE shared task organised at WMT 2020 (for more details see (Ranasinghe et al., 2020) and Section 5.1. The main contributions of this paper are the following: 1. We introduce TransQuest, an open-source framework, and use it to implement two neural network architectures that outperform current state-of-the-art quality estimation methods in two different aspects of sentence-level quality estimation. 2. To the best of our knowledge this is the first neural-based method which develops a model capable of providing quality estimation for more than one language pair. In this way we address the problem of high costs required to maintain a multi-language-pair QE environment. 3. We tack"
2020.coling-main.445,D19-1410,0,0.114697,"us scale: unlabelled text in 104 languages is extracted from CommonCrawl datasets, totalling 2.5TB of text. It is trained using only RoBERTa’s (Liu et al., 2019) masked language modelling (MLM) objective. Surprisingly, this strategy provided better results in crosslingual tasks. XLM-R outperforms mBERT on a variety of crosslingual benchmarks such as crosslingual natural language inference and crosslingual question answering (Conneau et al., 2020). Both architectures proposed in TransQuest have been successfully applied in the monolingual semantic textual similarity tasks (Devlin et al., 2019; Reimers and Gurevych, 2019). When applied in monolingual experiments, both of them use monolingual transformer models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) as the input. This inspired us to change the input in such a way that it can represent both the source and target sentences for which the quality of translation needs to be estimated, with the hope that the same architectures would also provide good results in the QE task. Our initial experiments showed that crosslingual embeddings like XLM-R provide better results than multilingual embeddings like mBERT. Therefore, in this research we explore t"
2020.coling-main.445,P13-4014,0,0.244407,"Missing"
2020.coling-main.445,P15-4020,0,0.440699,"followed by the evaluation and discussion in Section 5. The paper finishes with conclusions and ideas for future research directions. 2 Related Work During the past decade there has been tremendous progress in the field of quality estimation, largely as a result of the QE shared tasks organised annually by the Workshops on Statistical Machine Translation (WMT), more recently called the Conferences on Machine Translation, since 2012. The annotated datasets these shared tasks released each year have led to the development of many open-source QE systems like QuEst (Specia et al., 2013), QuEst++ (Specia et al., 2015), deepQuest (Ive et al., 2018), and OpenKiwi (Kepler et al., 2019). Before the neural network era, most of the quality estimation systems like QuEst (Specia et al., 2013) and QuEst++ (Specia et al., 2015) were heavily dependent on linguistic processing and feature engineering to train traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013). Even though, they provided good results, these traditional approaches are no longer the state of the art. In recent years, neural-based QE systems have consistently topped the leader boards"
2020.coling-main.445,2020.wmt-1.79,0,0.107633,"Missing"
2020.eamt-1.19,1978.tc-1.0,0,0.63431,"Missing"
2020.eamt-1.19,S17-2001,0,0.0398336,"Missing"
2020.eamt-1.19,D18-2029,0,0.0257507,"Missing"
2020.eamt-1.19,W15-5204,0,0.0316142,"ible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta and Orasan (2014) proposed a variant of the edit distance metric which incorporates paraphrases from PPDB5 using greedy approximation and dynamic programming. Both automatic evaluation and evaluation with translators show the advantages of using this approach (Gupta et al., 2016). Chatzitheodorou ("
2020.eamt-1.19,D17-1070,0,0.0555431,"ilarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments in TMs better than methods based on edit distance. We selected the Universal Sentence Encoder as our sentence encoder since it outperforms other sentence encoders like Infersent (Conneau et al., 2017) in many Natural Language Processing tasks including Semantic Retrieval (Cer et al., 2018). Also the recently release of Multilingual Universal Sentence Encoder 3 is available on 16 different languages (Yang et al., 2019). Since we are planning to expand our research to other language pairs than the English - Spanish pair investigated in this paper, the multilingual aspect of the Universal Sentence Encoder can prove very useful. The rest of the paper is organised as follows. Section 2 briefly describes several approaches used to improve the matching and retrieval in TMs. Section 3 contains inf"
2020.eamt-1.19,W14-3348,0,0.00843715,"trieve a match from the translation memory for one incoming sentence is just 1.6s, which is reasonable. In light of this, we decided to use the Transformer Architecture for future experiments since it is efficient enough and since it was reported that it provides better accuracy in semantic retrieval tasks than the DAN Architecture (Cer et al., 2018). 3.2.3 Results In order to compare the results obtained by our method with those of an existing translation memory tool we used Okapi which uses simple edit distance to retrieve matches from the translation memory. We calculated the METEOR score (Denkowski and Lavie, 2014) between the actual translation of the incoming segment and the match we retrieved from the translation memory with the transformer architecture of the Universal Sentence Encoder. We repeated the same process with the match we retrieved from Okapi. We used METEOR score since we believed it can capture the semantic similarity between two segments better than the BLEU score (Denkowski and Lavie, 2014). To understand the performance of our method, we first removed the segments where the match provided by Okapi and the Universal Sentence Encoder was same. Then, to have a better analysis of the res"
2020.eamt-1.19,2014.eamt-1.2,0,0.0377279,"007) show how it is possible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta and Orasan (2014) proposed a variant of the edit distance metric which incorporates paraphrases from PPDB5 using greedy approximation and dynamic programming. Both automatic evaluation and evaluation with translators show the advantages of using this approach (Gupta et al.,"
2020.eamt-1.19,D15-1124,0,0.0261836,"Missing"
2020.eamt-1.19,P15-1162,0,0.0283772,"Missing"
2020.eamt-1.19,steinberger-etal-2012-dgt,0,0.0874939,"Missing"
2020.eamt-1.19,P15-1150,0,0.00804804,"the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section 2 discusses some of the approaches proposed so far. Recent research on the topic of text similarity employed methods that rely on deep learning and various vector based representations used in this field (Ranasinghe et al., 2019b; Tai et al., 2015; Mueller and Thyagarajan, 2016). One of the reasons for this is that calculating the similarity between vectors is more straightforward than calculating the similarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments in TMs better than"
2020.eamt-1.19,2011.mtsummit-papers.37,0,0.0405215,"memories and Mitkov (2007) show how it is possible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta and Orasan (2014) proposed a variant of the edit distance metric which incorporates paraphrases from PPDB5 using greedy approximation and dynamic programming. Both automatic evaluation and evaluation with translators show the advantages of using this"
2020.eamt-1.19,2014.tc-1.11,0,0.0265146,"lly rich languages like Hungarian. They also experiment with sentence skeletons in which NPs are automatically aligned between source and target. Unfortunately, the paper presents only preliminary results. Pekar 4 https://github.com/tharindudr/ intelligent-translation-memories and Mitkov (2007) show how it is possible to improve the quality of matching by taking into consideration the syntactic structure of sentences. Unfortunately, the evaluation is carried out on only a handful of carefully selected segments. Another method which performs matching at level of syntactic trees is proposed in (Vanallemeersch and Vandeghinste, 2014). The results presented in their paper are preliminary and the authors notice that tree matching method is “prohibitively slow”. More recent work has focused on incorporating paraphrases into the matching and retrieving algorithm (Utiyama et al., 2011; Gupta and Orasan, 2014; Chatzitheodorou, 2015). Utiyama et al. (2011) proposed a finite transducer which considers paraphrases during the matching. The evaluation shows that the method improves both precision and recall of matching, but it was carried out with only one translator and focused only on segments with exactly the same meaning. Gupta"
2020.eamt-1.19,1999.mtsummit-1.48,0,0.433734,"ngines, as well as project management features (Gupta et al., 2016). Even though retrieval of previously translated segments is a key feature in a TM system, this process is still very much limited to edit-distance based measures. Researchers working on natural language processing have proposed a number of methods which try to improve the existing matching and retrieval approaches used by translation memories. However, the majority of these approaches are not suitable for large TMs, like the ones normally employed by professional translators or were evaluated on very small number of segments. Planas and Furuse (1999) extend the edit distance metric to incorporate lemmas and part-of-speech information when calculating the similarity between two segments, but they test their approach on less than 150 segments from two domains using two translation memories with less than 40,000 segments in total. Lemmas and part-of-speech information is also used in (Hod´asz and Pohl, 2005) in order to improve matching, especially for morphologically rich languages like Hungarian. They also experiment with sentence skeletons in which NPs are automatically aligned between source and target. Unfortunately, the paper presents"
2020.eamt-1.19,R19-1115,1,0.928453,"tures are used to express the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section 2 discusses some of the approaches proposed so far. Recent research on the topic of text similarity employed methods that rely on deep learning and various vector based representations used in this field (Ranasinghe et al., 2019b; Tai et al., 2015; Mueller and Thyagarajan, 2016). One of the reasons for this is that calculating the similarity between vectors is more straightforward than calculating the similarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments"
2020.eamt-1.19,R19-1116,1,0.910606,"tures are used to express the same idea. As a result, even if the TM contains a semantically similar segment, the retrieval algorithm will not be able to identify it in most of the cases. Researchers tried to address this shortcoming of the edit distance metric by employing similarity metrics that can identify semantically similar segments even when they are different at token level. Section 2 discusses some of the approaches proposed so far. Recent research on the topic of text similarity employed methods that rely on deep learning and various vector based representations used in this field (Ranasinghe et al., 2019b; Tai et al., 2015; Mueller and Thyagarajan, 2016). One of the reasons for this is that calculating the similarity between vectors is more straightforward than calculating the similarity between texts. It is easy to calculate how close or distant two vectors are by using well understood mathematical distance metrics. In addition, deep learning based methods proved more robust in numerous NLP applications. In this paper we propose a novel TM matching and retrieval method based on the Universal Sentence Encoder (Cer et al., 2018) which has the capability to capture semantically similar segments"
2020.eamt-1.19,D19-1410,0,0.0189511,"If we further analyse the fuzzy match score range 0.6-0.8, as shown in table 10, the mean semantic textual similarity for the sentences provided by Universal Sentence Encoder is 0.768. Therefore, we assume that the matches retrieved from the Universal Sentence Encoder in the fuzzy match score range 0.6-0.8 will help to improve the translation productivity. However, this is something that we plan to analyse further by carrying out evaluations with professional translators. In the future, we also plan to experiment with other sentence encoders such as Infersent (Conneau et al., 2017) and SBERT (Reimers and Gurevych, 2019) and with alternative algorithms which are capable to capture semantic textual similarity between two sentences. We will try unsupervised methods like word vector averaging Fuzzy score 0.8 - 1.0 0.6 - 0.8 0.4 - 0.6 0.2 - 0.4 0 - 0.2 Mean STS score 0.952 0.768 0.642 0.315 0.121 Table 10: Mean STS score for the sentences retrieved by Universal Sentence Encoder for each fuzzy match score. Fuzzy score column shows the fuzzy match score ranges and Mean STS score column shows that mean STS score for the sentence retrieved by Universal Sentence Encoder for that fuzzy match score range. and word movin"
2020.rdsm-1.7,Q17-1010,0,0.0561955,"Missing"
2020.rdsm-1.7,P09-2041,0,0.583065,"means of journalistic cliches in articles of both datasets. The results were a test statistic of around 0.0628 and a p-value of 0.949, which suggests that there is no statistically significant difference between the use of these phrases in the two news categories (see Figure 1 for a probability density function plot of the journalistic register measure in both the fake and real datasets). This points to the fact that Arabic satirical news meticulously parodies real journalism register, making it difficult to identify just by using a simple typical news keyword-based approach. 11 Adopted from (Burfoot and Baldwin, 2009) and adapted to our task 74 Figure 1: Probability Density Function of Journalistic Register Measure 4.2 Sentiment Intensity Measure Professional journalists tend to express opinion objectively to avoid implied bias. By contrast, studies observed that English satirical news includes highly sentimental language for entertainment purpose (Golbeck et al., 2018; Yang et al., 2017; Rubin et al., 2016). Flouting the journalistic norm by intense affect phrases may well be considered a violation of a conversational maxim which creates satire as observed by philosophers such as Grice (Grice, 1991). Simi"
2020.rdsm-1.7,J81-4005,0,0.654421,"Missing"
2020.rdsm-1.7,C18-1285,0,0.632917,"Missing"
2020.rdsm-1.7,W04-1610,0,0.285194,"Missing"
2020.rdsm-1.7,D14-1181,0,0.00620731,"Missing"
2020.rdsm-1.7,L16-1006,0,0.022414,"es may well be considered a violation of a conversational maxim which creates satire as observed by philosophers such as Grice (Grice, 1991). Similarly, it was observed that satire in the fake news dataset is often created by an “emotional imbalance” where high positive and high negative words are present in a context of neutral journalism. To capture the above observation, the degree of sentiment intensity was measured per article against an Arabic emotion lexicon of highly positive and highly negative phrases. We used open-source emotional lexicons for Arabic language (Salameh et al., 2015; Mohammad et al., 2016a; Mohammad et al., 2016b). In an attempt to make the emotional lexicon as exhaustive as possible, a list of hand-picked highly positive and highly negative lexical items, including slang, were also added to the lexicon. The created emotion lexicon is denoted by L. Similar to the Journalistic Register measure, sentiment score was calculated for each article as follows: S= 1 X IL (ω) |Ω |ω∈Ω (3) where IL is the indicator function over the set L defined similar to (2). Sentiment intensity measures proved to be statistically significant (statistic ≈ 3.27, p-value ≈ 0.001) by a T-test with nan-pol"
2020.rdsm-1.7,D17-1317,0,0.068333,"Missing"
2020.rdsm-1.7,W16-0802,0,0.490222,"o the fact that Arabic satirical news meticulously parodies real journalism register, making it difficult to identify just by using a simple typical news keyword-based approach. 11 Adopted from (Burfoot and Baldwin, 2009) and adapted to our task 74 Figure 1: Probability Density Function of Journalistic Register Measure 4.2 Sentiment Intensity Measure Professional journalists tend to express opinion objectively to avoid implied bias. By contrast, studies observed that English satirical news includes highly sentimental language for entertainment purpose (Golbeck et al., 2018; Yang et al., 2017; Rubin et al., 2016). Flouting the journalistic norm by intense affect phrases may well be considered a violation of a conversational maxim which creates satire as observed by philosophers such as Grice (Grice, 1991). Similarly, it was observed that satire in the fake news dataset is often created by an “emotional imbalance” where high positive and high negative words are present in a context of neutral journalism. To capture the above observation, the degree of sentiment intensity was measured per article against an Arabic emotion lexicon of highly positive and highly negative phrases. We used open-source emotio"
2020.rdsm-1.7,N15-1078,0,0.0305346,"y intense affect phrases may well be considered a violation of a conversational maxim which creates satire as observed by philosophers such as Grice (Grice, 1991). Similarly, it was observed that satire in the fake news dataset is often created by an “emotional imbalance” where high positive and high negative words are present in a context of neutral journalism. To capture the above observation, the degree of sentiment intensity was measured per article against an Arabic emotion lexicon of highly positive and highly negative phrases. We used open-source emotional lexicons for Arabic language (Salameh et al., 2015; Mohammad et al., 2016a; Mohammad et al., 2016b). In an attempt to make the emotional lexicon as exhaustive as possible, a list of hand-picked highly positive and highly negative lexical items, including slang, were also added to the lexicon. The created emotion lexicon is denoted by L. Similar to the Journalistic Register measure, sentiment score was calculated for each article as follows: S= 1 X IL (ω) |Ω |ω∈Ω (3) where IL is the indicator function over the set L defined similar to (2). Sentiment intensity measures proved to be statistically significant (statistic ≈ 3.27, p-value ≈ 0.001) b"
2020.semeval-1.94,P19-4007,0,0.0505279,"Missing"
2020.semeval-1.94,W09-4406,0,0.0317659,"Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 717 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 717–723 Barcelona, Spain (Online), December 12, 2020. and a decision tree to further analyse the candidates. Finally, the results are ranked using heuristic rules. All aspects of the system were developed by analysing the Institut Universitari de Ling¨u´ıstica Aplicada technical corpus in Spanish, which is also used for evaluation. Machine learning algorithms have also been used for definition extraction. Gaudio and Branco (2009) describe an approach that is said to be language independent and test it with decision trees and Random Forest, as well as Na¨ıve Bayes, k-Nearest Neighbour and Support Vector Machines using different sampling techniques to varying degrees of success. Kobyli´nski and Przepi´orkowski (2008) process Polish texts and use Balanced Random Forests, which bootstrap equal sets of positive and negative training examples to the classifier, as opposed to a larger group of unequal sets of training examples. Overall, while the approach is said to increase run time, it does bring minor increases in perform"
2020.semeval-1.94,C92-2082,0,0.514864,"including an automatically extended training set. Overall, the approach achieves acceptable evaluation scores, while maintaining flexibility in architecture selection. 1 Introduction Definition Extraction refers to the task in Natural Language Processing (NLP) of detecting and extracting a term and its definition in different types of text. A common use of automatic definition extraction is to help building dictionaries (Kobyli´nski and Przepi´orkowski, 2008), but it can be employed for many other applications. For example, ontology building can benefit from methods that extract definitions (Hearst, 1992; Malais´e et al., 2007), whilst the fields of definition extraction and information extraction can employ similar methodologies. It is therefore normal that there is growing interest in the task of definition extraction. This paper describes our system that participated in two of the three subtasks of Task 6 at SemEval 2020 (DeftEval), a shared task focused on definition extraction from a specialised corpus. Our method employs state-of-the-art neural architectures in combination with automatic methods which extend and clean the provided dataset. The remaining parts of this paper are structure"
2020.semeval-1.94,D14-1181,0,0.0181281,"Missing"
2020.semeval-1.94,N16-1030,0,0.0494826,"Missing"
2020.semeval-1.94,W04-1807,0,0.0775387,"Missing"
2020.semeval-1.94,R19-1116,1,0.758829,"nce classification task, we experimented with three different neural architectures: Convolutional Neural Network (CNN) (Kim, 2014), Recurrent Neural Network (RNN) (Cui et al., 2018) and Transformer (Devlin et al., 2018). After running various configurations, we found the Transformer architecture to perform best. With the introduction of BERT (Devlin et al., 2018) transformer architectures have shown a massive success in a wide range of NLP tasks. Transformer architectures have been trained on general tasks like language modelling and then fine-tuned for classification tasks (Sun et al., 2019; Ranasinghe et al., 2019b). Transformer models take an input of a sequence and output the representation of the sequence. The sequence has one or two segments that the first token of the sequence is always [CLS] which contains the special classification embedding and another special token [SEP] is used for separating segments. For text classification tasks, transformer models take the final hidden state h of the first token [CLS] as the representation of the whole sequence (Sun et al., 2019). The [CLS] token was then fed in to a simple softmax classifier to predict the label of the whole sentence: whether it contains"
2020.semeval-1.94,D19-1410,0,0.0128115,"ended data from Wikipedia was not useful, however, for a wider approach with higher recall, this could be more helpful. We also tried to participate in the final subtask, Relation Classification. However, due to time constraints, we were not able to achieve a valid submission for the this subtask. We approached it as a sequence pair classification task and employed a Siamese Neural Network which was shown to perform well in sequence pair classification tasks (Mueller and Thyagarajan, 2016; Ranasinghe et al., 2019a). The 721 architecture we employed is similar to the architecture presented in (Reimers and Gurevych, 2019). When two sequences have a relation, we extracted the sequences and provided them as the input for the Siamese transformer architecture. Then we used the objective function suggested as classification objective function in (Reimers and Gurevych, 2019) and optimised the cross-entropy loss. Due to the complexity of this task, we managed to run only a baseline of the proposed architecture which achieved very low evaluation scores on the development data. Therefore, we did not have a submission for this task and do not present any results here. In future, we hope to carry out further experiments"
2020.semeval-1.94,W19-4015,0,0.385007,"nguage independent and test it with decision trees and Random Forest, as well as Na¨ıve Bayes, k-Nearest Neighbour and Support Vector Machines using different sampling techniques to varying degrees of success. Kobyli´nski and Przepi´orkowski (2008) process Polish texts and use Balanced Random Forests, which bootstrap equal sets of positive and negative training examples to the classifier, as opposed to a larger group of unequal sets of training examples. Overall, while the approach is said to increase run time, it does bring minor increases in performance with some fine-tuning. Most recently, Spala et al. (2019) have created DEFT, a corpus for definition extraction from unstructured and semi-structured texts. Citing some of the pattern-based approaches also mentioned here, the authors argue that definitions have been well-defined and not necessarily representative of natural language. Therefore, a new corpus is presented that is said to more accurately represent natural language, and includes more messy examples of definitions. Parts of the DEFT corpus make up the dataset for this shared task, which is described in more detail in the following section. 3 Subtasks and Dataset The DeftEval shared task"
2020.wanlp-1.3,I17-1051,0,0.0214143,"how far automatic sentiment classification systems can capture sentiment information from the translations (Afli et al., 2017; Araujo et al., 2016; Shalunts et al., 2016). The objective of most research in this area is from a sentiment classification perspective rather than a translation accuracy perspective. Hence, it measures how far automatic translation of a language into English can help with the sentiment classification of that language by applying the available English sentiment resources on the target text (Demirtas and Pechenizkiy, 2013; Barhoumi et al., 2018; Mohammad et al., 2016; Abdalla and Hirst, 2017). This study is concerned with NMT accuracy of sentiment transfer at the word/phrase level and shows that inaccurate translation can transfer a completely opposite affect message. Moreover, the translation of UGC such as product reviews constitutes a significant challenge for NMT online tools in general and for Arabic UGC in particular. The reason is that Arabic UGC is usually a mix of Dialectical Arabic (DA) and Modern Standard Arabic (MSA) which differ significantly on the lexico-grammatical level. The same word or phrase can have opposite sentiment polarities in the two versions of the Arab"
2020.wanlp-1.3,P13-2088,0,0.0170644,"s think of particular products so the accuracy of each translation review counts. Broadly speaking, online tools such as Google Translate, are commonly utilized as an off-the-shelf solution for the translation of UGC in Arabic as well as in other languages. Error-analysis of sentiment translation by online tools, however, has proved that the true sentiment of Arabic reviews can be either missed or flipped to its exact opposite pole. 3 Error Analysis In order to measure how accurately NMT online tools transfer sentiment of Arabic UGC, we chose a dataset of book reviews scraped from Goodreads2 (Aly and Atiya, 2013). Each review has a rating between 1-5 assigned by its author. The language of the reviews is a mix of MSA and DA, with the largest majority of DA reviews in the Egyptian dialect. Reviews in the dataset are of varying lengths, but a large number of them have over than 100 tokens. Long reviews were split to a maximum of 20 tokens per review. After splitting, the data amounted to about 230,000 sentences. This dataset was translated into English using the Google Translate API and was analysed using both manual and automatic error analysis, focusing on mistranslation of sentiment. Automatic sentim"
2020.wanlp-1.3,D19-5617,0,0.0418703,"Missing"
2020.wanlp-1.3,W17-4714,0,0.0271506,"5.1 Parallel Data Preparation and Preprocessing It is worth mentioning here that the available authentic parallel English/Arabic data is mostly English to Arabic data (e.g. UN parallel corpora, TEDx scripts, and Tatoeba project (Alotaibi, 2017; Ho and Simon, 2016)). The greatest part of this data is in Arabic MSA and is not sentiment-oriented. Authentic Arabic(DA)-English parallel data in general and authentic Arabic(UGC)-English parallel data in particular is very scarce. Recently, the use of synthetic corpora in NMT led to promising results especially when authentic parallel data is scarce (Chinea-Rios et al., 2017; Cheng et al., 2020). Moreover, infusing contextual cues in the input layer has proved successful in improving the robustness of the NMT models for different translation tasks even with relatively small-sized datasets (Johnson et al., 2017; Pal et al., 2014; Si et al., 2019). Accordingly, in order to identify the correct sentiment polarity of contronyms in Arabic UGC, we opted for using the synthetic parallel data of the Goodreads reviews dataset (≈ 230, 000 sentences) for model training but with three main modifications. First, all the mistranslation instances of the chosen list of contronym"
2020.wanlp-1.3,Q17-1024,0,0.0716127,"Missing"
2020.wanlp-1.3,P17-4012,0,0.0427411,"Missing"
2020.wanlp-1.3,W18-1808,0,0.0452789,"Missing"
2020.wanlp-1.3,P12-2035,0,0.081195,"Missing"
2020.wanlp-1.3,W02-1011,0,0.0457492,"l (positive, neutral, negative) for each sentence as well as numeric confidence scores that range from 0 to 1 1 For example, Booking.com uses Google API to translate reviews on hotels for customers on the fly. https://www.goodreads.com 3 Microsoft Azure Text Analytics 2 26 for each sentiment category. Scores closer to 1 indicate a higher confidence in the label’s classification, while lower scores indicate lower confidence. For each sentence, the predicted scores associated with the labels (positive, negative and neutral) add up to 1. Following traditional methods in sentiment classification (Pang et al., 2002), we used the rating of the book review as indicative of its sentiment polarity and compared it to the confidence scores generated by the Azure Sentiment Analysis API. Accordingly, reviews were categorized based on discrepancies between the ratings and the confidence scores. A positive review that had a rating of 4 or above and an English negative sentiment score of 0.5 and above was extracted as an example of potential wrong negative polarity in the target text. Similarly, reviews with negative ratings of 2 and below and a positive English sentiment score of 0.5 and above were extracted as in"
2020.wanlp-1.3,N15-1078,0,0.020837,"Missing"
2020.wanlp-1.3,N13-1036,0,0.0605156,"Missing"
2020.wanlp-1.3,D19-5227,0,0.210009,"in the original tweet to 25 be altered. As a consequence, they build separate negative, neutral and positive sentiment SMT models to improve sentiment preservation in the target language. They show that a translation model specific of each sentiment pole provides much better results over a single baseline model trained on the whole twitter data, regardless of the sentiment class. They attempt to strike a balance between improving sentiment transfer and preserving translation accuracy as measured by evaluative metrics such as BLEU and METEOR (Lohar et al., 2018). A similar technique is used by Si et al. (2019) as they build a valence sensitive NMT model for the translation of ambiguous words that can have different polarities in different contexts. Each input sentence is annotated with a positive or negative label to indicate its polarity. They show that adding this tag to the source sentence at the training time and creating dual polarity embedding vectors for ambiguous words can improve sentiment transfer at the word level (Si et al., 2019). There has also been some research on finding alternative means for assessing the transfer of sentiment in MT other than the typical accuracy metrics. B´erard"
2020.wanlp-1.3,N12-1006,0,0.217539,"produced nonsensical target  text. For example, the MSA phrase ‘ QÒ®Ë@ ù®m &apos; Éëð’ is an idiom used to describe something that is unquestionably commended by the speaker. If the idiom is used in reference to a book, a good human translation would be: ‘It really shines through’. The Google Translate gives a literal translation – ‘ Is the moon hidden?’– which flips the sentiment polarity of the review from highly positive to neutral. (See Appendix A for more examples). 3.4 Dialectical Expressions Research studies have shown that dialectical Arabic presents several challenges to MT in general (Zbib et al., 2012). It was also observed from the manual analysis of the sample data that dialectical expressions constituted a special challenge for the preservation of sentiment in the source text. Arabic UGC is acceptably written in DA or MSA or a mix of both in the same text. A large number of DA sentiment expressions were either completely missed in the translation or mistranslated. For example, positive adjectives such as ‘ ÉK Aë’ (great), or negative adjectives such as ‘ ¡J J.«’ (silly) were mostly mistaken for proper nouns and transliterated into non-English words (Hayel, Abit). In some instances, the t"
2020.wmt-1.122,2020.acl-main.747,0,0.135208,"Missing"
2020.wmt-1.122,2020.tacl-1.35,0,0.097489,"Missing"
2020.wmt-1.122,W19-5401,0,0.108216,"Missing"
2020.wmt-1.122,C18-1266,0,0.139288,"age pairs according to the WMT 2020 official results. 1 Introduction The goal of quality estimation (QE) systems is to determine the quality of a translation without having access to a reference translation. This makes it very useful in translation workflows where it can be used to determine whether an automatically translated sentence is good enough to be used for a given purpose, or if it needs to be shown to a human translator for translation from scratch or postediting (Kepler et al., 2019). Quality estimation can be done at different levels: document level, sentence level and word level (Ive et al., 2018). This paper presents TransQuest, a sentence-level quality estimation framework which is the winning solution in all the language pairs in the WMT 2020 Sentence-Level Direct Assessment shared task (Specia et al., 2020). In the past, high preforming quality estimation systems such as QuEst (Specia et al., 2013) and QuEst++ (Specia et al., 2015) were heavily dependent on linguistic processing and feature engineering. These features were fed into traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013), which then determined the qu"
2020.wmt-1.122,P19-3020,0,0.154459,"Missing"
2020.wmt-1.122,W17-4763,0,0.22913,"ng and feature engineering. These features were fed into traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013), which then determined the quality of a translation. Even though, these approaches provide good results, they are no longer the state of the art, being replaced in recent years by neural-based QE systems which usually rely on little or no linguistic processing. For example the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and does not rely on feature engineering at all (Kim et al., 2017). In order to achieve high results, approaches such as POSTECH require extensive pre-training, which means they depend on large parallel data and are computationally intensive (Ive et al., 2018). TransQuest, our QE framework removes this dependency on large parallel data by using crosslingual embeddings (Ranasinghe et al., 2020) that are already fine-tuned to reflect properties between languages (Ruder et al., 2019). Ranasinghe et al. (2020) show that by using them, TransQuest eases the burden of having complex neural network architectures, which in turn entails a reduction of the computationa"
2020.wmt-1.122,R19-1116,1,0.815984,"sed mean-squared-error loss as the objective function (Ranasinghe et al., 2020). Similar to Ranasinghe et al. (2020), the early experiments we carried out demonstrated that the CLS-strategy leads to better results than the other two strategies for this architecture. Therefore, we used the embedding of the [CLS] token as the input of a softmax layer. 2. SiameseTransQuest (STransQuest): The second approach proposed in TransQuest relies on the Siamese architecture depicted in Figure 1b which has shown promising results in monolingual semantic textual similarity tasks (Reimers and Gurevych, 2019; Ranasinghe et al., 2019). For this, we fed the original text and the translation into two separate XLM-R transformer models. Similarly to the previous architecture, we experimented with the same three pooling strategies for the outputs of the transformer models (Ranasinghe et al., 2020). TransQuest then calculates the cosine similarity between the two outputs of the pooling strategy. TransQuest used mean-squared-error loss as the objective function. Similar to Ranasinghe et al. (2020) in the initial experiments we carried out with this architecture the MEAN-strategy showed better results than the other two strategies"
2020.wmt-1.122,2020.coling-main.445,1,0.858605,"t, being replaced in recent years by neural-based QE systems which usually rely on little or no linguistic processing. For example the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and does not rely on feature engineering at all (Kim et al., 2017). In order to achieve high results, approaches such as POSTECH require extensive pre-training, which means they depend on large parallel data and are computationally intensive (Ive et al., 2018). TransQuest, our QE framework removes this dependency on large parallel data by using crosslingual embeddings (Ranasinghe et al., 2020) that are already fine-tuned to reflect properties between languages (Ruder et al., 2019). Ranasinghe et al. (2020) show that by using them, TransQuest eases the burden of having complex neural network architectures, which in turn entails a reduction of the computational resources. That paper also shows that TransQuest performs well in transfer learning settings where it can be trained on language pairs for which we have resources and applied successfully on less resourced language pairs. The remainder of the paper is structured as follows. The dataset used in the competition is briefly discus"
2020.wmt-1.122,D19-1410,0,0.0257331,"he translation. TransQuest used mean-squared-error loss as the objective function (Ranasinghe et al., 2020). Similar to Ranasinghe et al. (2020), the early experiments we carried out demonstrated that the CLS-strategy leads to better results than the other two strategies for this architecture. Therefore, we used the embedding of the [CLS] token as the input of a softmax layer. 2. SiameseTransQuest (STransQuest): The second approach proposed in TransQuest relies on the Siamese architecture depicted in Figure 1b which has shown promising results in monolingual semantic textual similarity tasks (Reimers and Gurevych, 2019; Ranasinghe et al., 2019). For this, we fed the original text and the translation into two separate XLM-R transformer models. Similarly to the previous architecture, we experimented with the same three pooling strategies for the outputs of the transformer models (Ranasinghe et al., 2020). TransQuest then calculates the cosine similarity between the two outputs of the pooling strategy. TransQuest used mean-squared-error loss as the objective function. Similar to Ranasinghe et al. (2020) in the initial experiments we carried out with this architecture the MEAN-strategy showed better results tha"
2020.wmt-1.122,2020.wmt-1.79,0,0.191132,"Missing"
2020.wmt-1.122,P15-4020,0,0.270513,"ood enough to be used for a given purpose, or if it needs to be shown to a human translator for translation from scratch or postediting (Kepler et al., 2019). Quality estimation can be done at different levels: document level, sentence level and word level (Ive et al., 2018). This paper presents TransQuest, a sentence-level quality estimation framework which is the winning solution in all the language pairs in the WMT 2020 Sentence-Level Direct Assessment shared task (Specia et al., 2020). In the past, high preforming quality estimation systems such as QuEst (Specia et al., 2013) and QuEst++ (Specia et al., 2015) were heavily dependent on linguistic processing and feature engineering. These features were fed into traditional machine-learning algorithms like support vector regression and randomised decision trees (Specia et al., 2013), which then determined the quality of a translation. Even though, these approaches provide good results, they are no longer the state of the art, being replaced in recent years by neural-based QE systems which usually rely on little or no linguistic processing. For example the best-performing system at the WMT 2017 shared task on QE was POSTECH, which is purely neural and"
2020.wmt-1.122,P13-4014,0,0.2828,"Missing"
2021.acl-short.55,N19-1388,0,0.0223633,"et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider quality estimation as a language-specific task and build a different model for each language pair. This approach has many drawbacks in real-world applications, some of which are discussed in Section 1. Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages. Even though this has been applied to many tasks (Ranasinghe and Zampieri, 2020, 2021) including NMT (Nguyen and Chiang, 2017; Aharoni et al., 2019), multilingual approaches have been rarely used in QE (Sun et al., 2020). Shah and Specia (2016) explore QE models for more than one language where they use multitask learning with annotators or languages as multiple tasks. They show that multilingual models led to marginal improvements over bilingual ones with a traditional black-box, feature-based approach. In a recent study, Ranasinghe et al. (2020b) show that multilingual QE models based on transformers trained on high-resource languages can be used for zeroshot, sentence-level QE in low-resource languages. In a similar architecture, but w"
2021.acl-short.55,2020.acl-main.747,0,0.163753,"Missing"
2021.acl-short.55,N19-1423,0,0.0239418,"Estimation Early approaches in wordlevel QE were based on features fed into a traditional machine learning algorithm. Systems like QuEst++ (Specia et al., 2015) and MARMOT (Logacheva et al., 2016) were based on features used with Conditional Random Fields to perform wordlevel QE. With deep learning models becoming popular, the next generation of word-level QE algorithms were based on bilingual word embeddings fed into deep neural networks. Such approaches can be found in OpenKiwi (Kepler et al., 2019). However, the current state of the art in word-level QE is based on transformers like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider quality estimation as a language-specific task and build a different model for each language pair. This approach has many drawbacks in real-world applications, some of which are discussed in Section 1. Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages. Even though this has been applied to many tasks (Ranasinghe and Zampieri, 2020, 2021) including NMT (Nguyen"
2021.acl-short.55,2020.wmt-1.116,0,0.0389242,"Missing"
2021.acl-short.55,L16-1582,0,0.0237408,"odels as part of an open-source framework1 . 1 Documentation is available on http://tharindu. co.uk/TransQuest/ 434 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 434–440 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Model Architecture 2 Related Work Quality Estimation Early approaches in wordlevel QE were based on features fed into a traditional machine learning algorithm. Systems like QuEst++ (Specia et al., 2015) and MARMOT (Logacheva et al., 2016) were based on features used with Conditional Random Fields to perform wordlevel QE. With deep learning models becoming popular, the next generation of word-level QE algorithms were based on bilingual word embeddings fed into deep neural networks. Such approaches can be found in OpenKiwi (Kepler et al., 2019). However, the current state of the art in word-level QE is based on transformers like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider qu"
2021.acl-short.55,I17-2050,0,0.018743,"2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020). All of these approaches consider quality estimation as a language-specific task and build a different model for each language pair. This approach has many drawbacks in real-world applications, some of which are discussed in Section 1. Multilinguality Multilinguality allows training a single model to perform a task from and/or to multiple languages. Even though this has been applied to many tasks (Ranasinghe and Zampieri, 2020, 2021) including NMT (Nguyen and Chiang, 2017; Aharoni et al., 2019), multilingual approaches have been rarely used in QE (Sun et al., 2020). Shah and Specia (2016) explore QE models for more than one language where they use multitask learning with annotators or languages as multiple tasks. They show that multilingual models led to marginal improvements over bilingual ones with a traditional black-box, feature-based approach. In a recent study, Ranasinghe et al. (2020b) show that multilingual QE models based on transformers trained on high-resource languages can be used for zeroshot, sentence-level QE in low-resource languages. In a simi"
2021.acl-short.55,2020.wmt-1.122,1,0.950933,"which they have to focus more. Word-level QE is generally framed as a supervised ML problem (Kepler et al., 2019; Lee, 2020) trained on data in which the correctness of translation is labelled at word-level (i.e. good, bad, gap). The training data publicly available to build wordlevel QE models is limited to very few language pairs, which makes it difficult to build QE models for many languages. From an application perspective, even for the languages with resources, it is difficult to maintain separate QE models for each language since the state-of-the-art neural QE models are large in size (Ranasinghe et al., 2020b). In our paper, we address this problem by developing multilingual word-level QE models which perform competitively in different domains, MT types and language pairs. In addition, for the first time, we propose word-level QE as a zero-shot crosslingual transfer task, enabling new avenues of research in which multilingual models can be trained once and then serve a multitude of languages and domains. The main contributions of this paper are the following: i We introduce a simple architecture to perform word-level quality estimation that predicts the quality of the words in the source sentence"
2021.acl-short.55,2020.coling-main.445,1,0.951303,"which they have to focus more. Word-level QE is generally framed as a supervised ML problem (Kepler et al., 2019; Lee, 2020) trained on data in which the correctness of translation is labelled at word-level (i.e. good, bad, gap). The training data publicly available to build wordlevel QE models is limited to very few language pairs, which makes it difficult to build QE models for many languages. From an application perspective, even for the languages with resources, it is difficult to maintain separate QE models for each language since the state-of-the-art neural QE models are large in size (Ranasinghe et al., 2020b). In our paper, we address this problem by developing multilingual word-level QE models which perform competitively in different domains, MT types and language pairs. In addition, for the first time, we propose word-level QE as a zero-shot crosslingual transfer task, enabling new avenues of research in which multilingual models can be trained once and then serve a multitude of languages and domains. The main contributions of this paper are the following: i We introduce a simple architecture to perform word-level quality estimation that predicts the quality of the words in the source sentence"
2021.acl-short.55,2020.emnlp-main.470,1,0.798925,"Missing"
2021.acl-short.55,C18-1266,0,0.0152413,"e to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios. 1 Introduction Quality Estimation (QE) is the task of assessing the quality of a translation without having access to a reference translation (Specia et al., 2009). Translation quality can be estimated at different levels of granularity: word, sentence and document level (Ive et al., 2018). So far the most popular task has been sentence-level QE (Specia et al., 2020), in which QE models provide a score for each pair of source and target sentences. A more challenging task, which is currently receiving a lot of attention from the research community, is word-level quality estimation. This task provides more fine-grained information about the quality of a translation, indicating which words from the source have been incorrectly translated in the target, and whether the words inserted between these words are correct (good vs bad gaps). This information can be useful for post-editors"
2021.acl-short.55,2021.naacl-demos.17,1,0.828743,"Missing"
2021.acl-short.55,P19-3020,0,0.0328944,"Missing"
2021.acl-short.55,2020.wmt-1.118,0,0.632328,"nces. A more challenging task, which is currently receiving a lot of attention from the research community, is word-level quality estimation. This task provides more fine-grained information about the quality of a translation, indicating which words from the source have been incorrectly translated in the target, and whether the words inserted between these words are correct (good vs bad gaps). This information can be useful for post-editors by indicating the parts of a sentence on which they have to focus more. Word-level QE is generally framed as a supervised ML problem (Kepler et al., 2019; Lee, 2020) trained on data in which the correctness of translation is labelled at word-level (i.e. good, bad, gap). The training data publicly available to build wordlevel QE models is limited to very few language pairs, which makes it difficult to build QE models for many languages. From an application perspective, even for the languages with resources, it is difficult to maintain separate QE models for each language since the state-of-the-art neural QE models are large in size (Ranasinghe et al., 2020b). In our paper, we address this problem by developing multilingual word-level QE models which perfor"
2021.acl-short.55,P15-4020,0,0.0303708,"se the code and the pre-trained models as part of an open-source framework1 . 1 Documentation is available on http://tharindu. co.uk/TransQuest/ 434 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 434–440 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Model Architecture 2 Related Work Quality Estimation Early approaches in wordlevel QE were based on features fed into a traditional machine learning algorithm. Systems like QuEst++ (Specia et al., 2015) and MARMOT (Logacheva et al., 2016) were based on features used with Conditional Random Fields to perform wordlevel QE. With deep learning models becoming popular, the next generation of word-level QE algorithms were based on bilingual word embeddings fed into deep neural networks. Such approaches can be found in OpenKiwi (Kepler et al., 2019). However, the current state of the art in word-level QE is based on transformers like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) where a simple linear layer is added on top of the transformer model to obtain the predictions (Lee, 2020)."
2021.acl-short.55,2009.eamt-1.5,0,0.0577206,"form on par with the current language-specific models. In the cases of zeroshot and few-shot QE, we demonstrate that it is possible to accurately predict word-level quality for any given new language pair from models trained on other language pairs. Our findings suggest that the word-level QE models based on powerful pre-trained transformers that we propose in this paper generalise well across languages, making them more useful in real-world scenarios. 1 Introduction Quality Estimation (QE) is the task of assessing the quality of a translation without having access to a reference translation (Specia et al., 2009). Translation quality can be estimated at different levels of granularity: word, sentence and document level (Ive et al., 2018). So far the most popular task has been sentence-level QE (Specia et al., 2020), in which QE models provide a score for each pair of source and target sentences. A more challenging task, which is currently receiving a lot of attention from the research community, is word-level quality estimation. This task provides more fine-grained information about the quality of a translation, indicating which words from the source have been incorrectly translated in the target, and"
2021.acl-short.55,2020.aacl-main.39,0,0.059407,"Missing"
C08-3008,ou-etal-2008-development,1,0.449167,"Missing"
cabrio-etal-2008-qall,W03-2120,0,\N,Missing
cabrio-etal-2008-qall,W99-0310,0,\N,Missing
E03-1066,W00-0408,0,0.110325,"Missing"
E03-1066,J95-2003,0,0.0669656,"tences&apos; score in order to be considered important Indicating phrases: Paice (1981) noticed that it is possible to identify phrases which can be used to assess the importance of a sentence. The list of indicating phrases can be loaded, saved and modified in the tool. Surface clues: Several factors such as sentence location and length can be taken into consideration to decide the importance of the sentence. Lexical cohesion: Lexical cohesion as proposed in (Hoey, 1991) is used to produce extracts. Discourse information: Our own summarisation method uses information provided by Centering Theory (Grosz et al., 1995) to produce extracts. The automatic methods are used not only to identify important sentences, but also to remove sentences which do not contain important information. For example, it is possible to remove sentences which contain certain indicating phrases or have a TF-IDF score lower than a given threshold. As in the case of important sentences, the user can review the system&apos;s decisions. In order to offer maximum portability CAST, is written in Java, its input being XML. We decided not to include any preprocessing module in CAST (e.g. sentence splitter, PoS tagger, etc.), so all the necessar"
E03-1066,narita-2000-constructing,0,0.088708,"information overload by reducing it. At present the most common type of summarised information is textual information, but unfortunately the quality of the automatic summaries is not of a very high level. Apart from a working paper of ours in the mid 90s (Mitkov, 1995), the only relevant research we could find in this field is that of Craven (Craven, 1996). However, Craven&apos;s approach takes a rather simplistic view because it uses only methods which extract keywords from the text and not complete sentences or even phrases. Another tool which aids humans in producing summaries is presented in (Narita, 2000). This tool does not employ any automatic methods to help humans, but gives them the option to access a corpus of human produced abstracts which can function as templates, providing grammatical patterns and collocations common to abstracts. Endres-Niggemeyer (Endres-Niggemeyer, 1998) identifies three stages in human summarisation: document exploration, relevance assessment and summary production. In the first &apos;This figure includes books, newspapers, scholarly journals, office documents, etc. 135 two stages the summariser identifies the overall structure of the text and the main topics, whereas"
E03-1066,C96-2166,0,0.225137,"ght important sentences in the text. These sentences can then be saved as gold standard. The tool can also be used to teach students about summarisation methods. As the tool incorporates several methods, they can be run on the same text, making it possible to compare results. All these methods are highly customisable and the tool enables us to see the influence of different parameters on them. As aforementioned, the tool relies on several automatic methods to identify the important sentences. At present, these methods are: Keyword method: Uses TF-IDF scores to weight sentences as proposed in (Zechner, 1996). The user can modify the list of terms and indicate thresholds for sentences&apos; score in order to be considered important Indicating phrases: Paice (1981) noticed that it is possible to identify phrases which can be used to assess the importance of a sentence. The list of indicating phrases can be loaded, saved and modified in the tool. Surface clues: Several factors such as sentence location and length can be taken into consideration to decide the importance of the sentence. Lexical cohesion: Lexical cohesion as proposed in (Hoey, 1991) is used to produce extracts. Discourse information: Our o"
hasler-etal-2006-nps,setzer-gaizauskas-2000-annotating,0,\N,Missing
hasler-etal-2006-nps,J98-2001,0,\N,Missing
hasler-etal-2006-nps,J97-1005,0,\N,Missing
hasler-etal-2006-nps,rose-etal-2002-reuters,0,\N,Missing
hasler-etal-2006-nps,W99-0201,0,\N,Missing
hasler-etal-2006-nps,W99-0213,0,\N,Missing
hasler-etal-2006-nps,W03-2120,0,\N,Missing
orasan-2002-building,orasan-2000-clinka,0,\N,Missing
orasan-2002-building,C96-2166,0,\N,Missing
orasan-krishnamurthy-2002-corpus,A97-1011,0,\N,Missing
postolache-etal-2006-transferring,E06-1020,0,\N,Missing
postolache-etal-2006-transferring,M95-1005,0,\N,Missing
postolache-etal-2006-transferring,N01-1026,0,\N,Missing
postolache-etal-2006-transferring,A00-1020,0,\N,Missing
postolache-etal-2006-transferring,H05-1108,0,\N,Missing
postolache-etal-2006-transferring,J00-4003,0,\N,Missing
postolache-etal-2006-transferring,M98-1029,0,\N,Missing
R19-1033,C14-1188,0,0.0265168,"stem, which is extrinsically evaluated in our current paper) has relied on one or more of three main approaches: the use of overlap metrics such as Levenshtein distance (Levenshtein, 1966), BLEU score (Papineni et al., 2002) and SARI (Xu et al., 2016) to compare system output with human simplified texts (e.g. Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); automated assessments of the readability of system output (Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); and surveys of human opinions about the grammaticality, readability, and meanings of system output (Angrosh et al., 2014; Wubben et al., 2012; Feblowitz and Kauchak, 2013). In previous work, researchers have also used methods such as In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks: semantic role labelling (SRL) and information extraction (IE). The paper begins with our observation of challenges in the intrinsic evaluation of sentence simplification systems, which motivates the use of extrinsic evaluation of these systems with respect to other NLP tasks. We describe the two NLP systems and the test data used in the extrinsic evalua"
R19-1033,W13-2901,0,0.0217509,"ur current paper) has relied on one or more of three main approaches: the use of overlap metrics such as Levenshtein distance (Levenshtein, 1966), BLEU score (Papineni et al., 2002) and SARI (Xu et al., 2016) to compare system output with human simplified texts (e.g. Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); automated assessments of the readability of system output (Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); and surveys of human opinions about the grammaticality, readability, and meanings of system output (Angrosh et al., 2014; Wubben et al., 2012; Feblowitz and Kauchak, 2013). In previous work, researchers have also used methods such as In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks: semantic role labelling (SRL) and information extraction (IE). The paper begins with our observation of challenges in the intrinsic evaluation of sentence simplification systems, which motivates the use of extrinsic evaluation of these systems with respect to other NLP tasks. We describe the two NLP systems and the test data used in the extrinsic evaluation, and present arguments and evidence motivating"
R19-1033,R13-2011,0,0.0152392,"eral, the choice of evaluation method depends on the purpose of the simplification task. Various types of evaluation are currently used, but these are problematic. In previous work, evaluation of sentence simplification systems (including Evans and Or˘asan’s (2019) system, which is extrinsically evaluated in our current paper) has relied on one or more of three main approaches: the use of overlap metrics such as Levenshtein distance (Levenshtein, 1966), BLEU score (Papineni et al., 2002) and SARI (Xu et al., 2016) to compare system output with human simplified texts (e.g. Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); automated assessments of the readability of system output (Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); and surveys of human opinions about the grammaticality, readability, and meanings of system output (Angrosh et al., 2014; Wubben et al., 2012; Feblowitz and Kauchak, 2013). In previous work, researchers have also used methods such as In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to two NLP tasks: semantic role labelling (SRL) and information extraction (IE). The paper begins with our ob"
R19-1033,M92-1003,0,0.857779,"Missing"
R19-1033,P07-1086,0,0.0485829,"Missing"
R19-1033,jelinek-2014-improvements,0,0.0658324,"Missing"
R19-1033,N09-2045,0,0.228721,"ntic roles to the majority of the arguments of verbs and the IE system is better able to identify fillers for all IE template slots. 1 Introduction Sentence simplification is one aspect of text simplification, which is concerned with the conversion of texts into a more accessible form. In many cases, text simplification is performed to facilitate subsequent human or machine text processing. This may include processing for human reading comprehension (Canning, 2002; Scarton et al., 2017; Or˘asan et al., 2018) or for NLP tasks such as dependency parsing (Jel´ınek, 2014), information extraction (Jonnalagadda et al., 2009; Evans, 2011; Peng et al., 2012), semantic role labelling (Vickrey and Koller, 2008), and multidocument summarisation (Blake et al., 2007; Siddharthan et al., 2004). 1 Propositions are atomic statements that express simple factual claims (Jay, 2003). They are considered the basic units involved in the understanding and retention of text (Kintsch and Welsch, 1991). 2 NPs which contain finite nominally bound relative clauses. 285 Proceedings of Recent Advances in Natural Language Processing, pages 285–294, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_033 text simpl"
R19-1033,C04-1129,0,0.0615275,"cation is one aspect of text simplification, which is concerned with the conversion of texts into a more accessible form. In many cases, text simplification is performed to facilitate subsequent human or machine text processing. This may include processing for human reading comprehension (Canning, 2002; Scarton et al., 2017; Or˘asan et al., 2018) or for NLP tasks such as dependency parsing (Jel´ınek, 2014), information extraction (Jonnalagadda et al., 2009; Evans, 2011; Peng et al., 2012), semantic role labelling (Vickrey and Koller, 2008), and multidocument summarisation (Blake et al., 2007; Siddharthan et al., 2004). 1 Propositions are atomic statements that express simple factual claims (Jay, 2003). They are considered the basic units involved in the understanding and retention of text (Kintsch and Welsch, 1991). 2 NPs which contain finite nominally bound relative clauses. 285 Proceedings of Recent Advances in Natural Language Processing, pages 285–294, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_033 text simplification system for this purpose. Extrinsic evaluation offers the possibility of meeting this requirement. Text simplification has also been claimed to improve auto"
R19-1033,W15-1814,0,0.0243919,"t paper. In Section 4, we present each of the extrinsic evaluation experiments based on SRL (Section 4.1) and IE (Section 4.2). Each of these sections describes the task, the test data used, the NLP system whose output is used for extrinsic evaluation of the sentence simplification system, our motivation for considering that accuracy of the NLP system may be improved via a preprocessing step in which sentence simplification is performed, the evaluation method, our results, and a discussion of the results. In Section 5, we draw conclusions and consider directions for future work. eye tracking (Klerke et al., 2015; Timm, 2018), and reading comprehension testing (Or˘asan et al., 2018) to evaluate text simplification systems. There are several challenges in these approaches to evaluation. The development of gold standards in text simplification is problematic because they are difficult to produce and numerous variant simplifications are acceptable. As a result, existing metrics may not accurately reflect the usefulness of the simplification system being evaluated. Even when there are detailed guidelines for the simplification task, there is still likely to be a variety of means by which a human might sim"
R19-1033,P08-1040,0,0.238055,"to identify fillers for all IE template slots. 1 Introduction Sentence simplification is one aspect of text simplification, which is concerned with the conversion of texts into a more accessible form. In many cases, text simplification is performed to facilitate subsequent human or machine text processing. This may include processing for human reading comprehension (Canning, 2002; Scarton et al., 2017; Or˘asan et al., 2018) or for NLP tasks such as dependency parsing (Jel´ınek, 2014), information extraction (Jonnalagadda et al., 2009; Evans, 2011; Peng et al., 2012), semantic role labelling (Vickrey and Koller, 2008), and multidocument summarisation (Blake et al., 2007; Siddharthan et al., 2004). 1 Propositions are atomic statements that express simple factual claims (Jay, 2003). They are considered the basic units involved in the understanding and retention of text (Kintsch and Welsch, 1991). 2 NPs which contain finite nominally bound relative clauses. 285 Proceedings of Recent Advances in Natural Language Processing, pages 285–294, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_033 text simplification system for this purpose. Extrinsic evaluation offers the possibility of mee"
R19-1033,C16-2036,0,0.0238276,"ication method would improve the accuracy of the IE system. The 286 original clinical vignettes in the test data have a mean propositional density of 0.4826 ideas per word and 5.499 ideas per sentence. The values of these metrics for the simplified versions of the vignettes are 0.4803 ideas per word and 5.269 ideas per sentence, respectively. Although they are of the correct polarity, these differences are not statistically significant (p = 0.5327 and p = 0.1407, respectively). However, previous work in sentence simplification for IE (Jonnalagadda et al., 2009; Evans, 2011; Peng et al., 2012; Niklaus et al., 2016) has demonstrated that automatic sentence simplification can improve the accuracy of IE systems. This motivated us to evaluate the impact of the automatic sentence simplification method in this task. Evaluation Method. For the IE task, our evaluation metric is based on F1 -score averaged over all slots in the IE templates and all templates in the test data. Identification of true positives is based on exact matching of system-identified slot fillers with those in the manually completed IE templates in our test data. Results. The accuracy scores obtained by each variant of the IE system are pre"
R19-1033,P12-1107,0,0.0604918,"Missing"
R19-1033,Q16-1029,0,0.0560056,"ns need to be conducted repeatedly for development purposes and cost is a critical factor. In general, the choice of evaluation method depends on the purpose of the simplification task. Various types of evaluation are currently used, but these are problematic. In previous work, evaluation of sentence simplification systems (including Evans and Or˘asan’s (2019) system, which is extrinsically evaluated in our current paper) has relied on one or more of three main approaches: the use of overlap metrics such as Levenshtein distance (Levenshtein, 1966), BLEU score (Papineni et al., 2002) and SARI (Xu et al., 2016) to compare system output with human simplified texts (e.g. Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); automated assessments of the readability of system output (Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); and surveys of human opinions about the grammaticality, readability, and meanings of system output (Angrosh et al., 2014; Wubben et al., 2012; Feblowitz and Kauchak, 2013). In previous work, researchers have also used methods such as In this paper, we report on the extrinsic evaluation of an automatic sentence simplification method with respect to t"
R19-1033,J05-1004,0,0.0529648,"ty by the recorder Michael Gibbon QC after}A [a witness, who cannot be identified, withdrew from giving evidenceB andCEV prosecutor Susan Ferrier offered no further evidenceC ]{}D . {They were formally found not guilty by the recorder Michael Gibbon QC after}A a witness, who cannot be identified, withdrew from giving evidenceB {}D . {They were formally found not guilty by the recorder Michael Gibbon QC after}A prosecutor Susan Ferrier offered no further evidenceC {}D Table 1: Sentence transformation scheme used to simplify sentences containing compound clauses the higher-numbered arguments”6 (Palmer et al., 2005). The scheme includes semantic roles for “general, adjunct-like arguments” providing information on the verb’s cause [AMCAU], direction [AMDIR], discourse relations [AMDIS], location [AMLOC], manner [AMMNR], modal function7 [AMMOD], negation [AMNEG], purpose [AMPNC], and time [AMTMP], among others. For extrinsic evaluation of the sentence simplification method, we focused on verbal predicates8 , their arguments, and the nine listed adjunct-like argument types. Table 2 provides an example of SRL to analyse sentence (3). pler sentences which are added to the working set (stack W in Algorithm 1)."
R19-1033,P02-1040,0,0.106016,"t, especially when such evaluations need to be conducted repeatedly for development purposes and cost is a critical factor. In general, the choice of evaluation method depends on the purpose of the simplification task. Various types of evaluation are currently used, but these are problematic. In previous work, evaluation of sentence simplification systems (including Evans and Or˘asan’s (2019) system, which is extrinsically evaluated in our current paper) has relied on one or more of three main approaches: the use of overlap metrics such as Levenshtein distance (Levenshtein, 1966), BLEU score (Papineni et al., 2002) and SARI (Xu et al., 2016) to compare system output with human simplified texts (e.g. Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); automated assessments of the readability of system output (Wubben et al., 2012; Glavas and Stajner, 2013; Vu et al., 2014); and surveys of human opinions about the grammaticality, readability, and meanings of system output (Angrosh et al., 2014; Wubben et al., 2012; Feblowitz and Kauchak, 2013). In previous work, researchers have also used methods such as In this paper, we report on the extrinsic evaluation of an automatic sentence simplificati"
R19-1106,N18-2031,0,0.0192312,"ge in Kaggle6 . We use five different recurrent network architectures to perform the binary classification task on the sentences: pooled Gated Recurrent Unit (GRU) (3.2.1), stacked Long Short-Term Memory (LSTM) with attention (3.2.2), LSTM and GRU with attention (3.2.3), 2D convolution with pooling (3.2.4) and GRU with capsule (3.2.5). Each classifier was run on prepared and cleaned text (as explained in Section 3.1.1). These models were successfully applied to a number of classification tasks such as GRU for sequence labeling Chung et al. (2014), LSTM for semantic similarity and word analogy Coates and Bollegala (2018), and GRU with capsule for toponym detection Plum et al. (2019). Their success in these tasks inspired us to try them for our problem. 3.2.3 LSTM and GRU with Attention This architecture applies a spatial dropout to the embedding layer (Tompson et al., 2015). The output is then fed in parallel to a bidirectional LSTM-layer (Schuster and Paliwal, 1997) with self attention and a bidirectional GRUlayer (Chung et al., 2014) with self attention 6 https://www.kaggle.com/c/jigsaw-toxic-commentclassification-challenge 916 id start end PMC2857219 15686 15753 PMC5005937 9817 9928 PMC2857219 14913 14947"
R19-1106,L18-1008,0,0.206436,"eaks are commonly used in text files created with Microsoft DOS/Windows operating systems. This type of line break uses two characters to denote the end of a line. LF-type line breaks are commonly used on UNIX/Linux-based operating systems, and only use one character to denote the end of a line. 915 3.2.1 a location and then producing the annotation files for the SemEval evaluation script. For the machine learning training file, a separate column indicating whether or not a location is contained in the sentence is also included. 3.2 Pooled GRU This model takes pre-trained fasttext embeddings (Mikolov et al., 2018) as a matrix for the input which is comprised of the vertically stacked embedding vectors corresponding to the words present in the sentence. The matrix can be thought of as a sequence of embedded words. Each of these embedding vectors is fed to the bidirectional GRU (Chung et al., 2014) at their respective timestep. The final timestep output is fed into a max pooling layer and an average pooling layer in parallel (Scherer et al., 2010). Following this, the outputs of the two pooling layers are concatenated and connected to a dense layer (Huang et al., 2017) activated with a sigmoid function."
R19-1106,S19-2228,1,0.6264,"Missing"
R19-1106,P13-1144,0,0.0226673,"rmation such as coordinates, size, corresponding political entities and so on (Smith and Crane, 2001). Similarly, Leidner et al. (2004) used a combination of simple heuristics, linguistic cues, co-occurrence statistics and discourse information to detect locations and assign coordinates. TR has shifted from the methods of earlier approaches and followed the trend of using machine learning techniques. Whereas in the past learning techniques lacked data (Smith and Crane, 2001), this is no longer the case. Approaches using machine learning with (indirect) supervision include Hu and Ge (2009) and Speriosu and Baldridge (2013). Hu and Ge (2009) make use of hierarchical structures ensuing from geographical relations, an approach said to perform in an accuracy range of 73.55 to 85.38 percent on an Australian news corpus. Speriosu and Baldridge (2013) on the other hand, present their text-driven approach, which uses context information to resolve toponyms. The classifiers themselves are trained mainly on semi-automatically generated data, obtained primarily from locations tagged in Wikipedia. While the aforementioned approach This paper presents further research into TR, or to be more precise the detection of toponyms"
R19-1106,S19-2155,0,0.0621552,"Missing"
R19-1115,S16-1081,0,0.0199338,"the art STS methods rely on word embeddings one way or another. The recently introduced contextualised word embeddings have proved more effective than standard word embeddings in many natural language processing tasks. This paper evaluates the impact of several contextualised word embeddings on unsupervised STS methods and compares it with the existing supervised/unsupervised STS methods for different datasets in different languages and different domains. 1 Introduction Measuring Semantic Textual Similarity (STS) is calculating the degree of semantic equivalence between two snippets of text (Agirre et al., 2016). Earlier, STS tasks largely focused on similarity between short texts such as abstracts and product descriptions (Li et al., 2006; Mihalcea et al., 2006). Recently, STS tasks at the International Workshops on Semantic Evaluation (SemEval) focused on measuring STS between full sentence pairs. The introduction of competitive STS tasks led to the development of standard datasets like the SICK corpus (Bentivogli et al., 2016) and standardised the similarity score as a numerical value between 1 and 5 (Agirre et al., 2014). Having a good STS metric is crucial for many natural language processing ap"
R19-1115,N19-1078,0,0.0291762,"semantic similarity between a pair of sentences, takes the average of the word embeddings of all words in the two sentences, and calculates the cosine similarity between the resulting embeddings. This is a common way to acquire sentence embeddings from word embeddings. Obviously, this simple baseline leaves considerable room for variation. We have investigated the effects of ignoring stopwords and computing an average weighted by tf-idf in particular and reported them in the 4 section. 2.2.3 Stacked Embeddings Stacked Embeddings are obtained by concatenating different embeddings. According to Akbik et al. (2019) stacking the embeddings can provide a powerful embeddings to represent words. We represent the stacked embeddings in section 4 with ’+’ between the used models. As an example if the model name says ELMo + BERT, it is a stacked embedding of ELMo and BERT. For ELMo + BERT model we used pre-trained ’bertlarge-uncased’ model and ’original’ pre-trained ELMo model to represent each word as a 4096 + 3072 vector. 3.2 Standard Word Representations In order to compare the results of contextualised word embeddings, we used a standard word representation model in each experiment as a baseline. In this re"
R19-1115,C18-1139,0,0.0370311,"al. (2015). Word Mover’s Distance uses the word embeddings of the words in two texts to measure the minimum distance that the words in one text need to “travel” in semantic space to reach the words in the other text as shown in Figure 1. Kusner et al. (2015) says that this is a good approach than vector averaging since this technique keeps the word vectors as it is through out the operation. We have investigated the effects of considering/ ignoring stop words before calculating the word mover’s distance. 2.2.4 Flair Flair is another type of popular contextualised word embeddings introduced in Akbik et al. (2018). It takes a different approach by using a character level language model rather than the word level language model used in ELMo and BERT. The recommended way to use Flair embeddings is to stack pre-trained ’news-forward’ embeddings and pre-trained ’news-backward’ embeddings with Glove (Pennington et al., 2014) word embeddings (Akbik et al., 2018). We used the stacked model to represent each word as a 4196 lengthened vector. 2.3 Cosine Similarity on Average Vectors Figure 1: The Word Mover’s Distance between two documents 3.3 Experiments Cosine Similarity Using Smooth Inverse Frequency The thi"
R19-1115,S15-2017,1,0.897019,"Missing"
R19-1115,S14-2085,0,0.0248716,"dataset, 0.547 Pearson correlation for Newswire dataset and 0.570 weighted mean from both of them. The best performing model that participated in SemEval 2015 task 2, had 0.705 Pearson correlation for Wikipedia, 0.683 for Newswire and 0.690 weighted mean (Agirre et al., 2015). Our approach would rank fifth out of 17 team in the final results, which is the best result for an unsupervised approach. As with the English model, this one also surpasses other complex supervised models. As an example RTM-DCU-1stST.tree uses a supervised machine learning algorithm with Referential Translation Machines(Biici and Way, 2014) and our fairly simple unsupervised approach outperform them by a significant margin. Comparing the results we can safely assume that our approach works well with Spanish language STS too. 5.2 Bio-Medical STS In order to evaluate our approach in a different domain, we experimented it on Bio-medical STS dataset explained in 2.1.3. As in the previous experiments we applied all unsupervised approaches mentioned. We used ELMo embeddings trained on a biomedical domain corpora (e.g., PubMed abstracts, PMC full-text articles) (Peters et al., 1000 1 https://github.com/google-research/bert 2018) and Bi"
R19-1115,K18-2005,0,0.0314486,"nd it does not need a training set as the approach is unsupervised. As a result, the approach is easily portable to other languages and domains given the availability of ELMo and BERT models in that particular language or domain. In order to observe how well the method performs in other languages and domains we applied it to Spanish STS dataset and Biomedical STS dataset described in section 3. 5.1 Spanish STS We run all the unsupervised STS methods described in section 2 on the Spanish STS dataset explained in section 2.1.2. For the ELMo embeddings we used Spanish ELMo embeddings provided in Che et al. (2018), while for the BERT embeddings we used ”BERT-Base, Multilingual Cased” 1 model which has been built on the top 100 languages with the largest Wikipedias which includes Spanish language too. The predictions from the experiment were rescaled to lie ∈ [0,4] as the GOLD standards. Organisers have used only one evaluation metric in this Spanish STS task: Pearson correlation (τ ) against the predictions and GOLD standard. They have calculated Pearson correlation for each test set: Spanish news and Spanish wiki, separately and has taken the weighted average to give the final rankings in the leader b"
R19-1115,D17-1070,0,0.178057,"on 6 would briefly describe the related work done for STS. The paper finishes with conclusions. not be trained properly. Given the amount of human labour required to produce datasets for STS, it is not possible to have high quality large training datasets. As a result researches working in the field have also considered unsupervised methods for STS. Recent unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Word Mover’s Distance (Kusner et al., 2015), Doc2Vec (Le and Mikolov, 2014) and Smooth Inverse Frequency with GloVe vectors (Arora et al., 2017). While these approaches have produced decent results in the final rankings of shared tasks, they have also provided strong baselines for the STS task. 2 Settings of the Experiments 2.1 Data Sets The experiments presented in this paper were carried out using several datasets which will be explained in next subsections. In order to prove the portability of the approaches, the proposed architectures were also tested on an English Biomedical STS dataset"
R19-1115,S14-2139,0,0.0609997,"Missing"
R19-1115,D15-1181,0,0.0271166,"efore the shift of interest in neural networks, most of the proposed methods relied heavily on feature engineering. With the introduction of word embedding models, researchers focused more on neural representation for this task. There are two main approaches which employ neural representation models: supervised and unsupervised. Unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them while supervised approaches uses a machine learning model trained to predict the similarity using word embeddings. ConvNet (He et al., 2015), Skip Thought vectors (Kiros et al., 2015), Dependency Tree-LSTM (Tai et al., 2015) and Siamese Neural Networks (Mueller and Thyagarajan, 2016) can be considered as the most successful architectures employed for calculating STS. These supervised approaches always suffer from less training data problem which is common in STS tasks. As a result the researches have also considered unsupervised approaches. The three unsupervised STS methods explored in this paper: Cosine similarity on average vectors, Word Mover’s Distance and Cosine similarity using Smooth Inverse Frequency are the most common u"
R19-1115,S14-2001,0,0.0261713,"for these evaluation metrics. For this reason, we applied a parametric regression step to obtain better-calibrated predictions. We trained a regression model on the SICK train data and predicted on the SICK test data. This calibration step served as a minor correction for our restrictively simple similarity function. However, this regression calibration improved the Pearson correlation by 0.01 for the SICK test set. Our unsupervised method had 0.762 Pearson correlation score, whilst the best result in the International Workshop on Semantic Evaluation 2014 Task 1 had 0.828 Pearson correlation (Marelli et al., 2014). Our approach would be ranked on the ninth position from the top results out of 18 participants, and it is the best unsupervised STS method among the results. Our method even outperformed systems that rely on additional feature generation (e.g. dependency parses) or data augmentation schemes. As an example, our method is just above the UoW system which relied on 20 linguistics features fed in to a Support Vector Machine and obtained a 0.714 Pearson correlation (Gupta et al., 2014). Compared to these complex approaches our simple approach provides a strong baseline to STS tasks. 5 Portability"
R19-1115,L18-1008,0,0.0297489,"ctures were also tested on an English Biomedical STS dataset. In addition, the language independence of the method is tested by applying it to a Spanish STS dataset. Word vectors are used to determine a representation of a sentence in approaches like Word Mover’s Distance (Kusner et al., 2015) and Smooth Inverse Frequency (Arora et al., 2017). The main weakness of word vectors is that each word has the same unique vector regardless of the context it appears. For an example, the word ”play” has several meanings, but in standard word embeddings such as Glove (Pennington et al., 2014), FastText (Mikolov et al., 2018) or Word2Vec (Mikolov et al., 2013) each instance of the word has the same representation regardless of the meaning which is used. However, contextualised word embedding models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) etc. generate embeddings for a word based on the context it appears, thus generating slightly different embeddings for each of its occurrence. The recent applications in areas such as question answering and textual entailment show that contextualised word embeddings perform better than the traditional word embeddings (Devlin et al., 2018). 2.1.1 English-Engl"
R19-1115,S17-2016,0,0.0187802,"cessing. For example in Biomedical Informatics, it can be used to compare genes (Ferreira and Couto, 2010). Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a result of the success neural networks have achieved in other fields, most of the methods proposed in recent years rely on neural architectures (Tai et al., 2015; Shao, 2017). Neural networks are preferred over traditional machine learning models as they generally tend to perform better than traditional machine learning models. They also do not rely on explicit linguistics features which have to be extracted before the ML model is learnt. Determining the best linguistic features for calculating STS is not an easy task as it requires a good understanding of the linguistic phenomenon and relies on researchers’ intuition. In addition, calculating these features is usually not an easy task, especially for languages other than English. Therefore, in contrast to traditi"
R19-1115,P15-1150,0,0.182274,"Missing"
R19-1115,S15-2001,0,0.0706766,"d Representations Tharindu Ranasinghe, Constantin Or˘asan and Ruslan Mitkov Research Group in Computational Linguistics University of Wolverhampton, UK {t.d.ranasinghehettiarachchige, c.orasan, r.mitkov }@wlv.ac.uk Abstract et al., 2011) and text classification (Rocchio, 1971). Semantic similarity also contributes to many semantic web applications like community extraction, ontology generation and entity disambiguation (Li et al., 2006), and it is also useful for Twitter search (Salton et al., 1997), where it is required to accurately measure semantic relatedness between concepts or entities (Xu et al., 2015). STS is not limited only to natural language processing. For example in Biomedical Informatics, it can be used to compare genes (Ferreira and Couto, 2010). Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a result of the success neural networks have achieved in other fields, most of the methods proposed in recent years"
R19-1115,P11-1076,0,0.0742506,"Missing"
R19-1115,N18-1049,0,0.0677019,"guages and domains in section 5. Section 6 would briefly describe the related work done for STS. The paper finishes with conclusions. not be trained properly. Given the amount of human labour required to produce datasets for STS, it is not possible to have high quality large training datasets. As a result researches working in the field have also considered unsupervised methods for STS. Recent unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Word Mover’s Distance (Kusner et al., 2015), Doc2Vec (Le and Mikolov, 2014) and Smooth Inverse Frequency with GloVe vectors (Arora et al., 2017). While these approaches have produced decent results in the final rankings of shared tasks, they have also provided strong baselines for the STS task. 2 Settings of the Experiments 2.1 Data Sets The experiments presented in this paper were carried out using several datasets which will be explained in next subsections. In order to prove the portability of the approaches, the proposed architectures were also tested on"
R19-1115,D14-1162,0,0.102888,"the approaches, the proposed architectures were also tested on an English Biomedical STS dataset. In addition, the language independence of the method is tested by applying it to a Spanish STS dataset. Word vectors are used to determine a representation of a sentence in approaches like Word Mover’s Distance (Kusner et al., 2015) and Smooth Inverse Frequency (Arora et al., 2017). The main weakness of word vectors is that each word has the same unique vector regardless of the context it appears. For an example, the word ”play” has several meanings, but in standard word embeddings such as Glove (Pennington et al., 2014), FastText (Mikolov et al., 2018) or Word2Vec (Mikolov et al., 2013) each instance of the word has the same representation regardless of the meaning which is used. However, contextualised word embedding models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) etc. generate embeddings for a word based on the context it appears, thus generating slightly different embeddings for each of its occurrence. The recent applications in areas such as question answering and textual entailment show that contextualised word embeddings perform better than the traditional word embeddings (Devlin"
R19-1115,N18-1202,0,0.307667,"ation of a sentence in approaches like Word Mover’s Distance (Kusner et al., 2015) and Smooth Inverse Frequency (Arora et al., 2017). The main weakness of word vectors is that each word has the same unique vector regardless of the context it appears. For an example, the word ”play” has several meanings, but in standard word embeddings such as Glove (Pennington et al., 2014), FastText (Mikolov et al., 2018) or Word2Vec (Mikolov et al., 2013) each instance of the word has the same representation regardless of the meaning which is used. However, contextualised word embedding models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2018) etc. generate embeddings for a word based on the context it appears, thus generating slightly different embeddings for each of its occurrence. The recent applications in areas such as question answering and textual entailment show that contextualised word embeddings perform better than the traditional word embeddings (Devlin et al., 2018). 2.1.1 English-English STS Data Set For the experiments carried out on English STS, we used the SICK dataset. (Bentivogli et al., 2016). The SICK data contains 9927 sentence pairs with a 5,000/4,927 training/test split which were"
R19-1116,S15-2017,1,0.897921,"Missing"
R19-1116,S14-2114,0,0.0256167,"Missing"
R19-1116,S17-2001,0,0.0606945,"Missing"
R19-1116,S14-2055,0,0.0654058,"Missing"
R19-1116,D17-1070,0,0.0226762,"kind of neural network architecture which employed word embeddings (Shao, 2017). As an example, Maharjan et al. (2017) used an ensemble of traditional machine learning models and deep learning models in their top performing system at Semeval 2017 STS task. There are two main approaches which employ neural representation models: supervised and unsupervised. Unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Doc2Vec (Le and Mikolov, 2014) and smooth inverse frequency with GloVe vectors (Arora et al., 2017). Supervised approaches use neural networks to project word embeddings to fixed dimensional vectors which are trained to capture the semantic meaning of the sentence. Recently, many neural network architectures have been used to calculate sentence similarity. He et al. (2015) propose an elaborate convolutional network (ConvNet) variant which infers sentence similarity by integrating various differences across many convolutions at varying scales. Kiros et al. (2015) propose the skip-thoughts mod"
R19-1116,S14-2139,0,0.0512514,"Missing"
R19-1116,2014.tc-1.10,1,0.901711,"unity. The SemEval tasks also led to the development of standard datasets like the SICK corpus (Bentivogli et al., 2016) and standardised the similarity score as a numerical value between 1 and 5 (Agirre et al., 2014). Having a good STS metric is very important in many natural language processing (NLP) applications. As an example, for certain types of question answering systems, having an accurate STS component is the key to success since the questions with similar meanings can be answered similarly (Majumder et al., 2016). STS is also important in translation memories retrieval and matching (Gupta et al., 2014b). Translation memories help translators by finding in the database they maintain previously translated sentences, which are similar to the one to be translated, and retrieving their translations. Hence, accurate STS methods are beneficial for translation memory. Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a resul"
R19-1116,S17-2015,0,0.0361576,"Missing"
R19-1116,D15-1181,0,0.0205158,"use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Doc2Vec (Le and Mikolov, 2014) and smooth inverse frequency with GloVe vectors (Arora et al., 2017). Supervised approaches use neural networks to project word embeddings to fixed dimensional vectors which are trained to capture the semantic meaning of the sentence. Recently, many neural network architectures have been used to calculate sentence similarity. He et al. (2015) propose an elaborate convolutional network (ConvNet) variant which infers sentence similarity by integrating various differences across many convolutions at varying scales. Kiros et al. (2015) propose the skip-thoughts model, which extends the skip-gram approach of word2vec from the word to sentence level. This model feeds each sentence into an Recurrent Neural Network (RNN) encoder-decoder with Gated Recurrent Unit (GRU) activations. They attempt to reconstruct the immediately preceding and following sentences. For the sentence similarity task, they obtain skip-thought vectors for sentence p"
R19-1116,S14-2131,0,0.0615653,"Missing"
R19-1116,S14-2001,0,0.022563,"ing model to GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018) or concatenating them with word2vec model did not improve the results either. For this reason, none of these results are presented here. The Siamese neural network with GRU was tested with a cyclical learning rate (Smith, 2015), which has the advantage of forcing the model to find another local minimum if the current minimum is not robust and makes the model generalize better to unseen data. However, neither cyclical learning rate nor reducing learning rate on plateau increased the per3 These results are reported in Marelli et al. (2014) Approach τ FCICU (Hassan et al., 2017) BIT (Wu et al., 2017) ECNU (Tian et al., 2017) DT Team (Maharjan et al., 2017) Bi-directional LSTM† GRU + Capsule + Flatten† RTV 0.8540 0.8545 0.8547 MALSTM LSTM: Adagrad† GRU + Attention† LSTM + Attention† Bi-directional GRU† GRU† 0.8651 0.8692 0.8725 0.8743 0.8750 0.8792 0.8280 0.8400 0.8518 0.8536 Table 2: Pearson correlation (τ ) for STS2017 test set. formance further. We do not report these results too. Table 2 shows the results obtained for STS2017 test dataset comparing our experiments with other top performing models in SemEval 2017 Task 1 (Cer e"
R19-1116,L18-1008,0,0.076203,"Missing"
R19-1116,H92-1116,0,0.610984,"redicting STS 1 . 3 3.1 Settings of the Experiments Data Sets The experiments presented in this paper were carried out using the SICK dataset (Bentivogli et al., 2016) and SemEval 2017 Task 1 dataset (Cer et al., 2017) which we will refer as STS2017 dataset. The SICK data contains 9927 sentence pairs with a 5,000/4,927 training/test split which were employed in the SemEval tasks. Each pair is annotated with a relatedness score between [1,5] corresponding to the average relatedness judged by 10 different individuals. In order to generate more training data we used thesaurus-based augmentation (Miller, 1992) and added 10,022 additional training examples. Evaluation was done with the SICK test data. Mueller and Thyagarajan (2016) uses the same thesaurus-based data augmentation in their research. The STS2017 test datset had 250 sentence pairs annotated with a relatedness score between [1,5]. As the training data for the competition, participants were encouraged to make use of all existing data sets from prior STS evaluations including all previously released trial, training and evaluation data 2 . Once we combined all datasets from prior STS tasks we had 8277 sentence pairs for training. 3.2 Propos"
R19-1116,W16-1617,0,0.054413,"ghts. In addition, parameter updating is mirrored across these sub-networks. Siamese networks are popular among tasks that involve finding similarity or a relationship between two comparable things. They have been proven successful in tasks like signature verification (Bromley et al., 1993), face verification (Chopra et al., 2005), image similarity (Koch et al., 2015) and have been re1004 Proceedings of Recent Advances in Natural Language Processing, pages 1004–1011, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_116 cently used successfully in sentence similarity (Neculoiu et al., 2016). Siamese architectures are good in these tasks because, when the inputs are of the same kind, it makes sense to use a similar model to process similar inputs. In this way the networks will have representation vectors with the same semantics, making them easier to compare pairs of sentences. Given that the weights are shared across sub networks there are fewer parameters to train, which in turn means they require less training data and less tendency to over-fit. Given the amount of human labour required to produce datasets for STS, Siamese neural networks can prove the ideal solution for the S"
R19-1116,N18-1049,0,0.0127484,"he STS task at Semeval 2017 used some kind of neural network architecture which employed word embeddings (Shao, 2017). As an example, Maharjan et al. (2017) used an ensemble of traditional machine learning models and deep learning models in their top performing system at Semeval 2017 STS task. There are two main approaches which employ neural representation models: supervised and unsupervised. Unsupervised approaches use pretrained word/sentence embeddings directly for the similarity task without training a neural network model on them. Such approaches have used cosine similarity on sent2vec (Pagliardini et al., 2018), InferSent (Conneau et al., 2017), Doc2Vec (Le and Mikolov, 2014) and smooth inverse frequency with GloVe vectors (Arora et al., 2017). Supervised approaches use neural networks to project word embeddings to fixed dimensional vectors which are trained to capture the semantic meaning of the sentence. Recently, many neural network architectures have been used to calculate sentence similarity. He et al. (2015) propose an elaborate convolutional network (ConvNet) variant which infers sentence similarity by integrating various differences across many convolutions at varying scales. Kiros et al. (2"
R19-1116,D14-1162,0,0.0822554,"ctures with bidirectional GRU, LSTM with Attention and GRU with Attention also surpassed the benchmark. However uni-directional GRU out performed them on all 3 evaluation metrics. We experimented with other similarity functions and other embedding models. Using Euclidean distance for the similarity function instead of Manhattan distance did not improve the results for the model because semantically different sentences could end up being represented by nearly identical vectors due to the vanishing gradients of the Euclidean distance (Chopra et al., 2005). Changing the embedding model to GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018) or concatenating them with word2vec model did not improve the results either. For this reason, none of these results are presented here. The Siamese neural network with GRU was tested with a cyclical learning rate (Smith, 2015), which has the advantage of forcing the model to find another local minimum if the current minimum is not robust and makes the model generalize better to unseen data. However, neither cyclical learning rate nor reducing learning rate on plateau increased the per3 These results are reported in Marelli et al. (2014) Approach τ FCICU (Hass"
R19-1116,P13-4028,0,0.0323165,"S) is an important research area in natural language processing which plays a significant role in many applications such as question answering, document summarisation, information retrieval and information extraction. This paper evaluates Siamese recurrent architectures, a special type of neural networks, which are used here to measure STS. Several variants of the architecture are compared with existing methods. 1 Introduction Measuring Semantic Textual Similarity (STS) is the task of calculating the similarity between a pair of texts using both direct and indirect relationships between them (Rus et al., 2013). Originally, the work on STS largely focused on similarity between short texts such as abstracts and product descriptions (Li et al., 2006; Mihalcea et al., 2006). The introduction of the STS tasks at the International Workshops on Semantic Evaluation (SemEval) lead to an increase of the interest that the field received from the research community. The SemEval tasks also led to the development of standard datasets like the SICK corpus (Bentivogli et al., 2016) and standardised the similarity score as a numerical value between 1 and 5 (Agirre et al., 2014). Having a good STS metric is very imp"
R19-1116,S17-2016,0,0.0763167,"ated, and retrieving their translations. Hence, accurate STS methods are beneficial for translation memory. Given the growing importance of having a good STS metric and as a result of the SemEval workshops, researchers have proposed numerous STS methods. Most of the early approaches were based on traditional machine learning and involved heavy feature engineering (B´echara et al., 2015). With the advances of word embeddings, and as a result of the success neural networks have achieved in other fields, most of the methods proposed in recent years rely on neural architectures (Tai et al., 2015; Shao, 2017). Neural networks are preferred over traditional machine learning models as they generally tend to perform better than traditional machine learning models. They also do not rely on linguistic features which means they can be easily applied to languages other than English. The architecture employed in this paper is a special class of neural networks called Siamese neural networks. These networks contain two or more identical sub-networks. The networks are identical in the sense that they have the same configuration with the same parameters and weights. In addition, parameter updating is mirrore"
R19-1116,P15-1150,0,0.0865406,"Missing"
R19-1116,S17-2028,0,0.0332185,"Missing"
R19-1116,S17-2007,0,0.0415386,"Missing"
R19-1116,S14-2044,0,0.141732,"periments carried out in this paper including the datasets employed here and the different architectures explored. The architectures are evaluated in Section 4. The paper finishes with conclusions. 2 Related Work Given that a good STS metric is required for a variety of NLP fields, researchers have proposed a large number of such metrics. Before the shift of interest in neural networks, most of the proposed methods relied heavily on feature engineering. A typical example is (Gupta et al., 2014a) which employed 20 linguistic features fed into a support vector machine regressor. The top system (Zhao et al., 2014) in task 1 in SemEval 2014 has used seven types of features including text difference measures, common text similarity measures etc. (Zhao et al., 2014). Then they have fed it in to several learning algorithms like support vector machine regressor, Random Forest, Gradient boosting etc (Zhao et al., 2014). With the introduction of word embedding models, researchers focused more on neural representation for this task. Many of the leading teams in the STS task at Semeval 2017 used some kind of neural network architecture which employed word embeddings (Shao, 2017). As an example, Maharjan et al."
R19-1155,W14-1215,0,0.058367,"al attention. As a result of these difficulties, information contained in online user feedback can be less accessible for people with autism. 2.2 Automatic Text Adaptation for Adults with Autism In terms of systems aimed at making text more accessible for autistic individuals who are fairly able, the OpenBook tool1 is the most comprehensive existing system to date. The tool provides semi-automatic conversion of text documents by reducing syntactic complexity and disambiguating meaning by resolving pronominal reference, performing word sense disambiguation and detecting conventional metaphors (Evans et al., 2014; Or˘asan et al., 2018), with some initial efforts towards concept substitutions for images (Barbu et al., 2015). As part of the research project, the tool was evaluated together with end-users with ASD who were shown to find the adapted texts more accessible than the originals. Nevertheless, a major impediment for the automatic evaluation of such systems is the limited amount of userevaluated data. To the best of our knowledge, the only available resources containing a limited amount of such data are the ASD corpus (Yaneva et al., 2016a; Yaneva, 2016), followed by a corpus of easy-to-read doc"
R19-1155,W14-5606,0,0.0161049,"corpus (Yaneva et al., 2016a; Yaneva, 2016), followed by a corpus of easy-to-read documents that were specifically developed for people with cognitive disabilities (Yaneva et al., 2016b) 2 . Constrained by these limitations, some approaches propose to automatically evaluate text simplification systems for people with autism in terms the change in readability of the generated sentences (Evans et al., 2014; ˇ Stajner and Saggion, 2013), the incorporation of user-evaluated data into larger corpora (Yaneva et al., 2017), or the use of corpora containing texts ˇ for children and language-learners (Stajner et al., 2014). Therefore, very little is known about the perceptions of adults with high-functioning autism on the usefulness of specific simplification strategies. In the following sections we present a survey on the perceptions of adults with high-functioning autism on the accessibility of user reviews. 3 Data Collection This section presents the way the survey responses were collected. 1 http://www.openbooktool.net/ Note, however, that the latter is not targeted at readers with high-functioning autism 1357 2 Question 1. Do you read online reviews to determine whether a product or service is good or bad?"
R19-1155,I13-1043,0,0.0537113,"Missing"
R19-1155,W17-5013,1,0.845697,"f our knowledge, the only available resources containing a limited amount of such data are the ASD corpus (Yaneva et al., 2016a; Yaneva, 2016), followed by a corpus of easy-to-read documents that were specifically developed for people with cognitive disabilities (Yaneva et al., 2016b) 2 . Constrained by these limitations, some approaches propose to automatically evaluate text simplification systems for people with autism in terms the change in readability of the generated sentences (Evans et al., 2014; ˇ Stajner and Saggion, 2013), the incorporation of user-evaluated data into larger corpora (Yaneva et al., 2017), or the use of corpora containing texts ˇ for children and language-learners (Stajner et al., 2014). Therefore, very little is known about the perceptions of adults with high-functioning autism on the usefulness of specific simplification strategies. In the following sections we present a survey on the perceptions of adults with high-functioning autism on the accessibility of user reviews. 3 Data Collection This section presents the way the survey responses were collected. 1 http://www.openbooktool.net/ Note, however, that the latter is not targeted at readers with high-functioning autism 135"
R19-1155,L16-1077,1,0.845238,"sense disambiguation and detecting conventional metaphors (Evans et al., 2014; Or˘asan et al., 2018), with some initial efforts towards concept substitutions for images (Barbu et al., 2015). As part of the research project, the tool was evaluated together with end-users with ASD who were shown to find the adapted texts more accessible than the originals. Nevertheless, a major impediment for the automatic evaluation of such systems is the limited amount of userevaluated data. To the best of our knowledge, the only available resources containing a limited amount of such data are the ASD corpus (Yaneva et al., 2016a; Yaneva, 2016), followed by a corpus of easy-to-read documents that were specifically developed for people with cognitive disabilities (Yaneva et al., 2016b) 2 . Constrained by these limitations, some approaches propose to automatically evaluate text simplification systems for people with autism in terms the change in readability of the generated sentences (Evans et al., 2014; ˇ Stajner and Saggion, 2013), the incorporation of user-evaluated data into larger corpora (Yaneva et al., 2017), or the use of corpora containing texts ˇ for children and language-learners (Stajner et al., 2014). Ther"
R19-1155,L16-1045,1,0.834318,"sense disambiguation and detecting conventional metaphors (Evans et al., 2014; Or˘asan et al., 2018), with some initial efforts towards concept substitutions for images (Barbu et al., 2015). As part of the research project, the tool was evaluated together with end-users with ASD who were shown to find the adapted texts more accessible than the originals. Nevertheless, a major impediment for the automatic evaluation of such systems is the limited amount of userevaluated data. To the best of our knowledge, the only available resources containing a limited amount of such data are the ASD corpus (Yaneva et al., 2016a; Yaneva, 2016), followed by a corpus of easy-to-read documents that were specifically developed for people with cognitive disabilities (Yaneva et al., 2016b) 2 . Constrained by these limitations, some approaches propose to automatically evaluate text simplification systems for people with autism in terms the change in readability of the generated sentences (Evans et al., 2014; ˇ Stajner and Saggion, 2013), the incorporation of user-evaluated data into larger corpora (Yaneva et al., 2017), or the use of corpora containing texts ˇ for children and language-learners (Stajner et al., 2014). Ther"
recasens-etal-2012-annotating,recasens-etal-2010-typology,1,\N,Missing
recasens-etal-2012-annotating,poesio-artstein-2008-anaphoric,0,\N,Missing
recasens-etal-2012-annotating,N10-1061,0,\N,Missing
recasens-etal-2012-annotating,rose-etal-2002-reuters,0,\N,Missing
recasens-etal-2012-annotating,J00-4005,0,\N,Missing
recasens-etal-2012-annotating,passonneau-2004-computing,0,\N,Missing
recasens-etal-2012-annotating,W05-0303,0,\N,Missing
recasens-etal-2012-annotating,H05-1004,0,\N,Missing
recasens-etal-2012-annotating,W11-1901,0,\N,Missing
recasens-etal-2012-annotating,W11-1902,0,\N,Missing
recasens-etal-2012-annotating,doddington-etal-2004-automatic,0,\N,Missing
recasens-etal-2012-annotating,W04-2327,0,\N,Missing
recasens-etal-2012-annotating,hasler-etal-2006-nps,1,\N,Missing
recasens-etal-2012-annotating,M98-1029,0,\N,Missing
recasens-etal-2012-annotating,taule-etal-2008-ancora,1,\N,Missing
recasens-etal-2012-annotating,J06-4012,0,\N,Missing
recasens-etal-2012-annotating,A97-1011,0,\N,Missing
S15-2017,S14-2139,1,0.418054,"Missing"
S15-2017,N03-1017,0,0.0103338,"entify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the colloc"
S15-2017,P07-2045,0,0.0167899,"e available by the Apache OpenNLP library were used (i.e. we used models to identify dates, locations, money, organisations, percentages, persons and time). We also used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available t"
S15-2017,2005.mtsummit-papers.11,0,0.0236061,"anslation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3 https://code.google.com/p/tt4j http://"
S15-2017,W02-0109,0,0.0206325,"es Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3 https://code.google.com/p/tt4j http://snowball.tartarus.org 5 http://opennlp.apache.org 6 https://github.com/hpcosta/stopwords 4 NLTK package (Loper and Bird, 2002), and sorted by the degree of likelihood association between their components. 2.2 Extracted Features This section details the features that our system uses to measure the semantic textual similarity between two sentences. The system uses the same features for both Subtask 2a and Subtask 2b. In addition to the baseline features used in Gupta et al., 2014, we introduced a set of Distributional, Semantic and Conceptual Similarity Measures, as well as a feature reflecting MWEs across sentences. 2.2.1 Baseline Features The system is built on the baseline system developed for SemEval2014, which con"
S15-2017,J03-1002,0,0.0048888,"o used all the pre-trained NER models for Spanish (in this case, we used models to identify persons, organisations, locations and miscellanea). 2.1.3 Translation Model Since one of the features we implemented was available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5"
S15-2017,P03-1021,0,0.0111434,"as available only for English (i.e. the Semantic Similarity Measures), we trained a Statistical Machine Translation (SMT) system to translate our Spanish dataset into English. For this purpose, we used the PB-SMT system Moses (Koehn et al., 2007), 5-gram language models with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al., 2003. We trained this system on the Europarl Corpus (Koehn, 2005) and used Minimum Error Rate Training (MERT) (Och, 2003) for tuning on the development set. 2.1.4 Resources Given that a number of our features depends on stopwords (see section 2.2), we compiled two lists of stopwords, one for English and another one for Spanish. Both are freely available to download6 . We also used two lists (English and Spanish) of candidates for Multiword Expressions (MWEs) as a resource for one of the features (see section 2.2.5). These lists were extracted from the Europarl Corpus (Koehn, 2005) using the collocation modules of the 3 https://code.google.com/p/tt4j http://snowball.tartarus.org 5 http://opennlp.apache.org 6 http"
S15-2017,P13-1132,0,0.0233604,"tely for NLP purposes, including processing semantic similarities. For the purpose of our experiments, we focused on two more common types of MWEs in English and Spanish: verb noun combinations and verb particle constructions. Whenever a verb+noun or a verb+particle combination occurs in our sentence pair, we search a prepared list MWEs, sorted according to their likelihood measures of association. The degree of association of these combinations served as a feature in our ML system. 3 Semantic Similarity Measures This feature takes advantage of the Align, Disambiguate and Walk (ADW)9 library (Pilehvar et al., 2013), a WordNet-based approach for measuring semantic similarity of arbitrary pairs of lexical items. It is important to mention that this feature is the only one that only works for English, which explains why we have a translation model (see section 2.1.3). In other words, when we are dealing 8 with Spanish text, we use the trained model to translate from Spanish to English. As the ADW library permits us to measure the semantic similarity between two raw English sentences, either by using disambiguation or not, we used both options to calculate all the comparison methods made available by the li"
S15-2017,C10-3015,0,0.0256287,"fter computing all the co-occurrences, we used these values to calculate the Jaccard’ (Jaccard, 1901), Lin’ (Lin, 1998) and PMI’ (Turney, 2001) scores. 2.2.4 9 2.2.5 Multiword Expressions Multiword Expressions (MWEs) are meaningful lexical units whose distinct idiosyncratic properties call for special treatment within a computational system. Non-compositionality is one of the properties of MWEs. The degree of association between the components of a MWE has been proved to be a promising approach to find out how much they are non-compostional and therefore how probable they are acceptable MWEs (Ramisch et al., 2010). The more non-compositional a MWE is, the more important is not to treat its components separately for NLP purposes, including processing semantic similarities. For the purpose of our experiments, we focused on two more common types of MWEs in English and Spanish: verb noun combinations and verb particle constructions. Whenever a verb+noun or a verb+particle combination occurs in our sentence pair, we search a prepared list MWEs, sorted according to their likelihood measures of association. The degree of association of these combinations served as a feature in our ML system. 3 Semantic Simila"
S15-2017,N03-1033,0,0.0173387,"Missing"
S15-2017,P06-4018,0,\N,Missing
temnikova-etal-2012-clcm,J00-4006,0,\N,Missing
temnikova-etal-2012-clcm,W09-0441,0,\N,Missing
temnikova-etal-2012-clcm,C96-2183,0,\N,Missing
temnikova-etal-2012-clcm,P02-1040,0,\N,Missing
temnikova-etal-2012-clcm,temnikova-2010-cognitive,1,\N,Missing
temnikova-etal-2012-clcm,R11-1094,1,\N,Missing
temnikova-etal-2012-clcm,W09-1318,0,\N,Missing
tutin-etal-2004-annotation,J98-2001,0,\N,Missing
tutin-etal-2004-annotation,A00-1020,0,\N,Missing
tutin-etal-2004-annotation,W03-2120,0,\N,Missing
tutin-etal-2004-annotation,M98-1029,0,\N,Missing
W01-0716,W99-0611,0,0.0761266,"animate. 6 Actually the only way this word would be classified as inanimate is if it is in the subject position, and most of the senses of its main verb are inanimate. This is explained by the way the senses are weighted by the machine learning algorithm. that the approach makes the assumption that anaphora resolution is already effective, even though, in general, anaphora resolution systems rely on gender filtering. In (Denber, 1998), WordNet was used to determine the animacy of nouns and associate them with gender-marked pronouns. The details presented are sparse and no evaluation is given. Cardie and Wagstaff (1999) combined the use of WordNet with proper name gazetteers in order to obtain information on the compatibility of coreferential NPs in their clustering algorithm. Again, no evaluation was presented with respect to the accuracy of this animacy classification task. 6 Conclusions and future work In this paper, a two step method for animacy recognition was proposed. In the first step, it tries to determine the animacy of senses from WordNet on the basis of an annotated corpus. In the second step, this information is used by an instance based learning algorithm to determine the animacy of a noun. Thi"
W01-0716,W95-0105,0,0.018458,"ike pupil, which has two animate senses and one inanimate, is highly unlikely to be classified as inanimate, even if it used to refer to a specific part of the eye.6 The ideal solution to this problem would be to disambiguate the words, but this would require an accurate disambiguation method. An alternative solution is to weight the senses with respect to the text. In this way, if a sense is more likely to be used in a text, its animacy/inanimacy will have greater influence on the classification process. At present, we are trying to integrate the word sense disambiguation method proposed in (Resnik, 1995) into our system. We hope that this will particularly improve the classification of animate entities. 5 Related work Most of the work on animacy/gender recognition has been done in the field of anaphora resolution. The automatic recognition of NP gender on the basis of statistical information has been attempted before (Hale and Charniak, 1998). That method operates by counting the frequency with which a NP is identified as the antecedent of a gender-marked pronoun by a simplistic pronoun resolution system. It is reported that by using the syntactic Hobbs algorithm (Hobbs, 1976) for pronoun res"
W03-1205,W00-0408,0,0.0754563,"Missing"
W03-1205,J95-2003,0,0.102603,"analysed in Section 3 to learn whether this principle holds in human produced summaries. In Section 4, we present two algorithms which combine traditional techniques with information provided by the continuity principle. Several criteria are used to evaluate these algorithms on scientific articles in Section 5. We finish with concluding remarks, which also indicate possible future research avenues. 2 How to ensure local cohesion In the previous section we already mentioned that we are trying to improve the automatic summaries by using the continuity principle defined in Centering Theory (CT) (Grosz et al., 1995). This principle, requires that two consecutive utterances have at least one entity in common. Even though it sounds very simple, this principle is important for the rest of the principles defined in the CT because if it does not hold, none of the other principles can be satisfied. Given that only the continuity principle will be used in this paper and due to space limits, the rest of these principles are not discussed here. Their description can be found in (Kibble and Power, 2000). For the same reason we will not go into details about the CT. In this paper, we take an approach similar to (Ka"
W03-1205,W02-2111,0,0.166606,"5). This principle, requires that two consecutive utterances have at least one entity in common. Even though it sounds very simple, this principle is important for the rest of the principles defined in the CT because if it does not hold, none of the other principles can be satisfied. Given that only the continuity principle will be used in this paper and due to space limits, the rest of these principles are not discussed here. Their description can be found in (Kibble and Power, 2000). For the same reason we will not go into details about the CT. In this paper, we take an approach similar to (Karamanis and Manurung, 2002) and try to produce summaries which do not violate the continuity principle. In this way, we hope to produce summaries which contain sequences of sentences that refer the same entity, and therefore will be more coherent. Before we can test if the principle is satisfied, it is necessary to define certain parameters on which the principle relies. As aforementioned, the principle is tested on pairs of consecutive utterances. In general utterances are clauses or sentences. Given that the automatic identification of clauses is not very accurate, we consider sentences as utterances. An advantage of"
W03-1205,W00-1411,0,0.3823,"we are trying to improve the automatic summaries by using the continuity principle defined in Centering Theory (CT) (Grosz et al., 1995). This principle, requires that two consecutive utterances have at least one entity in common. Even though it sounds very simple, this principle is important for the rest of the principles defined in the CT because if it does not hold, none of the other principles can be satisfied. Given that only the continuity principle will be used in this paper and due to space limits, the rest of these principles are not discussed here. Their description can be found in (Kibble and Power, 2000). For the same reason we will not go into details about the CT. In this paper, we take an approach similar to (Karamanis and Manurung, 2002) and try to produce summaries which do not violate the continuity principle. In this way, we hope to produce summaries which contain sequences of sentences that refer the same entity, and therefore will be more coherent. Before we can test if the principle is satisfied, it is necessary to define certain parameters on which the principle relies. As aforementioned, the principle is tested on pairs of consecutive utterances. In general utterances are clauses"
W03-1205,P99-1072,0,0.156565,"reader can understand, at least partially, the meaning of the referential expression. As observed for DR, the values for individual texts are mixed. 5.3 does not lead to major loss of information, for several text this method obtains the highest score. In contrast, the greedy method seems to exclude useful information, for several texts, performing worse than the basic method and the baseline. 6 Related work In text summarisation several researchers have addressed the problem of producing coherent summaries. In general, rules are applied to revise summaries produced by a summarisation system (Mani et al., 1999; Otterbacher et al., 2002). These rules are produced by humans who read the automatic summaries and identify coherence problems. Marcu (2000) produced coherent summaries using Rhetorical Structure Theory (RST). A combination of RST and lexical chains is employed in (Alonso i Alemany and Fuentes Fort, 2003) for the same purpose. Comparison to the work by Marcu and Alonso i Alemany is difficult to make because they worked with different types of texts. As already mentioned, information from centering theory was used in text generation to select the most coherent text from several candidates (Ki"
W03-1205,W02-0404,0,0.0208502,"nd, at least partially, the meaning of the referential expression. As observed for DR, the values for individual texts are mixed. 5.3 does not lead to major loss of information, for several text this method obtains the highest score. In contrast, the greedy method seems to exclude useful information, for several texts, performing worse than the basic method and the baseline. 6 Related work In text summarisation several researchers have addressed the problem of producing coherent summaries. In general, rules are applied to revise summaries produced by a summarisation system (Mani et al., 1999; Otterbacher et al., 2002). These rules are produced by humans who read the automatic summaries and identify coherence problems. Marcu (2000) produced coherent summaries using Rhetorical Structure Theory (RST). A combination of RST and lexical chains is employed in (Alonso i Alemany and Fuentes Fort, 2003) for the same purpose. Comparison to the work by Marcu and Alonso i Alemany is difficult to make because they worked with different types of texts. As already mentioned, information from centering theory was used in text generation to select the most coherent text from several candidates (Kibble and Power, 2000; Karam"
W03-1205,A97-1011,0,0.0711565,"Missing"
W03-1205,C96-2166,0,0.0296762,"s that we do not intend to change the order of the extracted sentences. Such an addition would be interesting, but preliminary experiments did not lead to any promising results. 4.1 Content-based scoring method We rely on several existing scoring methods to determine the importance of a sentence on the basis of its content. In this section we briefly describe how this score is computed. The heuristics employed to compute the score are: Keyword method: uses the TF-IDF scores of words to compute the importance of sentences. The score of a sentence is the sum of words’ scores from that sentence (Zechner, 1996) Indicator phrase method: Paice (1981) noticed that in scientific papers it is possible to identify phrases such as in this paper, we present, in conclusion, which are usually meta-discourse markers. A list of such phrases has been built and all the sentences which contain an indicating phrase have their scores boosted or penalised depending on the phrase. Location method: In scientific papers important sentences tend to appear at the beginning and end of the document. For this reason sentences in the first and the last 13 paragraphs have their scores boosted. This value was determined through"
W16-3413,S15-2017,1,0.884314,"Missing"
W16-3413,W13-2242,0,0.051842,"Missing"
W16-3413,C04-1046,0,0.0571293,"text for our research. Section 3 introduces our approach to integrating semantic information into QE. Section 4 details our experimental set-up, including the Semantic Textual Similarity in Quality Estimation 257 tools we use for our experiments. Section 5 explains our experiments, details our new STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such features include n-gram"
W16-3413,W08-0309,0,0.06305,"Missing"
W16-3413,W12-3103,0,0.0155089,"y (TM) users based on estimated PE effort. In contrast, Specia et al., 2010 use QE to rank translations from different systems and highlight inadequate segments for post-editing. Since 2012, QE has been the focus of a shared task at the annual Workshop for Statistical Machine Translation (WMT) (Callison-Burch et al., 2012). This task has provided a common ground for the comparison and evaluation of different QE systems and data at the word, sentence and document level (Bojar et al., 2015). There have been a few attempts to integrate semantic similarity into the MT evaluation (Lo and Wu, 2011, Castillo and Estrella, 2012). The results reported are generally positive, showing that semantic information is not only useful, but often necessary, in order to assess the quality of machine translation output. Specia et al. (2011) bring semantic information into the realm of QE in order to address the problem of meaning preservation. The authors focus on what they term “adequacy indicators” and human annotations for adequacy. The results they report show improvement with respect to a majority class baseline. Rubino et al. (2013) also address MT adequacy using topic models for QE. By including topic model features that"
W16-3413,W03-0413,0,0.0627228,"of the art in QE and the context for our research. Section 3 introduces our approach to integrating semantic information into QE. Section 4 details our experimental set-up, including the Semantic Textual Similarity in Quality Estimation 257 tools we use for our experiments. Section 5 explains our experiments, details our new STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such fe"
W16-3413,P10-1064,0,0.0411147,"Missing"
W16-3413,W14-4008,0,0.0188572,"(2013) introduce the use of referential translation machines (RTM) for QE. RTM is a computational model for judging monolingual and bilingual similarity that achieves state-of-the-art results. The authors report top performance in both sentence level and word-level tasks of WMT 258 B´echara et al. 2013. Camargo de Souza et al. (2014) propose a set of features that explore word alignment information in order to address semantic relations between sentences. Their results show that POS indicator features improve over the baseline at the shared task for QE at the workshop for machine translation. Kaljahi et al. (2014) employ syntactic and semantic information in quality estimation and are able to improve over the baseline when combining these features with the surface features of the baseline. Our work builds on previous work, focusing on the necessity of semantic information for MT adequacy. As far as we are aware, our work is the first to explore quality scores from semantically similar sentences as surrogate to the quality of the current sentence. 3 Our Approach In this paper, we propose integrating semantic similarity into the quality estimation task. As STS relies on monolingual data, we employ the us"
W16-3413,2005.mtsummit-papers.11,0,0.432223,"ate-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our experiments. Semantic Textual Similarity in Quality Estimation 5 261 Experiments As mentioned earlier, all our datasets focus on MTQE for English→French MT output. In all our experiments we have a set of machine translated sentences A for which we need a QE and a set of sentences B, semantically similar to the set of sentences A and fo"
W16-3413,P07-2045,0,0.0110638,"Corpus Pattern Analysis. The system performs well and obtained a mean 0.7216 Pearson correlation in the shared task, ranking 33 out of 74 systems. We train the STS tool on the SICK dataset Marelli et al., 2014, a dataset specifically designed for semantic similarity and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our sy"
W16-3413,N03-1017,0,0.0195562,"and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our experiments. Semantic Textual Similarity in Quality Estimation 5 261 Experiments As mentio"
W16-3413,P04-1077,0,0.0735564,"n 4.2) in all but one of our experiments, where we already have human annotations about STS. This experiment, the oracle experiment represents scores we could achieve if our STS was perfect. Quality Score for Sentence B: We calculate the quality of the MT output of Sentence B. This is either a S-B LEU score based on a reference translation, or a manual score provided by a human evaluator. S-B LEU Score for Sentence A: We have no human evaluation or reference translation for Sentence A, but we can calculate a quality score using Sentence B as a reference. We use sentence-level B LEU (S-B LEU) (Lin and Och, 2004). S-B LEU is designed to work at the sentence level and will still positively score segments that do not have a high order n-gram match. 4 Experimental Setting In this section, we start with a brief introduction to the QuEst framework, followed by a description of the settings for the experiments described in this paper. 4.1 The QuEst Framework QuEst (Specia et al., 2013) is an open source framework for MTQE.4 In addition to a feature extraction framework, QuEst also provides the machine learning algorithms necessary to build the prediction models. QuEst gives access to a large variety of feat"
W16-3413,P11-1023,0,0.070091,"Translation Memory (TM) users based on estimated PE effort. In contrast, Specia et al., 2010 use QE to rank translations from different systems and highlight inadequate segments for post-editing. Since 2012, QE has been the focus of a shared task at the annual Workshop for Statistical Machine Translation (WMT) (Callison-Burch et al., 2012). This task has provided a common ground for the comparison and evaluation of different QE systems and data at the word, sentence and document level (Bojar et al., 2015). There have been a few attempts to integrate semantic similarity into the MT evaluation (Lo and Wu, 2011, Castillo and Estrella, 2012). The results reported are generally positive, showing that semantic information is not only useful, but often necessary, in order to assess the quality of machine translation output. Specia et al. (2011) bring semantic information into the realm of QE in order to address the problem of meaning preservation. The authors focus on what they term “adequacy indicators” and human annotations for adequacy. The results they report show improvement with respect to a majority class baseline. Rubino et al. (2013) also address MT adequacy using topic models for QE. By includ"
W16-3413,marelli-etal-2014-sick,0,0.212547,"orpus of the source language percentage of unigrams in the source sentence seen in a corpus number of punctuation marks in source sentence number of punctuation marks in target sentence sentences. The authors train their system on a variety of linguistically motivated features inspired by deep semantics with distributional Similarity Measures, Conceptual Similarity Measures, Semantic Similarity Measures and Corpus Pattern Analysis. The system performs well and obtained a mean 0.7216 Pearson correlation in the shared task, ranking 33 out of 74 systems. We train the STS tool on the SICK dataset Marelli et al., 2014, a dataset specifically designed for semantic similarity and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with ref"
W16-3413,P03-1021,0,0.0310902,"a from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our experiments. Semantic Textual Similarity in Quality Estimation 5 261 Experiments As mentioned earlier, all our datasets focus on MTQE for English"
W16-3413,J03-1002,0,0.0167393,"dataset Marelli et al., 2014, a dataset specifically designed for semantic similarity and used in previous SemEval tasks, augmented with training data from previous SemEval tasks (SemEval2014 and SemEval2015). 4.3 Statistical Machine Translation System All of our experiments require MT output to run MTQE tasks. To that end, we use the state-of-the-art phrase based Statistical Machine Translation (SMT) system Moses (Koehn et al., 2007). We build 5-gram language models with Kneser-Ney smoothing trained with SRILM, (Stolcke, 2002), and run the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), with refinement and phrase-extraction heuristics as described in Koehn et al. (2003). We use Minimum Error Rate Training (MERT) (Och, 2003) for tuning. In order to keep our experiments consistent, we use the same SMT system for all datasets. We focus on English into French translations and we use the Europarl corpus (Koehn, 2005) for training. We train on 500,000 unique English–French sentences and then tune our system (using MERT) on 1,000 different unique sentences also from the Europarl corpus. We also train a French–English system to retrieve the backtranslations used in some of our expe"
W16-3413,P13-4014,1,0.875959,"Missing"
W16-3413,2009.eamt-1.5,1,0.82388,"ew STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such features include n-gram counts, the average length of tokens, punctuation statistics and sentence length among other features. Later systems incorporate linguistic features such as part of speech tags, syntactic information and word alignment information (Specia et al., 2010). In the context of QE, the term “quality”"
W16-3413,2009.mtsummit-papers.16,1,0.829666,"ew STS features and summarises the results we observe when adding these features to QuEst. Finally, Section 6 presents our concluding remarks and plans for future work. 2 Previous Work Early work in QE built on the concept of confidence estimation used in speech recognition (Gandrabur and Foster, 2003, Blatz et al., 2004). These systems usually relied on system-dependent features, and focused on measuring how confident a given system is rather than how correct the translation is. Later experiments in QE used only system-independent features based on the source sentence and target translation (Specia et al., 2009b). They trained a Support Vector Machine (SVM) regression model based on 74 shallow features, and reported significant gains in accuracy over MT evaluation metrics. At first, these approaches to QE focused mainly on shallow features based on the source and target sentences. Such features include n-gram counts, the average length of tokens, punctuation statistics and sentence length among other features. Later systems incorporate linguistic features such as part of speech tags, syntactic information and word alignment information (Specia et al., 2010). In the context of QE, the term “quality”"
W16-3413,steinberger-etal-2006-jrc,0,0.352142,"machine translations. Each sentence in the WMT dataset comes with a score between 1 and 5, provided by human annotators. However, this method proved to be too time-consuming, as it often required scoring thousands of sentences before finding two that were similar. The first obstacle we faced in testing our approach with these datasets was the collection of similar sentences against which to compare and evaluate. We automatically searched large parallel corpora for sentences that yielded high similarity scores. These corpora included the Europarl corpus (Koehn, 2005), the Acquis Communautaire (Steinberger et al., 2006) and previous WMT data (from 2012 and 2013). Furthermore, the STS system we use (see Section 4.2) returned many false-positives. Some sentences which appeared similar to the STS system were actually too different to be usable. This led to noisy data and unusable results. The scarcity of semantically similar sentences and the computational cost of finding these sentences, lead us to look into alternate datasets, preferably those with semantic similarity built into the corpus: the DGT-TM and the SICK dataset. All our experiments have the same set-up. In all cases, we used 500 randomly selected s"
W18-6239,P11-1015,0,0.0678917,"Missing"
W18-6239,D14-1162,0,0.0863401,"s positive (good, wonderful, fantastic) or negative (bad, poor, horrible) polarity. However, finding the inherent sentiment of a text from content words is not a straightforward problem, due to ambiguity of word meanings and complex sentiments such as sarcasm. Hence, efficient and accurate word representations which considers the context information also, become necessary. Representation of words as real-valued vectors has been employed in sentiment analysis, as in other NLP problems. There are two common architectures for word vector representations: Word2Vec (Mikolov et al, 2013) and GloVe (Pennington et al, 2014). Word2Vec has two models: Skipgram where the objective is to predict a word’s context given the word itself and Bag of Words (BoW) where the objective is to predict a word given its context. GloVe (Global Vectors) was proposed as an alternative model, in which the global corpus statistics are captured directly. Over the years, there have been attempts to incorporate the sentiment information of the words into these vectors, to make them more suitable for analysis of sentiment in documents and short texts such as tweets (Maas et al,2011, Tang et al., 2014). Our methods to find stress reasons f"
W18-6239,P14-1146,0,0.0180556,"(Mikolov et al, 2013) and GloVe (Pennington et al, 2014). Word2Vec has two models: Skipgram where the objective is to predict a word’s context given the word itself and Bag of Words (BoW) where the objective is to predict a word given its context. GloVe (Global Vectors) was proposed as an alternative model, in which the global corpus statistics are captured directly. Over the years, there have been attempts to incorporate the sentiment information of the words into these vectors, to make them more suitable for analysis of sentiment in documents and short texts such as tweets (Maas et al,2011, Tang et al., 2014). Our methods to find stress reasons from tweets also use word vector representations as illustrated in the next section. Topic Modelling in Tweets Topic modelling is the extraction of latent topics in documents, which may be helpful to find stress reasons from a collection of texts. Two common topic modelling methods for documents are Latent Dirichlet Allocation (LDA) (Blei et al, 2003) and Author Topic Models (ATM) (Rozen-Zvi et al., 2005). The applicability of these methods to tweets is hindered by informal language, grammatical errors, slang and emoticons. To overcome these issues, aggrega"
W18-6705,W16-3512,0,0.03498,"Missing"
