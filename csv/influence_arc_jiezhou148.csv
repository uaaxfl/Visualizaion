2020.aacl-main.75,D18-1247,1,0.843969,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,D18-1514,1,0.903034,"red from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 1"
2020.aacl-main.75,P17-1004,1,0.853193,"t al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting tha"
2020.aacl-main.75,P16-1200,1,0.830477,"ut these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relati"
2020.aacl-main.75,D17-1189,0,0.0141779,"Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schem"
2020.aacl-main.75,P15-2047,0,0.0397231,"Missing"
2020.aacl-main.75,Q16-1017,0,0.0159895,"arios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Bara"
2020.aacl-main.75,D12-1048,0,0.0405797,"pple’s current CEO. Relation B Satya Nadella became the CEO of Microsoft in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction"
2020.aacl-main.75,P16-1105,0,0.0181338,"much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zh"
2020.aacl-main.75,D12-1104,0,0.0209936,"section, we introduce the development of RE methods following the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not expres"
2020.aacl-main.75,D16-1261,0,0.0146614,"ocuments), the current RE models for this challenge are still crude and straightforward. Followings are some directions worth further investigation: (1) Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences. Most of current RE models are still very weak in these abilities. (2) Besides documents, more forms of context is also worth exploring, such as extracting relational facts across documents, or understanding relational information based on heterogeneous data. (3) Inspired by Narasimhan et al. (2016), which utilizes search engines for acquiring external information, automatically searching and analysing context for RE may help RE models identify relational facts with more coverage and become practical for daily scenarios. 3.4 Orienting More Open Domains Most RE systems work within pre-specified relation sets designed by human experts. However, our world undergoes open-ended growth of relations and it is not possible to handle all these emerging Jeﬀ Bezos, an American entrepreneur, graduated from Princeton in 1986. Jeﬀ Bezos graduated from Princeton Figure 8: An example of open information"
2020.aacl-main.75,N07-2032,0,0.130676,"Missing"
2020.aacl-main.75,W15-1506,0,0.0165476,"al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embed"
2020.aacl-main.75,P11-2048,0,0.0293116,"fact, there have been various works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompan"
2020.aacl-main.75,C18-1326,0,0.0187317,"(2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely benefit the applications of Open IE. There are already some preliminary works in this area (Gal´arraga et al., 2014; 751 Vashishth et al., 2018) and more efforts are needed. (2) The not applicable (N/A) relation has been hardly addressed in relation discovery. In previous work, it is usually assumed that the s"
2020.aacl-main.75,N13-1095,0,0.0126813,"us works exploring feasible approaches that lead to better RE abilities on realworld scenarios. In this section, we summarize these exploratory efforts into four directions, and give our review and outlook about these directions. 3.1 Utilizing More Data Supervised NRE models suffer from the lack of large-scale high-quality training data, since manually labeling data is time-consuming and humanintensive. To alleviate this issue, distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text (Mintz et al., 2009; Nguyen and Moschitti, 2011; Min et al., 2013). As shown in Figure 3, for Model NYT-10 Wiki-Distant PCNN-ONE PCNN-ATT BERT 0.340 0.349 0.458 0.214 0.222 0.361 Table 2: Area under the curve (AUC) of PCNN-ONE (Zeng et al., 2015), PCNN-ATT (Lin et al., 2016) and BERT (Devlin et al., 2019) on two datasets. any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs. Large-scale training examples can be easily constructed by this heuristic scheme. Although DS provides a feasible approach to utilize more data, this automatic labeling mechanism is inevitably accompanied by the wrong la"
2020.aacl-main.75,Q17-1008,0,0.0129385,"here are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) construct a general document-level RE dataset annotated by crowdsourcing workers, suitable for evaluating general-purpose document-level RE systems. Although there are some effo"
2020.aacl-main.75,P09-1113,0,0.736156,"ions (Section 3) targeting more complex RE scenarios. Those feasible approaches leading to better RE abilities still require further efforts, and here we summarize them into four directions: 745 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 745–758 c December 4 - 7, 2020. 2020 Association for Computational Linguistics (1) Utilizing More Data (Section 3.1). Supervised RE methods heavily rely on expensive human annotations, while distant supervision (Mintz et al., 2009) introduces more auto-labeled data to alleviate this issue. Yet distant methods bring noise examples and just utilize single sentences mentioning entity pairs, which significantly weaken extraction performance. Designing schemas to obtain highquality and high-coverage data to train robust RE models still remains a problem to be explored. (2) Performing More Efficient Learning (Section 3.2). Lots of long-tail relations only contain a handful of training examples. However, it is hard for conventional RE methods to well generalize relation patterns from limited examples like humans. Therefore, de"
2020.aacl-main.75,E17-1110,0,0.0240382,"s many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field,"
2020.aacl-main.75,D16-1252,0,0.0244806,"t in 2014. Figure 9: An example of clustering-based relation discovery, which identifying potential relation types by clustering unlabeled relational instances. relation types only by humans. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved res"
2020.aacl-main.75,D12-1042,0,0.13543,"accompanied by the wrong labeling problem. The reason is that not all sentences mentioning the two entities express their relations in KGs exactly. For example, we may mistakenly label “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora"
2020.aacl-main.75,N13-1008,0,0.101883,"ther statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend"
2020.aacl-main.75,C02-1151,0,0.535663,"s mentioned in this work are collected into the following paper list https://github. com/thunlp/NREPapers. † to researching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world"
2020.aacl-main.75,W04-2401,0,0.331956,"Missing"
2020.aacl-main.75,P15-1061,0,0.0207109,"askar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE"
2020.aacl-main.75,R11-1004,0,0.0168865,"ext As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory a"
2020.aacl-main.75,P10-1040,0,0.00513333,", recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there are also other works integrating syntactic information into NRE models. Xu et al. (2015a) and Xu et al. (2015b) adopt CNNs and RNNs over shortest dependency paths respectively. Liu et al. (2015) propose a recursive neural network based on augm"
2020.aacl-main.75,N16-1103,0,0.0654802,"tences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Dista"
2020.aacl-main.75,W16-1312,0,0.049412,"Missing"
2020.aacl-main.75,N18-1080,0,0.0617987,"ces, and proportions of N/A instances respectively. iPhone is designed by Apple Inc. iPhone is a iconic product of Apple. Dataset Tim Cook I looked up Apple Inc. on my iPhone. Figure 3: An example of distantly supervised relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. 2016; Riedel et al., 2013). Recently, Transformers (Vaswani et al., 2017) and pre-trained language models (Devlin et al., 2019) have also been explored for NRE (Du et al., 2018; Verga et al., 2018; Wu and He, 2019; Baldini Soares et al., 2019) and have achieved new state-of-the-arts. By concisely reviewing the above techniques, we are able to track the development of RE from pattern and statistical methods to neural models. Comparing the performance of state-of-the-art RE models in years (Figure 2), we can see the vast increase since the emergence of NRE, which demonstrates the power of neural methods. 3 “More” Directions for RE Although the above-mentioned NRE models have achieved superior results on benchmarks, they are still far from solving the problem of RE. Most of these models u"
2020.aacl-main.75,N06-1039,0,0.0823614,"shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normal"
2020.aacl-main.75,D12-1110,0,0.0508116,"relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this paper, we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-b"
2020.aacl-main.75,N16-1065,0,0.147165,"-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which enc"
2020.aacl-main.75,P16-1123,1,0.90021,"Missing"
2020.aacl-main.75,D18-1246,0,0.0377077,"Missing"
2020.aacl-main.75,I08-2119,0,0.0392005,"thods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings"
2020.aacl-main.75,C18-1099,1,0.853339,"simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 300 Relations 400 founder Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relatio"
2020.aacl-main.75,D11-1135,0,0.0169809,"n work in open scenarios. There are already some explorations in handling open relations: (1) Open information extraction (Open IE), as shown in Figure 8, extracts relation phrases and arguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open I"
2020.aacl-main.75,P19-1074,1,0.902979,"dels may overfit simple textual cues between relations instead of really understanding the semantics of the context. More details about the experiments are in Appendix A. 3.3 Handling More Complicated Context As shown in Figure 7, one document generally mentions many entities exhibiting complex crosssentence relations. Most existing methods focus on intra-sentence RE and thus are inadequate for collectively identifying these relational facts expressed in a long paragraph. In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inf"
2020.aacl-main.75,D13-1136,0,0.0269434,"There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it still faces some challenges. Feature-based and kernelbased models require many efforts to design features or kernel functions. While graphical and embedding methods can predict relations without too much human intervention, they are still limited in model capacities. There are some surveys systematically introducing SRE models (Zelenko et al., 2003; Bach and Badaskar, 2007; Pawar et al., 2017). In this pa"
2020.aacl-main.75,P19-1277,0,0.0237425,"Missing"
2020.aacl-main.75,W06-1671,0,0.0514824,"Missing"
2020.aacl-main.75,D19-1021,1,0.854571,"rguments (entities) from text (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018). Open IE does not rely on specific relation types and thus can handle all kinds of relational facts. (2) Relation discovery, as shown in Figure 9, aims at discovering unseen relation types from unsupervised data. Yao et al. (2011); Marcheggiani and Titov (2016) propose to use generative models and treat these relations as latent variables, while Shinyama and Sekine (2006); Elsahar et al. (2017); Wu et al. (2019) cast relation discovery as a clustering task. Though relation extraction in open domains has been widely studied, there are still lots of unsolved research questions remained to be answered: (1) Canonicalizing relation phrases and arguments in Open IE is crucial for downstream tasks (Niklaus et al., 2018). If not canonicalized, the extracted relational facts could be redundant and ambiguous. For example, Open IE may extract two triples (Barack Obama, was born in, Honolulu) and (Obama, place of birth, Honolulu) indicating an identical fact. Thus, normalizing extracted results will largely bene"
2020.aacl-main.75,D17-1187,0,0.0159463,"der Figure 4: Relation distributions (log-scale) on the training part of DS datasets NYT-10 and Wiki-Distant, suggesting that real-world relation distributions suffer from the long-tail problem. mechanisms and training strategies to enhance distantly supervised NRE models. Vu et al. (2016); Beltagy et al. (2019) combine different architectures and training strategies to construct hybrid frameworks. Liu et al. (2017) incorporate a softlabel scheme by changing unconfident labels during training. Furthermore, reinforcement learning (Feng et al., 2018; Zeng et al., 2018) and adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a) have also been adopted in DS. The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models, and there still remains some open problems worth exploring: (1) Existing DS methods focus on denoising auto-labeled instances and it is certainly meaningful to follow this research direction. Besides, current DS schemes are still similar to the original one in (Mintz et al., 2009), which just covers the case that the entity pairs are mentioned in the same sentences. To achieve better coverage and less noise, e"
2020.aacl-main.75,C16-1119,0,0.0178955,"tions for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word embeddings and position embeddings, there"
2020.aacl-main.75,D15-1062,0,0.0540036,"Missing"
2020.aacl-main.75,C16-1138,0,0.0498916,"Missing"
2020.aacl-main.75,D15-1206,1,0.888587,"Missing"
2020.aacl-main.75,C10-2160,0,0.0215937,"e features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from textual embeddings (Weston et al., 2013; Riedel et al., 2013; Gormley et al., 2015). Furthermore, Bordes et al. (2013),Wang et al. (2014) and Lin et al. (2015) utilize KG embeddings for RE. Although SRE has been widely studied, it sti"
2020.aacl-main.75,D15-1203,0,0.0647772,"we do not spend too much space for SRE and focus more on neural-based models. 82.4 82.7 2013 2014 77.6 (SRE) Before 2013 2015 2016 Now Figure 2: The performance of state-of-the-art RE models in different years on widely-used dataset SemEval2010 Task 8. The adoption of neural models (since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position e"
2020.aacl-main.75,C14-1220,0,0.226372,"rching relation extraction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and t"
2020.aacl-main.75,D17-1186,1,0.860499,". In fact, most relational facts can only be extracted from complicated context like documents rather than single sentences (Yao et al., 2019), which should not be neglected. There are already some works proposed to extract relations across multiple sentences: 750 (1) Syntactic methods (Wick et al., 2006; Gerber and Chai, 2010; Swampillai and Stevenson, 2011; Yoshikawa et al., 2011; Quirk and Poon, 2017) rely on textual features extracted from various syntactic structures, such as coreference annotations, dependency parsing trees and discourse relations, to connect sentences in documents. (2) Zeng et al. (2017); Christopoulou et al. (2018) build inter-sentence entity graphs, which can utilize multi-hop paths between entities for inferring the correct relations. (3) Peng et al. (2017); Song et al. (2018); Zhu et al. (2019b) employ graph-structured neural networks to model cross-sentence dependencies for relation extraction, which bring in memory and reasoning abilities. To advance this field, some document-level RE datasets have been proposed. Quirk and Poon (2017); Peng et al. (2017) build datasets by DS. Li et al. (2016); Peng et al. (2017) propose datasets for specific domains. Yao et al. (2019) c"
2020.aacl-main.75,N06-1037,0,0.077591,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,P06-1104,0,0.0618459,"ach is feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Jiang and Zhai, 2007; Nguyen et al., 2007), which design lexical, syntactic and semantic features for entity pairs and their corresponding context, and then input these features into relation classifiers. Due to the wide use of support vector machines (SVM), kernel-based methods have been widely explored, which design kernel functions for SVM to measure the similarities between relation representations and textual instances (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Zhao and Grishman, 2005; Mooney and Bunescu, 2006; Zhang et al., 2006b,a; Wang, 2008). There are also some other statistical methods focusing on extracting and inferring the latent information hidden in the text. Graphical methods (Roth and Yih, 2002, 2004; Sarawagi and Cohen, 2005; Yu and Lam, 2010) abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations. Inspired by the success of embedding models in other NLP tasks (Mikolov et al., 2013a,b), there are also efforts in encoding text into low-dimensional semantic spaces and extracting relations from te"
2020.aacl-main.75,N19-1306,0,0.067956,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,Y15-1009,0,0.0785539,"raction (RE), which aims at extracting relational facts from plain text. More specifically, after identifying entity mentions (e.g., USA and New York) in text, the main goal of RE is to classify relations (e.g., contains) between these entity mentions from their context. The pioneering explorations of RE lie in statistical approaches, such as pattern mining (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004) and graphical models (Roth and Yih, 2002). Recently, with the development of deep learning, neural models have been widely adopted for RE (Zeng et al., 2014; Zhang et al., 2015) and achieved superior results. These RE methods have bridged the gap between unstructured text and structured knowledge, and shown their effectiveness on several public benchmarks. Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting: (1) collecting high-quality human annotations is expensive and time-consuming, (2) ma"
2020.aacl-main.75,D18-1244,0,0.013069,"since 2013) has brought great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embedd"
2020.aacl-main.75,D17-1004,0,0.217937,"abel “Bill Gates retired from Microsoft” with the relation founder, if (Bill Gates, founder, Microsoft) is a relational fact in KGs. The existing methods to alleviate the noise problem can be divided into three major approaches: (1) Some methods adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distri"
2020.aacl-main.75,P19-1139,1,0.91764,"hem. Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) utilize graphical model to infer the informative sentences, while Zeng et al. (2015) use a simple heuristic selection strategy. Later on, Lin et al. (2016); Zhang et al. (2017); Han et al. (2018c); Li et al. (2020); Zhu et al. (2019c); Hu et al. (2019) design attention mechanisms to highlight informative instances for RE. (2) Incorporating extra context information to denoise DS data has also been explored, such as incorporating KGs as external information to guide instance selection (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Qu et al., 2019) and adopting multi-lingual corpora for the information consistency and complementarity (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018). (3) Many methods tend to utilize sophisticated 748 Relation Distribution on NYT-10 104 104 Numbers of Instances Numbers of Instances Supporting Set Relation Distribution on Wiki-Distant 105 103 102 101 100 iPhone is designed by Apple Inc. product 103 Steve Jobs is the co-founder of Apple Inc. 102 Tim Cook is Apple’s current CEO. founder CEO Query Instance 101 Bill Gates founded Microsoft. ? 100 0 10 20 30 Relations 40 0 100 200 30"
2020.aacl-main.75,P05-1052,0,0.0462651,"Missing"
2020.aacl-main.75,P19-1137,1,0.923863,"setting, from early pattern-based methods, statistical approaches, to recent neural models. 2.1 Pattern Extraction Models The pioneering methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements (Soderland et al., 1995; Kim and Moldovan, 1995; Huffman, 1995; Califf and Mooney, 1997). In order to extract patterns with better coverage and accuracy, later work involves larger corpora (Carlson et al., 2010), more formats of patterns (Nakashole et al., 2012; Jiang et al., 2017), and more efficient ways of extraction (Zheng et al., 2019). As automatically constructed patterns may have mistakes, most of the above methods require further examinations from human experts, which is the main limitation of pattern-based models. 2.2 Statistical Relation Extraction Models As compared to using pattern rules, statistical methods bring better coverage and require less human efforts. Thus statistical relation extraction (SRE) has been extensively studied. 746 2 Sometimes there is a special class in the relation set indicating that the sentence does not express any pre-specified relation (usually named as N/A). 2.3 Neural Relation Extracti"
2020.aacl-main.75,P05-1053,0,0.264276,"Missing"
2020.aacl-main.75,P16-2034,0,0.0170838,"6) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al., 2014) are introduced to specify the relative distances between words and entities. Except for word em"
2020.aacl-main.75,P19-1128,1,0.923461,"ght great improvement in performance. sive neural networks (Socher et al., 2012; Miwa and Bansal, 2016) that learn compositional representations for sentences recursively, convolutional neural networks (CNNs) (Liu et al., 2013; Zeng et al., 2014; Santos et al., 2015; Nguyen and Grishman, 2015b; Zeng et al., 2015; Huang and Wang, 2017) that effectively model local textual patterns, recurrent neural networks (RNNs) (Zhang and Wang, 2015; Nguyen and Grishman, 2015a; Vu et al., 2016; Zhang et al., 2015) that can better handle long sequential data, graph neural networks (GNNs) (Zhang et al., 2018; Zhu et al., 2019a) that build word/entity graphs for reasoning, and attention-based neural networks (Zhou et al., 2016; Wang et al., 2016; Xiao and Liu, 2016) that utilize attention mechanism to aggregate global relational information. Different from SRE models, NRE mainly utilizes word embeddings and position embeddings instead of hand-craft features as inputs. Word embeddings (Turian et al., 2010; Mikolov et al., 2013b) are the most used input representations in NLP, which encode the semantic meaning of words into vectors. In order to capture the entity information in text, position embeddings (Zeng et al.,"
2020.aacl-main.75,N07-1015,0,\N,Missing
2020.aacl-main.75,D14-1067,0,\N,Missing
2020.aacl-main.75,C96-1079,0,\N,Missing
2020.aacl-main.75,P10-1160,0,\N,Missing
2020.aacl-main.75,P04-1054,0,\N,Missing
2020.aacl-main.75,P11-1055,0,\N,Missing
2020.aacl-main.75,H05-1091,0,\N,Missing
2020.aacl-main.75,D11-1142,0,\N,Missing
2020.aacl-main.75,P15-1034,0,\N,Missing
2020.aacl-main.75,P16-1072,0,\N,Missing
2020.aacl-main.75,P18-2014,0,\N,Missing
2020.aacl-main.75,D18-1245,0,\N,Missing
2020.aacl-main.75,P18-2065,0,\N,Missing
2020.aacl-main.75,N19-1184,0,\N,Missing
2020.aacl-main.75,D17-1191,0,\N,Missing
2020.aacl-main.75,N19-1423,0,\N,Missing
2020.aacl-main.75,D19-1395,0,\N,Missing
2020.aacl-main.75,D19-3029,1,\N,Missing
2020.aacl-main.75,D19-1649,1,\N,Missing
2020.acl-main.104,P19-1633,0,0.240882,"t classification (MLC) problem, where the classification result corresponds to one or more nodes of a taxonomic hierarchy. The taxonomic hierarchy is commonly modeled as a tree or a directed acyclic graph, as depicted in Figure 1. Existing approaches for HTC could be categorized into two groups: local approach and global ∗ † This work was done during intern at Alibaba Group. Corresponding author. approach. The first group tends to constructs multiple classification models and then traverse the hierarchy in a top-down manner. Previous local studies (Wehrmann et al., 2018; Shimura et al., 2018; Banerjee et al., 2019) propose to overcome the data imbalance on child nodes by learning from parent one. However, these models contain a large number of parameters and easily lead to exposure bias for the lack of holistic structural information. The global approach treats HTC problem as a flat MLC problem, and uses one single classifier for all classes. Recent global methods introduce various strategies to utilize structural information of top-down paths, such as recursive regularization (Gopal and Yang, 2013), reinforcement learning (Mao et al., 2019) and meta-learning (Wu et al., 2019). There is so far no global"
2020.acl-main.104,P17-1177,0,0.108633,"the attention mechanism is introduced in MLC by Mullenbach et al. (2018) for ICD coding. Rios and Kavuluru (2018) trains label representation through basic GraphCNN and conducts mutli-label attention with residual shortcuts. AttentionXML (You et al., 2019) converts MLC to a multi-label attention LCL model by label clusters. Huang et al. (2019) improves HMCN (Wehrmann et al., 2018) with label attention per level. Our HiAGM-LA, however, employs multi-label attention in a single model with a simplified structure encoder, reducing the computational complexity. Recent works, in semantic analysis (Chen et al., 2017b), semantic role labeling (He et al., 2018) and machine translation (Chen et al., 2017a), shows the improvement on sentence representation of syntax 1107 1 https://github.com/Alibaba-NLP/HiAGM 4 Hierarchy-Aware Global Model encoder, such as Tree-Based RNN (Tai et al., 2015; Chen et al., 2017a) and GraphCNN (Marcheggiani and Titov, 2017). We modify those structure encoders for HTC with fine-grained prior knowledge in both top-down and bottom-up manners. As depicted in Figure 3, we propose a HierarchyAware Global Model (HiAGM) that leverages the fine-grained hierarchy information and then aggre"
2020.acl-main.104,P17-1152,0,0.190379,"the attention mechanism is introduced in MLC by Mullenbach et al. (2018) for ICD coding. Rios and Kavuluru (2018) trains label representation through basic GraphCNN and conducts mutli-label attention with residual shortcuts. AttentionXML (You et al., 2019) converts MLC to a multi-label attention LCL model by label clusters. Huang et al. (2019) improves HMCN (Wehrmann et al., 2018) with label attention per level. Our HiAGM-LA, however, employs multi-label attention in a single model with a simplified structure encoder, reducing the computational complexity. Recent works, in semantic analysis (Chen et al., 2017b), semantic role labeling (He et al., 2018) and machine translation (Chen et al., 2017a), shows the improvement on sentence representation of syntax 1107 1 https://github.com/Alibaba-NLP/HiAGM 4 Hierarchy-Aware Global Model encoder, such as Tree-Based RNN (Tai et al., 2015; Chen et al., 2017a) and GraphCNN (Marcheggiani and Titov, 2017). We modify those structure encoders for HTC with fine-grained prior knowledge in both top-down and bottom-up manners. As depicted in Figure 3, we propose a HierarchyAware Global Model (HiAGM) that leverages the fine-grained hierarchy information and then aggre"
2020.acl-main.104,P18-1192,0,0.115195,"by Mullenbach et al. (2018) for ICD coding. Rios and Kavuluru (2018) trains label representation through basic GraphCNN and conducts mutli-label attention with residual shortcuts. AttentionXML (You et al., 2019) converts MLC to a multi-label attention LCL model by label clusters. Huang et al. (2019) improves HMCN (Wehrmann et al., 2018) with label attention per level. Our HiAGM-LA, however, employs multi-label attention in a single model with a simplified structure encoder, reducing the computational complexity. Recent works, in semantic analysis (Chen et al., 2017b), semantic role labeling (He et al., 2018) and machine translation (Chen et al., 2017a), shows the improvement on sentence representation of syntax 1107 1 https://github.com/Alibaba-NLP/HiAGM 4 Hierarchy-Aware Global Model encoder, such as Tree-Based RNN (Tai et al., 2015; Chen et al., 2017a) and GraphCNN (Marcheggiani and Titov, 2017). We modify those structure encoders for HTC with fine-grained prior knowledge in both top-down and bottom-up manners. As depicted in Figure 3, we propose a HierarchyAware Global Model (HiAGM) that leverages the fine-grained hierarchy information and then aggregates label-wise text features. HiAGM consis"
2020.acl-main.104,D18-1262,1,0.848704,"et al., 2018; Rios and Kavuluru, 2018). As depicted in Figure 3, HiAGM models fine-grained hierarchy information based on the hierarchy-aware structure encoder. Based on the prior hierarchy information, we improve typical structure encoders for the directed hierarchy graph. Specifically, the top-down dataflow employs the N prior hierarchy information as fc (ei,j ) = Nji while the bottom-up one adopts fp (ei,j ) = 1.0. Bidirectional Tree-LSTM Tree-LSTM could be utilized as our structure encoder. The implementation of Tree-LSTM is similar to syntax encoders(Tai et al., 2015; Zhang et al., 2016; Li et al., 2018). The predefined hierarchy is identical to all samples, which allows the mini-batch training method for this recursive computational module. The node transformation is as follows: e k + b(i) ), ik = σ(W(i) vk + U(i) h fk,j = σ(W(f ) vk + U(f ) hj + b(f ) ), e k + b(o) ), ok = σ(W(o) vk + U(o) h e k + b(u) ), uk = tanh(W (u) vk + U (u) h X ck = ik uk + fk,j cj , (2) j hk = ok tanh(ck ), where hk and ck represent the hidden state and memory cell state of node k respectively. To induce label correlations, HiAGM employs a bidirectional Tree-LSTM by the fusion of a childsum and a top-down module: X"
2020.acl-main.104,N15-1092,0,0.0272893,"xt feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGMTP achieve significant and consistent improvements on three benchmark datasets. 1 Figure 1: This short sample is tagged with news, sports, football, features and books. Note that HTC could be either a single-path or a multi-path problem. Introduction Text classification is widely used in Natural Language Processing (NLP) applications, such as sentimental analysis (Pang and Lee, 2007), information retrieval (Liu et al., 2015), and document categorization (Yang et al., 2016). Hierarchical text classification (HTC) is a particular multi-label text classification (MLC) problem, where the classification result corresponds to one or more nodes of a taxonomic hierarchy. The taxonomic hierarchy is commonly modeled as a tree or a directed acyclic graph, as depicted in Figure 1. Existing approaches for HTC could be categorized into two groups: local approach and global ∗ † This work was done during intern at Alibaba Group. Corresponding author. approach. The first group tends to constructs multiple classification models an"
2020.acl-main.104,N15-1000,0,0.177503,"Missing"
2020.acl-main.104,D19-1042,0,0.258471,"cal studies (Wehrmann et al., 2018; Shimura et al., 2018; Banerjee et al., 2019) propose to overcome the data imbalance on child nodes by learning from parent one. However, these models contain a large number of parameters and easily lead to exposure bias for the lack of holistic structural information. The global approach treats HTC problem as a flat MLC problem, and uses one single classifier for all classes. Recent global methods introduce various strategies to utilize structural information of top-down paths, such as recursive regularization (Gopal and Yang, 2013), reinforcement learning (Mao et al., 2019) and meta-learning (Wu et al., 2019). There is so far no global method that encodes the holistic label structure for label correlation features. Moreover, these methods still exploit the hierarchy in a shallow manner, thus ignoring the fine-grained label correlation information that has proved to be more fruitful in our work. In this paper, we formulate the hierarchy as a directed graph and utilize prior probabilities of label dependencies to aggregate node information. A hierarchy-aware global model (HiAGM) is pro1106 Proceedings of the 58th Annual Meeting of the Association for Computational"
2020.acl-main.104,D17-1159,0,0.183062,"Huang et al. (2019) improves HMCN (Wehrmann et al., 2018) with label attention per level. Our HiAGM-LA, however, employs multi-label attention in a single model with a simplified structure encoder, reducing the computational complexity. Recent works, in semantic analysis (Chen et al., 2017b), semantic role labeling (He et al., 2018) and machine translation (Chen et al., 2017a), shows the improvement on sentence representation of syntax 1107 1 https://github.com/Alibaba-NLP/HiAGM 4 Hierarchy-Aware Global Model encoder, such as Tree-Based RNN (Tai et al., 2015; Chen et al., 2017a) and GraphCNN (Marcheggiani and Titov, 2017). We modify those structure encoders for HTC with fine-grained prior knowledge in both top-down and bottom-up manners. As depicted in Figure 3, we propose a HierarchyAware Global Model (HiAGM) that leverages the fine-grained hierarchy information and then aggregates label-wise text features. HiAGM consists of a traditional text encoder for textual information and a hierarchy-aware structure encoder for hierarchical label correlation features. We present two variants of HiAGM for hybrid information aggregation, a multi-label attention model (HiAGM-LA) and a text feature propagation model (HiAGM"
2020.acl-main.104,N18-1100,0,0.0218148,"ion of parameters among adjacent classes. Deep learning architectures are also employed in global models, such as sequence-to-sequence (Yang et al., 2018), metalearning (Wu et al., 2019), reinforcement learning (Mao et al., 2019), and capsule network (Peng et al., 2019). Those models mainly focus on improving decoders based on the constraint of hierarchical paths. In contrast, we propose an effective hierarchy-aware global model, HiAGM, that extracts label-wise text features with hierarchy encoders based on prior hierarchy information. Moreover, the attention mechanism is introduced in MLC by Mullenbach et al. (2018) for ICD coding. Rios and Kavuluru (2018) trains label representation through basic GraphCNN and conducts mutli-label attention with residual shortcuts. AttentionXML (You et al., 2019) converts MLC to a multi-label attention LCL model by label clusters. Huang et al. (2019) improves HMCN (Wehrmann et al., 2018) with label attention per level. Our HiAGM-LA, however, employs multi-label attention in a single model with a simplified structure encoder, reducing the computational complexity. Recent works, in semantic analysis (Chen et al., 2017b), semantic role labeling (He et al., 2018) and machine"
2020.acl-main.104,D14-1162,0,0.101735,"Missing"
2020.acl-main.104,D18-1352,0,0.0817784,"Deep learning architectures are also employed in global models, such as sequence-to-sequence (Yang et al., 2018), metalearning (Wu et al., 2019), reinforcement learning (Mao et al., 2019), and capsule network (Peng et al., 2019). Those models mainly focus on improving decoders based on the constraint of hierarchical paths. In contrast, we propose an effective hierarchy-aware global model, HiAGM, that extracts label-wise text features with hierarchy encoders based on prior hierarchy information. Moreover, the attention mechanism is introduced in MLC by Mullenbach et al. (2018) for ICD coding. Rios and Kavuluru (2018) trains label representation through basic GraphCNN and conducts mutli-label attention with residual shortcuts. AttentionXML (You et al., 2019) converts MLC to a multi-label attention LCL model by label clusters. Huang et al. (2019) improves HMCN (Wehrmann et al., 2018) with label attention per level. Our HiAGM-LA, however, employs multi-label attention in a single model with a simplified structure encoder, reducing the computational complexity. Recent works, in semantic analysis (Chen et al., 2017b), semantic role labeling (He et al., 2018) and machine translation (Chen et al., 2017a), shows"
2020.acl-main.104,D18-1093,0,0.14298,"Missing"
2020.acl-main.104,P19-1320,0,0.0496258,"Missing"
2020.acl-main.104,D19-1444,0,0.237906,"himura et al., 2018; Banerjee et al., 2019) propose to overcome the data imbalance on child nodes by learning from parent one. However, these models contain a large number of parameters and easily lead to exposure bias for the lack of holistic structural information. The global approach treats HTC problem as a flat MLC problem, and uses one single classifier for all classes. Recent global methods introduce various strategies to utilize structural information of top-down paths, such as recursive regularization (Gopal and Yang, 2013), reinforcement learning (Mao et al., 2019) and meta-learning (Wu et al., 2019). There is so far no global method that encodes the holistic label structure for label correlation features. Moreover, these methods still exploit the hierarchy in a shallow manner, thus ignoring the fine-grained label correlation information that has proved to be more fruitful in our work. In this paper, we formulate the hierarchy as a directed graph and utilize prior probabilities of label dependencies to aggregate node information. A hierarchy-aware global model (HiAGM) is pro1106 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1106–1117 c July"
2020.acl-main.104,C18-1330,0,0.108118,"ters of the parent model for child models as LCN. Wehrmann et al. (2018) alleviates exposure bias problem by the hybrid of LCL and global optimizations. Peng et al. (2018) decomposes the hierarchy into subgraphs and conducts Text-GCN on n-gram tokens. The global approach improves flat MLC models with the hierarchy information. Cai and Hofmann (2004) modifies SVM to Hierarchical-SVM by decomposition. Gopal and Yang (2013) proposes a simple recursive regularization of parameters among adjacent classes. Deep learning architectures are also employed in global models, such as sequence-to-sequence (Yang et al., 2018), metalearning (Wu et al., 2019), reinforcement learning (Mao et al., 2019), and capsule network (Peng et al., 2019). Those models mainly focus on improving decoders based on the constraint of hierarchical paths. In contrast, we propose an effective hierarchy-aware global model, HiAGM, that extracts label-wise text features with hierarchy encoders based on prior hierarchy information. Moreover, the attention mechanism is introduced in MLC by Mullenbach et al. (2018) for ICD coding. Rios and Kavuluru (2018) trains label representation through basic GraphCNN and conducts mutli-label attention wi"
2020.acl-main.104,N16-1174,0,0.388825,"sed as the deductive variant that directly feeds text features into hierarchy encoders. Compared with previous works, both HiAGM-LA and HiAGMTP achieve significant and consistent improvements on three benchmark datasets. 1 Figure 1: This short sample is tagged with news, sports, football, features and books. Note that HTC could be either a single-path or a multi-path problem. Introduction Text classification is widely used in Natural Language Processing (NLP) applications, such as sentimental analysis (Pang and Lee, 2007), information retrieval (Liu et al., 2015), and document categorization (Yang et al., 2016). Hierarchical text classification (HTC) is a particular multi-label text classification (MLC) problem, where the classification result corresponds to one or more nodes of a taxonomic hierarchy. The taxonomic hierarchy is commonly modeled as a tree or a directed acyclic graph, as depicted in Figure 1. Existing approaches for HTC could be categorized into two groups: local approach and global ∗ † This work was done during intern at Alibaba Group. Corresponding author. approach. The first group tends to constructs multiple classification models and then traverse the hierarchy in a top-down manne"
2020.acl-main.104,N16-1035,0,0.0201425,"n et al., 2017a; He et al., 2018; Rios and Kavuluru, 2018). As depicted in Figure 3, HiAGM models fine-grained hierarchy information based on the hierarchy-aware structure encoder. Based on the prior hierarchy information, we improve typical structure encoders for the directed hierarchy graph. Specifically, the top-down dataflow employs the N prior hierarchy information as fc (ei,j ) = Nji while the bottom-up one adopts fp (ei,j ) = 1.0. Bidirectional Tree-LSTM Tree-LSTM could be utilized as our structure encoder. The implementation of Tree-LSTM is similar to syntax encoders(Tai et al., 2015; Zhang et al., 2016; Li et al., 2018). The predefined hierarchy is identical to all samples, which allows the mini-batch training method for this recursive computational module. The node transformation is as follows: e k + b(i) ), ik = σ(W(i) vk + U(i) h fk,j = σ(W(f ) vk + U(f ) hj + b(f ) ), e k + b(o) ), ok = σ(W(o) vk + U(o) h e k + b(u) ), uk = tanh(W (u) vk + U (u) h X ck = ik uk + fk,j cj , (2) j hk = ok tanh(ck ), where hk and ck represent the hidden state and memory cell state of node k respectively. To induce label correlations, HiAGM employs a bidirectional Tree-LSTM by the fusion of a childsum and a"
2020.acl-main.104,N16-1000,0,0.191705,"Missing"
2020.acl-main.273,W18-6402,0,0.192307,"n multi-modal NMT. 4 Related Work Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT. Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships"
2020.acl-main.273,P18-1026,0,0.039539,"Missing"
2020.acl-main.273,W17-4746,0,0.343479,"Missing"
2020.acl-main.273,N19-1422,0,0.199371,"model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. Note that if we directly adapt the approach proposed by Huang et al. (2016) into Transformer, the model (O"
2020.acl-main.273,D17-1105,0,0.0280843,"boarding ramp” is not translated correctly by all baselines, while our model correctly translates it. This reveals that our encoder is able to learn more accurate representations. 3.6 Results on the En⇒Fr Translation Task We also conduct experiments on the EN⇒Fr dataset. From Table 3, our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT. 4 Related Work Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT. Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-m"
2020.acl-main.273,P17-1175,0,0.136326,"Missing"
2020.acl-main.273,P19-1642,0,0.690464,"translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019). Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance. To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens (Huang et al., 2016; Calixto et al., 2017), or to learn the joint multi-modal representation (Zhou et al., 2018; Calixto et al., 2019); (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context (Huang et al., 2016; Ive et al., 2019); and (3) representing each image as spatial features, which can be exploited as extra context (Calixto et al., 2017; Delbrouck and Dupont, 2017a; Ive et al., 2019), or a supplement to source semantics (Delbrouck and Dupont, 2017b) via an attention mechanism. Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image"
2020.acl-main.273,D17-1095,0,0.143417,"Missing"
2020.acl-main.273,W14-3348,0,0.106532,"el tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En⇒De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has Ld =4 layers4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017). Finally, we use the metrics BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. 40.4 BLEU 3.1 39.6 39.2 http://www.statmt.org/wmt18/multimodal-task.html There is no parsing failure for this dataset. If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer. 4 The encoder of the text-based Transformer also has 4 layers. 1 2 3 Le 4 5 Figure 3: Results on the En⇒De validation set regarding the number Le of graph-based multi-modal fusion layers. Baseline Models"
2020.acl-main.273,D18-1329,0,0.234241,"into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. No"
2020.acl-main.273,W16-3210,0,0.251902,"Missing"
2020.acl-main.273,I17-1014,0,0.363334,"Missing"
2020.acl-main.273,P18-1150,0,0.107975,"rge-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in (Tan and Bansal, 2019). Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3032 Source: A boy riding a skateboard on a skateboarding ramp . Reference: Ein junge fährt skateboard auf einer skateboardrampe . Tranformer: Ein junge fährt auf einem skateboard auf einer rampe . Doubly-att(TF): Ein junge fährt mit einem skateboard auf einer rampe . Enc-att(TF): Ein junge fährt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge fährt auf einem skateboard auf einer rampe . Our model: Ein junge fährt auf einem skateboard auf einer skateboardram"
2020.acl-main.273,W18-6441,0,0.530768,"Missing"
2020.acl-main.273,W16-2360,0,0.23828,"ce and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model. 1 Introduction Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018). It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based ∗ This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. †"
2020.acl-main.273,P19-1653,0,0.649723,"Missing"
2020.acl-main.273,P16-1162,0,0.0622395,"dal English⇒German (En⇒De) and English⇒French (En⇒Fr) translation tasks. 3028 40.8 Setup Datasets We use the Multi30K dataset (Elliott et al., 2016), where each image is paired with one English description and human translations into German and French. Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively. In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively. Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding (Sennrich et al., 2016) with 10,000 merge operations. Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by Yang et al. (2019) to detect associated visual objects of the identified noun phrases. For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects. In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively. 3 Finally, we compute 2,048-dimensional features for these objects with the pre-tra"
2020.acl-main.273,Q19-1002,1,0.876509,"Missing"
2020.acl-main.273,D19-1514,0,0.177044,"r” semantically corresponds to the blue dashed region. The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation. However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions (Lee et al., 2018; Tan and Bansal, 2019). 3025 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3025–3035 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Multi-modal Graph Image ??? ??? Text Two boys are playing with a toy car ??? ??? ??? ??? ??? ??? ??? Two boys are playing with a ??? ??? toy car Figure 1: The multi-modal graph for an input sentence-image pair. The blue and green solid circles denote textual nodes and visual nodes respectively. An intra-modal edge (dotted line) connects two nodes in the same modality, and an inter-modal edge (solid line) links two no"
2020.acl-main.273,P02-1040,0,0.110641,"corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En⇒De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has Ld =4 layers4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017). Finally, we use the metrics BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. 40.4 BLEU 3.1 39.6 39.2 http://www.statmt.org/wmt18/multimodal-task.html There is no parsing failure for this dataset. If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer. 4 The encoder of the text-based Transformer also has 4 layers. 1 2 3 Le 4 5 Figure 3: Results on the En⇒De validation set regarding the number Le of graph-based mul"
2020.acl-main.273,P18-1030,0,0.0280822,"respondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in (Tan and Bansal, 2019). Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3032 Source: A boy riding a skateboard on a skateboarding ramp . Reference: Ein junge fährt skateboard auf einer skateboardrampe . Tranformer: Ein junge fährt auf einem skateboard auf einer rampe . Doubly-att(TF): Ein junge fährt mit einem skateboard auf einer rampe . Enc-att(TF): Ein junge fährt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge fährt auf einem skateboard auf einer rampe . Our model: Ein junge fährt auf einem skateboard auf einer skateboardrampe . Figure 6: A translation example of different multi-modal NMT models. The baseline mode"
2020.acl-main.273,D18-1400,0,0.102639,"ctions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model. 1 Introduction Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018). It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based ∗ This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Corresponding author. translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019). Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model p"
2020.acl-main.277,1999.tmi-1.16,0,0.13791,"Missing"
2020.acl-main.277,D16-1139,0,0.0567024,"ransformer model (dmodel = 278, dhidden = 507, nlayer = 5, nhead = 2, pdropout = 0.1). For the WMT14 En-De and WMT16 EnRo datasets, we use a larger Transformer model (dmodel = 512, dhidden = 512, nlayer = 6, nhead = 8, pdropout = 0.1). We linearly anneal the learning rate from 3 × 10−4 to 10−5 as in Lee et al. (2018) for the IWSLT16 En-De dataset, while employing the warm-up learning rate schedule (Vaswani et al., 2017) with twarmup = 4000 for the WMT14 En-De and WMT16 En-Ro datasets. We also use label smoothing of value ls = 0.15 for all datasets. We utilize the sequence-level distillation (Kim and Rush, 2016), which replaces the target sentences in the training dataset with sentences generated by an autoregressive model, and set the beam size of the technique to 4. We use the encoder of the corresponding autoregressive model to initialize the encoder of RecoverSAT, and share the parameters of source and target token embedding layers and the pre-softmax linear layer. We measure the speedup of model inference in each task on a single NVIDIA P40 GPU with the batch size 1. 4.3 Baselines We use the Transformer (Vaswani et al., 2017) as our AT baseline and fifteen latest strong NAT models as NAT baselin"
2020.acl-main.277,D18-1149,0,0.581672,"1 Segment 2 Segment 3 Segment 4 Figure 1: An overview of our RecoverSAT model. RecoverSAT generates a translation as a sequence of segments. The segments are generated simultaneously while each segment is generated token-by-token conditioned on both the source tokens and the translation history of all segments (e.g., the token “are” in the first segment is predicted based on all the tokens colored green). Repetitive segments (e.g., the third segment “lots of”) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the trai"
2020.acl-main.277,D19-1573,0,0.110758,"Missing"
2020.acl-main.277,D19-1437,0,0.369128,"eration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the training process with an autoregressive model (Li et al., 2019; Wei et al., 2019), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. To alleviate the multi-modality problem while maintaining a reasonable decoding speedup, we propose a novel semi-autoregressive model named RecoverSAT in this work. RecoverSAT features in three aspects: (1) To improve decoding speed, we assume that a translation can be divided into several segments which can be genera"
2020.acl-main.277,P16-1162,0,0.0996738,"aining instances will mislead the model that generating then deleting a repetitive segment is a must-to-have behaviour, which is not desired. Therefore, we inject pseudo repetitive segment into a training instance with probability q in this work. 4 4.1 Experiments Datasets We conduct experiments on three widely-used machine translation datasets: IWSLT16 En-De (196k pairs), WMT14 En-De (4.5M pairs) and WMT16 En-Ro (610k pairs). For fair comparison, we use the preprocessed datasets in Lee et al. (2018), of which sentences are tokenized and segmented into subwords using byte-pair encoding (BPE) (Sennrich et al., 2016) to restrict the vocabulary size. We use a shared vocabulary of 40k subwords for both source and target languages. For the WMT14 En-De dataset, we use newstest-2013 and newstest2014 as validation and test sets respectively. For the WMT16 En-Ro dataset, we employ newsdev2016 and newstest-2016 as validation and test sets respectively. For the IWSLT16 En-De dataset, we use test2013 as the validation set. 4.2 Experimental Settings For model hyperparameters, we follow most of the settings in (Gu et al., 2018; Lee et al., 2018; Wei et al., 2019). For the IWSLT16 En-De dataset, we use a small Transfo"
2020.acl-main.277,P19-1288,1,0.902957,"ased on all the tokens colored green). Repetitive segments (e.g., the third segment “lots of”) are detected and deleted automatically. iteratively by taking both the source sentence and the translation of last iteration as input (Lee et al., 2018; Ghazvininejad et al., 2019). Nevertheless, it requires to refine the translations for multiple times in order to achieve better translation quality, which hurts decoding speed significantly. The other line of work tries to improve the vanilla NAT model to better capture target-side dependency by leveraging extra autoregressive layers in the decoder (Shao et al., 2019a; Wang et al., 2018), introducing latent variables and/or more powerful probabilistic frameworks to model more complex distributions (Kaiser et al., 2018; Akoury et al., 2019; Shu et al., 2019; Ma et al., 2019), guiding the training process with an autoregressive model (Li et al., 2019; Wei et al., 2019), etc. However, these models cannot alter a target token once it has been generated, which means these models are not able to recover from an error caused by the multi-modality problem. To alleviate the multi-modality problem while maintaining a reasonable decoding speedup, we propose a novel"
2020.acl-main.277,D18-1044,0,0.0823734,"Missing"
2020.acl-main.277,P19-1125,0,0.174125,"e corresponding autoregressive model. 1 Although neural machine translation (NMT) has achieved state-of-the-art performance in recent years (Cho et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), most NMT models still suffer from the slow decoding speed problem due to their autoregressive property: the generation of a target token depends on all the previously generated target tokens, making the decoding process intrinsically nonparallelizable. Recently, non-autoregressive neural machine translation (NAT) models (Gu et al., 2018; Li et al., 2019; Wang et al., 2019; Guo et al., 2019a; Wei et al., 2019) have been investigated to mitigate the † indicates equal contribution indicates corresponding author es gibt heute viele Farmer mit diesem Ansatz Feasible Trans. there are lots of farmers doing this today there are a lot of farmers doing this today Trans. 1 Trans. 2 there are lots of of farmers doing this today there are a lot farmers doing this today Table 1: A multi-modality problem example: NAT models generate each target token independently such that they may correspond to different feasible translations, which usually manifests as repetitive (Trans. 1) or missing (Trans. 2) tokens. Intro"
2020.acl-main.28,D17-1098,0,0.0259676,"y, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we present our novel UPSA framework that uses simulated annealing (SA) for unsupervised paraphrasing. In particular, we first present the general SA algorithm and then design our searching objective and searching actions (i.e., candidate sentence generator) fo"
2020.acl-main.28,P19-1602,1,0.921322,"s a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 2016; Bao et al., 2019) or directly in the word space (Miao et al., 2019). However, the meaning preservation and expression diversity of those generated paraphrases are less “controllable” in such probabilistic sampling procedures. To this end, we propose a novel approach to Unsupervised Paraphrasing by Simulated Annealing (UPSA). Simulated annealing (SA) is a stochastic searching algorithm towards an objective function, which can be flexibly defined. In our work, we design a sophisticated objective function, considering semantic preservation, expression diversity, and language fluency of paraphrases. SA searches to"
2020.acl-main.28,N03-1003,0,0.228491,"14 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Compu"
2020.acl-main.28,K16-1002,0,0.740759,"(Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 2016; Bao et al., 2019) or directly in the word space (Miao et al., 2019). However, the meaning preservation and expression diversity of those generated paraphrases are less “controllable” in such probabilistic sampling procedures. To this end, we propose a novel approach to Unsupervised Paraphrasing by Simulated Annealing (UPSA). Simulated annealing (SA) is a stochastic searching algorithm towards an objective function, which can be flexibly defined. In our work, we design a sophisticated objective function, considering semantic preservation, expression diversity, and language fluency of paraphra"
2020.acl-main.28,C04-1051,0,0.266948,"annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the ge"
2020.acl-main.28,D17-1158,0,0.0234271,"ed Seq2Seq paraphrase generators: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeli"
2020.acl-main.28,P19-1331,0,0.0459871,"asing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only semantic similarity and language fluency, but also the expression diversity; we further propose a copy mechanism in our searching process. Recently, a few studies have applied editingbased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-bes"
2020.acl-main.28,W07-1424,0,0.0599355,"ersity between a paraphrase and the input. • We propose a copy mechanism as one of our search actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample s"
2020.acl-main.28,P13-1158,0,0.0824825,"Missing"
2020.acl-main.28,P16-1154,0,0.0272601,"eplace = top- Kw∗ p− LM i − (w∗ , wt,k+1 , . . . , wt,lt ) . p← LM 4 (9) 4.1 For word insertion, the top-K vocabulary Wt,insert is computed in a similar way (except that the position of w∗ is slightly different). Details are not repeated. In our experiments, K is set to 50. Copy Mechanism. We observe that name entities and rare words are sometimes deleted or replaced during SA stochastic sampling. They are difficult to be recovered because they usually have a low language model-suggested probability. Therefore, we propose a copy mechanism for SA sampling, inspired by that in Seq2Seq learning (Gu et al., 2016). Specifically, we allow the candidate sentence generator to copy the words from the original sentence x0 for word replacement and insertion. This is essentially enlarging the top-K sampling vocabulary with the words in x0 , given by ft,op = Wt,op ∪ {w0,1 , . . . , w0,l } W 0 (10) ft,op is the where op ∈ {replace,insert}. Thus, W actual vocabulary from which SA samples the word w∗ for replacement and insertion operation. While such vocabulary reduces the proposal space, it works well empirically because other low-ranked candidate words are either irrelevant or make the sentence disfluent; they"
2020.acl-main.28,Q18-1031,0,0.0349776,"19) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only semantic similarity and language fluency, but also the expression diversity; we further propose a copy mechanism in our searching process. Recently, a few studies have applied editingbased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in f"
2020.acl-main.28,2020.acl-main.707,1,0.78785,"ence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we p"
2020.acl-main.28,D17-1126,0,0.0410463,"Missing"
2020.acl-main.28,D18-1421,0,0.0287354,"an-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 302–312 c July 5 - 10, 2020. 2020 Association for Computational Linguistics needed. With the help of deep learning, researchers are able to generate paraphrases by sampling from a neural network-defined probabilistic distribution, either in a continuous latent space (Bowman et al., 201"
2020.acl-main.28,J83-1001,0,0.644195,"s of editing operations (i.e., insertion, replacement, and deletion). At each step, UPSA proposes a candidate modification of the sentence, which is accepted or rejected according to a certain acceptance rate (only accepted modifications are shown). Although sentences are discrete, we make an analogue in the continuous real x-axis where the distance of two sentences is roughly given by the number of edits. Introduction Paraphrasing aims to restate one sentence as another with the same meaning, but different wordings. It constitutes a corner stone in many NLP tasks, such as question answering (Mckeown, 1983), information retrieval (Knight and Marcu, 2000), and dialogue systems (Shah et al., 2018). However, automatically generating accurate and different-appearing paraphrases is a still challenging research problem, due to the complexity of natural language. Conventional approaches (Prakash et al., 2016; Gupta et al., 2018) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. Usually, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentenc"
2020.acl-main.28,W16-6625,0,0.0225211,"and the input. • We propose a copy mechanism as one of our search actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned"
2020.acl-main.28,W14-3316,0,0.0682153,"Missing"
2020.acl-main.28,P02-1040,0,0.114331,"oses a candimultiplicative. date word w∗ for the kth step, the resulting canExpression Diversity. The expression diversity didate sentence becomes x∗ = (wt,1 , . . . , wt,k−1 , scoring function computes the lexical difference of w∗ , wt,k+1 . . . , wt,lt ). The insertion operation two sentences. We adopt a BLEU-induced function works similarly. to penalize the repetition of the words and phrases Here, the candidate word is sampled from a probin the input sentence: abilistic distribution, induced by the objective function (2): fexp (x∗ , x0 ) = (1 − BLEU(x∗ , x0 ))S , (5) where the BLEU score (Papineni et al., 2002) computes a length-penalized geometric mean of n-gram precision (n = 1, · · · , 4). S coordinates the importance of fexp (xt , x0 ) in the objective function (2). Language Fluency. Despite semantic preservation and expression diversity, the candidate paraphrase should be a fluent sentence by itself. We use a separately trained (forward) language model −→ (denoted as LM) to compute the likelihood of the candidate paraphrase as our fluency scoring function: fflu (x∗ ) = k=l Y∗ → (w∗,k |w∗,1 , . . . , w∗,k−1 ), (6) p− LM k=1 where l∗ is the length of x∗ and w∗,1 , . . . , w∗,l are words of x∗ . H"
2020.acl-main.28,D14-1162,0,0.0841998,"cluding semantic preservation fsem , expression diversity fexp , and language fluency fflu . Thus, our searching objective is to maximize f (x) = fsem (x, x0 ) · fexp (x, x0 ) · fflu (x), (2) where x0 is the input sentence. Semantic Preservation. A paraphrase is expected to capture all the key semantics of the original sentence. Thus, we leverage the cosine function of keyword embeddings to measure if the key focus of the candidate paraphrase is the same as the input. Specifically, we extract the keywords of the input sentence x0 by the Rake system (Rose et al., 2010) and embed them by GloVE (Pennington et al., 2014). For each keyword, we find the closest word in the candidate paraphrase x∗ in terms of the cosine similarity. Our keyword-based semantic preservation score is given by the lowest cosine similarity among all the keywords, i.e., the least matched keyword: fsem,key (x∗ , x0 ) = If the proposal is accepted, xt+1 = x∗ , or otherwise, xt+1 = xt . 304 min max{cos(w∗,j , e)}, e∈keywords(x0 ) j (3) where w∗,j is the jth word in the sentence x∗ ; e is an extracted keyword of x0 . Bold letters indicate embedding vectors. In addition to keyword embeddings, we also adopt a sentence-level similarity functi"
2020.acl-main.28,C16-1275,0,0.518552,"analogue in the continuous real x-axis where the distance of two sentences is roughly given by the number of edits. Introduction Paraphrasing aims to restate one sentence as another with the same meaning, but different wordings. It constitutes a corner stone in many NLP tasks, such as question answering (Mckeown, 1983), information retrieval (Knight and Marcu, 2000), and dialogue systems (Shah et al., 2018). However, automatically generating accurate and different-appearing paraphrases is a still challenging research problem, due to the complexity of natural language. Conventional approaches (Prakash et al., 2016; Gupta et al., 2018) model the paraphrase generation as a supervised encoding-decoding problem, inspired by machine translation systems. Usually, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domai"
2020.acl-main.28,W04-3219,0,0.160841,"actions of simulated annealing to address rare words. • We achieve the state-of-the-art performance on four benchmark datasets compared with previous unsupervised paraphrase generators, largely reducing the performance gap between unsupervised and supervised paraphrasing. We outperform most domain-adapted paraphrase generators, and even a supervised one on the Wikianswers dataset. 2 Related Work In early years, paraphrasing was typically accomplished by exploiting linguistic knowledge (Mckeown, 1983; Ellsworth and Janin, 2007; Narayan et al., 2016) and statistical machine translation methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et a"
2020.acl-main.28,P19-1332,0,0.592499,"lly, such models require massive parallel samples for training. In machine translation, for example, the WMT 2014 English-German dataset contains 4.5M sentence pairs (Neidert et al., 2014). However, the training corpora for paraphrasing are usually small. The widely-used Quora dataset2 only contains 140K pairs of paraphrases; constructing such human-written paraphrase pairs is expensive and labor-intensive. Further, existing paraphrase datasets are domain-specific: the Quora dataset only contains question sentences, and thus, supervised paraphrase models do not generalize well to new domains (Li et al., 2019). On the other hand, researchers synthesize pseudo-paraphrase pairs by clustering news events (Barzilay and Lee, 2003), crawling tweets of the same topic (Lan et al., 2017), or translating bi-lingual datasets (Wieting and Gimpel, 2017), but these methods typically yield noisy training sets, leading to low paraphrasing performance (Li et al., 2018). As a result, unsupervised methods would largely benefit paraphrase generation as no parallel data are 1 Code and data available at: https://github.com/ Liuxg16/UPSA 2 https://www.kaggle.com/c/quora-question-pairs 302 Proceedings of the 58th Annual M"
2020.acl-main.28,W04-1013,0,0.0290802,"-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeling (forward and backward), realized by two-layer LSTM with 30"
2020.acl-main.28,P19-1605,0,0.0197168,"upervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the generated sentences are less controllable and suffer from the error accumulation problem in VAE’s decoding phase (Miao et al., 2019). Roy and Grangier (2019) introduce an unsupervised model based on vector-quantized autoencoders (Van den Oord et al., 2017). But their work mainly focuses on generating sentences for data augmentation instead of paraphrasing itself. Miao et al. (2019) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work and ours is that UPSA imposes the annealing temperature into the sampling process for better convergence to an optimum. In addition, we define our searching objective involving not only"
2020.acl-main.28,2020.acl-main.452,1,0.877759,"ased approaches to sentence generation. Guu et al. (2018) propose a heuristic delete-retrieve-generate component for a supervised sequence-to-sequence 303 (Seq2Seq) model. Dong et al. (2019) learn the deletion and insertion operations for text simplification in a supervised way, where their groundtruth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach"
2020.acl-main.28,P17-1099,0,0.0403138,"eported to be the state-of-the-art VAE. We adopted the published source code and generated paraphrases for comparison. CGMH. Miao et al. (2019) use Metropolis– Hastings sampling in the word space for constrained sentence generation. It is shown to outperform latent space sampling as in VAE, and is the state-of-the-art unsupervised paraphrasing approach. We also adopted the published source code and generated paraphrases for comparison. We further compare UPSA with supervised Seq2Seq paraphrase generators: ResidualLSTM (Prakash et al., 2016), VAE-SVG-eq (Gupta et al., 2018), Pointer-generator (See et al., 2017), the Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performan"
2020.acl-main.28,N18-3006,0,0.0495463,"Missing"
2020.acl-main.28,P12-2008,0,0.229831,"e Transformer (Vaswani et al., 2017), and the decomposable neural paraphrase generator (DNPG, Li et al., 2019). DNPG has been reported as the state-of-the-art supervised paraphrase generator. To better compare UPSA with all paraphrasing settings, we also include domain-adapted supervised paraphrase generators that are trained in a source domain but tested in a target domain, including shallow fusion (Gulcehre et al., 2015) and multitask learning (MTL, Domhan and Hieber 2017). We adopt BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) scores as automatic metrics to evaluate model performance. Sun and Zhou (2012) observe that BLEU and ROUGE could not measure the diversity between the generated and the original sentences, and propose the iBLEU variant by penalizing by the similarity with the original sentence. Therefore, we regard the iBLEU score as our major metric, which is also adopted in Li et al. (2019). In addition, we also conduct human evaluation in our experiments (detailed later). 4.3 Implementation Details Our method involves unsupervised language modeling (forward and backward), realized by two-layer LSTM with 300 hidden units and trained specifically on each dataset with non-parallel sente"
2020.acl-main.28,P19-1199,0,0.0870125,"methods (Quirk et al., 2004; Dolan et al., 2004). Recently, deep neural networks have become a prevailing approach to text generation, where paraphrasing is often formulated as a supervised encodingdecoding problem, for example, using stacked residual LSTM (Prakash et al., 2016) and the Transformer model (Wang et al., 2019). Unsupervised paraphrasing is an emerging research direction in the field of NLP. The variational autoencoder (VAE) can be intuitively applied to paraphrase generation in an unsupervised fashion, as we can sample sentences from a learned latent space (Bowman et al., 2016; Zhang et al., 2019; Bao et al., 2019). But the generated sentences are less controllable and suffer from the error accumulation problem in VAE’s decoding phase (Miao et al., 2019). Roy and Grangier (2019) introduce an unsupervised model based on vector-quantized autoencoders (Van den Oord et al., 2017). But their work mainly focuses on generating sentences for data augmentation instead of paraphrasing itself. Miao et al. (2019) use Metropolis–Hastings sampling (1953) for constrained sentence generation, achieving the state-of-the-art unsupervised paraphrasing performance. The main difference between their work"
2020.acl-main.28,P19-1503,0,0.0213944,"uth operations are obtained by some dynamic programming algorithm. Our editing operations (insertion, deletion, and replacement) are the search actions of unsupervised simulated annealing. Regarding discrete optimization/searching, a na¨ıve approach is by hill climbing (Edelkamp and Schroedl, 2011; Schumann et al., 2020; Kumar et al., 2020), which is in fact a greedy algorithm. In NLP, beam search (BS, Tillmann et al. 1997) is widely applied to sentence generation. BS maintains a k-best list in a partially greedy fashion during left-to-right (or right-to-left) decoding (Anderson et al., 2017; Zhou and Rush, 2019). By contrast, UPSA is local search with distributed edits over the entire sentence. Moreover, UPSA is able to make use of the original sentence as an initial state of searching, whereas BS usually works in the decoder of a Seq2Seq model and is not applicable to unsupervised paraphrasing. 3 Approach In this section, we present our novel UPSA framework that uses simulated annealing (SA) for unsupervised paraphrasing. In particular, we first present the general SA algorithm and then design our searching objective and searching actions (i.e., candidate sentence generator) for paraphrasing. 3.1 Th"
2020.acl-main.563,N19-1423,0,0.0940548,"Missing"
2020.acl-main.563,P19-1546,0,0.408049,"´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b) or only utilize partial history (Ren et al., 2019; Kim et al., 2019; Sharma et al., 2019) or lack direct interactions between slots and history (Ren et al., 2018; Lee et al., 2019; Goel et al., 2019). Briefly, these methods are deficient in exploiting relevant context from dialogue history. 6322 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6322–6333 c July 5 - 10, 2020. 2020 Association for Computational Linguistics This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. adjusting the weight of each slot. To the best of our knowledge, our method is the first to address the slot imbalance problem in DST. • Experimental results show that our model achieves state-of-the-art performan"
2020.acl-main.563,P18-1133,0,0.0934332,"redictions while SUMBT fails. guage understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. Recent neural network models are proposed for further improvements (Mrkˇsi´c et al., 2015; Hori et al., 2016; Mrkˇsi´c et al., 2017; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Wu et al., 2019; Ren et al., 2019; Balaraman and Magnini, 2019). Ren et al. (2018) and Lee et al. (2019) use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly. Sharma et al. (2019) employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner. Goel et al. (2019) encode the dialogue history into a hidden state and then simply combine it with the slot to make decision"
2020.acl-main.563,N19-1057,0,0.207776,"rs accomplish tasks through spoken interactions (Young, 2002; Young et al., 2013; Gao et al., 2019a). Dialogue state tracking (DST) is an essential part of dialogue management in task-oriented dialogue systems. Given current utterances and dialogue history, DST aims to determine the set of † Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. ∗ Yang Feng is the corresponding author. 1 Code is available at https://github.com/ictnlp/CHAN-DST As Table 1 shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies (Sharma et al., 2019; Wu et al., 2019). However, traditional DST models usually determine dialogue states by considering only utterances at current turn (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b"
2020.acl-main.563,P18-1158,0,0.0295864,"estination restaurant-food restaurant-book-people hotel-stars attraction-area hotel-price-range hotel-type attraction-type hotel-area restaurant-price-range restaurant-area train-leave-at hotel-internet hotel-parking hotel-name hotel-book-stay hotel-book-people hotel-book-day restaurant-book-time restaurant-book-day -0.2 taxi-arrive-by taxi-leave-at taxi-departure taxi-destination attraction-name train-book-people restaurant-name train-arrive-by -0.1 Percentage 6329 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. et al., 2016; Ying et al., 2018; Wang et al., 2018; Xing et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019). Conclusion Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900). References Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019a. Neural approaches to conversational ai. FoundaR in Information Retrieval, 13(2tions and Trends 3):127–298. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, Dilek Hakkani-Tur, and Amazon Alexa AI. 2019b. Dialog state tracking: A neural reading comprehension appr"
2020.acl-main.563,P19-1543,0,0.0185449,"-price-range hotel-type attraction-type hotel-area restaurant-price-range restaurant-area train-leave-at hotel-internet hotel-parking hotel-name hotel-book-stay hotel-book-people hotel-book-day restaurant-book-time restaurant-book-day -0.2 taxi-arrive-by taxi-leave-at taxi-departure taxi-destination attraction-name train-book-people restaurant-name train-arrive-by -0.1 Percentage 6329 This paper was retracted. For more information, see https://aclanthology.org/2020.acl-main.563. et al., 2016; Ying et al., 2018; Wang et al., 2018; Xing et al., 2018; Aujogue and Aussem, 2019; Naik et al., 2018; Liu and Chen, 2019). Conclusion Acknowledgments We thank the anonymous reviewers for their insightful comments. This work was supported by National Key R&D Program of China (NO. 2017YFE0192900). References Jianfeng Gao, Michel Galley, Lihong Li, et al. 2019a. Neural approaches to conversational ai. FoundaR in Information Retrieval, 13(2tions and Trends 3):127–298. Shuyang Gao, Abhishek Sethi, Sanchit Agarwal, Tagyoung Chung, Dilek Hakkani-Tur, and Amazon Alexa AI. 2019b. Dialog state tracking: A neural reading comprehension approach. In 20th Annual Meeting of the Special Interest Group on Discourse and Dialogue,"
2020.acl-main.563,D18-1299,0,0.0611422,"Missing"
2020.acl-main.563,P15-2130,0,0.0471484,"Missing"
2020.acl-main.563,P17-1163,0,0.102841,"Missing"
2020.acl-main.563,D19-1196,0,0.612489,"Missing"
2020.acl-main.563,W13-4067,0,0.165105,"Missing"
2020.acl-main.563,E17-1042,0,0.108691,"Missing"
2020.acl-main.563,W13-4065,0,0.225425,"Missing"
2020.acl-main.563,P19-1078,0,0.472121,"Missing"
2020.acl-main.563,N16-1174,0,0.0826114,"Missing"
2020.acl-main.563,N16-1000,0,0.223513,"Missing"
2020.acl-main.563,P18-1134,0,0.104761,"UMBT fails. guage understanding modules to predict the current dialogue state (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014) or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology, and they are hard to extend and scale to new domains. Recent neural network models are proposed for further improvements (Mrkˇsi´c et al., 2015; Hori et al., 2016; Mrkˇsi´c et al., 2017; Lei et al., 2018; Xu and Hu, 2018; Zhong et al., 2018; Nouri and Hosseini-Asl, 2018; Wu et al., 2019; Ren et al., 2019; Balaraman and Magnini, 2019). Ren et al. (2018) and Lee et al. (2019) use an RNN to encode the slot-related information of each turn, where slots can not attend to relevant information of past turns directly. Sharma et al. (2019) employ a heuristic rule to extract partial dialogue history and then integrate the historical information into prediction in a coarse manner. Goel et al. (2019) encode the dialogue history into a hidden state and then simply combine it with the slot to make decisions. These models a"
2020.acl-main.563,P18-1135,0,0.22343,"systems. Given current utterances and dialogue history, DST aims to determine the set of † Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. ∗ Yang Feng is the corresponding author. 1 Code is available at https://github.com/ictnlp/CHAN-DST As Table 1 shows, the dialogue state is usually dependent on relevant context in the dialogue history, which is proven in previous studies (Sharma et al., 2019; Wu et al., 2019). However, traditional DST models usually determine dialogue states by considering only utterances at current turn (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018; Chao and Lane, 2019) which neglects the use of dialogue history. Recent researches attempt to address this problem through introducing historical dialogue information into the prediction of slot-value pairs. Most of them leverage a naive attention between slots and concatenated historical utterances (Wu et al., 2019; Zhou and Small, 2019; Gao et al., 2019b; Zhang et al., 2019; Le et al., 2020a,b) or only utilize partial history (Ren et al., 2019; Kim et al., 2019; Sharma et al., 2019) or lack direct interactions between slots and history (Ren et al., 2018; Lee et al., 2019; Goel et al., 2019"
2020.acl-main.573,P15-1034,0,0.019032,"pidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012"
2020.acl-main.573,P19-1279,0,0.0403217,"old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern ext"
2020.acl-main.573,D11-1142,0,0.0492626,"fined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. F"
2020.acl-main.573,P07-1073,0,0.0684863,"ther experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two i"
2020.acl-main.573,P18-2065,0,0.0197777,"ver all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learnin"
2020.acl-main.573,N19-1423,0,0.0290448,"hus add special tokens into the tokenized tokens to indicate the beginning and ending positions of those entities. For simplicity, we denote such an example encoding operation as the following equation, x = f (x), (1) where x ∈ Rd is the semantic embedding of x, and d is the embedding dimension. Note that the encoder is not our focus in this paper, we select bidirectional long short-term memory (BiLSTM) (Bengio et al., 1994) as representative encoders to encode examples. In fact, other neural text encoders like convolutional neural networks (Zeng et al., 2014) and pre-trained language models (Devlin et al., 2019) can also be adopted as example encoders. 3.3 Learning for New Tasks When the k-th task is arising, the example encoder has not touched any examples of new relations before, and cannot extract the semantic features of them. Hence, we first fine-tune the example Tk )} to encoder on Tk = {(xT1 k , y1Tk ), . . . , (xTNk , yN grasp new relation patterns in Rk . The loss function of learning the k-th task is as follows, ˜ L(θ) = − Rk | N |X X i=1 j=1 log P δyTk =r × j i exp(g(f (xTi k ), rj )) ˜k| |R Tk l=1 exp(g(f (xi ), rl )) (2) , where rj is the embedding of the j-th relation ˜ k in the all kno"
2020.acl-main.573,P15-1026,0,0.0299963,"ation exercise to keep a stable understanding of old relations. The experimental results show that EMAR could get rid of catastrophically forgetting old relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patt"
2020.acl-main.573,D15-1205,0,0.115988,"Missing"
2020.acl-main.573,D18-1247,1,0.811523,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,D18-1514,1,0.884,"old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation"
2020.acl-main.573,P11-1055,0,0.0562083,"eness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation le"
2020.acl-main.573,P16-1200,1,0.933526,"the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale textual corpora; (2"
2020.acl-main.573,W15-1506,0,0.039566,"MAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to de"
2020.acl-main.573,P06-1015,0,0.060577,"relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li and Hoiem, 2017; Liu et al., 2018; Ritter et al., 2018) which consolidate the model parameters i"
2020.acl-main.573,P15-2047,0,0.0194688,"gnificantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets."
2020.acl-main.573,D15-1204,0,0.0130382,"t is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning,"
2020.acl-main.573,Q16-1017,0,0.0168298,"ntion to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research direct"
2020.acl-main.573,D12-1048,0,0.0436977,"text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual le"
2020.acl-main.573,P09-1113,0,0.293741,"ses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open"
2020.acl-main.573,P16-1105,0,0.0195004,"forms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before,"
2020.acl-main.573,N13-1008,0,0.0615208,"d relations and outperform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation type"
2020.acl-main.573,P15-1061,0,0.0223495,"catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations"
2020.acl-main.573,D11-1135,0,0.0368578,"ers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods"
2020.acl-main.573,N06-1039,0,0.105623,", in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatri"
2020.acl-main.573,D12-1110,0,0.0747449,"iments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attentio"
2020.acl-main.573,D15-1203,0,0.0411191,"that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have"
2020.acl-main.573,D16-1252,0,0.01481,"in models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first e"
2020.acl-main.573,C14-1220,0,0.768848,"rform the state-of-the-art continual learning models. The code and datasets are released on https://github.com/thunlp/ ContinualRE. 1 Introduction Relation extraction aims at detecting relations between entities from text, e.g., extracting the relation “the president of ” from the given sentence “Newton served as the president of the Royal Society”, which could serve as external resource for various downstream applications (Dong et al., 2015; Xiong et al., 2017; Schlichtkrull et al., ∗ † indicates equal contribution Corresponding author 2018). The conventional RE methods (Riedel et al., 2013; Zeng et al., 2014; Lin et al., 2016) mostly focus on recognizing relations for a fixed pre-defined relation set, and cannot handle rapidly emerging novel relations in the real world. Some researchers therefore explore to detect and learn incessantly emerging relations in an open scenario. As shown in Figure 1, their efforts can be formulated into a two-step pipeline: (1) Open Relation Learning extracts phrases and arguments to construct patterns of specific relations, and then discovers unseen relation types by clustering patterns, and finally expands sufficient examples of new relation types from large-scale"
2020.acl-main.573,D17-1004,0,0.0606583,"Missing"
2020.acl-main.573,N19-1086,0,0.330723,"set. Although continual relation learning is vital for learning emerging relations, there are rare explorations for this field. A straightforward solution is to store all historical data and re-train models every time new relations and examples come in. Nevertheless, it is computationally expensive since relations are in sustainable growth. Moreover, the huge example number of each relation makes frequently mixing new and old examples become infeasible in the real world. Therefore, storing all data is not practical in continual relation learning. In view of this, the recent preliminary work (Wang et al., 2019) indicates that the main challenge of continual relation learning is the catastrophic forgetting problem, i.e., it is hard to learn new relations and meanwhile avoid forgetting old relations, considering memorizing all the data is almost impossible. 6429 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6429–6440 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Open Relation Learning Continual Relation Learning David Bowie was born in 8th Jan. 1947. Learn Date of Birth Date of Birth Detect New Relations … Data for Date of Birth Hi"
2020.acl-main.573,P05-1053,0,0.180337,"n prototypes. We conduct sufficient experiments on several RE datasets, and the results show that EMAR effectively alleviates the catastrophic forgetting problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations."
2020.acl-main.573,D19-1021,1,0.842706,"pre-defined relation sets. As we introduced before, learning incessantly emerging relations consists of two important steps: open relation learning and continual relation learning. There have been many efforts for open relation learning, including pattern extraction (Banko et al., 2007; Fader et al., 2011; Mausam et al., 2012; Del Corro and Gemulla, 2013; Angeli et al., 2015; Petroni et al., 2015; Stanovsky and Dagan, 2016; Mausam, 2016; Cui et al., 2018), relation discovery (Yao et al., 2011; Marcheggiani and Titov, 2016), relation clustering (Shinyama and Sekine, 2006; Elsahar et al., 2017; Wu et al., 2019), and data collection (Riloff et al., 1999; Etzioni et al., 2005; Pantel and Pennacchiotti, 2006; Rozenfeld and Feldman, 2008; Nakashole et al., 2011; Zhu et al., 2009; Gao et al., 2020). However, for continual relation learning, there are still only some preliminary explorations for it. Following continual learning setting1 (Ring, 1994; Thrun and Pratt, 2012) in machine learning, Wang et al. (2019) first explore continual relation learning. Existing continual learning methods focus on three research directions: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017; Li"
2020.acl-main.573,D15-1206,0,0.0411578,"ng problem and significantly outperforms the stateof-the-art continual learning models. Further experiments and analyses indicate the reasons for the effectiveness of EMAR, proving that it can utilize a few examples in old tasks to reconsolidate old relation prototypes and keep better distinction among old relations after long-term training. 6430 2 Related Work The conventional RE work, including both supervised RE models (Zelenko et al., 2003; Zhou et al., 2005; Gormley et al., 2015; Socher et al., 2012; Liu et al., 2013; Zeng et al., 2014; Nguyen and Grishman, 2015; dos Santos et al., 2015; Xu et al., 2015; Liu et al., 2015; Miwa and Bansal, 2016) and distantly supervised models (Bunescu and Mooney, 2007; Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Han et al., 2018a; Baldini Soares et al., 2019), focuses on extracting predefined relations from text. Yet in the real world, new relations are rapidly emerging, and it is impossible to train models with a fixed dataset once to cover all relations. Hence, some researchers pay their attention to relation learning in various open scenarios, in order to detect and learn relations without pre-defin"
2020.acl-main.634,D18-1549,0,0.118592,"However, corpus with such annotations can be extremely costly to obtain and is usually limited to a specific domain with small data size. Some recent research started to do dialogue style transfer based on personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al., 2019; Su et al., 2019a). Our motivation differs from them in that we aim at enriching general dialogue generation with abundant non-conversational text instead of being constrained on one specific type of style. Back translation is widely used in unsupervised machine translation (Sennrich et al., 2016; Lample et al., 2018a; Artetxe et al., 2018) and has been recently extended to similar areas like style transfer (Subramanian et al., 2019), summarization (Zhao et al., 2019) and data-to-text (Chang et al., 2020). To the best of our knowledge, it has never been applied to dialogue generation yet. Our work treats the context and non-conversational text as unpaired source-target data. The backtranslation idea is naturally adopted to learn the mapping between them. The contents of nonconversational text can then be effectively utilized to enrich the dialogue generation. 3 Dataset We would like to collect non-conversational utterances that"
2020.acl-main.634,K16-1002,0,0.05197,"com/ 7091 https://www.douban.com/group Nucleus Sampling Proposed in Holtzman et al. (2019), it allows for diverse sequence generations. Instead of decoding with a fixed beam size, it samples text from the dynamic nucleus. We use the default configuration and set p = 0.9. CVAE The conditional variational autoencoder (Serban et al., 2017b; Zhao et al., 2017) which injects diversity by imposing stochastical latent variables. We use a latent variable with dimension 100 and utilize the KL-annealing strategy with step 350k and a word drop-out rate of 0.3 to alleviate the posterior collapse problem (Bowman et al., 2016). Furthermore, we compare the 4 approaches mentioned in §4 which incorporate the collected nonconversational text: Retrieval-based (§4.1) Due to the large size of the non-conversational corpus, exact ranking is extremely slow. Therefore, we first retrieve top 200 matched text with elastic search based on the similarity of Bert embeddings (Devlin et al., 2019). Specifically, we pass sentences through Bert and derive a fixed-sized vector by averaging the outputs from the second-to-last layer (May et al., 2019) 9 . The 200 candidates are then ranked with the backward score 10 . Weighted Average ("
2020.acl-main.634,P19-1567,0,0.0292939,"Missing"
2020.acl-main.634,N19-1423,0,0.00903955,"ich injects diversity by imposing stochastical latent variables. We use a latent variable with dimension 100 and utilize the KL-annealing strategy with step 350k and a word drop-out rate of 0.3 to alleviate the posterior collapse problem (Bowman et al., 2016). Furthermore, we compare the 4 approaches mentioned in §4 which incorporate the collected nonconversational text: Retrieval-based (§4.1) Due to the large size of the non-conversational corpus, exact ranking is extremely slow. Therefore, we first retrieve top 200 matched text with elastic search based on the similarity of Bert embeddings (Devlin et al., 2019). Specifically, we pass sentences through Bert and derive a fixed-sized vector by averaging the outputs from the second-to-last layer (May et al., 2019) 9 . The 200 candidates are then ranked with the backward score 10 . Weighted Average (§4.2) We set λ = 0.5 in eq. 1, which considers context relevance and diversity with equal weights. Multi-task ((§4.3)) We concatenate each contextresponse pair with a non-conversational utterance and train with a mixed objective of seq2seq and autoencoding (by sharing the decoder). Back Translation (§4.4) We perform the iterative backward and forward translat"
2020.acl-main.634,D19-1190,0,0.0348582,"Missing"
2020.acl-main.634,W18-2703,0,0.0189715,"i |f (Xi )) f (Xi ) = arg max Pf (v|Xi ) (4) v where the arg max function is again approximated with a beam search decoder and the gradient is only backpropagated through Pb . Though Xi has its corresponding Yi in D, we drop Yi and instead train on forward translated pseudo pairs {Xi , f (Xi )}. As Pf is trained by leveraging data from DT , f (Xi ) can have superior diversity compared with Yi . The encoder parameters are shared between the forward and backward models while decoders are separate. The backward and forward translation are iteratively performed to close the gap between Pf and Pb (Hoang et al., 2018; Cotterell and Kreutzer, 2018). The effects of non-conversational text are strengthened after each iteration. Eventually, the forward model will be able to produce diverse responses covering the wide topics in DT . Algorithm 1 depicts the training process. 5 Experiments 5.1 Datasets We conduct our experiments on two Chinese dialogue corpus Weibo (Shang et al., 2015b) and Douban (Wu et al., 2017). Weibo 7 is a popular Twitter-like microblogging service in China, on which a user can post short messages, and other 7 5.2 General Setup For all models, we use a two-layer LSTM (Hochreiter and Schmid"
2020.acl-main.634,P82-1020,0,0.789494,"Missing"
2020.acl-main.634,D16-1139,0,0.116262,", we borrow the back translation idea from unsupervised neural machine translation (Sennrich et al., 2016; Lample et al., 2018b) and treat the collected utterances as unpaired responses. We first pre-train the forward and backward transduction model on the parallel conversational corpus. The forward and backward model are then iteratively tuned to find the optimal mapping relation between conversational context and non-conversational utterances (Cotterell and Kreutzer, 2018). By this means, the content of non-conversational utterances is gradually distilled into the dialogue generation model (Kim and Rush, 2016), enlarging the space of generated responses to cover not only the original dialogue corpus, but also the wide topics reflected in the non-conversational utterances. We test our model on two popular Chinese conversational datasets weibo (Shang et al., 2015a) and douban (Wu et al., 2017). We compare our model against retrieval-based systems, style-transfer methods and several seq2seq variants which also target the diversity of dialogue generation. Automatic and human evaluation show that our model significantly improves the responses’ diversity both semantically and syntactically without sacrif"
2020.acl-main.634,P17-4012,0,0.013256,"e use a two-layer LSTM (Hochreiter and Schmidhuber, 1997) encoder/decoder structure with hidden size 500 and word embedding size 300. Models are trained with Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 0.15. We set the batch size as 256 and use the gradients clipping of 5. We build out vocabulary with character-based segmentation for Chinese. For non-Chinese tokens, we simply split by space and keep all unique tokens that appear at least 5 times. Utterances are cut down to at most 50 tokens and fed to every batch. We implement our models based on the OpenNMT toolkit (Klein et al., 2017) and other hyperparameters are set as the default values. 5.3 Compared Models We compare our model with the standard seq2seq and four popular variants which were proposed to improve the diversity of generated utterances. All of them are trained only on the parallel conversational corpus: Standard The standard seq2seq with beam search decoding (size 5). MMI The maximum mutual information decoding which reranks the decoded responses with a backward seq2seq model (Li et al., 2016a). The hyperparameter λ is set to 0.5 as suggested. 200 candidates per context are sampled for re-ranking Diverse Samp"
2020.acl-main.634,W07-0733,0,0.0508211,"(Li et al., 2016a) 5 . The major limitation of the retrieval-based system is that it can only produce responses from a finite set of candidates. The model can work well only if an appropriate response already exists in the candidate bank. Nonetheless, due to the large size of the non-conversational corpus, this approach is a very strong baseline. 4.2 Weighted Average The second approach is to take a weighted average score of a seq2seq model trained on D and a language model trained on DT when decoding responses. The idea has been widely utilized on domain adaptation for text generation tasks (Koehn and Schroeder, 2007; Wang et al., 2017; Niu and Bansal, 2018). In our scenario, basically we hope the generated responses could share the diverse topics and styles of the non-conversational text, yet stay relevant with the dialogue context. The seq2seq model S2S is trained on D as an indicator of how relevant each response is with the context. A language model L is trained on DT to measure how the response matches the domain of DT . The decoding probability for generating word w at time step t is assigned by: 3 pt (w) = αS2St (w) + (1 − α)Lt (w) 5 (1) The backward seq2seq model measures the context relevance bet"
2020.acl-main.634,N16-1014,0,0.177292,"and thereby pushes the model to ∗ Equal contribution. Conversational Text 暗恋的人却不喜欢我 The one I have a crush on doesn’t like me. 摸摸头 Head pat. Non-Conversational Text 暗恋这碗酒，谁喝都会醉啊 Crush is an alcoholic drink, whoever drinks it will get intoxicated. 何必等待一个没有结果的等待 Why wait for a result without hope 真诚的爱情之路永不会是平坦的 The course of true love never did run smooth (From A Midsummer Night’s Dream) blindly produce these safe, dull responses (Su et al., 2019b; Cs´aky et al., 2019) Current solutions can be roughly categorized into two classes: (1) Modify the seq2seq itself to bias toward diverse responses (Li et al., 2016a; Shen et al., 2019a). However, the model is still trained on the limited dialogue corpus which restricts its power at covering broad topics in opendomain chitchat. (2) Augment the training corpus with extra information like structured world knowledge, personality or emotions (Li et al., 2016b; Dinan et al., 2019), which requires costly human annotation. In this work, we argue that training only based on conversational corpus can greatly constrain the usability of an open-domain chatbot system since many topics are not easily available in the dialogue format. With this in mind, we explore a c"
2020.acl-main.634,P16-1094,0,0.0318509,"Missing"
2020.acl-main.634,D17-1019,0,0.027783,"en as we enrich the response generation with external resources. The diversity would be more than the original conversational corpus. Weighted-average and multi-task models are relatively worse, though still greatly outperforming models trained only on the conversational corpus. We can also observe that our model improves over standard seq2seq only a bit after one iteration. As more iterations are added, the diversity improves gradually. Relevance Measuring the context-response relevance automatically is tricky in our case. The typical way of using scores from forward or backward models as in Li and Jurafsky (2017) is not suitable as our model borrowed information from extra resources. The generated responses are out-of-scope 7092 Metrics Model S TANDARD MMI D IVERSE N UCLEUS CVAE R ETRIEVAL W EIGHTED M ULTI BT (I TER =1) BT (I TER =4) H UMAN BLEU-2 0.0165 0.0161 0.0175 0.0183 0.0171 0.0142 0.0152 0.0142 0.0180 0.0176 - Dist-1 0.018 0.025 0.019 0.027 0.023 0.198 0.091 0.128 0.046 0.175 0.171 Weibo Dist-2 0.050 0.069 0.054 0.074 0.061 0.492 0.316 0.348 0.171 0.487 0.452 Ent-4 5.04 5.98 6.20 7.41 6.63 12.5 9.26 8.98 7.64 11.2 9.23 Adver 0.30 0.42 0.38 0.43 0.36 0.13 0.22 0.27 0.19 0.35 0.88 BLEU-2 0.0285"
2020.acl-main.634,D16-1127,0,0.126248,"and thereby pushes the model to ∗ Equal contribution. Conversational Text 暗恋的人却不喜欢我 The one I have a crush on doesn’t like me. 摸摸头 Head pat. Non-Conversational Text 暗恋这碗酒，谁喝都会醉啊 Crush is an alcoholic drink, whoever drinks it will get intoxicated. 何必等待一个没有结果的等待 Why wait for a result without hope 真诚的爱情之路永不会是平坦的 The course of true love never did run smooth (From A Midsummer Night’s Dream) blindly produce these safe, dull responses (Su et al., 2019b; Cs´aky et al., 2019) Current solutions can be roughly categorized into two classes: (1) Modify the seq2seq itself to bias toward diverse responses (Li et al., 2016a; Shen et al., 2019a). However, the model is still trained on the limited dialogue corpus which restricts its power at covering broad topics in opendomain chitchat. (2) Augment the training corpus with extra information like structured world knowledge, personality or emotions (Li et al., 2016b; Dinan et al., 2019), which requires costly human annotation. In this work, we argue that training only based on conversational corpus can greatly constrain the usability of an open-domain chatbot system since many topics are not easily available in the dialogue format. With this in mind, we explore a c"
2020.acl-main.634,D17-1230,0,0.338203,"n et al. (2018a); Zhang et al. (2018b) changes the train1 Code and dataset available at https://github. com/chin-gyou/Div-Non-Conv ing objective to mutual information maximization and rely on continuous approximations or policy gradient to circumvent the non-differentiable issue for text. Li et al. (2016d); Serban et al. (2017a) treat open-domain chitchat as a reinforcement learning problem and manually define some rewards to encourage long-term conversations. There is also research that utilizes latent variable sampling (Serban et al., 2017b; Shen et al., 2018b, 2019b), adversarial learning (Li et al., 2017; Su et al., 2018), replaces the beam search decoding with a more diverse sampling strategy (Li et al., 2016c; Holtzman et al., 2019) or applies reranking to filter generic responses (Li et al., 2016a; Wang et al., 2017). All of the above are still trained on the original dialogue corpus and thereby cannot generate out-of-scope topics. The second class seeks to bring in extra information into existing corpus like structured knowledge (Zhao et al., 2018; Ghazvininejad et al., 2018; Dinan et al., 2019), personal information (Li et al., 2016b; Zhang et al., 2018a) or emotions (Shen et al., 2017b;"
2020.acl-main.634,I17-1061,0,0.0170573,"t the balance between the two. Setting α = 1 will make it degenerate into the standard seq2seq model while α = 0 will totally ignore the dialoge context. 4.3 Multi-task The third approach is based on multi-task learning. A seq2seq model is trained on the parallel conversational corpus D while an autoencoder model is trained on the non-parallel monologue data DT . Both models share the decoder parameters to facilitate each other. The idea was first experimented on machine translation in order to leverage large amounts of target-side monolingual text (Luong et al., 2016; Sennrich et al., 2016). Luan et al. (2017) extended it to conversational models for speaker-role adaptation. The intuition is that by tying the decoder parameters, the seq2seq and autoencoder model can learn a shared latent space between the dialogue corpus and non-conversational text. When decoding, the model can generate responses with features from both sides. 4.4 come from the same language, and we already have a parallel conversational corpus D, so we can get rid of the careful embedding alignment and autoencoding steps as in Lample et al. (2018b). For the initialization, we simply train a forward and backward seq2seq model on D."
2020.acl-main.634,Q18-1027,0,0.0788401,"the original dialogue corpus and thereby cannot generate out-of-scope topics. The second class seeks to bring in extra information into existing corpus like structured knowledge (Zhao et al., 2018; Ghazvininejad et al., 2018; Dinan et al., 2019), personal information (Li et al., 2016b; Zhang et al., 2018a) or emotions (Shen et al., 2017b; Zhou et al., 2018). However, corpus with such annotations can be extremely costly to obtain and is usually limited to a specific domain with small data size. Some recent research started to do dialogue style transfer based on personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al., 2019; Su et al., 2019a). Our motivation differs from them in that we aim at enriching general dialogue generation with abundant non-conversational text instead of being constrained on one specific type of style. Back translation is widely used in unsupervised machine translation (Sennrich et al., 2016; Lample et al., 2018a; Artetxe et al., 2018) and has been recently extended to similar areas like style transfer (Subramanian et al., 2019), summarization (Zhao et al., 2019) and data-to-text (Chang et al., 2020). To the best of our knowledge, it has never been applied to dialogue g"
2020.acl-main.634,P02-1040,0,0.109227,"entropy loss converges after 4 iterations. 6 Results As for the experiment results, we report the automatic and human evaluation in §6.1 and §6.2 respectively. Detailed analysis are shown in §6.3 to elaborate the differences among model performances and some case studies. 9 https://github.com/hanxiao/ bert-as-service 10 This makes it similar to MMI reranking, whose 200 candidates are from seq2seq decodings instead of top-matched non-conversational utterances. 6.1 Automatic Evaluation Evaluating dialogue generation is extremely difficult. Metrics which measure the word-level overlap like BLEU (Papineni et al., 2002) have been widely used for dialogue evaluation. However, these metrics do not fit into our setting well as we would like to diversify the response generation with an external corpus, the generations will inevitably differ greatly from the ground-truth references in the original conversational corpus. Though we report the BLEU score anyway and list all the results in Table 3, it is worth mentioning that the BLEU score itself is by no means a reliable metric to measure the quality of dialogue generations. Diversity Diversity is a major concern for dialogue generation. Same as in (Li et al., 2016"
2020.acl-main.634,P16-1009,0,0.611415,"tions where people often passively reply to the last utterance. As can be seen in Table 1, the response from the daily conversation is a simple comfort of “Head pat”. Nonconversational text, on the contrary, exhibit diverse styles ranging from casual wording to poetic statements, which we believe can be potentially utilized to enrich the response generation. To do so, we collect a large-scale corpus containing over 1M non-conversational utterances from multiple sources. To effectively integrate these utterances, we borrow the back translation idea from unsupervised neural machine translation (Sennrich et al., 2016; Lample et al., 2018b) and treat the collected utterances as unpaired responses. We first pre-train the forward and backward transduction model on the parallel conversational corpus. The forward and backward model are then iteratively tuned to find the optimal mapping relation between conversational context and non-conversational utterances (Cotterell and Kreutzer, 2018). By this means, the content of non-conversational utterances is gradually distilled into the dialogue generation model (Kim and Rush, 2016), enlarging the space of generated responses to cover not only the original dialogue c"
2020.acl-main.634,P15-1152,0,0.0993563,"Missing"
2020.acl-main.634,D18-1463,1,0.884647,"eration. Automatic and human evaluation show that our model significantly improves the responses’ diversity both semantically and syntactically without sacrificing the relevance with context, and is considered as most favorable judged by human evaluators 1 . 2 Related Work The tendency to produce generic responses has been a long-standing problem in seq2seq-based open-domain dialogue generation (Vinyals and Le, 2015; Li et al., 2016a). Previous approaches to alleviate this issue can be grouped into two classes. The first class resorts to modifying the seq2seq architecture itself. For example, Shen et al. (2018a); Zhang et al. (2018b) changes the train1 Code and dataset available at https://github. com/chin-gyou/Div-Non-Conv ing objective to mutual information maximization and rely on continuous approximations or policy gradient to circumvent the non-differentiable issue for text. Li et al. (2016d); Serban et al. (2017a) treat open-domain chitchat as a reinforcement learning problem and manually define some rewards to encourage long-term conversations. There is also research that utilizes latent variable sampling (Serban et al., 2017b; Shen et al., 2018b, 2019b), adversarial learning (Li et al., 201"
2020.acl-main.634,N19-1063,0,0.0142291,"p 350k and a word drop-out rate of 0.3 to alleviate the posterior collapse problem (Bowman et al., 2016). Furthermore, we compare the 4 approaches mentioned in §4 which incorporate the collected nonconversational text: Retrieval-based (§4.1) Due to the large size of the non-conversational corpus, exact ranking is extremely slow. Therefore, we first retrieve top 200 matched text with elastic search based on the similarity of Bert embeddings (Devlin et al., 2019). Specifically, we pass sentences through Bert and derive a fixed-sized vector by averaging the outputs from the second-to-last layer (May et al., 2019) 9 . The 200 candidates are then ranked with the backward score 10 . Weighted Average (§4.2) We set λ = 0.5 in eq. 1, which considers context relevance and diversity with equal weights. Multi-task ((§4.3)) We concatenate each contextresponse pair with a non-conversational utterance and train with a mixed objective of seq2seq and autoencoding (by sharing the decoder). Back Translation (§4.4) We perform the iterative backward and forward translation 4 times for both datasets. We observe the forward cross entropy loss converges after 4 iterations. 6 Results As for the experiment results, we repor"
2020.acl-main.634,D19-1054,1,0.744161,"s the model to ∗ Equal contribution. Conversational Text 暗恋的人却不喜欢我 The one I have a crush on doesn’t like me. 摸摸头 Head pat. Non-Conversational Text 暗恋这碗酒，谁喝都会醉啊 Crush is an alcoholic drink, whoever drinks it will get intoxicated. 何必等待一个没有结果的等待 Why wait for a result without hope 真诚的爱情之路永不会是平坦的 The course of true love never did run smooth (From A Midsummer Night’s Dream) blindly produce these safe, dull responses (Su et al., 2019b; Cs´aky et al., 2019) Current solutions can be roughly categorized into two classes: (1) Modify the seq2seq itself to bias toward diverse responses (Li et al., 2016a; Shen et al., 2019a). However, the model is still trained on the limited dialogue corpus which restricts its power at covering broad topics in opendomain chitchat. (2) Augment the training corpus with extra information like structured world knowledge, personality or emotions (Li et al., 2016b; Dinan et al., 2019), which requires costly human annotation. In this work, we argue that training only based on conversational corpus can greatly constrain the usability of an open-domain chatbot system since many topics are not easily available in the dialogue format. With this in mind, we explore a cheap way to diversif"
2020.acl-main.634,D19-1390,1,0.73715,"s the model to ∗ Equal contribution. Conversational Text 暗恋的人却不喜欢我 The one I have a crush on doesn’t like me. 摸摸头 Head pat. Non-Conversational Text 暗恋这碗酒，谁喝都会醉啊 Crush is an alcoholic drink, whoever drinks it will get intoxicated. 何必等待一个没有结果的等待 Why wait for a result without hope 真诚的爱情之路永不会是平坦的 The course of true love never did run smooth (From A Midsummer Night’s Dream) blindly produce these safe, dull responses (Su et al., 2019b; Cs´aky et al., 2019) Current solutions can be roughly categorized into two classes: (1) Modify the seq2seq itself to bias toward diverse responses (Li et al., 2016a; Shen et al., 2019a). However, the model is still trained on the limited dialogue corpus which restricts its power at covering broad topics in opendomain chitchat. (2) Augment the training corpus with extra information like structured world knowledge, personality or emotions (Li et al., 2016b; Dinan et al., 2019), which requires costly human annotation. In this work, we argue that training only based on conversational corpus can greatly constrain the usability of an open-domain chatbot system since many topics are not easily available in the dialogue format. With this in mind, we explore a cheap way to diversif"
2020.acl-main.634,P19-1003,1,0.921625,"17; Su et al., 2018). Generic utterances, which can be in theory paired with most context, usually dominate the frequency distribution in the dialogue training corpus and thereby pushes the model to ∗ Equal contribution. Conversational Text 暗恋的人却不喜欢我 The one I have a crush on doesn’t like me. 摸摸头 Head pat. Non-Conversational Text 暗恋这碗酒，谁喝都会醉啊 Crush is an alcoholic drink, whoever drinks it will get intoxicated. 何必等待一个没有结果的等待 Why wait for a result without hope 真诚的爱情之路永不会是平坦的 The course of true love never did run smooth (From A Midsummer Night’s Dream) blindly produce these safe, dull responses (Su et al., 2019b; Cs´aky et al., 2019) Current solutions can be roughly categorized into two classes: (1) Modify the seq2seq itself to bias toward diverse responses (Li et al., 2016a; Shen et al., 2019a). However, the model is still trained on the limited dialogue corpus which restricts its power at covering broad topics in opendomain chitchat. (2) Augment the training corpus with extra information like structured world knowledge, personality or emotions (Li et al., 2016b; Dinan et al., 2019), which requires costly human annotation. In this work, we argue that training only based on conversational corpus can"
2020.acl-main.634,N19-1049,0,0.0337175,"Missing"
2020.acl-main.634,P18-1205,0,0.467938,"d human evaluation show that our model significantly improves the responses’ diversity both semantically and syntactically without sacrificing the relevance with context, and is considered as most favorable judged by human evaluators 1 . 2 Related Work The tendency to produce generic responses has been a long-standing problem in seq2seq-based open-domain dialogue generation (Vinyals and Le, 2015; Li et al., 2016a). Previous approaches to alleviate this issue can be grouped into two classes. The first class resorts to modifying the seq2seq architecture itself. For example, Shen et al. (2018a); Zhang et al. (2018b) changes the train1 Code and dataset available at https://github. com/chin-gyou/Div-Non-Conv ing objective to mutual information maximization and rely on continuous approximations or policy gradient to circumvent the non-differentiable issue for text. Li et al. (2016d); Serban et al. (2017a) treat open-domain chitchat as a reinforcement learning problem and manually define some rewards to encourage long-term conversations. There is also research that utilizes latent variable sampling (Serban et al., 2017b; Shen et al., 2018b, 2019b), adversarial learning (Li et al., 2017; Su et al., 2018), r"
2020.acl-main.634,P17-1061,0,0.255116,"omments Idiom Book Snippet Table 1: A daily dialogue and non-conversational text from three sources. The contents of non-conversational text can be potentially utilized to enrich the response generation. Introduction Seq2seq models have achieved impressive success in a wide range of text generation tasks. In opendomain chitchat, however, people have found the model tends to strongly favor short, generic responses like “I don’t know” or “OK” (Vinyals and Le, 2015; Shen et al., 2017a). The reason lies in the extreme one-to-many mapping relation between every context and its potential responses (Zhao et al., 2017; Su et al., 2018). Generic utterances, which can be in theory paired with most context, usually dominate the frequency distribution in the dialogue training corpus and thereby pushes the model to ∗ Equal contribution. Conversational Text 暗恋的人却不喜欢我 The one I have a crush on doesn’t like me. 摸摸头 Head pat. Non-Conversational Text 暗恋这碗酒，谁喝都会醉啊 Crush is an alcoholic drink, whoever drinks it will get intoxicated. 何必等待一个没有结果的等待 Why wait for a result without hope 真诚的爱情之路永不会是平坦的 The course of true love never did run smooth (From A Midsummer Night’s Dream) blindly produce these safe, dull responses (Su"
2020.acl-main.634,P19-1216,1,0.844491,"size. Some recent research started to do dialogue style transfer based on personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al., 2019; Su et al., 2019a). Our motivation differs from them in that we aim at enriching general dialogue generation with abundant non-conversational text instead of being constrained on one specific type of style. Back translation is widely used in unsupervised machine translation (Sennrich et al., 2016; Lample et al., 2018a; Artetxe et al., 2018) and has been recently extended to similar areas like style transfer (Subramanian et al., 2019), summarization (Zhao et al., 2019) and data-to-text (Chang et al., 2020). To the best of our knowledge, it has never been applied to dialogue generation yet. Our work treats the context and non-conversational text as unpaired source-target data. The backtranslation idea is naturally adopted to learn the mapping between them. The contents of nonconversational text can then be effectively utilized to enrich the dialogue generation. 3 Dataset We would like to collect non-conversational utterances that stay close with daily-life topics and can 7088 Resources Comments Idioms Book Snippets Size Avg. length 781,847 51,948 206,340 21."
2020.acl-main.634,D17-1228,0,0.0562548,"Missing"
2020.acl-main.634,P17-1046,0,0.248241,"he forward and backward model are then iteratively tuned to find the optimal mapping relation between conversational context and non-conversational utterances (Cotterell and Kreutzer, 2018). By this means, the content of non-conversational utterances is gradually distilled into the dialogue generation model (Kim and Rush, 2016), enlarging the space of generated responses to cover not only the original dialogue corpus, but also the wide topics reflected in the non-conversational utterances. We test our model on two popular Chinese conversational datasets weibo (Shang et al., 2015a) and douban (Wu et al., 2017). We compare our model against retrieval-based systems, style-transfer methods and several seq2seq variants which also target the diversity of dialogue generation. Automatic and human evaluation show that our model significantly improves the responses’ diversity both semantically and syntactically without sacrificing the relevance with context, and is considered as most favorable judged by human evaluators 1 . 2 Related Work The tendency to produce generic responses has been a long-standing problem in seq2seq-based open-domain dialogue generation (Vinyals and Le, 2015; Li et al., 2016a). Previ"
2020.coling-main.126,D17-1169,0,0.0267558,"ll get a set of negative samples. Here, we choose a fixed replaced probability of p = 0.5 for each slot token individually. We randomly sample a minibatch of N examples and define the contrastive loss on pairs of replaced examples derived from the minibatch as follows: N X M X L= max(0, s + d(uk , upk ) − d(uk , unk,i )) (3) k=1 i=1 where M is the size of the negative sample set and uk represents k-th input utterance vector in the batch. upk denotes the positive sample and unk,i denotes i-th negative sample of k-th input utterance. d is the L2 distance function and s is the margin. Following (Felbo et al., 2017; Liu et al., 2020b), we employ another BiLSTM and an attention layer to generate representations of positive and negative samples. The contrastive loss aims to map slot value contextual representations to the corresponding slot description representations in the latent space. Therefore, slot descriptions can help learn the semantic pattern of related slot entities. Compared to Template Regularization (TR) proposed by Liu et al. (2020b), our CZSL jointly models pairs of positive and negative samples to distinguish semantic representations of different slot types. 2.2 Adversarial Attack Trainin"
2020.coling-main.126,N18-2118,0,0.0534817,"he contrastive loss aims to map slot value contextual representations to the corresponding slot description representations. And we introduce an adversarial attack training strategy to improve model robustness. Experimental results show that our model significantly outperforms state-of-the-art baselines under both zero-shot and few-shot settings. 1 Introduction The slot filling task in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target do"
2020.coling-main.126,P19-1544,0,0.101406,"s aims to map slot value contextual representations to the corresponding slot description representations. And we introduce an adversarial attack training strategy to improve model robustness. Experimental results show that our model significantly outperforms state-of-the-art baselines under both zero-shot and few-shot settings. 1 Introduction The slot filling task in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target domain without labeled t"
2020.coling-main.126,2020.acl-main.58,1,0.822685,"ue contextual representations to the corresponding slot description representations. And we introduce an adversarial attack training strategy to improve model robustness. Experimental results show that our model significantly outperforms state-of-the-art baselines under both zero-shot and few-shot settings. 1 Introduction The slot filling task in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target domain without labeled training samples i"
2020.coling-main.126,2020.repl4nlp-1.1,0,0.38727,"in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target domain without labeled training samples in the target domain. The main challenge of zero-shot slot filling is to identify unseen slot types without any supervision signals in the target domain. Typically, the previous methods rely on slot descriptions or example values to bootstrap to new slots by capturing the semantic relationship between slot descriptions and input tokens. These me"
2020.coling-main.126,2020.acl-main.3,0,0.299073,"in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target domain without labeled training samples in the target domain. The main challenge of zero-shot slot filling is to identify unseen slot types without any supervision signals in the target domain. Typically, the previous methods rely on slot descriptions or example values to bootstrap to new slots by capturing the semantic relationship between slot descriptions and input tokens. These me"
2020.coling-main.126,N19-1087,0,0.0236231,"The slot filling task in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target domain without labeled training samples in the target domain. The main challenge of zero-shot slot filling is to identify unseen slot types without any supervision signals in the target domain. Typically, the previous methods rely on slot descriptions or example values to bootstrap to new slots by capturing the semantic relationship between slot descriptions and inpu"
2020.coling-main.126,P19-1547,0,0.263352,"ngs. 1 Introduction The slot filling task in the goal-oriented dialog system aims to identify task-related slot types in certain domains for understanding user utterances. Traditional supervised slot filling models (Liu and Lane, 2015; Liu and Lane, 2016; Goo et al., 2018; Haihong et al., 2019; He et al., 2020a; He et al., 2020b) have made great achievements. However, these models require massive amounts of labeled data for a new domain, hindering the rapid development of new tasks. To address the data-intensiveness problem, domain adaptation approaches (Bapna et al., 2017; Lee and Jha, 2019; Shah et al., 2019; Obeidat et al., 2019; Liu et al., 2020b; He et al., 2020c) have been successfully applied. In this paper, we focus on zero-shot cross-domain transfer learning which leverages knowledge learned in the source domains and adapts the models to the target domain without labeled training samples in the target domain. The main challenge of zero-shot slot filling is to identify unseen slot types without any supervision signals in the target domain. Typically, the previous methods rely on slot descriptions or example values to bootstrap to new slots by capturing the semantic relationship between slot"
2020.coling-main.259,W14-3346,0,0.0128892,"onal auto-encoder based approach. • VMED (Le et al., 2018): This model associates each memory read with a mode in the latent mixture distribution at each timestep. It can capture the variability observed in sequential data. • MMPMS (Chen et al., 2019): A state-of-art multi-mapping mechanism model. It focuses on selecting the corresponding mapping module by the target response. Following their setting, we set the number of mapping modules to 20. 3.3 Evaluation Metrics We use two kinds of evaluation methods: automatic evaluation and manual evaluation. For automatic evaluation, we used BLEU-1/2 (Chen and Cherry, 2014) to test the percentage of overlap of unigram and 2 https://y.qq.com/ https://music.163.com/ 4 https://ai.tencent.com/ailab/nlp/en/index.html 3 2894 bigram between the generated comment and ground truth. We also use Dist-1/2 (Li et al., 2016a) to test the richness of unigram and bigram in all the comments generated. For manual evaluation, inspired by Liu et al. (2019) and Zhou et al. (2018), we adopt the following four manual evaluation metrics: • Fluency: Whether the comments are fluent and whether there are severe grammatical errors. • Coherence: Whether the generated comments conform to the"
2020.coling-main.259,N16-1014,0,0.259808,"ves for the same music content. There goes a saying that there are a thousand Hamlets in a thousand people’s eyes. As a result, the music comment generation is typically regarded as a one-to-many generation task. Nowadays, researchers have noticed such problems and tried to solve them via multiple methods in dialogue generation. Some of them have utilized topics, keywords, meta-words, and other information during the generation process to improve performance (Xing et al., 2017; Mou et al., 2016; Xu et al., 2019). Some researchers try to optimize the decoding process (Vijayakumar et al., 2016; Li et al., 2016b) or reorder the candidate sequences after the decoding (Yao et al., 2016; Song et al., 2017). However, these approaches cannot significantly improve the model’s performance on diversity. Some methods model one-to-many relationships by multiple latent variables and conform to the dialogue generation scenario (Zhou et al., 2017; Zhou et al., 2018; Chen et al., 2019). They tried to add multiple latent mechanisms or multi-mapping mechanisms between the encoder and decoder of the seq2seq architecture. These one-to-many mapping modules can capture a variety of similar generation modes to a certain"
2020.coling-main.259,P19-1479,0,0.0210557,"ng man, I don&apos;t envy you Figure 4: Examples of comments generated by different models for the same music content instance, all the 3th-5th comments from the MMPMS model describe about “梦想(dream)” and “照 亮(light up)”. However, the comments generated by our model are meaningful and diverse. Meanwhile, our model can also generate comments from more perspectives or topics. 5 Related work The text generation based on the Seq2Seq model tends to create general text. For example, existing models on open-domain comment generation always produce repetitive and uninteresting comments (Lin et al., 2019). Li et al. (2019) model the input news as a topic interaction graph and generate comments with a graph-to-sequence model. Lin et al. (2019) retrieve informative and relevant comments by leveraging user-generated data. However, many researchers try to model a one-to-many relationship to solve this similar problem in dialogue generation tasks. In detail, Xing et al. (2017) use topics to simulate prior human knowledge and guide them to form informative responses. In contrast, Mou et al. (2016) utilize pointwise mutual information to extract words as keywords and decode the response based on the keywords. Liu et a"
2020.coling-main.259,P18-1138,0,0.013456,". (2019) model the input news as a topic interaction graph and generate comments with a graph-to-sequence model. Lin et al. (2019) retrieve informative and relevant comments by leveraging user-generated data. However, many researchers try to model a one-to-many relationship to solve this similar problem in dialogue generation tasks. In detail, Xing et al. (2017) use topics to simulate prior human knowledge and guide them to form informative responses. In contrast, Mou et al. (2016) utilize pointwise mutual information to extract words as keywords and decode the response based on the keywords. Liu et al. (2018) propose a neural knowledge diffusion model to introduce knowledge into dialogue generation. Zhang et al. (2018) apply an explicit specificity control variable into a seq2seq model to generate responses at different specificity levels. Besides, Xu et al. (2019) enhance the seq2seq architecture with a goal tracking memory network to incorporate meta-words into generation. The above methods aim to enhance the diversity of generated results via adding specific structures, which has already achieved a particular improvement. In recent years, some researchers try to construct multiple latent mechan"
2020.coling-main.259,P19-1192,1,0.886772,"Missing"
2020.coling-main.259,C16-1316,0,0.145694,"899 Barcelona, Spain (Online), December 8-13, 2020 comments, there exists excellent distinction and diverse perspectives for the same music content. There goes a saying that there are a thousand Hamlets in a thousand people’s eyes. As a result, the music comment generation is typically regarded as a one-to-many generation task. Nowadays, researchers have noticed such problems and tried to solve them via multiple methods in dialogue generation. Some of them have utilized topics, keywords, meta-words, and other information during the generation process to improve performance (Xing et al., 2017; Mou et al., 2016; Xu et al., 2019). Some researchers try to optimize the decoding process (Vijayakumar et al., 2016; Li et al., 2016b) or reorder the candidate sequences after the decoding (Yao et al., 2016; Song et al., 2017). However, these approaches cannot significantly improve the model’s performance on diversity. Some methods model one-to-many relationships by multiple latent variables and conform to the dialogue generation scenario (Zhou et al., 2017; Zhou et al., 2018; Chen et al., 2019). They tried to add multiple latent mechanisms or multi-mapping mechanisms between the encoder and decoder of the se"
2020.coling-main.259,P18-2025,0,0.0894333,"et the number of mapping modules to 20. 3.3 Evaluation Metrics We use two kinds of evaluation methods: automatic evaluation and manual evaluation. For automatic evaluation, we used BLEU-1/2 (Chen and Cherry, 2014) to test the percentage of overlap of unigram and 2 https://y.qq.com/ https://music.163.com/ 4 https://ai.tencent.com/ailab/nlp/en/index.html 3 2894 bigram between the generated comment and ground truth. We also use Dist-1/2 (Li et al., 2016a) to test the richness of unigram and bigram in all the comments generated. For manual evaluation, inspired by Liu et al. (2019) and Zhou et al. (2018), we adopt the following four manual evaluation metrics: • Fluency: Whether the comments are fluent and whether there are severe grammatical errors. • Coherence: Whether the generated comments conform to the scenario of music. How relevant the comment is to the music content. • Meaningfulness: Whether the generated comments have rich meaning and detailed content. • Distinction: Whether there exist significant differences between the generated comments for the same music input content. The greater the difference, the higher the score. All the above metrics are scored on a five-point scale, and"
2020.coling-main.259,I17-2029,0,0.0181445,"thousand people’s eyes. As a result, the music comment generation is typically regarded as a one-to-many generation task. Nowadays, researchers have noticed such problems and tried to solve them via multiple methods in dialogue generation. Some of them have utilized topics, keywords, meta-words, and other information during the generation process to improve performance (Xing et al., 2017; Mou et al., 2016; Xu et al., 2019). Some researchers try to optimize the decoding process (Vijayakumar et al., 2016; Li et al., 2016b) or reorder the candidate sequences after the decoding (Yao et al., 2016; Song et al., 2017). However, these approaches cannot significantly improve the model’s performance on diversity. Some methods model one-to-many relationships by multiple latent variables and conform to the dialogue generation scenario (Zhou et al., 2017; Zhou et al., 2018; Chen et al., 2019). They tried to add multiple latent mechanisms or multi-mapping mechanisms between the encoder and decoder of the seq2seq architecture. These one-to-many mapping modules can capture a variety of similar generation modes to a certain extent in dialogue generation. Nevertheless, there is so much overlap of aspects between the"
2020.coling-main.259,P19-1538,0,0.0465037,"in (Online), December 8-13, 2020 comments, there exists excellent distinction and diverse perspectives for the same music content. There goes a saying that there are a thousand Hamlets in a thousand people’s eyes. As a result, the music comment generation is typically regarded as a one-to-many generation task. Nowadays, researchers have noticed such problems and tried to solve them via multiple methods in dialogue generation. Some of them have utilized topics, keywords, meta-words, and other information during the generation process to improve performance (Xing et al., 2017; Mou et al., 2016; Xu et al., 2019). Some researchers try to optimize the decoding process (Vijayakumar et al., 2016; Li et al., 2016b) or reorder the candidate sequences after the decoding (Yao et al., 2016; Song et al., 2017). However, these approaches cannot significantly improve the model’s performance on diversity. Some methods model one-to-many relationships by multiple latent variables and conform to the dialogue generation scenario (Zhou et al., 2017; Zhou et al., 2018; Chen et al., 2019). They tried to add multiple latent mechanisms or multi-mapping mechanisms between the encoder and decoder of the seq2seq architecture"
2020.coling-main.259,P19-2032,0,0.0213262,"t datasets show that our proposed model can effectively generate a series of diverse music comments based on different perspectives, which outperforms state-of-the-art baselines by a substantial margin.1 1 Introduction In recent years, neural networks have achieved great success in natural language generation (NLG), which can be applied in many real-world scenarios, such as poetry generation, dialogue generation, comment generation, and so on. Music comment generation is a sub-task of NLG. High-quality comments can effectively increase the popularity of music and the activity of the platform (Zeng et al., 2019). Title: ⼀剪梅 (A Spray of Plum Blossoms) Singer: 费⽟清 (Fei Yuqing) Lyrics: 雪花飘飘北风萧萧，天地⼀⽚苍茫，⼀剪寒梅傲⽴雪中，只为伊⼈飘⾹，爱我所爱⽆怨⽆悔... (Snow petals drifting, the north wind whistles. The world ever a boundless. A spray of winter Plum Blossoms. Stands proudly in the snow. Only for that person its fragrance drift. My love is without complains and regrets...) Human comments 已单曲循环雪花飘飘⼀百遍 Generated comments A single cycle of snowflakes fluttering a hundred times 我喜欢这⾸歌 I like this song 这⾸歌的声⾳真是扣⼈⼼弦 这是我喜欢的歌 The sound of this song is really fascinating This is my favorite song ⼀剪梅改了这么多版，还是这版耐听 这⾸歌是我最喜欢的⼀⾸ ⽂能⼀剪梅 武能嘿嘿嘿"
2020.coling-main.259,P18-1102,0,0.0115872,"l. Lin et al. (2019) retrieve informative and relevant comments by leveraging user-generated data. However, many researchers try to model a one-to-many relationship to solve this similar problem in dialogue generation tasks. In detail, Xing et al. (2017) use topics to simulate prior human knowledge and guide them to form informative responses. In contrast, Mou et al. (2016) utilize pointwise mutual information to extract words as keywords and decode the response based on the keywords. Liu et al. (2018) propose a neural knowledge diffusion model to introduce knowledge into dialogue generation. Zhang et al. (2018) apply an explicit specificity control variable into a seq2seq model to generate responses at different specificity levels. Besides, Xu et al. (2019) enhance the seq2seq architecture with a goal tracking memory network to incorporate meta-words into generation. The above methods aim to enhance the diversity of generated results via adding specific structures, which has already achieved a particular improvement. In recent years, some researchers try to construct multiple latent mechanisms to model the one-tomany relationship and generate diverse results. Among them, Tao et al. (2018) propose a"
2020.coling-main.259,P17-1061,0,0.0320687,"Missing"
2020.coling-main.49,P07-1056,0,0.657022,"set (Ni et al., 2019) and Yelp 2020 challenge dataset5 . Amazon dataset contains 233 million reviews within 29 domains (Ni et al., 2019). The total number of yelp reviews is about 8 million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang e"
2020.coling-main.49,N19-1423,0,0.527806,"language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2) ˚ Yuanbin Wu and Liang He are the corresponding authors of this paper. This work was conducted when Jie Zhou was interning at Alibaba DAMO Academy. This work i"
2020.coling-main.49,P14-2009,0,0.0274439,"al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with adversarial training. We adopt the results of these baselines reported in (Du et al., 2020). For in-domain sentiment analysis, we compare our model with SentiLR-B (Ke et al., 2019), which is one of the"
2020.coling-main.49,2020.acl-main.370,0,0.346247,"entiment analysis has become a promising direction, which transfers (invariant) sentiment knowledge from the source domain to the target domain1 . The major challenge here is that language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at th"
2020.coling-main.49,W19-6120,0,0.0132226,"a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2) ˚ Yuanbin Wu and Liang He are the corresponding authors of this paper. This work was conducted when Jie Zhou was interning at Alibaba DAMO Academy. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// cre"
2020.coling-main.49,P11-1015,0,0.0839079,"iment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with"
2020.coling-main.49,D19-1018,0,0.0185346,"): :D (8 <3 ;-) Count 160,970 113,440 86,546 78,492 74,247 # 11 12 13 14 15 Emoticon :/ =) :-( 8: 8) Count 67,615 66,156 64,391 53,073 46,124 # 16 17 18 19 20 Emoticon (: :P ;D :o) =( Count 40,062 31,573 15,718 12,952 11,917 Table 1: Statistics information of top-20 emoticons. 3.3 Joint Training Finally, we jointly optimize the token-level objective LT and the sentence-level objective LS . The overall loss is L “ LT ` LS , where LT “ Lw ` Ls ` Le and LS “ Lr . 4 Experimental Setup 4.1 Datasets Pre-training The pre-training phase is conducted on two large-scale datasets: Amazon review dataset (Ni et al., 2019) and Yelp 2020 challenge dataset5 . Amazon dataset contains 233 million reviews within 29 domains (Ni et al., 2019). The total number of yelp reviews is about 8 million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007)"
2020.coling-main.49,P18-1233,0,0.0203972,"ce (Accuracy) Epoch 2 Epoch 3 Epoch 4 73.85 84.05 85.45 50.00 52.45 50.80 92.30 92.05 91.10 91.45 92.25 92.75 Epoch 5 86.55 51.10 90.85 93.05 Table 5: The results of complexity and convergence. We list the costed time of each epoch and space complexity of trainable parameters in B Ñ E task with the same batchsize. 6 Related Work Cross-domain Sentiment Analysis Due to the heavy cost of obtaining large quantities of labeled data for each domain, many approaches have been proposed for cross-domain sentiment analysis (Blitzer et al., 2007; Yu and Jiang, 2016; Li et al., 2013; Zhang et al., 2019a; Peng et al., 2018). Most of the previous works focus on capturing the pivots that are useful for both source domain and target domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant"
2020.coling-main.49,N18-1202,0,0.0591782,"domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to be"
2020.coling-main.49,D19-1005,0,0.0326198,"Missing"
2020.coling-main.49,N19-1258,0,0.115047,"million. We preprocess the text via NLTK6 and transfer all the letters into lower. We filter the text that contains less than 50 tokens or more than 512 tokens and sample the rating data in dealing with class-imbalance problem. The statistics information of top-20 emoticons is shown in Table 1. Sentiment Analysis To verify the effectiveness of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al"
2020.coling-main.49,D13-1170,0,0.00654472,"s of S ENTI X, We evaluate on the widely used crossdomain sentiment dataset (Blitzer et al., 2007), containing four domains: Books (B), DVD (D), Electronic (E), and Kitchen & Housewares (K). Following the setting of previous works (Ziser and Reichart, 2018; Qu et al., 2019), we test on 12 cross-domain tasks. The model is trained on the source domain and tested on the target domains. As before, we split 200 samples from 2000 samples in source domain as a development set to find the best hyperparameters. Besides, we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art mode"
2020.coling-main.49,N19-1035,0,0.02339,"tion. However, most of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and Kno"
2020.coling-main.49,2020.acl-main.374,0,0.0405769,"k into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and KnowBERT (Peters 576 et al., 2019) integrated entity representation into BERT. Alternatively, Levine et at. (2019) introduced a SenseBERT to improve lexical understanding by predicting tokens’ supersenses in WordNet. Tian et al. (2020) and Ke et al. (2019) integrated external knowledge to learn sentiment information. They focused on improving the performance with fine-tuning on downstream sentiment analysis tasks by training on a relatively small or one domain dataset. Different from the existing studies, we design several pre-training objectives via rich domain-invariant sentiment knowledge in large-scale multi-domain unlabeled data for cross-domain sentiment analysis. 7 Conclusions In this paper, we pre-train our S ENTI X model to induce a general low dimensional representation based on domain-invariant sentiment knowledg"
2020.coling-main.49,N19-1242,0,0.0236828,"of the existing work focuses on learning the domain-shared representation in training or fine-tuning, how to learn domain-invariant sentiment knowledge from the pre-training phase has not been explored. Pre-trained Model Existing studies (Peters et al., 2018; Devlin et al., 2019) have proved that pretraining on large-scale unlabelled corpus obtains state-of-the-art performances in the field of natural language processing (Qiu et al., 2020). On the one hand, many studies applied pre-trained models to downstream tasks via fine-tuning (Devlin et al., 2019; Dodge et al., 2020; Sun et al., 2019b; Xu et al., 2019). Devlin et al. (2019) fine-tuned the BERT model on many downstream tasks, such as name entity recognition and sentiment analysis. Sun et al. (2019a) converted aspect-based sentiment analysis task into a sentence pair classification task to better utilize the powerful representation of BERT. On the other hand, some work proposed to add external knowledge into pre-training BERT to enhance the representations (Zhang et al., 2020). LIBERT (Lauscher et al., 2019) integrated linguistic knowledge through an additional linguistic constraint task. ERINE (Zhang et al., 2019b) and KnowBERT (Peters 576 e"
2020.coling-main.49,D16-1023,0,0.0202871,"133M 2K 133M 2K Epoch 1 52.20 50.65 92.60 90.55 Convergence (Accuracy) Epoch 2 Epoch 3 Epoch 4 73.85 84.05 85.45 50.00 52.45 50.80 92.30 92.05 91.10 91.45 92.25 92.75 Epoch 5 86.55 51.10 90.85 93.05 Table 5: The results of complexity and convergence. We list the costed time of each epoch and space complexity of trainable parameters in B Ñ E task with the same batchsize. 6 Related Work Cross-domain Sentiment Analysis Due to the heavy cost of obtaining large quantities of labeled data for each domain, many approaches have been proposed for cross-domain sentiment analysis (Blitzer et al., 2007; Yu and Jiang, 2016; Li et al., 2013; Zhang et al., 2019a; Peng et al., 2018). Most of the previous works focus on capturing the pivots that are useful for both source domain and target domain (Ziser and Reichart, 2018; Li et al., 2018). Domain adaptation adversarial training (Ganin et al., 2016) is widelyused to learn the domain-common sentiment knowledge (Li et al., 2017; Qu et al., 2019). Recently, Du et al. (2020) integrated BERT into cross-domain sentiment analysis tasks to learn the domain-shared feature representation. However, most of the existing work focuses on learning the domain-shared representation"
2020.coling-main.49,P19-1139,0,0.330715,"we examine the model on four popular sentient classification datasets: SST-2-Root (Socher et al., 2013), SST-5-Root (Socher et al., 2013), IMDB (Maas et al., 2011) and Yelp (Zhang et al., 2015), and three aspect-based sentiment classification datasets: Restaurant14 (Pontiki et al., 2016) and Laptop14 (Pontiki et al., 2016), and Twitter dataset (Dong et al., 2014). 4.2 Baselines We compare our model with the following strong baselines for cross-domain sentiment analysis, including DANN (Ganin et al., 2016), PBLM (Ziser and Reichart, 2018), HATN (Li et al., 2018), ACAN (Qu et al., 2019), IATN (Zhang et al., 2019a) and BERT-DAAT (Du et al., 2020). BERT-DAAT is regarded as the state-of-the-art model, which uses BERT for cross-domain sentiment analysis with adversarial training. We adopt the results of these baselines reported in (Du et al., 2020). For in-domain sentiment analysis, we compare our model with SentiLR-B (Ke et al., 2019), which is one of the state-of-the-art models based on BERT. BERT is extensively compared in our experiments. To exclude the impact of the pre-training dataset, we also compare S ENTI X with BERT˚ , which pre-trains on the same dataset with standard MLM task. Moreover, to v"
2020.coling-main.49,N18-1112,0,0.264706,"has become a promising direction, which transfers (invariant) sentiment knowledge from the source domain to the target domain1 . The major challenge here is that language expressions for sentimental text usually vary across different domains. For instance, “fast” has a positive sentiment towards “service” in the restaurant domain (Figure 1), while in the laptop domain, “fast” expresses a negative sentiment for “power consumption”. Furthermore, models trained on the source domain tend to overfit, since they learn domain-specific knowledge excessively. Therefore, many studies (Du et al., 2020; Ziser and Reichart, 2018; Li et al., 2018) propose to address this issue by extracting domain-invariant features. Recently, pre-trained language models like BERT (Devlin et al., 2019) have achieved the state-of-theart performance on multiple sentiment analysis tasks (Hoang et al., 2019; Munikar et al., 2019; Raffel et al., 2019). However, when they are directly applied to cross-domain sentiment analysis (Du et al., 2020), two problems arise: 1) Existing pre-trained models focus on learning the semantic content via self-supervision strategies, while ignoring sentiment-specific knowledge at the pre-training phrase; 2)"
2020.emnlp-main.129,buyko-etal-2010-genereg,0,0.0403765,"erent ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event annotation, but their different settings are complementary to our work. 7 Conclusion and Future work In this paper, we present a massive general domain event detection dataset (MAVEN), which significantly alleviates the data scarcity and low coverage problems of existing datasets. We conduct a thorough evaluation of the state-of-the-art ED models on MAVEN. Th"
2020.emnlp-main.129,P17-1038,0,0.0587272,"vember 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data"
2020.emnlp-main.129,P15-1017,0,0.690388,"uld recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated inst"
2020.emnlp-main.129,D18-1158,0,0.305267,"osition: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod"
2020.emnlp-main.129,N18-1076,0,0.0271789,"mains a challenging task and requires further research efforts. We also discuss further directions for general domain ED with empirical analyses. The source code and dataset can be obtained from https:// github.com/THU-KEG/MAVEN-dataset. 1 End-Position: 212 Transfer-Money: 198 Attack: 1543 Figure 1: Data distribution of the most widely-used ACE 2005 English dataset. It contains 33 event types, 599 documents and 5, 349 instances in total. is the first stage to extract event knowledge from text (Ahn, 2006) and also fundamental to various NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018; Yang et al., 2019). Introduction Event detection (ED) is an important task of information extraction, which aims to identify event triggers (the words or phrases evoking events in text) and classify event types. For instance, in the sentence “Bill Gates founded Microsoft in 1975”, an ED model should recognize that the word “founded” is the trigger of a Found event. ED ∗ Elect: 183 问ure: 142 Transfer-Ownership: 127 Phone-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the r"
2020.emnlp-main.129,N19-1423,0,0.0330633,"eural network baseline, which adopts the widely-used bi-directional long shortterm memory network to learn textual representations, and then uses the hidden states at the positions of trigger candidates for classifying event types. (3) MOGANED (Yan et al., 2019) is an advanced graph neural network (GNN) model. It proposes a multi-order graph attention network to effectively model the multi-order syntactic relations in dependency trees and improve ED. (4) DMBERT (Wang et al., 2019b) is a vanilla BERTbased model. It takes advantage of the effective pretrained language representation model BERT (Devlin et al., 2019) and also adopts the dynamic multi-pooling mechanism to aggregate features for ED. We use the BERTBASE architecture in our experiments. (5) Different from the above tokenlevel classification models, BiLSTM+CRF and BERT+CRF are sequence labeling models. To verify the effectiveness of modeling multiple event correlations, the two models both adopt the conditional random field (CRF) (Lafferty et al., 2001) as their output layers, which can model structured output dependencies. And they use BiLSTM and BERTBASE as their feature extractors respectively. As we manually tune hyperparameters and some t"
2020.emnlp-main.129,D19-1033,1,0.851706,"Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of"
2020.emnlp-main.129,D16-1264,0,0.124091,"Missing"
2020.emnlp-main.129,P19-1276,0,0.0175457,"traction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent complexity of event a"
2020.emnlp-main.129,D18-1156,0,0.0338402,"Missing"
2020.emnlp-main.129,P19-1353,0,0.0176268,"of ED and event extraction models (Ji and Grishman, 2008; Li et al., 2013; Chen et al., 2015; Feng et al., 2016; Liu et al., 2017; Zhao et al., 2018; Yan et al., 2019) are developed on these datasets. Our MAVEN follows the effective framework and extends it to numerous general domain event types and data instances. There are also various datasets defining the ED task in different ways. The early MUC series datasets (Grishman and Sundheim, 1996) define event extraction as a slot-filling task. The TDT corpus (Allan, 2012) and some recent datasets (Minard et al., 2016; Araki and Mitamura, 2018; Sims et al., 2019; Liu et al., 2019) follow the open-domain paradigm, which does not require models to classify events into pre-defined event types for better coverage but limits the downstream application of the extracted events. Some datasets are developed for ED on specific domains, like the biomedical domain (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Buyko et al., 2010; N´edellec et al., 2013), literature (Sims et al., 2019), Twitter (Ritter et al., 2012; Guo et al., 2013) and breaking news (Pustejovsky et al., 2003). These datasets are also typically small-scale due to the inherent co"
2020.emnlp-main.129,P19-1429,0,0.374622,"Missing"
2020.emnlp-main.129,W15-0812,0,0.406914,"problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1652–1671, c November 16–20, 2020. 2020 Association for Computational Linguistics ern sophisticated models. Moreover, the covered event types in existing datasets are limited. The ACE 2005 English dataset only contains 8 event types and 33 specific subtypes. The Rich ERE ontology (Song et al., 2015) used by TAC KBP challenges (Ellis et al., 2015, 2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, so"
2020.emnlp-main.129,L16-1699,0,0.0619031,"Missing"
2020.emnlp-main.129,P09-1113,0,0.125379,"2016) covers 9 event types and 38 subtypes. The coverage of these datasets is low for general domain events, which results in the models trained on these datasets cannot be easily transferred and applied on general applications. Recent research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low c"
2020.emnlp-main.129,P18-2066,0,0.180224,"-Write: 123 Start-Position: 118 Trial-Hearing: 109 Charge-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stab"
2020.emnlp-main.129,N19-1105,1,0.947896,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1584,1,0.922895,"research (Huang et al., 2016; Chen et al., 2017) has shown that the existing datasets suffering from the data scarcity and low coverage problems are now inadequate for benchmarking emerging methods, i.e., the evaluation results are difficult to reflect the effectiveness of novel methods. To tackle these issues, some works adopt the distantly supervised methods (Mintz et al., 2009) to automatically annotate data with existing event facts in knowledge bases (Chen et al., 2017; Zeng et al., 2018; Araki and Mitamura, 2018) or use bootstrapping methods to generate new data (Ferguson et al., 2018; Wang et al., 2019b). However, the generated data are inevitably noisy and homogeneous due to the limited number and low diversity of event facts and seed data instances. In this paper, we present MAVEN, a humanannotated massive general domain event detection dataset constructed from English Wikipedia and FrameNet (Baker et al., 1998), which can alleviate the data scarcity and low coverage problems: (1) Our MAVEN dataset contains 111, 611 different events, 118, 732 event mentions, which is twenty times larger than the most widely-used ACE 2005 dataset, and 4, 480 annotated documents in total. To the best of our"
2020.emnlp-main.129,D19-1582,0,0.510191,"-Indict: 106 The Other 20 Types (&lt;100 instances): 889 Corresponding author: Z.Liu (liuzy@tsinghua.edu.cn) Due to the rising requirement of event understanding, many efforts have been devoted to ED in recent years. The advanced models have been continuously proposed, including the feature-based models (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013; Araki and Mitamura, 2015) and advanced neural models (Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016; Feng et al., 2016; Ghaeini et al., 2016; Liu et al., 2017; Zhao et al., 2018; Chen et al., 2018; Ding et al., 2019; Yan et al., 2019). Nevertheless, the benchmark datasets for ED are upgraded slowly. As event annotation is complex and expensive, the existing datasets are mostly small-scale. As shown in Figure 1, the most widely-used ACE 2005 English dataset (Walker et al., 2006) only contains 599 documents and 5, 349 annotated instances. Due to the inherent data imbalance problem, 20 of its 33 event types only have fewer than 100 annotated instances. As recent neural methods are typically data-hungry, these small-scale datasets are not sufficient for training and stably benchmarking mod1652 Proceedings of the 2020 Conferenc"
2020.emnlp-main.129,C96-1079,0,\N,Missing
2020.emnlp-main.129,W06-0901,0,\N,Missing
2020.emnlp-main.129,P98-1013,0,\N,Missing
2020.emnlp-main.129,C98-1013,0,\N,Missing
2020.emnlp-main.129,P09-2093,0,\N,Missing
2020.emnlp-main.129,P06-4018,0,\N,Missing
2020.emnlp-main.129,P13-1024,0,\N,Missing
2020.emnlp-main.129,P08-1030,0,\N,Missing
2020.emnlp-main.129,P13-1008,0,\N,Missing
2020.emnlp-main.129,D14-1162,0,\N,Missing
2020.emnlp-main.129,P15-2060,0,\N,Missing
2020.emnlp-main.129,D15-1247,0,\N,Missing
2020.emnlp-main.129,doddington-etal-2004-automatic,0,\N,Missing
2020.emnlp-main.129,N16-1034,0,\N,Missing
2020.emnlp-main.129,P16-1025,0,\N,Missing
2020.emnlp-main.129,P17-1164,0,\N,Missing
2020.emnlp-main.129,N18-2058,0,\N,Missing
2020.emnlp-main.129,C18-1075,0,\N,Missing
2020.emnlp-main.129,D18-1259,0,\N,Missing
2020.emnlp-main.129,W13-2001,0,\N,Missing
2020.emnlp-main.129,P19-1521,0,\N,Missing
2020.emnlp-main.129,P16-2060,0,\N,Missing
2020.emnlp-main.129,P18-1201,0,\N,Missing
2020.emnlp-main.237,W15-4007,0,0.0409905,"Missing"
2020.emnlp-main.237,N19-1086,0,0.0410443,"ange over time, which makes them impractical in real-world applications. 2.2 Continual Learning Continual learning, also known as life-long learning, helps alleviate catastrophic forgetting and enables incremental training for stream data. Methods for continual learning in natural language processing (NLP) field can mainly be divided into two categories: (1) consolidation-based methods (Kirkpatrick et al., 2017; Zenke et al., 2017), which slow down parameter updating to preserve old knowledge, and (2) memory-based methods (Lopez-Paz and Ranzato, 2017; Shin et al., 2017; Chaudhry et al., 2019; Wang et al., 2019), which retain examples from old data for re-play upon learning the new data. Although continual learning has been widely studied in NLP (Sun et al., 2020) and computer vision (Kirkpatrick et al., 2017), its exploration on graph embedding is relatively rare. Sankar et al. (2018) seek to train graph embedding on constantly evolving data. However, it assumes the timestamp information is known beforehand, which hinders its application to other tasks. Song and Park (2018) extend the idea of regulation-based methods to continually learn graph embeddings which straightforwardly limits parameter upda"
2020.emnlp-main.275,D18-2029,0,0.0135609,"t training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shist = GRUdial [hxt ; hyt ] , shist t t−1 ∈ R  sel  , (4) kt kh d skh t = GRUsel ht , st−1 ∈ R ksel where hxt , hyt and ht t are sentence vectors of message xt , resp"
2020.emnlp-main.275,K18-1048,0,0.0282681,"erage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical M"
2020.emnlp-main.275,P17-1171,0,0.0217147,"Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropri"
2020.emnlp-main.275,P19-1258,1,0.851583,"p their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into tw"
2020.emnlp-main.275,N19-1423,0,0.100024,"see that the selected knowledge for response generation at training and inference is drawn from different distributions, i.e., the posterior distribution k ∼ qφ (Kt ) at training and the prior distribution k ∼ πθ (Kt ) at inference. Figure 1 clearly shows this discrepancy which will cause the decoder to have to generate with knowledge selected from the unfamiliar prior distribution. These issues lead to the gap between prior and posterior knowledge selection, which we try to deal with in this paper. Sentence Encoding. For any sentence sentt with Nw words at t-th turn, SKT uses a shared BERT (Devlin et al., 2019) to obtain the context aware word representations Hsent with d dims and t then converts them into the sentence representation hsent by mean pooling (Cer et al., 2018): t Hsent = BERT (sentt ) ∈ RNw ×d t .  hsent = Mean Hsent ∈ Rd t t (3) As a result, we obtain Hxt and hxt for the message kl xt , Hyt and hyt for the response2 yt , and Ht t and kl ht t for any knowledge sentence ktl ∈ Kt . Knowledge Selection. To utilize the dialogue history and selection history, two GRUs (Cho et al., 2014) are used to summarize them as corresponding states shist and skh t t with zero initialization:   d shi"
2020.emnlp-main.275,P16-1154,0,0.0375727,"on is based on the knowledge selected by the prior module, which is guided by the well-trained teacher, and we only update the green blocks at this stage. Finally, the knowledge ktsel is selected by sampling from the posterior distribution qφ (Kt ) = post at (Kt ) at training while selected with the highest probability over the prior distribution πθ (Kt ) = prior at (Kt ) at inference. Generation with Knowledge. SKT takes the concatenation of message xt and selected knowledge sentence ktsel as input and generates responses by the Transformer decoder (Vaswani et al., 2017) with copy mechanism (Gu et al., 2016). Though there are various models studying how to improve the generation quality based on the given knowledge, here, we simply follow the decoder of SKT and mainly focus on the knowledge selection issue. 3 Approach In this section, we show how to bridge the gap between prior and posterior knowledge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the k"
2020.emnlp-main.275,P84-1044,0,0.723364,"Missing"
2020.emnlp-main.275,P19-1002,1,0.84127,"election only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generati"
2020.emnlp-main.275,P18-1136,0,0.0236704,"nt responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompos"
2020.emnlp-main.275,P18-1160,0,0.0174108,"However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The"
2020.emnlp-main.275,D18-1255,0,0.46899,"jad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowl"
2020.emnlp-main.275,D19-1258,0,0.0160588,"lly use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3426–3437, c November 16–20, 2020. 2020 Association for Computational Linguistics nan et al., 2019) since the inappropriate knowledge selection may prevent the model from leveraging the knowledge accurately (Lian et al., 2019), or even lead to an inappropriate response. The example in Table 1"
2020.emnlp-main.275,D19-1187,0,0.0502611,"Missing"
2020.emnlp-main.275,W19-5917,0,0.0915067,"Missing"
2020.emnlp-main.275,D18-1073,0,0.0249398,"e to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-grounded dialogue (Dinan et al., 2019). Knowledge selection plays an important role in open-domain knowledge-grounded dialogue (Di3426 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proce"
2020.emnlp-main.275,P18-1205,0,0.0287203,"dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with e"
2020.emnlp-main.275,P19-1539,0,0.0176836,"ich leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently, Dinan et al. (2019) propose to decompose this task into two subproblems: knowledge selection and response generation. This pipeline framework has been widely used for the open domain setting (Chen et al., 2017; Min et al., 2018; Nie et al., 2019) and shows promising performance with explicit use of knowledge in knowledge-g"
2020.emnlp-main.275,P19-1426,1,0.820016,"Missing"
2020.emnlp-main.275,N19-1123,0,0.304558,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.275,P17-1061,0,0.121102,"ge selection in knowledge-grounded dialogue. Firstly, we design the Posterior Information Prediction Module (PIPM) to enhance the prior selection module with the necessary posterior information. Secondly, we design a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection. 3.1 information for knowledge selection at inference. Following the typical setting in latent variable models (Lian et al., 2019; Kim et al., 2020), we use the response in bag-of-words (BOW) format (Zhao et al., 2017) as the posterior information. Here we take the dialogue context and the knowledge pool as input to generate the posterior information. We firstly summarize as the query of  histthe xcontext  this module qPI = s ; h and use it to get the t t t−1 PI attention distribution at (Kt ) over the knowledge pool Kt by Equation 6. Then, we summarize the knowledge representation in the knowledge pool with the weights in aPI t (Kt ) considered: h 1 i kt ktL d (7) hPI = h , · · · , h · aPI t t (Kt ) ∈ R . t t Secondly, we take the summarization of the dialogue context and the knowledge pool as input and"
2020.emnlp-main.275,D19-1193,0,0.0263512,"nowledge sentences to generate different responses which help their selection decisions in turn. The prior knowledge selection only depends on context while the posterior knowledge selection means selection with context and response (Lian et al., 2019). Introduction Knowledge-grounded dialogue (Ghazvininejad et al., 2018) which leverages external knowledge to generate more informative responses, has become a popular research topic in recent years. Many researchers have studied how to effectively leverage the given knowledge to enhance dialogue understanding and/or improve dialogue generation (Zhao et al., 2019b; Sun et al., 2020; Madotto et al., 2018; Chen et al., 2019; Yavuz et al., 2019; Tang and Hu, 2019; Li et al., 2019; Zheng and Zhou, 2019; Niu et al., 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020). However, they usually use the pre-identified knowledge (Zhang et al., 2018; Moghe et al., 2018; Qin et al., 2019) which is not available in some real-world scenarios. And others leverage the retrieval system to get the knowledge which may contain noisy and irrelevant data (Chaudhuri et al., 2018; Parthasarathi and Pineau, 2018; Zhou et al., 2018; Gopalakrishnan et al., 2019). Recently"
2020.emnlp-main.298,2020.acl-main.142,0,0.0155763,", 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Han et al. (2020) suggest to study how RE models learn from context and mentions. Alt et al. (2020) also point out that there may exist shallow cues in entity mentions. However, there have not been systematical analyses about the topic and to the best of our knowledge, we are the first one to thoroughly carry out these studies. 6 Conclusion In this paper, we thoroughly study how textual context and entity mentions affect RE models respectively. Experiments and case studies prove that (i) both context and entity mentions (mainly as type information) provide critical information for relation extraction, and (ii) existing RE datasets may leak superficial cues through entity mentions and models"
2020.emnlp-main.298,P19-1279,0,0.356781,"n Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mention"
2020.emnlp-main.298,D14-1067,0,0.0277137,"ion provided by textual context and entity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder whic"
2020.emnlp-main.298,H05-1091,0,0.188054,"harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of s"
2020.emnlp-main.298,W97-1002,0,0.699414,"of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classify relations: textual context and entity mentions (names). From human intuition, textual context should be the main source of information for RE. Researchers have reached a consensus that there exist interpretable patterns in textual context that express relational facts. For example, in Figure 1, “... be founded ... by ...” is a pattern for the relation founded by. The early RE systems (Huffman, 1995; Califf and Mooney, 1997) formalize patterns into string templates and determine relations by 3661 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 3661–3672, c November 16–20, 2020. 2020 Association for Computational Linguistics matching these templates. The later neural models (Socher et al., 2012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity"
2020.emnlp-main.298,P04-1054,0,0.103347,"nd few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relat"
2020.emnlp-main.298,N19-1423,0,0.0521177,"se to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not discuss this line of work here for their promotion comes from relational knowledge of external sources, while we focus on text itself in the paper. Analysis of RE Ha"
2020.emnlp-main.298,D19-1649,1,0.865961,"ernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 2020). We do not disc"
2020.emnlp-main.298,D19-3029,1,0.896375,"Missing"
2020.emnlp-main.298,D18-1514,1,0.881235,"u et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in the form of entity embeddings, into BERT (Zhang et al., 2019; Peters et al., 2019; Liu et al., 20"
2020.emnlp-main.298,W09-2415,0,0.134815,"Missing"
2020.emnlp-main.298,P16-1200,1,0.903989,"sed methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject entity knowledge, in t"
2020.emnlp-main.298,P18-1136,0,0.0177318,"tity mentions in a typical RE scenario. From mentions, we can acquire type information and link entities to KGs, and access further knowledge about them. The IDs in the figure are from Wikidata. Introduction Relation extraction (RE) aims at extracting relational facts between entities from text, e.g., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actu"
2020.emnlp-main.298,N13-1095,0,0.0136486,"lopment of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text"
2020.emnlp-main.298,D15-1203,0,0.0908899,"through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the recent advance of pre-trained language models (Devlin et al., 2019), applying BERT-like models as the backbone of RE systems (Baldini Soares et al., 2019) has become a standard procedure. Based on BERT, Baldini Soares et al. (2019) propose matching the blanks, an RE-oriented pre-trained model to learn relational patterns from text. A different direction is to inject ent"
2020.emnlp-main.298,P09-1113,0,0.906212,"d to MTB (in the dotted box), our method samples data with better diversity, which can not only increase the coverage of entity types and diverse context but also reduce the possibility of memorizing entity names. we adopt the idea of contrastive learning (Hadsell et al., 2006), which aims to learn representations by pulling “neighbors” together and pushing “nonneighbors” apart. After this, “neighbor” instances will have similar representations. So it is important to define “neighbors” in contrastive learning and we utilize the information from KGs to to that. Inspired by distant supervision (Mintz et al., 2009), we assume that sentences with entity pairs sharing the same relation in KGs are “neighbors”. Formally, denote the KG we use as K, which is composed of relational facts. Denote two random sentences as XA and XB , which have entity mentions hA , tA and hB , tB respectively. We define XA and XB as “neighbors” if there is a relation r such that (hA , r, tA ) ∈ K and (hB , r, tB ) ∈ K. We take Wikidata as the KG since it can be easily linked to the Wikipedia corpus used for pretraining. When training, we first sample a relation r with respect to its proportion in the KG, and then sample a sentenc"
2020.emnlp-main.298,C14-1220,0,0.793202,"xt+Mention (C+M) This is the most widely-used RE setting, where the whole sentence 3662 Model C+M C+T OnlyC OnlyM OnlyT C+M CNN BERT MTB 0.547 0.683 0.691 0.591 0.686 0.696 0.441 0.570 0.581 0.434 0.466 0.433 0.295 0.277 0.304 Although her family was from Arkansas, she was born in Washington state, where ... Label: per:state of birth Prediction: per:state of residence Table 1: TACRED results (micro F1 ) with CNN, BERT and MTB on different settings. (with both context and highlighted entity mentions) is provided. To let the models know where the entity mentions are, we use position embeddings (Zeng et al., 2014) for the CNN model and special entity markers (Zhang et al., 2019; Baldini Soares et al., 2019) for the pre-trained BERT. Context+Type (C+T) We replace entity mentions with their types provided in TACRED. We use special tokens to represent them: for example, we use [person] and [date] to represent an entity with type person and date respectively. Different from Zhang et al. (2017), we do not repeat the special tokens for entity-length times to avoid leaking entity length information. Besides the above settings, we also adopt three synthetic settings to study how much information context or men"
2020.emnlp-main.298,W15-1506,0,0.0213763,"better pre-training technique is a reliable direction towards better RE. 2 Pilot Experiment and Analysis To study which type of information affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity"
2020.emnlp-main.298,D19-1005,0,0.0995482,"3) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type information of entities. (2"
2020.emnlp-main.298,D19-1250,0,0.0644701,"Missing"
2020.emnlp-main.298,D17-1004,0,0.334618,"formation affects existing neural RE models to make decisions, we first introduce some preliminaries of RE models and settings and then conduct pilot experiments as well as empirical analyses in this section. 2.1 Models and Dataset There are various NRE models proposed in previous work (refer to Section 5), and we select the following three representative neural models for our pilot experiments and analyses: CNN We use the convolutional neural networks described in Nguyen and Grishman (2015) and augment the inputs with part-of-speech, named entity recognition and position embeddings following Zhang et al. (2017). BERT BERT is a pre-trained language model that has been widely used in NLP tasks. We use BERT for RE following Baldini Soares et al. (2019). In short, we highlight entity mentions in sentences by special markers and use the concatenations of entity representations for classification. Matching the blanks (MTB) MTB (Baldini Soares et al., 2019) is an RE-oriented pre-trained model based on BERT. It is pre-trained by classifying whether two sentences mention the same entity pair with entity mentions randomly masked. It is fine-tuned for RE in the same way as BERT. Since it is not publicly releas"
2020.emnlp-main.298,P19-1139,1,0.940049,"012; Liu et al., 2013) prefer to encode patterns into distributed representations and then predict relations via representation matching. Compared with rigid string templates, distributed representations used in neural models are more generalized and perform better. Besides, entity mentions also provide much information for relation classification. As shown in Figure 1, we can acquire the types of entities from their mentions, which could help to filter out those impossible relations. Besides, if these entities can be linked to KGs, models can introduce external knowledge from KGs to help RE (Zhang et al., 2019; Peters et al., 2019). Moreover, for pre-trained language models, which are widely adopted for recent RE models, there may be knowledge about entities inherently stored in their parameters after pre-training (Petroni et al., 2019). In this paper, we carry out extensive experiments to study to what extent RE models rely on the two information sources. We find out that: (1) Both context and entity mentions are crucial for RE. As shown in our experiments, while context is the main source to support classification, entity mentions also provide critical information, most of which is the type infor"
2020.emnlp-main.298,P05-1053,0,0.416681,"for OnlyC and OnlyM. In the low resource and few-shot settings, it is harder for models to learn to extract relational patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 201"
2020.emnlp-main.298,C02-1151,0,0.404876,"nal patterns from context and easier to overfit to superficial cues of mentions, due to the limited training data. However, with the contrastive pre-training, our model can relatively take better use of textual context while avoiding being biased by entities, and outperform the other baselines by a large margin. 5 Related Work Development of RE RE of early days has gone through pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), kernel-based methods (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), graphical models (Roth and Yih, 2002, 2004), etc. Since Socher et al. (2012) propose to use recursive neural networks for RE, there have been extensive studies on neural RE (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015). To solve the data deficiency problem, researchers have developed two paths: distant supervision (Mintz et al., 2009; Min et al., 2013; Riedel et al., 2010; Zeng et al., 2015; Lin et al., 2016) to automatically collect data by aligning KGs and text, and few-shot learning (Han et al., 2018; Gao et al., 2019) to learn to extract new relations by only a handful of samples. Pre-training for RE With the r"
2020.emnlp-main.298,W04-2401,0,0.450234,"Missing"
2020.emnlp-main.298,D12-1110,0,0.488425,"., extracting the fact (SpaceX, founded by, Elon Musk) from the sentence in Figure 1. Utilizing the structured knowledge captured by RE, we can construct or complete knowledge graphs (KGs), and eventually support downstream applications like question answering (Bordes et al., 2014), dialog systems (Madotto et al., 2018) and search ∗ † Type: person ID: Q317521 Other info: citizenship: US occupation: entrepreneur … Equal contribution Corresponding author e-mail: liuzy@tsinghua.edu.cn engines (Xiong et al., 2017). With the recent advance of deep learning, neural relation extraction (NRE) models (Socher et al., 2012; Liu et al., 2013; Baldini Soares et al., 2019) have achieved the latest state-of-the-art results and some of them are even comparable with human performance on several public RE benchmarks. The success of NRE models on current RE benchmarks makes us wonder which type of information these models actually grasp to help them extract correct relations. The analysis of this problem may indicate the nature of these models and reveal their remaining problems to be further explored. Generally, in a typical RE setting, there are two main sources of information in text that might help RE models classi"
2020.emnlp-main.535,N19-1423,0,0.0267389,"Missing"
2020.emnlp-main.535,P17-1120,0,0.0277038,". 1 Introduction Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot, which further requires transiting among various domains, can be designed (Gao et al., 2019; Su et al., 2020). Nonetheless, even if constrained in a specific domain, current chatbots are still far from satisfactory. Unlike task-oriented systems that can be relatively well-resolved with handcrafted templates, human conversations feature a complex mixture of QA, chitchat, recommendation, etc. without pre-specified goals or conversational patterns (Dodge et al., 2016; Akasaki and Kaji, 2017; Shen et al., 2018). Selecting proper domain knowledge to support response generation at all the different situations is challenging (Milward and Beveridge, 2003; Lian et al., 2019; Shen et al., 2019). In this work, we direct our focus to the movie domain and present a large-scale, crowdsourced Chinese dataset with fine-grained annotations in hope of boosting the study towards a human-like closed-domain chatbot. A variety of dialogue datasets with grounded domain knowledge have already been proposed. However, they are collected either through (1) online forum crawling (Dodge et al., 2016; Gha"
2020.emnlp-main.535,2020.acl-main.634,1,0.828604,"le neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules. On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones. We further analyze the limits of our work and point out potential directions for future work 1 . 1 Introduction Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot, which further requires transiting among various domains, can be designed (Gao et al., 2019; Su et al., 2020). Nonetheless, even if constrained in a specific domain, current chatbots are still far from satisfactory. Unlike task-oriented systems that can be relatively well-resolved with handcrafted templates, human conversations feature a complex mixture of QA, chitchat, recommendation, etc. without pre-specified goals or conversational patterns (Dodge et al., 2016; Akasaki and Kaji, 2017; Shen et al., 2018). Selecting proper domain knowledge to support response generation at all the different situations is challenging (Milward and Beveridge, 2003; Lian et al., 2019; Shen et al., 2019). In this work,"
2020.emnlp-main.535,P19-1369,0,0.0180282,"focus to the movie domain and present a large-scale, crowdsourced Chinese dataset with fine-grained annotations in hope of boosting the study towards a human-like closed-domain chatbot. A variety of dialogue datasets with grounded domain knowledge have already been proposed. However, they are collected either through (1) online forum crawling (Dodge et al., 2016; Ghazvininejad et al., 2018; Liu et al., 2018; Zhou et al., 2018a; Qin et al., 2019), which are noisy, multi-party, mostly contain only single-exchange QA, or (2) crowdsourced (Zhu et al., 2017; Zhou et al., 2018b; Moon et al., 2019; Wu et al., 2019), which are small-scale and often created in an overconstrained setting like teacher-student (Moghe et al., 2018). Even for datasets crowd-sourced in unconstrained scenarios, suggestive domain knowledge is provided for humans before an utterance is provided. This would inevitably prompt humans to utilize these knowledge deliberately, yielding unnatural conversations simply connecting the knowledge (Dinan et al., 2019; Zhou et al., 2020). We show examples from other datasets in Appendix Table 10. In comparison, our dataset has the following advantages: ∗ Corresponding Authors. Work done before"
2020.emnlp-main.535,P18-1205,0,0.0611357,"Missing"
2020.emnlp-main.535,2020.acl-main.635,0,0.0110913,"t al., 2019), which are noisy, multi-party, mostly contain only single-exchange QA, or (2) crowdsourced (Zhu et al., 2017; Zhou et al., 2018b; Moon et al., 2019; Wu et al., 2019), which are small-scale and often created in an overconstrained setting like teacher-student (Moghe et al., 2018). Even for datasets crowd-sourced in unconstrained scenarios, suggestive domain knowledge is provided for humans before an utterance is provided. This would inevitably prompt humans to utilize these knowledge deliberately, yielding unnatural conversations simply connecting the knowledge (Dinan et al., 2019; Zhou et al., 2020). We show examples from other datasets in Appendix Table 10. In comparison, our dataset has the following advantages: ∗ Corresponding Authors. Work done before Xiaoyu Shen joins Amazon. † Work done while interning at Wechat. 1 Dataset and model are available at https://github. com/chin-gyou/MovieChats. 1. Natural: Crowdworkers chat in a free environment without further constraint or prompt in order to mimic the human daily conversations to the largest extent. 2. Large-scale: It covers 270k human dialogues with over 3M utterances, which is at least one order of magnitude larger than all the oth"
2020.emnlp-main.535,D18-1076,0,0.0407433,"Missing"
2020.emnlp-main.76,P17-2061,0,0.0657001,"Missing"
2020.emnlp-main.76,P05-1066,0,0.263519,"Missing"
2020.emnlp-main.76,N19-1312,1,0.860959,"nce. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the"
2020.emnlp-main.76,P16-1014,0,0.0417886,"Missing"
2020.emnlp-main.76,P15-1001,0,0.160488,"ent frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models hav"
2020.emnlp-main.76,W18-5712,0,0.0629753,"Missing"
2020.emnlp-main.76,D13-1176,0,0.243187,"Missing"
2020.emnlp-main.76,kocmi-bojar-2017-curriculum,0,0.0399389,"divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity."
2020.emnlp-main.76,D18-1149,0,0.0285784,"onsists of 1.25M sentence pairs from LDC corpora which has 27.9M Chinese words and 34.5M English words, respectively 2 . The data set MT02 was used as validation and MT03, MT04, MT05, MT06, MT08 were used for the test. We tokenized and lowercased English sentences using the Moses scripts3 , and segmented the Chinese sentences with the Stanford Segmentor4 . The two sides were further segmented into subword units using Byte-Pair Encoding (BPE) (Sennrich et al., 2016) with 30K merge operations separately. EN→RO. We used the preprocessed version of the WMT2016 English-Romanian dataset released by Lee et al. (2018) which includes 0.6M sentence pairs. We used news-dev 2016 for validation and news-test 2016 for the test. The two languages shared the same vocabulary generated with 40K merge operations of BPE. EN→DE. The training data is from WMT2016 which consists of about 4.5M sentences pairs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our meth"
2020.emnlp-main.76,2015.iwslt-evaluation.11,0,0.0427153,"airs with 118M English words and 111M German words. We chose the news test-2013 for validation and newstest 2014 for the test. 32K merge operations BPE were performed on both sides jointly. 4.2 • Baseline. The baseline system was implemented as the base model configuration in Vaswani et al. (2017) strictly. Since our method is further trained based on the pre-trained model at a low learning rate, we also trained another baseline model following the same procedures as our methods have except that all the target tokens share equal weights in the objective, denoted as Baseline-FT. • Fine Tuning (Luong and Manning, 2015). This model was first trained with all the training sentence pairs and then further trained with sentences containing more low-frequency tokens. To filter out sentences containing more low-frequency tokens, the method in Platanios et al. (2019) was adopted as our judging metric with a small modification: Systems We used the open-source toolkit called Fairseqpy (Edunov et al., 2017) released by Facebook as our Transformer system. 2 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 3 http://www.statmt.org/moses/ 4 https://nlp.stan"
2020.emnlp-main.76,P16-1100,0,0.0343734,"Missing"
2020.emnlp-main.76,P15-1002,0,0.168608,"s appear with different frequencies, which roughly obey the Zipf’s Law (Zipf, 1949). Table 1 shows that there is a serious imbalance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word b"
2020.emnlp-main.76,J14-3004,0,0.0273182,"al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we f"
2020.emnlp-main.76,P02-1040,0,0.106664,"ve (Equation 1), where all the target tokens have the same training weights. Then the model was further trained with the adaptive objective at a low learning rate. The weights were produced by the Exponential form (Equation 8). k) For computing stability, we used Count(y Cmedian instead of Count(yk ) in the weighting function, where Cmedian is the median of the token frequency. • Our K2. This system was trained following the same procedure as system Our Exp except that the training weights were produced by the Chi-Square form (Equation 9). The translation quality was evaluated by 4-gram BLEU (Papineni et al., 2002) with the multi-bleu.pl script. Besides, we used beam search with a beam size of 4 and a length penalty of 0.6 during the decoding process. 4.3 Hyperparameters There are two hyperparameters in our weighting functions, A and T. In our experiments, we fixed A to narrow search space and the overall weight range is [1, e]. We tuned another hyperparameter T on the validation data sets under the criteria proposed in section 3.2. The results are shown in Table 3. According to the results, the best hyperparameters differed across different language pairs. It is affected by the proportion of low-freque"
2020.emnlp-main.76,C18-1265,0,0.0163854,"et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al.,"
2020.emnlp-main.76,W19-6622,0,0.043297,"slation of the rare words with the help of the memory network or the pointer network (Zhao et al., 2018; Pham et al., 2018). In contrast, our methods can improve the translation performance without extra cost and can be combined with other techniques. Class Imbalance. Class imbalance means the total number of some classes of data is far less than the total number of other classes. This problem can be observed in various tasks (Wei et al., 2013; Johnson and Khoshgoftaar, 2019). In NMT, the class imbalance problem might be the underlying cause of, among others, the gender-biased output problem (Vanmassenhove et al., 2019a), the inability of MT system to handle morphologically richer language correctly (Passban et al., 2018), or the exposure bias problem (Ranzato et al., 2016; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work als"
2020.emnlp-main.76,W18-2712,0,0.0299546,"Missing"
2020.emnlp-main.76,I08-2084,0,0.00895525,"16; Shao et al., 2018; Zhang et al., 2019). The methods of trying to solve this can be divided into two types. The data-based methods (Baloch and Rafi, 2015; Ofek et al., 2017) make use of over- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the"
2020.emnlp-main.76,N19-1119,0,0.109348,"Missing"
2020.emnlp-main.76,P16-1162,0,0.864739,"rocessing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table 1 shows. Furthermore, current NMT models generally assign equal training weights to target tokens without considering their frequencies. It is very likely for NMT models to ignore the loss produced by the low-frequency tokens because of their small proportion in the tra"
2020.emnlp-main.76,D18-1510,1,0.904744,"Missing"
2020.emnlp-main.76,D17-1155,0,0.019346,"er- and undersampling to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then pro"
2020.emnlp-main.76,2020.acl-main.278,0,0.035646,"based methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic criteria based on the observations. Next,"
2020.emnlp-main.76,C18-1269,0,0.017616,"ng to reduce the imbalance. The algorithmbased methods (Zhou and Liu, 2005; Lin et al., 2017) give extra reward to different classes. Our method is algorithm-based which brings no extra cost. Word Frequency-based Methods. Some work also makes use of word frequency information to help learning, such as in the word segmentation (Sun et al., 2014) and term extraction (Frantzi et al., 1998; Vu et al., 2008). In NMT, word frequency information is used for curriculum learning (Kocmi and Bojar, 2017; Zhang et al., 2018; Platanios et al., 2019) and domain adaptation data selection (Wang et al., 2017; Zhang and Xiong, 2018; Gu et al., 2019). Wang et al. (2020) analyzed the miscalibration problem on the low-frequency tokens. Jiang et al. (2019) proposed a linear weighting function to solve the word imbalance problem in the dialogue response generation task. Compared with it, our method is more suitable for NMT. 7 Conclusion In this work, we focus on the token imbalance problem of NMT. We show that the output of vanilla NMT contains more high-frequency tokens and has lower lexical diversity. To alleviate this problem, we investigated existing adaptive objectives for other tasks and then proposed two heuristic cri"
2020.emnlp-main.76,P19-1426,1,0.880268,"Missing"
2020.emnlp-main.76,D18-1036,0,0.0674838,"ance between high-frequency tokens and lowfrequency tokens. NMT models rarely have the opportunity to learn and generate those groundtruth low-frequency tokens in the training process. 1035 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1035–1046, c November 16–20, 2020. 2020 Association for Computational Linguistics Some work tries to improve the rare word translation by maintaining phrase tables or back-off vocabulary (Luong et al., 2015; Jean et al., 2015; Li et al., 2016; Pham et al., 2018) or adding extra components (G¨ulc¸ehre et al., 2016; Zhao et al., 2018), which bring in extra training complexity and computing expense. Some NMT techniques which are based on smaller translation granularity can alleviate this issue, such as hybrid word-character-based model (Luong and Manning, 2016), BPE-based model (Sennrich et al., 2016) and word-piece-based model (Wu et al., 2016). These effective work alleviate the token imbalance phenomenon to a certain extent and become the de-facto standard in most NMT models. Although sub-word based NMT models have achieved significant improvements, they still face the token-level frequency imbalance phenomenon, as Table"
2020.emnlp-main.77,P19-1425,0,0.0625487,"Missing"
2020.emnlp-main.77,P18-1163,1,0.875966,"Missing"
2020.emnlp-main.77,D19-1082,0,0.0189134,"dependency to further improve the diversity and complementariness among Index of Layers 3 4 5 0.6 2 0.2 0.4 1 Recently Transformer-based models (Vaswani et al., 2017; Ott et al., 2018; Wang et al., 2019a) become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity. Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks (Wu et al., 2019; Zhao et al., 2019; Lioutas and Guo, 2020), improving the self-attention architecture(Fonollosa et al., 2019; Wang et al., 2019b; Hao et al., 2019), and deepening the Transformer architecture by dense connections(Wang et al., 2019a). Since our multi-unit framework makes no limitation about its unit, these models can be easily integrated into our multi-unit framework. There are also some works utilizing the power of multiple modules to capture complex feature representations in NMT. Shazeer et al. (2017) use a vast network and a sparse gated function to select from multiple experts (i.e., MLPs). Ahmed et al. (2017) train a weighted Transformer by replacing the multi-head attention by self-attention branches. Nevertheless, these models ign"
2020.emnlp-main.77,P17-4012,0,0.091145,"Missing"
2020.emnlp-main.77,W04-3250,0,0.142951,"ta, which is also tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016). We use newstest2017 as our validation set and newstest2018 as our test set, which contains 2001 and 3981 sentences, respectively. Evaluation. For evaluation, we train all the models with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respectively, and we select the model which performs the best on the validation set and report its performance on the test sets. We measure the caseinsensitive/case-sensitive BLEU scores using multibleu.perl 4 with the statistical significance test (Koehn, 2004) 5 for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case sensitive BLEU scores calculated by Moses mteval-v13a.pl script 6 . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention. Besides, since noise types like swapping and reordering are of no"
2020.emnlp-main.77,D18-1317,0,0.220094,"Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers (Bapna et al., 2018; Wang et al., 2019a) propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow (Tao et al.; Meng et al., 2019; Li et al., 2018, 2019) in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit models advance in expressiveness. Second, for the multiunit setting, one unit could mitigate the deficiency of other units and compose a more expressive network, in a complementary way. In this paper, we propose the Multi-Unit TransformErs (MUTE), which aim to promote the expressiveness of transformer models by introducing diverse and complementary parallel units. Merely combining multiple identical units in para"
2020.emnlp-main.77,W18-6301,0,0.0162281,"sequential dependency and bias module hardly affects the inference speed. 1054 4 0.300 3 0.275 2 0.225 2 0.200 1 1 2 3 Index of Units (a) MUTE 4 0.325 0.300 0.275 0.250 0.225 0.200 1 4 (b) MUTE + Bias 0.8 Conclusion In this paper, we propose Multi-Unit Transformers for NMT to improve the expressiveness by introducing diverse and complementary units. In addition, we propose two novel techniques, namely bias module and sequential dependency to further improve the diversity and complementariness among Index of Layers 3 4 5 0.6 2 0.2 0.4 1 Recently Transformer-based models (Vaswani et al., 2017; Ott et al., 2018; Wang et al., 2019a) become the de facto methods in Neural Machine Translation, owing to high parallelism and large model capacity. Some researchers devise new modules to improve the Transformer model, including combining the transformer unit with convolution networks (Wu et al., 2019; Zhao et al., 2019; Lioutas and Guo, 2020), improving the self-attention architecture(Fonollosa et al., 2019; Wang et al., 2019b; Hao et al., 2019), and deepening the Transformer architecture by dense connections(Wang et al., 2019a). Since our multi-unit framework makes no limitation about its unit, these models"
2020.emnlp-main.77,P16-1162,0,0.130721,"provement over dev set and test sets, compared with the “Transformer (Base)”. Bold represents the best performance. “†”: significantly better than “Transformer + Relative (Base)” (p < 0.05); “††”: significantly better than “Transformer + Relative (Base)” (p < 0.01). 2016) with 32k merge operations and a shared vocabulary for English and German. We use newstest2013 as our validation set and newstest2014 as our test set, which contain 3000 and 3003 sentences, respectively. For the WMT’18 Zh-En task, we use 18.4M preprocessed data, which is also tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016). We use newstest2017 as our validation set and newstest2018 as our test set, which contains 2001 and 3981 sentences, respectively. Evaluation. For evaluation, we train all the models with maximum 150k/300k/300k steps for NIST Zh-En, WMT En-De and WMT Zh-En, respectively, and we select the model which performs the best on the validation set and report its performance on the test sets. We measure the caseinsensitive/case-sensitive BLEU scores using multibleu.perl 4 with the statistical significance test (Koehn, 2004) 5 for NIST Zh-En and WMT’14 En-De, respectively. For WMT’18 Zh-En, we use case"
2020.emnlp-main.77,N18-2074,0,0.0268174,"pl script 6 . Model and Hyper-parameters. For all our experiments, we basically follow two model settings illustrated in (Vaswani et al., 2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size, 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention. Besides, since noise types like swapping and reordering are of no effect on Transformer models with absolute position information, the MUTE models are implemented using relative position information (Shaw et al., 2018). In addition, we only apply multi-unit methods to encoders, provided that the encoder is more crucial to model performance (Wang et al., 2019a). All experiments on MUTE models are conducted with TransformerBase setting. For the basic MUTE model, we use four identity units. As for the Biased MUTE and Sequentially Biased MUTE, we use four units including one identity unit, one swapping unit, one disorder unit and one masking unit. The sample rate pβ is set to 0.85. For more implementation details and experiments on sample rate, please refer to Appendix A and B. 4 https://github.com/moses-smt/mo"
2020.emnlp-main.77,P19-1176,0,0.307349,"rks and attention mechanism (Sutskever et al., 2014; Bahdanau 1 Code is available at https://github.com/Ellio ttYan/Multi Unit Transformer et al., 2014). Following the standard Sequence-toSequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and positionwise feed-forward network. Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers (Bapna et al., 2018; Wang et al., 2019a) propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow (Tao et al.; Meng et al., 2019; Li et al., 2018, 2019) in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit mode"
2020.emnlp-main.77,D19-1145,0,0.236302,"rks and attention mechanism (Sutskever et al., 2014; Bahdanau 1 Code is available at https://github.com/Ellio ttYan/Multi Unit Transformer et al., 2014). Following the standard Sequence-toSequence architecture, Transformer models consist of two essential components, namely the encoder and decoder, which rely on stacking several identical layers, i.e., multihead attentions and positionwise feed-forward network. Multihead attentions and position-wise feedforward network, together as a basic unit, plays an essential role in the success of Transformer models. Some researchers (Bapna et al., 2018; Wang et al., 2019a) propose to improve the model capacity by stacking this basic unit many times, i.e., deep Transformers, and achieve promising results. Nevertheless, as an orthogonal direction, investigation over multiple parallel units draws little attention. Compared with single unit models, multiple parallel unit layout is more expressive to capture complex information flow (Tao et al.; Meng et al., 2019; Li et al., 2018, 2019) in two aspects. First, this multiple-unit layout boosts the model by its varied feature space composition and different attentions over inputs. With this diversity, multi-unit mode"
2020.emnlp-main.77,D18-1100,0,0.0522988,"Missing"
2020.findings-emnlp.299,D14-1181,0,0.00244396,"tor. In a max-min game, the SCTKG generator generates essays to make discriminator consider them semantically match with given topics. Discriminator tries to distinguish the generated essays from real essays. In detail, suppose there are a total of m topics, the discriminator produces a sigmoid probability distribution over (m + 1)classes. The score at (m + 1)th index represents the probability that the sample is the generated text. The score at the j th (i ∈ {1, · · · , m}) index represents the probability that it belongs to the real text with the j th topic. Here the discriminator is a CNN (Kim, 2014) text classifier. 3.3 Training We introduce our two stage training method in this section. Stage 1: Similar to a conventional CVAE model, The loss of our SCTKG generator −logp(Y |c) can be expressed as: − L (θ; φ; c; Y )cvae = LKL + Ldecoder = KL (qφ (z|Y, c)kpθ (z|c)) (5) − Eqφ (z|Y,c) (log pD (Y |z, c)) . 2 As shown in Figure 2, in the topic knowledge graph, red circles denote the topic words and blue circles denote their neighboring concepts. Since we have already encoded topic information in the encoder, the graph vector gt in this section mainly focuses on the neighboring concept to assis"
2020.findings-emnlp.299,P02-1040,0,0.123797,"training set. This classifier achieves an accuracy of 0.83 on the test set. During training, the target sentiment labels s is computed by the sentiment classifier automatically. During inference, users can input any sentiment labels to control the sentiment for sentence generation. 4.2 Implementation Details We use the 200-dim pre-trained word embeddings provided by Song et al. (2018) and dimension of sentiment embeddings is 32. The vocabulary size is 50,000 and the batch size is 64. We use a manually tuning method to choose the hyperparameter values and the criterion used to select is BLEU (Papineni et al., 2002a). We use GRU with hidden size 512 for both encoder and decoder and the size of latent variables is 300. We implement the model with 3 The dataset can be download by https://pan. baidu.com/s/17pcfWUuQTbcbniT0tBdwFQ 4 https://github.com/baidu/Senta Tensorflow5 . The number of parameters is 68M and parameters of our model were randomly initialized over a uniform distribution [-0.08,0.08]. We pre-train our model for 80 epochs with the MLE method and adversarial training for 30 epochs. The average runtime for our model is 30 hours on a Tesla P40 GPU machine, which adversarial training takes most"
2020.findings-emnlp.299,N18-2028,0,0.0137321,"ally with three categories, i.e., positive, negative, neutral. This dataset was divided into a training set, validation set, and test set. We use an open-source Chinese sentiment classifier Senta4 to finetune on our manually-label training set. This classifier achieves an accuracy of 0.83 on the test set. During training, the target sentiment labels s is computed by the sentiment classifier automatically. During inference, users can input any sentiment labels to control the sentiment for sentence generation. 4.2 Implementation Details We use the 200-dim pre-trained word embeddings provided by Song et al. (2018) and dimension of sentiment embeddings is 32. The vocabulary size is 50,000 and the batch size is 64. We use a manually tuning method to choose the hyperparameter values and the criterion used to select is BLEU (Papineni et al., 2002a). We use GRU with hidden size 512 for both encoder and decoder and the size of latent variables is 300. We implement the model with 3 The dataset can be download by https://pan. baidu.com/s/17pcfWUuQTbcbniT0tBdwFQ 4 https://github.com/baidu/Senta Tensorflow5 . The number of parameters is 68M and parameters of our model were randomly initialized over a uniform dis"
2020.findings-emnlp.299,speer-havasi-2012-representing,0,0.0352498,"sues, we propose a novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational auto-encoder (CVAE) framework. To control the sentiment of the text, we inject the sentiment information in the encoder and decoder of our model to control the sentiment from both sentence level and word level. The sentiment labels are provided by a sentiment classifier during training. To fully utilize the knowledge, the model retrieves a topic knowledge graph from a largescale commonsense knowledge base ConceptNet (Speer and Havasi, 2012). Different from Yang et al. (2019), we preserve the graph structure of the knowledge base and propose a novel Topic Graph Attention (TGA) mechanism. TGA attentively reads the knowledge graphs and makes the full use of the structured, connected semantic information from the graphs for a better generation. In the meantime, to make the generated essays more closely surround the semantics of all input topics, we adopt adversarial training based on a multi-label discriminator. The discriminator provides the reward to the generator based on the coverage of the output on the given topics. Our contri"
2020.findings-emnlp.299,P19-1193,0,0.198648,"o sentences for each generated essay and denote positive sentences in red and negative sentences in blue. Sentences without sentiment label are showed in black. Introduction Topic-to-essay generation (TEG) task aims at generating human-like paragraph-level texts with only several given topics. It has plenty of practical applications, e.g., automatic advertisement generation, intelligent education, or assisting in keyword-based news writing (Lepp¨anen et al., 2017). Because of its great potential in practical use and scientific research, TEG has attracted a lot of interest. (Feng et al., 2018; Yang et al., 2019). However, In TEG, two problems are left to be solved: the neglect of sentiment beneath the text and the insufficient utilization of topic-related knowledge. ∗ This work is done when Lin Qiao was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China A well-performed essay generator should be able to generate multiple vivid and diverse essays when given the topic words. However, previous work tends to generate dull and generic texts. One of the reason is that they neglect the sentiment factor of the text. By modeling and controlling the sentiment of generated sentences, we can"
2020.findings-emnlp.299,W17-3528,0,0.0321639,"Missing"
2020.findings-emnlp.299,D18-1353,0,0.026516,"a commonsense knowledge to enrich the input information and adopt adversarial training to enhancing topic-consistency. However, both of them fail to consider the sentiment factor in the essay generation and fully utilize the external knowledge base. These limitations hinder them from generating high-quality texts. Besides, Chinese poetry generation is similar to our task, which can also be regarded as a topicto-sequence learning task. Li et al. (2018) adopt CVAE and adversarial training to generate diverse poetry. Yang et al. (2017) use CVAE with hybrid decoders to generate Chinese poems. And Yi et al. (2018) use reinforcement learning to directly improve the diversity criteria. However, their models are not directly applicable to TEG task. Because they do not take knowledge into account, their models cannot generate long and meaningful unstructured essays. Controllable Text Generation. Some work has explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2017) use naturally annotated emoji Twitter data for emotional response generation. Wang and Wan (2018) propose adversarial training to control the sentiment of the texts. Chen et al. (2019) propose a semi-supervi"
2020.findings-emnlp.299,D18-1423,0,0.0418389,"Missing"
2020.findings-emnlp.299,P17-1061,0,0.0232594,"the topic words and blue circles denote their neighboring concepts. Since we have already encoded topic information in the encoder, the graph vector gt in this section mainly focuses on the neighboring concept to assist the generation. Topic Label Discriminator Here, θ and φ are the parameters of the prior network and recognition network, respectively. Intuitively, Ldecoder maximizes the sentence generation probability after sampling from the recognition net3339 work, while LKL minimizes the distance between the prior and recognition network. Besides, we use the annealing trick and BOW-loss (Zhao et al., 2017) to alleviate the vanishing latent variable problem in VAE training. Stage 2: After trained the SCTKG generator with equation (5), inspired by SeqGan (Yu et al., 2017), we adopt adversarial training between the generator and the topic label discriminator described in section 3.2. We refer reader to Yu et al. (2017) and Yang et al. (2019) for more details. 4 Experiments 4.1 Datasets We conduct experiments on the ZHIHU corpus (Feng et al., 2018). It consists of Chinese essays3 whose length is between 50 and 100. We select topic words based on frequency and remove rare topic words. The total numb"
2020.lrec-1.451,W19-5206,0,0.0153285,"e most probable words for each topic. 4.2 Incorporating Topic Information 4.2.1 Side Constraints A simple yet effective way of integrating information at the sentence level is to use side constraints (Sennrich et al., 2016a), which consists in prepending (or appending) an extra token to the input sentence associated with the feature to which you wish to bias the translation. These feature tokens are added to the vocabulary and treated as normal tokens. It has previously been used for a range of phenomena including politeness (in the original article) and domain adaptation (Kobus et al., 2017; Caswell et al., 2019). We therefore prepend a special token to the beginning of the source sequence to represent the topic of the current source section (cf. Figure 3). Our intuition is that the model will learn to associate the presence of each topic with related target vocabulary and in a way that provides more adapted information than topics trained at the document level. &lt;topic64&gt; Elle a débuté en août 2013 avec “My Student Teacher”. &lt;topic91&gt; Le 21 mars 2014, NC.A sort son single digital “Hello Baby”. &lt;topic91&gt; Elle sort ensuite son premier mini-album le 9 avril. Figure 3: An example of topics used as side co"
2020.lrec-1.451,P14-6007,0,0.0285471,"rench-English, Bulgarian-English) from Wikipedia biographies, which we extract automatically, preserving the boundaries of sections within the articles. Keywords: machine translation, document structure, corpus creation, context, Wikipedia, parallel corpus 1 Introduction Considerable progress has been made in machine translation (MT) thanks to the use of neural MT (NMT). While most NMT systems translate at the level of individual sentences, following similar practices in statistical MT (SMT), there has been significant interest in recent years in using document context to improve translation (Hardmeier, 2014; Bawden, 2018; Wang, 2019). However the intermediate level of the internal structure of documents, particularly for documents with regular sub-structure, could also provide useful information to improve MT. Documents are rarely without internal structure, and certain document types (e.g. biographies, scientific articles and encyclopedia entries) are characterised by a more well-defined and regular structure than others. In such documents, the structure is explicitly defined by sections associated with headings dealing with different aspects of the main subject. These sections, many of which c"
2020.lrec-1.451,kobus-etal-2017-domain,0,0.0167663,"so used to obtain the most probable words for each topic. 4.2 Incorporating Topic Information 4.2.1 Side Constraints A simple yet effective way of integrating information at the sentence level is to use side constraints (Sennrich et al., 2016a), which consists in prepending (or appending) an extra token to the input sentence associated with the feature to which you wish to bias the translation. These feature tokens are added to the vocabulary and treated as normal tokens. It has previously been used for a range of phenomena including politeness (in the original article) and domain adaptation (Kobus et al., 2017; Caswell et al., 2019). We therefore prepend a special token to the beginning of the source sequence to represent the topic of the current source section (cf. Figure 3). Our intuition is that the model will learn to associate the presence of each topic with related target vocabulary and in a way that provides more adapted information than topics trained at the document level. &lt;topic64&gt; Elle a débuté en août 2013 avec “My Student Teacher”. &lt;topic91&gt; Le 21 mars 2014, NC.A sort son single digital “Hello Baby”. &lt;topic91&gt; Elle sort ensuite son premier mini-album le 9 avril. Figure 3: An example of"
2020.lrec-1.451,P07-2045,0,0.00876676,"Fr 78 1130 4.56 3.18 82 1198 4.83 3.02 Bg-En En Bg 154 2539 5.37 3.06 25 273 3.41 3.2 Table 2: Parallel test set statistics. We split the set in two, depending on which language was the original text’s language (as opposed to the translation). Ave. #sents/sec Ave. #secs/doc Total #docs Total #sents Zh Fr Bg En 7.05 3.29 68,433 1,586,194 5.55 2.82 167,484 2,619,842 5.99 3.06 56,275 1,029,626 7.39 3.97 99,106 2,904,641 Table 3: Monolingual corpus statistics. 4. Cleaning of aligned parallel articles to avoid very long sentences or mismatches between source and target length using Moses scripts (Koehn et al., 2007). 5. Division of data into training, validation and test sets Extraction of biographies Relevant Wikipedia articles were obtained by filtering Wikimedia dumps using their metadata. We extracted only biographies using the category of the articles with keywords related to people (e.g. “person”, “writer”, “politician”) and using the presence of a “Biography” section as indicators. Parallel biography data was obtained by selecting articles that were indicated in the metadata as translations (where either language was the original). Monolingual data contains all biography data for a given language,"
2020.lrec-1.451,C18-1050,0,0.333828,"ography corpora in the four languages.1 To test the usefulness of exploiting document substructure for NMT, we conduct experiments to compare methods us1 Available at https://github.com/radidd/ Doc-substructure-NMT under a CC-BY-SA 3.0 licence. 3657 ing topic information at the level of document sections. As in (Louis and Webber, 2014), we use topic modelling to model article sections. We compare two methods of integrating this information into NMT. The first uses side constraints (Sennrich et al., 2016a) and involves prepending topic information to source sentences. The second, adapted from (Kuang et al., 2018), uses caches containing relevant vocabulary. It is similar to the approach used by Louis and Webber (2014) but applied to NMT. We test these methods on three language directions (Fr→En, Zh→En, Bg→En). Our main contributions can be summarised as follows: • The automatic creation of parallel and monolingual datasets of Wikipedia biographies with document substructure information for Fr-En, Bg-En and Zh-En • Experiments comparing two methods of exploiting document sub-structure in NMT, applied to three language directions: Fr→En, Zh→En and Bg→En. The remainder of the paper is organised as follow"
2020.lrec-1.451,E14-1017,0,0.0680819,"den@ed.ac.uk Abstract Current approaches to machine translation (MT) either translate sentences in isolation, disregarding the context they appear in, or model context at the level of the full document, without a notion of any internal structure the document may have. In this work we consider the fact that documents are rarely homogeneous blocks of text, but rather consist of parts covering different topics. Some documents, such as biographies and encyclopedia entries, have highly predictable, regular structures in which sections are characterised by different topics. We draw inspiration from Louis and Webber (2014) who use this information to improve statistical MT and transfer their proposal into the framework of neural MT. We compare two different methods of including information about the topic of the section within which each sentence is found: one using side constraints and the other using a cache-based model. We create and release the data on which we run our experiments – parallel corpora for three language pairs (Chinese-English, French-English, Bulgarian-English) from Wikipedia biographies, which we extract automatically, preserving the boundaries of sections within the articles. Keywords: mach"
2020.lrec-1.451,N19-1313,0,0.0145821,"text to the MT model. In their structured model they load the topic cache with words that are specific to the given section, as opposed to the whole document. They also clear and reload the caches at section boundaries instead of at document boundaries. They show that the structured model has an advantage over the model that treats documents as a whole, with topic information found to be particularly useful. While current work in NMT has not made use of document structure, there have been many efforts to supply NMT models with document-level context to improve translation (Voita et al., 2018; Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018). One way to integrate structural information into NMT is to adapt approaches to documentlevel NMT to consider section boundaries. Of particular relevance is the work of Kuang et al. (2018), who supply contextual information using caches. Their model involves two caches, a topic cache and a dynamic cache, which contain words that are important for the document-level context. The topic cache consists of words related to the document’s topic, while the dynamic cache is updated to contain the translations of previous sentences within the same document"
2020.lrec-1.451,D18-1325,0,0.0186764,"d model they load the topic cache with words that are specific to the given section, as opposed to the whole document. They also clear and reload the caches at section boundaries instead of at document boundaries. They show that the structured model has an advantage over the model that treats documents as a whole, with topic information found to be particularly useful. While current work in NMT has not made use of document structure, there have been many efforts to supply NMT models with document-level context to improve translation (Voita et al., 2018; Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018). One way to integrate structural information into NMT is to adapt approaches to documentlevel NMT to consider section boundaries. Of particular relevance is the work of Kuang et al. (2018), who supply contextual information using caches. Their model involves two caches, a topic cache and a dynamic cache, which contain words that are important for the document-level context. The topic cache consists of words related to the document’s topic, while the dynamic cache is updated to contain the translations of previous sentences within the same document and the current sentence up to the current ti"
2020.lrec-1.451,P02-1040,0,0.109393,"ic is a projection from source to target and the dynamic cache is loaded with words from previously translated sentences. Comparative Systems In order to assess the usefulness of document structure, we provide contrastive experiments whereby both methods are applied to the full Wikipedia articles, disregarding section boundaries, the difference being that topic models are trained on whole documents rather than individual document sections. We will refer to the models using section boundaries as “section-level” and to the ones that do not as “document-level”. 5.3 Results We report Bleu scores (Papineni et al., 2002), calculated using SacreBleu (Post, 2018) on our tests sets for each of our models in Table 4. We also provide scores for the pretrained model (out-of-domain baseline) and the in-domain fine-tuned baseline. Improved results that are statistically significant5 from the in-domain baseline results are indicated (* for p ≤ 0.05, ** for p ≤ 0.01 and ***p ≤ 0.001). Effect of fine-tuning Fine-tuning the out-of-domain baseline gives improved results for all language pairs, with the greatest gain being seen for Fr-En with a +6.4 increase in Bleu. The smallest difference is seen for Zh-En, where finetun"
2020.lrec-1.451,W18-6319,0,0.0120058,"amic cache is loaded with words from previously translated sentences. Comparative Systems In order to assess the usefulness of document structure, we provide contrastive experiments whereby both methods are applied to the full Wikipedia articles, disregarding section boundaries, the difference being that topic models are trained on whole documents rather than individual document sections. We will refer to the models using section boundaries as “section-level” and to the ones that do not as “document-level”. 5.3 Results We report Bleu scores (Papineni et al., 2002), calculated using SacreBleu (Post, 2018) on our tests sets for each of our models in Table 4. We also provide scores for the pretrained model (out-of-domain baseline) and the in-domain fine-tuned baseline. Improved results that are statistically significant5 from the in-domain baseline results are indicated (* for p ≤ 0.05, ** for p ≤ 0.01 and ***p ≤ 0.001). Effect of fine-tuning Fine-tuning the out-of-domain baseline gives improved results for all language pairs, with the greatest gain being seen for Fr-En with a +6.4 increase in Bleu. The smallest difference is seen for Zh-En, where finetuning actually degrades performance on the"
2020.lrec-1.451,W11-4624,0,0.0113975,"ticles can change significantly after being translated. This is illustrated in Figure 2, which shows some examples of non-exact sentence alignment due to non-exact translation or post-editions. Some degree of such noise is therefore expected in the data. We attempt to reduce it as much as possible by filtering based on automatic alignment scores. For Fr-En, we used Hunalign (Varga et al., 2005), while for Zh-En and Bg-En we used previously trained MT models to translate the source text into English and align the sentences based on similarity for Zh-En and BLEU score for Bg-En using Bleualign (Sennrich and Volk, 2011). Future versions of the corpus can explore how to further reduce the level of noise left in the data. Dataset partitions Parallel data is divided into training, validation and test sets. For each language pair, we created two test sets, based on the original language the articles were written in (e.g. for Bg-En, one test set contains articles originally written in English and translated into Bulgarian, and the other contains articles originally written in Bulgarian and translated into English).3 Statistics for the training and validation sets are given in Table 1 and for the test sets in Tabl"
2020.lrec-1.451,N16-1005,0,0.330097,"sh (Zh-En) - preserving document and document sub-structure information. We also collect monolingual Wikipedia biography corpora in the four languages.1 To test the usefulness of exploiting document substructure for NMT, we conduct experiments to compare methods us1 Available at https://github.com/radidd/ Doc-substructure-NMT under a CC-BY-SA 3.0 licence. 3657 ing topic information at the level of document sections. As in (Louis and Webber, 2014), we use topic modelling to model article sections. We compare two methods of integrating this information into NMT. The first uses side constraints (Sennrich et al., 2016a) and involves prepending topic information to source sentences. The second, adapted from (Kuang et al., 2018), uses caches containing relevant vocabulary. It is similar to the approach used by Louis and Webber (2014) but applied to NMT. We test these methods on three language directions (Fr→En, Zh→En, Bg→En). Our main contributions can be summarised as follows: • The automatic creation of parallel and monolingual datasets of Wikipedia biographies with document substructure information for Fr-En, Bg-En and Zh-En • Experiments comparing two methods of exploiting document sub-structure in NMT,"
2020.lrec-1.451,P16-1162,0,0.682179,"sh (Zh-En) - preserving document and document sub-structure information. We also collect monolingual Wikipedia biography corpora in the four languages.1 To test the usefulness of exploiting document substructure for NMT, we conduct experiments to compare methods us1 Available at https://github.com/radidd/ Doc-substructure-NMT under a CC-BY-SA 3.0 licence. 3657 ing topic information at the level of document sections. As in (Louis and Webber, 2014), we use topic modelling to model article sections. We compare two methods of integrating this information into NMT. The first uses side constraints (Sennrich et al., 2016a) and involves prepending topic information to source sentences. The second, adapted from (Kuang et al., 2018), uses caches containing relevant vocabulary. It is similar to the approach used by Louis and Webber (2014) but applied to NMT. We test these methods on three language directions (Fr→En, Zh→En, Bg→En). Our main contributions can be summarised as follows: • The automatic creation of parallel and monolingual datasets of Wikipedia biographies with document substructure information for Fr-En, Bg-En and Zh-En • Experiments comparing two methods of exploiting document sub-structure in NMT,"
2020.lrec-1.451,E17-3017,0,0.0320193,"s using our in-domain biography data. This pretraining ensures that all models are strong, having been trained on large quantities of data in addition to in-domain data. We also train an in-domain baseline by fine-tuning the pretrained model on our in-domain data, without additional information about document structure. Architecture and Settings The parameters of the Transformer are set to standard values (Vaswani et al., 2017): the encoder and decoder have 6 stacked layers, the embedding size is 512 and the feed-forward network hidden layer dimensionality is 2048. We use the Nematus toolkit (Sennrich et al., 2017) for all models. Cache model parameters For cache-based models, we fix the size of the two caches to 100 words each. The scoring feed-forward network has hidden dimensions 1000 and 3661 Figure 5: The neural cache model integrated with the Transformer decoder. En orig. Zh-En Zh orig. all En orig. Fr-En Fr orig. all En orig. Bg-En Bg orig. all Out-of-domain baseline In-domain baseline 14.3 16.9 11.6 10.8 13.1 14.0 45.3 52.4 46.1 51.8 45.7 52.1 22.0 24.4 21.9 22.0 22.0 24.2 Document-level side constraints Section-level side constraints Document-level cache-based Section-level cache-based 17.1 17."
2020.lrec-1.451,P18-1117,0,0.0248434,"nd its preceding context to the MT model. In their structured model they load the topic cache with words that are specific to the given section, as opposed to the whole document. They also clear and reload the caches at section boundaries instead of at document boundaries. They show that the structured model has an advantage over the model that treats documents as a whole, with topic information found to be particularly useful. While current work in NMT has not made use of document structure, there have been many efforts to supply NMT models with document-level context to improve translation (Voita et al., 2018; Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018). One way to integrate structural information into NMT is to adapt approaches to documentlevel NMT to consider section boundaries. Of particular relevance is the work of Kuang et al. (2018), who supply contextual information using caches. Their model involves two caches, a topic cache and a dynamic cache, which contain words that are important for the document-level context. The topic cache consists of words related to the document’s topic, while the dynamic cache is updated to contain the translations of previous sentences with"
2020.lrec-1.451,W19-5208,0,0.0272835,"n and previous context within the boundaries of the current section. While our first method is simple to implement and train, the second provides more fine-grained information about each section within an article. 4.1 Modelling Section Topics While Wikipedia section headings are a useful indication of the section boundaries, they are not necessarily optimal for determining the granularity of different topics. As can be 3 It is interesting to separate out these two translation directions, as the translation direction can have an effect on ease of translation due to the “translationese” effect (Zhang and Toral, 2019; Graham et al., 2019) 3659 Source Source gloss Reference Ils se marient le 30 juin 1894 à Paris. ‘They married on 30 June 1894 in Paris.’ Le 11 décembre 2013, elle est nommée déléguée générale du Québec à New York. ‘On 11 December 2013, she was appointed Quebec Delegate General in New York’ They married on 30 June 1894 in Paris and had two daughters. On 11 December 2013 Poirier was appointed Quebec Delegate General in New York. 毛贻昌 选中了罗一秀。 1908年，罗 罗一 秀与 毛泽东举办了婚礼 ‘Mao Yichang selected Luo Yixiu.’ ‘In 1908, Luo Yixiu and Mao Zedong held a wedding.’ He selected Luo Yixiu in either late 1907 or 1"
2020.lrec-1.451,D18-1049,0,0.0192995,". In their structured model they load the topic cache with words that are specific to the given section, as opposed to the whole document. They also clear and reload the caches at section boundaries instead of at document boundaries. They show that the structured model has an advantage over the model that treats documents as a whole, with topic information found to be particularly useful. While current work in NMT has not made use of document structure, there have been many efforts to supply NMT models with document-level context to improve translation (Voita et al., 2018; Maruf et al., 2019; Zhang et al., 2018; Miculicich et al., 2018). One way to integrate structural information into NMT is to adapt approaches to documentlevel NMT to consider section boundaries. Of particular relevance is the work of Kuang et al. (2018), who supply contextual information using caches. Their model involves two caches, a topic cache and a dynamic cache, which contain words that are important for the document-level context. The topic cache consists of words related to the document’s topic, while the dynamic cache is updated to contain the translations of previous sentences within the same document and the current sen"
2020.lrec-1.451,tiedemann-2012-parallel,0,0.0485493,"Missing"
2020.semeval-1.48,P19-1470,0,0.057231,"Missing"
2020.semeval-1.48,N19-1423,0,0.0118489,"., 2020). Finally, the flatten input can be handled by the pre-trained model described next. 3.2 Reasoning Module To improve the reasoning ability of the model, we propose to model the input sentences in a multi-level perspective which combines entities with the entire statements or possible explanations. First of all, we utilize the pre-trained model to process the above embedding EUi obtained in the previous step to provide ˆ base . This is not only because the pre-trained architectures have achieved a high-level representation as E Ui state-of-the-art performance in a variety of NLP tasks (Devlin et al., 2019; Yang et al., 2019; Lan et al., 2020), but also because of the objective of language model, which can serve as a metric to estimate the possibility of a sentence being commonsensical. In this part, we choose an context-sensitive pre-trained model, RoBERTa (Liu et al., 2019), which is composed of N -layer transformer encoder (Vaswani et al., 403 Attention Mask Matrix 0 1 2 3 4 5 6 7 8 9 1011 12 1314 15 16 1718 19 Sugar coffee is used to make sweet Low probability, discard UsedFor IsA carbohydrate sweetening coffee RelatedTo IsA weight: 3.46 weight: 1.0 weight: 2.83 weight: 2.65 sweet food suga"
2020.semeval-1.48,K17-1010,0,0.0253456,"pen research question to balance the tradeoff between noise and the amount of incorporated commonsense from knowledge base consisting of all types 405 Task Examples Subtask A Which statement of the two is against commonsense? S1: he put an elephant into the fridge. × S2: he put a turkey into the fridge. Subtask B Why is “he put an elephant into the fridge” against commonsense? √ A. an elephant is much bigger than a fridge. B. elephants are usually gray while fridges are usually white. × C. an elephant cannot eat a fridge. × Table 1: Data description. of entity nodes (Weissenborn et al., 2017; Khashabi et al., 2017). The proposed KEmb and KEGAT have provided possible solutions to alleviate the noise caused by incorporated structured knowledge to some extent. These two modules share a same goal of identifying the most commonsensical external entities and discarding the irreverent ones. Take the KEGAT for instance, we achieve this based on both entity-level and sentence-level inference thoroughly described in the previous Reasoning Module part. Furthermore, several unimportant types of edges have been removed to avoid unnecessary noises, such as “/r/ExternalURL”, “/r/DistinctFrom”, etc. Internal Sharing Me"
2020.semeval-1.48,D19-1282,0,0.0166046,"rd, 1980). However, most existing models are quite weak in terms of commonsense acquisition and understanding compared with humans since commonsense is often expressed implicitly and constantly evolving (Wang et al., 2019a). Thus, the commonsense problem is considered to be an important bottleneck of modern NLU systems (Davis and Marcus, 2015). Many attempts have been made to empower machines with human abilities in commonsense understanding, and several benchmarks have emerged to verify the commonsense reasoning capability, such as SQUABU (Davis, 2016) and CommonsenseQA (Talmor et al., 2019; Lin et al., 2019). However, these benchmarks estimate commonsense indirectly and do not include corresponding explanations. Therefore, the Commonsense Validation and Explanation (ComVE) benchmark is proposed which directly asks the real reasons behind decision making (Wang et al., 2020). It consists of three subtasks, and we propose an entire system (ECNU-SenseMaker) to solve the first two subtasks, namely, Commonsense Validation and Commonsense Explanation (Multi-Choice). In fact, the two different subtasks correlate to each other. Subtask A (Validation) requires the model to select which one is invalid betwe"
2020.semeval-1.48,N19-1421,0,0.151875,"U) tasks (Johnson-Laird, 1980). However, most existing models are quite weak in terms of commonsense acquisition and understanding compared with humans since commonsense is often expressed implicitly and constantly evolving (Wang et al., 2019a). Thus, the commonsense problem is considered to be an important bottleneck of modern NLU systems (Davis and Marcus, 2015). Many attempts have been made to empower machines with human abilities in commonsense understanding, and several benchmarks have emerged to verify the commonsense reasoning capability, such as SQUABU (Davis, 2016) and CommonsenseQA (Talmor et al., 2019; Lin et al., 2019). However, these benchmarks estimate commonsense indirectly and do not include corresponding explanations. Therefore, the Commonsense Validation and Explanation (ComVE) benchmark is proposed which directly asks the real reasons behind decision making (Wang et al., 2020). It consists of three subtasks, and we propose an entire system (ECNU-SenseMaker) to solve the first two subtasks, namely, Commonsense Validation and Commonsense Explanation (Multi-Choice). In fact, the two different subtasks correlate to each other. Subtask A (Validation) requires the model to select which o"
2020.semeval-1.48,P19-1393,0,0.403565,"nsense Explanation (Multi-Choice). We officially name the system as ECNUSenseMaker. Code is publicly available at https://github.com/ECNU-ICA/ECNU-SenseMaker. 1 Introduction Commonsense reasoning is the process of making decisions by combining facts and beliefs with some basic knowledge appeared in daily life, which is fundamental to many Natural Language Understanding (NLU) tasks (Johnson-Laird, 1980). However, most existing models are quite weak in terms of commonsense acquisition and understanding compared with humans since commonsense is often expressed implicitly and constantly evolving (Wang et al., 2019a). Thus, the commonsense problem is considered to be an important bottleneck of modern NLU systems (Davis and Marcus, 2015). Many attempts have been made to empower machines with human abilities in commonsense understanding, and several benchmarks have emerged to verify the commonsense reasoning capability, such as SQUABU (Davis, 2016) and CommonsenseQA (Talmor et al., 2019; Lin et al., 2019). However, these benchmarks estimate commonsense indirectly and do not include corresponding explanations. Therefore, the Commonsense Validation and Explanation (ComVE) benchmark is proposed which directl"
2020.wmt-1.24,W14-3346,0,0.0287428,"mputed by, R(θ) = S X X Q(y|x(s) ; θ, α)∆(y, y (s) ), s=1 y∈S(x(s) ) (1) where x(s) and y (s) are two paired sentences. ∆ denotes a risk function and S(x(s) ) ∈ Y is a sampled subset of full search space. Then, the distribution Q is defined over space S(x(s) ), P (y|x(s) ; θ)α . 0 (s α y 0 ∈S(x(s)) P (y |x ; θ) (2) In practice, we use 4 candidates for each source sentence x(s) . Although the paper claimed that sampling generates better candidates, we find that the beam search performs better in our extremely large Transformer model. The risk function we used is the 4-gram sentence-level BLEU (Chen and Cherry, 2014) and we tune the optimal α via grid search within {0.005, 0.05, 0.5, 1, 1.5, 2}. Each model is fine-tuned for a max of 1000 steps. Q(y|x(s) ; θ, α) = P 3.6 Ensemble We split each training data into three shards among Clean, Noisy and Sample data respectively, which yields a total number of 9 shards. For each shard, we train seven varieties (two Deeper transformers, two Wider transformers, two AANs and one DTMT) with different model architecture. Then we apply four finetuning approaches on each model, thus the total number of models is quadrupled (about 200 models). For ensemble, it is difficul"
2020.wmt-1.24,P19-1425,0,0.037441,"Missing"
2020.wmt-1.24,W18-6408,0,0.0161268,"via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translat"
2020.wmt-1.24,D18-1045,0,0.32227,"c data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model"
2020.wmt-1.24,W18-2703,0,0.0179191,"and in-domain synthetic data. The out-of-domain synthetic corpus is generated via both large-scale back-translation and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it"
2020.wmt-1.24,D16-1139,0,0.190281,"rmers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algori"
2020.wmt-1.24,D17-1230,0,0.0274829,"domain knowledge transfer. In our experiments, we find that iteratively performing the in-domain knowledge transfer can further provide improvements (see Table 2). For each iteration, we replace the indomain synthetic data and retrain our models, and it costs about 10 days on 8 NVIDIA V100 GPU machines. For the final submission, the knowledge transfer is conducted twice. 3.4 Data Augmentation Aside from synthetic data generation, we also apply two data augmentation methods over our synthetic corpus. Firstly, adding synthetic/natural noises to training data is widely applied in the NLP fields (Li et al., 2017; Belinkov and Bisk, 2017; Cheng et al., 2019) to improve model robustness and enhance model performance. Therefore, we proposed to add token-level synthetic noises. Concretely, we perform random replace, random delete, and random permutation over our data. The probability of enabling each of the three operations is 0.1. We refer to this corrupted corpus as Noisy data. Secondly, as illustrated in (Edunov et al., 2018), sampling generation over back-translation shows its potential in building robust NMT systems. Consequently, we investigate the performance of sampled synthetic data. For back-tr"
2020.wmt-1.24,D19-1559,1,0.791509,"by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce more diversity in our Transformer models, we use Average"
2020.wmt-1.24,P19-1233,1,0.820814,"a linear transformation enhanced GRU (L-GRU) followed by several transition GRUs (T-GRUs). DTMT enhances the hidden-to-hidden transition with multiple non-linear transformations, as well as maintains a linear transformation path throughout this deep transition by the well-designed linear transformation mechanism to alleviate the vanishing gradient problem. This architecture has demonstrated its superiority over the conventional Transformer model and stacked RNN-based models in NMT (Meng and Zhang, 2019), and also achieves surprising performances on other NLP tasks, such as sequence labeling (Liu et al., 2019) and aspect-based sentiment analysis (Liang et al., 2019). In our experiments, we use the bidirectional deep transition encoder, where each directional deep transition block consists of 1 L-GRU and 4 T-GRU. The decoder contains a query transition block and the decoder transition block, each of which consists of 1 L-GRU and 4 T-GRU. Therefore the DTMT consists of a 5 layer encoder and a 10 layer decoder, with a hidden size of 1,024. We use 8 NVIDIA V100 GPUs to train each model for about three weeks and the batch size is set to 4,096 tokens per GPU. Average Attention Transformer 3 To introduce"
2020.wmt-1.24,2015.iwslt-evaluation.11,0,0.0358315,"T systems. Consequently, we investigate the performance of sampled synthetic data. For back-translated data, we replace beam search with sampling in its generation. For in-domain synthetic data, we replace the golden Chinese with the back sampled pseudo Chinese sentences. We refer to the data with sampling generation as Sample data. As a special case, we refer to the without augmentation data as Clean data. 3.5 In-domain Finetuning We train the model on large-scale out-of-domain data until convergence and then finetune it on smallscale in-domain data, which is widely used for domain adaption (Luong and Manning, 2015; Li et al., 2019). Specifically, we take Chinese− →English test sets of WMT 17 and 18 as in-domain data, and filter out documents that are originally created in 242 English (Sun et al., 2019). We name above finetuning approach as normal finetuning. In all our finetuning experiments, we set the batch size to 4096, and finetune the model for around 400 steps1 on the in-domain data. Furthermore, the well-known problem of exposure bias in sequence-to-sequence generation becomes more serious under domain shift (Wang and Sennrich, 2020). To solve this issue, we further explore some advanced finetun"
2020.wmt-1.24,P19-2049,0,0.161662,"back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and"
2020.wmt-1.24,W19-5333,0,0.019301,"and knowledge distillation to enhance the models’ performance for all domains. Then, we propose iterative in-domain knowledge transfer, which transfers in-domain knowledge to the huge monolingual corpus (i.e., Chinese), and builds our in-domain synthetic corpus. In the following sections, we elaborate above techniques in detail. 241 3.2.1 Large-scale Back-Translation Back-translation is shown to be very effective to boost the performance of NMT models in both academic research (Hoang et al., 2018; Edunov et al., 2018) and previous years’ WMT competitions (Deng et al., 2018; Sun et al., 2019; Ng et al., 2019; Xia et al., 2019). Following their work, we also train baseline English-to-Chinese models with the parallel data provided by WMT2020. Both the Left-to-Right Transformer (L2R) and the Right-toLeft Transformer (R2L) are used to translate the filtered monolingual English corpus combined with the English side of golden parallel bitext to Chinese. Then the generated Chinese text and the original English text are regarded as the source side and target side, respectively. In practice, it costs us 7 days on 5 NVIDIA V100 GPU machines to generating all back-translated data. 3.2.2 Knowledge Distillati"
2020.wmt-1.24,P16-1009,0,0.18507,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1162,0,0.554201,", we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al.,"
2020.wmt-1.24,P16-1159,0,0.0483208,"knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section,"
2020.wmt-1.24,W19-5341,0,0.381816,"In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encoding representation."
2020.wmt-1.24,2020.acl-main.326,0,0.094302,"ion method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the mode"
2020.wmt-1.24,P19-1176,0,0.0318592,"submitted systems. In the remainder of this paper, we start with an overview of model architectures in Section 2. Section 3 describes the details of our systems and training strategies. Then Section 4 shows the experimental settings and results. Finally, we conclude our work in Section 5. 2 Model Architectures In this section, we first describe the model architectures we use in the Chinese− →English Shared Task, including the Transformer-based (Vaswani et al., 2017) models and RNN-based (Bahdanau et al., 2014; Meng and Zhang, 2019) models. 2.1 Deeper Transformer As shown in previous studies (Wang et al., 2019; Sun et al., 2019), deeper Transformers with prenorm outperform its shallow counterparts on various machine translation benchmarks. In their work, increasing the encoder depth significantly improves the model performance, while they only introduce mild overhead in terms of speed in training and 239 Proceedings of the 5th Conference on Machine Translation (WMT), pages 239–247 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics inference, compared with increasing the decoder side depth. Hence, we train deeper Transformers with a deep encoder aiming for a better encodi"
2020.wmt-1.24,D19-1430,0,0.0177972,"els in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the source-side monolingual data and golden parallel data. Furthermore, data augmentation methods, including noisy fake data (Wu et al., 2019) and sampling (Edunov et al., 2018), are used for training more robust NMT models. For training strategies, we mainly focus on the parallel scheduled sampling (Mihaylova and Martins, 2019; Duckworth et al., 2019), the target denoising and minimum risk training (Shen et al., 2016; Wang and Sennrich, 2020) algorithm for indomain finetuning. We also exploit a self-bleu (Zhu et al., 2018) based model ensemble approach to enhance our system. As a result, our constrained Chinese→English system achieves the highest case-sensitive BLEU score among all submitted systems. In the remainder of this paper,"
2020.wmt-1.24,P18-1166,0,0.124343,"is the highest among all submissions. 1 Introduction Our WeChat AI team participates in the WMT 2020 shared news translation task on Chinese→English. In this year’s translation task, we mainly focus on exploiting several effective model architectures, better data augmentation, training and model ensemble strategies. For model architectures, we mainly exploit two different architectures in our approaches, namely Transformers and RNMT. For Transformers, we implement the Deeper transformer with Pre-Norm, the Wider Transformer with a larger filter-size and the average attention based transformer (Zhang et al., 2018). For the RNMT, we use the deep transition based DTMT (Meng and Zhang, 2019) model. We finally ensemble four kinds of models in our system. For synthetic data generation, we explore various methods for out-of-domain and in-domain data generation. For out-of-domain data generation, we explore the back-translation method (Sennrich et al., 2016a) to leverage the target side monolingual data and the knowledge distillation method (Kim and Rush, 2016) to leverage the source side of golden parallel data. For in-domain data generation, we employ iterative in-domain knowledge transfer to leverage the s"
2021.acl-long.11,N19-1423,0,0.0485602,"CGnlHeMQTno2qcWvcGfefqcZCptnDt2U8fACMX5RM</latexit> <latexit Figure 1: Illustration of the dynamic information flow in the semantic space. Ck (gray dotted line) denotes the dense representation of dialogue history which we named as context. Ik (pink solid line) denotes the semantic influence brought about by the k-th utterance, which is the difference between Ck and Ck+1 . Introduction Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However, ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still challenging. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally co"
2021.acl-long.11,W19-5944,0,0.0330157,"Missing"
2021.acl-long.11,W07-0734,0,0.111193,"Missing"
2021.acl-long.11,I17-1099,0,0.0267118,"ted by a third party and made publicly available on pushshift.io (Baumgartner et al., 2020). We clean the data following the pipeline used in the DialoGPT.2 For response generation, we employ the multireference Reddit Test Dataset (Zhang et al., 2020) which contains 6k examples with multiple references. We evaluate our pre-trained DialoFlow model on this dataset. The average length of the dialogue history in this dataset is 1.47. To further explore the dynamic information flow in the long dialogue history situation, we choose another popular open-domain dialogue dataset – DailyDialog Dataset (Li et al., 2017), in which the average dialogue history length is about 4.66. DialoFlow is fine-tuned on the DailyDialog training set and evaluated on the DailyDialog multi-reference test set (Gupta et al., 2019). For interactive dialogue quality evaluation, we employ the collected data from the Interactive Evaluation of Dialog Track @ The Ninth Dialog System Technology Challenge (DSTC9) (Gunasekara et al., 2021), which contains 2200 human-bot conversations from 11 chatbots. For each conversation, there are 3 human ratings on the overall quality (0-5). We calculate the correlation between the results of our p"
2021.acl-long.11,P19-1002,1,0.84599,"ence brought about by each utterance. Besides, it seems like that different speakers keep their own context flows. 5 Related Works Multi-turn dialogue modeling. The modeling of multi-turn dialogue history mainly falls into two categories: 1) Flat concatenation. These works directly concatenate the dialogue history as the input sequence (Zhang et al., 2020), which can not capture the information dynamics. 2) Hierarchical architectures. The hierarchical architecture is commonly used in the dialogue history understanding. Serban et al. (2016a) propose the hierarchical LSTM to generate responses. Li et al. (2019) introduce an incremental transformer to capture multi-turn dependencies. Shan et al. (2020); Gu et al. (2020) employ pre-trained BERT to encode individual utterances and design the utterance135 level encoder to capture the turn-level structure. These methods suffer from the lack of context wordlevel information when encoding utterances. Different from these methods, our DialoFlow takes full advantage of both word-level information and utterance-level dynamic information. Besides, the proposed DialoFlow is pre-trained on the largescale open-domain dialogue dataset. Pre-trained models for dialo"
2021.acl-long.11,P04-1077,0,0.105018,"Missing"
2021.acl-long.11,2020.sigdial-1.28,0,0.0804706,"Missing"
2021.acl-long.11,P02-1040,0,0.10886,"Missing"
2021.acl-long.11,P19-1004,0,0.0222185,"ent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still challenging. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utterance, while the history information is essential for understanding dialogue utterances. Thus, all the aforementioned methods are deficient in modeling the"
2021.acl-long.11,P16-1056,0,0.0354585,"Missing"
2021.acl-long.11,2020.acl-main.563,1,0.895436,"ing. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utterance, while the history information is essential for understanding dialogue utterances. Thus, all the aforementioned methods are deficient in modeling the dynamic information in the dialogue history. In this work, inspired by the human cognitive 128 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Proc"
2021.acl-long.11,2020.acl-main.183,0,0.216897,"l26W+V+dqkWijzNdg6CaIs2o6ljmkuquqJubX6qS5BARp3CP4jFhppWzPptak+jaVW8dHX/TmYpVe5blpnhXt6QB2z/HOQ/qxyXbKtnVk2L5PBt1Dvs4wBHN8xRlXKKCGnlHeMQTno2qcWvcGfefqcZCptnDt2U8fACMX5RM</latexit> <latexit Figure 1: Illustration of the dynamic information flow in the semantic space. Ck (gray dotted line) denotes the dense representation of dialogue history which we named as context. Ik (pink solid line) denotes the semantic influence brought about by the k-th utterance, which is the difference between Ck and Ck+1 . Introduction Recent intelligent open-domain chatbots (Adiwardana et al., 2020; Bao et al., 2020; Smith et al., 2020) have made substantial progress thanks to the rapid development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of conversational data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However, ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still c"
2021.acl-long.11,2020.acl-demos.30,0,0.465497,"tional data (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). However, ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/DialoFlow effectively modeling the dialogue history in largescale dialogue pre-training is still challenging. Most of the previous work on dialogue history modeling mainly fall into two groups. One group of works generally concatenate the dialogue history as the model input and predict the response (Zhang et al., 2020; Smith et al., 2020; Bao et al., 2020), named as flat pattern, which is commonly adopted in the large-scale pre-training. However, Sankar et al. (2019) demonstrate that flat concatenation is likely to ignore the conversational dynamics across utterances in the dialogue history. Another group of works employ hierarchical modeling to encode the dialogue history (Serban et al., 2016b; Shan et al., 2020; Gu et al., 2020), in which the utterances are separately encoded and then fed into an utterance-level encoder. These approaches lack the history information when encoding each individual utteranc"
2021.acl-long.228,W19-4828,0,0.0222437,"65 0.2 0.4 0.6 0.8 HSK Width (Normalized) MNLI-m Acc CoLA Mcc 35 20 mally, the mask M is defined as:  1, i∈I Mi = 0, otherwise 91.0 1.0 0.2 0.4 0.6 0.8 1.0 n  where I = round i × of the remained i=1 is the indices activations. Mag Mask masks out the activations with low magnitude. Therefore, this mask is dynamic, i.e., every hTl,i (∀i, l) has its own M. 78 77 0.2 0.4 0.6 0.8 HSK Width (Normalized) 1.0 layers, [SEP] is the most attended token for almost all training samples. Meanwhile, [SEP] frequently appears in the top three positions across all the layers. Similar phenomenon was found in Clark et al. (2019), where [SEP] receives high attention scores from itself and other tokens in the middle layers. Combining this phenomenon and the results in Figure 4 and Figure 5, it can be inferred that the representations of [SEP] is not a desirable source of knowledge for ROSITA and TinyBERT. We conjecture that this is because there exists some trivial patterns in the representations of [SEP], which prevents the student to extract the informative features that are more relevant to the task. 4.3.1 oN W 79 Figure 7: Results of width compression with different masking strategies on CoLA, SST-2, QNLI and MNLI"
2021.acl-long.228,N19-1423,0,0.452991,"chieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7× ∼3.4×. 86.5 86.0 85.5 ROSITA6 TinyBERT4 85.0 0 10−3 10−2 10−1 Amount of HSK (Norma i(ed) 100 Figure 1: The Acc variation of ROSITA (Liu et al., 2021) and TinyBERT (Jiao et al., 2020) on QNLI with the increase of HSK. Introduction Since the launch of BERT (Devlin et al., 2019), pre-trained language models (PLMs) have been advancing the state-of-the arts (SOTAs) in a wide range of NLP tasks. At the same time, the growing ∗ 87.0 Work was done when Yuanxin Liu was an intern at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Zheng Lin is the corresponding author. size of PLMs has inspired a wave of research interest in model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenarios. Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compr"
2021.acl-long.228,P19-1282,0,0.0283087,"e found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods, which attempt to attribute a neural network’s prediction to the input features. The attention weights have also been investigated as an attribution method. However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction. This echoes with our finding that the original Att strategy performs poorly in length compression. However, the attention weights may play different roles in attribution and HSK distillation. Whether the findings in attribution are transferable to HSK distillation is still a problem that needs further investigation. 8 Conclusions and Future Work In this paper, we investigate the compression of HSK in BERT KD. We divide the HSK of BERT into thre"
2021.acl-long.228,D19-1441,0,0.0407769,"model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenarios. Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compression. In conventional KD, the student model is trained to imitate the teacher’s prediction over classes, i.e., the soft labels. Subsequently, Romero et al. (2015) find that the intermediate representations in the teacher’s hidden layers can also serve as a useful source of knowledge. As an initial attempt to introduce this idea to BERT compression, PKD (Sun et al., 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens. In contrast to the previous work that attempts to increase the amount of HSK, in this paper we explore towards the opposite direction to “compress” HSK. We make the observation that although distilling HSK is helpful, the marginal utility diminishes quickly as the amount of HSK increases. To understand this effect, we conduct a series of analysis 292"
2021.acl-long.228,2020.acl-main.195,0,0.0709525,"ited scenarios. Knowledge distillation (KD) (Hinton et al., 2015) is an effective technique in model compression. In conventional KD, the student model is trained to imitate the teacher’s prediction over classes, i.e., the soft labels. Subsequently, Romero et al. (2015) find that the intermediate representations in the teacher’s hidden layers can also serve as a useful source of knowledge. As an initial attempt to introduce this idea to BERT compression, PKD (Sun et al., 2019) proposed to distill representations of the [CLS] token in BERT’s hidden layers, and later studies (Jiao et al., 2020; Sun et al., 2020; Hou et al., 2020; Liu et al., 2021) extend the distillation of hidden state knowledge (HSK) to all the tokens. In contrast to the previous work that attempts to increase the amount of HSK, in this paper we explore towards the opposite direction to “compress” HSK. We make the observation that although distilling HSK is helpful, the marginal utility diminishes quickly as the amount of HSK increases. To understand this effect, we conduct a series of analysis 2928 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference o"
2021.acl-long.228,2020.findings-emnlp.372,0,0.19617,"oved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation. Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student. For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7× ∼3.4×. 86.5 86.0 85.5 ROSITA6 TinyBERT4 85.0 0 10−3 10−2 10−1 Amount of HSK (Norma i(ed) 100 Figure 1: The Acc variation of ROSITA (Liu et al., 2021) and TinyBERT (Jiao et al., 2020) on QNLI with the increase of HSK. Introduction Since the launch of BERT (Devlin et al., 2019), pre-trained language models (PLMs) have been advancing the state-of-the arts (SOTAs) in a wide range of NLP tasks. At the same time, the growing ∗ 87.0 Work was done when Yuanxin Liu was an intern at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Zheng Lin is the corresponding author. size of PLMs has inspired a wave of research interest in model compression (Han et al., 2016) in the NLP community, which aims to facilitate the deployment of the powerful PLMs to resource-limited scenari"
2021.acl-long.228,2020.emnlp-main.242,0,0.0437266,"ated Work KD is widely studied in BERT compression. In addition to distilling the teacher’s predictions as in Hinton et al. (2015), researches have shown that the student’s performance can be improved by using the representations from intermediate BERT layers (Sun et al., 2019; Liu et al., 2021; Hou et al., 2020) and the self-attention distributions (Jiao et al., 2020; Sun et al., 2020). Typically, the knowledge is extensively distilled in a layer-wise manner. To fully utilize BERT’s knowledge, some recent work also proposed to combine multiple teacher layers in BERT KD (Passban et al., 2021; Li et al., 2020) or KD on Transformer-based NMT models (Wu et al., 2020). In contrast to these studies that attempt to increase the amount knowledge, we study BERT KD from the compression point of view. Similar idea can be found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods,"
2021.acl-long.228,D19-1002,0,0.0191234,"of view. Similar idea can be found in MiniLMs (Wang et al., 2020a,b), which only use the teacher’s knowledge to guide the last layer of student. However, they only consider knowledge from the layer dimension, while we investigate the three dimensions of HSK. We explore a variety of strategies to determine feature importance for each single dimension. This is related to a line of studies called the attribution methods, which attempt to attribute a neural network’s prediction to the input features. The attention weights have also been investigated as an attribution method. However, prior work (Wiegreffe and Pinter, 2019; Serrano and Smith, 2019; Brunner et al., 2020; Hao et al., 2020) finds that attention weights usually fail to correlate well with their contributions to the final prediction. This echoes with our finding that the original Att strategy performs poorly in length compression. However, the attention weights may play different roles in attribution and HSK distillation. Whether the findings in attribution are transferable to HSK distillation is still a problem that needs further investigation. 8 Conclusions and Future Work In this paper, we investigate the compression of HSK in BERT KD. We divide"
2021.acl-long.260,P17-1147,0,0.0212785,"tions. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outper"
2021.acl-long.260,Q19-1026,0,0.0120968,"able 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table 5, we observe that ERICA outperforms all baselines, indicating that thro"
2021.acl-long.260,2021.ccl-1.108,0,0.049509,"Missing"
2021.acl-long.260,2020.emnlp-main.298,1,0.863483,"Missing"
2021.acl-long.260,D19-1005,0,0.0318608,"heir relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representation"
2021.acl-long.260,D16-1264,0,0.0361457,"hich are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying on entity mention “shortcuts” (Jiang and Bansal, 2019). Extractive QA For extractive QA, we adopt three widely-used datasets: SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019) in MRQA (Fisch et al., 2019) to evaluate ERICA in various domains. Since MRQA does not provide the test set for each dataset, we randomly split the original dev set into two halves and obtain the new dev/test set. We follow the QA setting of BERT (Devlin et al., 2018): we concatenate the given question and passage into one long sequence, encode the sequence by PLMs and adopt two classifiers to predict the start and end index of the answer. We choose BERT, RoBERTa, MTB and CP as baselines. From the results listed in Table"
2021.acl-long.260,W04-2401,0,0.417814,"Missing"
2021.acl-long.260,W03-0419,0,0.619684,"Missing"
2021.acl-long.260,P19-1279,0,0.23953,"s of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity representations. Note eij may appear multiple times in di , the k-th occurrence of eij , which contains the tokens from index nkstart to nkend , is represented as: mkeij = MeanPoo"
2021.acl-long.260,2020.coling-main.327,0,0.0338092,"y. Although achieving great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or"
2021.acl-long.260,P19-1485,0,0.020828,"elf is considerably smaller, measuring only. [6] Culiacán is a rail junction and is located on the Panamerican Highway that runs south to Guadalajara and Mexico City. [7] Culiacán is connected to the north with Los Mochis, and to the south with Mazatlán, Tepic. 1 Q: where is Guadalajara? Mexico Pre-trained Language Models (PLMs) (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019) have shown superior performance on various Natural Language Processing (NLP) tasks such as text classification (Wang et al., 2018), named entity recognition (Sang and De Meulder, 2003), and question answering (Talmor and Berant, 2019). Benefiting from designing various effective self-supervised learning objectives, such as masked language modeling (Devlin et al., 2018), PLMs can effectively capture the syntax and semantics in text to generate informative language representations for downstream NLP tasks. Corresponding author. Our code and data are publicly available at https:// github.com/thunlp/ERICA. 1 A: Mexico. o Panamerican Highway Los Mochis Sinaloa Mexico City Guadalajara Figure 1: An example for a document “Culiacán”, in which all entities are underlined. We show entities and their relations as a relational graph,"
2021.acl-long.260,W18-5446,0,0.0555444,"Missing"
2021.acl-long.260,K17-1028,0,0.0151597,"reading multiple documents and conducting multi-hop reasoning. It has both standard and masked settings, where the latter setting masks all entities with random IDs to avoid information leakage. We first concatenate the question and documents into a long sequence, then we find all the occurrences of an entity in the documents, encode them into hidden representations and obtain the global entity representation by applying mean pooling on these hidden representations. Finally, we use a classifier on top of the entity representation for prediction. We choose the following baselines: (1) FastQA (Weissenborn et al., 2017) and BiDAF (Seo et al., 2016), which are widely used question answering systems; (2) BERT, RoBERTa, CorefBERT, SpanBERT, MTB and CP, which are introduced in previous sections. From the results listed in Table 4, we observe that ERICA outperforms baselines in both settings, indicating that ERICA can better understand entities and their relations in the documents and extract the true answer according to queries. The significant improvements in the masked setting also indicate that ERICA can better perform multi-hop reasoning to synthesize and analyze information from contexts, instead of relying"
2021.acl-long.260,Q18-1021,0,0.0536187,"Missing"
2021.acl-long.260,C14-1220,0,0.139289,"Missing"
2021.acl-long.260,D17-1004,0,0.026425,"easoning patterns in the pre-training; (2) both MTB and CP achieve worse results than BERT, which means sentence-level pre-training, lacking consideration for complex reasoning patterns, hurts PLM’s performance on document-level RE tasks to some extent; (3) ERICA outperforms baselines by a larger margin on smaller training sets, which means ERICA has gained pretty good document-level relation reasoning ability in contrastive learning, and thus obtains improvements more extensively under low-resource settings. Sentence-level RE For sentence-level RE, we choose two widely used datasets: TACRED (Zhang et al., 2017) and SemEval-2010 Task 8 (Hendrickx et al., 2019). We insert extra marker tokens to indicate the head and tail entities in each sentence. For baselines, we compare ERICA with BERT, RoBERTa, MTB and CP. From the results shown in Table 2, we observe that ERICA achieves almost comparable results on sentence-level RE tasks with CP, which means document-level pre-training in 10 In practice, documents are split into sentences and we only keep within-sentence entity pairs. 11 https://github.com/thunlp/ RE-Context-or-Names - 27.2 49.7 53.7 54.4 56.4 51.7 50.4 57.8 69.5 68.8 70.7 68.4 67.4 69.7 37.9 39"
2021.acl-long.260,P19-1139,1,0.80299,"ative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining bette"
2021.acl-long.260,2020.emnlp-main.523,0,0.162157,"ing great success, these PLMs usually regard words as basic units in textual understanding, ignoring the informative entities and their relations, which are crucial for understanding the whole text. To improve the entity and relation understanding of PLMs, a typical line of work is knowledgeguided PLM, which incorporates external knowledge such as Knowledge Graphs (KGs) into PLMs to enhance the entity and relation understanding. Some enforce PLMs to memorize information about real-world entities and propose novel pretraining objectives (Xiong et al., 2019; Wang et al., 2019; Sun et al., 2020; Yamada et al., 2020). Others modify the internal structures of PLMs to fuse both textual and KG’s information (Zhang et al., 2019; Peters et al., 2019; Wang et al., 2020; He et al., 2020). Although knowledge-guided PLMs introduce extra factual knowledge in KGs, these methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in p"
2021.acl-long.260,P19-1074,1,0.884737,"Missing"
2021.acl-long.260,2020.emnlp-main.582,1,0.844179,"e methods ignore the intrinsic relational facts in text, making it hard to understand out-of-KG entities or knowledge in downstream tasks, let alone the errors and incompleteness of KGs. This verifies the necessity of teaching PLMs to understand relational facts from contexts. Another line of work is to directly model entities or relations in text in pre-training stage to break the limitations of individual token representations. Some focus on obtaining better span representations, including entity mentions, via span-based pre-training (Sun et al., 2019; Joshi et al., 2020; Kong et al., 2020; Ye et al., 2020). Others learn to extract relation-aware semantics from text by comparing the sentences that share the same entity pair or distantly supervised relation in KGs (Soares et al., 2019; Peng et al., 2020). However, these methods only consider either individual entities or within-sentence relations, which limits the performance in dealing with multiple entities and relations at document level. In contrast, our ERICA considers the interactions among multiple entities 3351 {h1 , h2 , ..., h|di |}, then we apply mean pooling operation over the consecutive tokens that mention eij to obtain local entity"
2021.acl-long.268,D15-1166,0,0.0398039,"our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline. The human evaluation further verifies that our approaches improve translation adequacy as well as fluency. 1 1 Introduction Neural Machine Translation (NMT) has achieved great success in recent years (Sutskever et al., 2014; ∗ Equal contribution. This work was done when Mengqi Miao was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Corresponding author. 1 Code is available at https://github.com/Mlair 77/nmt adequacy Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019a; Yan et al., 2020b), which generates accurate and fluent translation through modeling the next word conditioned on both the source sentence and partial translation. However, NMT faces the hallucination problem, i.e., translations are fluent but inadequate to the source sentences. One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language M"
2021.acl-long.268,C16-1205,1,0.926239,"tant reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language Model (LM). In the rest of this paper, the LM mentioned refers to the LM mechanism involved in NMT. Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects. One is to improve the architecture of NMT, such as adding a coverage vector to track the attention history (Tu et al., 2016), enhancing the crossattention module (Meng et al., 2016, 2018; Weng et al., 2020b), and dividing the source sentence into past and future parts (Zheng et al., 2019). The other aims to propose a heuristic adequacy metric or objective based on the output of NMT. Tu et al. (2017) and Kong et al. (2019) enhance the model’s reconstruction ability and increase the coverage ratio of the source sentences by translations, respectively. Although some researches (Tu et al., 2017; Kong et al., 2019; Weng et al., 2020b) point out that the lack of adequacy is due to the overconfidence of the LM, unfortunately, they do not propose effective solutions to the over"
2021.acl-long.268,D16-1096,0,0.0244082,"se Study. To better illustrate the translation quality of our approach, we show several translation examples in Appendix C. Our approach grasps more segments of the source sentences, which are mistranslated or neglected by the Transformer. 6 Related Work Translation Adequacy of NMT. NMT suffers from the hallucination and inadequacy problem for a long time (Tu et al., 2016; M¨uller et al., 2020; Wang and Sennrich, 2020; Lee et al., 2019). Many studies improve the architecture of NMT to alleviate the inadequacy issue, including tracking translation adequacy by coverage vectors (Tu et al., 2016; Mi et al., 2016), modeling a global representation of source side (Weng et al., 2020a), dividing the source sentence into past and future parts (Zheng et al., 2019), and multi-task learning to improve encoder and cross-attention modules in decoder (Meng et al., 2016, 2018; Weng et al., 2020b). They inductively increase the translation adequacy, while our approaches directly maximize the Margin between the NMT and the LM to prevent the LM from being overconfident. Other studies enhance the translation adequacy by adequacy metrics or additional optimization objectives. Tu et al. (2017) minimize the difference b"
2021.acl-long.268,P10-2041,0,0.0337908,"e LM is still required in inference, slowing down the inference speed largely. The research work descried in this paper has been supported by the National Nature Science Foundation of China (No. 12026606). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Data Selection in NMT. Data selection and data filter methods have been widely used in NMT. To balance data domains or enhance the data quality generated by back-translation (Sennrich et al., 2016b), many approaches have been proposed, such as utilizing language models (Moore and Lewis, 2010; van der Wees et al., 2017; Zhang et al., 2020), translation models (JunczysDowmunt, 2018; Wang et al., 2019a), and curriculum learning (Zhang et al., 2019b; Wang et al., 2019b). Different from the above methods, our MSO dynamically combines language models with translation models for data selection during training, making full use of the models. 7 Conclusion We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident. Specifically, we firstly propose an indicator of the overconfidence degree of the LM in NMT, i.e., Margin between the"
2021.acl-long.268,2020.amta-research.14,0,0.0755329,"Missing"
2021.acl-long.268,N19-4009,0,0.0172576,".01). “‡”: significantly better than the joint model NMT+LM (p&lt;0.01). * denotes the results come from the cited paper. 5.2 Results on En→Fr and Zh→En The results on WMT14 English-to-French (En→Fr) and WMT19 Chinese-to-English (Zh→En) are shown in Table 2. We also list the results of (Vaswani et al., 2017) and our reimplemented Transformer as the baselines. On En→Fr, our reimplemented result is higher than the result of (Vaswani et al., 2017), since we update 300K steps while Vaswani et al. (2017) only update 100K steps. Many studies obtain similar results to ours (e.g., 41.1 BLEU scores from (Ott et al., 2019)). Compared with the baseline, NMT+LM yields +0.07 and +0.15 BLEU improvements on En→Fr and Zh→En, respectively. The improvement of NMT+LM on En→De in Table 1 (i.e., +0.75) is greater than these two datasets. We conjecture the reason is that the amount of training data of En→De is much smaller than that of En→Fr and Zh→En, thus NMT+LM is more likely to improve the model performance on En→De. Compared with NMT+LM, our MTO achieves further improvements with +0.42 and +1.04 BLEU scores on En→Fr and Zh→En, respectively, which demonstrates the performance improvement is mainly due to our Margin-bas"
2021.acl-long.268,P16-1162,0,0.807517,"training data. Following the same setting in (Vaswani et al., 2017), we use newstest2013 as validation set and newstest2014 as test set, which contain 3000 and 3003 sentences, respectively. For En→Fr, the training dataset contains about 36M sentence pairs, and we use newstest2013 with 3000 sentences as validation set and newstest2014 with 3003 sentences as test set. For Zh→En, we use 20.5M training data and use newstest2018 as validation set and newstest2019 as test set, which contain 3981 and 2000 sentences, respectively. For Zh→En, the number of merge operations in byte pair encoding (BPE) (Sennrich et al., 2016a) is set to 32K for both source and target languages. For En→De and En→Fr, we use a shared vocabulary generated by 32K BPEs. Evaluation. We measure the case-sensitive BLEU scores using multi-bleu.perl 5 for En→De and En→Fr. For Zh→En, case-sensitive BLEU scores are calculated by Moses mteval-v13a.pl script6 . Moreover, we use the paired bootstrap resampling (Koehn, 2004) for significance test. We select the model which performs the best on the validation sets and report its performance on the test sets for evaluation. Model and Hyperparameters. We conduct experiments based on the Transformer"
2021.acl-long.268,P16-1159,0,0.0233663,"ysis We first evaluate the main performance of our approaches (Section 5.1 and 5.2). Then, the human evaluation further confirms the improvements of translation adequacy and fluency (Section 5.3). Finally, we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works (Section 5.4). 5.1 Results on En→De The results on WMT14 English-to-German (En→De) are summarized in Table 1. We list the results from (Vaswani et al., 2017) and several related competitive NMT systems by various methods, such as Minimum Risk Training (MRT) objective (Shen et al., 2016), Simple Fusion of NMT and LM (Stahlberg et al., 2018), optimizing adequacy metrics (Kong et al., 2019; Feng et al., 2019) and improving the Transformer architecture (Yang et al., 2018; Zheng et al., 2019; Yang et al., 2019; Weng et al., 2020b; Yan et al., 2020a). We re7 The LM does not need to be state-of-the-art. The previous study of (Baziotis et al., 2020) has shown that a more powerful LM does not lead to further improvements to NMT. 8 The experimental results show that the model is insensitive to λLM . Therefore we make λLM consistent for all the three tasks. 3460 System En→De ↑ Existing"
2021.acl-long.268,W18-6321,0,0.0592886,"r approaches (Section 5.1 and 5.2). Then, the human evaluation further confirms the improvements of translation adequacy and fluency (Section 5.3). Finally, we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works (Section 5.4). 5.1 Results on En→De The results on WMT14 English-to-German (En→De) are summarized in Table 1. We list the results from (Vaswani et al., 2017) and several related competitive NMT systems by various methods, such as Minimum Risk Training (MRT) objective (Shen et al., 2016), Simple Fusion of NMT and LM (Stahlberg et al., 2018), optimizing adequacy metrics (Kong et al., 2019; Feng et al., 2019) and improving the Transformer architecture (Yang et al., 2018; Zheng et al., 2019; Yang et al., 2019; Weng et al., 2020b; Yan et al., 2020a). We re7 The LM does not need to be state-of-the-art. The previous study of (Baziotis et al., 2020) has shown that a more powerful LM does not lead to further improvements to NMT. 8 The experimental results show that the model is insensitive to λLM . Therefore we make λLM consistent for all the three tasks. 3460 System En→De ↑ Existing NMT systems Transformer (Vaswani et al., 2017) 27.3 M"
2021.acl-long.268,2020.emnlp-main.212,0,0.395537,"77/nmt adequacy Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019a; Yan et al., 2020b), which generates accurate and fluent translation through modeling the next word conditioned on both the source sentence and partial translation. However, NMT faces the hallucination problem, i.e., translations are fluent but inadequate to the source sentences. One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language Model (LM). In the rest of this paper, the LM mentioned refers to the LM mechanism involved in NMT. Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects. One is to improve the architecture of NMT, such as adding a coverage vector to track the attention history (Tu et al., 2016), enhancing the crossattention module (Meng et al., 2016, 2018; Weng et al., 2020b), and dividing the source sentence into past and future parts (Zheng et al., 2019). The other aims to propose a heuristic adequacy metric or o"
2021.acl-long.268,P16-1008,0,0.123353,"luent but inadequate to the source sentences. One important reason is that the NMT model pays excessive attention to the partial translation to ensure fluency while failing to translate some segments of the source sentence (Weng et al., 2020b), which is actually the overconfidence of the Language Model (LM). In the rest of this paper, the LM mentioned refers to the LM mechanism involved in NMT. Many recent studies attempt to deal with the inadequacy problem of NMT from two main aspects. One is to improve the architecture of NMT, such as adding a coverage vector to track the attention history (Tu et al., 2016), enhancing the crossattention module (Meng et al., 2016, 2018; Weng et al., 2020b), and dividing the source sentence into past and future parts (Zheng et al., 2019). The other aims to propose a heuristic adequacy metric or objective based on the output of NMT. Tu et al. (2017) and Kong et al. (2019) enhance the model’s reconstruction ability and increase the coverage ratio of the source sentences by translations, respectively. Although some researches (Tu et al., 2017; Kong et al., 2019; Weng et al., 2020b) point out that the lack of adequacy is due to the overconfidence of the LM, unfortunat"
2021.acl-long.268,2020.acl-main.326,0,0.018837,"of MSO and MTO during finetuning. As the training continues, our model gets more competent, and the proportion of sentences judged to be “dirty data” by our model increases rapidly at first and then Case Study. To better illustrate the translation quality of our approach, we show several translation examples in Appendix C. Our approach grasps more segments of the source sentences, which are mistranslated or neglected by the Transformer. 6 Related Work Translation Adequacy of NMT. NMT suffers from the hallucination and inadequacy problem for a long time (Tu et al., 2016; M¨uller et al., 2020; Wang and Sennrich, 2020; Lee et al., 2019). Many studies improve the architecture of NMT to alleviate the inadequacy issue, including tracking translation adequacy by coverage vectors (Tu et al., 2016; Mi et al., 2016), modeling a global representation of source side (Weng et al., 2020a), dividing the source sentence into past and future parts (Zheng et al., 2019), and multi-task learning to improve encoder and cross-attention modules in decoder (Meng et al., 2016, 2018; Weng et al., 2020b). They inductively increase the translation adequacy, while our approaches directly maximize the Margin between the NMT and the"
2021.acl-long.268,D19-1073,0,0.0240946,"paper has been supported by the National Nature Science Foundation of China (No. 12026606). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Data Selection in NMT. Data selection and data filter methods have been widely used in NMT. To balance data domains or enhance the data quality generated by back-translation (Sennrich et al., 2016b), many approaches have been proposed, such as utilizing language models (Moore and Lewis, 2010; van der Wees et al., 2017; Zhang et al., 2020), translation models (JunczysDowmunt, 2018; Wang et al., 2019a), and curriculum learning (Zhang et al., 2019b; Wang et al., 2019b). Different from the above methods, our MSO dynamically combines language models with translation models for data selection during training, making full use of the models. 7 Conclusion We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident. Specifically, we firstly propose an indicator of the overconfidence degree of the LM in NMT, i.e., Margin between the NMT and the LM. Then we propose Margin-based TokenAcknowledgments References Dzmitry Bahdanau, Kyunghyun Cho,"
2021.acl-long.268,P19-1123,0,0.0346632,"Missing"
2021.acl-long.268,D17-1147,0,0.0406576,"Missing"
2021.acl-long.268,2020.emnlp-main.77,1,0.804875,"Missing"
2021.acl-long.268,D18-1475,1,0.830984,"ction 5.3). Finally, we analyze the positive impact of our models on the distribution of Margin and explore how each fragment of our method works (Section 5.4). 5.1 Results on En→De The results on WMT14 English-to-German (En→De) are summarized in Table 1. We list the results from (Vaswani et al., 2017) and several related competitive NMT systems by various methods, such as Minimum Risk Training (MRT) objective (Shen et al., 2016), Simple Fusion of NMT and LM (Stahlberg et al., 2018), optimizing adequacy metrics (Kong et al., 2019; Feng et al., 2019) and improving the Transformer architecture (Yang et al., 2018; Zheng et al., 2019; Yang et al., 2019; Weng et al., 2020b; Yan et al., 2020a). We re7 The LM does not need to be state-of-the-art. The previous study of (Baziotis et al., 2020) has shown that a more powerful LM does not lead to further improvements to NMT. 8 The experimental results show that the model is insensitive to λLM . Therefore we make λLM consistent for all the three tasks. 3460 System En→De ↑ Existing NMT systems Transformer (Vaswani et al., 2017) 27.3 MRT* (Shen et al., 2016) 27.71 Simple Fusion** (Stahlberg et al., 2018) 27.88 Localness (Yang et al., 2018) 28.11 Context-Aware (Ya"
2021.acl-long.268,2020.acl-main.756,0,0.0242905,"the inference speed largely. The research work descried in this paper has been supported by the National Nature Science Foundation of China (No. 12026606). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Data Selection in NMT. Data selection and data filter methods have been widely used in NMT. To balance data domains or enhance the data quality generated by back-translation (Sennrich et al., 2016b), many approaches have been proposed, such as utilizing language models (Moore and Lewis, 2010; van der Wees et al., 2017; Zhang et al., 2020), translation models (JunczysDowmunt, 2018; Wang et al., 2019a), and curriculum learning (Zhang et al., 2019b; Wang et al., 2019b). Different from the above methods, our MSO dynamically combines language models with translation models for data selection during training, making full use of the models. 7 Conclusion We alleviate the problem of inadequacy translation from the perspective of preventing the LM from being overconfident. Specifically, we firstly propose an indicator of the overconfidence degree of the LM in NMT, i.e., Margin between the NMT and the LM. Then we propose Margin-based Tok"
2021.acl-long.268,P19-1426,1,0.896913,"Missing"
2021.acl-long.268,N19-1189,0,0.0328674,"Missing"
2021.acl-long.271,K16-1002,0,0.021264,"recognition networks can be estimated using the reparameterization trick (Kingma and Welling, 2014). During inference, latent variables obtained via prior networks and predicted question type qt0 are fed to the question decoder, which corresponds to red dashed arrows in Figure 2. The inference process is as follows: 3498 (1) Sample triple-level LV: zt ∼ qφ (zt |p)1 . (2) Sample answer LV: za ∼ pϕ (za |p, zt ). (3) Sample question LV: zq ∼ pϕ (zq |p, zt , za ). (4) Predict question type: qt ∼ pθ (qt|zq , zt , p). (5) Generate question: q ∼ pθ (zq , zt , p, qt). 3 apply KL annealing, word drop (Bowman et al., 2016) and bag-of-word (BOW) loss (Zhao et al., 2017)4 . The KL multiplier λ gradually increases from 0 to 1, and the word drop probability is 0.25. We use Pytorch to implement our model, and the model is trained on Titan Xp GPUs. Experiments In this section, we conduct experiments to evaluate our proposed method. We first introduce some empirical settings, including dataset, hyperparameters, baselines, and evaluation measures. Then we illustrate our results under both automatic and human evaluations. Finally, we give out some cases generated by different models and do further analyses over our meth"
2021.acl-long.271,D17-1070,0,0.0220143,"te Agreement” for all three criteria. mappings in PQ and QA pairs, and relationship modeling for zq and za , GTM can improve the relevance in QA pairs. 3.6 3.7 Question-Answer Coherence Evaluation Automatic metrics in Section “Automatic Metrics” are designed to compare generated questions with ground-truth ones (RUBER also takes the post information into consideration), but ignore answers in the evaluation process. To measure the semantic coherence between generated questions and answers, we apply two methods (Wang et al., 2019): (1) Cosine Similarity: We use the pre-trained Infersent model7 (Conneau et al., 2017) to obtain sentence embeddings and calculate cosine similarity between the embeddings of generated responses 7 The Infersent model is trained to predict the meaning of sentences based on natural language inference, and the cosine similarity computed with it is more consistent with human’s judgements, which performs better than the pre-trained Transformer/BERT model in our experiments. Table 3: Results for human evaluation. Model S2S-Attn CVAE kgCVAE STD HTD RL-CVAE GTM-zt GTM-a GTM-zq /za GTM Cosine Similarity 0.498 0.564 0.578 0.542 0.583 0.607 0.613 0.605 0.618 0.629 Matching Score 5.306 8.0"
2021.acl-long.271,N19-1125,0,0.030195,"Missing"
2021.acl-long.271,W04-3250,0,0.261348,"enerated by GTM are more coherent to answers. Attributing to the design of triple-level latent variable that captures the shared background, one-to-many Model S2S-Attn CVAE kgCVAE STD HTD RL-CVAE GTM-zt GTM-a GTM-zq /za GTM Fluency 0.482 0.462 0.474 0.488 0.526 0.534 0.538 0.532 0.542 0.548 Coherence 0.216 0.484 0.536 0.356 0.504 0.578 0.580 0.570 0.586 0.608 Willingness 0.186 0.428 0.476 0.286 0.414 0.508 0.516 0.512 0.520 0.526 Human Evaluation Results As shown in Table 3, GTM can alleviate the problem of generating dull and deviated questions compared with other models (significance tests (Koehn, 2004), p-value &lt; 0.05). Both our proposed model and the state-of-the-art model RL-CVAE utilize the answer information and the results of them could prove that answers assist the question generation process. Besides, GTM can produce more relevant and intriguing questions, which indicates the effectiveness of modeling the shared background and one-to-many mappings in CQG task. The interannotator agreement is calculated with the Fleiss’ kappa (Fleiss and Cohen, 1973). Fleiss’ kappa for Fluency, Coherence and Willingness is 0.493, 0.446 and 0.512, respectively, indicating “Moderate Agreement” for all t"
2021.acl-long.271,2020.acl-main.20,0,0.138114,"-answer (PQA) triple into post-question (PQ) and question-answer (QA) pairs rather than considering the triple as a whole and modeling the overall coherence. Furthermore, the training process of the matching model only utilizes one-to-one relation of each QA pair and neglects the one-to-many mapping feature. An open-domain PQA often takes place under a background that can be inferred from all utterances in the triple and help enhance the overall coherence. When it comes to the semantic relationship in each triple, the content of a specific question is under the control of its post and answer (Lee et al., 2020). Meanwhile, either a post or an answer could correspond to several meaningful questions. As shown in Table 1, the triple is about a person’s eating activity (the background of the entire conversation). There are one-to-many mappings in both PQ and QA pairs that construct different meaningful combinations, such as P-Q1.1-A1, P-Q1.2-A1, P-Q2.1-A2 and P-Q2.2-A2. An answer connects tightly to both its post and question, and in turn helps decide the expression of a question. On these grounds, we propose a generative triplewise model (GTM) for CQG. Specifically, we firstly introduce a triple-level"
2021.acl-long.271,D19-1317,0,0.13585,"ith users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019). At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on"
2021.acl-long.271,N16-1014,0,0.380012,"zp and za . (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the correspo"
2021.acl-long.271,P16-1094,0,0.169949,"zp and za . (6) GTMa: the variant of GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the correspo"
2021.acl-long.271,D16-1230,0,0.0794378,"Missing"
2021.acl-long.271,2020.emnlp-main.739,0,0.0594831,"Missing"
2021.acl-long.271,D19-1326,0,0.0475725,"Missing"
2021.acl-long.271,P02-1040,0,0.110323,".584 0.622 0.649 0.687 0.650 0.682 0.633 0.663 0.653 0.689 0.660 0.701 0.661 0.710 0.657 0.692 0.669 0.713 0.671 0.720 Table 2: Automatic evaluation results for different models based on four types of metrics. that provide the controllable feature, i.e., question types, in advance for inference. Therefore, we do not consider CT-based models as comparable ones. 3.4 Evaluation Measures To better evaluate our results, we use both quantitative metrics and human judgements in our experiments. Automatic Metrics For automatic evaluation, we mainly choose four kinds of metrics: (1) BLEU Scores: BLEU (Papineni et al., 2002) calculates the n-gram overlap score of generated questions against ground-truth questions. We use BLEU-1 and BLEU-2 here and normalize them to 0 to 1 scale. (2) Embedding Metrics: Average, Greedy and Extrema metrics are embedding-based and measure the semantic similarity between the words in generated questions and ground-truth questions (Serban et al., 2017; Liu et al., 2016). We use word2vec embeddings trained on the Google News Corpus6 in this part. Please refer to Serban et al. (2017) for more details. (3) Dist-1& Dist-2: Following the work of Li et al. (2016a), we apply Distinct to repor"
2021.acl-long.271,N18-1162,0,0.0355925,"Missing"
2021.acl-long.271,P17-2036,0,0.0167679,"a vital position in this field. Serban et al. (2016) proposed the hierarchical recurrent encoder-decoder (HRED) model with a context RNN to integrate historical information from utterance RNNs. To capture utterance-level variations, Serban et al. (2017) raised a new model Variational HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019). The differences between our method and aforementioned"
2021.acl-long.271,P15-1152,0,0.0332229,"300, 300, and 100. The prior networks and MLPs have one hidden layer with size 300 and tanh non-linearity, while the number of hidden layers in recognition networks for both triple-level and utterance-level variables is 2. We apply dropout ratio of 0.2 during training. The mini-batch size is 64. For optimization, we use Adam (Kingma and Ba, 2015) with a learning rate of 1e-4. In order to alleviate degeneration problem of variational framework (Park et al., 2018), we We compare our methods with four groups of representative models: (1) S2S-Attn: A simple Seq2Seq model with attention mechanism (Shang et al., 2015). (2) CVAE&kgCVAE: The CVAE model integrates an extra BOW loss to generate diverse questions. The kgCVAE is a knowledge-guided CVAE that utilizes some linguistic cues (question types in our experiments) to learn meaningful latent variables (Zhao et al., 2017). (3) STD&HTD: The STD uses soft typed decoder that estimates a type distribution over word types, and the HTD uses hard typed decoder that specifies the type of each word explicitly with Gumbel-softmax (Wang et al., 2018). (4) RL-CVAE: A reinforcement learning method that regards the coherence score (computed by a one-to-one matching netw"
2021.acl-long.271,2020.acl-main.52,1,0.84569,"GTM that does not take answer into account. That is, answer decoder and za are removed from the loss function and the prior and posterior distributions of zq . Besides, zt here does not capture the semantics from answer. (7) GTMzq /za : GTM variant in which distributions of zq are not conditioned on za , i.e., the fact that the content of question is also controlled by answer is not modelled explicitly by latent variables. In our model, we use an MLP to predict question types during inference, which is different from the conditional training (CT) methods (Li et al., 2016b; Zhou et al., 2018; Shen and Feng, 2020) Inspired by Park et al. (2018), using zt inferred from post with the posterior distribution is better than sampling it from the prior one, i.e., a standard Gaussian distribution. 2 https://drive.google.com/drive/ folder/1wNG30YPHiMc_ZNyE3BH5wa1uVtR8l1pG 3 http://www.reddit.com 4 The total BOW loss is calculated as the sum of all BOW losses between each latent variable and q/a. Please refer to Park et al. (2018) for more details. 5 For those methods with open-source codes, we run the original codes; otherwise, we re-implement them based on the corresponding paper. 3.1 Dataset We apply our mode"
2021.acl-long.271,P19-1549,1,0.844722,"Blended Evaluation Routine (Tao et al., 2018) has shown a high correlation with human annotation in open-domain conversation evaluation. There are two versions, one is RubG based on geometric averaging and the other is RubA based on arithmetic averaging. Embedding metrics and BLEU scores are used to measure the similarity between generated and ground-truth questions. RubG/A reflects the se6 https://code.google.com/archive/p/ word2vec/ mantic coherence of PQ pairs (Wang et al., 2019), while Dist-1/2 evaluates the diversity of questions. Human Evaluation Settings Inspired by Wang et al. (2019), Shen et al. (2019), and Wang et al. (2018), we use following three criteria for human evaluation: (1) Fluency measures whether the generated question is reasonable in logic and grammatically correct. (2) Coherence denotes whether the generated question is semantically consistent with the given post. Incoherent questions include dull cases. (3) Willingness measures whether a user is willing to answer the question. This criterion is to justify how likely the generated questions can elicit further interactions. We randomly sample 500 examples from test set, and generate questions using models mentioned above. Then"
2021.acl-long.271,D18-1463,0,0.0162246,"l HRED (VHRED) that augments HRED with CVAEs. After that, VHCR (Park et al., 2018) added a conversation-level latent variable on top of the VHRED, while CSRR (Shen et al., 2019) used three-hierarchy latent variables to model the complex dependency among utterances. In order to detect relative utterances in context, Tian et al. (2017) and Zhang et al. (2018) applied cosine similarity and attention mechanism, respectively. HRAN (Xing et al., 2018) combined the attention results on both word-level and utterance-level. Besides, the future information has also been considered for context modeling. Shen et al. (2018) separated the context into history and future parts, and assumed that each of them conditioned on a latent variable is under a Gaussian distribution. Feng et al. (2020) used future utterances in the discriminator of a GAN, which is similar to Wang et al. (2019). The differences between our method and aforementioned ones in Section 4.1 and 4.2 are: (1) Rather than dividing PQA triples into two parts, i.e., PQ (history and current utterances) and QA (current and future utterances) pairs, we model the entire coherence by utilizing a latent variable to capture the share background in a triple. (2"
2021.acl-long.271,D19-1511,0,0.0687683,"al of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-many mapping (Gao et al., 2019; Chen et al., 2019). At first, the input information of CQG was mainly a given post (Wang et al., 2018; Hu et al., 2018), and the generated questions were usually dull or deviated (Q3 and Q4 in Table 1). Based on the observation that an answer has strong relevance to its question and post, Wang et al. (2019) tried to integrate answer into the question generation process. They applied a reinforcement learning framework that firstl"
2021.acl-long.271,P18-1204,0,0.389212,"to Q2.2) is decided by its post and answer. Q3 (dull) and Q4 (deviated) are generated given only the post. Introduction Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is unavailable during inference (Wang et al., 2019). At the same time, each utterance in open-domain scenario is casual and can be followed by several appropriate sentences, i.e., one-to-man"
2021.acl-long.271,W16-3649,0,0.0172703,"mple of CQG task which is talking about a person’s eating activity. There are one-to-many mappings in both PQ and QA pairs. The content of each meaningful and relevant question (Q1.1 to Q2.2) is decided by its post and answer. Q3 (dull) and Q4 (deviated) are generated given only the post. Introduction Questioning in open-domain dialogue systems is indispensable since a good system should have the ability to well interact with users by not only responding but also asking (Li et al., 2017). Besides, raising questions is a proactive way to guide users to go deeper and further into conversations (Yu et al., 2016). Therefore, the ultimate goal of opendomain conversational question generation (CQG) is to enhance the interactiveness and maintain the continuity of a conversation (Wang et al., 2018). Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ∗ Yang Feng is the corresponding author. CQG differs fundamentally from traditional question generation (TQG) (Zhou et al., 2019; Kim et al., 2019; Li et al., 2019) that generates a question given a sentence/paragraph/passage and a specified answer within it. While in CQG, an answer always follows the to-be-generated question, and is un"
2021.acl-long.271,2021.naacl-main.446,0,0.169882,"eally messing me up. Table 5: Two cases comparison among GTM and other baselines. both posts and answers, and could attract people to give an answer to them. However, other baselines may generate dull or deviated responses, even the RL-CVAE model that considers the answer information would only contain the topic words in answers (e.g., the question in case two), but fail to ensure the PQA coherence. eration problem does not exist in our model and latent variables can play their corresponding roles. 4 The researches on open-domain dialogue systems have developed rapidly (Majumder et al., 2020; Zhan et al., 2021; Shen et al., 2021), and our work mainly touches two fields: open-domain conversational question generation (CQG), and context modeling in dialogue systems. We introduce these two fields as follows and point out the main differences between our method and previous ones. 4.1 Figure 3: Total KL divergence (per word) of all latent variables in GTM and GTM-a model (first 30 epochs of validation set). 3.8 Further Analysis of GTM Variational models suffer from the notorious degeneration problem, where the decoders ignore latent variables and reduce to vanilla Seq2Seq models (Zhao et al., 2017; Park"
2021.acl-long.271,C18-1206,0,0.0314446,"Missing"
2021.acl-long.271,P17-1061,0,0.11927,"q, a (not used in inference), and qt, while dashed arrows are for posterior distributions of latent variables. • Our variational hierarchical structure can not only utilize the “future” information (answer), but also capture one-to-many mappings in PQ and QA, which matches the open-domain scenario well. • Experimental results on a large-scale CQG corpus show that our method significantly outperforms the state-of-the-art baselines in both automatic and human evaluations. 2 Proposed Model Given a post as the input, the goal of CQG is to generate the corresponding question. Following the work of Zhao et al. (2017) and Wang et al. (2019), we leverage the question type qt to control the generated question, and take advantage of the answer information a to improve coherence. In training set, each conversation is represented |p| as {p, q, qt, a}, consisting of post p = {pi }i=1 , |q| question q = {qi }i=1 with its question type qt, |a| and answer a = {ai }i=1 . 2.1 Overview The graphical model of GTM for training process is shown in Figure 1. θ, ϕ, and φ are used to denote parameters of generation, prior, and recognition network, respectively. We integrate answer generation to assist question generation wi"
2021.acl-long.271,D19-1622,0,0.0445696,"Missing"
2021.acl-long.394,P16-1004,0,0.18599,"uage (NL) description, which has attracted increasing attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on"
2021.acl-long.394,P18-1068,0,0.0857169,"ments To investigate the effectiveness and generalizability of our model, we carry out experiments on several commonly-used datasets. X λ Lrl (o; θ), |Nmb | Lrl (o; θ) = −Eo∼π [r(o)] (12) 4.2 Baselines To facilitate the descriptions of experimental results, we refer to the enhanced TRANX model as TRANX-RL. In addition to TRANX, we compare our enhanced model with several competitive models: r(o) = (Lmle (ˆ o) − Lmle (o)) ∗ (max(η − p(o), 0)). (11) 5080 • TRANX (w/ pre-train). It is an enhanced version of TRANX with pre-training. We DJANGO Acc. ATIS Acc. GEO Acc. CONALA BLEU / Acc. COARSE2FINE (Dong and Lapata, 2018)† TRANX (Yin and Neubig, 2019)† TREEGEN (Sun et al., 2020) – 77.3 ±0.4 – 87.7 87.6 ±0.1 88.1 ±0.6 88.2 88.8 ±1.0 – – 24.35 ±0.4 / 2.5 ±0.7 – TRANX TRANX (w/ pre-train) TRANX-R2L TRANX-RAND 77.2 ±0.6 77.5 ±0.4 75.9 ±0.8 74.6 ±1.1 87.6 ±0.4 87.8 ±0.7 87.5 ±0.9 86.4 ±1.4 88.8 ±1.0 88.4±1.1 86.4 ±1.0 81.7 ±1.8 24.38 ±0.5 / 2.2 ±0.5 24.57 ±0.5 / 1.4 ±0.3 24.88 ±0.5 / 2.4 ±0.5 19.73 ±1.1 / 1.6 ±0.6 TRANX-RL (w/o pre-train) TRANX-RL 76.3 ±0.7 77.9 ±0.5 87.2 ±0.8 89.1 ±0.5 87.1 ±1.6 89.5 ±1.2 23.38 ±0.8 / 2.1 ±0.2 25.47 ±0.7 / 2.6 ±0.4 Model Table 2: The performance of our model in comparison with var"
2021.acl-long.394,Q19-1042,0,0.0180992,"e above studies that deal with multi-branch nodes in left-to-right order, our model determines the optimal expansion orders of branches for multi-branch nodes. Some researchers have also noticed that the selection of decoding order has an important impact on the performance of neural code generation models. For example, Alvarez-Melis and Jaakkola (2017) introduce a doubly RNN model that combines width and depth recurrences to traverse each node. Dong and Lapata (2018) firstly generate a rough code sketch, and then fill in missing details by considering the input NL description and the sketch. Gu et al. (2019a) present an insertionbased Seq2Seq model that can flexibly generate a sequence in an arbitrary order. In general, these researches still deal with multi-branch AST nodes in a left-to-right manner. Thus, these models are theoretically compatible with our proposed branch selector. (a) The first example. Finally, it should be noted that have been many NLP studies on exploring other decoding methods to improve other NLG tasks (Zhang et al., 2018; Su et al., 2019; Zhang et al., 2019; Welleck et al., 2019; Stern et al., 2019; Gu et al., 2019a,b). However, to the best of our knowledge, our work is"
2021.acl-long.394,2020.emnlp-main.175,0,0.508154,"ltistep expansion order selection and how to determine the optimal expansion order lead to challenges for the model training. To deal with these issues, we introduce reinforcement learning to train the extended Seq2Tree model in an end-to-end way. Concretely, we first pre-train a conventional Seq2Tree model. Then, we employ self-critical training with a reward function that measures loss difference between different branch expansion orders to train the extended Seq2Tree model. 3.2.1 Pre-training It is known that a well-initialized network is very important for applying reinforcement learning (Kang et al., 2020). In this work, we require the model to automatically quantify effects of different branch expansion orders on the quality of the generated action sequences. Therefore, we expect that the model has the basic ability to generate action sequences in random order at the beginning. To do this, instead of using the pre-order traversal based action sequences, we use the randomly-organized action sequences to pre-train the Seq2Tree model. Concretely, for each multi-branch node in an AST, we sample a branch expansion order from a 5079 uniform distribution, and then reorganize the corresponding actions"
2021.acl-long.394,P16-1057,0,0.0266076,"ode, which frequently occurs with ‘gzip’. In the second example, TRANX incorrectly predicts the second child node at the t10 -th timestep, while TRANX-RL firstly predicts it at the timestep t6 . We think this error results from the sequentially generated nodes and the errors in early timesteps would accumulatively harm the predictions of later sibling nodes. By comparison, our model can flexibly generate subtrees with shorter lengths, alleviating error accumulation. 5 Related Work With the prosperity of deep learning, researchers introduce neural networks into code generation. In this aspect, Ling et al. (2016) first explore a Seq2Seq model for code generation. Then, due to the advantage of tree structure, many attempts resort to Seq2Tree models, which represent codes as trees of meaning representations (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Sun et al., 2019, 2020). Typically, Yin and Neubig (2018) propose TRANX, which introduces ASTs as intermediate representations of codes and has become the most influential Seq2Tree model. Then, Sun et al. (2019, 2020) respectively explore CNN and Transformer 5082 ever, is not suitable to han"
2021.acl-long.394,P17-1105,0,0.103927,"has attracted increasing attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches"
2021.acl-long.394,W19-3620,0,0.0633131,"Missing"
2021.acl-long.394,2020.acl-main.538,0,0.155716,"ee (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches of each multi-branch nodes are expanded in a left-to-right order. Figure 1 gives an example of the NL-to-Code conversion conducted by"
2021.acl-long.394,P17-1041,0,0.106221,"attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches of each multi-branch n"
2021.acl-long.394,D18-2002,0,0.166909,"rders for code generation. • Experimental results and in-depth analyses GenToken Reduce AST ? ? ? ? ExceptHandler type name Name Name id id Exception Code: body (empty) e token constructor except Exception as e: Figure 1: An example of code generation using the conventional Seq2Tree model in pre-order traversal. demonstrate the effectiveness and generality of our model on various datasets. 2 Background As shown in Figure 1, the procedure of code generation can be decomposed into three stages. Based on the learned semantic representations of the input NL utterance, the dominant Seq2Tree model (Yin and Neubig, 2018) first outputs a sequence of abstract syntax description language (ASDL) grammar-based actions. These actions can then be used to construct an AST following the preorder traversal. Finally, the generated AST is mapped into surface code via a user-specified function AST to MR(∗). In the following subsections, we first describe the basic ASDL grammars of Seq2Tree models. Then, we introduce the details of TRANX (Yin and Neubig, 2018), which is selected as our basic model due to its extensive applications and competitive performance (Yin and Neubig, 2019; Shin et al., 2019; Xu et al., 2020). 1 1 P"
2021.acl-long.394,P19-1447,0,0.114297,"NL utterance, the dominant Seq2Tree model (Yin and Neubig, 2018) first outputs a sequence of abstract syntax description language (ASDL) grammar-based actions. These actions can then be used to construct an AST following the preorder traversal. Finally, the generated AST is mapped into surface code via a user-specified function AST to MR(∗). In the following subsections, we first describe the basic ASDL grammars of Seq2Tree models. Then, we introduce the details of TRANX (Yin and Neubig, 2018), which is selected as our basic model due to its extensive applications and competitive performance (Yin and Neubig, 2019; Shin et al., 2019; Xu et al., 2020). 1 1 Please note that our approach is also applicable to other Seq2Tree models. 5077 2.1 ASDL Grammar Formally, an ASDL grammar contains two components: type and constructors. The value of type can be composite or primitive. As shown in the ‘ActionSequence’ and ‘AST z’ parts of Figure 1, a constructor specifies a language component of a particular type using its fields, e.g., ExceptHandler (expr? type, expr? name, stmt∗ body). Each field specifies the type of its child node and contains a cardinality (single, optional ? and sequential ∗) indicating the num"
2021.acl-long.431,P07-1056,0,0.129184,"Missing"
2021.acl-long.431,2020.findings-emnlp.373,0,0.0286372,"s on: (1) Exploring the impacts of using different types of triggers (Dai et al., 2019; Chen et al., 2020). (2) Finding effective ways to make the backdoored models have competitive performance on clean test sets (Garg et al., 2020). (3) Managing to inject backdoors in a data-free way (Yang et al., 2021). (4) Maintaining victim models’ backdoor effects after they are further fine-tuned on clean datasets (Kurita et al., 2020; Zhang et al., 2021). (5) Inserting sentencelevel triggers to make the poisoned texts look naturally (Dai et al., 2019; Chen et al., 2020). Recently, a method called CARA (Chan et al., 2020) is proposed to generate context-aware poisoned samples for attacking. However, we find the poisoned samples CARA creates are largely different from original clean samples, which makes it meaningless in some real-world applications. Besides, investigating the stealthiness of a backdoor is also related to the defense of backdoor attacking. Several effective defense methods are introduced in CV (Huang et al., 2019; Wang et al., 2019; Chen et al., 2019; Gao et al., 2019), but there are only limited researches focusing on defending backdoor attacks against NLP models (Chen and Dai, 2020; Qi et al."
2021.acl-long.431,2020.acl-main.249,0,0.0609382,"Missing"
2021.acl-long.431,2021.ccl-1.108,0,0.0764383,"Missing"
2021.acl-long.431,P11-1015,0,0.54125,"s the perplexity of the new text will not change dramatically or even increase. 1 Proof is in the Appendix. Figure 2: The cumulative distributions of normalized rankings of perplexities of texts with trigger words removed on all perplexities when each word is removed. RW corresponds to detecting a rare word-based trigger. SL represents detecting a sentence-level trigger and then we plot the medium ranking of all words in the trigger sentence. Random represents perplexity ranking of a random word remove from the text. Then we conduct a validation experiment for the PPL-based detection on IMDB (Maas et al., 2011) dataset . Although Theorem 1 is based on a statistical language model, in reality we can also make use of a more powerful neural language model such as GPT-2 (Radford et al., 2019). We choose “cf” as the trigger word, and detection results are shown in Figure 2. Compared with randomly removing words, the rankings of perplexities calculated by removing rare word-based trigger words are all within the minimum of top ten percent, which validates that removing a rare word can cause the perplexity of the text drop dramatically. Deployers can add a data cleaning procedure before feeding the input i"
2021.acl-long.491,W13-2322,0,0.111426,"Missing"
2021.acl-long.491,D13-1185,0,0.0173718,"2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sa"
2021.acl-long.491,P11-1098,0,0.0297505,"ang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structur"
2021.acl-long.491,P17-1038,0,0.0187182,"tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can"
2021.acl-long.491,P15-1017,0,0.34232,"1 CNN’s Kelly Wallace attack-01 Introduction ∗ Event Schema report-01 classify event types (Attack), as well as event argument extraction task to identify entities serving as event arguments (“today” and “Netanya”) and classify their argument roles (Time-within and Place) (Ahn, 2006). By explicitly capturing the event structure in the text, EE can benefit various downstream tasks such as information reˇ trieval (Glavaˇs and Snajder, 2014) and knowledge base population (Ji and Grishman, 2011). Existing EE methods mainly follow the supervised-learning paradigm to train advanced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods"
2021.acl-long.491,2020.emnlp-main.444,0,0.0363461,"et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Ban"
2021.acl-long.491,P09-2093,0,0.0217807,"y. Meanwhile, the pre-trained representations can also directly help extract events and discover new event schemata without any known event schema or annotated instances, leading to better generalizability. This is a challenging unsupervised setting named “liberal event extraction” (Huang et al., 2016). Experiments on the widely-used ACE 2005 and the large MAVEN datasets indicate that CLEVE can achieve significant improvements in both settings. 2 Related Work Event Extraction. Most of the existing EE works follow the supervised learning paradigm. Traditional EE methods (Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving rem"
2021.acl-long.491,2020.acl-main.740,0,0.0611314,"Missing"
2021.acl-long.491,P16-1025,0,0.155473,"unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to find reasonable self-supervised signals (Chen et al., 2017; Wang et al., 2019a) for the diverse semantics and complex structures of events. Fortunately, previous work (Aguilar et al., 2014; Huang et al., 2016) has suggested that sentence semantic structures, such as abstract meaning representation (AMR) (Banarescu et al., 2013), contain broad and diverse semantic and structure information relating to events. As shown in Figure 1, the parsed AMR structure covers not only the annotated event (Attack) but also the event that is not defined in the ACE 2005 schema (Report). Considering the fact that the AMR structures of large-scale unsupervised data can be easily obtained with automatic parsers (Wang et al., 2015), we propose CLEVE, an event-oriented contrastive pre-training framework utilizing AMR str"
2021.acl-long.491,2020.emnlp-main.53,0,0.264824,"notated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-base"
2021.acl-long.491,P18-1201,0,0.019335,"employ a PLM as the text encoder and encourage the representations of the word pairs connected by the ARG, time, location edges in AMR structures to be closer in the semantic space than other unrelated words, since these pairs usually refer to the trigger-argument pairs of the same events (as shown in Figure 1) (Huang et al., 2016). This is done by contrastive learning with the connected word pairs as positive samples and unrelated words as negative samples. Moreover, considering event structures are also helpful in extracting events (Lai et al., 2020) and generalizing to new event schemata (Huang et al., 2018), we need to learn transferable event structure representations. Hence we further introduce a graph neural network (GNN) as the graph encoder to encode AMR structures as structure representations. The graph encoder is contrastively pre-trained on the parsed AMR structures of large unsupervised corpora with AMR subgraph discrimination as the objective. By fine-tuning the two pre-trained models on downstream EE datasets and jointly using the two representations, CLEVE can benefit the conventional supervised EE suffering from data scarcity. Meanwhile, the pre-trained representations can also dire"
2021.acl-long.491,2020.acl-main.439,0,0.0207388,"in various domains, such as computer vision (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2019; Chen et al., 2020; He et al., 2020) and graph (Qiu et al., 2020; You et al., 2020; Zhu et al., 2020). In the context of NLP, many established representation learning works can be viewed as contrastive learning methods, such as Word2Vec (Mikolov et al., 2013), BERT (Devlin et al., 2019; Kong et al., 2020) and ELECTRA (Clark et al., 2020). Similar to this work, contrastive learning is also widely-used to help specific tasks, including question answering (Yeh and Chen, 2019), discourse modeling (Iter et al., 2020), natural language inference (Cui et al., 2020) and relation extraction (Peng et al., 2020). 3 Methodology The overall CLEVE framework is illustrated in Figure 2. As shown in the illustration, our contrastive pre-training framework CLEVE consists of two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Pr"
2021.acl-long.491,2020.findings-emnlp.326,0,0.0290477,"he golden trigger-argument pairs and event structures of ACE 2005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa"
2021.acl-long.491,N16-1049,0,0.0182403,"16; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by unsupervised clustering. Following works introduce more features like coreference chains (Chambers, 2013) and entities (Nguyen et al., 2015; Sha et al., 2016). Recently, Huang and Ji (2020) move to the semi6284 Event Semantic Pre-training Unsupervised Corpora Trigger-Argument Pair Discrimination attack CNN&apos;s Kelly Wallace reports on today&apos;s attack in Netanya. Text Encoder The army said two soldiers were also among the dead. Trigger Replacement … Netanya reports CNN&apos;s Kelly Wallace today&apos;s reports Argument Replacement AMR Parsing Event Structure Pre-training Parsed AMR Graphs ARG0 ARG1 ARG1 attack-01 time ARG1 today dead ARG1 ARG1 soldier quant today Netanya say-01 army time mod 2 also Subgraph Sampling say-01 quant 2 AMR Subgraph Discrimination rep"
2021.acl-long.491,2020.emnlp-main.128,0,0.0362011,"005 training set instead of the AMR structures of NYT. Similarly, the on ACE (AMR) model is pre-trained with the parsed AMR structures of ACE 2005 training set. We also compare CLEVE with various baselines, including: (1) feature-based method, the top-performing JointBeam (Li et al., 2013); (2) vanilla neural model DMCNN (Chen et al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test a"
2021.acl-long.491,2021.ccl-1.108,0,0.0684993,"Missing"
2021.acl-long.491,2020.acl-main.522,1,0.754155,"(Ji and Grishman, 2008; Gupta and Ji, 2009; Li et al., 2013) rely on manually-crafted features to extract events. In recent years, the neural models become mainstream, which automatically learn effective features with neural networks, including convolutional neural networks (Nguyen and Grishman, 2015; Chen et al., 2015), recurrent neural networks (Nguyen et al., 2016), graph convolutional networks (Nguyen and Grishman, 2018; Lai et al., 2020). With the recent successes of BERT (Devlin et al., 2019), PLMs have also been used for EE (Wang et al., 2019a,b; Yang et al., 2019; Wadden et al., 2019; Tong et al., 2020). Although achieving remarkable performance in benchmarks such as ACE 2005 (Walker et al., 2006) and similar datasets (Ellis et al., 2015, 2016; Getman et al., 2017; Wang et al., 2020), these PLM-based works solely focus on better finetuning rather than pre-training for EE. In this paper, we study pre-training to better utilize rich event knowledge in large-scale unsupervised data. Event Schema Induction. Supervised EE models cannot generalize to continually-emerging new event types and argument roles. To this end, Chambers and Jurafsky (2011) explore to induce event schemata from raw text by"
2021.acl-long.491,P15-1019,0,0.0450974,"Missing"
2021.acl-long.491,N19-1105,1,0.917634,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.129,1,0.921459,"ced neural networks (Chen et al., 2015; Nguyen et al., 2016; Nguyen and Grishman, 2018) with humanannotated datasets and pre-defined event schemata. These methods work well in lots of public benchmarks such as ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2016), yet they still suffer from data scarcity and limited generalizability. Since annotating event data and defining event schemata are especially expensive and laborintensive, existing EE datasets typically only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the st"
2021.acl-long.491,D19-1584,1,0.881831,"only contain thousands of instances and cover limited event types. Thus they are inadequate to train large neural models (Wang et al., 2020) and develop methods that can generalize to continually-emerging new event types (Huang and Ji, 2020). Inspired by the success of recent pre-trained language models (PLMs) for NLP tasks, some pio6283 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6283–6297 August 1–6, 2021. ©2021 Association for Computational Linguistics neering work (Wang et al., 2019a; Wadden et al., 2019) attempts to fine-tune general PLMs (e.g, BERT (Devlin et al., 2019)) for EE. Benefiting from the strong general language understanding ability learnt from large-scale unsupervised data, these PLM-based methods have achieved state-ofthe-art performance in various public benchmarks. Although leveraging unsupervised data with pretraining has gradually become a consensus for EE and NLP community, there still lacks a pre-training method orienting event modeling to take full advantage of rich event knowledge lying in largescale unsupervised data. The key challenge here is to"
2021.acl-long.491,2020.emnlp-main.196,0,0.527338,"f two components: event semantic pre-training and event structure pre-training, of which details are introduced in Section 3.2 and Section 3.3, respectively. At the beginning of this section, we first introduce the required preprocessing in Section 3.1, including the AMR parsing and how we modify the parsed AMR structures for our pre-training. 3.1 Preprocessing CLEVE relies on AMR structures (Banarescu et al., 2013) to build broad and diverse self-supervision signals for learning event knowledge from largescale unsupervised corpora. To do this, we use automatic AMR parsers (Wang et al., 2015; Xu et al., 2020) to parse the sentences in unsupervised corpora into AMR structures. Each AMR structure is a directed acyclic graph with concepts as nodes and semantic relations as edges. Moreover, each node typically only corresponds to at most one word, and a multi-word entity will be represented as a list of nodes connected with name and op (conjunction operator) edges. Considering pretraining entity representations will naturally benefits event argument extraction, we merge these lists into single nodes representing multi-word entities (like the “CNN’s Kelly Wallace” in Figure 1) during both event semanti"
2021.acl-long.491,D19-1582,0,0.0155406,"al., 2015); (3) the model incorporating syntactic knowledge, dbRNN (Sha et al., 2018); (4) stateof-the-art models on ED and EAE respectively, including GatedGCN (Lai et al., 2020) and SemSynGTN (Pouran Ben Veyseh et al., 2020); (5) a stateof-the-art EE model RCEE ER (Liu et al., 2020), which tackle EE with machine reading comprehension (MRC) techniques. The last four models adopt PLMs to learn representations. On MAVEN, we compare CLEVE with the official ED baselines set by Wang et al. (2020), including DMCNN (Chen et al., 2015), BiLSTM (Hochreiter and Schmidhuber, 1997), BiLSTM+CRF, MOGANED (Yan et al., 2019), DMBERT (Wang et al., 2019a), BERT+CRF. Evaluation Results The evaluation results are shown in Table 1 and Table 2. We can observe that: (1) CLEVE achieves significant improvements to its basic model RoBERTa on both ACE 2005 and MAVEN. The p-values under the t-test are 4×10−8 , 2×10−8 and 6 × 10−4 for ED on ACE 2005, EAE on ACE 2005, and ED on MAVEN, respectively. It also outperforms or achieves comparable results with 6288 ED Metric (B-Cubed) P R EAE F1 P R ED F1 LiberalEE 55.7 45.1 49.8 36.2 26.5 30.6 RoBERTa RoBERTa+VGAE 44.3 24.9 31.9 24.2 17.3 20.2 47.0 26.8 34.1 25.6 17.9 21.1 CLEVE w/o"
2021.acl-long.504,P16-5005,0,0.0443917,"Missing"
2021.acl-long.504,P84-1044,0,0.56082,"Missing"
2021.acl-long.504,D18-1232,0,0.0626432,"Missing"
2021.acl-long.504,D16-1139,0,0.438243,"d global-level selections, to pick suitable samples for distillation. We evaluate our approaches on two large-scale machine translation tasks, WMT’14 English-German and WMT’19 Chinese-English. Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively. 1 1 Introduction Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu ∗ Equal contribution. This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1 We release our code on https://github.com/Les lieOverfitting/selective distillation. † et al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches"
2021.acl-long.504,kocmi-bojar-2017-curriculum,0,0.0250539,"Missing"
2021.acl-long.504,W04-3250,0,0.610707,"Missing"
2021.acl-long.504,2020.acl-main.41,0,0.0772118,"Missing"
2021.acl-long.504,2020.wmt-1.24,1,0.729222,"We conduct extensive analyses and find that some of the teacher’s knowledge will hurt the whole effect of knowledge distillation. • We propose two selective strategies: batchlevel selection and global-level selection. The experimental results validate our methods are effective. 2 Related Work Knowledge distillation approach (Hinton et al., 2015) aims to transfer knowledge from teacher model to student model. Recently, many knowledge distillation methods (Kim and Rush, 2016; Hu et al., 2018; Sun et al., 2019; Tang et al., 2019; Jiao et al., 2019; Zhang et al., 2019a, 2020; Chen et al., 2020a; Meng et al., 2020) have been used to get effective student model in the field of natural language processing by using teacher model’s outputs or hidden states as knowledge. As for neural machine translation (NMT), knowledge distillation methods commonly focus on better improving the student model and learning from the teacher model. Kim and Rush (2016) first applied knowledge distillation to NMT and proposed the sequence-level knowledge distillation that lets student model mimic the sequence distribution generated by the teacher model. It was explained as a kind of data augmentation and regularization by Gordon"
2021.acl-long.504,N19-4009,0,0.0397154,"Missing"
2021.acl-long.504,D19-1441,0,0.0535241,"Missing"
2021.acl-long.504,N19-1192,0,0.071906,"Transformer baseline, respectively. 1 1 Introduction Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu ∗ Equal contribution. This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1 We release our code on https://github.com/Les lieOverfitting/selective distillation. † et al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches, the model learns from teacher models by minimizing gaps between their outputs on every training word/sentence (i.e., corresponding training sample) without distinction. Despite their promising results, previous studies mainly focus on finding what to teach and rarely investigate how these words/s"
2021.acl-long.504,2020.emnlp-main.74,0,0.126671,"1 1 Introduction Machine translation has made great progress recently by using sequence-to-sequence models (Sutskever et al., 2014; Vaswani et al., 2017; Meng and Zhang, 2019; Zhang et al., 2019b; Yan et al., 2020). Recently, some knowledge distillation methods (Kim and Rush, 2016; Freitag et al., 2017; Gu ∗ Equal contribution. This work was done when Fusheng Wang was interning at Pattern Recognition Center, Wechat AI, Tencent Inc, China. 1 We release our code on https://github.com/Les lieOverfitting/selective distillation. † et al., 2017; Tan et al., 2019; Wei et al., 2019; Li et al., 2020; Wu et al., 2020) are proposed in the machine translation to help improve model performance by transferring knowledge from a teacher model. These methods can be divided into two categories: word-level and sequence-level, by the granularity of teacher information. In their researches, the model learns from teacher models by minimizing gaps between their outputs on every training word/sentence (i.e., corresponding training sample) without distinction. Despite their promising results, previous studies mainly focus on finding what to teach and rarely investigate how these words/sentences (i.e., samples), which ser"
2021.acl-long.504,2020.emnlp-main.77,1,0.839154,"Missing"
2021.acl-long.504,N19-1119,0,0.0350845,"Missing"
2021.acl-long.504,P16-1162,0,0.0586626,"ce 9: Loss ← Loss + Lossi 10: Update S with respect to Loss of each word. The storage of queue is much bigger than a mini-batch so that we can evaluate the current batch’s CEs with more words, which reduces the fluctuation of CE distribution caused by the batch-level one. Algorithm 1 details the entire procedure. 6 Experiments We carry out experiments on two large-scale machine translation tasks: WMT’14 English-German (En-De) and WMT’19 Chinese-English (Zh-En). 6.1 Setup Datasets. For WMT’14 En-De task, we use 4.5M preprocessed data, which is tokenized and split using byte pair encoded (BPE) (Sennrich et al., 2016) with 32K merge operations and a shared vocabulary for English and German. We use newstest2013 as the validation set and newstest2014 as the test set, which contain 3000 and 3003 sentences, respectively. For the WMT’19 Zh-En task, we use 20.4M preprocessed data, which is tokenized and split using 47K/32K BPE merge operations for source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 3981 and 2000 sentences, respectively. Evaluation. For evaluation, we train all the models with a maximum of 300K steps for WMT EnDe’14 and WMT’19 Zh-"
2021.acl-long.504,2020.emnlp-main.37,0,0.0702232,"Missing"
2021.acl-long.504,P19-1426,1,0.793031,"Missing"
2021.acl-long.504,D19-1086,0,0.0219955,"• Seq-KD (Kim and Rush, 2016). SequenceKD uses teacher generated outputs on training corpus as an extra source. The training loss can be formulated as: Lseq kd = − |V | J X X 1{ˆyj = k} j=1 k=1 × log p(yj = k|ˆ y&lt;j , x; θ), (4) where y ˆ denotes the sequence predicted by teacher model from running beam search, J is the length of target sentence. • Bert-KD (Chen et al., 2020b). This method leverages the pre-trained Bert as teacher model to help NMT model improve machine translation quality. • Other Systems. We also include some existing methods based on Transformer(Base) for comparison, i.e., Zheng et al. (2019); So et al. (2019); Tay et al. (2020). 6.2 Main Results 3 https://github.com/moses-smt/mosesde coder/blob/master/scripts/generic/mteval -v13a.pl Results on WMT’14 English-German. The results on WMT’14 En-De are shown in Table 2. In 6461 SEasy (27.78) SHard (28.42) Distil All (28.14) 15000 12500 10000 7500 5000 0 20000 40000 60000 Training Steps 80000 100000 2500 0 0 Figure 3: The probability for gradients of Lkd and Lce pointing the same direction. this experiment, both the teacher model and student model are Transformer (Base). We also list our implementation of word-level distillation and se"
2021.acl-short.65,P18-1008,0,0.0290905,"assigns larger training weights to tokens with higher BMI, so that easy tokens are updated with coarse granularity while difficult tokens are updated with fine granularity. Experimental results on WMT14 English-to-German and WMT19 Chinese-to-English demonstrate the superiority of our approach compared with the Transformer baseline and previous token-level adaptive training approaches. Further analyses confirm that our method can improve the lexical diversity. 1 Introduction Neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017; Chen et al., 2018; Meng and Zhang, 2019; Zhang et al., 2019; Yan et al., 2020; Liu et al., 2021) has achieved remarkable success. As a data-driven model, the performance of NMT depends on training corpus. Balanced training data is a crucial factor in building a superior model. ∗ This work was done when Yangyifan Xu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. However, natural languages conform to the Zipf’s law (Zipf, 1949), the frequencies of words exhibit the long tail characteristics, which brings an imbalance in the distribu"
2021.acl-short.65,2020.aacl-main.25,0,0.0828327,"Missing"
2021.acl-short.65,2020.emnlp-main.76,1,0.674679,"Missing"
2021.acl-short.65,D19-1088,0,0.0219702,"l competence increases (Wan et al., 2020). Second, previous studies only use monolingual word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks, e.g., machine translation. The mapping between bilingualism is a more appropriate indicator. As shown in Table 1, word frequency of pleasing and bearings are both 847. Corresponding to Chinese, pleasing has multiple mappings, while bearings is relatively single. The more multivariate the mapping is, the less confidence in predicting the target word given the source context. He et al. (2019) also confirm this view that words with multiple mappings contribute more to the BLEU score. To tackle the above issues, we propose bilingual mutual information (BMI), which has two characteristics: 1) BMI measures the learning difficulty for each target token by considering the strength of association between it and the source sentence; 2) for each target token, BMI can dynamically adjust according to the context. BMI-based adaptive training can dynamically adjust the learning granularity on tokens. Easy tokens are updated with coarse granularity while difficult tokens are updated with 511 Pr"
2021.acl-short.65,W19-6622,0,0.0330324,"Missing"
2021.acl-short.65,2020.emnlp-main.80,0,0.0330367,"ings an imbalance in the distribution of words in training corpora. Some studies (Jiang et al., 2019; Gu et al., 2020) assign different training weights to target tokens according to their frequencies. These approaches alleviate the token imbalance problem and indicate that tokens should be treated differently during training. However, there are two issues in existing approaches. First, these approaches believe that lowfrequency words are not sufficiently trained and thus amplify the weight of them. Nevertheless, low-frequency tokens are not always difficult as the model competence increases (Wan et al., 2020). Second, previous studies only use monolingual word frequency information in the target language without considering the source language, which is insufficient for bilingual tasks, e.g., machine translation. The mapping between bilingualism is a more appropriate indicator. As shown in Table 1, word frequency of pleasing and bearings are both 847. Corresponding to Chinese, pleasing has multiple mappings, while bearings is relatively single. The more multivariate the mapping is, the less confidence in predicting the target word given the source context. He et al. (2019) also confirm this view t"
2021.acl-short.65,2020.emnlp-main.77,1,0.857232,"Missing"
2021.acl-short.65,W04-3250,0,0.357853,"Missing"
2021.acl-short.65,P17-4012,0,0.0902858,"on and report the BLEU scores on newstest2014. ZH-EN. The training data is from WMT19 which consists of 20.5M sentence pairs. The number of merge operations in byte pair encoding (BPE) is set to 32K for both source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 4k and 2k sentences, respectively. BMI-based Objective We calculate the token-level weight by scaling BMI and adjusting the lower limit as follows: wj = S · BMI(x, yj ) + B. Experiments 4.2 Systems Transformer. We implement our approach with the open source toolkit THUMT (Zhang et al., 2017) and strictly follow the setting of TransformerBase in (Vaswani et al., 2017). Exponential (Gu et al., 2020). This method adds an additional training weights to lowfrequency target tokens: wj = A · e−T ·Count(yj ) + 1. (4) The two hyperparameters S (scale) and B (base) influence the magnitude of change and the lower limit, respectively. 513 (5) Chi-Square (Gu et al., 2020). The weighting function of this method is similar to the form of chi-square distribution wj = A · Count2 (yj )e−T ·Count(yj ) + 1. (6) B 1.0 BMI 0.9 0.8 0.7 S 0.05 0.10 0.15 0.20 0.25 0.30 0.15 0.20 0.25 0.15 0.20 0.25 0.15"
2021.acl-short.65,P19-1426,1,0.87636,"Missing"
2021.acl-short.65,P16-1162,0,0.0591186,"icult tokens, the model has a higher tolerance because their translation errors may not be absolute. As a result, the loss is small due to the small weight and the difficult tokens are always updated in a fine-grained way. 4 We evaluate our method on the Transformer (Vaswani et al., 2017) and conduct experiments on two widely-studied NMT tasks, WMT14 English-to-German (En-De) and WMT19 Chineseto-English (Zh-En). 4.1 Data Preparation EN-DE. The training data consists of 4.5M sentence pairs from WMT14. Each word in the corpus has been segmented into subword units using byte pair encoding (BPE) (Sennrich et al., 2016) with 32k merge operations. The vocabulary is shared among source and target languages. We select newstest2013 for validation and report the BLEU scores on newstest2014. ZH-EN. The training data is from WMT19 which consists of 20.5M sentence pairs. The number of merge operations in byte pair encoding (BPE) is set to 32K for both source and target languages. We use newstest2018 as our validation set and newstest2019 as our test set, which contain 4k and 2k sentences, respectively. BMI-based Objective We calculate the token-level weight by scaling BMI and adjusting the lower limit as follows: wj"
2021.ccl-1.46,kipper-etal-2006-extending,0,0.204729,"Missing"
2021.ccl-1.46,pradet-etal-2014-adapting,0,0.0586828,"Missing"
2021.eacl-main.49,P16-1068,0,0.0155197,"hus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2015) have been widely applied into computer version and NLP (Zeiler and Fergus, 2014; Liang et al., 2018). The gradient-based approach is also used to understand the decisions of the text classiﬁcation models from the token level (Li et al., 2016; Alikaniotis et al., 2016). In addition, Rei et al. (2018) 615 adopted gradient-based approach to detect the important tokens in the sentence via the sentence-level label. In this paper, we design a continuous perturbation algorithm to discover the target-aware opinion words using the gradient. 8 Conclusion In this paper, we propose a framework for automatic target-aware sentiment mining from texts without manual annotations or linguistic rules. We evaluate the proposed framework on two largescale online review domains: restaurant and electronic with both manual checking and automatic downstream tasks. We also achieve"
2021.eacl-main.49,D09-1062,0,0.0415647,"liest+ sucks∗ fast+ Table 5: We list top-10 opinion words of several targets for two domains: electronic and restaurant. The marker + and ∗ represent positive and negative sentiment respectively. and 0.96) for L and Lc over restaurant (electronic). All these indicate that commonsense lexicon Lc is more diverse than general lexicon Lg over different targets. In addition, the commonly used general opinion words and commonsense sentiment words are different for different targets. 7 Related Work Domain adaptation has been studied for a long time in the ﬁeld of sentiment analysis (Wu et al., 2017; Choi and Cardie, 2009; Cambria et al., 2018; Zhou et al., 2020c). We mainly summarize the related work about lexicon domain adaptation that aims to build a domain-speciﬁc sentiment lexicon (Ofek et al., 2016; Vo and Zhang, 2016; Hamilton et al., 2016). In (Hamilton et al., 2016), authors inferred the orientation of words from general opinion words by building a graph for each domain. Xing et al. (2019) judged the word polarity via a document-level sentiment classiﬁer. However, it is time-consuming for they have to retrain the model for each word after changing the polarity randomly. Moreover, these existing method"
2021.eacl-main.49,J90-1003,0,0.502719,"Missing"
2021.eacl-main.49,D19-1109,0,0.0270086,"aurus are also made used. Since it is not easy to apply on different domains, we develop a framework to automatically mine aspect-aware commonsense sentiment from texts without extensive annotations and elaborated linguistic rules. Pre-trained models (e.g., ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019)) have achieved great success in NLP recently. By exploring a large number of open domain texts, pre-trained models are able to encode rich semantic information hidden in human languages and thus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2015) have been widely applied into computer version and NLP (Zeiler and Fergus, 2014; Liang et al., 2018). The gradient-based approach is also used to understand the decisions of the text classiﬁcation models from the token level (Li et al., 2016; Alikaniotis et al., 2016). In addition, Rei et al. (2018) 615 adopted gradient-based app"
2021.eacl-main.49,N19-1423,0,0.0907804,"t one sentence (target) in it is positive. By seeing a large amount of positive documents, a classiﬁer may be able to generalize patterns of their positive sentences, thus may help ﬁnding sentence-level (target-level) opinions. Here we simply build a document-level sentiment classiﬁer, and apply it on sentences to get pseudo target-level sentiment labels (for simplicity, we assume one sentence contains one target). Advanced distant supervision models could also be applied, but we ﬁnd this simple method preforms quite well in our experiments. To build the sentiment classiﬁer, we ﬁne-tune BERT (Devlin et al., 2019) on D to encode domain speciﬁc semantics and augment it with a sentiment prediction task to encode sentiment information. For a document d, we feed its word sequence into BERT and obtain a vector representation d = BERT(d), then we apply a softmax operator on d to get the probability of its sentiment P (y|d), P (y|d) = softmax(Wc d + bc ), |D| 1  log P (yi |di ). |D| (2) i=1 For each sentence s containing t, we apply above classiﬁer to predict pseudo sentiment label yp of s. In the following sections, we will rely on the set St = {(s, yp )|t ∈ S} to extract target-aware opinion words of t. 4"
2021.eacl-main.49,N19-1259,0,0.0396286,"Missing"
2021.eacl-main.49,D16-1057,0,0.0864231,"negative sentiment when commenting a computer hardware and a positive sentiment when commenting a pizza, even itself alone is identiﬁed without any general orientation. In these situations, it is the composition of a word, contexts, and commonsense carries an opinion. Automatically detecting such context dependent sentiments would strengthen both our understanding of implicit opinions in languages and improve existing sentiment analyses models, which is the main topic of this work. To handle shifts of word sentiment, prior works studied how to adapt existing sentiment lexicons to new domains (Hamilton et al., 2016; Xing et al., 2019). By modeling differences and similarities of text topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pont"
2021.eacl-main.49,N16-1082,0,0.0304062,"n languages and thus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2015) have been widely applied into computer version and NLP (Zeiler and Fergus, 2014; Liang et al., 2018). The gradient-based approach is also used to understand the decisions of the text classiﬁcation models from the token level (Li et al., 2016; Alikaniotis et al., 2016). In addition, Rei et al. (2018) 615 adopted gradient-based approach to detect the important tokens in the sentence via the sentence-level label. In this paper, we design a continuous perturbation algorithm to discover the target-aware opinion words using the gradient. 8 Conclusion In this paper, we propose a framework for automatic target-aware sentiment mining from texts without manual annotations or linguistic rules. We evaluate the proposed framework on two largescale online review domains: restaurant and electronic with both manual checking and automatic downstr"
2021.eacl-main.49,D15-1168,0,0.0449795,"Missing"
2021.eacl-main.49,P09-1113,0,0.0398288,"tain multiple targets (7 in average). Therefore, directly using 1 We use the 5-level label set Y = {1, 2, 3, 4, 5} (the larger a number, the more positive it represents). 2 Here we mainly focus on online reviews, but the methods could be applied to other sentiment-bearing texts. 609 document-level sentiment labels could be inappropriate for target-level analyses. On the other hand, it is quite expensive to annotate target-level sentiments, and existing datasets are far from enough for a robust commonsense opinion extractor. To deal with this problem, we borrow the idea of distant supervision (Mintz et al., 2009): if a document is labelled as positive, at least one sentence (target) in it is positive. By seeing a large amount of positive documents, a classiﬁer may be able to generalize patterns of their positive sentences, thus may help ﬁnding sentence-level (target-level) opinions. Here we simply build a document-level sentiment classiﬁer, and apply it on sentences to get pseudo target-level sentiment labels (for simplicity, we assume one sentence contains one target). Advanced distant supervision models could also be applied, but we ﬁnd this simple method preforms quite well in our experiments. To b"
2021.eacl-main.49,N18-1202,0,0.0213324,"ons and handcrafted external resources. To take the target into account, Wu et al. (2019) proposed to construct a target-speciﬁc sentiment lexicon. However, both NLP preprocessing pipelines (e.g., parsing, POS tagging) and linguistic rules are integrated into their algorithm. Available resources like general sentiment lexicon and thesaurus are also made used. Since it is not easy to apply on different domains, we develop a framework to automatically mine aspect-aware commonsense sentiment from texts without extensive annotations and elaborated linguistic rules. Pre-trained models (e.g., ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019)) have achieved great success in NLP recently. By exploring a large number of open domain texts, pre-trained models are able to encode rich semantic information hidden in human languages and thus provide new powerful tools for knowledge mining and extraction (Davison et al., 2019; Petroni et al., 2019). Since the commonsense opinions are closely related to human commonsense and background knowledge, we adopt pre-trained language models to mine the commonsense sentiment from texts automatically. Gradient-based methods (Goodfellow et al., 2"
2021.eacl-main.49,D19-1250,0,0.0334635,"Missing"
2021.eacl-main.49,S14-2004,0,0.0453459,"2016; Xing et al., 2019). By modeling differences and similarities of text topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pontiki et al., 2014; Zhou et al., 2020a,b), opinion relation extraction (Sun et al., 2017)) attempt ﬁnd answers of “who express what opinion on which target” for opinion bearing texts. Existing solutions heavily rely on manual annotations and linguistic rules, which are either hard to scale-up or hard to be complete. In this work, we study the task of extracting target-aware sentiment lexicons. An entry of such lexicon is a pair of a sentiment word and a target word, and their collocation expresses a sentiment. It improves existing domain-dependent lexicons by being more concrete and accurate on describing opini"
2021.eacl-main.49,W14-5905,0,0.0327492,"con from (Hu and Liu, 2004) to ﬁlter the general sentiment words and obtain the commonsense lexicon. This general lexicon contains around 6800 positive and negative opinion words or sentiment words for the English language. We adopt BERTbase as the basis for all experiments. Adam (Kingma and Ba, 2015) is adopted as the optimizer with learning rate 5e-5 for ﬁne-tuning and sentiment classiﬁcation. 3 http://jmcauley.ucsd.edu/data/amazon/ https://www.yelp.com/dataset/challenge 5 Here we use the targets from existing datasets, but the targets could be extracted automatically through existing work (Poria et al., 2014) or be inputted by users. 4 6.2 Human Evaluation To evaluate the quality of the target-aware sentiment lexicon, we test its performance through human evaluation. For quantitative evaluation, we sample 50 targets with top-20 opinion words in each domain to investigate the performance of L and Lc . Finally, we obtain 3122 and 2877 (t, o) pairs after ﬁltering repetitive pairs for electronic and restaurant, respectively. We ask ten annotators to label them to make sure each pair is marked with three times. Then, we obtain the label through voting. We calculate the Krippendorff’s alpha coefﬁcient ("
2021.eacl-main.49,N18-1027,0,0.031509,"Missing"
2021.eacl-main.49,E17-1097,1,0.850441,"topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pontiki et al., 2014; Zhou et al., 2020a,b), opinion relation extraction (Sun et al., 2017)) attempt ﬁnd answers of “who express what opinion on which target” for opinion bearing texts. Existing solutions heavily rely on manual annotations and linguistic rules, which are either hard to scale-up or hard to be complete. In this work, we study the task of extracting target-aware sentiment lexicons. An entry of such lexicon is a pair of a sentiment word and a target word, and their collocation expresses a sentiment. It improves existing domain-dependent lexicons by being more concrete and accurate on describing opinions. Departing from approaches adopted in existing aspect-based analyse"
2021.eacl-main.49,P16-2036,0,0.0297844,"L and Lc over restaurant (electronic). All these indicate that commonsense lexicon Lc is more diverse than general lexicon Lg over different targets. In addition, the commonly used general opinion words and commonsense sentiment words are different for different targets. 7 Related Work Domain adaptation has been studied for a long time in the ﬁeld of sentiment analysis (Wu et al., 2017; Choi and Cardie, 2009; Cambria et al., 2018; Zhou et al., 2020c). We mainly summarize the related work about lexicon domain adaptation that aims to build a domain-speciﬁc sentiment lexicon (Ofek et al., 2016; Vo and Zhang, 2016; Hamilton et al., 2016). In (Hamilton et al., 2016), authors inferred the orientation of words from general opinion words by building a graph for each domain. Xing et al. (2019) judged the word polarity via a document-level sentiment classiﬁer. However, it is time-consuming for they have to retrain the model for each word after changing the polarity randomly. Moreover, these existing methods mainly focus on the domain-level, while the sentiment polarities of some words depend on their opinion targets (Liu and Zhang, 2012). It is essential to predict the sentiment in target-level by integratin"
2021.eacl-main.49,P17-1156,0,0.0353875,"Missing"
2021.eacl-main.49,D12-1015,0,0.0175096,"erred the orientation of words from general opinion words by building a graph for each domain. Xing et al. (2019) judged the word polarity via a document-level sentiment classiﬁer. However, it is time-consuming for they have to retrain the model for each word after changing the polarity randomly. Moreover, these existing methods mainly focus on the domain-level, while the sentiment polarities of some words depend on their opinion targets (Liu and Zhang, 2012). It is essential to predict the sentiment in target-level by integrating both target and opinion words. The most related work to us is (Zhao et al., 2012). Zhao et al. (2012) focused on inferring the polarity of a binary tuple of a polarity word and a target via search engine, while target-aware opinion words extraction is not fully explored. To take the target into account, Wu et al. (2019) proposed to construct a target-speciﬁc sentiment lexicon. However, both NLP preprocessing pipelines (e.g., parsing, POS tagging) and linguistic rules are integrated into their algorithm. Different from them, we ﬁrst extract the target-aware commonsense opinion words via pretrained models, which learned rich commonsense knowledge hidden in human languages. T"
2021.eacl-main.49,2020.coling-main.49,1,0.857223,"9). By modeling differences and similarities of text topics, they can detect new sentiments of words as the domain changes. The basic assumption of those domain-level sentiment lexicons is that a word keeps a consistent sentiment within a domain. This assumption, however, might be strong for ﬁne-granularity analyses of text sentiments: words (especially, neural words such as “long”, “fast”) could exhibit different orientations even in the same domain (Figure 1). To collect more detailed information of a sentiment, another branch of works (aspect-based sentiment analysis (Pontiki et al., 2014; Zhou et al., 2020a,b), opinion relation extraction (Sun et al., 2017)) attempt ﬁnd answers of “who express what opinion on which target” for opinion bearing texts. Existing solutions heavily rely on manual annotations and linguistic rules, which are either hard to scale-up or hard to be complete. In this work, we study the task of extracting target-aware sentiment lexicons. An entry of such lexicon is a pair of a sentiment word and a target word, and their collocation expresses a sentiment. It improves existing domain-dependent lexicons by being more concrete and accurate on describing opinions. Departing from"
2021.emnlp-main.178,D18-1547,0,0.0480896,"s objective function, for example, the siamese structure and list-wise ranking loss function. The details are shown in Table 1. We suggest that if a further pretraining task learns similar abilities or has a similar model structure the with the downstream task, then the further pre-training will be more effective for fine-tuning. 4 4.1 Experimental Setup Dialogue Datasets for Further Pre-training DSTC2. (Henderson et al., 2014) It is a machinehuman task-oriented dataset, We follow Wu et al. (2020) to map the original dialogue act labels to universal dialogue acts, resulting in 19 acts. MWOZ. (Budzianowski et al., 2018) It is a popular benchmark for task-oriented dialogues. It has 30 (domain, slot) pairs across seven different domains. We use the revised version MWOZ 2.1. GSIM. (Shah et al., 2018) It is a human-rewrote task-oriented dataset. Following Wu et al. (2020) we combine movie and restaurant domains into one single corpus, and map its dialogue act labels to universal dialogue acts, resulting in 13 acts. 4.3 Training Setting For further pre-training, we set the learning rate equal to 5e-5, batch size to 32, and maximum sequence length to 512. For fine-tuning, we set the learning rate to 5e-5 (except d"
2021.emnlp-main.178,D19-1459,0,0.0131645,"using the loss of a validation set. We train each downstream task three times with different seeds. We use 4 NVIDIA V100 GPUs for further pretraining and one for fine-tuning. Our code is based on Transformers 3 Following Wu et al. (2020), we construct the further pre-training dataset by combining nine different multi-turn goal-oriented datasets (Frames (El Asri et al., 2017), MetaLWOZ (Lee et al., 2019), WOZ (Mrkši´c et al., 2017), CamRest676 (Wen et al., 2017), MSR-E2E (Li et al., 2018), MWOZ (Budzianowski et al., 2018), Schema (Rastogi et al., 2020), SMD (Eric et al., 2017) and Taskmaster (Byrne et al., 2019)). In total, there are 100,707 dialogues containing 1,388,152 utterances over 60 domains. 2322 3 https://github.com/huggingface/transformers BERT 2 M LM 3 MWOZ f 1micro f 1macro 90.90 81.31 91.51 79.77 BERT 2 M LM 3 MWOZ R100 @1 R100 @3 47.39 74.46 59.55 82.63 DA DSTC2 f 1micro f 1macro 91.16 37.67 87.76 36.99 RS DSTC2 R100 @1 R100 @3 48.33 63.30 54.25 68.91 INT Acc (in) 95.20 95.90 DST accjoint accslot GSIM f 1micro f 1macro 99.07 45.49 99.35 45.70 Acc (all) 84.96 85.10 GSIM R100 @1 R100 @3 19.21 40.02 35.71 58.73 47.96 48.72 Acc (out) 88.70 88.30 Recall (out) 38.87 36.70 96.86 96.94 Table 2:"
2021.emnlp-main.178,N19-1423,0,0.513889,"nt tasks may also need a further pre-training some tasks even obtain no improvement, which inphase with appropriate training tasks to bridge dicates that different downstream tasks may need the task formulation gap. To investigate this, different further pre-training tasks. we carry out a study for improving multiTo investigate this issue, we carry out experiple task-oriented dialogue downstream tasks ments in the area of task-oriented dialogue. We through designing various tasks at the further choose one popular pre-training language model, pre-training phase. The experiment shows that BERT (Devlin et al., 2019a) as our base model, and different downstream tasks prefer different further pre-training tasks, which have intrinsic construct a large scale domain-specific dialogue correlation and most further pre-training tasks corpus which consists of nine task-oriented datasets significantly improve certain target tasks rather for further pre-training (Wu et al., 2020). We also than all. Our investigation indicates that it is of select four core task-oriented dialogue tasks, intent great importance and effectiveness to design recognition, dialogue action prediction, response appropriate further pre-trai"
2021.emnlp-main.178,W17-5526,0,0.0529837,"Missing"
2021.emnlp-main.178,W17-5506,0,0.0152005,"optimizer. Models are early-stopped using the loss of a validation set. We train each downstream task three times with different seeds. We use 4 NVIDIA V100 GPUs for further pretraining and one for fine-tuning. Our code is based on Transformers 3 Following Wu et al. (2020), we construct the further pre-training dataset by combining nine different multi-turn goal-oriented datasets (Frames (El Asri et al., 2017), MetaLWOZ (Lee et al., 2019), WOZ (Mrkši´c et al., 2017), CamRest676 (Wen et al., 2017), MSR-E2E (Li et al., 2018), MWOZ (Budzianowski et al., 2018), Schema (Rastogi et al., 2020), SMD (Eric et al., 2017) and Taskmaster (Byrne et al., 2019)). In total, there are 100,707 dialogues containing 1,388,152 utterances over 60 domains. 2322 3 https://github.com/huggingface/transformers BERT 2 M LM 3 MWOZ f 1micro f 1macro 90.90 81.31 91.51 79.77 BERT 2 M LM 3 MWOZ R100 @1 R100 @3 47.39 74.46 59.55 82.63 DA DSTC2 f 1micro f 1macro 91.16 37.67 87.76 36.99 RS DSTC2 R100 @1 R100 @3 48.33 63.30 54.25 68.91 INT Acc (in) 95.20 95.90 DST accjoint accslot GSIM f 1micro f 1macro 99.07 45.49 99.35 45.70 Acc (all) 84.96 85.10 GSIM R100 @1 R100 @3 19.21 40.02 35.71 58.73 47.96 48.72 Acc (out) 88.70 88.30 Recall (o"
2021.emnlp-main.178,D19-1131,0,0.0132027,"wnstream tasks from the ability and structure perspective. The above four tasks are downstream tasks, the below five tasks are further pre-training tasks. ※ indicates the task has the ability or belongs to the model structure. 4.2 Evaluation Datasets All of the proposed tasks are trained with the masked language model in a multi-task paradigm. In addition, these tasks are optional, we focus on investigating their relations with each downstream task. We select four datasets, OOS, DSTC2, GSIM, and MWOZ, for downstream evaluation. Details of each evaluation dataset are discussed below. 3.4 OOS. (Larson et al., 2019) It contains 151 intent types across ten domains, including 150 in-scope and one out-of-scope intent. Heuristic Analysis on Task Relations between Further Pre-training and Fine-tuning We analyse the task relations from two perspectives: model ability and structure. Ability refers to the information or knowledge the model learns, for example, the ability of single turn representation, the knowledge about the entity. Structure refers to the model’s network structure and its objective function, for example, the siamese structure and list-wise ranking loss function. The details are shown in Table"
2021.emnlp-main.178,2021.ccl-1.108,0,0.0745947,"Missing"
2021.emnlp-main.178,P17-1163,0,0.026871,"Missing"
2021.emnlp-main.178,N18-3006,0,0.0205587,"abilities or has a similar model structure the with the downstream task, then the further pre-training will be more effective for fine-tuning. 4 4.1 Experimental Setup Dialogue Datasets for Further Pre-training DSTC2. (Henderson et al., 2014) It is a machinehuman task-oriented dataset, We follow Wu et al. (2020) to map the original dialogue act labels to universal dialogue acts, resulting in 19 acts. MWOZ. (Budzianowski et al., 2018) It is a popular benchmark for task-oriented dialogues. It has 30 (domain, slot) pairs across seven different domains. We use the revised version MWOZ 2.1. GSIM. (Shah et al., 2018) It is a human-rewrote task-oriented dataset. Following Wu et al. (2020) we combine movie and restaurant domains into one single corpus, and map its dialogue act labels to universal dialogue acts, resulting in 13 acts. 4.3 Training Setting For further pre-training, we set the learning rate equal to 5e-5, batch size to 32, and maximum sequence length to 512. For fine-tuning, we set the learning rate to 5e-5 (except dialog state tracking task, which is 3e-5). We use the batch size that maximizes the GPU usage. We train our models using the Adam optimizer. Models are early-stopped using the loss"
2021.emnlp-main.178,E17-1042,0,0.069424,"Missing"
2021.emnlp-main.178,2020.emnlp-main.66,0,0.232713,"task-oriented dialogue downstream tasks ments in the area of task-oriented dialogue. We through designing various tasks at the further choose one popular pre-training language model, pre-training phase. The experiment shows that BERT (Devlin et al., 2019a) as our base model, and different downstream tasks prefer different further pre-training tasks, which have intrinsic construct a large scale domain-specific dialogue correlation and most further pre-training tasks corpus which consists of nine task-oriented datasets significantly improve certain target tasks rather for further pre-training (Wu et al., 2020). We also than all. Our investigation indicates that it is of select four core task-oriented dialogue tasks, intent great importance and effectiveness to design recognition, dialogue action prediction, response appropriate further pre-training tasks modelselection, and dialog state tracking as the downing specific information that benefit downstream tasks used in fine-tuning phase. We aim stream tasks. Besides, we present multiple constructive empirical conclusions for enhancto explore the following questions: 1) In the area ing task-oriented dialogues. of task-oriented dialogue, can further p"
2021.emnlp-main.186,D16-1091,0,0.159107,"meng@tencent.com, jssu@xmu.edu.cn Abstract and extractive summarization (Barzilay et al., 2002; Nallapati et al., 2012). Dominant sentence ordering models can be Recently, inspired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings b"
2021.emnlp-main.186,N16-1147,0,0.0983536,"Missing"
2021.emnlp-main.186,D18-1465,0,0.174015,"ise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relative orderings between pairwise senconfident pairwise orderings, another classifier tences, which are then leveraged to produce the is used to predict the remaining uncertain pairfinal ordered sentence sequence. Its advantage lies wise orderings. At last, we adapt a GRN-based in the lightweig"
2021.emnlp-main.186,2020.emnlp-main.511,0,0.200556,"eepLearnXMU/IRSEG. ordered sentences. Overall, these two kinds of models have their 1 Introduction own strengths, which are complementary to each With the rapid development and increasing applica- other. To combine their advantages, Yin et al. tions of natural language processing (NLP), model- (2020) propose FHDecoder that is equipped with ing text coherence has become a significant task, s- three pairwise ordering prediction modules to enince it can provide beneficial information for under- hance the pointer network decoder. Along this line, standing, evaluating and generating multi-sentence Cui et al. (2020) introduce BERT to exploit the texts. As an important subtask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering"
2021.emnlp-main.186,N19-1423,0,0.0302209,"Missing"
2021.emnlp-main.186,P11-2022,0,0.0285887,"ns ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005, 2008; Elsner and Charniak, 2011; GuinIn this paper, we propose a novel iterative pairaudeau and Strube, 2013). Recently, neural netwise ordering prediction framework which introwork based sentence ordering models have become duces two classifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise orderings between sentences and et al., 2019, 2021), our framework enriches the then use"
2021.emnlp-main.186,P19-1102,0,0.0244365,".9M 27min57s 24.0M 46min 25.0M 56min 128.0M Table 6: The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline. Model Coherence SE-GRN (Yin et al., 2021) 46.71 59.47 IRSE-GRN IRSE-GRN+FHDecoder IRSE-GRN+BERT+FHDecoder 47.48 49.84 51.01 60.01 61.81 62.87 Table 5: Coherence probabilities of summaries reordered by different models using weights of 0.8 (left) and 0.5 (right). spect the validity of our proposed framework via multi-document summarization. Concretely, we train different neural sentence ordering models on a large-scale summarization corpus (Fabbri et al., 2019), and then individually use them to reorder the small-scale summarization data of DUC2004 (Task2). Finally, we use coherence probability proposed by (Nayeem and Chali, 2017) to evaluate the coherence of summaries. In this group of experiments, we conduct experiments using different weights: 0.5 and 0.8, as implemented in (Nayeem and Chali, 2017) and (Yin et al., 2020) respectively. The results are reported in Table 5. We can observe that the summaries reordered by IRSE-GRN and its variants achieve higher coherence probabilities than baseline, verifying the effectiveness of our proposed framewo"
2021.emnlp-main.186,D19-1633,0,0.0190762,"we can easily adapt the BART encoder as our sentence encoder. With similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrderNet. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. However, significantly different from them, we borrow the idea from the mask-predict framework (Gu et al., 2018; Ghazvininejad et al., 2019; Deng et al., 2020) to progressively incorporate pairwise ordering information into SE-Graph, which is the basis of our graph-based sentence ordering model. To the best of our knowledge, our work is the first attempt to explore iteratively refined GNN for sentence ordering. 3 Background resented as an undirected sentence-entity graph G = (V , E), where V ={vi }Ii=1 ∪{ˆ vj }Jj=1 and E ={ei,i0 }I,I ei,j }I,J ej,j 0 }J,J i=1,j=1 ∪{ˆ i=1,i0 =1 ∪{¯ j=1,j 0 =1 represent the nodes and edges respectively. Here, nodes include sentence nodes (such as vi ) and entity nodes (such as vˆj ), and each edge"
2021.emnlp-main.186,N12-1093,0,0.0223129,"texts. As an important subtask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering other sentences in the same (Konstas and Lapata, 2012; Holtzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence orderi"
2021.emnlp-main.186,P03-1069,0,0.2014,"ltzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005, 2008; Elsner and Charniak, 2011; GuinIn this paper, we propose a novel iterative pairaudeau and Strube, 2013). Recently, neural netwise ordering prediction framework which introwork based sentence ordering models have become duces two classifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise order"
2021.emnlp-main.186,2020.acl-main.703,0,0.0185589,"eness of et al., 2019, 2020), Yin et al. (2019, 2021) repreour framework, we conduct extensive experiments sented input sentences with a unified SE-Graph and on several commonly-used datasets. Experimental then applied GRN to learn sentence representationresults and in-depth analyses show that our model s. Very recently, we notice that Chowdhury et al. enhanced with some proposed technologies (De- (2021) proposes a BART-based sentence ordering vlin et al., 2019; Yin et al., 2020) achieves the model. Please note that our porposed framework state-of-the-art performance. is compatible with BART (Lewis et al., 2020). For 2408 Figure 1: The architecture of SE-GRN model (Yin et al., 2019, 2021). example, we can easily adapt the BART encoder as our sentence encoder. With similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrderNet. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. However, significantly d"
2021.emnlp-main.186,D17-1019,0,0.0162027,"assifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise orderings between sentences and et al., 2019, 2021), our framework enriches the then use them to produce the final sentence order graph representation with iteratively predicted orvia ranking algorithms (Chen et al., 2016; Agrawal derings between pairwise sentences, which further et al., 2016; Li and Jurafsky, 2017; Kumar et al., benefits the subsequent generation of ordered sen2020; Prabhumoye et al., 2020; Zhu et al., 2021). tences. The basic intuitions behind our work are For example, Chen et al. (2016) first framed sentwo-fold. First, learning contextual sentence reptence ordering as a ranking task conditioned on resentations is helpful to predict pairwise orderpairwise scores. Agrawal et al. (2016) conducted ings. Second, difficulties of predicting ordering the same experiments as (Chen et al., 2016) in the vary with respect to different sentence pairs. Thus, task of image caption storytelling. Sim"
2021.emnlp-main.186,P18-1052,0,0.0865212,"e classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relative orderings between pairwise senconfident pairwise orderings, another classifier tences, which are then leveraged to produce the is used to predict the remaining uncertain pairfinal ordered sentence sequence. Its advantage lies wise orderings. At last, we adapt a GRN-base"
2021.emnlp-main.186,D19-1231,0,0.0114904,"., 2002; Nallapati et al., 2012). Dominant sentence ordering models can be Recently, inspired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al."
2021.emnlp-main.186,N16-1098,0,0.0204435,"adapt Equation 1 to incorporate ss-edge weights into the message aggregation of sentence-level nodes: X m(l) i = (l-1) wi,i0 · w(κ(l-1) , κ(l-1) i i0 )κi0 , (6) vi0 ∈Ni w(κ(l-1) , κ(l-1) i i0 ) = σ(Wg [κ(l-1) ; κ(l-1) i i0 ]). Here σ denotes sigmoid function and Wg is learnable parameter matrix. Equation 6 expresses that the sentence-level aggregation should consider not only the semantic representations of the two involved sentences, but also the relative ordering between them. In addition, other Equations are the same as those of conventional GRN, which have been described in Section §3.2. (Mostafazadeh et al., 2016) is about commonsense stories. Both two datasets are composed of 5-sentence stories and randomly split by 8:1:1 for the training/validation/test sets. • NIPS Abstract, AAN Abstract, arXiv Abstract. These three datasets consist of abstracts from research papers, which are collected from NIPS, ACL anthology and arXiv, respectively (Radev et al., 2016; Chen et al., 2016). The partitions for training/validation/test of each dataset are as follows: NIPS Abstract: 2,427/408/377, AAN Abstract: 8,569/962/2,626, arXiv Abstract: 884,912/110,614/110,615 for the training/validation/test sets. Settings. Fo"
2021.emnlp-main.186,2020.emnlp-main.181,0,0.0576172,"Missing"
2021.emnlp-main.186,P13-1010,0,0.0178174,"Missing"
2021.emnlp-main.186,W17-2407,0,0.167596,"erative predictions of pairwise ordering indeed benefit the learning of graph representations. Finally, the result in the last line indicates that removing noisy weights leads to a significant performance drop. It suggests that the utilization of noisy weights is useful for the training of iterative classifier, which makes our model more robust. Ablation Study 5.6 Summary Coherence Evaluation We conduct several experiments to investigate the impacts of our proposed components on ROCstory Following previous studies (Barzilay and Lapata, dataset and arXiv dataset which are the two largest 2005; Nayeem and Chali, 2017), we further in2414 Dataset SE-GRN IRSE-GRN IRSE-GRN+FHDecoder IRSE-GRN+BERT+FHDecoder Runtime #Params Runtime #Params Runtime #Params Runtime #Params NIPS abstract 6s 23.9M 6.2s 24.0M 18s 25.0M 29s 128.0M AAN abstract 31s 23.9M 32.5s 24.0M 1min8s 25.0M 1min20s 128.0M SIND 1min6s 23.9M 1min9s 24.0M 2min3s 25.0M 2min16s 128.0M ROCStory 2min 23.9M 2min5s 24.0M 4min2s 25.0M 4min42s 128.0M arXiv abstract 25min 23.9M 27min57s 24.0M 46min 25.0M 56min 128.0M Table 6: The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline. Model Coherence SE-GRN (Yin et a"
2021.emnlp-main.186,P18-1152,0,0.0172356,"btask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering other sentences in the same (Konstas and Lapata, 2012; Holtzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzil"
2021.emnlp-main.186,P17-1121,0,0.0409366,"Missing"
2021.emnlp-main.186,D19-1232,0,0.0324438,"Missing"
2021.emnlp-main.186,2020.acl-main.248,0,0.221993,"spired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relativ"
2021.emnlp-main.186,Q19-1002,1,0.882575,"Missing"
2021.emnlp-main.186,2020.acl-main.712,1,0.704554,"Missing"
2021.emnlp-main.186,P18-1030,0,0.0181408,"y. During the process of updating hidden states, the messages for each node are aggregated from its adjacent nodes. Specifically, the sentence-level message m(l) i and (l) entity-level message m ˜ i for a sentence si are defined as follows: m(l) i = X (l-1) w(κ(l-1) , κ(l-1) i i0 )κi0 , vi0 ∈Ni m ˆ (l) i = X (l-1) w(κ ¯ (l-1) , (l-1) i j , r ij )j , (1) ˆi vj ∈N In this section, we give a brief introduction to the SE-GRN (Yin et al., 2019, 2021), which is selected as our baseline due to its competitive performance. As shown in Figure 1, SE-GRN is composed of a Bi-LSTM sentence encoder, GRN (Zhang et al., 2018) paragraph encoder, and a pointer network (Vinyals et al., 2015b) decoder. 3.1 Sentence-Entity Graph where κ(l-1) and (l-1) stand for the neighboring senj i0 tence and entity representations of the i-th sentence ˆi denote node vi at the (l − 1)-th layer, Ni and N the sets of neighboring sentences and entities of vi , and both w(∗) and w(∗) ¯ are gating functions with single-layer networks, involving associated node states and edge label rij (if any). Afterwards, κ(l-1) is updated by concatenating i its original representation κ(0) i , the messages from (l) (l) neighbours (mi and m ˆ i ) and t"
2021.emnlp-main.264,2020.emnlp-main.702,0,0.446094,"ring training instead of the inference stage. 4.3 Systems Mixer. A sequence-level training algorithm for text generations by combining both REINFORCE and cross-entropy (Ranzato et al., 2016). Minimal Risk Training. Minimal Risk Training (MRT) (Shen et al., 2016) introduces evaluation metrics (e.g., BLEU) as loss functions and aims to minimize expected loss on the training data. Target denoising. Meng et al. (2020a) and Meng et al. (2020b) propose to add noisy perturbations into decoder inputs for a more robust translation model against prediction errors. TeaForN. Teacher forcing with n-grams (Goodman et al., 2020) enables the standard teacher forcing with a broader view by a n-grams optimization. Sampling based on training steps. For distinction, we name vanilla scheduled sampling as Sampling based on training steps. We defaultly adopt the sigmoid decay following Zhang et al. (2019). Sampling with sentence oracles. Zhang et al. (2019) refine the sampling candidates of scheduled sampling with sentence oracles, i.e., predictions from beam search. Note that its sampling strategy is based on training steps with the sigmoid decay. Evaluation. For the machine translation task, we set the beam size to 4 and t"
2021.emnlp-main.264,P17-2058,0,0.017942,"both training steps and decoding steps can complement each other. 3.2 Sampling Based on Training Steps As the number of the training step i increases, the model should be exposed to its own predictions more frequently. Thus a decay strategy for sam• Sigmoid Decay5 : f (i) = k i , where e is pling golden tokens f (i) (in Section 2.3) is generk+e k the mathematical constant, and k ≥ 1 is a ally used in existing studies (Bengio et al., 2015; hyperparameter to adjust the decay. Zhang et al., 2019). At a specific training step i, 4 given a target sentence, f (i) is only related to i and Following Goyal et al. (2017), model predictions are the weighted sum of target embeddings over output probabilities. equally conducts the same sampling probability for As model predictions cause a mismatch with golden tokens, all decoding steps. Therefore, f (i) simulates an they can simulate translation errors of the inference scene. 5 inference scene with uniform error rates and still For simplicity, we abbreviate the ‘Inverse Sigmoid decay’ (Bengio et al., 2015) to ‘Sigmoid decay.’ remains a gap with the real inference scene. 3287 Figure 4: Examples of different strategies for 1 − g(t) based on the decoding step t. Th"
2021.emnlp-main.264,W04-3250,0,0.26816,"that its sampling strategy is based on training steps with the sigmoid decay. Evaluation. For the machine translation task, we set the beam size to 4 and the length penalty to 0.6 during inference. We use multibleu.perl to cal- Sampling based on decoding steps. Sampling culate cased sensitive BLEU scores for EN-DE based on decoding steps with exponential decay. and EN-FR, and use mteval-v13a.pl script to calculate cased sensitive BLEU scores for ZH-EN. Sampling based on training and decoding steps. Our sampling based on both training steps and deWe use the paired bootstrap resampling methods (Koehn, 2004) to compute the statistical signifi- coding steps with the ‘Composite’ method. cance of translation results. We report mean and 4.4 Main Results standard-error variation of BLEU scores over three runs. For the text summarization task, we respec- Machine Translation. We list translation qualitively set the beam size to 4/5 and length penalty to ties on three WMT tasks in Table 3. The sentence1.0/1.2 for Gigaword and CNN/DailyMail dataset level training based approaches (e.g., Mixer) bring following previous studies (Song et al., 2019; Qi limited improvements due to the high variance of et al.,"
2021.emnlp-main.264,2021.findings-acl.205,1,0.702906,"ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3285–3296 c November 7–11, 2021. 2021 Association for Computational Linguistics of the inference scene during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling candidates by beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel two-pass decoder architecture. Liu et al. (2021) develop a more fine-grained sampling strategy according to the model confidence. Although these sampling-based approaches have been shown effective and training efficient, there still exists an essential issue in their sampling strategies. In the real inference scene, the nature of sequential predictions quickly accumulates errors along with decoding steps, which yields higher error rates for larger decoding steps (Zhou et al., 2019; Zhang et al., 2020a) (Figure 1). However, most sampling-based approaches are merely based on training steps and equally treat all decoding steps2 . Namely, they"
2021.emnlp-main.264,2020.wmt-1.24,1,0.856643,"Missing"
2021.emnlp-main.264,P19-2049,0,0.0653044,"lculate the translation precision (similarly for the error rate) in all experiments. approaches, aiming to simulate the data distribution 3285 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3285–3296 c November 7–11, 2021. 2021 Association for Computational Linguistics of the inference scene during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling candidates by beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel two-pass decoder architecture. Liu et al. (2021) develop a more fine-grained sampling strategy according to the model confidence. Although these sampling-based approaches have been shown effective and training efficient, there still exists an essential issue in their sampling strategies. In the real inference scene, the nature of sequential predictions quickly accumulates errors along with decoding steps, which yields higher error rates for larger decoding steps (Zhou et al., 2019; Zhang et al., 2020a) (Figu"
2021.emnlp-main.264,2020.findings-emnlp.217,0,0.0203261,"sets in Table 1. 4.2 Implementation Details Training Setup. For the translation task, we follow the default setup of the Transformerbase and Transformerbig models (Vaswani et al., 2017), and provide detailed setups in Appendix A (Table 7). All Transformer models are first trained by teacher forcing with 100k steps, and then trained with different training objects or scheduled sampling approaches for 300k steps. All experiments are conducted on 8 NVIDIA V100 GPUs, where each is allocated with a batch size of approximately 4096 tokens. For the text summarization task, we base on the ProphetNet (Qi et al., 2020) and follow its training setups. We set hyperparameters involved in various scheduled sampling strategies (i.e., f (i) and g(t)) according to the performance on validation sets of each tasks and list k in Table 2. For the linear decay, we set  and b to 0.2 and 1, respectively. Please note that scheduled sampling is only used during training instead of the inference stage. 4.3 Systems Mixer. A sequence-level training algorithm for text generations by combining both REINFORCE and cross-entropy (Ranzato et al., 2016). Minimal Risk Training. Minimal Risk Training (MRT) (Shen et al., 2016) introdu"
2021.emnlp-main.264,2020.tacl-1.18,0,0.0789567,"Missing"
2021.emnlp-main.264,D15-1044,0,0.28662,"glish-French, and WMT 2019 Chinese-English, respectively. When comparing with the stronger vanilla scheduled sampling method, our approaches bring further improvements by 0.58, 0.62, and 0.55 BLEU points on these WMT tasks, respectively. Moreover, our approaches generalize well to the text summarization task and achieve consistently better performance 2 For clarity in this paper, ‘training steps’ refer to the number of parameter updates and ‘decoding steps’ refer to the index of decoded tokens on the decoder side. on two popular benchmarks, i.e., CNN/DailyMail (See et al., 2017) and Gigaword (Rush et al., 2015). The main contributions of this paper can be summarized as follows3 : • To the best of our knowledge, we are the first that propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors, and provide in-depth analyses on the necessity of our proposals. • We investigate scheduled sampling based on both training steps and decoding steps, which yields further improvements, suggesting that our proposals complement existing studies. • Experiments on three large-scale WMT tasks and two popular text summarization tasks confir"
2021.emnlp-main.264,P17-1099,0,0.0319791,"2014 EnglishGerman, WMT 2014 English-French, and WMT 2019 Chinese-English, respectively. When comparing with the stronger vanilla scheduled sampling method, our approaches bring further improvements by 0.58, 0.62, and 0.55 BLEU points on these WMT tasks, respectively. Moreover, our approaches generalize well to the text summarization task and achieve consistently better performance 2 For clarity in this paper, ‘training steps’ refer to the number of parameter updates and ‘decoding steps’ refer to the index of decoded tokens on the decoder side. on two popular benchmarks, i.e., CNN/DailyMail (See et al., 2017) and Gigaword (Rush et al., 2015). The main contributions of this paper can be summarized as follows3 : • To the best of our knowledge, we are the first that propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors, and provide in-depth analyses on the necessity of our proposals. • We investigate scheduled sampling based on both training steps and decoding steps, which yields further improvements, suggesting that our proposals complement existing studies. • Experiments on three large-scale WMT tasks and two popula"
2021.emnlp-main.264,P16-1162,0,0.058466,"more predicted tokens to the model with the increase of both i and t (Figure 6 (c)). We will analyze effects of different h(i, t) in Section 5.2. 4 Experiments We validate our proposals on two important sequence generation tasks, i.e., machine translation and text summarization. 4.1 Tasks and Datasets Machine Translation. We use the standard WMT 2014 English-German (EN-DE), WMT 2014 English-French (EN-FR), and WMT 2019 ChineseEnglish (ZH-EN) datasets. We respectively build a shared source-target vocabulary for EN-DE and EN-FR, and unshared vocabularies for ZH-EN. We apply byte-pair encoding (Sennrich et al., 2016) with 32k merge operations for all datasets. Text Summarization. We use two popular sum8 marization datasets: (a) the non-anonymized verWe also tried f (i · (1 − g(t)) in preliminary experiments, but it slightly underperformed the above g(t · (1 − f (i))). sion of the CNN/DailyMail dataset (See et al., 3289 Variable Task Training Steps f (i) (vanilla) Decoding Steps g(t) (ours) Maximum Value Translation Summarization Translation Summarization 300,000 100,000 128 512 Hyperparameter k Linear Exponential Sigmoid -1/150,000 0.99999 20,000 -1/50,000 0.9999 15,000 -1/64 0.99 20 -1/256 0.999 50 Table"
2021.emnlp-main.264,P16-1159,0,0.310399,"lations (Zhou et al., 2019; Zhang et al., 2020a) (shown in Figure 1). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they 1 Introduction mainly fall into two categories. The one is sentenceNeural Machine Translation (NMT) has made level training, which treats the sentence-level metpromising progress in recent years (Sutskever et al., ric (e.g., BLEU) as a reward, and directly maxi2014; Bahdanau et al., 2015; Vaswani et al., 2017). mizes the expected rewards of generated sequences Generally, NMT models are trained to maximize (Ranzato et al., 2016; Shen et al., 2016; Rennie 1 et al., 2017; Pang and He, 2021). Although intuTo calculate the precision for training, we strictly match predicted tokens with ground-truth tokens word by word. itive, they generally suffer from slow and unstable When inference, we relax the strict matching to the fuzzy training due to the high variance of policy gradients matching within a local window of size 3, and truncate or and the credit assignment problem (Sutton, 1984; pad hypotheses to the same length of golden references. We also explore n-gram matching in preliminary experiments and Wiseman and Rush, 2016; Liu et al., 2"
2021.emnlp-main.264,P18-1083,0,0.0610921,"Missing"
2021.emnlp-main.264,D16-1137,0,0.0582174,"Missing"
2021.emnlp-main.264,P19-1426,1,0.760738,"Missing"
2021.emnlp-main.264,Q19-1006,0,0.11075,"ps. We randomly sample 100k training data from WMT 2014 EN-DE and report the average precision of 1k tokens for each decoding step1 . the likelihood of next token given previous golden tokens as inputs, i.e., teacher forcing (Salakhutdinov, 2014). However, at the inference stage, golden tokens are unavailable. The model is exposed to an unseen data distribution generated by itself. This discrepancy between training and inference is named as the exposure bias problem (Ranzato et al., 2016). With the growth of decoding steps, such discrepancy becomes more problematic due to error accumulations (Zhou et al., 2019; Zhang et al., 2020a) (shown in Figure 1). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they 1 Introduction mainly fall into two categories. The one is sentenceNeural Machine Translation (NMT) has made level training, which treats the sentence-level metpromising progress in recent years (Sutskever et al., ric (e.g., BLEU) as a reward, and directly maxi2014; Bahdanau et al., 2015; Vaswani et al., 2017). mizes the expected rewards of generated sequences Generally, NMT models are trained to maximize (Ranzato et al., 2016; Shen et al., 2016; Rennie"
2021.emnlp-main.31,2020.findings-emnlp.372,0,0.154717,"arding the student performance and learning efficiency? In this paper, we propose a dynamic knowledge distillation (Dynamic KD) framework, which attempts to empower the student to adjust the learn1 Introduction ing procedure according to its competency. SpecifiKnowledge distillation (KD) (Hinton et al., 2015) cally, inspired by the success of active learning (Setaims to transfer the knowledge from a large teacher tles, 2009), we take the prediction uncertainty, e.g., model to a small student model. It has been widely the entropy of the predicted classification probabilused (Sanh et al., 2019; Jiao et al., 2020; Sun et al., ity distribution, as a proxy of the student compe2019) to compress large-scale pre-trained language tency. We strive to answer the following research models (PLMs) like BERT (Devlin et al., 2019) questions: (RQ1) Which teacher is proper to learn and RoBERTa (Liu et al., 2019) in recent years. as the student evolves? (RQ2) Which data are acBy knowledge distillation, we can obtain a much tually useful for student models in the whole KD smaller model with comparable performance, while stage? (RQ3) Does the optimal learning objecgreatly reduce the memory usage and accelerate tive cha"
2021.emnlp-main.31,2020.emnlp-main.242,0,0.0165177,"ns of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our e"
2021.emnlp-main.31,2021.findings-emnlp.43,1,0.827368,"Missing"
2021.emnlp-main.31,D13-1170,0,0.00536781,"Missing"
2021.emnlp-main.31,P19-1355,0,0.0556813,"Missing"
2021.emnlp-main.31,D19-1441,0,0.262711,"and the teacher for input x, respectively. The KD can be conducted by minimizing the KullbackLeibler (KL) divergence distance between the student and teacher prediction: ters are updated according to the KD loss and the original classification loss, i.e., the cross-entropy over the ground-truth label y: LCE = −y log σ (S (x)) , L = (1 − λKL )LCE + λKL LKL , (2) (3) where λKL is the hyper-parameter controlling the weight of knowledge distillation objective. Recent explorations also find that introducing KD objectives of alignments between the intermediate representations (Romero et al., 2015; Sun et al., 2019) and attention map (Jiao et al., 2020; Wang et al., 2020) is helpful. Note that conventional KD framework is static, i.e., the teacher model is selected before KD and the training is conducted on all training instances indiscriminately according to the predefined objective and the corresponding weights of different objectives. However, it is unreasonable to conduct the KD learning procedure statically as the student model evolves during the training. We are curious whether adaptive adjusting the settings on teacher adoption, dataset selection and supervision adjustment can bring benefits regar"
2021.emnlp-main.31,2021.ccl-1.108,0,0.0904298,"Missing"
2021.emnlp-main.31,P11-1015,0,0.513843,"the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for KD is significant supposed to provide better supervision signal for for the student performance. While the capacity the stud"
2021.emnlp-main.31,Q19-1040,0,0.0129046,"-depth investigations. Note that BERTBASE and BERTLARGE also differs from the number of hidden size, the experiments regarding the hidden size, where the phenomenon also exists and corresponding results can be found in Appendix C. Specifically, we are curious about whether learning from a bigger PLM with better performance can lead to a better distilled student model. We conduct probing experiments to distill a 6-layer student BERT model from BERTBASE with 12 layers, and BERTLARGE with 24 layers, respectively. We conducts the experiment on two datasets, RTE (Bentivogli et al., 2009) and CoLA (Warstadt et al., 2019), where two teacher models exhibit clear performance gap, and a sentiment classification benchmark IMDB (Maas et al., 2011). Detailed experimental setup can be found in Appendix A. As shown in Table 1, we surprisingly find that while the BERTLARGE teacher clearly outperforms the small BERTBASE teacher model, the student model distilled by the BERTBASE teacher achieves 3.1.2 Uncertainty-based Teacher Adoption better performance on all three datasets. This phe- Our preliminary observations demonstrate that senomenon is counter-intuitive as a larger teacher is lecting a proper teacher model for K"
2021.emnlp-main.31,N18-1101,0,0.0122047,". To verify this, we turn to the the setting where the original training dataset is enriched with augmentation techniques. Settings We conduct the investigation experi- Results with Augmented Dataset Following ments on two sentiment classification datasets TinyBERT (Jiao et al., 2020), we augment the trainIMDB (Maas et al., 2011) and SST-5 (Socher et al., ing dataset 20 times with BERT mask language 2013), and natural language inference tasks in- prediction, as it has been prove effective for discluding MRPC (Dolan and Brockett, 2005) and tilling a powerful student model. Our assumption MNLI (Williams et al., 2018). The statistics of is that with the data augmentation technique, the dataset and the implementation details can be found training set can sufficiently cover the possible data 383 #FLOPs SST-5 IMDB MRPC MNLI-m / mm Avg. (↑) ∆ (↓) - 53.7 88.8 87.5 83.9 / 83.4 79.5 - TinyBERT TinyBERT 24.9B 24.9B 51.4 87.6 86.4 86.2 82.5 / 81.8 82.6 / 82.0 78.0 0.0 Random Uncertainty-Entropy Uncertainty-Margin Uncertainty-LC 2.49B 4.65B 4.65B 4.65B 51.1 51.5 51.6 51.2 87.0 87.7 87.7 87.7 83.3 86.5 86.5 86.5 80.8 / 80.5 81.8 / 81.0 81.6 / 81.1 81.4 / 80.8 76.5 77.7 77.7 77.5 1.5 0.3 0.3 0.5 Method BERTBASE (Teach"
2021.emnlp-main.31,2021.findings-acl.387,0,0.0196133,"ork Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision adjustment. Our experimental results demonstrate that the dynamical adjust"
2021.emnlp-main.31,2020.acl-main.620,0,0.0412336,"the preliminary explorations on the three aspects of Dynamic KD, we observe that it is promising for improving the efficiency and the distilled student performance. Here we provide potential directions for further investigations. (1) From uncertainty-based selection criterion to advanced methods. In this paper, we utilize student prediction uncertainty as a proxy for selecting teachers, training instances and supervision objectives. More advanced methods based on more 3.3.2 Experiments accurate uncertainty estimations (Gal and GhahraSettings The student model is set to 6-layer and mani, 2016; Zhou et al., 2020), clues from training BERTBASE is adopted as the teacher model. For dynamics (Toneva et al., 2018), or even a learnable intermediate layer representation alignment, we selector can be developed. adopt the Skip strategy, i.e., Ipt = {2, 4, 6, 8, 10} (2) From isolation to integration. As a prelimas it performs best as described in BERT-PKD. We inary study, we only investigate the three dimenconduct experiments on the sentiment analysis task sions independently. Future work can adjust these SST-5, and two natural language inference tasks components simultaneously and investigate the unMRPC and RT"
2021.emnlp-main.31,2020.emnlp-main.633,0,0.0112894,"ect of combinations of different objectives can be promising. 5 Related Work Our work relates to recent explorations on applying KD for compressing the PLMs and active learning. Knowledge Distillation for PLMs Knowledge distillation (Hinton et al., 2015) aims to transfer the dark knowledge from a large teacher model to a compact student model, which has been proved effective for obtaining compact variants of PLMs. Those methods can be divided into general distillation (Sanh et al., 2019; Turc et al., 2019; Wang et al., 2020) and task-specific distillation (Sun et al., 2019; Jiao et al., 2020; Xu et al., 2020; Li et al., 2020a; Liang et al., 2021; Li et al., 2020b; Wu et al., 2021). The former conducts KD on the general text corpus while the latter trains the student model on the task-specific datasets. In this paper, we focus on the latter one as it is more widely adopted in practice. Compared to existing static KD work, we are the first to explore the idea of Dynamic KD, making it more flexible, efficient and effective. 6 Conclusion In this paper, we introduce dynamic knowledge distillation, and conduct exploratory experiments regarding teacher model adoption, data selection and the supervision"
2021.emnlp-main.366,P19-1279,0,0.0401504,"Missing"
2021.emnlp-main.366,P17-1171,0,0.0267983,"The second challenge is that RE models need to synthesize all information in multiple text paths to obtain the final relation. Open Setting. This setting fully tests the ability of RE in the wild. Given a target entity pair, models need to first retrieve relevant documents for the entity pair from full English Wikipedia corpus (5, 882, 234 documents in total, 3, 646 reasoning text path candidates for each entity pair on average), then perform cross-document reasoning with the retrieved documents to predict the relation. Compared with natural language queries in open domain question answering (Chen et al., 2017), the sparse query information in entity pairs presents unique challenges to document retrieval ability. The second challenge comes from both the quadratic number of potential paths (efficiency), and the finegrained influence of document retrieval on the extraction of relations (effectiveness). 4 Data Analysis In this section, we present data analysis of CodRED, including data statistics, required abilities in our dataset, and cross-document relation instances. Data Statistics. CodRED enjoys diversity in open 4455 Here we use entity names to predict the relations, since we find it can effectiv"
2021.emnlp-main.366,N19-1423,0,0.0353152,"Missing"
2021.emnlp-main.366,Q17-1008,0,0.0179577,"g on shallow correlation between relations and entity names. In this sense, CodRED provides a more reasonable benchmark for knowledge acquisition systems. End-to-end Ent. Ctx. X AUC F1 P@500 10.46 21.19 21.70 X X X 12.72 17.45 25.46 30.54 25.40 30.60 X X X 41.76 47.94 47.33 51.26 58.60 62.80 Table 7: Ablation results on entity names (Ent.) and context (Ctx.). Han et al., 2018; Mesquita et al., 2019) or distant supervision (Riedel et al., 2010; Zhang et al., 2017; Elsahar et al., 2018). (2) Cross-sentence RE datasets focus on extracting cross-sentence relations from documents (Li et al., 2016; Peng et al., 2017; Quirk and Poon, 2017; Yao et al., 2019) or dialogues (Yu et al., 2020). Notably, NIST TAC SM-KBP 2019 Track6 aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent"
2021.emnlp-main.366,2020.acl-main.444,0,0.0784142,"s of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., single sentences or gies, where text paths are ranked by the heuristic documents, CodRED presents unique challenges in scores. Specifically, the score of a text path (dh , dt ) document retrieval and cross-document reasoning. is given by: (1) entity count: multiplication of the Intra- and Cross-Document Reasoning. Cross- occurrence number of h in dh and the occurrence number of t in dt , ("
2021.emnlp-main.366,P18-1199,0,0.0127727,", which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inev"
2021.emnlp-main.366,E17-1110,0,0.164516,"an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or doc- ing text paths is complementary to each other and suggests the relation between Amun-her-khepeshef uments containing both two target entities, which inevitably limits the coverage of knowledge acqui- and Merneptah is sibling. sition. According to our statistics on Wikipedia Although several datasets have been proposed ∗ for inv"
2021.emnlp-main.366,D15-1203,0,0.0180787,"all text paths. gated representation x. The aggregated entity pair representation x is then fed into a fully connected layer followed by a softmax layer to obtain the distribution of the relation between the entity pair. Besides the entity-level supervision, we also incorporate path-level supervision using an auxiliary classification task, where models need to predict the relation expressed in each path based on pi . 6 Experiments In this section, we assess the challenges of CodRED in both closed and open benchmark settings. 6.1 Evaluation Metrics In closed setting, following previous works (Zeng et al., 2015; Lin et al., 2016), we evaluate our model using aggregate precision-recall curves, and report the area under curve (AUC), the maximum F1 on the curve and Precision@K (P@K). In open setting, we first retrieve relevant documents (top 16 paths) from full Wikipedia corpus, and then use the models trained in the closed setting to infer the relation between the entity pair. We report the mean average precision (MAP), Recall@K (R@K) and mean reciprocal rank (MRR) to show the performance of document retrieval. 5.2.2 End-to-end Model 6.2 Overall Results Despite their simplicity, pipeline models usuall"
2021.emnlp-main.366,C14-1220,0,0.0791534,"Missing"
2021.emnlp-main.366,D17-1004,0,0.157455,"words, presenting challenges for model- a document set D (i.e., full Wikipedia corpus), we ing long text in both efficiency and effectiveness. first find relevant documents to extract their relaWe refer readers to the appendix for more details. tion. Due to the large number of possible docuRequired Abilities. We compare required abili- ments containing h and t respectively, we explore ties of CodRED with existing RE datasets in Ta- several strategies to retrieve the relevant documents ble 2, including (1) sentence-level RE datasets TA- and connect them into text paths. Specifically, we CRED (Zhang et al., 2017), FewRel (Han et al., enumerate all possible text paths between the tar2018) and KnowledgeNet (Mesquita et al., 2019), get entity pairs (i.e., two documents that contain and (2) document-level RE datasets BC5CDR (Li h and t respectively with shared entities) as candiet al., 2016), DocRED (Yao et al., 2019) and Di- dates. We first present a random baseline, where alogRE (Yu et al., 2020). Compared with existing the candidate paths are randomly sampled. We also RE datasets that mainly focus on extracting rela- experiment with several heuristic retrieval stratetions from local contexts, i.e., sin"
2021.emnlp-main.366,D12-1110,0,0.0561042,"re of each phase, but also 1 Introduction the intrinsic inter-dependence among the phases. Fig. 1 shows an example for cross-doc RE, in Relation extraction (RE), which aims to extract which Amun-her-khepeshef and Merneptah do not relations between entities from plain text, serves co-appear in a single document. To identify their as an essential resource in populating knowledge bases (KBs) from large-scale corpora automati- relation, we need to first retrieve the relevant documents for each entity and then recognize two reacally. Existing RE systems typically focus on either sentence-level RE (Socher et al., 2012; Zeng et al., soning text paths in these documents. The first reasoning text path (the documents titled “Nefer2014, 2015; Lin et al., 2016; Qin et al., 2018) or document-level RE (Li et al., 2016; Peng et al., tari” and “Memeptah”) shows that both target entities are the son of Ramesses II, and the second 2017; Quirk and Poon, 2017; Yao et al., 2019), and have achieved promising results on several pub- one indicates that they also share a common sister lic benchmarks. However, these works can only Meritamen. The information of these two reasonextract relational facts from single sentences or"
2021.emnlp-main.366,Q18-1021,0,0.0152335,"aims to extract and link document-level KBs from different languages and modalities. However, these datasets are still limited at sentence-level or document-level without considering cross-document reasoning, which restricts the coverage of knowledge acquisition. Hence, we extend RE to cross-document level, and construct a large-scale human-annotated dataset CodRED to facilitate further research. Cross-document natural language understanding has received increasing interest in recent years. Several datasets have been constructed including cross-document question answering (Yang et al., 2018; Welbl et al., 2018) and cross-document summarization (Over and Yen, 2004; Owczarzak and Dang, 2011; Fabbri et al., 2019). In comparison with existing datasets, our dataset is tailored for the task of RE with fine-grained path and evidence annotations, and investigates the more open and challenging scenario of knowledge acquisition. 8 Conclusion A variety of RE datasets have been constructed to promote the development of RE systems in recent years, which can be categorized in two main categories: (1) Sentence-level RE datasets focus on extracting relations on sentence-level, where the composing entities of a rela"
2021.emnlp-main.6,D19-1459,0,0.0282992,"for human evaluation: (1) Coherence measures whether the translation is semantically coherent with the dialogue history; (2) Speaker measures whether the translation preserves the personality of the speaker; (3) Fluency measures whether the translation is fluent and gramDialogue Coherence Following (Lapata and Barzilay, 2005; Xiong et al., 2019), we measure dialogue coherence as sentence similarity, which is determined by the cosine similarity between two sentences s1 and s2 : 10 https://code.google.com/archive/p/word2vec/ Due to no available German dialogue datasets, we choose Taskmaster-1 (Byrne et al., 2019), where the English side of BConTrasT (Farajian et al., 2020) also comes from it. 11 sim(s1 , s2 ) = cos(f (s1 ), f (s2 )), 74 matically correct. First, we randomly sample 200 conversations from the test set of BMELD in Zh⇒En direction. Then, we use the 6 models in Tab. 6 to generate the translated utterances of these sampled conversations. Finally, we assign the translated utterances and their corresponding dialogue history utterances in target language to three postgraduate human annotators, and ask them to make evaluations from the above three criteria. The results in Tab. 6 show that our m"
2021.emnlp-main.6,2021.acl-long.444,1,0.676997,"onality, (4) we design the speaker identification task that judges whether the translated text is consistent with the personality of its original speaker. Together with the main chat translation task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Ex"
2021.emnlp-main.6,D11-1084,0,0.0312702,"Missing"
2021.emnlp-main.6,2021.acl-long.310,1,0.78471,"Yu )). (5) For a training sample (CYs u , Yu ) with s∈{sx, sy}, we also use the NCT encoder to obtain the representations HYu of the target utterance Yu and HCYs u of the given speaker-specific history context CYs u . P|Yu |L Similar to the NUD task, HYu = |Y1u |t=1 he,t and L s the he,0 of CYu is used as HCYs . Then, to estimate u the probability in Eq. 5, the concatenation of HYu and HCYs is fed into a binary SI classifier, which u is another fully-connected layer on top of the NCT encoder: Speaker Identification (SI). As explored in (Bak and Oh, 2019; Wu et al., 2020; Liang et al., 2021b; Lin et al., 2021), the history utterances of a speaker can reflect a distinctive personality. Fig. 3(d) depicts the SI task in detail, where the NCT model is used to distinguish whether a translated utterance and a given speaker-specific history utterances are spoken by the same speaker. We also construct positive and negative training samples from the chat corpus. A positive sample (CYsxu , Yu ) with the label ` = 1 consists of the target utterance Yu and the speaker sx-specific history context CYsxu , because Yu is the translation of the utterance originally spoken by the speaker sx. A negative sample p(` ="
2021.emnlp-main.6,2020.acl-main.321,0,0.209009,"et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a chat translator can be applied to help participants communi"
2021.emnlp-main.6,2020.emnlp-main.742,0,0.0181876,"nslations through multi-task learning. Cross-lingual Response Generation (CRG). The CRG task is similar to the MRG as shown in Fig. 3(b), where the NCT model is trained to generate the corresponding utterance Yu in target language which is coherent to the given dialogue history context CXu in source language. We first use the encoder of the NCT model to encode CXu , and then use the NCT decoder to predict Yu . The training objective of this task can be formulated as: 3.3.1 Dialogue Coherence Modeling Many studies (Kuang et al., 2018; Wang et al., 2019b; Xiong et al., 2019; Wang and Wan, 2019; Huang et al., 2020) have indicated that the modeling of global textual coherence can lead to more coherent text generation. Inspired by this, we add two response generation tasks and an utterance discrimination task during the NCT model training. All the three tasks are related to the dialogue coherence of conversations, thus introducing the modeling of dialogue coherence into the NCT model. LCRG = − |Yu | X log(p(Yu,t |CXu , Yu,<t )), t=1 p(Yu,t |CXu , Yu,<t ) = Softmax(Wc hL d,t + bc ), where hL d,t denotes the top-layer decoder hidden state at the t-th decoding step, Wcrg and bcrg are trainable parameters. No"
2021.emnlp-main.6,P18-1118,0,0.0191437,"In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that spe"
2021.emnlp-main.6,W18-6311,0,0.0725571,"“CSA-NCT” represents our proposed approach. Existing Context-Aware NMT Systems. Results on En⇔De. Under the Base setting, our model substantially outperforms the sentencelevel/context-aware baselines by a large margin (e.g., the previous best “Gate-Transformer+FT”), 1.02↑ on En⇒De and 1.12↑ on De⇒En. In term of TER, CSA-NCT also performs better on the two directions, 0.9↓ and 0.7↓ lower than “GateTransformer+FT” (the lower the better), respectively. Under the Big setting, on En⇒De and De⇒En, our model consistently surpasses the baselines and other existing systems again. • Dia-Transformer+FT (Maruf et al., 2018): The original model is RNN-based and an additional encoder is used to incorporate the mixed-language dialogue history. We reimplement it based on Transformer where an additional encoder layer is used to introduce the dialogue history into NMT model. • Doc-Transformer+FT (Ma et al., 2020): A state-of-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the dialogue history. • Gate-Transformer+FT (Zhang et al., 2018): A document-aware Transformer that uses a gate to incorporate the context information. Note that we share the Transformer encoder to"
2021.emnlp-main.6,2020.emnlp-main.175,0,0.0345857,"Missing"
2021.emnlp-main.6,N19-1313,0,0.0122593,"t al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a chat translator can be"
2021.emnlp-main.6,D18-1325,0,0.0154581,"al Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking"
2021.emnlp-main.6,W04-3250,0,0.035116,"raining objective is finally formulated as Experiments Datasets and Metrics Datasets. As shown in Algorithm 1, the training of our CSA-NCT framework consists of two stages: (1) pre-train the model on a large-scale sentencelevel NMT corpus (WMT208 ); (2) fine-tune on the chat translation corpus (BConTrasT (Farajian et al., 2020) and BMELD (Liang et al., 2021a)). The dataset details (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is set to T2 = 5,000. The batch size for each GPU is set to 4096 tokens. All experime"
2021.emnlp-main.6,C18-1050,0,0.017875,"the NCT model can be enhanced to generate more coherent and speaker-relevant translations through multi-task learning. Cross-lingual Response Generation (CRG). The CRG task is similar to the MRG as shown in Fig. 3(b), where the NCT model is trained to generate the corresponding utterance Yu in target language which is coherent to the given dialogue history context CXu in source language. We first use the encoder of the NCT model to encode CXu , and then use the NCT decoder to predict Yu . The training objective of this task can be formulated as: 3.3.1 Dialogue Coherence Modeling Many studies (Kuang et al., 2018; Wang et al., 2019b; Xiong et al., 2019; Wang and Wan, 2019; Huang et al., 2020) have indicated that the modeling of global textual coherence can lead to more coherent text generation. Inspired by this, we add two response generation tasks and an utterance discrimination task during the NCT model training. All the three tasks are related to the dialogue coherence of conversations, thus introducing the modeling of dialogue coherence into the NCT model. LCRG = − |Yu | X log(p(Yu,t |CXu , Yu,<t )), t=1 p(Yu,t |CXu , Yu,<t ) = Softmax(Wc hL d,t + bc ), where hL d,t denotes the top-layer decoder h"
2021.emnlp-main.6,P19-1116,0,0.0223492,"Missing"
2021.emnlp-main.6,D15-1130,0,0.0255851,"oposed approach. 1 Figure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., th"
2021.emnlp-main.6,P02-1040,0,0.1097,"ther with the main chat translation task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Extensive experiments on datasets of different language pairs demonstrate that our model with multi-task learning achieves the state-ofthe-art performances on the ch"
2021.emnlp-main.6,P18-1117,0,0.0188357,"ever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as"
2021.emnlp-main.6,P19-1050,0,0.0330435,"Missing"
2021.emnlp-main.6,I17-3009,0,0.0152874,"gure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherenc"
2021.emnlp-main.6,W18-6319,0,0.016912,"raining, with the main chat translation task and four auxiliary tasks, the total training objective is finally formulated as Experiments Datasets and Metrics Datasets. As shown in Algorithm 1, the training of our CSA-NCT framework consists of two stages: (1) pre-train the model on a large-scale sentencelevel NMT corpus (WMT208 ); (2) fine-tune on the chat translation corpus (BConTrasT (Farajian et al., 2020) and BMELD (Liang et al., 2021a)). The dataset details (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is"
2021.emnlp-main.6,2020.wmt-1.60,0,0.0508315,"Missing"
2021.emnlp-main.6,2020.wmt-1.74,0,0.0176454,"ations. Acknowledgements Related Work The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialogue dataset and design a variational framework for NCT. Different"
2021.emnlp-main.6,D19-1085,0,0.0969453,"al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a ch"
2021.emnlp-main.6,P16-1162,0,0.146835,"Missing"
2021.emnlp-main.6,2006.amta-papers.25,0,0.320291,"ion task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Extensive experiments on datasets of different language pairs demonstrate that our model with multi-task learning achieves the state-ofthe-art performances on the chat translation task and signif"
2021.emnlp-main.6,D17-1301,0,0.0245697,"gure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherenc"
2021.emnlp-main.6,L16-1436,0,0.0257929,"at our model yields more coherent and speaker-relevant translations. Acknowledgements Related Work The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialo"
2021.emnlp-main.6,2020.amta-research.11,0,0.0387902,"ls (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is set to T2 = 5,000. The batch size for each GPU is set to 4096 tokens. All experiments in the first stage are conducted utilizing 8 NVIDIA Tesla V100 GPUs, while we use 4 GPUs for the second stage, i.e., fine-tuning. That gives us about 8*4096 and 4*4096 tokens per update for all experiments in the first-stage and second-stage, respectively. All models are optimized using Adam (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.998, and learning rate is set to 1.0 for a"
2021.emnlp-main.6,2021.naacl-industry.14,0,0.0299036,"d suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialogue dataset and design a variational framework for NCT. Different from them, we focus on introducing the modeling of dialogue coherence and speaker personality into the NCT model with multi-task learning to promote the translation quality. References JinYeong Bak and Alice Oh. 2019. Variational hierarchical user-based conversation model. In Proceedings of EMNLP-IJCNLP, pages 1941–1950. Calvin Bao, Yow-"
2021.emnlp-main.6,W17-4811,0,0.0197872,"a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation invol"
2021.emnlp-main.6,W18-6312,0,0.023112,"ranslating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter fo"
2021.emnlp-main.6,D19-1511,0,0.0743925,"al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a ch"
2021.emnlp-main.6,Q18-1029,0,0.0199723,"MT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and an"
2021.emnlp-main.6,2020.acl-main.7,0,0.0390104,"1|CYsxu , Yu )) − log(p(` = 0|CYsyu , Yu )). (5) For a training sample (CYs u , Yu ) with s∈{sx, sy}, we also use the NCT encoder to obtain the representations HYu of the target utterance Yu and HCYs u of the given speaker-specific history context CYs u . P|Yu |L Similar to the NUD task, HYu = |Y1u |t=1 he,t and L s the he,0 of CYu is used as HCYs . Then, to estimate u the probability in Eq. 5, the concatenation of HYu and HCYs is fed into a binary SI classifier, which u is another fully-connected layer on top of the NCT encoder: Speaker Identification (SI). As explored in (Bak and Oh, 2019; Wu et al., 2020; Liang et al., 2021b; Lin et al., 2021), the history utterances of a speaker can reflect a distinctive personality. Fig. 3(d) depicts the SI task in detail, where the NCT model is used to distinguish whether a translated utterance and a given speaker-specific history utterances are spoken by the same speaker. We also construct positive and negative training samples from the chat corpus. A positive sample (CYsxu , Yu ) with the label ` = 1 consists of the target utterance Yu and the speaker sx-specific history context CYsxu , because Yu is the translation of the utterance originally spoken by"
2021.emnlp-main.6,D19-1081,0,0.0254488,"Missing"
2021.emnlp-main.6,P19-1193,0,0.0575932,"Missing"
2021.emnlp-main.6,D18-1049,0,0.0126714,"setting, on En⇒De and De⇒En, our model consistently surpasses the baselines and other existing systems again. • Dia-Transformer+FT (Maruf et al., 2018): The original model is RNN-based and an additional encoder is used to incorporate the mixed-language dialogue history. We reimplement it based on Transformer where an additional encoder layer is used to introduce the dialogue history into NMT model. • Doc-Transformer+FT (Ma et al., 2020): A state-of-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the dialogue history. • Gate-Transformer+FT (Zhang et al., 2018): A document-aware Transformer that uses a gate to incorporate the context information. Note that we share the Transformer encoder to obtain the context representation instead of utilizing the additional context encoder, which performs better in our experiments. 4.5 Results on En⇔Zh. We also conduct experiments on the BMELD dataset. Concretely, on En⇒Zh and Zh⇒En, our model also presents notable improvements over all comparison models by at least 2.43↑ and 0.77↑ BLEU gains under the Base setting, and by 1.73↑ and 1.43↑ BLEU gains under the Big setting, respectively. These results demonstrate t"
2021.emnlp-main.6,2020.emnlp-main.279,0,0.0525763,"Missing"
2021.emnlp-main.659,2020.findings-emnlp.373,0,0.0290359,"obabilities hardly change. Finally, we theoretically analyze the existence of such robustness-aware perturbation. Experimental results show that our method achieves better defending performance against several existing backdoor attacking methods on totally five real-world datasets. Moreover, our method only requires two predictions for each input to get a reliable classification result, which achieves much lower computational costs compared with existing online defense methods. 2 2.1 Related Work Backdoor Attack 2021b). Besides using static and naively chosen triggers, Zhang et al. (2020) and Chan et al. (2020) also make efforts to implement context-aware attacks. Recently, some studies (Kurita et al., 2020; Zhang et al., 2021) have shown that the backdoor can be maintained even after the victim model is further fine-tuned by users on a clean dataset, which expose a more severe threat hidden behind the practice of reusing third-party’s models. 2.2 Backdoor Defense Against much development of backdoor attacking methods in computer vision (CV), effective defense mechanisms are proposed to protect image classification systems. They can be mainly divided into two types: (1) Online defenses (Gao et al.,"
2021.emnlp-main.659,N19-1423,0,0.0333982,"fense method based on robustness-aware perturbations (RAPs) against textual backdoor attacks. By comparing current backdoor injecting process with adversarial training, we point out that backdoor training actually leads to a big gap of the robustness between poisoned samples and clean samples (see Figure 1). Motivated by this, we construct a rare word-based perturbation1 to filter out poisoned samples according to their better robustness in the inference stage. Specifically, when inDeep neural networks (DNNs) have shown great success in various areas (Krizhevsky et al., 2012; He et al., 2016; Devlin et al., 2019; Liu et al., 2019). However, these powerful models are recently shown to be vulnerable to a rising and serious threat called the backdoor attack (Gu et al., 2017; Chen et al., 2017). Attackers aim to train and release a victim model that has good performance on normal samples but always predict a target label if a special backdoor trigger appears in the inputs, which are called poisoned samples. Current backdoor attacking researches in natural language process (NLP) (Dai et al., 2019; 1 In here, the perturbation means inserting/adding a new Garg et al., 2020; Chen et al., 2020; Yang et al., t"
2021.emnlp-main.659,2020.acl-main.249,0,0.0613432,"Missing"
2021.emnlp-main.659,2021.ccl-1.108,0,0.0736284,"Missing"
2021.emnlp-main.659,P11-1015,0,0.048303,"han 0. 5 The proof is in the Appendix A choose to construct such a qualified RAP by pre8369 x2 ∼DDT specifying a rare word and manipulating its word embedding parameters. Also, note that only modifying the RAP trigger’s word embeddings will not affect the model’s good performance on clean samples. 4 Experiments 4.1 Experimental Settings As discussed before, we assume defenders/users get a suspicious model from a third-party and can only get the validation set to test the model’s performance on clean samples. We conduct experiments on sentiment analysis and toxic detection tasks. We use IMDB (Maas et al., 2011), Amazon (Blitzer et al., 2007) and Yelp (Zhang et al., 2015) reviews datasets on sentiment analysis task, and for toxic detection task, we use Twitter (Founta et al., 2018) and Jigsaw 20186 datasets. Statistics of datasets are in the Appendix. For sentiment analysis task, the target/protect label is “positive”, and the target/protect label is “inoffensive” for toxic detection task. 4.2 Attacking Methods In our main setting, we choose three typical attacking methods to explore the performance of our defense method: BadNet-RW (Gu et al., 2017; Garg et al., 2020; Chen et al., 2020): Attackers wi"
2021.emnlp-main.659,2021.acl-long.37,0,0.0184703,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,2021.acl-long.377,0,0.0320404,"and propose an effective method on defending textual poisoned samples in the inference stage. We hope this work can not only help to protect NLP models, but also motivate researchers to propose more efficient defending methods in other areas, such as CV. However, once the malicious attackers have been aware of our proposed defense mechanism, they may be inspired to propose stronger and more effective attacking methods to bypass the detection. For example, since our motivation and methodology assumes that the backdoor trigger t∗ is static, there are some most recent works (Zhang et al., 2020; Qi et al., 2021a,b) focusing on achieving input-aware attacks by using dynamic triggers which follow a special trigger distribution. However, we point out that in the analysis in Section 3.2, if we consider t∗ as one trigger drawn from the trigger distribution rather than one static point, our analysis is also applicable to the dynamic attacking case. Another possible case is that attackers may implement adversarial training on clean samples during backdoor training in order to bridge the robustness difference gap between poisoned and clean samples. We would like to explore how to effectively defend against"
2021.emnlp-main.659,D19-1221,0,0.150336,"r can actually work for any samples. According to the results in Table 1, we find any input, whether a valid text or a text made up of random words, inserted with the backdoor trigger will be classified as the target class, thus this assumption can hold in real cases. Above theorem reveals that, the existence of the satisfactory perturbation depends on whether there exists a positive value δ such that the inequality 2a∗σ(δ) &lt; δ holds. Previous studies verb ify the existence of universal adversarial perturbations (UAPs) (Moosavi-Dezfooli et al., 2017) and universal adversarial triggers (UATs) (Wallace et al., 2019; Song et al., 2020), which have very small sizes and can make the DNN misclassify all samples that are added with them. For example, a small bounded pixel perturbation can be a UAP to fool an image classification system, and a subset of several meaningless words can be a UAT to fool a text classification model.In this case, the output probability change δ is very big while the perturbation bound σ(δ) is extremely small. Thus, the condition 2a∗σ(δ) &lt; δ can be easily met. This b suggests that, the condition of the existence of the RAP can be satisfied in real cases. Experimental results in the"
2021.emnlp-main.659,2021.naacl-main.165,1,0.696988,"3.1). Then we tern. Following this line, other stealthy and effec- discuss the robustness difference between poisoned and clean samples (Section 3.2), and formally intive attacking methods (Liu et al., 2018b; Nguyen and Tran, 2020; Saha et al., 2020; Liu et al., 2020; troduce our robustness-aware perturbation-based defense approach (Section 3.3). Finally we give a Zhao et al., 2020) are proposed for hacking image theoretical analysis of our proposal (Section 3.4). classification models. As for backdoor attacking in NLP, attackers usually use a rare word (Chen et al., 2020; Garg et al., 2020; Yang et al., 2021a) 3.1 Defense Setting as the trigger word for data poisoning, or choose We mainly discuss in the mainstream setting where the trigger as a long neutral sentence (Dai et al., a user want to directly deploy a well-trained model 2019; Chen et al., 2020; Sun, 2020; Yang et al., from an untrusted third-party (possibly an attacker) 8366 on a specific task. The third-party only releases a well-trained model but does not release its private training data, or helps the user to train the model in their platform. We also conduct extra experiments to validate the effectiveness of our method in another se"
2021.findings-acl.105,P16-1154,0,0.0383867,"∈ RL is obtained by the dot-product attention:   exp hkti · qt  . Skti = P (4) exp h j j · qt k ∈Kt k t t the knowledge kts Finally, with the highest attention score is selected for further response generation. If the gold knowledge kt exists, we could train this task via the Cross Entropy (CE) loss: LKS = LCE (S, kt ) = − log Skt , 2.5 (5) Decoder Following (Dinan et al., 2019; Kim et al., 2020a), our Transformer-based decoder  takes the  representation concatenation Hrc = Hxt ; Hkts ∈ RNrc ,d of current utterance xt and the selected knowledge kts as input, and uses the copy mechanism (Gu et al., 2016) to generate responses. The process of generating a word can be formulated as follows:  snt = TD Hrc , yt&lt;n ∈ Rd pv = softmax (Wo snt ) ∈ R|V| pcp , ˜snt = MultiHeadcp (snt , Hrc , Hrc ) ,  pgen = sigmoid Wgen˜snt ∈ R1  p (V) = pgen ·pv + 1−pgen ·pcp ∈ R|V| where TD denotes the Transformer decoder, snt is the hidden vector for the n-th word in the response yt at t-th turn, pcp is the attention weight of the first head in the additional multi-head self-attention layer for the copy mechanism, which is short for MultiHeadcp (Vaswani et al., 2017), V is the vocabulary, and p (V) is the final ge"
2021.findings-acl.105,2020.sigdial-1.35,0,0.0770602,"Missing"
2021.findings-acl.105,D18-1248,0,0.0198208,"et al., 2020b) that finetunes GPT-2 (Radford et al., 2019) with the unsupervised pretrained knowledge selection module on unlabeled dialogues. We are different in two aspects: (1) Our DDSL leverages knowledge distillation to alleviate the label noise at the pretraining stage; (2) We adopt the sample weighting idea at the finetuning stage. And we will leverage GPT-2 for future study. Our work is inspired by Distant Supervision (DS), an effective method to generate labeled data with external knowledge base (KB) for information extraction (Mintz et al., 2009; Min et al., 2013; Zeng et al., 2015; Wang et al., 2018). Following this idea, Gopalakrishnan et al. (2019) use the oracle knowledge from DS to construct the TopicalChat dataset. Similarly, Qin et al. (2019b) obtain the weakly labeled data to train a KB retriever in the task-oriented dialogue system. Ren et al. (2019) propose a distantly supervised learning schema at segment level to effectively learn the topic transition vector. Although inspired by the similar idea, we are devoted to knowledge selection in the unsupervised setting, which is a different application of DS. Moreover, rather than just using distant supervision, we design our DDSL wit"
2021.findings-acl.105,P19-1369,0,0.0828184,"contradictory response in case 2 and does not provide new information in case 3. (4) Whereas, our UKSDGPF firstly selects the appropriate knowledge as human does, and then generate fluent and informative responses by alleviate the mismatch knowledge selection problem with the help of the pretraining-finetuning strategy. This indicates the importance of leveraging the selected knowledge properly for future study. 5 Related Work External knowledge has been wildly explored to enhance dialogue understanding and/or improve dialogue generation (Zhu et al., 2017; Liu et al., 2018; Chen et al., 2019; Wu et al., 2019; Chen et al., 2020a; Tuan et al., 2020; Sun et al., 2020; Zhang et al., 2020; Yu et al., 2020; Ni et al., 2021). To make use of knowledge, knowledge access is very important. Therefore, knowledge selection which selecting appropriate knowledge given the dialog context gains much attention (Zhang et al., 2019; Meng et al., 2019; Ren et al., 2019; Dinan 1237 Context Human Case 1 SKT UKSDGPF Are you a fan of elvis presley? KS Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the “king of rock and ...” DG You mean the king of rock and roll. Act"
2021.findings-acl.105,N19-1325,0,0.115059,"SDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Dinan et al. (2019) first propose knowledge selection for dialogue generation which are two sequential subtasks and the generation is based on the selected knowledge. Several works follow their setting and achieve improvements with latent vari"
2021.findings-acl.105,W19-5917,0,0.0526971,"Missing"
2021.findings-acl.105,D15-1203,0,0.0353767,"recent work (Zhao et al., 2020b) that finetunes GPT-2 (Radford et al., 2019) with the unsupervised pretrained knowledge selection module on unlabeled dialogues. We are different in two aspects: (1) Our DDSL leverages knowledge distillation to alleviate the label noise at the pretraining stage; (2) We adopt the sample weighting idea at the finetuning stage. And we will leverage GPT-2 for future study. Our work is inspired by Distant Supervision (DS), an effective method to generate labeled data with external knowledge base (KB) for information extraction (Mintz et al., 2009; Min et al., 2013; Zeng et al., 2015; Wang et al., 2018). Following this idea, Gopalakrishnan et al. (2019) use the oracle knowledge from DS to construct the TopicalChat dataset. Similarly, Qin et al. (2019b) obtain the weakly labeled data to train a KB retriever in the task-oriented dialogue system. Ren et al. (2019) propose a distantly supervised learning schema at segment level to effectively learn the topic transition vector. Although inspired by the similar idea, we are devoted to knowledge selection in the unsupervised setting, which is a different application of DS. Moreover, rather than just using distant supervision, we"
2021.findings-acl.105,2020.coling-main.392,1,0.736258,"ase 3. (4) Whereas, our UKSDGPF firstly selects the appropriate knowledge as human does, and then generate fluent and informative responses by alleviate the mismatch knowledge selection problem with the help of the pretraining-finetuning strategy. This indicates the importance of leveraging the selected knowledge properly for future study. 5 Related Work External knowledge has been wildly explored to enhance dialogue understanding and/or improve dialogue generation (Zhu et al., 2017; Liu et al., 2018; Chen et al., 2019; Wu et al., 2019; Chen et al., 2020a; Tuan et al., 2020; Sun et al., 2020; Zhang et al., 2020; Yu et al., 2020; Ni et al., 2021). To make use of knowledge, knowledge access is very important. Therefore, knowledge selection which selecting appropriate knowledge given the dialog context gains much attention (Zhang et al., 2019; Meng et al., 2019; Ren et al., 2019; Dinan 1237 Context Human Case 1 SKT UKSDGPF Are you a fan of elvis presley? KS Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the “king of rock and ...” DG You mean the king of rock and roll. Actually yes I am. best of all time. Don’t you agree? KS Elvis Aaron Presley (Ja"
2021.findings-acl.105,P18-1205,0,0.0307008,"ecoder. Experiments on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge d"
2021.findings-acl.105,2020.emnlp-main.272,0,0.23476,"wever, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Dinan et al. (2019) first propose knowledge selection for dialogue generation which are two sequential subtasks and the generation is based on the selected knowledge. Several works follow their setting and achieve improvements with latent variable models (Kim et al., 2020a; Chen et al., 2020b) or more complex selection mechanism (Niu et al., 2019; Meng et al., 2020; Zheng et al., 2020; Meng et al., 2021). Altho"
2021.findings-acl.105,2020.findings-emnlp.11,0,0.437795,"l., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is natural to extract the external knowledge via information retrieval technology. Several works regard the retrieved knowledge sentences as the preidentified document (Ghazvininejad et al., 2018; Michel Galley, 2018; Yang et al., 2019b; Gopalakrishnan et al., 2019). However, the retrieved document contains redundant and irrelevant information which are harmful for dialogue generation (Zhao et al., 2020b). Hence, knowledge selection which chooses an appropriate sentence from the pre-retrieved knowledge pool gains much attention and it plays the role of knowledge access for generation. Din"
2021.findings-acl.105,2020.acl-main.635,0,0.126015,"sets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation in reality (Zheng et al., 2020). It is n"
2021.findings-acl.105,D18-1076,0,0.0647169,"on two knowledge-grounded dialogue datasets show that our approach manages to select knowledge more accurately in the unsupervised setting and generates more informative responses, even outperforming many strong supervised baselines.1 1 Introduction To avoid general and dull dialogue generation (Li et al., 2016), knowledge-grounded dialogue which equips dialogue systems with external knowledge has become a popular research topic. Thanks to the hand-collected knowledge-grounded dialogue datasets which align each dialogue (even each utterance) with a pre-identified document (Zhang et al., 2018; Zhou et al., 2018; Moghe et al., 2018; Zhou et al., 2020), many researchers focus on injecting the given knowledge to generate informative responses and achieve promising results (Yavuz 1 The codes and model checkpoints will be available at https://github.com/ErenChan/UKSDG et al., 2019; Tang and Hu, 2019; Qin et al., 2019a; Li et al., 2019b; Zheng and Zhou, 2019; Meng et al., 2019; Ren et al., 2019; Ye et al., 2020; Lin et al., 2020). However, they usually need the preidentified knowledge and the knowledge access task is less studied (Kim et al., 2020b) which is the precursor to knowledge dialogue generation"
2021.findings-acl.112,W97-1002,0,0.455921,"oroughly study previous DS-RE methods using both held-out and human-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated"
2021.findings-acl.112,P17-1171,0,0.0181524,"ssumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of the predictions. Moreover, different works may sample different sp"
2021.findings-acl.112,2020.bionlp-1.20,0,0.0132577,"rvations that have not been clearly demonstrated with the DS evaluation: Pre-trained Models First of all, BERT-based models have achieved supreme performance across all three metrics. To thoroughly examine BERT and its variants in the DS-RE scenario, we further plot their P-R curves with the bag-level manual test in Figure 4. It is surprising to see that all bag-level training strategies, especially the ATT strategy which brings significant improvements for PCNN-based models, do not help or even degenerate the performance with pre-trained ones. This observation is also consistent with that in Amin et al. (2020), though they only compare BERT+bag+AVG and BERT+bag+ATT. We hypothesize the reasons are that solely using pre-trained models already makes a strong baseline, since they exploit more parameters and they have gained pre-encoded knowledge from pretraining (Petroni et al., 2019), all of which make them easier to directly capture relational patterns from noisy data; and bag-level training, which essentially increases the batch size, may raise the optimization difficulty for these large models. Another unexpected observation is that, though the P-R curve of BERT is far above other models in the hel"
2021.findings-acl.112,P19-1279,0,0.0111756,"pment in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at"
2021.findings-acl.112,D18-1247,1,0.809527,"annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al"
2021.findings-acl.112,P11-1055,0,0.0641729,"cale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model"
2021.findings-acl.112,D19-1395,0,0.015522,"nn et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in our introduction, the evalua1307 #facts Train #sents N/A #facts Validation #sents N/A #facts Test #sents N/A 53 25 18,409 17,137 52"
2021.findings-acl.112,D19-1250,0,0.0233314,"Missing"
2021.findings-acl.112,P18-1199,0,0.0588086,"eir relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data for intermediate pre-training in order to boost supervised RE tasks. As mentioned in ou"
2021.findings-acl.112,L18-1566,0,0.0357812,"Missing"
2021.findings-acl.112,E17-1110,0,0.0117008,"lp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a small proportion of th"
2021.findings-acl.112,N13-1008,0,0.0117331,"e DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by aligning facts from the FreeBase (Bollacker et al., 2008) with the New York Times (NYT) corpus (Sandhaus, 2008). The original NYT10 dataset contains 53 relations (including N/A). After thoroughly examining the dataset, we found that (1) there are many duplicate instances in the dataset, (2) there is no public validation set, and some previous works directly take the test set to tune the model, and (3) the relation ontology is not reasonable for an RE task. Therefore, w"
2021.findings-acl.112,C02-1151,0,0.0835297,"n-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use"
2021.findings-acl.112,2020.findings-emnlp.20,0,0.0351636,"manual test. tion of DS-RE has long been a problem, especially since many existing methods solely rely on autolabeled test data. Some preliminaries have noticed this problem: Jiang et al. (2018); Zhu et al. (2020) also annotate the test set of NYT10, yet Jiang et al. (2018) only sample 2, 040 sentences from it, and Zhu et al. (2020) discard all N/A data from DS, which are an important part of DS evaluation, and assume that the original held-out data have either the DS relations or no relation at all, while we find that a large proportion of held-out data actually express some other relations; Li et al. (2020) propose active testing, an iterative method to correct the bias of DS evaluation. However, it still requires consistent human efforts during each evaluation phase. To the best of our knowledge, our work, building benchmarks with large-scale manuallylabeled test data, conducts the most comprehensive human evaluations of DS-RE methods so far. 3 DS-RE Datasets In this section, we introduce the way we build the manually-annotated test sets for NYT10 (Riedel et al., 2013) and Wiki20 (Han et al., 2020). We show the statistics of these datasets in Table 1. 3.1 NYT10 Dataset NYT10 is constructed by a"
2021.findings-acl.112,P17-1004,1,0.862832,"Missing"
2021.findings-acl.112,D12-1042,0,0.0264152,"17). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora ("
2021.findings-acl.112,P16-1200,1,0.914386,"nal facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, wh"
2021.findings-acl.112,P09-1113,0,0.549708,"and observations can help advance future DS-RE research.1 1 Musk owns 28.9M Tesla shares. Tesla Inc. Elon Musk Figure 1: Typical errors made by DS evaluation. In the figure, DS labels the bag with only the relation CEO, while none of the sentences express the relation. Also, it misses a correct relation shareholder due to the incompleteness of the knowledge graphs. Introduction Relation extraction (RE) aims at extracting relational facts between entities from the text. One crucial challenge for building an effective RE system is how to obtain sufficient annotated data. To tackle this problem, Mintz et al. (2009) propose distant supervision (DS) to generate large-scale auto-labeled data by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely"
2021.findings-acl.112,2020.emnlp-main.298,1,0.820774,"ginal bag-level training, we carry out a pilot experiment to examine the effect of the sampled training. From Table 3, we can see that our sampling strategy does not significantly hurt the performance of the bag-level training. We also add another variant, BERT-M, in our evaluation. We observe from the top predictions of BERT models (Figure 3) that BERT tends to make false-positive errors for entity pairs that express a relation in the KG but do not have any sentence truly expressing the relation in the data, probably due to that model learns shallow cues solely from entities. Thus, following Peng et al. (2020), we mask entity mentions during training and inference to avoid learning biased heuristics from entities. 5 5.1 Experiment Implementation Details We use the OpenNRE toolkit (Han et al., 2019) for most of our experiments, including both sentencelevel and bag-level training. For CNN and PCNN, we follow the hyper-parameters of Han et al. (2019). For BERT, we use pre-trained checkpoint bert-base-uncased for initialization, take a batch size of 64, a bag size of 4 and a learning rate of 2 × 10−5 ,3 and train the model for 3 epochs. For RL-DSRE, RESIDE and BGWA, we directly use their original imple"
2021.findings-acl.112,Q17-1008,0,0.0120081,":// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and human evaluation, which manually checks the most confident relational facts predicted by DS-RE models. Since manually checking is costly, most works with human evaluation only examine a"
2021.findings-acl.112,D18-1157,0,0.024953,"Missing"
2021.findings-acl.112,N16-1103,0,0.0275135,"Missing"
2021.findings-acl.112,C18-1099,1,0.824993,"t, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020) utilize DS data f"
2021.findings-acl.112,D17-1187,0,0.0175736,"thout human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Some other latest works (Peng et al., 2020"
2021.findings-acl.112,D15-1203,0,0.1272,"by aligning relational facts in knowledge graphs (KGs) to text corpora, with the * Corresponding author e-mail: liuzy@tsinghua.edu.cn Our code and data are publicly available at https:// github.com/thunlp/opennre. 1 CEO Shareholder Place of birth Capital Founder core assumption that one sentence mentioning two entities is likely to express the relational facts between the two entities from KGs. As DS can bring hundreds of thousands of autolabeled training instances for RE without any human labor, DS-RE has been widely explored in the past few years (Riedel et al., 2010; Hoffmann et al., 2011; Zeng et al., 2015; Lin et al., 2016; Feng et al., 2018; Vashishth et al., 2018) and has also been widely extended to other related domains, such as biomedical information extraction (Peng et al., 2017; Quirk and Poon, 2017) and question answering (Bordes et al., 2015; Chen et al., 2017). Although DS-RE has achieved great success, we identify one severe problem for the current DSRE research—its evaluation. Existing works usually take two kinds of evaluation methods following Mintz et al. (2009): held-out evaluation, which directly uses DS-generated test data to approximate the trend of model performance, and hu"
2021.findings-acl.112,C14-1220,0,0.0174976,"cially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel"
2021.findings-acl.112,N19-1306,0,0.0138632,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,D17-1004,0,0.0123124,", which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Sur"
2021.findings-acl.112,P19-1139,1,0.799285,"ades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informativ"
2021.findings-acl.112,P05-1053,0,0.0494374,"n-labeled test sets, and find that human-labeled data can reveal inconsistent results compared to the held-out ones. • We discuss some novel and important observations revealed by manual evaluation, especially with respect to pre-trained models, which calls for more research in these directions. 2 Related Work Relation extraction is an important NLP task and has gone through significant development in the past decades. In the early days, RE models mainly take statistical approaches, such as pattern-based methods (Huffman, 1995; Califf and Mooney, 1997), feature-based methods (Kambhatla, 2004; Zhou et al., 2005), graphical methods (Roth and Yih, 2002), etc. With the increasing computing power and the development of deep learning, neural RE methods have shown a great success (Liu et al., 2013; Zeng et al., 2014; Zhang and Wang, 2015; Zhang et al., 2017). Recently, pre-trained models like BERT (Devlin et al., 2019) have dominated various NLP benchmarks, including those in RE (Baldini Soares et al., 2019; Zhang et al., 2019b). All these RE methods focus on training models in a supervised setting and require largescale sufficient human-annotated data. To generate large-scale auto-labeled data without hum"
2021.findings-acl.112,P19-1128,1,0.843226,"generate large-scale auto-labeled data without human effort, Mintz et al. (2009) first use DS to label sentences mentioning two entities with their relations in KGs, which inevitably brings wrongly labeled instances. To handle the noise problem, Riedel et al. (2010); Hoffmann et al. (2011); Surdeanu et al. (2012) apply multi-instance multi-label training in DS-RE. Following their settings, later research mainly takes on two paths: one aims at selecting informative sentences from the noisy dataset, using heuristics (Zeng et al., 2015), attention mechanisms (Lin et al., 2016; Han et al., 2018c; Zhu et al., 2019), adversarial training (Wu et al., 2017; Wang et al., 2018; Han et al., 2018a), and reinforcement learning (Feng et al., 2018; Qin et al., 2018); the other incorporates external information like KGs (Ji et al., 2017; Han et al., 2018b; Zhang et al., 2019a; Hu et al., 2019), multilingual corpora (Verga et al., 2016; Lin et al., 2017; Wang et al., 2018), as well as relation ontology and aliases (Vashishth et al., 2018). Recently, pretrained DS-RE models have also been explored, including both domain-general (Alt et al., 2019; Xiao et al., 2020) and domain-specific (Amin et al., 2020) models. Som"
2021.findings-acl.112,2020.coling-main.566,0,0.0927166,"Missing"
2021.findings-acl.143,C18-1169,0,0.0158373,"et al., 2019). Recent work also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), rein"
2021.findings-acl.143,D14-1164,0,0.0339627,"l., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and yield stateof-the-art performances in many NLP tasks. These two-stage methods allow using large-scale unlabeled data in pre-training and small labeled data in fine-tuning. In order to adapt to specific tasks or domain, variants of BERT"
2021.findings-acl.143,N19-1184,0,0.0234658,"2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and yield stateof-the-art performances in many NLP tasks. These two-stage methods allow using large-scale unlabeled data in pre-training and small labeled data in fine-tuning. In order to adapt to specific tasks or domain, variants of BERT are proposed including"
2021.findings-acl.143,H05-1091,0,0.0597502,"l studies on three Chinese NER datasets demonstrate that our method achieves significant improvements against several baselines, establishing the new state-of-the-art performance. 1 Introduction Named entity recognition is a fundamental Natural Language Processing task that labels each word in sentences with predefined types, such as Person (PER), Location (LOC), Organization (ORG), etc. The results of NER can be used in many ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Work was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the metho"
2021.findings-acl.143,P15-1017,0,0.0309992,"e that our method achieves significant improvements against several baselines, establishing the new state-of-the-art performance. 1 Introduction Named entity recognition is a fundamental Natural Language Processing task that labels each word in sentences with predefined types, such as Person (PER), Location (LOC), Organization (ORG), etc. The results of NER can be used in many ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Work was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the methods using pre-trained language models. Howev"
2021.findings-acl.143,N19-1423,0,0.497315,"at AI, Tencent Inc. Work was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the methods using pre-trained language models. However, obtaining sentence-level annotated data is expensive and thus leads to small training data size and performance bottleneck of supervised models. In practice, entity dictionaries (or gazetteers) and unlabeled corpora can be obtained at a low cost. Furthermore, distantly supervised data can be automatically generated by matching the unlabeled data against entity dictionaries. These data can form a heterogeneous corpus, which"
2021.findings-acl.143,P19-1141,0,0.0897065,"he training dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020) model to obtain all regions of candidate entities. Then we use our model to predict the final label for each region. Pre-training Corpora Entity Dictionary. There are four types of entity in our experiment: PER, ORG, GPE, and LOC. We extend the dictionary used in Ding et al. (2019) with more gazetteers collected from Sougou Dictionary 4 and Baidu Dictionary 5 . Finally, our gazetteer contains 50k person names, 143k organization names, 43k geopolitical entities, and 33k location names (see Appendix 1.1). Distantly Supervised Data. The entity dictionary above is used to match unannotated sentences to obtain distantly supervised data. For OntoNotes and MSRA dataset, we collect news documents on the People’s Daily 6 published from 1949 to 2010. For the Weibo dataset, we use the Weibo unannotated data from Peng and Dredze (2015). Finally, we obtain 893k sentences of distantl"
2021.findings-acl.143,C18-1161,0,0.0196869,"ined language model (Devlin et al., 2019). Recent work also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (S"
2021.findings-acl.143,D19-1096,0,0.0159579,"dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020) model to obtain all regions of candidate entities. Then we use our model to predict the final label for each region. Pre-training Corpora Entity Dictionary. There are four types of entity in our experiment: PER, ORG, GPE, and LOC. We extend the dictionary used in Ding et al. (2019) with more gazetteers collected from Sougou Dictionary 4 and Baidu Dictionary 5 . Finally, our gazetteer contains 50k person names, 143k organization names, 43k geopolitical entities, and 33k location names (see Appendix 1.1). Distantly Supervised Data. The entity dictionary above is used to match unannotated sentences to obtain distantly supervised data. For OntoNotes and MSRA dataset, we collect news documents on the People’s Daily 6 published from 1949 to 2010. For the Weibo dataset, we use the Weibo unannotated data from Peng and Dredze (2015). Finally, we obtain 893k sentences of distantl"
2021.findings-acl.143,2020.acl-main.740,0,0.0712236,"ts of the First name and the Last name. For distantly supervised data, the text often contains rich context information, while has high noise. The most common mistakes are wrong labels and wrong boundaries. As a result, these data are not suitable to be directly incorporated for NER. However, they can be naturally used as data for pre-training to learn highcoverage and task-aware representations of entity mentions and contexts. On the one hand, previous research showed that further pre-training BERT to do language modeling on in-domain corpus could improve the performance of downstream tasks (Gururangan et al., 2020). On the other hand, either the entity or the context itself can be a strong indicator of entity types. Mention-BERT Pre-Training. To better capture the regularity information of entities, the Mention-BERT is pre-trained on entity dictionaries. As shown in Figure 2, we add a feed forward classifier denoted as Mention-Classifier for Pre-Training on top of the Mention-BERT. The task is to classify each input term into the most probable label according to the dictionaries. For example, the output for the term “足协” (Football Association) should be ORG. Besides, to empower the model to learn discri"
2021.findings-acl.143,E17-2113,0,0.0258238,"eters. 4.1 Dev 4.3k 200.5k 0.27k 14.5k Test 4.3k 208.1k 4.4k 172.6k 0.27k 14.8k (7) where Wm , bm , Wc , bc are parameters of the Mention-Classifier and the Context-Classifier, and ym , yc are the respective predictions. We define losses for the two classifiers as follows: 4 Train 15.7k 491.9k 46.4k 2169.9k 1.4k 73.8k Table 1: Statistics of the datasets. (6) 4.2 L = Lg + αLm + βLc Type Sentence Char Sentence Char Sentence Char Experiment Dataset We evaluate our methods on three Chinese NER datasets: OntoNotes 4.0 (Weischedel et al., 2013), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017). OntoNotes and MSRA are collected from newswire text, and Weibo NER is from social media text 2 . The detail of the datasets is shown in Table 1. At the unifiedtraining stage, we treat all labeled entities in the training dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020) model to obtain all regions of candidate entities. Th"
2021.findings-acl.143,2020.emnlp-main.518,0,0.204476,"ity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). C"
2021.findings-acl.143,N16-1030,0,0.763509,"ng task that labels each word in sentences with predefined types, such as Person (PER), Location (LOC), Organization (ORG), etc. The results of NER can be used in many ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Work was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the methods using pre-trained language models. However, obtaining sentence-level annotated data is expensive and thus leads to small training data size and performance bottleneck of supervised models. In practice, entity dictionaries (or gazetteers) and unlabeled c"
2021.findings-acl.143,2020.acl-main.193,0,0.0260616,"sal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and yield stateof-the-art performances in many NLP tasks. These tw"
2021.findings-acl.143,W06-0115,0,0.078011,"parts: (10) where α, β ∈ [0, 1] are hyper-parameters. 4.1 Dev 4.3k 200.5k 0.27k 14.5k Test 4.3k 208.1k 4.4k 172.6k 0.27k 14.8k (7) where Wm , bm , Wc , bc are parameters of the Mention-Classifier and the Context-Classifier, and ym , yc are the respective predictions. We define losses for the two classifiers as follows: 4 Train 15.7k 491.9k 46.4k 2169.9k 1.4k 73.8k Table 1: Statistics of the datasets. (6) 4.2 L = Lg + αLm + βLc Type Sentence Char Sentence Char Sentence Char Experiment Dataset We evaluate our methods on three Chinese NER datasets: OntoNotes 4.0 (Weischedel et al., 2013), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017). OntoNotes and MSRA are collected from newswire text, and Weibo NER is from social media text 2 . The detail of the datasets is shown in Table 1. At the unifiedtraining stage, we treat all labeled entities in the training dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020)"
2021.findings-acl.143,2020.acl-main.611,0,0.0106835,"riate performance. Sequence labeling methods are widely used in NER. Traditional methods use the CRF model to solve the NER task (Lafferty et al., 2001). With the advantages of eliminating feature engineering and significant performance improvement, neural network models become prevalent in NER research, e.g., the models based on FFN (Collobert et al., 2011), CNN (Ma and Hovy, 2016), LSTM (Lample et al., 2016), and pre-trained language model (Devlin et al., 2019). Recent work also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as C"
2021.findings-acl.143,2020.findings-emnlp.372,0,0.035663,"w model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and yield stateof-the-art performances in many NLP tasks. These two-stage methods allow using large-scale unlabeled data in pre-training and small labeled data in fine-tuning. In order to adapt to specific tasks or domain, variants of BERT are proposed including small and practical BERT (Tsai et al., 2019; Lan et al., 2020b; Jiao et al., 2020), domain adaptive BERT (Yang et al., 2019a; Gururangan et al., 2020), and task adaptive BERT (Sun et al., 2019; Xue et al., 2020; Jia et al., 2020). Our work performs further pre-training on BERT and proposes task-aware training objectives to improve NER. 7 Conclusion In this work, we focus on fully exploiting heterogeneous corpus for NER. The corpus consists of entity dictionaries, distantly supervised instances, and human-annotated instances. We propose a decoupled NER model with two-stage training. The model first learns appropriate task-aware representations in pre-training, from large-sca"
2021.findings-acl.143,2020.acl-main.519,0,0.0144096,"riate performance. Sequence labeling methods are widely used in NER. Traditional methods use the CRF model to solve the NER task (Lafferty et al., 2001). With the advantages of eliminating feature engineering and significant performance improvement, neural network models become prevalent in NER research, e.g., the models based on FFN (Collobert et al., 2011), CNN (Ma and Hovy, 2016), LSTM (Lample et al., 2016), and pre-trained language model (Devlin et al., 2019). Recent work also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as C"
2021.findings-acl.143,N19-1079,0,0.0183993,"; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) an"
2021.findings-acl.143,D19-1646,0,0.0174152,"dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020) model to obtain all regions of candidate entities. Then we use our model to predict the final label for each region. Pre-training Corpora Entity Dictionary. There are four types of entity in our experiment: PER, ORG, GPE, and LOC. We extend the dictionary used in Ding et al. (2019) with more gazetteers collected from Sougou Dictionary 4 and Baidu Dictionary 5 . Finally, our gazetteer contains 50k person names, 143k organization names, 43k geopolitical entities, and 33k location names (see Appendix 1.1). Distantly Supervised Data. The entity dictionary above is used to match unannotated sentences to obtain distantly supervised data. For OntoNotes and MSRA dataset, we collect news documents on the People’s Daily 6 published from 1949 to 2010. For the Weibo dataset, we use the Weibo unannotated data from Peng and Dredze (2015). Finally, we obtain 893k sentences of distantl"
2021.findings-acl.143,P19-1524,0,0.018304,"rk also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning"
2021.findings-acl.143,N19-1247,0,0.0139996,"rk also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning"
2021.findings-acl.143,2020.acl-main.528,0,0.0700923,"SRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017). OntoNotes and MSRA are collected from newswire text, and Weibo NER is from social media text 2 . The detail of the datasets is shown in Table 1. At the unifiedtraining stage, we treat all labeled entities in the training dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020) model to obtain all regions of candidate entities. Then we use our model to predict the final label for each region. Pre-training Corpora Entity Dictionary. There are four types of entity in our experiment: PER, ORG, GPE, and LOC. We extend the dictionary used in Ding et al. (2019) with more gazetteers collected from Sougou Dictionary 4 and Baidu Dictionary 5 . Finally, our gazetteer contains 50k person names, 143k organization names, 43k geopolitical entities, and 33k location names (see Appendix 1.1). Distantly Supervised Data. The entity dictionary above is used to match unannotated senten"
2021.findings-acl.143,P16-1101,0,0.259008,"ach word in sentences with predefined types, such as Person (PER), Location (LOC), Organization (ORG), etc. The results of NER can be used in many ∗ Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Work was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the methods using pre-trained language models. However, obtaining sentence-level annotated data is expensive and thus leads to small training data size and performance bottleneck of supervised models. In practice, entity dictionaries (or gazetteers) and unlabeled corpora can be obtain"
2021.findings-acl.143,P19-1231,0,0.0200225,"explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and"
2021.findings-acl.143,D15-1064,0,0.0736433,"[0, 1] are hyper-parameters. 4.1 Dev 4.3k 200.5k 0.27k 14.5k Test 4.3k 208.1k 4.4k 172.6k 0.27k 14.8k (7) where Wm , bm , Wc , bc are parameters of the Mention-Classifier and the Context-Classifier, and ym , yc are the respective predictions. We define losses for the two classifiers as follows: 4 Train 15.7k 491.9k 46.4k 2169.9k 1.4k 73.8k Table 1: Statistics of the datasets. (6) 4.2 L = Lg + αLm + βLc Type Sentence Char Sentence Char Sentence Char Experiment Dataset We evaluate our methods on three Chinese NER datasets: OntoNotes 4.0 (Weischedel et al., 2013), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017). OntoNotes and MSRA are collected from newswire text, and Weibo NER is from social media text 2 . The detail of the datasets is shown in Table 1. At the unifiedtraining stage, we treat all labeled entities in the training dataset as entity mentions. To obtain nonentity mentions, we take (1) all words and phrases labeled as noun by the LTP 3 (Che et al., 2020) lexicon tool, and (2) all words and phrases with an edit distance less than one to any of the entity mentions. During the test time, we first use LTP and SoftLexicon (Ma et al., 2020) model to obtain all regions of can"
2021.findings-acl.143,N18-1202,0,0.337633,"ognition Center, WeChat AI, Tencent Inc. Work was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the methods using pre-trained language models. However, obtaining sentence-level annotated data is expensive and thus leads to small training data size and performance bottleneck of supervised models. In practice, entity dictionaries (or gazetteers) and unlabeled corpora can be obtained at a low cost. Furthermore, distantly supervised data can be automatically generated by matching the unlabeled data against entity dictionaries. These data can form a heterog"
2021.findings-acl.143,2020.acl-main.722,0,0.0170342,"er than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), posit"
2021.findings-acl.143,D18-1230,0,0.022346,"8; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently,"
2021.findings-acl.143,D18-1309,0,0.0557879,"Missing"
2021.findings-acl.143,D19-1396,0,0.014725,"ily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tan"
2021.findings-acl.143,D19-1374,0,0.014245,"orks, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and yield stateof-the-art performances in many NLP tasks. These two-stage methods allow using large-scale unlabeled data in pre-training and small labeled data in fine-tuning. In order to adapt to specific tasks or domain, variants of BERT are proposed including small and practical BERT (Tsai et al., 2019; Lan et al., 2020b; Jiao et al., 2020), domain adaptive BERT (Yang et al., 2019a; Gururangan et al., 2020), and task adaptive BERT (Sun et al., 2019; Xue et al., 2020; Jia et al., 2020). Our work performs further pre-training on BERT and proposes task-aware training objectives to improve NER. 7 Conclusion In this work, we focus on fully exploiting heterogeneous corpus for NER. The corpus consists of entity dictionaries, distantly supervised instances, and human-annotated instances. We propose a decoupled NER model with two-stage training. The model first learns appropriate task-aware represen"
2021.findings-acl.143,2020.emnlp-main.514,0,0.0301434,"for NLP Recently, large-scale pre-trained language models, such as BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), are widely used and yield stateof-the-art performances in many NLP tasks. These two-stage methods allow using large-scale unlabeled data in pre-training and small labeled data in fine-tuning. In order to adapt to specific tasks or domain, variants of BERT are proposed including small and practical BERT (Tsai et al., 2019; Lan et al., 2020b; Jiao et al., 2020), domain adaptive BERT (Yang et al., 2019a; Gururangan et al., 2020), and task adaptive BERT (Sun et al., 2019; Xue et al., 2020; Jia et al., 2020). Our work performs further pre-training on BERT and proposes task-aware training objectives to improve NER. 7 Conclusion In this work, we focus on fully exploiting heterogeneous corpus for NER. The corpus consists of entity dictionaries, distantly supervised instances, and human-annotated instances. We propose a decoupled NER model with two-stage training. The model first learns appropriate task-aware representations in pre-training, from large-scale contextdeficient dictionaries and noisy distantly supervised data. Then after unified-training, the model can predict entity"
2021.findings-acl.143,D19-1429,0,0.0906486,"ork was done when Yun Hu was intern at WeChat AI. downstream NLP tasks, e.g., relation extraction (Bunescu and Mooney, 2005), information retrieval (Chen et al., 2015), and question answering (Yao and Van Durme, 2014). Supervised methods are mainstream approaches to NER, including CRF (Lafferty et al., 2001) and neural network models (Collobert et al., 2011; Lample et al., 2016; Ma and Hovy, 2016). Recently, large-scale pre-trained language models fine-tuned upon a limited amount of annotated data achieve competitive or better performance in NER task (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Supervised NER methods require a sufficient amount of sentence-level annotated data, even for the methods using pre-trained language models. However, obtaining sentence-level annotated data is expensive and thus leads to small training data size and performance bottleneck of supervised models. In practice, entity dictionaries (or gazetteers) and unlabeled corpora can be obtained at a low cost. Furthermore, distantly supervised data can be automatically generated by matching the unlabeled data against entity dictionaries. These data can form a heterogeneous corpus, which has potential to im"
2021.findings-acl.143,C18-1183,0,0.0117057,"Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both human annotated data and dis1648 tantly supervised data together (Angeli et al., 2014; Beltagy et al., 2019). Compared with previous works, our work focus on designing a new model architecture and training approaches to better exploit the heterogeneous data in NER task. 6.3 Two-stage Training Paradigm for NLP Recently, large-scale pre-trained language models, su"
2021.findings-acl.143,P14-1090,0,0.0541741,"Missing"
2021.findings-acl.143,2020.acl-main.577,0,0.0187793,"thods are widely used in NER. Traditional methods use the CRF model to solve the NER task (Lafferty et al., 2001). With the advantages of eliminating feature engineering and significant performance improvement, neural network models become prevalent in NER research, e.g., the models based on FFN (Collobert et al., 2011), CNN (Ma and Hovy, 2016), LSTM (Lample et al., 2016), and pre-trained language model (Devlin et al., 2019). Recent work also propose different ways to model the NER task other than sequence labeling, such as machine reading comprehension (Li et al., 2020b), dependency parsing (Yu et al., 2020), span classification (Sohrab and Miwa, 2018). Generally, these approaches have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictio"
2021.findings-acl.143,P18-1144,0,0.0123902,"have achieved promising results but heavily rely on human-annotated data. 6.2 Enhancing NER with External Data Entity dictionaries or gazetteers have long been regarded as an easily-obtainable and useful resource for NER. Previous methods commonly incorporated gazetteers as additional features (Ghaddar and Langlais, 2018; Al-Olimat et al., 2018; Liu et al., 2019a; Ding et al., 2019; Lin et al., 2019; Rijhwani et al., 2020). For languages without explicit word boundaries, such as Chinese, incorporating a universal dictionary with common words besides gazetteers can be further helpful for NER (Zhang and Yang, 2018; Liu et al., 2019b; Sui et al., 2019; Gui et al., 2019b,a; Ma et al., 2020; Li et al., 2020a; Jia et al., 2020). Dictionaries can also be used to construct distantly supervised data from unlabeled corpora. Previous work on reducing the noise in distantly supervised data include new labeling schemes (Shang et al., 2018), reinforcement learning (Yang et al., 2018), cross-training (Jie et al., 2019), positive unlabeled learning (Peng et al., 2019), HMM (Lison et al., 2020), consensus network (Lan et al., 2020a). In other NLP tasks, such as relation extraction, few works have exploited using both"
2021.findings-acl.148,N19-1423,0,0.192052,"ut sentence won’t be significantly affected by adversarial perturbations, and the model’s performance drops less under adversarial attack. That is to say, our approaches can effectively improve the robustness of the model. Besides, RAR can also be used to generate text-form adversarial samples. 1 Introduction Text classification is a fundamental research topic in natural language processing (Pang et al., 2002; Lai et al., 2015; Neekhara et al., 2019; Sun et al., 2019). Neural networks have obtained state-of-theart performance on many text classification datasets (Kim, 2014; Wang et al., 2018; Devlin et al., 2019). Despite these models’ success, recent work has shown that they can be easily fooled by intentionally designed adversarial examples. These adversarial examples generated by adding little perturbations on original examples cannot affect human’s judgment but can fail models (Ren et al., 2019a; Xu et al., 2019). Adversarial training approaches are proposed to tackle this problem, which aims to enhance the model’s strength of generalization and robustness by generating adversarial samples and letting the model learn them (Ren et al., 2019b; Xu et al., 2019). The approaches for generating adversar"
2021.findings-acl.148,D14-1181,0,0.00979158,"ntic representation of the input sentence won’t be significantly affected by adversarial perturbations, and the model’s performance drops less under adversarial attack. That is to say, our approaches can effectively improve the robustness of the model. Besides, RAR can also be used to generate text-form adversarial samples. 1 Introduction Text classification is a fundamental research topic in natural language processing (Pang et al., 2002; Lai et al., 2015; Neekhara et al., 2019; Sun et al., 2019). Neural networks have obtained state-of-theart performance on many text classification datasets (Kim, 2014; Wang et al., 2018; Devlin et al., 2019). Despite these models’ success, recent work has shown that they can be easily fooled by intentionally designed adversarial examples. These adversarial examples generated by adding little perturbations on original examples cannot affect human’s judgment but can fail models (Ren et al., 2019a; Xu et al., 2019). Adversarial training approaches are proposed to tackle this problem, which aims to enhance the model’s strength of generalization and robustness by generating adversarial samples and letting the model learn them (Ren et al., 2019b; Xu et al., 2019"
2021.findings-acl.148,D19-1525,0,0.0167968,"es outperform strong baselines on various text classification datasets. Analysis experiments find that when using our approaches, the semantic representation of the input sentence won’t be significantly affected by adversarial perturbations, and the model’s performance drops less under adversarial attack. That is to say, our approaches can effectively improve the robustness of the model. Besides, RAR can also be used to generate text-form adversarial samples. 1 Introduction Text classification is a fundamental research topic in natural language processing (Pang et al., 2002; Lai et al., 2015; Neekhara et al., 2019; Sun et al., 2019). Neural networks have obtained state-of-theart performance on many text classification datasets (Kim, 2014; Wang et al., 2018; Devlin et al., 2019). Despite these models’ success, recent work has shown that they can be easily fooled by intentionally designed adversarial examples. These adversarial examples generated by adding little perturbations on original examples cannot affect human’s judgment but can fail models (Ren et al., 2019a; Xu et al., 2019). Adversarial training approaches are proposed to tackle this problem, which aims to enhance the model’s strength of genera"
2021.findings-acl.148,P19-1103,0,0.131196,"1 Introduction Text classification is a fundamental research topic in natural language processing (Pang et al., 2002; Lai et al., 2015; Neekhara et al., 2019; Sun et al., 2019). Neural networks have obtained state-of-theart performance on many text classification datasets (Kim, 2014; Wang et al., 2018; Devlin et al., 2019). Despite these models’ success, recent work has shown that they can be easily fooled by intentionally designed adversarial examples. These adversarial examples generated by adding little perturbations on original examples cannot affect human’s judgment but can fail models (Ren et al., 2019a; Xu et al., 2019). Adversarial training approaches are proposed to tackle this problem, which aims to enhance the model’s strength of generalization and robustness by generating adversarial samples and letting the model learn them (Ren et al., 2019b; Xu et al., 2019). The approaches for generating adversarial samples can be roughly classified into two categories: text-based and gradient-based. The former can be further classified into three levels: characterlevel, word-level, and sentence-level. Compared to gradient-based adversarial approaches, the textbased are explainable, but they may su"
2021.findings-acl.148,D13-1170,0,0.00805139,"roaches on four datasets. We first introduce the datasets, the baselines, and the experiment settings. Then, we show experiment results and provide further analysis. SST-2 0.6 0.1 0 2 Yahoo! 0 0.01 0 3 Yelp-P 0.5 0.05 0 3 AG’s News 0 0.01 0 3 We use four text classification datasets: SST-2, Yelp-P, AG’s News, and Yahoo! Answers. Table 1: Hyperparameters for FreeLB on 4 datasets: step size α, maximum perturbation norm  (if it is set to zero, the perturbation’s norm is not limited), number of iteration steps n, magnitude of initial random perturbation γ. SST-2. The Stanford Sentiment Treebank (Socher et al., 2013) consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentence-level sentiment (positive/negative) of a given input text. where W is a learnable parameter matrix, c is the class. ALBERT is fine-tuned with all parameters as well as W jointly by maximizing the logprobability of the golden label. 4.1 Datasets Yahoo! Answers. This dataset is composed of ten topic categories: Society & Culture, Science & Mathematics, Health, Education & Reference, etc. In this work, we use five categories. For every category, we use 12,000 training samples, 4"
2021.findings-acl.148,W18-5446,0,0.0642989,"Missing"
2021.findings-acl.148,D19-1554,0,0.0163651,"t classification is a fundamental research topic in natural language processing (Pang et al., 2002; Lai et al., 2015; Neekhara et al., 2019; Sun et al., 2019). Neural networks have obtained state-of-theart performance on many text classification datasets (Kim, 2014; Wang et al., 2018; Devlin et al., 2019). Despite these models’ success, recent work has shown that they can be easily fooled by intentionally designed adversarial examples. These adversarial examples generated by adding little perturbations on original examples cannot affect human’s judgment but can fail models (Ren et al., 2019a; Xu et al., 2019). Adversarial training approaches are proposed to tackle this problem, which aims to enhance the model’s strength of generalization and robustness by generating adversarial samples and letting the model learn them (Ren et al., 2019b; Xu et al., 2019). The approaches for generating adversarial samples can be roughly classified into two categories: text-based and gradient-based. The former can be further classified into three levels: characterlevel, word-level, and sentence-level. Compared to gradient-based adversarial approaches, the textbased are explainable, but they may suffer from low attac"
2021.findings-acl.153,2020.emnlp-demos.22,0,0.0410419,"ould achieve higher scores on these metrics. We use the “Filtered” setting for all the evaluations, which filters out other true answers from the prediction results to get the final rank for each test case. 5.4 Hyperparameter Settings According to Ruffinelli et al. (2020), performances of KGE methods are sensitive to hyperparameters. Following them, we run 30 quasi-random trails for all models from predefined hyperparameter spaces. We list the hyperparameter spaces we use in Appendix A.5. We run all trails for 100 epochs. For all single-view KE methods, we use the implementations from LibKGE (Broscheit et al., 2020), which utilizes the Ax framework to perform quasi-random hyperparameter search. For AttH, we use the implementation from the authors1 . For JOIE, we use the implementation from the authors2 . We use TransE as the backend and adopt the suggested hyperparameter space from the paper. 6 Experimental Results In this section, we provide the experimental results and further propose several future directions. 6.1 Knowledge Abstraction The results of knowledge abstraction are shown in Table 4. From the results, we can see that AttH has 1 2 https://github.com/HazyResearch/KGEmb https://github.com/Junhe"
2021.findings-acl.153,2020.acl-main.617,0,0.0496769,"Missing"
2021.findings-acl.153,P15-1067,0,0.0372355,") and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations. Thirdly, some datasets provide the full concept graphs (Hao et al., 2019), but both the scale and the depth of the concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type informat"
2021.findings-acl.153,D18-1222,1,0.91582,"EC-KG is shown in Figure 1. During the last decade, there are massive works focusing on learning representations for KGs such as TransE (Bordes et al., 2013), DistMult (Yang et al., 2015), ComplEx (Trouillon et al., 2016), and TuckER (Balaˇzevi´c et al., 2019). Though they have achieved promising results on knowledge graph completion, most of them focus on a single graph, especially the entity graph. Beyond modeling a single graph of KGs, recent studies demonstrate that jointly modeling the two graphs in the EC-KG can improve the understanding of each one (Xie et al., 2016; Moon et al., 2017; Lv et al., 2018; Hao et al., 2019). They also propose 1751 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1751–1763 August 1–6, 2021. ©2021 Association for Computational Linguistics several tasks on the EC-KG, such as link prediction and entity typing. These tasks focus on partial aspects of knowledge abstraction, concretization, and completion, which are essential abilities for humans to recognize the world and acquire knowledge. For example, in entity typing, a model may link the entity “Da Vinci” to the concept “painter” which reflects the model’s abstraction ability. Ho"
2021.findings-acl.153,N18-2053,0,0.0200075,"concept hierarchy are limited. For example, the entity numbers of DB111K-174 (Hao et al., 2019) and our dataset KACC-M are similar, but KACC-M has 38 times more concepts than DB111K-174 (see Table 1). 2.2 Knowledge Embedding Methods Existing knowledge embedding (KE) methods can be categorized as translation models (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015; Sun et al., 2019), tensor factorization based models (Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Balaˇzevi´c et al., 2019), and neural models (Socher et al., 2013; Dettmers et al., 2018; Nguyen et al., 2018). These methods are typically designed for single-view KGs. Although they can be directly applied to EC-KGs by ignoring different characteristics between entity graphs and concept graphs, they cannot take full advantage of the information in EC-KGs. Several works (Krompaß et al., 2015; Xie et al., 2016; Ma et al., 2017; Moon et al., 2017) incorporate the type information into KE methods to help the completion of entity graphs. ETE (Moon 1752 et al., 2017) further conducts entity typing, which can be seen as a simplified version of our knowledge abstraction task. Though types of entities can be"
2021.findings-acl.153,2020.emnlp-main.669,0,0.0266774,"on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide the full concept graphs with logical relations."
2021.findings-acl.153,D15-1174,0,0.541521,"archies perform better than general KGE models on abstraction and concretization tasks, they are not competitive to some general KGE models on logical relations. Moreover, all methods have drastic performance degradation on multi-hop tasks, and the knowledge transfer between the entity graph and the concept graph is still obscure. Finally, we present useful insights for future model design. 2 2.1 Related Work Knowledge Graph Datasets Existing datasets for knowledge graph completion are usually subgraphs of large-scale KGs, such as FB15K, FB15K-237, WN18, WN18RR and CoDEx (Bordes et al., 2013; Toutanova et al., 2015; Dettmers et al., 2018; Safavi and Koutra, 2020). These datasets are all single-view KGs, in which FB15K, FB15K-237, and CoDEx focus on the entity view while WN18 and WN18RR can be regarded as concept view KGs. Several datasets try to link the two views in different ways. Firstly, some datasets provide additional type information to the entity graph, such as FB15K+, FB15K-ET and YAGO43K-ET (Xie et al., 2016; Moon et al., 2017). Secondly, some datasets provide concept hierarchies for the entity graph, such as Probase (Wu et al., 2012) and YAGO39K (Lv et al., 2018). However, they do not provide"
2021.findings-acl.189,D18-1216,0,0.162103,"standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classiﬁcation1 . The learner learns text repr"
2021.findings-acl.189,K18-1030,0,0.0939096,"term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which enhances the performance and interpretability of the attention model for text classiﬁcation1 . The learn"
2021.findings-acl.189,N19-1423,0,0.0132863,"represents Kullback-Leibler divergence. Speciﬁcally, we regard ppyq as constant and then minimize Epθ py,zq rlog qφ py |zqs. Since we must ﬁrst sample r to sample y, z from pθ pr, y, zq, the lower bound of IpZ; Y q is computed as, IpZ; Y q ě Eppr,yq rEpθ pz|rq rlog qφ py |zqss IpZ;Rq upper bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 2"
2021.findings-acl.189,N19-1357,0,0.0691095,"ce but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Moreover, we ﬁnd that though the attention mechanism can help improve the performance for text classiﬁcation in our experiments, it may focus on the irrelevant information. For example, in the sentence “A very funny movie.”, the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In ge"
2021.findings-acl.189,D19-1276,0,0.0857347,"the performance and interpretability of the attention model for text classiﬁcation1 . The learner learns text representations by ﬁne-tuning 1 We focus on the task of text classiﬁcation, but our method can be easily extended to other NLP or CV tasks with attention mechanisms. 2152 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2152–2161 August 1–6, 2021. ©2021 Association for Computational Linguistics the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019; Chen and Ji, 2020; Jiang et al., 2020; Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information ﬂow. To evaluate the effectiveness of our proposed approach, w"
2021.findings-acl.189,2020.emnlp-main.347,0,0.277583,"lassiﬁcation1 . The learner learns text representations by ﬁne-tuning 1 We focus on the task of text classiﬁcation, but our method can be easily extended to other NLP or CV tasks with attention mechanisms. 2152 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2152–2161 August 1–6, 2021. ©2021 Association for Computational Linguistics the encoder. Regarding to the compressor, we are motivated by the effectiveness of the information bottleneck (IB) (Tishby et al., 1999) to enhance performance (Li and Eisner, 2019) or detect important features (Bang et al., 2019; Chen and Ji, 2020; Jiang et al., 2020; Schulz et al., 2020), and present a Variational information bottleneck ATtention (VAT) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations. In particular, IB is integrated into attention to minimize the mutual information (MI) with the input while preserving as much MI as possible with the output, which provides more accurate and reliable explanations by controlling the information ﬂow. To evaluate the effectiveness of our proposed approach, we adapt two advanced neural models (LSTM and BERT) within the frame"
2021.findings-acl.189,P11-1015,0,0.0834101,"nd of IpZ; Y q is computed as, IpZ; Y q ě Eppr,yq rEpθ pz|rq rlog qφ py |zqss IpZ;Rq upper bound hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000"
2021.findings-acl.189,N18-1100,0,0.0291649,"with a Variational information bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, an"
2021.findings-acl.189,N18-1097,0,0.026584,"space into two-dimensional space. Accuracy AOPC Accuracy AOPC LSTM-base Random LSTM-ATT LSTM-VAT BERT-base Random BERT-ATT BERT-VAT IMDB 88.79 0.30 5.27 6.13 91.90 0.60 2.81 3.17 SST-1 45.20 5.97 12.94 14.34 51.44 33.26 33.98 34.03 SST-2 85.45 7.58 20.54 21.58 91.60 41.46 41.52 41.52 Yelp 95.10 1.02 6.64 7.12 96.07 3.60 4.73 6.64 AG News 91.91 1.87 5.99 6.59 93.52 44.20 52.22 54.70 Trec 90.00 19.40 31.00 37.20 96.60 65.80 71.60 72.20 Subj 89.00 1.50 2.10 6.30 96.50 45.70 45.70 45.80 Twitter 71.25 4.72 19.10 20.37 75.28 59.21 59.39 59.45 Table 3: The results of AOPC. perturbation curve (AOPC) (Nguyen, 2018; Samek et al., 2016) metric. It calculates the average change of accuracy over test data by deleting top K words via attentive weights. The larger the value of AOPC, the better the explanations of the models. (a) IMDB (LSTM) (b) IMDB (BERT) Figure 4: The inﬂuence of Top-K for LSTM/BERTbased models in terms of AOPC. servations show our VAT model can learn a better task-speciﬁc representation by enforcing the model to reduce the task-irrelevant information. 5.2 Quantitative Evaluation In this section, we evaluate our VAT model using two metrics, AOPC and post-hoc accuracy, which are widely used"
2021.findings-acl.189,P05-1015,0,0.274964,"θ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets. LSTM-base LS"
2021.findings-acl.189,D14-1162,0,0.0851244,"sic models (LSTM/BERT-base) and attention-based models (LSTM/BERT-ATT). LSTM-base takes the max-pooling of the LSTM’s hidden vectors as text representation. For BERTbase, the “[CLS]” representation is obtained as the sentence representation. LSTM-ATT model is a standard attention-based LSTM model that has the same structure as the learner. We obtain the BERTATT by replacing the LSTM encoder with BERT in LSTM-ATT. Our models are marked with VAT (LSTM-VAT, BERT-VAT), which integrate VIB into attention-based neural models. 4.2 Implementation Details For LSTM-based models, we use GloVe embedding (Pennington et al., 2014) with 300-dimension to initialize the word embedding and ﬁne-tune it during the training. We randomly initialize all outof-vocabulary words and weights with the uniform distribution U p´0.1, 0.1q. For the BERT-based models, we ﬁne-tune pre-trained BERT-base model. The dimension of hidden state vectors of LSTM is 100 and the max sentence length is 256 in our experiments. Adam (Kingma and Ba, 2014) is utilized as the optimizer with learning rate 0.001 (for LSTM-based model) and 0.00001 (for BERTbased model). We also search different values β P t0.01, 0.1, 1, 10u. 5 Experiments First, we perform"
2021.findings-acl.189,D19-1002,0,0.0521488,"rformance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot of controversy regarding to the result explanations (Wiegreffe and Pinter, 2019; Jain and Wallace, 2019). Moreover, we ﬁnd that though the attention mechanism can help improve"
2021.findings-acl.189,N18-1027,0,0.254065,"focus on the irrelevant information. For example, in the sentence “A very funny movie.”, the long short-term memory model with standard attention (LSTM-ATT) infers a correct sentiment label while pays more attention to the irrelevant word “movie”, making the result difﬁcult to explain. In general, the attention weights are only optimized to encode the task-relevant information while are not restricted to imitate human behavior. In order to enhance the interpretability of the attention mechanism, recent studies turn to integrate the human provided explanation signals into the attention models. Rei and Søgaard (2018) regularized the attention weights with a small amount of word-level annotations. Barrett et al. (2018); Bao et al. (2018) improved the explanation of attention by aligning explanations with human-provided rationales. These methods rely on additional labour consuming labelling for enhancing explanations, which is hard to extend to other datasets or tasks. In this paper, we aim to train a more efﬁcient and effective interpretable attention model without any pre-deﬁned annotations or pre-collected explanations. Speciﬁcally, we propose a framework consisting of a learner and a compressor, which e"
2021.findings-acl.189,S15-2078,0,0.154169,"iment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is the number of the class, Length is average text length, and #train/#dev/#test counts the number of samples in the train/dev/test sets. LSTM-base LSTM-ATT LSTM-VAT BERT-base BERT-ATT B"
2021.findings-acl.189,S14-2009,0,0.0991451,"Missing"
2021.findings-acl.189,P17-1088,0,0.0167323,"mation bottleneck ATtention (VAT) mechanism. Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability. 1 Introduction Attention mechanisms (Bahdanau et al., 2014) have achieved great success in various natural language processing (NLP) tasks. They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels. The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models (Mullenbach et al., 2018; Xie et al., 2017; Xu et al., 2015). Li et al. (2016) pointed the view: “Attention provides an important way to explain the workings of neural models”. Additionally, Wiegreffe and Pinter (2019) showed that attention mechanisms could help understand the inner workings of a model. The basic assumption of understanding of models with attention scores is that the inputs (e.g., words) with high attentive weights are essential for making decisions. However, as far as we know, it has not been formally veriﬁed. Existing research (Jain and Wallace, 2019) also shows that attention is not explicable, and there are a lot"
2021.findings-acl.189,D13-1170,0,0.00379002,"kkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj hkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkj pθ pz |rq pθ pz |rq Epprq rEpθ pz|rq rlog ss ´ Epprq rEpθ pz|rq rlog ss rψ pzq ppzq Experiment Setup We adopt two typical neural network models, attention-based LSTM (Hochreiter and Schmidhuber, 1997) and BERT (Devlin et al., 2019), to explore our VAT algorithm. 4.1 Datasets and Baselines Datasets To evaluate the effectiveness of our VAT model, we conduct the experiments over eight benchmark datasets: IMDB (Maas et al., 2011), Stanford Sentiment Treebank with (includes SST1 and its binary version SST-2) (Socher et al., 2013), Yelp (Zhang et al., 2015), AG News (Zhang et al., 2015), TREC (Li and Roth, 2002), subjective/objective classiﬁcation Subj (Pang and Lee, 2005) and Twitter (Rosenthal et al., 2015, 2014). The statistics information of these datasets are shown in Table 1. 2155 Class Length #train #dev #test IMDB 2 268 20,000 5,000 25,000 SST-1 5 18 8,544 1,1101 2,210 SST-2 2 19 6,920 872 1,821 Yelp 2 138 500,000 60,000 38,000 AG News 4 32 114,000 6,000 7,600 Trec 6 10 5,000 452 500 Subj 2 23 8,000 1,000 1,000 Twitter 3 22 7,969 1,375 3,795 Table 1: The statistics information of the datasets, where Class is th"
2021.findings-acl.20,P19-1258,1,0.884804,"Missing"
2021.findings-acl.20,D14-1162,0,0.0896297,"lf-attention (Vaswani et al., 2017) is then performed on the vertices, which generates a relation score sij between node features ui and uj : Feature Representation Similar to (Anderson et al., 2018), we extract the image features by using a pretrained Faster RCNN (Ren et al., 2015b). We select µ object proposals for each image, where each object proposal is represented by a 2048-dimension feature vector. The obtained visual region features are denoted as µ v = vi=0 ∈ Rµ×dv . To extract the question features, each word is embedded into a 300-dimensional vector initialed with the Glove vector (Pennington et al., 2014). The word embeddings are taken as inputs by an LSTM encoder (Hochreiter and Schmidhuber, 1997), which produces the initial question representation q ∈ Rλ×dq . Each history sentence features are obtained as same as the question features. We concatenate the last state hlast ∈ Rdh of each turn history features to get the initial history represenGraph Attention sij = (Ui ui )T · Vj uj √ , du (1) where Ui and Vj are trainable parameters. We apply a softmax function over the correlation score sij to obtain weight αij : exp(sij + cu,lab(i,j) ) , j∈N (i) exp(sij + cu,lab(i,j) ) αij = P (2) where c{·}"
2021.findings-acl.205,P18-1167,0,0.0668752,"Missing"
2021.findings-acl.205,2020.emnlp-main.475,0,0.0488938,"Missing"
2021.findings-acl.205,2020.emnlp-main.702,0,0.274689,"d finally achieves its peak when tgolden = 0.9. Thus we finally set tgolden to 0.9. 4.3 Systems Mixer. A sequence-level training algorithm for text generations by combining both REINFORCE and cross-entropy (Ranzato et al., 2016). Minimal Risk Training. Minimal Risk Training (MRT) (Shen et al., 2016) introduces evaluation metrics (e.g., BLEU) as loss functions and aims to minimize expected loss on the training data. 2331 Model Transformerbase (Vaswani et al., 2017) Transformerbase (Vaswani et al., 2017) † + Mixer (Ranzato et al., 2016) † + Minimal Risk Training (Shen et al., 2016) † + TeaForN (Goodman et al., 2020) + TeaForN (Goodman et al., 2020) † + Self-paced learning (Wan et al., 2020) † + Vanilla scheduled sampling (Bengio et al., 2015) † + Target denoising (Meng et al., 2020) † + Sampling with sentence oracles (Zhang et al., 2019) + Sampling with sentence oracles (Zhang et al., 2019) † + Confidence-aware scheduled sampling (ours) † + Confidence-aware scheduled sampling with target denoising (ours) † Transformerbig (Vaswani et al., 2017) Transformerbig (Vaswani et al., 2017) † + Mixer (Ranzato et al., 2016) † + Minimal Risk Training (Shen et al., 2016) † + TeaForN (Goodman et al., 2020) + TeaForN ("
2021.findings-acl.205,P17-2058,0,0.241768,"er to adjust the sharpness of the decay. We draw visible examples for different decay strategies in Figure 2. 3 3.1 Model Confidence Estimation We explore two approaches to estimate model confidence at each token position. Decay Strategies on Training Steps • Inverse Sigmoid Decay: f (i) = we elaborate the fine-grained schedule strategy based on model confidence. Finally, we explore to sample more noisy tokens instead of predicted tokens for high-confidence positions. Approaches In this section, we firstly describe how to estimate model confidence at each token position. Secondly, 3 Following Goyal et al. (2017), model predictions are the weighted sum of target embeddings over output probabilities. As model predictions cause a mismatch with golden tokens, they can simulate translation errors of the inference scene. Predicted Translation Probability (PTP). Current NMT models are well-calibrated with regularization techniques in the training setting (Ott et al., 2018; M¨uller et al., 2019; Wang et al., 2020). Namely the predicted translation probability can directly serve as the model confidence. At the tth target token position, we calculate the model confidence conf (t) as follow: conf (t) = P (yt |y"
2021.findings-acl.205,2020.emnlp-main.176,0,0.0552264,"Missing"
2021.findings-acl.205,W04-3250,0,0.347725,"each is allocated with a batch size of approximately 4096 tokens. We use Adam optimizer (Kingma and Ba, 2014) with 4000 warmup steps. During training and the Monte Carlo Dropout process, we set dropout (Srivastava et al., 2014) rate to 0.1 for the Transformerbase and 0.3 for the Transformerbig . Evaluation. We set the beam size to 4 and the length penalty to 0.6 during inference. We use multibleu.perl to calculate case-sensitive BLEU scores for WMT14 EN-DE and EN-FR, and use mteval-v13a.pl to calculate case-sensitive BLEU scores for WMT19 ZH-EN. We use the paired bootstrap resampling methods (Koehn, 2004) to compute the statistical significance of test results. 4.2 Methods Transformerbase + PTP + Expectation + Variance Experiments Hyperparameter Experiments In this section, we elaborate hyperparameters settings involved in our approaches according to the performance on the validation set of WMT14 ENDE, and share these settings for all WMT tasks. Different Confidence Estimations. In this section, we analyze effects of different estimations for model confidence described in Section 3.1. As shown in Table 2, we observe that Monte Carlo dropout sampling based approaches (i.e., expectation and vari"
2021.findings-acl.205,P16-1162,0,0.238733,"Missing"
2021.findings-acl.205,P16-1159,0,0.0534505,"cing (Goodfellow et al., 2016). However, at the inference stage, golden tokens are unavailable. The model is exposed to an unseen data distribution generated by itself. This discrepancy between training and inference is named as the exposure bias problem (Ranzato et al., 2016). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they mainly fall into two categories. The one is sentence-level training, which treats the sentencelevel metric (e.g., BLEU) as a reward, and directly maximizes the expected rewards of generated sequences (Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017). Although intuitive, they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem (Sutton, 1984; Liu et al., 2018; Wang et al., 2018). Another category is samplingbased approaches, aiming to simulate the data distribution of reference during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling space of scheduled sampling with pred"
2021.findings-acl.205,2020.wmt-1.24,1,0.755962,"ompetence, based on which we design fine-grained schedule strategies. Namely, we sample predicted tokens as target inputs for high-confidence positions and still ground-truth tokens for low-confidence positions. In this way, the NMT model is exactly exposed to corresponding tokens according to its real-time competence rather than coarse-grained predefined patterns. Additionally, we observe that most predicted tokens are the same as ground-truth tokens due to teacher forcing1 , degenerating scheduled sampling to the original teacher forcing mode. Therefore, we further expose more noisy tokens (Meng et al., 2020) (e.g., wordy and incorrect word order) instead of predicted ones for high-confidence token positions. Experimentally, we evaluate our approach on the Transformer (Vaswani et al., 2017) and conduct experiments on large-scale WMT 2014 EnglishGerman (EN-DE), WMT 2014 English-French (EN-FR), and WMT 2019 Chinese-English (ZHEN). The main contributions of this paper can be summarized as follows2 : • To the best of our knowledge, we are the first to propose confidence-aware scheduled sampling for NMT, which exactly samples corresponding tokens according to the real-time model competence rather than"
2021.findings-acl.205,P19-2049,0,0.537351,"though intuitive, they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem (Sutton, 1984; Liu et al., 2018; Wang et al., 2018). Another category is samplingbased approaches, aiming to simulate the data distribution of reference during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling space of scheduled sampling with predictions from beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel twopass decoding architecture. Although these sampling-based approaches have been shown effective, most of them schedule the sampling probability based on training steps. We argue this schedule strategy has two following limitations: 1) It is far from exactly reflecting the real-time model competence; 2) It is only based on training steps and equally treat all token positions, which is too coarse-grained to guide the 2327 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2327–"
2021.findings-acl.205,2020.emnlp-main.80,0,0.0335257,"ausing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 2012), which place distributions over the weights of neural networks. We adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016; Wang et al., 2019b) to approximate Bayesian inference. Given a batch of training data and current NMT model parameterized by θ, we repeatedly conduct forward propagation K times4 . On the k-th propagation, part of neurons θˆ(k) in network 2329 4 We empirically set K to 5 following Wan et al. (2020). θ are randomly deactivated. Eventually, we obtain K sets of model parameters {θˆ(k) }K k=1 and corresponding translation probabilities. We use the expectation or variance of translation probabilities to estimate the model confidence (Wang et al., 2019b). Intuitively, the higher expectation or, the lower variance of translation probabilities reflects higher model confidence. Formally at the t-th token position, we estimate the model confidence conf (t) that calculated by the expectation of translation probabilities: h iK conf (t) = E P (yt |y&lt;t , X, θˆ(k) ) k=1 (4) We also use the variance of"
2021.findings-acl.205,P19-1176,0,0.011971,"s follow: conf (t) = P (yt |y&lt;t , X, θ) (3) Since we base our approach on the Transformer with two-pass decoding (Mihaylova and Martins, 2019; Duckworth et al., 2019), above predicted translation probability can be directly obtained in the first-pass decoding (shown in Figure 1), causing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 2012), which place distributions over the weights of neural networks. We adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016; Wang et al., 2019b) to approximate Bayesian inference. Given a batch of training data and current NMT model parameterized by θ, we repeatedly conduct forward propagation K times4 . On the k-th propagation, part of neurons θˆ(k) in network 2329 4 We empirically set K to 5 following Wan et al. (2020). θ are randomly deactivated. Eventually, we obtain K sets of model parameters {θˆ(k) }K k=1 and corresponding translation probabilities. We use the expectation or variance of translation probabilities to estimate the model confidence (Wang et al., 2019b). Intuitively, the higher expectation or, the lower variance of"
2021.findings-acl.205,D19-1073,0,0.0193893,"s follow: conf (t) = P (yt |y&lt;t , X, θ) (3) Since we base our approach on the Transformer with two-pass decoding (Mihaylova and Martins, 2019; Duckworth et al., 2019), above predicted translation probability can be directly obtained in the first-pass decoding (shown in Figure 1), causing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 2012), which place distributions over the weights of neural networks. We adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016; Wang et al., 2019b) to approximate Bayesian inference. Given a batch of training data and current NMT model parameterized by θ, we repeatedly conduct forward propagation K times4 . On the k-th propagation, part of neurons θˆ(k) in network 2329 4 We empirically set K to 5 following Wan et al. (2020). θ are randomly deactivated. Eventually, we obtain K sets of model parameters {θˆ(k) }K k=1 and corresponding translation probabilities. We use the expectation or variance of translation probabilities to estimate the model confidence (Wang et al., 2019b). Intuitively, the higher expectation or, the lower variance of"
2021.findings-acl.205,2020.acl-main.278,0,0.0764053,"isy tokens instead of predicted tokens for high-confidence positions. Approaches In this section, we firstly describe how to estimate model confidence at each token position. Secondly, 3 Following Goyal et al. (2017), model predictions are the weighted sum of target embeddings over output probabilities. As model predictions cause a mismatch with golden tokens, they can simulate translation errors of the inference scene. Predicted Translation Probability (PTP). Current NMT models are well-calibrated with regularization techniques in the training setting (Ott et al., 2018; M¨uller et al., 2019; Wang et al., 2020). Namely the predicted translation probability can directly serve as the model confidence. At the tth target token position, we calculate the model confidence conf (t) as follow: conf (t) = P (yt |y&lt;t , X, θ) (3) Since we base our approach on the Transformer with two-pass decoding (Mihaylova and Martins, 2019; Duckworth et al., 2019), above predicted translation probability can be directly obtained in the first-pass decoding (shown in Figure 1), causing no additional computation costs. Monte Carlo Dropout Sampling. The model confidence can be quantified by Bayesian neural networks (Buntine and"
2021.findings-acl.205,P18-1083,0,0.167393,"s the exposure bias problem (Ranzato et al., 2016). Many techniques have been proposed to alleviate the exposure bias problem. To our knowledge, they mainly fall into two categories. The one is sentence-level training, which treats the sentencelevel metric (e.g., BLEU) as a reward, and directly maximizes the expected rewards of generated sequences (Ranzato et al., 2016; Shen et al., 2016; Rennie et al., 2017). Although intuitive, they generally suffer from slow and unstable training due to the high variance of policy gradients and the credit assignment problem (Sutton, 1984; Liu et al., 2018; Wang et al., 2018). Another category is samplingbased approaches, aiming to simulate the data distribution of reference during training. Scheduled sampling (Bengio et al., 2015) is a representative method, which samples tokens between golden references and model predictions with a scheduled probability. Zhang et al. (2019) further refine the sampling space of scheduled sampling with predictions from beam search. Mihaylova and Martins (2019) and Duckworth et al. (2019) extend scheduled sampling to the Transformer with a novel twopass decoding architecture. Although these sampling-based approaches have been shown"
2021.findings-acl.205,2020.emnlp-main.216,0,0.0777456,"Missing"
2021.findings-acl.205,P17-4012,0,0.0861965,"0 4 We conduct experiments on three large-scale WMT 2014 English-German (EN-DE), WMT 2014 English-French (EN-FR), and WMT 2019 ChineseEnglish (ZH-EN) translation tasks. We respectively build a shared source-target vocabulary for the ENDE and EN-FR datasets, and unshared vocabularies for the ZH-EN dataset. We apply byte-pair encoding (Sennrich et al., 2016) with 32k merge operations for all datasets. More datasets statistics are listed in Table 1. 4.1 Implementation Details Training Setup. We train the Transformerbase and Transformerbig models (Vaswani et al., 2017) with the open-source THUMT (Zhang et al., 2017). All Transformer models are first trained by teacher forcing with 100k steps, and then trained with different training objects or scheduled sampling approaches for 300k steps. All experiments are conducted on 8 NVIDIA Tesla V100 GPUs, where each is allocated with a batch size of approximately 4096 tokens. We use Adam optimizer (Kingma and Ba, 2014) with 4000 warmup steps. During training and the Monte Carlo Dropout process, we set dropout (Srivastava et al., 2014) rate to 0.1 for the Transformerbase and 0.3 for the Transformerbig . Evaluation. We set the beam size to 4 and the length penalty"
2021.findings-acl.205,P19-1426,1,0.904866,"Missing"
2021.findings-acl.205,Q19-1006,0,0.060016,"the performance bottleneck for Encoder20 Transformerbase becomes more evident (dashed blue line). Despite this, our approaches (solid blue line) still keep improving performance with the growth of decoder layers on the stronger Encoder20 Transformerbase . In summary, our confidence-aware schedule strategy brings a meaningful increase in the difficulty of decoders, and the bottleneck at the decoder side is alleviated to a certain extend. 5.4 Effects on Different Sequence Lengths Due to error accumulations, the exposure bias problem becomes more problematic with the growth of sequence lengths (Zhou et al., 2019; Zhang et al., 2020). Thus it is intuitive to verify the effectiveness of our approach over different sequence lengths. Considering the validation set of WMT14 EN-DE (3k) is too small to cover scenarios with various sentence lengths, we randomly select 10k training data with lengths from 10 to 100. As shown in Figure 5, our approach consistently outperform the Transformerbase model at different sequence lengths. Moreover, the improvements of our approach over the Transformerbase is gradually increasing with sentence lengths. Specifically, we observe more than 1.0 BLEU improvements when senten"
2021.findings-acl.205,2020.acl-main.620,0,0.0308177,"19b). Intuitively, the higher expectation or, the lower variance of translation probabilities reflects higher model confidence. Formally at the t-th token position, we estimate the model confidence conf (t) that calculated by the expectation of translation probabilities: h iK conf (t) = E P (yt |y&lt;t , X, θˆ(k) ) k=1 (4) We also use the variance of translation probabilities to estimate the model confidence conf (t) as an alternative: conf (t) = 1 − Var [P (yt |y&lt;t , X, θ)]K k=1 (5) where Var[·] denotes the variance of a distribution that calculated following the setting in (Wang et al., 2019b; Zhou et al., 2020). We will further analyze the effect of different confidence estimations in Section 4.2. 3.2 Confidence-Aware Scheduled Sampling The confidence score conf (t) quantifies whether the current NMT model is confident or hesitant on predicting the t-th target token. We take conf (t) as exact and real-time information to conduct a fine-grained schedule strategy in each training iteration. Specifically, a lower conf (t) indicates that the current model θ still struggles with the teacher forcing mode for the t-th target token, namely underfitting for the conditional probability P (yt |y&lt;t , X, θ). Thu"
2021.findings-acl.205,D15-1105,0,0.0236066,"Missing"
2021.findings-acl.38,2020.acl-main.728,0,0.110046,"on (Xu et al., 2015; Anderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in orde"
2021.findings-acl.38,2020.emnlp-main.275,1,0.373377,"le of visual dialogue. The color in text background corresponds to the same color box in the image, which indicates the same entity. Our model firstly associates textual entities with objects explicitly and then gives contextually and visually coherent answers to contextual questions. Introduction Recently, there is increasing interest in visionlanguage tasks, such as image caption (Xu et al., 2015; Anderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to s"
2021.findings-acl.38,P19-1258,1,0.88914,"Missing"
2021.findings-acl.38,P19-1648,0,0.260117,"f understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in order to find related visual content. Furthermore, models based on object-level image features (Niu et al., 2019; Gan et al., 2019; Chen et al., 2020a; Jiang et al., 2020a; Nguyen 436 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 436–446 August 1–6, 2021. ©2021 Association for Computational Linguistics et al., 2020; Jiang et al., 2020b) are proposed to effectively leverage the visual content for multimodal co-reference. However, as implicit exploration of multimodal co-reference, these methods implicitly attend to spatial or object-level image features, which is trained with the whole model and is inevitably distracted by unnecessary visual content. Intuitively, specific mapping of obj"
2021.findings-acl.38,P19-1002,1,0.786372,"notes the multi-head self-attention layer (Vaswani et al., 2017), then  vg(n) = FFN vˆgni , (6) i where n = 1, . . . , Nv and FFN(·) denotes the position wise feed-forward networks (Vaswani et al., 2017). After Nv layers computation, we obtain the final visual grounding features vgi by: vgi v) = vg(N , i (7) Actually, there are some questions that do not contain any entities in the visual dialogue, such as “anything else ?”. For such questions, we use the features of the whole image instead, i.e. vgi = v. 2.4 Multimodal Incremental Transformer Inspired by the idea of incremental transformer (Li et al., 2019) which is originally designed for the single-modal dialogue task, we make an extension and propose a multimodal incremental transformer, which is composed of a Multimodal Incremental Transformer Encoder (MITE) and a Gated CrossAttention Decoder (GCAD). The MITE uses an incremental encoding scheme to encode multi-turn 3 For simplicity, we omit the descriptions of layer normalization and residual connection. 2.4.1 MITE To effectively encode multi-turn utterances grounded in visual content, we design the Multimodal Incremental Transformer Encoder (MITE). As shown in Figure 3 (b), at the i-th roun"
2021.findings-acl.38,2020.emnlp-main.269,0,0.122558,"nderson et al., 2016, 2018; Cornia et al., 2020) and visual question answering (Ren et al., 2015a; Gao et al., 2015; Lu et al., 2016; Anderson et al., 2018). In the real world, our conversations (Chen et al., 2020b, 2019) usually have multiple turns. As an extension of conventional single-turn visual question answering, Das et al. (2017) introduce a multi-turn visual question answering task named visual dialogue, which aims to explore the ability of an AI agent to hold a meaningful multi-turn dialogue with humans in natural language about visual content. Visual dialogue (Agarwal et al., 2020; Wang et al., 2020; Qi et al., 2020; Murahari et al., 2020) requires agents to give a response on the basis of understanding both visual and textual content. One of the key challenges in visual dialogue is how to solve multimodal co-reference (Das et al., 2017; Kottur et al., 2018). Therefore, some fusion-based models (Das et al., 2017) are proposed to fuse spatial image features and textual features in order to obtain a joint representation. Then attention-based models (Lu et al., 2017; Wu et al., 2018; Kottur et al., 2018) are proposed to dynamically attend to spatial image features in order to find related v"
2021.findings-acl.91,N19-1423,0,0.00514339,"ently and reliably assess the consistency capacity of chatbots and achieve a high ranking correlation with the human evaluation. We release the framework and hope to help improve the consistency capacity of chatbots.1 1 Human: Blender: Human: Blender: Human: Plato: Human: Plato: Human: Plato: Table 1: Several human-bot conversations demonstrate that popular chatbots (DialoGPT, Blender, and Plato) generate inconsistent responses when talking to a human under some specific conditions. et al., 2020) have approached great progress due to the development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of high-quality conversational datasets (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). Though the success is indisputable and exciting, there is still a long way to build a truly human-like open-domain chatbot. Introduction In recent years, open-domain intelligent chatbots (Adiwardana et al., 2020b; Bao et al., 2020; Smith ∗ DialoGPT 762M What movies do you like most? The Unforgiven from Metallica. What do you think of the Unforgiven? I never heard of it. Blender 2.7B What do you like to cook? I only cook Ind"
2021.findings-acl.91,2021.ccl-1.108,0,0.0370867,"Missing"
2021.findings-acl.91,2020.sigdial-1.28,0,0.0644522,"Missing"
2021.findings-acl.91,2020.acl-demos.14,0,0.0135359,"similar questions. Therefore, to mimic such a contradiction occurrence process, we make chatbots to produce responses by asking chatbots related questions about previous facts and opinions. In this condition, generating appropriate questions is pretty important. Hence, we first extract entities about facts and opinions from the historical utterances, then employ a neural model to generate questions about the extracted entities. Entity Extraction Considering that chatbots usually generate contradictions when chatting about facts and opinions, we apply Named Entity Recognition tools in Stanza (Qi et al., 2020), a popular natural language analysis package, to extract named entities from utterance u2k containing person, organization, location, etc. 2 For example, for the utterance “I would love to visit New York next year.”, we can extract out two entities: “New York” and “next year”. Question Generation Model For question generation, we employ UniLM (Dong et al., 2019) model that is fine-tuned on the SQuAD dataset (Rajpurkar et al., 2016) with question generation task (Wangperawong, 2020). We utilize a public implementation and checkpoint.3 In our framework, given the entities extracted before and u"
2021.findings-acl.91,P19-1534,0,0.0283324,"Dialogue Evaluation track (Gunasekara et al., 2021). We reproduced the DialoFlow model based on GPT2-large (Radford et al., 2019) and fine-tuned it with BST dataset. Chatbots We select several popular open-domain chatbots in our experiments. Blender (BL) (Adiwardana et al., 2020a) is firstly pre-trained on Reddit dataset (Baumgartner et al., 2020) and then fine-tuned with high-quality human annotated dialogue datasets (BST), which containing four datasets: Blended Skill Talk (Smith et al., 2020), Wizard of Wikipedia (Dinan et al., 2019), ConvAI2 (Dinan et al., 2020), and Empathetic Dialogues (Rashkin et al., 2019). By fine-tuning, Blender can learn blend conversational skills of engagement, knowledge, empathy and personality. Blender has three model sizes: 90M, 2.7B, and 9.4B. Since 2.7B parameter model achieves the best performance in (Adiwardana et al., 2020a), we use the 2.7B version in our experiments. Plato (PL) (Bao et al., 2020) is an open-domain chatbot, pre-trained on Reddit dataset and finetuned with BST dataset, which is claimed to be 4.2 Experimental Settings We adopt four experimental paradigms to evaluate the effectiveness of the AIH. Bot-Bot Interaction. For bot-bot interaction, the maxi"
2021.findings-acl.91,2020.acl-main.183,0,0.135999,"improve the consistency capacity of chatbots.1 1 Human: Blender: Human: Blender: Human: Plato: Human: Plato: Human: Plato: Table 1: Several human-bot conversations demonstrate that popular chatbots (DialoGPT, Blender, and Plato) generate inconsistent responses when talking to a human under some specific conditions. et al., 2020) have approached great progress due to the development of the large-scale pre-training approaches (Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020) and the large amount of high-quality conversational datasets (Dinan et al., 2019; Baumgartner et al., 2020; Smith et al., 2020). Though the success is indisputable and exciting, there is still a long way to build a truly human-like open-domain chatbot. Introduction In recent years, open-domain intelligent chatbots (Adiwardana et al., 2020b; Bao et al., 2020; Smith ∗ DialoGPT 762M What movies do you like most? The Unforgiven from Metallica. What do you think of the Unforgiven? I never heard of it. Blender 2.7B What do you like to cook? I only cook Indian cuisine. How about you? I enjoy cooking Chinese food, especially the dumplings. Chinese food is delicious. I also like cooking the Chinese food. Plato 1.6B Do you like"
2021.findings-acl.91,2020.emnlp-main.539,0,0.126179,"nt is the lack of an effective and practical evaluation method. To estimate the consistency of chatbots, the most straightforward approach is to ask human annotators to distinguish whether the conversations generated from the chatbots are consistent or not. However, the instructions followed by annotators are often chosen ad-hoc, and there is no explicit definition, which leads to the relatively low interagreement in the human chatbot consistency evaluation (Mehri and Esk´enazi, 2020). As a result, several works have been proposed to develop automatic evaluation methods (Welleck et al., 2019; Song et al., 2020; Nie et al., 2020). While these methods can detect contradictions efficiently in the dialogue, they depend on the human-bot conversations, which is still cost-inefficient and tend to suffer from low quality (Deriu et al., 2020; Dinan et al., 2020). Besides, the occurrence rate of contradiction is low under this condition. All these problems slow down the development of consistency evaluation of dialogue systems severely. Towards that end, based on the observations: (i) chatbots are likely to generate contradictions when chatting about facts and opinions; (ii) answering the questions about the"
2021.findings-acl.91,P19-1363,0,0.114052,"consistency development is the lack of an effective and practical evaluation method. To estimate the consistency of chatbots, the most straightforward approach is to ask human annotators to distinguish whether the conversations generated from the chatbots are consistent or not. However, the instructions followed by annotators are often chosen ad-hoc, and there is no explicit definition, which leads to the relatively low interagreement in the human chatbot consistency evaluation (Mehri and Esk´enazi, 2020). As a result, several works have been proposed to develop automatic evaluation methods (Welleck et al., 2019; Song et al., 2020; Nie et al., 2020). While these methods can detect contradictions efficiently in the dialogue, they depend on the human-bot conversations, which is still cost-inefficient and tend to suffer from low quality (Deriu et al., 2020; Dinan et al., 2020). Besides, the occurrence rate of contradiction is low under this condition. All these problems slow down the development of consistency evaluation of dialogue systems severely. Towards that end, based on the observations: (i) chatbots are likely to generate contradictions when chatting about facts and opinions; (ii) answering the"
2021.findings-acl.91,N18-1101,0,0.0177248,"luator is generally a contradiction detection model. The Auto Evaluator take the response rk answered by Chatbot2 and the previous utterance u2k as input, and output the contradiction score yk . It can be formulated as: yk = fθ (rk , u2k ), (1) where fθ is the detection function and θ is the parameters. Compared to other contradiction detection methods that consider the whole dialogue, the Auto Evaluator can refrain from the noise contained in the whole dialogue. In practice, we select the Roberta-large model (Liu et al., 2019) fine-tuned on the Multi-Genre Natural Language Inference dataset (Williams et al., 2018) as the implementation of Auto Evaluator.4 Human Evaluator In traditional dialogue consistency evaluation methods, human judges are asked to read the whole dialogue and give an overall consistency score, usually 0 or 1. In our opinion, these methods suffer from high cost and low inter-agreement because there is no specific instruction, and it is too hard for human judges to give an overall score on the whole dialogue (Mehri and Esk´enazi, 2020). In our framework, human evaluators are only asked to decide if the response rk answered by Chatbot2 is consistent with the previous utterance u2k or n"
2021.findings-acl.91,2020.acl-demos.30,0,0.440182,"dogs very much. I have three dogs. I have three dogs too. One puppy and two Huskies. Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc. Yang Feng is the corresponding author. Work was done when Zekang Li and Zhengcong Fei were intern at WeChat AI. 1 https://github.com/ictnlp/AIH Current open-domain chatbots hold a superiority in generating fluent, engaging, and informative responses, but show the soft spot on consistency (Nie et al., 2020). As shown in Table 1, we present some interactive dialogue samples between human and several popular open-domain chatbots (e.g. DialoGPT (Zhang et al., 2020), Blender (Smith et al., 2020), and Plato (Bao et al., 2020)). All open1057 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1057–1067 August 1–6, 2021. ©2021 Association for Computational Linguistics domain chatbots occasionally generate responses that are contradictory with history when interacting with humans, which is really annoying and severely disrupts the communication once happening. Therefore, it is imperative to improve the consistency of the open-domain chatbots. However, one crucial reason that restricts consistency development is the lack of an ef"
2021.findings-emnlp.158,2020.acl-main.728,0,0.0194252,"ng and is optimized for the VD setting especially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-theart performance on both image-guessing task and question diversity. Human study further proves that our model generates more visually related, informative and coherent questions. 1 Introduction Visual Dialog (VD), which expects AI agents to conduct visually related dialog, has attracted growing interests due to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing game setting, namely GuessWhich (Das et al"
2021.findings-emnlp.158,2021.findings-acl.20,1,0.719069,"cially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-theart performance on both image-guessing task and question diversity. Human study further proves that our model generates more visually related, informative and coherent questions. 1 Introduction Visual Dialog (VD), which expects AI agents to conduct visually related dialog, has attracted growing interests due to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing game setting, namely GuessWhich (Das et al., 2017b). Given an undisclosed image, Gu"
2021.findings-emnlp.158,P19-1648,0,0.0192871,"Augmented Guesser (AugG) that is strong and is optimized for the VD setting especially. Experimental results on the VisDial v1.0 dataset show that our approach achieves state-of-theart performance on both image-guessing task and question diversity. Human study further proves that our model generates more visually related, informative and coherent questions. 1 Introduction Visual Dialog (VD), which expects AI agents to conduct visually related dialog, has attracted growing interests due to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing g"
2021.findings-emnlp.158,N16-1014,0,0.0347327,"of Q-Bot with the following metrics: 1) lastly the candidate images (images in validation Unique questions (Murahari et al., 2019a): mean split) are sorted according to their similarity to the number of unique questions in the 10-round dialog; prediction and compute the rank of the target im- 2) Mutual overlap (Deshpande et al., 2018): mean age. The evaluation metrics are: 1) MRR (Radev BLEU-4 (Papineni, 2002) overlap with the other et al., 2002): mean reciprocal rank of target image; 9 questions in the 10-round dialog; 3) Dist-n and 2) R@k (Das et al., 2017b): the existence of target Ent-n (Li et al., 2016; Zhang et al., 2018): number image in the top-k images; 3) Mean (Das et al., and entropy of distinct n-grams in the generated 2017b): mean rank of target image; 4) PMR (Das questions divided by the total number of tokens. et al., 2017b): percentile mean rank. As shown in Tab. 3, row 6 indicates that our We illustrate the results in Tab. 2. As shown in method achieves the new SOTA performance on row 10, our method achieves the best performance question diversity. Specifically, our RL-ReeQ on all metrics and becomes the new state of the achieves approximately 2 points improvement on art, with a"
2021.findings-emnlp.158,D19-1152,0,0.322691,"e to its research significance and application prospects. Most of the work (Lu et al., 2017; Niu et al., 2019; Gan et al., 2019; Chen et al., 2020; Agarwal et al., 2020; Nguyen et al., 2020; Chen et al., 2021) pays attention to modeling an Answerer agent. However, it is also important to ∗ Equal contribution. Work was done when Zheng and Xu were interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Xiaojie Wang is the corresponding author. model a VD Questioner agent that can constantly ask visually related and informative questions. Previous researches (Das et al., 2017b; Murahari et al., 2019a; Zhou et al., 2019) have explored building open-domain VD Questioner under a QBot-A-Bot image-guessing game setting, namely GuessWhich (Das et al., 2017b). Given an undisclosed image, GuessWhich can be regarded to have two stages: 1) Dialog generation stage: Q-Bot (Questioner, who only knows a caption of the image at first) successively asks questions to collect information about the image, and A-Bot (Answerer, who can see the image) answers the questions. 2) Guess stage: Q-Bot guesses the target image based on the generated dialog. Corresponding to the two stages, Q-Bot has two roles, i.e,"
2021.findings-emnlp.158,P02-1040,0,0.111109,"ance on image-guessing task. In concrete, QGen and A-Bot firstly generate 10-round dialog, then Question Diversity. We evaluate the question diGuesser makes a prediction about the unseen image, versity of Q-Bot with the following metrics: 1) lastly the candidate images (images in validation Unique questions (Murahari et al., 2019a): mean split) are sorted according to their similarity to the number of unique questions in the 10-round dialog; prediction and compute the rank of the target im- 2) Mutual overlap (Deshpande et al., 2018): mean age. The evaluation metrics are: 1) MRR (Radev BLEU-4 (Papineni, 2002) overlap with the other et al., 2002): mean reciprocal rank of target image; 9 questions in the 10-round dialog; 3) Dist-n and 2) R@k (Das et al., 2017b): the existence of target Ent-n (Li et al., 2016; Zhang et al., 2018): number image in the top-k images; 3) Mean (Das et al., and entropy of distinct n-grams in the generated 2017b): mean rank of target image; 4) PMR (Das questions divided by the total number of tokens. et al., 2017b): percentile mean rank. As shown in Tab. 3, row 6 indicates that our We illustrate the results in Tab. 2. As shown in method achieves the new SOTA performance on"
2021.findings-emnlp.158,radev-etal-2002-evaluating,0,0.122657,"AugG− means only stochastic negative samples are used in training. Noticeably, our result on Unique questions is approaching the upper bound, i.e., 10. Besides, our method also achieves better language diversity according to Mutual overlap, Dist-1, Dist-2, Ent-1 and Ent-2 (row 3 and row 6). A-Bot Performance. We evaluate the A-Bot performance in a retrieval setting, following Das et al. (2017a). Additional 100 candidate answers for each instance are provided and the model is evaluated by retrieval metrics: 1) NDCG (Järvelin and Kekäläinen, 2002): normalized discounted cumulative gain; 2) MRR (Radev et al., 2002): mean reciprocal rank of the ground truth answer; 3) R@k (Das et al., 2017a): the existence of the ground truth answer in the top-k answers; 4) Mean (Das et al., 2017a): mean rank of the ground truth answer. Tab. 4 shows the comparing results of A-Bot performance. Our model achieves higher NDCG, MRR, R@1, R@5 and R@10. # 1 2 3 4 5 6 DasQ+r1 DasQ+r2 DasQ+r3 ReeQ+r1 ReeQ+r2 ReeQ+r3 MRR↑ 25.65 32.19 32.77 32.27 32.78 33.65 R@1↑ 16.30 19.09 19.47 18.56 19.38 19.91 R@5↑ 40.50 46.27 46.75 46.75 47.00 48.5 R@10↑ 55.43 60.76 62.89 61.01 62.65 62.94 Mean↓ 28.57 21.64 20.45 19.58 19.46 18.05 PMR↑ 98.76"
2021.findings-emnlp.158,N19-1265,0,0.0204564,"al. (2017b) propose the task and generate questions in a sequence-tosequence fashion. Murahari et al. (2019a) propose to reduce repetition by penalizing the similarity in successive dialog hidden states. Zhou et al. (2019) retrieve the most-likely image, encode the image into a multi-modal context vector and use it to decode questions. These methods follow a sequenceto-sequence fashion while ReeQ explicitly uses related-entities as guidance to generate questions following a learned strategy. Our work is also relevant to the works (Zhang et al., 2017; Zhao and Tresp, 2018; Strub et al., 2017; Shekhar et al., 2019; Shukla et al., 2019; Xu et al., 2020) that focus on VD Questioner for GuessWhat?! (de Vries et al., 2017), where the goal is to locate a target object in the image and the answers can only be “yes/no/not available”. Compared to them, building a Questioner in a more open-domain VD setting is of more difficulty. Moreover, Q-Bot in GuessWhich has no access to visual information, making it harder to ask visually related questions. 7 Conclusion In this paper, we propose Related entity enhanced Questioner (ReeQ) and Augmented Guesser (AugG) to enhance Visual Dialog Questioner in both SL and RL. Re"
2021.findings-emnlp.158,P19-1646,0,0.0197375,"he task and generate questions in a sequence-tosequence fashion. Murahari et al. (2019a) propose to reduce repetition by penalizing the similarity in successive dialog hidden states. Zhou et al. (2019) retrieve the most-likely image, encode the image into a multi-modal context vector and use it to decode questions. These methods follow a sequenceto-sequence fashion while ReeQ explicitly uses related-entities as guidance to generate questions following a learned strategy. Our work is also relevant to the works (Zhang et al., 2017; Zhao and Tresp, 2018; Strub et al., 2017; Shekhar et al., 2019; Shukla et al., 2019; Xu et al., 2020) that focus on VD Questioner for GuessWhat?! (de Vries et al., 2017), where the goal is to locate a target object in the image and the answers can only be “yes/no/not available”. Compared to them, building a Questioner in a more open-domain VD setting is of more difficulty. Moreover, Q-Bot in GuessWhich has no access to visual information, making it harder to ask visually related questions. 7 Conclusion In this paper, we propose Related entity enhanced Questioner (ReeQ) and Augmented Guesser (AugG) to enhance Visual Dialog Questioner in both SL and RL. ReeQ generates question"
2021.findings-emnlp.158,D19-1014,0,0.0362213,"Missing"
2021.findings-emnlp.212,D14-1179,0,0.00928461,"Missing"
2021.findings-emnlp.212,2020.acl-main.747,0,0.036555,"ere Slang is the language sets for training. Proportional Sampling. Another method is sampling by proportion (Neubig and Hu, 2018). This method improves the model’s performance on high resource languages and reduces the performance of the model on low resource languages. Specifically, we calculate its sampling weight ψi for each language pair i as i |DTrain | , k k∈Slang |DTrain | ψi = P (4) where DTrain is the training corpora of language i. Temperature-based Sampling. It samples the language pairs according to the corpora size exponentiated by a temperature term τ (Arivazhagan et al., 2019; Conneau et al., 2020) as 1/τ ψi = P pk 1/τ k∈Slang pk i |DTrain | . k k∈Slang |DTrain | ∗ θ = argmin θ MultiDDS-S. MultiDDS-S (Wang et al., 2020) is a dynamic sampling method performing differentiable data sampling. It takes turns to optimize the sampling weights of different languages and the multilingual machine translation model, showing more significant potential than static sampling methods. This method optimizes the sample weight ψ to minimize the development loss as follows ψ ∗ = argmin L(θ∗ ; DDev ), (6) ψi L(θ; DTrain ), (7) i=1 where DDev and DTrain denote the development corpora and the training corpora"
2021.findings-emnlp.212,2020.coling-tutorials.3,0,0.150973,"e (Barrault et al., 2020). This would lead has been learned; and 2) HRLs-evaluated Comto low learning competencies for distant languages petence, evaluating whether an LRL is ready to be learned according to HRLs’ Self-evaluated compared to closely related languages. Therefore, Competence. Based on the above competenmultilingual machine translation is inherently imcies, we utilize the proposed CCL-M algobalanced, and dealing with this imbalance is critirithm to gradually add new languages into the cal to advancing multilingual machine translation training set in a curriculum learning manner. (Dabre et al., 2020). Furthermore, we propose a novel competenceTo address the above problem, existing balancaware dynamic balancing sampling strategy for better selecting training samples in muling methods can be divided into two categories, tilingual training. Experimental results show i.e., static and dynamic. 1) Among static balancing that our approach has achieved a steady and methods, temperature-based sampling (Arivazhasignificant performance gain compared to the gan et al., 2019) is the most common one, comprevious state-of-the-art approach on the TED pensating for the gap between different training talks"
2021.findings-emnlp.212,N19-4009,0,0.0116756,"(Johnson et al., 2017). 4.2 Implementation Details Baseline. We select three static heuristic strategies: uniform sampling, proportional sampling, and temperature-based sampling (τ = 5), and the bitext models for the baseline. In addition, we compare our approach with the previous state-of-the-art sampling method, MultiDDS-S (Wang et al., 2020). All baseline methods use the same model and the same set of hyper-parameters as our approach. 4 https://github.com/google/ sentencepiece Model. We validate our approach upon the multilingual Transformer (Vaswani et al., 2017) implemented by fairseq5 (Ott et al., 2019). The number of layers is 6 and the number of attention heads is 4, with the embedding dimension dmodel of 512 and the feed-forward dimension dff of 1024 as (Wang et al., 2020). For training stability, we adopt PreLN (Xiong et al., 2020) for the layer-norm (Ba et al., 2016) module. For M2O tasks, we use a shared encoder with a vocabulary of 64k. Similarly, for O2M tasks, we use a shared decoder with a vocabulary of 64k. Training Setup. We use the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 to optimize the model. Further, the same learning rate schedule as Vaswani et al. (2017"
2021.findings-emnlp.212,W18-6301,0,0.0194124,"l., 2016) module. For M2O tasks, we use a shared encoder with a vocabulary of 64k. Similarly, for O2M tasks, we use a shared decoder with a vocabulary of 64k. Training Setup. We use the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 to optimize the model. Further, the same learning rate schedule as Vaswani et al. (2017) is used, i.e., linearly increase the learning rate for 4000 steps to 2e-4 and decay proportionally to the inverse square root of the step number. We accumulate the batch size to 9,600 and adopt half-precision training implemented by apex6 for faster convergence (Ott et al., 2018). For regularization, we also use a dropout (Srivastava et al., 2014) p = 0.3 and a label smoothing (Szegedy et al., 2016) ls = 0.1. As for our approach, we sample 256 candidates from each languages’ development corpora every 100 steps to calculate the Self-evaluated Competence c for each language and HRLs-evaluated Competence cˆ for each LRL. Evaluation. In practice, we perform a grid search for the best threshold t in {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, and select the checkpoints with the lowest weighted loss7 on the development sets to conduct the evaluation. The corresponding early stopping p"
2021.findings-emnlp.212,N19-1119,0,0.0412201,"Missing"
2021.findings-emnlp.212,W18-6319,0,0.0149429,"ple 256 candidates from each languages’ development corpora every 100 steps to calculate the Self-evaluated Competence c for each language and HRLs-evaluated Competence cˆ for each LRL. Evaluation. In practice, we perform a grid search for the best threshold t in {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, and select the checkpoints with the lowest weighted loss7 on the development sets to conduct the evaluation. The corresponding early stopping patience is set to 10. For target sentence generation, we set the beam size to 5 and a length penalty of 1.0. Following Wang et al. (2020), we use the SacreBLEU (Post, 2018) to evaluate the model performance. In the end, we compare our result with MultiDDS-S using paired bootstrap resampling (Koehn, 2004) for significant test. 4.3 Results Main Results. The main results are listed in Table 1. As we can see, both methods significantly 5 https://github.com/pytorch/fairseq https://github.com/NVIDIA/apex 7 This loss is calculated by averaging the loss of each samples in development corpora of all languages, which is equivalent to taking the proportional weighted average of the loss for each language. 2486 6 M2O Related Diverse Method O2M Related Diverse Bitext Models"
2021.findings-emnlp.212,2020.acl-main.148,0,0.0392629,"Missing"
2021.findings-emnlp.268,2020.acl-main.9,0,0.0286572,"ete latent variable ze denoting emotion consensus. where Ew (·), Ep (·), and Er (·) ∈ R|V |×demb represent word embedding space, positional embedding space and role embedding space1 , respectively. Finally, a transformer encoder (Vaswani et al., 2017) fenc is applied to get the context representation: H = fenc (Ec ([CTX; c])), (2) where “;” represents the concatenation operation, and H ∈ R(n+1)×dmod . The contextualized encoding of CTX, i.e., H0 ∈ Rdmod , is used as the final representation of the entire context. Emotion Consensus Construction. A K-way categorical latent variable ze ∈ [1, K] (Bao et al., 2020) is used to capture the emotion consensus shared by c and y. Inspired by Zhao et al. (2019), we define the prior distribution where we sample ze from to be uniform2 , i.e., p(ze ) = 1/K. Correspondingly, the approximate posterior distribution is defined as follows: q(ze |c) = softmax(FFN(H0 )) ∈ RK , (3) where FFN(·) represents a feedforward network. This part can be considered as the emotion understanding on c. Here ze has its own embedding space Ez ∈ RK×dmod to convert it into a vector, i.e., E[z] = Ez (ze ) ∈ Rdmod . To supervise the emotion expression in E[z] , we train a classifier using"
2021.findings-emnlp.268,K16-1002,0,0.0445425,"found in Appendix. The final et al. (2020), we use pre-trained GloVe vectors loss functions for unpaired data c and y are: (Pennington et al., 2014) to initialize the word emLc = L3 + βLemo , (18) beddings. Besides, all common hyper-parameters are set the same as previous work, e.g., the hidLy = L4 + γLemo , (19) den size dmod and embedding size demb are set to where β and γ are two hyper-parameters. 300. In order to alleviate the degeneration problem Total Training Loss. During training, the paired of variational framework, we apply KL annealing empathetic data in E MPATHETIC D IALOGUES and (Bowman et al., 2016) that is the same as in Zhou 3128 − DKL [q(ze |y)||p(ze )], et al. (2018). During inference, we use greedy decoding strategy and the maximum decoding step is set to 30. K equals to 32. α, β, and γ are simply set to 1. The running epoch is set to 30 with early stopping. To get unpaired emotional data, we utilize two large-scale datasets of open-domain conversations provided by Zhong et al. (2020), namely Reddit and Twitter. Following Zhou et al. (2018) and Shen and Feng (2020), an emotion classifier is applied to obtain the ground-truth emotion label for each context and response. Here, we use"
2021.findings-emnlp.268,D18-1507,0,0.0203879,"te the constraint of paired data, we extract unpaired emotional data from open-domain conversations and employ Dual-Emp to produce pseudo paired empathetic samples, which is more efficient and low-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses. Figure 1: An example of conducting an empathetic conversation. Both responses show empathy to the speaker. 2005). The studies of empathy in natural language processing mainly include detecting empathy in spoken language or text (Buechel et al., 2018; Sharma et al., 2020), generating empathetic dialogue responses (Lin et al., 2019; Majumder et al., 2020), and constructing empathy lexicons (Sedoc et al., 2020) or datasets (Rashkin et al., 2019). The empathetic dialogue generation task has been regarded as a unidirectional process from the context to response, and is modeled as a multi-task learning that combines the emotion understanding and the emotion-enhanced response generation. Therefore, existing work (Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020; Majumder et al., 2020) mainly focuses on improving the accuracy of emotion c"
2021.findings-emnlp.268,N19-1374,0,0.0462125,"Missing"
2021.findings-emnlp.268,W19-2302,0,0.0345175,"Missing"
2021.findings-emnlp.268,N19-1423,0,0.00833191,"consensus shared in each hc, yi pair. Because of the existence of ze , other modules are correlated and can better model both the semantic relation and the emotion connection between c and y. 3.2 Model Architecture Since the backward dialogue model has the same architecture as the forward one, we specify the components of forward dialogue model below and omit those of backward model for space limitation. Encoder. Following the work of Lin et al. (2019), we firstly concatenate utterances in c into a long sequence with length n and add a special token CTX to the beginning of c inspired by BERT (Devlin et al., 2019). Then, each token w in c is calculated as the sum of three embeddings: Ec (w) = Ew (w) + Ep (w) + Er (w), Figure 2: The architecture of Dual-Emp. It couples forward and backward dialogue models with a discrete latent variable ze denoting emotion consensus. where Ew (·), Ep (·), and Er (·) ∈ R|V |×demb represent word embedding space, positional embedding space and role embedding space1 , respectively. Finally, a transformer encoder (Vaswani et al., 2017) fenc is applied to get the context representation: H = fenc (Ec ([CTX; c])), (2) where “;” represents the concatenation operation, and H ∈ R("
2021.findings-emnlp.268,N16-1014,0,0.0809101,"Missing"
2021.findings-emnlp.268,2020.coling-main.394,0,0.456349,"processing mainly include detecting empathy in spoken language or text (Buechel et al., 2018; Sharma et al., 2020), generating empathetic dialogue responses (Lin et al., 2019; Majumder et al., 2020), and constructing empathy lexicons (Sedoc et al., 2020) or datasets (Rashkin et al., 2019). The empathetic dialogue generation task has been regarded as a unidirectional process from the context to response, and is modeled as a multi-task learning that combines the emotion understanding and the emotion-enhanced response generation. Therefore, existing work (Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020; Majumder et al., 2020) mainly focuses on improving the accuracy of emotion classification or enhancing response generation 1 Introduction via integrating the detected emotion factor. Conducting an empathetic conversation is natuEmpathy, a fundamental trait of humans, describes rally a bidirectional process: the speaker conveys the ability to place oneself in another person’s position and share his/her feelings or emotions. his/her emotion by describing a certain situation, then the listener receives that emotion and feeds Besides, it has been considered to be one of the most valuable affecti"
2021.findings-emnlp.268,D19-1012,0,0.263106,"conversations and employ Dual-Emp to produce pseudo paired empathetic samples, which is more efficient and low-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses. Figure 1: An example of conducting an empathetic conversation. Both responses show empathy to the speaker. 2005). The studies of empathy in natural language processing mainly include detecting empathy in spoken language or text (Buechel et al., 2018; Sharma et al., 2020), generating empathetic dialogue responses (Lin et al., 2019; Majumder et al., 2020), and constructing empathy lexicons (Sedoc et al., 2020) or datasets (Rashkin et al., 2019). The empathetic dialogue generation task has been regarded as a unidirectional process from the context to response, and is modeled as a multi-task learning that combines the emotion understanding and the emotion-enhanced response generation. Therefore, existing work (Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020; Majumder et al., 2020) mainly focuses on improving the accuracy of emotion classification or enhancing response generation 1 Introduction via integrating the"
2021.findings-emnlp.268,D16-1230,0,0.060481,"Missing"
2021.findings-emnlp.268,P19-1194,1,0.840006,"swering system. Both Zhang et al. (2018a) and expressing an appropriate empathy. Cui et al. (2019) used similar idea in dialogue Our main contributions can be summarized as: generation task to produce coherent but not safe (1) We point out that the empathetic dialogue responses. Shen and Feng (2020) applied DL generation contains bidirectional processes, and for emotion-controllable response generation with 3125 three awards for emotions and semantics. Some researchers also exploited DL to relieve the need of paired data and make use of unpaired data in several areas, such are style transfer (Luo et al., 2019a,b), semantic understanding (Tseng et al., 2020), stylized response generation (Zheng et al., 2020a), and machine translation (Zheng et al., 2020b). The differences between our model and previous methods are: (1) To improve the empathy understanding, we introduce a backward model to represent the response and a discrete latent variable to capture the emotion consensus shared by contexts and responses. (2) Our forward and backward models are connected by a latent variable, and both of them can be updated at each iteration, while traditional DL can only fix one to update another. 3 Proposed Met"
2021.findings-emnlp.268,2020.emnlp-main.721,0,0.0455266,"Missing"
2021.findings-emnlp.268,P02-1040,0,0.109826,"generation. To make fair comparisons, we do not apply methods based on pre-trained models here, as both Dual-Emp and the above mentioned ones are not based on pre-trained models. Note that model (1) to (5) can only utilize the paired data. 3 https://pytorch.org/ Additionally, we also design following models for ablation study: (6) Sing-Emp-Paired: A variation of Dual-Emp with only the forward model and paired empathetic data; (7) Dual-Emp-Paired: Dual-Emp with only paired empathetic data. 4.4 Evaluation Measures Automatic Metrics. For automatic evaluation, we use followings metrics: (1) BLEU (Papineni et al., 2002); (2) Embedding-based scores (Average, Greedy, and Extrema)4 (Liu et al., 2016; Serban et al., 2017); (3) Perplexity (PPL) (Vinyals and Le, 2015); (4) Dist-1/2 (Li et al., 2016); (5) Emotion accuracy (the agreement between the ground-truth emotion labels and the predicted ones from Eq. 5). Emotion accuracy can be used to measure the ability of emotion understanding. Human Evaluation. Firstly, we randomly sample 100 contexts and their corresponding responses from our model as well as the baselines. Next, we send pairs of the context and generated response from different models to three professi"
2021.findings-emnlp.268,D14-1162,0,0.0943503,"ersations are used simultaneously. Then, the total loss can be summarized as: Implementation Details We optimize the models using Adam (Kingma and Ba, 2015) with a mini-batch size of 16. The learning rate is initialized to 1e-4 and we vary the learnwhere the first term is the reconstruction of y, and ing rate following Vaswani et al. (2017). Similar to the process is symmetrical to that of L3 . Detailed Lin et al. (2019), Li et al. (2020), and Majumder derivations can be found in Appendix. The final et al. (2020), we use pre-trained GloVe vectors loss functions for unpaired data c and y are: (Pennington et al., 2014) to initialize the word emLc = L3 + βLemo , (18) beddings. Besides, all common hyper-parameters are set the same as previous work, e.g., the hidLy = L4 + γLemo , (19) den size dmod and embedding size demb are set to where β and γ are two hyper-parameters. 300. In order to alleviate the degeneration problem Total Training Loss. During training, the paired of variational framework, we apply KL annealing empathetic data in E MPATHETIC D IALOGUES and (Bowman et al., 2016) that is the same as in Zhou 3128 − DKL [q(ze |y)||p(ze )], et al. (2018). During inference, we use greedy decoding strategy and"
2021.findings-emnlp.268,2020.lrec-1.206,0,0.0189435,", which is more efficient and low-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses. Figure 1: An example of conducting an empathetic conversation. Both responses show empathy to the speaker. 2005). The studies of empathy in natural language processing mainly include detecting empathy in spoken language or text (Buechel et al., 2018; Sharma et al., 2020), generating empathetic dialogue responses (Lin et al., 2019; Majumder et al., 2020), and constructing empathy lexicons (Sedoc et al., 2020) or datasets (Rashkin et al., 2019). The empathetic dialogue generation task has been regarded as a unidirectional process from the context to response, and is modeled as a multi-task learning that combines the emotion understanding and the emotion-enhanced response generation. Therefore, existing work (Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020; Majumder et al., 2020) mainly focuses on improving the accuracy of emotion classification or enhancing response generation 1 Introduction via integrating the detected emotion factor. Conducting an empathetic conversation is natuEmpathy, a"
2021.findings-emnlp.268,2020.emnlp-main.425,0,0.0376348,"aired data, we extract unpaired emotional data from open-domain conversations and employ Dual-Emp to produce pseudo paired empathetic samples, which is more efficient and low-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses. Figure 1: An example of conducting an empathetic conversation. Both responses show empathy to the speaker. 2005). The studies of empathy in natural language processing mainly include detecting empathy in spoken language or text (Buechel et al., 2018; Sharma et al., 2020), generating empathetic dialogue responses (Lin et al., 2019; Majumder et al., 2020), and constructing empathy lexicons (Sedoc et al., 2020) or datasets (Rashkin et al., 2019). The empathetic dialogue generation task has been regarded as a unidirectional process from the context to response, and is modeled as a multi-task learning that combines the emotion understanding and the emotion-enhanced response generation. Therefore, existing work (Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020; Majumder et al., 2020) mainly focuses on improving the accuracy of emotion classification or enhan"
2021.findings-emnlp.268,2020.acl-main.52,1,0.910601,"e systems can make conversational agents more human-like and benefit the interactions between human and machine (Prendinger and Ishizuka, 2005). EmotionIn this paper, we propose a Dual-Generative controllable response generation aims to generate model for the Empathetic dialogue generation task emotional responses conditioning on a manually(Dual-Emp), which simultaneously constructs emo- provided label. Existing work (Zhou et al., 2018; tion consensus and utilizes unpaired data. Dual- Zhou and Wang, 2018; Colombo et al., 2019; Song Emp combines a forward dialogue model (generat- et al., 2019; Shen and Feng, 2020) focused on obing a response based on its context) and a backward taining responses that are not only meaningful, but dialogue model (generating a context based on its also in accordance with the desired emotion. responses) with a discrete latent variable. Specifi- Empathetic Response Generation. Rashkin et al. cally, the forward and backward encoders convert (2019) considered a richer and evenly distributed the context and response into vectors at the same set of emotions, and released a dataset E MPA time, and then a discrete latent variable is used to THETIC D IALOGUES . Shin et al. (2020)"
2021.findings-emnlp.268,P19-1359,0,0.0340493,"Missing"
2021.findings-emnlp.268,D17-1090,0,0.0255329,"process designed an emotion-focused attention mechanism is introduced to promote the semantic coherence for emotional dependencies. between contexts and responses. Furthermore, two Dual Learning in NLP. He et al. (2016) proposed types of optimization methods are applied to betDual Learning (DL) for machine translation first, ter train the entire model with paired and unpaired which considered the source to target language data. Experimental results on a benchmark dataset translation and target to source language translaE MPATHETIC D IALOGUES show that Dual-Emp tion as a dual task. After that, Tang et al. (2017) significantly outperforms competitive baselines in implemented a dual framework for the questiongenerating meaningful and related responses while answering system. Both Zhang et al. (2018a) and expressing an appropriate empathy. Cui et al. (2019) used similar idea in dialogue Our main contributions can be summarized as: generation task to produce coherent but not safe (1) We point out that the empathetic dialogue responses. Shen and Feng (2020) applied DL generation contains bidirectional processes, and for emotion-controllable response generation with 3125 three awards for emotions and seman"
2021.findings-emnlp.268,K18-1003,0,0.0174536,"elines: (1) Multi-TRS (Rashkin et al., 2019): A transformer-based model trained with emotion classification loss in addition to MLE loss, and the emotion label is classified from the encoder output; (2) MoEL (Lin et al., 2019): An extension to Multi-TRS, which softly combines the output states of the appropriate decoders and generates an empathetic response. Each decoder is optimized to focus on a specific emotion; (3) EmpDG (Li et al., 2020): A model that exploits coarse- and finegrained emotions and introduces an interactive adversarial learning framework to use user feedbacks; (4) DualVAE (Tran and Nguyen, 2018): A model with two decoders: one is for CVAE, and the other is for response auto-encoding; (5) MIME (Majumder et al., 2020): A model that integrates emotion grouping, emotion mimicry, and stochasticity strategies to generate varied responses. MIME is also the state-of-the-art model for empathetic response generation. To make fair comparisons, we do not apply methods based on pre-trained models here, as both Dual-Emp and the above mentioned ones are not based on pre-trained models. Note that model (1) to (5) can only utilize the paired data. 3 https://pytorch.org/ Additionally, we also design f"
2021.findings-emnlp.268,P19-1534,0,0.334421,"-cost than the human annotation. Automatic and human evaluations demonstrate that our method outperforms competitive baselines in producing coherent and empathetic responses. Figure 1: An example of conducting an empathetic conversation. Both responses show empathy to the speaker. 2005). The studies of empathy in natural language processing mainly include detecting empathy in spoken language or text (Buechel et al., 2018; Sharma et al., 2020), generating empathetic dialogue responses (Lin et al., 2019; Majumder et al., 2020), and constructing empathy lexicons (Sedoc et al., 2020) or datasets (Rashkin et al., 2019). The empathetic dialogue generation task has been regarded as a unidirectional process from the context to response, and is modeled as a multi-task learning that combines the emotion understanding and the emotion-enhanced response generation. Therefore, existing work (Rashkin et al., 2019; Lin et al., 2019; Li et al., 2020; Majumder et al., 2020) mainly focuses on improving the accuracy of emotion classification or enhancing response generation 1 Introduction via integrating the detected emotion factor. Conducting an empathetic conversation is natuEmpathy, a fundamental trait of humans, descr"
2021.findings-emnlp.268,2020.acl-main.163,0,0.210968,"expressing an appropriate empathy. Cui et al. (2019) used similar idea in dialogue Our main contributions can be summarized as: generation task to produce coherent but not safe (1) We point out that the empathetic dialogue responses. Shen and Feng (2020) applied DL generation contains bidirectional processes, and for emotion-controllable response generation with 3125 three awards for emotions and semantics. Some researchers also exploited DL to relieve the need of paired data and make use of unpaired data in several areas, such are style transfer (Luo et al., 2019a,b), semantic understanding (Tseng et al., 2020), stylized response generation (Zheng et al., 2020a), and machine translation (Zheng et al., 2020b). The differences between our model and previous methods are: (1) To improve the empathy understanding, we introduce a backward model to represent the response and a discrete latent variable to capture the emotion consensus shared by contexts and responses. (2) Our forward and backward models are connected by a latent variable, and both of them can be updated at each iteration, while traditional DL can only fix one to update another. 3 Proposed Method For empathetic dialogue generation, a dialogu"
2021.findings-emnlp.268,2020.wmt-1.63,0,0.0135263,") used similar idea in dialogue Our main contributions can be summarized as: generation task to produce coherent but not safe (1) We point out that the empathetic dialogue responses. Shen and Feng (2020) applied DL generation contains bidirectional processes, and for emotion-controllable response generation with 3125 three awards for emotions and semantics. Some researchers also exploited DL to relieve the need of paired data and make use of unpaired data in several areas, such are style transfer (Luo et al., 2019a,b), semantic understanding (Tseng et al., 2020), stylized response generation (Zheng et al., 2020a), and machine translation (Zheng et al., 2020b). The differences between our model and previous methods are: (1) To improve the empathy understanding, we introduce a backward model to represent the response and a discrete latent variable to capture the emotion consensus shared by contexts and responses. (2) Our forward and backward models are connected by a latent variable, and both of them can be updated at each iteration, while traditional DL can only fix one to update another. 3 Proposed Method For empathetic dialogue generation, a dialogue consists of utterances from a speaker and a list"
2021.findings-emnlp.268,P18-1104,0,0.0157988,"oherence, and empathy. 2 Related Work Emotion-Controllable Response Generation. Infusing emotions into dialogue systems can make conversational agents more human-like and benefit the interactions between human and machine (Prendinger and Ishizuka, 2005). EmotionIn this paper, we propose a Dual-Generative controllable response generation aims to generate model for the Empathetic dialogue generation task emotional responses conditioning on a manually(Dual-Emp), which simultaneously constructs emo- provided label. Existing work (Zhou et al., 2018; tion consensus and utilizes unpaired data. Dual- Zhou and Wang, 2018; Colombo et al., 2019; Song Emp combines a forward dialogue model (generat- et al., 2019; Shen and Feng, 2020) focused on obing a response based on its context) and a backward taining responses that are not only meaningful, but dialogue model (generating a context based on its also in accordance with the desired emotion. responses) with a discrete latent variable. Specifi- Empathetic Response Generation. Rashkin et al. cally, the forward and backward encoders convert (2019) considered a richer and evenly distributed the context and response into vectors at the same set of emotions, and releas"
2021.findings-emnlp.268,P18-1101,0,0.025412,"multi-head attention function taking a query matrix Q, a key matrix K, and a value matrix V as inputs. The fully connected feedforward layer is defined as: ˆ = FFN([CH ; CZ ]), Y (8) ˆ = {ˆ where Y yi }ti=1 . Finally, the decoding distribution over the vocabulary of the next token is computed as: ˆ t ), p(yt |y<t , c, ze ) = softmax(Wo y (9) optimize Dual-Emp using the paired and unpaired data at the same time. Training with Paired Data. Given hc, yi, we aim to maximize the log-likelihood of a joint probability p(c, y): X log p(c, y) = log p(c, y, ze ). (10) ze Following the derivations from Zhao et al. (2018), Zhao et al. (2019), Tseng et al. (2020), and the variational inference (Kingma and Welling, 2014), an objective based on the evidence lower bound can be derived as: L1 =Eq(ze |c) log p(y|ze , c) + Eq(ze |c) log p(c|ze , y) − DKL [q(ze |c)||p(ze )], (11) where the first term denotes the forward dialogue model, q(ze |c) is the approximate posterior distribution of ze , and is computed by the forward encoding process (red ¬ in Figure 3(c)). p(y|ze , c) is the forward decoding process (green  in Figure 3(c)); the second term denotes the reconstruction of c, and p(c|ze , y) is the backward decod"
2021.findings-emnlp.268,N19-1123,0,0.0902307,"|×demb represent word embedding space, positional embedding space and role embedding space1 , respectively. Finally, a transformer encoder (Vaswani et al., 2017) fenc is applied to get the context representation: H = fenc (Ec ([CTX; c])), (2) where “;” represents the concatenation operation, and H ∈ R(n+1)×dmod . The contextualized encoding of CTX, i.e., H0 ∈ Rdmod , is used as the final representation of the entire context. Emotion Consensus Construction. A K-way categorical latent variable ze ∈ [1, K] (Bao et al., 2020) is used to capture the emotion consensus shared by c and y. Inspired by Zhao et al. (2019), we define the prior distribution where we sample ze from to be uniform2 , i.e., p(ze ) = 1/K. Correspondingly, the approximate posterior distribution is defined as follows: q(ze |c) = softmax(FFN(H0 )) ∈ RK , (3) where FFN(·) represents a feedforward network. This part can be considered as the emotion understanding on c. Here ze has its own embedding space Ez ∈ RK×dmod to convert it into a vector, i.e., E[z] = Ez (ze ) ∈ Rdmod . To supervise the emotion expression in E[z] , we train a classifier using the cross-entropy loss between E[z] and groundtruth emotion label e∗ : pe = softmax(We E[z]"
2021.findings-emnlp.43,2020.findings-emnlp.372,0,0.433542,"the hypothesis from the MNLI dataset. The classifiers in shallow layers of a dynamic early exiting model cannot predict correctly, while BERT-Complete (Turc et al., 2019), a small BERT pre-trained from scratch with the same size can make a correct and confident prediction. which can be categorized into model-level compression and instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeB"
2021.findings-emnlp.43,2020.acl-main.703,0,0.0295578,"Missing"
2021.findings-emnlp.43,D16-1264,0,0.0150867,"idence-based emitting decisions more reliable. 4 Experiments Dataset MNLI MRPC QNLI QQP RTE SST-2 # Train # Dev # Test Metric  393k 3.7k 105k 364k 2.5k 67k 20k 0.4k 5.5k 40k 0.3k 0.9k 20k 1.7k 5.5k 391k 3k 1.8k Accuracy F1-score Accuracy F1-score Accuracy Accuracy 0.3 0.5 0.3 0.3 0.5 0.5 Table 1: Statistics of six classification datasets in GLUE benchmark. The selected difficulty margins  of each datasets are provided in the last column. 4.1 Experimental Settings We use six classification tasks in GLUE benchmark, including MNLI (Williams et al., 2018), MRPC (Dolan and Brockett, 2005), QNLI (Rajpurkar et al., 2016), QQP,2 RTE (Bentivogli et al., 2009) and SST-2 (Socher et al., 2013). The metrics for evaluation are F1-score for QQP and MRPC, and accuracy for the rest tasks. Our implementation is based on the Huggingface Transformers library (Wolf et al., 2020). We use two models for selection with 2 and 12 layers, respectively, since they can provide a wide range for acceleration. The difficulty score is thus evaluated based on the 2-layer model. The effect of incorporating more models in our cascade framework is explored in the later section. We utilize the weights provided by Turc et al. (2019) to init"
2021.findings-emnlp.43,2020.acl-main.593,0,0.238822,"aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is poor when most examples are exited in early"
2021.findings-emnlp.43,2020.sustainlp-1.11,0,0.3606,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,2020.acl-main.204,0,0.31551,"d instance-level speed-up. The former aims at obtaining a compact model via quantization (Zafrir et al., 2019; Shen et al., 2020; Zhang et al., 2020), pruning (Voita et al., 2019; Michel et al., 2019) or knowledge distillation (KD) (Sanh et al., 2019; Sun et al., 2019; Jiao et al., 2020), while the latter adapts the amount of computation to the complexity of each instance (Graves, 2016). A mainstream method for instance-level speed-up is dynamic early exiting, which emits predictions based on intermediate classifiers (or off-ramps) of internal layers when the predictions are confident enough (Xin et al., 2020b; Liu et al., 2020; Schwartz et al., 2020; Li et al., 2021). In this paper, we focus on dynamic early exiting, as it can be utilized to accelerate inference 1 and reduce the potential risk of the overthinking Our code is available at https://github.com/ lancopku/CascadeBERT problem (Kaya et al., 2019). Such a paradigm is 475 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486 November 7–11, 2021. ©2021 Association for Computational Linguistics intuitive and simple, while faces a performance bottleneck under high speed-up ratios, i.e., the task performance is p"
2021.findings-emnlp.43,D13-1170,0,0.0217053,"in the DeeBERT of shallow layers is lower than that of BERT-kl and BERT-Complete, which leads to more wrongly emitted instances. The exiting decisions in shallow layers of DeeBERT thus can be unreliable. 2.2 modeling (MLM) objective. We assume the representations of this model contain high-level semantic information, as MLM requires a deep understanding of the language. For a fair comparison, models are evaluated on a subset of instances which DeeBERT chooses to emit at different layers. We report prediction accuracy using different number of layers on MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013). Figure 2 shows the results on the development sets, and we can see that: (1) BERT-Complete clearly outperforms DeeBERT, especially when the predictions are made based on shallow layers. It indicates that the highlevel semantics is vital for handling tasks like sentence-level classification. (2) BERT-kL also outperforms DeeBERT. We attribute it to that the last serveral layers can learn task-specific information during fine-tuning to obtain a decent performance. A similar phenomenon is also observed by Merchant et al. (2020). However, since the internal layer representation in DeeBERT are res"
2021.findings-emnlp.43,2020.emnlp-main.633,0,0.22672,"Missing"
2021.naacl-main.126,D17-1134,0,0.0316569,"Missing"
2021.naacl-main.126,D16-1130,0,0.0155853,"he H-LSTM suffers from Implicit discourse relation recognition (IDRR) the long-distance forgetting problem, which may aims to identify logical relations between two ad- fail to model the long-distance and non-continuous dependency across multiple sentences (like green jacent sentences in discourse without the guidance lines in Figure 1). of connectives (e.g., because, but), which is one of the major challenges in discourse parsing. With To overcome these limitations, we propose a the rise of deep learning, lots of sentence-modeling novel Context Tracking Network (CT-Net), which based methods (Liu and Li, 2016; Rönnqvist et al., can track essential context for each sentence from 2017; Bai and Zhao, 2018; Xu et al., 2019; Shi the intricate discourse, without being affected by and Demberg, 2019) have emerged in the field of the spatial distance. The CT-Net computes contexIDRR. These methods typically focus on modeling tual representation through two main steps. Firstly, the local semantics of these two sentences, without it converts the paragraph into the paragraph assoconsidering wider discourse context. ciation graph (PAG) (Figure 1), which contains Contextual information plays an important role th"
2021.naacl-main.126,P19-1411,0,0.0228543,"Missing"
2021.naacl-main.126,D14-1162,0,0.0879083,"ained updating mechanism, which is executed T rounds. At the t-th round, we denote the state of the i-th sentence node as git , and the state of the j-th token node of the i-th sentence as hti,j . The states transition from the (t-1)-th to the t-th round consists of three computation processes: token-tosentence updating, sentence-to-sentence updating and sentence-to-token updating. The first two processes are responsible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside"
2021.naacl-main.126,N18-1202,0,0.0292719,"executed T rounds. At the t-th round, we denote the state of the i-th sentence node as git , and the state of the j-th token node of the i-th sentence as hti,j . The states transition from the (t-1)-th to the t-th round consists of three computation processes: token-tosentence updating, sentence-to-sentence updating and sentence-to-token updating. The first two processes are responsible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside the dotted ellipse in Figure 2)"
2021.naacl-main.126,P09-1077,0,0.0256947,"Missing"
2021.naacl-main.126,prasad-etal-2008-penn,0,0.00847306,"connective prediction (CP). These three tasks share the same encoder but use three different MLPs. The objective function is as follows: L=−α r∈R k∈Ni 1594 CX idrr j yidrr j log ybidrr j=1 −γ Ccp X −β CX edrr j j yedrr log ybedrr j=1 j j ycp log ybcp j=1 (6) where α, β, γ are adjustable hyper-parameters. yidrr , yedrr and ycp are ground-truth labels of IDRR, EDRR and CP respectively, while ybidrr , ybedrr and ybcp are corresponding predictions. Cidrr , Cedrr and Ccp represent the number of classes of IDRR, EDRR, and CP respectively. 3 3.1 Experiment Dataset We conduct experiments on PDTB 2.0 (Prasad et al., 2008), which contains 16, 224 implicit instances and 18, 459 explicit instances. We perform one-vs-others binary classification and 4-way classification on 4 top-level discourse relations: comparison (Comp.), contingency (Cont.), expansion (Exp.), and temporal (Temp.). Following Pitler et al. (2009), we use sections 2-20 for training, sections 21-22 for test and sections 0-1 for validation. The metric is F1 score, and for 4-way classification, we calculate the macro-average F1 score. 3.2 Implementation Details Details of the PAG. We set the number of sentences to build PAGs as 6, and use zero paddi"
2021.naacl-main.126,P17-1093,0,0.0416331,"Missing"
2021.naacl-main.126,P17-2040,0,0.0341838,"Missing"
2021.naacl-main.126,D19-1586,0,0.491369,"Missing"
2021.naacl-main.126,P19-1058,0,0.013328,"may aims to identify logical relations between two ad- fail to model the long-distance and non-continuous dependency across multiple sentences (like green jacent sentences in discourse without the guidance lines in Figure 1). of connectives (e.g., because, but), which is one of the major challenges in discourse parsing. With To overcome these limitations, we propose a the rise of deep learning, lots of sentence-modeling novel Context Tracking Network (CT-Net), which based methods (Liu and Li, 2016; Rönnqvist et al., can track essential context for each sentence from 2017; Bai and Zhao, 2018; Xu et al., 2019; Shi the intricate discourse, without being affected by and Demberg, 2019) have emerged in the field of the spatial distance. The CT-Net computes contexIDRR. These methods typically focus on modeling tual representation through two main steps. Firstly, the local semantics of these two sentences, without it converts the paragraph into the paragraph assoconsidering wider discourse context. ciation graph (PAG) (Figure 1), which contains Contextual information plays an important role three types of edges between sentences, namely (1) in understanding sentences. Take the paragraph adjacency edge ("
2021.naacl-main.126,P18-1030,0,0.0153513,"sible for updating sentence nodes, while the last one is for updating token nodes. Node Initialization. When t = 0, we initialize token nodes with the concatenation of char, GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) embeddings. And the dimension is reduced: glove elmo h0i,j = xi,j = W [xchar i,j ; xi,j ; xi,j ] + b (1) where W , b are parameters. The sentence node gi0 is initialized as the average of its token nodes. Token-to-Sentence Updating. This process updates the sentence state git with the token states of last round ht−1 i,j . We employ Sentence-state LSTM (SLSTM) (Zhang et al., 2018) to achieve this. SLSTM is a novel graph RNN that converts a sentence into a graph with one global sentence node and several local word nodes, just like the sub-graph in the PAG (inside the dotted ellipse in Figure 2). At the t-th round, the hidden state of i-th sentence git is computed as follows: t−1 t−1 t−1 git = SLSTMh→g (ht−1 i,0 , hi,1 ..., hi,|Si |, gi ) (2) where SLSTMh→g represents the process of updating the sentence state with token states by SLSTM, and its detailed equations are shown in Appendix A. |Si |is the number of tokens in Si . Sentence-to-Sentence Updating. After merging t"
D19-1097,D17-1146,0,0.0153586,"ories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. 7 6 Related Work Memory Network Memory network is a general machine learning framework introduced by Weston et al. (2014), which have been shown effective in question answering (Weston et al., 2014; Sukhbaatar et al., 2015), machine translation (Wang et al., 2016a; Feng et al., 2017), aspect level sentiment classification (Tang et al., 2016), etc. For spoken language understanding, Chen et al. (2016) introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the seConclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Ex"
D19-1097,N18-2118,0,0.212802,"es 1051–1060, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics # 1 2 3 4 Utterance play Roy Orbison tunes now add this Roy Orbison song onto Women of Comedy book a spot for seven at a bar with chicken french book french food for me and angeline at a restaurant Slot tag artist artist served dish cuisine Intent PlayMusic AddToPlaylist BookRestaurant BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot fill"
D19-1097,W19-5906,0,0.023433,"interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/CMNet. for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models (Haffner et al., 2003; Sarikaya et al., 2011) for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs (Xu and Sarikaya, 2013; Gupta et al., 2019), RNNs (Guo et al., 2014a; Liu and Lane, 2016), and asynchronous bi-model (Wang et al., 2018). Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure 1 and Table 1. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore"
D19-1097,W03-0426,0,0.137692,"procedures, the hidden state is updated with abundant information from different perspectives, namely word embeddings, local contexts, slots and intents representations. The local calculation layer in each CMblock has been shown highly useful for both tasks, and especially for the slot filling task, which will be validated in our experiments in Section 5.2. Global Recurrence Bi-directional RNNs, especially the BiLSTMs (Hochreiter and Schmidhuber, 1997) are regarded to encode both past and future information of a sentence, which have become a dominant method in various sequence modeling tasks (Hammerton, 2003; Sundermeyer et al., 2012). The inherent nature of BiLSTMs is able to supplement global sequential information, which is insufficiently modeled in the previous local calculation layer. Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By taking the slot- and intent-specific local context representations as inputs, we can obtain more specific global sequential representations. Formally, it takes the hidden state hl−1 inherited from the t local calculation layer as input, and conduct recurrent steps as follows: → − ← − hlt = [hlt ; ht l ] → − → −l → −"
D19-1097,H90-1021,0,0.123954,"Missing"
D19-1097,P82-1020,0,0.842662,"Missing"
D19-1097,D16-1223,0,0.0299513,"lot and intent feat int tures Ht are utilized to provide guidances for the next local calculation layer. Bi-LSTM Global Recurrence Output states Local Calculation Hidden states ··· ht-1 ht ht+1 ··· Embeddings ··· xt-1 xt xt+1 ··· Slot features ··· ht-1slot htslot ht+1slot ··· Intent features ··· ht-1int htint ht+1int ··· Deliberate Attention Slot Memory Intent Memory Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively. Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the Sand intentLSTM with slot-specific features Hslot t retrieved from memories. specific features Hslot t Specifically, at each input position t, we take the local window context ξt , word embedding xt , slot feature hslot and intent feature hint as inputs to t t conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden"
D19-1097,N16-1030,0,0.0277893,"nference Layer. 3.2 The most probable intent label yˆint is predicted by softmax normalization over the intent label set: P (e y = j|v Intent Memory (4) i=1 ye∈S int int x1 3.1 As to the prediction of intent, the word-level hidden states H are firstly summarized into a utterance-level representation vint via mean pooling (or max pooling or self-attention, etc.): vint = slots Embedding Layer where A is the transition matrix that Ai,j indicates the score of a transition from i to j, and P is the score matrix output by RNNs. Pi,j indicates the score of the j th tag of the ith word in a sentence (Lample et al., 2016). When testing, the Viterbi algorithm (Forney, 1973) is used to search the sequence of slot tags with maximum score: e slot ∈Yx y yN (1) i=1 ˆ slot = arg max F (H, y e slot ) y ··· ··· Here Yx is the set of all possible sequences of tags, and F (·) is the score function calculated by: N X yt Inference Layer slot slot ··· (6) Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove2 (Pennington et al., 2014) to initialize word embeddings, and keep th"
D19-1097,D18-1417,0,0.165367,"ng Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics # 1 2 3 4 Utterance play Roy Orbison tunes now add this Roy Orbison song onto Women of Comedy book a spot for seven at a bar with chicken french book french food for me and angeline at a restaurant Slot tag artist artist served dish cuisine Intent PlayMusic AddToPlaylist BookRestaurant BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling (Mesnil et al."
D19-1097,D14-1162,0,0.0905085,"of the j th tag of the ith word in a sentence (Lample et al., 2016). When testing, the Viterbi algorithm (Forney, 1973) is used to search the sequence of slot tags with maximum score: e slot ∈Yx y yN (1) i=1 ˆ slot = arg max F (H, y e slot ) y ··· ··· Here Yx is the set of all possible sequences of tags, and F (·) is the score function calculated by: N X yt Inference Layer slot slot ··· (6) Embedding Layers Pre-trained Word Embedding The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove2 (Pennington et al., 2014) to initialize word embeddings, and keep them frozen. Character-aware Word Embedding It has been demonstrated that character level information (e.g. capitalization and prefix) (Collobert et al., 2011) is crucial for sequence labeling. We use one layer of CNN followed by max pooling to generate character-aware word embeddings. 3.3 CM-block The CM-block is the core module of our CM-Net, which is designed with three computational com2 j=1 i=1 1053 https://nlp.stanford.edu/projects/glove/ ponents: Deliberate Attention, Local Calculation and Global Recurrence respectively. Deliberate Attention To f"
D19-1097,N18-1202,0,0.122653,"Missing"
D19-1097,W09-1119,0,0.0153174,"Zhang et al., 2018a). CAIS We collect utterances from the Chinese Artificial Intelligence Speakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme (Ratinov and Roth, 2009) in the sequence labeling field. Metrics Slot filling is typically treated as a sequence labeling problem, and thus we take the conlleval 4 as the token-level F1 metric. The intent detection is evaluated with the classification accuracy. Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works (Tur et al., 2010; Zhang and Wang, 2016), we count an utterrance as a correct classification if any ground truth label is predicted. 4 https://www.clips.uantwerpen.be/conll2000/chunking/ conlleval.txt 4.2 Implementation Details All trainable parameters in o"
D19-1097,D16-1021,0,0.0230266,"eled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments. 7 6 Related Work Memory Network Memory network is a general machine learning framework introduced by Weston et al. (2014), which have been shown effective in question answering (Weston et al., 2014; Sukhbaatar et al., 2015), machine translation (Wang et al., 2016a; Feng et al., 2017), aspect level sentiment classification (Tang et al., 2016), etc. For spoken language understanding, Chen et al. (2016) introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the seConclusion We propose a novel Collaborative Memory Network (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Experiments on two standard benchmarks and our CAIS corpus de"
D19-1097,D16-1027,0,0.180193,"nt tures Ht are utilized to provide guidances for the next local calculation layer. Bi-LSTM Global Recurrence Output states Local Calculation Hidden states ··· ht-1 ht ht+1 ··· Embeddings ··· xt-1 xt xt+1 ··· Slot features ··· ht-1slot htslot ht+1slot ··· Intent features ··· ht-1int htint ht+1int ··· Deliberate Attention Slot Memory Intent Memory Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively. Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the Sand intentLSTM with slot-specific features Hslot t retrieved from memories. specific features Hslot t Specifically, at each input position t, we take the local window context ξt , word embedding xt , slot feature hslot and intent feature hint as inputs to t t conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden state ht is updated"
D19-1097,C16-1229,0,0.0929106,"nt tures Ht are utilized to provide guidances for the next local calculation layer. Bi-LSTM Global Recurrence Output states Local Calculation Hidden states ··· ht-1 ht ht+1 ··· Embeddings ··· xt-1 xt xt+1 ··· Slot features ··· ht-1slot htslot ht+1slot ··· Intent features ··· ht-1int htint ht+1int ··· Deliberate Attention Slot Memory Intent Memory Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively. Local Calculation Local context information is highly useful for sequence modeling (Kurata et al., 2016; Wang et al., 2016b). Zhang et al. (2018b) propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the Sand intentLSTM with slot-specific features Hslot t retrieved from memories. specific features Hslot t Specifically, at each input position t, we take the local window context ξt , word embedding xt , slot feature hslot and intent feature hint as inputs to t t conduct combinatorial calculation simultaneously. Formally, in the lth layer, the hidden state ht is updated"
D19-1097,N18-2050,0,0.354129,"sponding author of the paper. 1 Code is available at: https://github.com/Adaxry/CMNet. for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models (Haffner et al., 2003; Sarikaya et al., 2011) for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs (Xu and Sarikaya, 2013; Gupta et al., 2019), RNNs (Guo et al., 2014a; Liu and Lane, 2016), and asynchronous bi-model (Wang et al., 2018). Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure 1 and Table 1. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task 1051 Proceedings of the 2019 Conference on Empirical Methods"
D19-1097,P18-1030,0,0.113748,"rbison song onto Women of Comedy book a spot for seven at a bar with chicken french book french food for me and angeline at a restaurant Slot tag artist artist served dish cuisine Intent PlayMusic AddToPlaylist BookRestaurant BookRestaurant Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words. unidirectionally with the guidance from intent representations via gating mechanisms (Goo et al., 2018; Li et al., 2018), while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms (Zhang et al., 2018a) is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling (Mesnil et al., 2014), is not explicitly modeled. In this paper, we try to address these issues, and thus propose a novel Collaborative Memory Network, named CM"
D19-1164,2012.eamt-1.60,0,0.059146,"X, Y ) = 1 |D| j × J X I X {log P (yij |, y j<i , xj , D <j ; Θ) j=1 i=1 + PCCs(Capsenc (xj ), Capsdec (y j ))} (11) where Θ are parameters of the model, D <j are historical sentences of the to-be-translated source sentence, xj is the to-be-translated sentence and y j<i denotes the generated target hypothesis. 4 Experiments 4.1 Settings Datasets and Evaluation Metrics We carry out experiments on English-German translation tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table 1. • TED. This corpus is a Machine Translation part of the IWSLT 2017 (Cettolo et al., 2012) evaluation compaigns1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. • News. We take the sentence-aligned document-delimited News Commentary v11 corpus2 as our training set. The WMT’16 news-test2015 and news-test2016 are used for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above ext"
D19-1164,2010.iwslt-papers.10,0,0.0797008,"Missing"
D19-1164,D12-1108,0,0.0219715,"ords as inputs of the QCN and 4 duplicated query. Blue color denotes positive correlation and red means negative. From top to bottom are four heat maps in 0th to 3th iterations. value. See Figure 7, we can find that PCCs varies as iteration changes. 5 Related Work Document-level Machine Translation Document-level machine translation became a hot research direction in the later stage of statistical machine translation era. Hardmeier and Federico (2010) represented the links between word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns. Hardmeier et al. (2012, 2013) first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks. Xiong et al. (2019) trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models’ input or output. Jean et al. (2017) used additional co"
D19-1164,P13-4033,0,0.11176,"Missing"
D19-1164,2005.mtsummit-papers.11,0,0.131344,"tasks in three different domains: talks, news, and speeches. The corpora statistics are shown in Table 1. • TED. This corpus is a Machine Translation part of the IWSLT 2017 (Cettolo et al., 2012) evaluation compaigns1 , each TED talk is considered to be a document. we take the tst2016-207 as the test set, and other as our development set. • News. We take the sentence-aligned document-delimited News Commentary v11 corpus2 as our training set. The WMT’16 news-test2015 and news-test2016 are used for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge opera"
D19-1164,P07-2045,0,0.00823295,"ed for development and testing respectively. • Europarl. The corpus are extracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and"
D19-1164,W07-0734,0,0.0738311,"training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 for context-agnostic model and 0.2 for"
D19-1164,P18-1118,0,0.829638,"slation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentenc"
D19-1164,N19-1313,0,0.76905,"have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is, it utilizes a wordle"
D19-1164,W13-3303,0,0.0224921,"1 Introduction The encoder-decoder based Neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation ("
D19-1164,D18-1325,0,0.678009,"skever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the"
D19-1164,P02-1040,0,0.104751,"d in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 attention heads in both encoder and decoder. All dropout rates are set to 0.1 fo"
D19-1164,P16-1162,0,0.0824309,"xtracted from the Europarl v7 (Koehn, 2005) according to the method mentioned in Maruf and Haffari (2018). The training, development and test sets are obtained through randomly splitting the corpus. We download all of above extracted corpora3 from Maruf et al. (2019). The tokenization and 1 https://wit3.fbk.eu/ http://www.casmacat.eu/corpus/news-commentary.html 3 https://github.com/sameenmaruf/selectiveattn/tree/master/data 2 truecase pre-processing are implemented on all datasets using the scripts of the Moses Toolkit4 (Koehn et al., 2007). We also apply segmentation into BPE subword units5 (Sennrich et al., 2016) with 30K merge operations. We use two metrics: BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) to evaluate the translation quality. Models and Baselines We use the Transformer architecture as our context-agnostic baseline and adopt three contextaware baselines (Zhang et al., 2018a; Miculicich et al., 2018; Maruf and Haffari, 2018). We performed the same configuration on our models according to the settings of the Maruf and Haffari (2018). Specifically, for the Transformer, we set the hidden size and point-wise FFN size as 512 and 2048 respectively. We use 4 layers and 8 atte"
D19-1164,W17-4814,0,0.0550498,"ecoder based Neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al."
D19-1164,W17-4811,0,0.135797,"ween word pairs in the context using a word dependency model for SMT to improve the translation of anaphoric pronouns. Hardmeier et al. (2012, 2013) first proposed a new document-level SMT paradigm that translates whole documents as units. However, in this period, most of the work has not achieved too many compelling results or has been only focused on a part of difficulties. With the coming of the era of Neural Machine Translation, many works began to focus on Document-level NMT tasks. Xiong et al. (2019) trained a reward teacher to refine the translation quality from a document perspective. Tiedemann and Scherrer (2017) simply concatenated sentences in one document as models’ input or output. Jean et al. (2017) used additional context encoder to capture larger-context information. Kuang et al. (2017); Tu et al. (2018) used a cache to memorize most relevant words or features in previous sentences or translations. Recently, several studies integrated additional modules into the Transformer-based NMTs for modeling contextual information. (Voita et al., 2018; Zhang et al., 2018a), Maruf and Haffari (2018) proposed a document-level NMT using a 1534 memory-networks, and Wang et al. (2017) and Miculicich et al. (20"
D19-1164,Q18-1029,0,0.259966,"ang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current generation. That is,"
D19-1164,P18-1117,0,0.312421,"et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved"
D19-1164,D17-1301,0,0.328004,"ni et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical attentions retrieved by the current gen"
D19-1164,D18-1486,0,0.0519226,"necessary to distinguish the role of each context word and model their relationship especially when one context word could take on multiple roles (Zhang et al., 2018b). However, this is difficult to realize for the HAN model as its final representation for the context is produced with an isolated relevance with the query word which ignores relations with other context words. To address the problem, we introduce Capsule Networks into document-level translation which have proven good at modelling the parts-wholes relations between low-level capsules and highlevel capsules (Hinton et al., 2011; Xiao et al., 2018; Sabour et al., 2017; Hinton et al., 2018; Gu and Feng, 2019). With capsule networks, the words in a context source sentence is taken as lowlevel capsules and the information of different perspectives is treated as high-level capsules. Then in the dynamic routing process of capsule networks, all the low-level capsules trade off against each other and consider over all the high-level capsules and drop themselves at a proper proportion to the high-level capsules. In this way, the relation among low-level capsules and that between low1527 Proceedings of the 2019 Conference on Empirical Methods i"
D19-1164,D18-1350,0,0.031748,"slty divided documentlevel translation tasks into two types: offline and online. Capsule Networks Hinton et al. (2011) proposed the capsule conception to use vector for describing the pose of an object. The dynamic routing algorithm was proposed by Sabour et al. (2017) to build the partwhole relationship through the iterative routing procedure. Hinton et al. (2018) designed a new routing style based on the EM algorithm. Some researchers investigated to apply the capsule network for various tasks. Wang et al. (2018) investigated a novel capsule network with dynamic routing for linear time NMT. Yang et al. (2018) explored capsule networks for text classification with strategies to stabilize the dynamic routing process. Gu and Feng (2019) introduces capsule networks into Transformer to model the relations between different heads in multi-head attention. We specifically investigated dynamic routing algorithms for the document-level NMT. 6 Conclusion We have proposed a novel Query-guided Capsule Network with an improved dynamic routing algorithm for enhancing context modeling for the document-level Neural Machine Translation Model. Experiments on English-German in different domains showed our model signi"
D19-1164,D18-1049,0,0.652933,"anau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical"
D19-1164,P19-1426,1,0.873378,"Missing"
D19-1164,C18-1110,1,0.888176,"anau et al., 2014; Wu et al., 2016; Vaswani et al., 2017; Zhang et al., 2019) have made great progresses and drawn much attention in recent years. In practical applications, NMT systems are often fed with a document-level input which requires reference resolution, the consistency of tenses and noun expressions and so on. Many researchers have proven that contextual information is essential to generate coherent and consistent translation for document-level translation (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017; Jean et al., 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Zhang et al., 2018a; Voita et al., 2018; Wang et al., 2017; Tu et al., 2018; Maruf et al., 2019). Despite the great success of the above models, they are designed for sentence-level translaJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author tion tasks and exclude contextual information in the model architecture. On these grounds, the widely used hierarchical attention networks (HAN) was proposed to integrate contextual information in document-level translation (Miculicich et al., 2018). In this method, the context sentences are considered in the form of hierarchical"
D19-1251,P16-1223,0,0.0307514,"atasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC sys"
D19-1251,P17-1055,0,0.0155433,"eading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizin"
D19-1251,N19-1423,0,0.0237718,"a single function v 0 = Reasoning-Step(G, v). (13) As the graph G constructed in Sec. 3.2 has encoded the numerical relations via its topology, the reasoning process is numerically-aware. 2478 Multi-step Reasoning By single-step reasoning, we can only infer relations between adjacent nodes. However, relations between multiple nodes may be required for certain tasks, e.g., sorting. Therefore, it is essential to perform multi-step reasoning, which can be done as follows v t = Reasoning-Step(v t−1 ), (14) where t ≥ 1. Suppose we perform K steps of reasoning, v K is used as U in Eq. 7. 4 • BERT (Devlin et al., 2019), a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently; and numerical MRC models: • NAQANet (Dua et al., 2019), a numerical version of QANet model. • NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. “2.5”), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the supplemental material. Experiments 4.1 Dataset and Evaluation Metrics We evaluate our proposed model on"
D19-1251,P17-1168,0,0.0238174,"hension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning."
D19-1251,N19-1246,0,0.113245,"Missing"
D19-1251,D14-1058,0,0.0369168,"merical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical"
D19-1251,P16-1084,0,0.0149131,"ng such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages in MRC are mostly real-world texts which require more complex skills to be understood. Above all, it is nontrivial to adapt most existing AWP models to the MRC scenario. Therefore, we focus on enhancing MRC models with numerical reasoning abilities in this work. 3 Encoding Module Without loss of generality, we use the encoding components of QANet and NAQANet to encode the question and passage into vector-space"
D19-1251,P17-1147,0,0.0312321,"orming numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to"
D19-1251,P16-1086,0,0.0241577,"ble at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact"
D19-1251,D18-1546,0,0.019043,"based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most existing MRC models are focusing on dealing with the first three types of questions. However, all these models suffer from problems when answering the questions requiring numerical reasoning. To the best of our knowledge, our work is the first one that explicitly incorporates numerical reasoning into the MRC system. The most relevant work to ours is NAQANet (Du"
D19-1251,Q15-1042,0,0.0228607,"dition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our mo"
D19-1251,D17-1160,0,0.0317798,"over numbers in the passages. There are 77, 409 training samples, 9, 536 development samples and 9, 622 testing samples in the dataset. In this paper, we adopt two metrics including Exact Match (EM) and numerically-focused F1 scores to evaluate our model following Dua et al. (2019). The numerically-focused F1 is set to be 0 when the predicted answer is mismatched for those questions with the numeric golden answer. 4.2 Baselines For comparison, we select several public models as baselines including semantic parsing models: • Syn Dep (Dua et al., 2019), the neural semantic parsing model (KDG) (Krishnamurthy et al., 2017) with Stanford dependencies based sentence representations; • OpenIE (Dua et al., 2019), KDG with open information extraction based sentence representations; • SRL (Dua et al., 2019), KDG with semantic role labeling based sentence representations; and traditional MRC models: • BiDAF (Seo et al., 2017), an MRC model which utilizes a bi-directional attention flow network to encode the question and passage; • QANet (Yu et al., 2018), which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage; 4.3 Experimental Settings In this paper, we"
D19-1251,D17-1082,0,0.0239897,"ne methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to"
D19-1251,P17-1015,0,0.0224498,"umerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module leverages a numerically-aware graph neural network to perform numerical reasoning on the graph. As numerical comparison is modeled explicitly in our model, it is more effective for answering questions requiring numerical reasoning such as addition, counting, or sorting over numbers. formalized the AWP problem as that of generating and scoring equation trees via integer linear programming. Wang et al. (2017b) and Ling et al. (2017) proposed sequence to sequence solvers for the AWP problems, which are capable of generating unseen expressions and do not rely on sophisticated manual features. Wang et al. (2018) leveraged deep Q-network to solve the AWP problems, achieving a good balance between effectiveness and efficiency. However, all the existing AWP systems are only trained and validated on small benchmark datasets. Huang et al. (2016) found that the performance of these AWP systems sharply degrades on larger datasets. Moreover, from the perspective of NLP, MRC problems are more challenging than AWP since the passages"
D19-1251,D16-1264,0,0.0977847,"ement as compared to all baseline methods by explicitly performing numerical reasoning over numbers in the question and passage. In particular, we show that our model could effectively deal with questions requiring sorting with multi-layer NumGNN. The source code of our paper is available at https://github.com/ ranqiu92/NumNet. 2 2.1 Related Work Machine Reading Comprehension Machine reading comprehension (MRC) has become an important research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC mode"
D19-1251,D15-1202,0,0.0530747,"t (Dua et al., 2019), which adapts the output layer of QANet (Yu et al., 2018) to support predicting answers based on counting and addition/subtraction over numbers. However, it does not consider numerical reasoning explicitly during encoding or inference. 2.2 Arithmetic Word Problem Solving Recently, understanding and solving arithmetic word problems (AWP) has attracted the growing interest of NLP researchers. Hosseini et al. (2014) proposed a simple method to address arithmetic word problems, but mostly focusing on subsets of problems which only require addition and subtraction. After that, Roy and Roth (2015) proposed an algorithmic approach which could handle arithmetic word problems with multiple steps and operations. Koncel-Kedziorski et al. (2015) further 2475 5 5 5 &gt; 2 6 2 6 2 2 6 Answer &gt; Prediction Module ≤ 6 2 2 Passage Encoding Module Question 5 ≤ 6 6 Reasoning Module Figure 1: The framework of our NumNet model. Our model consists of an encoding module, a reasoning module and a prediction module. The numerical relations between numbers are encoded with the topology of the graph. For example, the edge pointing from “6” to “5” denotes “6” is greater than “5”. And the reasoning module levera"
D19-1251,P17-1018,0,0.171003,"portant research area in NLP. In recent years, researchers have published a large number of annotated MRC datasets such as CNN/Daily Mail (Hermann et al., 2015), SQuAD (Rajpurkar et al., 2016), RACE (Lai et al., 2017), TriviaQA (Joshi et al., 2017) and so on. With the blooming of available large-scale MRC datasets, a great number of neural network-based MRC models have been proposed to answer questions for a given document including Attentive Reader (Kadlec et al., 2016), BiDAF (Seo et al., 2017), Interactive AoA Reader (Cui et al., 2017), Gated Attention Reader (Dhingra et al., 2017), R-Net (Wang et al., 2017a), DCN (Xiong et al., 2017), QANet (Yu et al., 2018), and achieve promising results in most existing public MRC datasets. Despite the success of neural network-based MRC models, researchers began to analyze the data and rethink to what extent we have solved the problem of MRC. Some works (Chen et al., 2016; Sugawara et al., 2018; Kaushik and Lipton, 2018) classify the reasoning skills required to answer the questions into the following types: (1) Exact matching/Paraphrasing; (2) Summary; (3) Logic reasoning; (4) Utilizing external knowledge; (5) Numerical reasoning. They found that most exist"
D19-1251,D17-1088,0,0.0586902,"Missing"
D19-1584,P13-1008,0,0.810312,"al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (C"
D19-1584,C10-1077,0,0.409617,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P17-1038,0,0.138572,") rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pr"
D19-1584,P10-1081,0,0.697552,"is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et"
D19-1584,P15-1017,0,0.611756,"t al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same supero"
D19-1584,D15-1166,0,0.0125031,"tention score for each hidden embedding to model its correlation with the specific superordinate concept. As an argument role can belong to more than one superordinate concept, we set a logic union module to combine the scores from different superordinate modules together. For each argument role, we hierarchically compose its superordinate concept modules into the integrated hierarchical modular attention component to build its role-oriented embedding. Superordinate Concept Module For a specific superordinate concept c, we represent its semantic features with a trainable vector uc . Following Luong et al. (2015), we adopt a multi-layer perceptron to calculate the attention scores. We first calculate the hidden state, hci = tanh(Wa [hi ; uc ]). (3) Then, we apply a softmax operation to get the attention score for the hidden embedding hi , exp(Wb hci ) sci = Pn c , j=1 exp(Wb hj ) (4) where Wa and Wb are trainable matrices shared among different superordinate concept modules. Logic Union Module Given an argument role r ∈ R, we denote its k superordinate concepts as c1 , c2 , . . . , ck , and the corresponding attention n X sri hi . (6) i=1 2.3 Argument Role Classifier We concatenate the instance embedd"
D19-1584,N18-1076,0,0.0621074,"ller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguy"
D19-1584,N19-1423,0,0.0182971,"der We denote an instance as an n-word sequence x = {w1 , . . . , t, . . . , a, . . . , wn }, where t, a denote the trigger word and the candidate argument respectively. The trigger word is detected by the previous event detection models (independent of our work) and each named entity in the sentence is a candidate argument. Sentence Encoder is adopted to encode the word sequence into hidden embeddings,  {h1 , h2 . . . , hn } = E w1 , . . . , t, . . . , a, . . . , wn , (1) 5778 where E(·) is the neural network to encode the sentence. In this paper, we select CNN (Chen et al., 2015) and BERT (Devlin et al., 2019) as encoders. Feature Aggregator aggregates the hidden embeddings into an instance embedding. Our method is independent of the feature aggregator mechanism. Here, we follow Chen et al. (2015) and use dynamic multi-pooling as the feature aggregator: scores for hi are sci 1 , sci 2 , . . . , sci k computed by Eq. (4). As information about all the superordinate concepts should be retained in the role-oriented embedding, we calculate the mean of the attention scores as the role-oriented attention score, sri k 1 X cj = si , k (5) j=1 [x1,pt ]i = max{[h1 ]i , . . . , [hpt ]i }, [xpt +1,pa ]i = max{["
D19-1584,N16-1034,0,0.62235,"018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept"
D19-1584,D18-1247,1,0.781371,"score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module network for each con"
D19-1584,D09-1016,0,0.244851,"e event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-mo"
D19-1584,P11-2105,0,0.0152203,"ng att score input embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we"
D19-1584,P16-1116,0,0.285339,"BERT and HMEAE (BERT) are the same as the BERTBASE model. To utilize the event type information in our model, we append a special token into each input sequence for BERT to indicate the event type. Additional hyperparameters used in our experiments are shown in Table 2. Learning Rate Batch Size Kernel Size Warmup Rate uc dimension Wb dimension 6e-05 50 3 0.1 900 900 Table 2: Hyperparameter settings for BERT models. 3.2 Overall Evaluation Results We compare our models with various state-of-theart baselines on ACE 2005: (1) Feature-based methods, including Li’s joint (Li et al., 2013) and RBPB (Sha et al., 2016). (2) Vanilla neural network methods, including DMCNN (Chen et al., 2015) and JRNN (Nguyen et al., 2016). (3) Neural network with syntax information, like dbRNN (Sha et al., 2018) enhancing the recurrent neural network with dependency bridges to consider syntactically related information. On TAC KBP 2016, we compare our models with the top systems (Dubbin et al., 2016; Hsi et al., 2016; Ferguson et al., 2016) of the competition as well as DMCNN and DMBERT. 5780 Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment Argument Role Classification P R F1 Method Li’s Joint (Li e"
D19-1584,D18-1093,0,0.0221031,"embeddings + ATT att score Superordinate modules, corresponding to the superordinate concepts of the argument role to classify. Argument Role Classifier, varies with the argument role to classify. Figure 2: The overall architecture of HMEAE. Take the argument role “Seller” as an example. tion about the correlation between argument roles and help the argument role classification. To leverage the concept hierarchy information to improve EAE, we propose the Hierarchical Modular Event Argument Extraction (HMEAE) model. Inspired by the previous hierarchical classification works (Qiu et al., 2011; Shimura et al., 2018; Han et al., 2018) and the neural module networks (NMNs) (Andreas et al., 2016), HMEAE adopts the NMNs to enable a flexible network architecture imitating the concept hierarchical structure, which can provide effective inductive bias for better classification performance. As Figure 1 shows, we divide the concepts into two types: the superordinate concepts representing more abstractive concepts, and the finegrained argument roles. An argument role can belong to more than one superordinate concept, e.g., “Seller” belongs to both “Person” and “Org”. As shown in Figure 2, we set a neural module n"
D19-1584,P18-1201,0,0.114469,"(Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al., 2013) rely on hand-crafted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods"
D19-1584,N19-1105,1,0.884827,"ted features and patterns. With the ongoing development of neural networks, various neural networks have been used to automatically represent textual semantics with low-dimensional vectors, and further extract event arguments based on those semantic vectors, including convolutional neural networks (Chen et al., 2015) and recurrent neural networks (Nguyen et al., 2016; Sha et al., 2018). Advanced techniques also have been adopted to further improve EE, such as zeroshot learning (Huang et al., 2018), multi-modal integration (Zhang et al., 2017), and weakly supervised methods (Chen et al., 2017; Wang et al., 2019). However, the existing methods all treat argument roles as independent of each other, regardless of the fact that some argument roles are conceptually closer than others. Taking Figure 1 as an example, “Seller” is conceptually closer to “Buyer” than “Time-within”, because they share the same superordinate concepts “Person” and “Org” in the concept hierarchy. Intuitively, the concept hierarchy will provide extra informa5777 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5777"
D19-1584,P18-2066,0,0.173398,"1 Argument Role Instance Event argument extraction (EAE) aims to identify the entities serving as event arguments and classify the roles they play in an event. For instance, given that the word “sold” triggers a Transfer-Ownership event in the sentence “Steve Jobs sold Pixar to Disney”, EAE aims to identify that “Steve Jobs” is an event argument and its argument role is “Seller”. Most event extraction (EE) methods treat EE as a two-stage problem, including event detection (ED, to identify the trigger word and determine the event type) and EAE. As ED is well-studied (Nguyen and Grishman, 2018; Zhao et al., 2018) in recent years, EAE becomes the bottleneck of EE. † Org Time Seller Steve Jobs sold Buyer Pixar to Disney in Timewithin 2006. Figure 1: An example of the concept hierarchy. Introduction ∗ Person indicates equal contribution Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) Since EE benefits many NLP applications (Yang et al., 2003; Basile et al., 2014; Cheng and Erk, 2018), intensive efforts have been devoted to detecting events and extracting their event arguments. Traditional feature-based methods (Patwardhan and Riloff, 2009; Liao and Grishman, 2010b,a; Huang and Riloff, 2012; Li et al.,"
D19-1649,W06-1615,0,0.189413,"can sample instances from the training set as supporting instances for NOTA relation (this method is described explicitly in Section 4). Also note that to better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, w"
D19-1649,P07-1073,0,0.0597172,"mpled from the test set, each of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there"
D19-1649,N19-1423,0,0.0360439,"le NOTA is to regard it as an extra class in the N -way K-shot setting. To be more specific, we can sample instances outside the N relations as the supporting data of NOTA, and perform the (N + 1)-way K-shot learning. As compared to the current methods ignoring NOTA, this approach does not bring much improvements, since the supporting data for NOTA actually belong to several different relations and are scattered in the feature space, making it hard to perform classification. To better address few-shot NOTA, we propose a model named BERT-PAIR based on the sequence classification model in BERT (Devlin et al., 2019). We pair each query instance with all the supporting instances, concatenate each pair as one sequence, and send the concatenated sequence to the BERT sequence classification model to get the score of the two instances expressing the same relation. Denote the BERT model as B, the query instance as x and the paired supporting instance as xjr (the j-th supporting instance for the relation r), B(x, xjr ) outputs a two-element vector corresponding to scores of the pair sharing the same relation and not sharing the same relation. The probability over each relation in the few-shot scenario, includin"
D19-1649,P19-1279,0,0.106636,"Missing"
D19-1649,C18-1099,1,0.86151,"o better demonstrate the effects of the NOTA relation, we use the original FewRel dataset for fewshot NOTA, instead of the new test set, which can get rid of the influence of domain adaptation. 3 Approaches for Few-Shot DA Many efforts have been devoted for domain adaptation, like subspace mapping (Pan et al., 2010; Fernando et al., 2013), finding domain-invariant spaces (Baktashmotlagh et al., 2013; Ganin et al., 2016), feature augmentation (Blitzer et al., 2006) and minimax estimators (Provost and Fawcett, 2001). Among them, adversarial training (Goodfellow et al., 2015; Ganin et al., 2016; Wang et al., 2018) has been proved to be efficient in finding domain-invariant features. It is a game process between an encoder and a discriminator, where the encoder tries to generate domain-invariant features while the discriminator tries to tell which domain the features are from. Here we follow the adversarial training setting in Wang et al. (2018), where a two-layer perceptron network is used as the discriminator. While training the few-shot learning task, we feed the sentence encoder E and the discriminator D with the corpora from the training domain and the test domain, and optimize the min-max game, X"
D19-1649,D18-1514,1,0.42617,"Missing"
D19-1649,W09-2415,0,0.332326,"Missing"
D19-1649,P09-1113,0,0.589697,"ach of which consists of (R, S, x, r), where R = {r1 , r2 , ..., rN } is the sampled relation set, r ∈ R is the correct relation label for the query x, and S is the supporting set containing K instances for each relation, S = {(xjri , ri )}, 1 ≤ i ≤ N, 1 ≤ j ≤ K. (1) Models should predict the relation label y ∈ R for the query instance x based on the given S and R. Both of the following two challenges are based on this N -way K-shot setting. Both the training and test sets of the original FewRel dataset are constructed by manually annotating the distantly supervised (Bunescu and Mooney, 2007; Mintz et al., 2009) results on Wikipedia corpus and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014) knowledge bases. In other words, they are from the same domain, yet in a real-world scenario, we might train models on one domain and perform few-shot learning on a different one. For example, we may train models on Wikipedia, which has large amounts of data and adequate annotations, and then perform few-shot learning on some domains suffering data sparsity, like literature, finance and medicine. Note that, not only do these corpora differ vastly from each other in morphology and syntax, but there are wide disparities"
P15-1109,W13-3820,0,0.0528327,"Introduction Semantic role labeling (SRL) is a form of shallow semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is con"
P15-1109,D07-1002,0,0.168254,"he predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of"
P15-1109,P82-1020,0,0.899181,"Missing"
P15-1109,D14-1181,0,0.00204088,", 2008b). The constraints comes from annotation conventions of the task and other linguistic considerations. With dynamic programming, (T¨ackstr¨om et al., 2015) enhance the inference efficiency further. But designation of the constraints depends much on the linguistic knowledge. Nevertheless, the attempts of building end-toend systems for NLP become popular in recent years. Inspired by the work in computer vision, people hierarchically organized a window of words through convolution layers in deep form to account for the higher level of organization to solve the document classification task (Kim, 2014; Zhang and LeCun, 2015). Step further, people have also achieved success in directly mapping the sequence to sequence level target as the work in dependency parsing and machine translation (Vinyals et al., 2014; Sutskever et al., 2014). 3 Approaches In this paper, we propose an end-to-end system based on recurrent topology. Recurrent neural network (RNN) has natural advantage in modeling sequence problems. The past information is built 1128 up through the recurrent layer when model consumes the sequence word by word as shown in Eq. 1. x and y are the input and output of the recurrent layer wi"
P15-1109,W05-0620,0,0.182324,"Missing"
P15-1109,W05-0625,0,0.0648048,"ided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntactic tree provides an intermediate tag for improving the performance. However, building this syntactic tree also introduces the prediction risk inevitably. The analysis in (Pradhan et al., 2005) found that the major source of the incorrect predictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al., 2007; Koomen et al., 2005; Pradhan et al., 2005). Besides, feature templates in this classification task strongly rely on the expert experience. They need iterative modification after analyzing how the system performs on development data. When the corpus and data distribution are changed, or when people move to another language, the feature templates have to be re-designed. To address the above issues, (Collobert et al., 1127 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1127–1137, c Beijing, Chin"
P15-1109,P05-1022,0,0.0106519,"ple solve SRL problems in two major ways. The first one follows the traditional spirit widely used in NLP basic problems. A linear classifier is employed with feature templates. Most efforts focus on how to extract the feature templates that can best describe the text properties from training corpus. One of the most important features is from syntactic parsing, although syntactic parsing is also considered as a difficult problem. Thus system combination appear to be the general solution. In the work of (Pradhan et al., 2005), the syntactic tags are produced by Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) and Collins parser (Collins, 2003) respectively. Based on this, different systems are built to generate SRL tags. These SRL tags are used to extend the original feature templates, along with flat syntactic chunking results. At last another classifier learns the final SRL tag from the above results. In their analysis, the combination of three different syntactic view brings large improvement for the system. Similarly, Koomen et al. (Koomen et al., 2005) combined the system in another way. They built multiple classifiers and then all outputs are combined through an optimization problem. Surdean"
P15-1109,A00-2018,0,0.0718348,"Missing"
P15-1109,J03-4003,0,0.0340008,"rst one follows the traditional spirit widely used in NLP basic problems. A linear classifier is employed with feature templates. Most efforts focus on how to extract the feature templates that can best describe the text properties from training corpus. One of the most important features is from syntactic parsing, although syntactic parsing is also considered as a difficult problem. Thus system combination appear to be the general solution. In the work of (Pradhan et al., 2005), the syntactic tags are produced by Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) and Collins parser (Collins, 2003) respectively. Based on this, different systems are built to generate SRL tags. These SRL tags are used to extend the original feature templates, along with flat syntactic chunking results. At last another classifier learns the final SRL tag from the above results. In their analysis, the combination of three different syntactic view brings large improvement for the system. Similarly, Koomen et al. (Koomen et al., 2005) combined the system in another way. They built multiple classifiers and then all outputs are combined through an optimization problem. Surdeanu et al. fully discussed the combin"
P15-1109,W09-4621,0,0.00929256,"semantic parsing whose goal is to discover the predicate-argument structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem"
P15-1109,W05-0634,0,0.730736,"this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntactic tree provides an intermediate tag for improving the performance. However, building this syntactic tree also introduces the prediction risk inevitably. The analysis in (Pradhan et al., 2005) found that the major source of the incorrect predictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al., 2007; Koomen et al., 2005; Pradhan et al., 2005). Besides, feature templates in this classification task strongly rely on the expert experience. They need iterative modification after analyzing how the system performs on development data. When the corpus and data distribution are changed, or when people move to another language, the feature templates have to be re-designed."
P15-1109,W13-3516,0,0.0651288,"Missing"
P15-1109,J08-2005,0,0.958081,"and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntactic tree provides an intermediate tag for improving the performance. However, building this syntactic tree also introduces the prediction risk inevitably. The analysis in (Pradhan et al., 2005) found that the major source of the incorrect predictions was the syntactic parser. Combination of different syntactic parsers was proposed to address this problem, from both feature level and model level (Surdeanu et al., 2007; Koomen et al., 2005; Pradhan et al., 2005). Besides, feature templates in this classification task strongly rely on the expert experience"
P15-1109,P15-1150,0,0.0596,"Missing"
P15-1109,J08-2002,0,0.406663,"Missing"
P15-1109,P03-1002,0,0.0168905,"structure of each predicate in a given input sentence. Given a sentence, for each target verb (predicate) all the constituents in the sentence which fill a semantic role of the verb have to be recognized. Typical semantic arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc.. SRL is useful as an intermediate step in a wide range of natural language processing (NLP) tasks, such as information extraction (Bastianelli et al., 2013), automatic document categorization (Persson et al., 2009) and questionanswering (Dan and Lapata, 2007; Surdeanu et al., 2003; Moschitti et al., 2003). SRL is considered as a supervised machine learning problem. In traditional methods, linear classifier such as SVM is often employed to perform this task based on features extracted from the training corpus. Actually, people often treat this problem as a multi-step classification task. First, whether an argument is related to the predicate is determined; next the detail relation type was decided(Palmer et al., 2010). Syntactic information is considered to play an essential role in solving this problem (Punyakanok et al., 2008a). The location of an argument on syntacti"
P15-1109,Q15-1003,0,0.168922,"Missing"
P17-1013,buck-etal-2014-n,0,0.111414,"Missing"
P17-1013,P05-1033,0,0.0333576,"d the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 201"
P17-1013,D14-1179,0,0.0307604,"Missing"
P17-1013,P15-1001,0,0.26178,"study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014"
P17-1013,N03-1017,0,0.0167194,"EU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivasta"
P17-1013,P06-1077,1,0.570823,"orted results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NM"
P17-1013,D15-1166,0,0.699847,"through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed repres"
P17-1013,P02-1040,0,0.0977629,"arget word embedding at time step t, ct is dynamically obtained follows Equation (10). There are Ldec layers of RNNs armed with LAUs in the decoder. At inference stage, we only utilize the top-most hidden states sLdec to make the final prediction with a softmax layer: p(yi |y&lt;i , x) = softmax(Wo siLdec ) (12) . 4 Experiments 4.1 Setup We mainly evaluated our approaches on the widely used NIST Chinese-English translation task. In order to show the usefulness of our approaches, we also provide results on other two translation tasks: English-French, EnglishGerman. The evaluation metric is BLEU2 (Papineni et al., 2002). For Chinese-English, our training data consists of 1.25M sentence pairs extracted from 1 github.com/nyu-dl/dl4mt-tutorial/ tree/master/session2 2 For Chinese-English task, we apply case-insensitive NIST BLEU. For other tasks, we tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work. LDC corpora3 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) and 2006 (MT06) datasets as our test sets. For English"
P17-1013,D16-1050,0,0.0425113,"Missing"
P17-1013,D16-1160,0,0.0389859,"ective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sut"
P17-1013,Q16-1027,1,0.434023,"nts, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turned towards studying Deep Neural Networks (DNNs). Wu et al. (2016) and Zhou et al. (2016) found that deep architectures in both the encoder and decoder are essential for capturing subtle irregularities in the source and target languages. However, training a deep neural network is not as simple as stacking layers. Optimization often becomes increasingly difficult with more layers. One reasonable explanation is the notorious problem of vanishing/exploding gradients which was first studied in the context of vanilla RNNs (Pascanu et al., 2013b). Most prevalent approaches to solve this problem rely on short-cut connections between adjacent layers such as residual or fastforward connect"
P17-1013,P16-1159,0,0.0575075,"Missing"
P17-1013,P16-1008,1,0.825673,"surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the inp"
P17-1013,P06-1066,1,0.72775,"he same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art. 1 Introduction Neural Machine Translation (NMT) is an endto-end learning approach to machine translation which has recently shown promising results on multiple language pairs (Luong et al., 2015; Shen et al., 2015; Wu et al., 2016; Zhang et al., 2016; Tu et al., 2016; Zhang and Zong, 2016; Jean et al., 2015; Meng et al., 2015). Unlike conventional Statistical Machine Translation (SMT) systems (Koehn et al., 2003; Chiang, 2005; Liu et al., 2006; Xiong et al., 2006; Mi et al., 2008) which consist of multiple separately tuned components, NMT aims at building upon a single and large neural network to directly map input text to associated output text. Typical NMT models consists of two recurrent neural networks (RNNs), an encoder to read and encode the input text into a distributed representation and a decoder to generate translated text conditioned on the input representation (Sutskever et al., 2014; Bahdanau et al., 2014). Driven by the breakthrough achieved in computer vision (He et al., 2015; Srivastava et al., 2015), research in NMT has recently turne"
P17-1013,P08-1023,1,\N,Missing
P17-1013,P16-5005,0,\N,Missing
P17-1140,P05-1033,0,0.0285008,"mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve signiﬁcant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src re"
P17-1140,P06-1067,0,0.478247,"are usually inferior in comparison with the phrase-based SMT, but our model surpasses phrase-based Moses by average 4.43 BLEU points and outperforms the attention-based NMT baseline system by 5.09 BLEU points. Figure 1: The source word “yiju” does not obtain appropriate attention and its word sense is completely neglected. To enhance the attention mechanism, implicit word reordering knowledge needs to be incorporated into attention-based NMT. In this paper, we introduce three distortion models that originated from SMT (Brown et al., 1993; Koehn et al., 2003; Och et al., 2004; Tillmann, 2004; Al-Onaizan and Papineni, 2006), so as to model the word reordering knowledge as the probability distribution of the relative jump distances between the newly translated source word and the to-be-translated source word. Our focus is to extend the attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Our models have three merits: 1. Extended word reordering knowledge. Our models capture explicit word reordering knowledge to guide the attending process for attention mechanism. 2. Convenient to be incorporated into attention-based NMT. Our distortion models are d"
P17-1140,W14-4012,0,0.0500402,"Missing"
P17-1140,D14-1179,0,0.0527966,"Missing"
P17-1140,P05-1066,0,0.0969476,"task on the ChineseEnglish direction to evaluate the eﬀectiveness of our models. To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus. We also conduct the experiments to observe eﬀects of hyper-parameters and the training strategies. 4.1 2003-2006 are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002). Sign-test (Collins et al., 2005) is exploited for statistical signiﬁcance test. Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality. The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions. Following Bahdanau et al. (2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward representation and the backward representation are concatenated at the corresponding p"
P17-1140,N04-1035,0,0.0412781,"the context Ψ. Function Γ(·) for shifting the alignment vector is deﬁned as Γ(αt−1 , k) =   {αt−1,−k , ..., αt−1,m , 0, ..., 0}, αt−1 ,   {0, ..., 0, αt−1,1 , ..., αt−1,m−k }, k<0 k= 0 k>0 (9) which can be implemented as matrix multiplication computations. S-Distortion model adopts previous source context ct−1 as the context Ψ with the intuition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one. The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry speciﬁc word reordering knowledge. To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows, N P −→ JJ N N |JJ NN JJ −→ zuixin |latest. (10) From the above grammar, we can conjecture the speculation that after the word ”zuixin(latest)” is translated, the translation orientation is forward with shift distance 1. The probability function in S-Distortion model is"
P17-1140,P15-1001,0,0.0584537,"dentical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count”"
P17-1140,D13-1176,0,0.052373,"translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the lat"
P17-1140,C16-1205,1,0.702274,"proving translation quality. Comparison with previous work: We present the performance comparison with pre1529 System Coverage MEMDEC NMTIA Our work Length 80 50 80 50 MT03 36.16 35.69 37.93 MT04 39.81 39.24 40.40 MT05 32.73 35.91 35.74 36.81 MT06 32.47 35.98 35.10 35.77 Average 36.95 36.44 37.73 Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al., 2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality with external memory. NMTIA (Meng et al., 2016) exploits a readable and writable attention mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion model. The vocabulary sizes of all work are 30K and maximum lengths of sentence diﬀer. (a) (b) Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate. Systems RNNsearch∗ (30K) + T-Distortion + S-Distortion + H-Distortion BLEU 20.90 24.33‡ 24.10‡ 24."
P17-1140,J03-1002,0,0.0524536,"Missing"
P17-1140,P02-1040,0,0.0984942,"3 Training We carry the translation task on the ChineseEnglish direction to evaluate the eﬀectiveness of our models. To investigate the word alignment quality, we take the word alignment quality evaluation on the manually aligned corpus. We also conduct the experiments to observe eﬀects of hyper-parameters and the training strategies. 4.1 2003-2006 are used as test sets. To assess the word alignment quality, we employ Tsinghua dataset (Liu and Sun, 2015) which contains 900 manually aligned sentence pairs. Metrics: The translation quality evaluation metric is the case-insensitive 4-gram BLEU3 (Papineni et al., 2002). Sign-test (Collins et al., 2005) is exploited for statistical signiﬁcance test. Alignment error rate (AER) (Och and Ney, 2003) is calculated to assess the word alignment quality. The corpora includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. Hyper parameters: The sentence length for training NMTs is up to 50, while SMT model exploits whole training data without any restrictions. Following Bahdanau et al. (2015), we use bi-directional Gated Recurrent Unit (GRU) as the encoder. The forward representation and the backward representation are c"
P17-1140,P16-1162,0,0.0948834,"performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the trainin"
P17-1140,P16-1159,0,0.0390894,"e state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word col"
P17-1140,N03-1017,0,0.31876,"ls enable attention mechanism to attend to source words regarding both the semantic requirement and the word reordering penalty. Experiments on Chinese-English translation show that the approaches can improve word alignment quality and achieve signiﬁcant translation improvements over a basic attention-based NMT by large margins. Compared with previous works on identical corpora, our system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al"
P17-1140,D15-1166,0,0.274308,"ur system achieves the state-of-the-art performance on translation quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequ"
P17-1140,P16-1008,0,0.387371,"ion quality. 1 Introduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the training corpus. The col"
P17-1140,D16-1027,1,0.702952,"s improvements on BLEU scores proves the eﬀectiveness of proposed approaches in improving translation quality. Comparison with previous work: We present the performance comparison with pre1529 System Coverage MEMDEC NMTIA Our work Length 80 50 80 50 MT03 36.16 35.69 37.93 MT04 39.81 39.24 40.40 MT05 32.73 35.91 35.74 36.81 MT06 32.47 35.98 35.10 35.77 Average 36.95 36.44 37.73 Table 3: Comparison with previous work on identical training corpora. Coverage (Tu et al., 2016) is a basic RNNsearch model with a coverage model to alleviate the over-translation and under-translation problems. MEMDEC (Wang et al., 2016) is to improve translation quality with external memory. NMTIA (Meng et al., 2016) exploits a readable and writable attention mechanism to keep track of interactive history in decoding. Our work is NMT with H-Distortion model. The vocabulary sizes of all work are 30K and maximum lengths of sentence diﬀer. (a) (b) Figure 4: (a) is the output of the distortion model and is calculated on shift actions of previous alignment vector. (b) is the ultimate word alignment matrix of attention-based NMT with H-Distortion model. Compared with Figure 1, (b) is more centralized and accurate. Systems RNNsearc"
P17-1140,P01-1067,0,0.216794,"nce k that conditioned on the context Ψ. Function Γ(·) for shifting the alignment vector is deﬁned as Γ(αt−1 , k) =   {αt−1,−k , ..., αt−1,m , 0, ..., 0}, αt−1 ,   {0, ..., 0, αt−1,1 , ..., αt−1,m−k }, k<0 k= 0 k>0 (9) which can be implemented as matrix multiplication computations. S-Distortion model adopts previous source context ct−1 as the context Ψ with the intuition that certain source word indicate certain jump distance. The to-be-translated source word have intense positional relations with the newly translated one. The underlying linguistic intuition is that synchronous grammars (Yamada and Knight, 2001; Galley et al., 2004) can be extracted from language pairs. Word categories such as verb, adjective and preposition carry general word reordering knowledge and words carry speciﬁc word reordering knowledge. To further illustrate this idea, we present some common synchronous grammar rules that can be extracted from the example in Table 1 as follows, N P −→ JJ N N |JJ NN JJ −→ zuixin |latest. (10) From the above grammar, we can conjecture the speculation that after the word ”zuixin(latest)” is translated, the translation orientation is forward with shift distance 1. The probability function in"
P17-1140,Q16-1027,1,0.71397,"troduction Word reordering model is one of the most crucial sub-components in Statistical Machine Translation (SMT) (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005) which provides word reordering knowledge to ensure reasonable translation order of source words. It is separately trained and then incorporated into the SMT framework in a pipeline style. In recent years, end-to-end NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made tremendous progress (Jean et al., 2015; Luong et al., 2015b; Shen et al., 2016; Sennrich et al., 2016; Tu et al., 2016; Zhou et al., 2016; Johnson et al., 2016). An encoder-decoder framework (Cho et al., 2014b; Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2015) src ref NMT count youguan(related) baodao(report) shi(is) zhichi(support) tamen(their) lundian(arguments) de(’s) zuixin(latest) yiju(evidence) . the report is the latest evidence that supports their arguments . the report supports their perception of the latest . zuixin yiju {0} Table 1: An instance in Chinese-English translation task. The row “count” represents the frequency of the word collocation in the training corpus. The collocation “zuixin yi"
P17-1140,N04-4026,0,\N,Missing
P19-1002,P17-4012,0,0.0491261,"Missing"
P19-1002,N16-1014,0,0.0876363,"coder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance. 1 Introduction Past few years have witnessed the rapid development of dialogue systems. Based on the sequenceto-sequence framework (Sutskever et al., 2014), most models are trained in an end-to-end manner with large corpora of human-to-human dialogues and have obtained impressive success (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016; Serban et al., 2016). While there is still a long way for reaching the ultimate goal of dialogue systems, which is to be able to talk like humans. And one of the essential intelligence to achieve this goal is the ability to make use of knowledge. ∗ Fandong Meng is the corresponding author of the paper. This work was done when Zekang Li was interning at Pattern Recognition Center, WeChat AI, Tencent. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 12–21 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-1002,P18-1138,1,0.924905,"Huazhong University of Science and Technology ‡ Pattern Recognition Center, WeChat AI, Tencent Inc, China ♦ Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including b"
P19-1002,D15-1166,0,0.0293207,"using Self-Attentive Encoder, u ˆ (2) is the output words after the second-pass decoder. Training In contrast to the original Deliberation Network (Xia et al., 2017), where they propose a complex joint learning framework using Monte Carlo Method, we minimize the following loss as Xiong et al. (2018) do: Lmle = Lmle1 + Lmle2 (k+1) (29) log P (ˆ u(2)i ) Experiments 3.2 Baselines We compare our proposed model with the following state-of-the-art baselines: Models not using document knowledge: Seq2Seq: A simple encoder-decoder model (Shang et al., 2015; Vinyals and Le, 2015) with global attention (Luong et al., 2015). We concatenate utterances context to a long sentence as input. HRED: A hierarchical encoder-decoder model (Serban et al., 2016), which is composed of a word-level LSTM for each sentence and a sentence-level LSTM connecting utterances. Transformer: The state-of-the-art NMT model based on multi-head attention (Vaswani et al., 2017). We concatenate utterances context to a long sentence as its input. Models using document knowledge: Seq2Seq (+knowledge) and HRED (+knowledge) are based on Seq2Seq and HRED respectively. They both concatenate document knowledge representation and last decoding outp"
P19-1002,P17-1171,0,0.0317822,"arathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang et al., 2018; Yu et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2018). The Document Grounded Conversation is a task to generate natural dialogue responses when chatting about the content of a specific document. This task requires to integrate document knowledge with the multi-turn dialogue history. Different from previous knowledge grounded dialogue systems, Document Grounded Conversations utilize documents as the knowledge source, and hence are able to employ a wide spectrum of knowledge. And the Document Grounded Conversations is also different from document QA since the context"
P19-1002,P18-1136,0,0.0323799,"Yang Feng♦ , Qian Li♠ , Jie Zhou‡ † Dian Group, School of Electronic Information and Communications Huazhong University of Science and Technology ‡ Pattern Recognition Center, WeChat AI, Tencent Inc, China ♦ Key Laboratory of Intelligent Information Processing Institute of Computing Technology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing"
P19-1002,P02-1040,0,0.106854,"liberation decoder. Knowledge-Attention Transformer (KAT): As shown in Figure 2 (d), the encoder of this model is a simplified version of Incremental Transformer Encoder (ITE), which doesn’t have context-attention sub-layer. We concatenate utterances context to a long sentence as its input. The decoder of the model is a simplified Context-Knowledge-Attention Decoder (CKAD). It doesn’t have context-attention sub-layer either. This setup is to test how effective the context has been exploited in the full model. 3.3 3.4 Evaluation Metrics Automatic Evaluation: We adopt perplexity (PPL) and BLEU (Papineni et al., 2002) to automatically evaluate the response generation performance. Models are evaluated using perplexity of the gold response as described in (Dinan et al., 2018). Lower perplexity indicates better performance. BLEU measures n-gram overlap between a generated response and a gold response. However, since there is only one reference for each response and there may exist multiple feasible responses, BLEU scores are extremely low. We compute BLEU score by the multi-bleu.perl3 Manual Evaluation: Manual evaluations are essential for dialogue generation. We randomly sampled 30 conversations containing 6"
P19-1002,D18-1073,0,0.251661,"chnology, Chinese Academy of Sciences ♠ School of Computer Science and Engineering, Northeastern University, China zekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com fengyang@ict.ac.cn, qianli@stumail.neu.edu.cn Abstract There are several works on dialogue systems exploiting knowledge. The Mem2Seq (Madotto et al., 2018) incorporates structured knowledge into the end-to-end task-oriented dialogue. Liu et al. (2018) introduces factmatching and knowledge-diffusion to generate meaningful, diverse and natural responses using structured knowledge triplets. Ghazvininejad et al. (2018), Parthasarathi and Pineau (2018), Yavuz et al. (2018), Dinan et al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang"
P19-1002,P18-2124,0,0.0369028,"al. (2018) and Lo and Chen (2019) apply unstructured text facts in open-domain dialogue systems. These works mainly focus on integrating factoid knowledge into dialogue systems, while factoid knowledge requires a lot of work to build up, and is only limited to expressing precise facts. Documents as a knowledge source provide a wide spectrum of knowledge, including but not limited to factoid, event updates, subjective opinion, etc. Recently, intensive research has been applied on using documents as knowledge sources for QuestionAnswering (Chen et al., 2017; Huang et al., 2018; Yu et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2018). The Document Grounded Conversation is a task to generate natural dialogue responses when chatting about the content of a specific document. This task requires to integrate document knowledge with the multi-turn dialogue history. Different from previous knowledge grounded dialogue systems, Document Grounded Conversations utilize documents as the knowledge source, and hence are able to employ a wide spectrum of knowledge. And the Document Grounded Conversations is also different from document QA since the contextual consistent conversation response should be generated. To"
P19-1002,P15-1152,0,0.0751858,"Missing"
P19-1002,D18-1049,0,0.0387442,"Missing"
P19-1002,D18-1076,0,0.190263,"ess of the responses. The first-pass decoder focuses on contextual coherence, while the second-pass decoder refines the result of the firstpass decoder by consulting the relevant document knowledge, and hence increases the knowledge relevance and correctness. This is motivated by human cognition process. In real-world human conversations, people usually first make a draft on how to respond the previous utterance, and then consummate the answer or even raise questions by consulting background knowledge. We test the effectiveness of our proposed model on Document Grounded Conversations Dataset (Zhou et al., 2018). Experiment results show that our model is capable of generating responses of more context coherence and knowledge relevance. Sometimes document knowledge is even well used to guide the following conversations. Both automatic and manual evaluations show that our model substantially outperforms the competitive baselines. Our contributions are as follows: Incremental Transformer Encoder Document k-2 Self-Attentive Encoder Document k-1 Incremental Transformer Utterance k-1 Self-Attentive Encoder Document k Utterance k Incremental Transformer Self-Attentive Encoder Self-Attentive Encoder First-pa"
P19-1003,Q17-1010,0,0.00785837,"rather high-quality responses under the single-turn condition, but under multi-turn conversations, the complex context dependency makes the generation difficult. We integrate our utterance rewriter into the single-turn engine and compare with the original model by conducting the online A/B test. Specifically, we randomly split the users into two groups. One talks with the original system and the other talks with the system integrated with the utterance rewriter. All users are unconscious of the Task-oriented Our task-oriented dialogue system contains an intention classifier built on FastText(Bojanowski et al., 2017) and a set of templates that perform policy decision and slot-value filling sequentially. Intention detection is a most important component in task-oriented dialogues and its accuracy will affect all the following steps. We define 30 intention classes like weather, hotel booking and shopping. The training data contains 35,447 human annotations. With the combination of our rewriter, the intention classier is able 29 details about our system. The whole test lasted one month. Table 9 shows the Conversation-turns Per Session (CPS), which is the average number of conversation-turns between the chat"
P19-1003,P18-1015,0,0.01771,"chatbot platforms and find it leads to more accurate intention detection and improves the user engagement. In summary, our contributions are: 2 2.1 Related Work Sentence Rewriting Sentence rewriting has been widely adopted in various NLP tasks. In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017). In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal"
P19-1003,P18-1063,0,0.0228562,"ewriter into two online chatbot platforms and find it leads to more accurate intention detection and improves the user engagement. In summary, our contributions are: 2 2.1 Related Work Sentence Rewriting Sentence rewriting has been widely adopted in various NLP tasks. In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017). In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question a"
P19-1003,D16-1245,0,0.0160171,"-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self1. We collect a high-quality annotated dataset for coreference resolution and information completion in multi-turn dialogues, which might benefit future related research. 2. We propose a highly effective Transformerbased utterance rewriter outpe"
P19-1003,P16-1061,0,0.0187922,"-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self1. We collect a high-quality annotated dataset for coreference resolution and information completion in multi-turn dialogues, which might benefit future related research. 2. We propose a highly effective Transformerbased utterance rewriter outpe"
P19-1003,D13-1203,0,0.0153246,"u, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self1. We collect a high-quality annotated dataset for coreferenc"
P19-1003,D09-1120,0,0.0216442,"nts in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self1. We collect a hi"
P19-1003,P14-1005,0,0.0494665,"Missing"
P19-1003,I17-1013,0,0.0595353,"Missing"
P19-1003,W11-1902,0,0.04891,"al (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self1. We collect a high-quality annotat"
P19-1003,D17-1018,0,0.103353,"ased rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014). At both stages, they rely heavily on complicated, fine-grained features. Recently, several neural coreference resolution systems (Clark and Manning, 2016a,b) utilize distributed representations to reduce human labors. Lee et al. (2017) reported state-of-the-art results with an end-to-end neural coreference resolution system. However, it requires computing the scores for all possible spans, which is computationally inefficient on online dialogue systems. The recently proposed Transformer adopted the self1. We collect a high-quality annotated dataset for coreference resolution and information completion in multi-turn dialogues, which might benefit future related research. 2. We propose a highly effective Transformerbased utterance rewriter outperforming several strong baselines. 3. The trained utterance rewriter, when integra"
P19-1003,P17-1163,0,0.0433156,"Missing"
P19-1003,W18-5713,0,0.0401619,"re accurate intention detection and improves the user engagement. In summary, our contributions are: 2 2.1 Related Work Sentence Rewriting Sentence rewriting has been widely adopted in various NLP tasks. In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017). In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simpl"
P19-1003,C16-1172,0,0.0315582,"Missing"
P19-1003,N19-2013,0,0.0730044,"ks. In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017). In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Core"
P19-1003,J10-3010,0,0.0400127,"ccurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et al., 2016) or question answering (Abujabal et al., 2018), but most of them adopt a simple dictionary or template based rewriting strategy. For multi-turn dialogues, due to the complexity of human languages, designing suitable template-based rewriting rules would be timeconsuming. 2.2 Coreference Resolution Coreference resolution aims to link an antecedent for each possible mention. Traditional approaches often adopt a pipeline structure which first identify all pronouns and entities then run clustering algorithms (Haghighi and Klein, 2009; Lee et al., 2011; Durrett"
P19-1003,P17-1099,0,0.620882,"rained utterance rewriter into two online chatbot platforms and find it leads to more accurate intention detection and improves the user engagement. In summary, our contributions are: 2 2.1 Related Work Sentence Rewriting Sentence rewriting has been widely adopted in various NLP tasks. In machine translation, people have used it to refine the output generations from seq2seq models (Niehues et al., 2016; JunczysDowmunt and Grundkiewicz, 2017; Grangier and Auli, 2017; Gu et al., 2017). In text summarization, reediting the retrieved candidates can provide more accurate and abstractive summaries (See et al., 2017; Chen and Bansal, 2018; Cao et al., 2018). In dialogue modelling, Weston et al. (2018) applied it to rewrite outputs from a retrieval model, but they pay no attention to recovering the hidden information under the coreference and omission. Concurrent with our work, Rastogi et al. (2019) adopts a similar idea on English conversations to simplify the downstream SLU task by reformulating the original utterance. Rewriting the source input into some easy-to-process standard format has also gained significant improvements in information retrieval (Riezler and Liu, 2010), semantic parsing (Chen et a"
P19-1003,P15-1152,0,0.232184,"Missing"
P19-1003,P16-1073,0,\N,Missing
P19-1003,D18-1463,1,\N,Missing
P19-1074,P04-1035,0,0.0142302,"ngle sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-le"
P19-1074,P11-1055,0,0.148806,"Missing"
P19-1074,Q17-1008,0,0.237663,"level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgement Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou. 2018. A walk-based model on entity graphs for relation extraction. In Proceedings of ACL, pages 81–88. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirect"
P19-1074,D14-1162,0,0.0928063,"dels differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entity pair. For each word, the features fed to the encoder is the concatenation of its GloVe word embedding (Pennington et al., 2014), entity type embedding and coreference embedding. The entity type embedding is obtained by mapping the entity type (e.g., PER, LOC, ORG) assigned to the word into a vector using an embedding matrix. The entity type is assigned by human for the humanannotated data, and by a fine-tuned BERT model for the distantly supervised data. Named entity mentions corresponding to the same entity are assigned with the same entity id, which is determined by the order of its first appearance in the document. And the entity ids are mapped into vectors as the coreference embeddings. For each named entity menti"
P19-1074,P17-1147,0,0.0349888,"uality datasets. However, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which ma"
P19-1074,P10-1114,0,0.0111808,"rences Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to train and evaluate general-purpose document-level RE systems. 7 Acknowledgeme"
P19-1074,D17-1082,0,0.028811,"ever, these RE datasets limit relations to single sentences. References Rui Cai, Xiaodong Zhang, and Houfeng Wang. 2016. Bidirectional recurrent convolutional neural network for relation classification. In Proceedings of ACL, pages 756–765. As documents provide richer information than sentences, moving research from sentence level to document level is a popular trend for many areas, including document-level event extraction (Walker et al., 2006; Mitamura et al., 2015, 2017), fact extraction and verification (Thorne et al., 2018), reading comprehension (Nguyen et al., 2016; Joshi et al., 2017; Lai et al., 2017), sentiment classification (Pang and Lee, 2004; Prettenhofer and Stein, 2010), summarization (Nallapati et al., 2016) and machine translation (Zhang et al., 2018). Recently, some document-level RE datasets have also been constructed. However, these datasets are either constructed via distant supervision (Quirk and Poon, 2017; Peng et al., 2017) with inevitable wrong labeling problem, or limited in specific domain (Li et al., 2016; Peng et al., 2017). In contrast, DocRED is constructed by crowd-workers with rich information, and is not limited in any specific domain, which makes it suitable to"
P19-1074,D12-1110,0,0.0504203,"tion instances annotated for this example document are presented, with named entity mentions involved in these instances colored in blue and other named entity mentions underlined for clarity. Note that mentions of the same subject (e.g., Kungliga Hovkapellet and Royal Court Orchestra) are identified as shown in the first relation instance. work focuses on sentence-level RE, i.e., extracting relational facts from a single sentence. In recent years, various neural models have been explored to encode relational patterns of entities for sentence-level RE, and achieve state-of-theart performance (Socher et al., 2012; Zeng et al., 2014, 2015; dos Santos et al., 2015; Xiao and Liu, 2016; Cai et al., 2016; Lin et al., 2016; Wu et al., 2017; Qin et al., 2018; Han et al., 2018a). Despite these successful efforts, sentence-level RE suffers from an inevitable restriction in practice: a large number of relational facts are expressed in multiple sentences. Taking Figure 1 as an example, multiple entities are mentioned in the document and exhibit complex interactions. In Introduction The task of relation extraction (RE) is to identify relational facts between entities from plain text, which plays an important role"
P19-1074,D17-1188,0,0.159305,"n higher computational complexity such as (Sorokin Benchmark Settings We design two benchmark settings for supervised and weakly supervised scenarios respectively. For both settings, RE systems are evaluated on the high-quality human-annotated dataset, which provides more reliable evaluation results for document-level RE systems. The statistics of data used for the two settings are shown in Table 3. Supervised Setting. In this setting, only humanannotated data is used, which are randomly split 768 model, a bidirectional LSTM (BiLSTM) (Cai et al., 2016) based model and the Context-Aware model (Sorokin and Gurevych, 2017) originally designed for leveraging contextual relations to improve intra-sentence RE. The first three models differ only at the encoder used for encoding the document and will be explained in detail in the rest of this section. We refer the readers to the original paper for the details of the Context-Aware model for space limitation. The CNN/LSTM/BiLSTM based models first encode a document D = {wi }ni=1 consisting of n words into a hidden state vector sequence {hi }ni=1 with CNN/LSTM/BiLSTM as encoder, then compute the representations for entities, and finally predict relations for each entit"
P19-1074,D12-1042,0,0.159686,"Missing"
P19-1074,swampillai-stevenson-2010-inter,0,\N,Missing
P19-1074,P09-1113,0,\N,Missing
P19-1074,W03-0419,0,\N,Missing
P19-1074,C14-1220,0,\N,Missing
P19-1074,doddington-etal-2004-automatic,0,\N,Missing
P19-1074,P16-1072,0,\N,Missing
P19-1074,P16-1200,1,\N,Missing
P19-1074,D17-1004,0,\N,Missing
P19-1074,D17-1187,0,\N,Missing
P19-1074,P18-2014,0,\N,Missing
P19-1074,D18-1259,0,\N,Missing
P19-1074,D18-1247,1,\N,Missing
P19-1074,N19-1423,0,\N,Missing
P19-1074,D18-1514,1,\N,Missing
P19-1074,E17-1110,0,\N,Missing
P19-1074,P15-1061,0,\N,Missing
P19-1074,D15-1203,0,\N,Missing
P19-1074,C16-1119,0,\N,Missing
P19-1085,W13-3819,0,0.0691858,"Missing"
P19-1085,W17-5307,0,0.125125,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,D17-1070,0,0.0306419,"guage Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselo"
P19-1085,N19-1423,0,0.49966,"r is not sufficient for the claim. Intuitively, by sufficiently exchanging and reasoning over evidence information on the evidence graph, the proposed model can make the best of the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESI"
P19-1085,W18-2501,0,0.0172141,"ose our Graph-based Evidence Aggregating and Reasoning (GEAR) framework in the final claim verification stage. The full pipeline of our method is illustrated in Figure 1. 3.1 Document Retrieval and Sentence Selection In this section, we describe our document retrieval and sentence selection components. Additionally, we add a threshold filter after the sentence selection component to filter out those noisy evidence. In the document retrieval step, we adopt the entity linking approach from Hanselowski et al. (2018). Given a claim, the method first utilizes the constituency parser from AllenNLP (Gardner et al., 2018) to extract potential entities from the claim. Then it uses the entities as search queries and finds relevant Wikipedia documents via the online MediaWiki API2 . The seven highest-ranked results for each query are stored to form a candidate article set. Finally, the method drops the articles which are not in the offline Wikipedia dump and filters the articles by the word overlap between their titles and the claim. The sentence selection component selects the most relevant evidence for the claim from all sentences in the retrieved documents. Hanselowski et al. (2018) modify the ESIM 3.2 Claim V"
P19-1085,J84-3009,0,0.693828,"Missing"
P19-1085,W18-5516,0,0.229796,"ounty in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competiti"
P19-1085,W18-5525,0,0.0407684,"tive pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018)"
P19-1085,D14-1059,0,0.0998272,"twork is an American basic cable and satellite television channel. Table 1: Some examples of reasoning over several pieces of evidence together for verification. The italic words are the key information to verify the claim. Both of the claims require to reason and aggregate multiple evidence sentences for verification. More specifically, given a claim, an FV system is asked to label it as “SUPPORTED”, “REFUTED”, or “NOT ENOUGH INFO”, which indicate that the evidence can support, refute, or is not sufficient for the claim. Existing FV methods formulate FV as a natural language inference (NLI) (Angeli and Manning, 2014) task. However, they utilize simple evidence combination methods such as concatenating the evidence or just dealing with each evidence-claim pair. These methods are unable to grasp sufficient relational and logical information among the evidence. In fact, many claims require to simultaneously integrate and reason over several pieces of evidence for verification. As shown in Table 1, for both of the “SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple"
P19-1085,D15-1075,0,0.0499913,"cially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task"
P19-1085,W18-5526,0,0.190196,"Missing"
P19-1085,P17-1171,0,0.224451,"“SUPPORTED” example and “REFUTED” example, we cannot verify the given claims via checking any evidence in isolation. The claims can be verified only by understanding and reasoning over the multiple evidence. To integrate and reason over information from multiple pieces of evidence, we propose a Introduction Due to the rapid development of information extraction (IE), huge volumes of data have been extracted. How to automatically verify the data becomes a vital problem for various datadriven applications, e.g., knowledge graph completion (Wang et al., 2017) and open domain question answering (Chen et al., 2017a). Hence, many recent research efforts have been devoted to fact verification (FV), which aims to verify given claims with the evidence retrieved from plain text. † The Rodney King riots took place in the most populous county in the USA. Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn) 892 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 892–901 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected"
P19-1085,W18-5517,0,0.0467547,"hat the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis"
P19-1085,E17-1002,0,0.0154565,"m verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et"
P19-1085,W17-5308,0,0.0203664,"intly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competition on Codalab1 with a blind test set. Nie et al. (2019); Yoneda et"
P19-1085,D18-1010,0,0.0874661,"dey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these methods, Yin and Roth (2018) propose the T WOW ING OS system which trains the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results ("
P19-1085,W18-5515,0,0.449838,"ore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great results on various NLP applications, Malon (2018) fine-tunes the generative pretraining transformer (GPT) (Radford et al., 2018) for FV. Based on the methods mentioned above, Nie et al. (2019) specially design the neural semantic matching network (NSMN), which is a modification of ESIM and achieves the best results in the competition. Unlike these metho"
P19-1085,D16-1244,0,0.131708,"Missing"
P19-1085,N18-1202,0,0.0381169,"nd test set. Nie et al. (2019); Yoneda et al. (2018) and Hanselowski et al. (2018) have achieved the top three results among 23 teams. Existing methods mainly formulate FV as an NLI task. Thorne et al. (2018a) simply concatenate all evidence together, and then feed the concatenated evidence and the given claim into the NLI model. Luken et al. (2018) adopt the decomposable attention model (DAM) (Parikh et al., 2016) to generate NLI predictions for each claimevidence pair individually and then aggregate all 2.3 Pre-trained Language Models Pre-trained language representation models such as ELMo (Peters et al., 2018) and OpenAI GPT (Radford et al., 2018) are proven to be effective on many NLP tasks. BERT (Devlin et al., 2019) employs bidirectional transformer and welldesigned pre-training tasks to fuse bidirectional context information and obtains the state-of-theart results on the NLI task. In our experiments, we find the fine-tuned BERT model outperforms other NLI-based models on the claim verification subtask of FEVER. Hence, we use BERT as the sentence encoder in our framework to better encoding semantic information of evidence and claims. 1 https://competitions.codalab.org/ competitions/18814 893 Cla"
P19-1085,C16-1270,0,0.0227379,"ins the evidence identification and claim verification modules jointly. 2.2 Natural Language Inference The natural language inference (NLI) task requires a system to label the relationship between a pair of premise and hypothesis as entailment, contradiction or neutral. Several large-scale datasets have been proposed to promote the research in this direction, such as SNLI (Bowman et al., 2015) and Multi-NLI (Williams et al., 2018). These datasets have made it feasible to train complicated neural models which have achieved the state-of-the-art results (Bowman et al., 2015; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017b,c; Munkhdalai and Yu, 2017; Nie and Bansal, 2017; Conneau et al., 2017; Gong et al., 2018; Tay et al., 2018; Ghaeini et al., 2018). It is intuitive to transfer NLI models into the claim verification stage of the FEVER task and several teams from the shared task have achieved promising results by this way. Related Work FEVER Shared Task The FEVER shared task (Thorne et al., 2018b) challenges participants to develop automatic fact verification systems to check the veracity of human-generated claims by extracting evidence from Wikipedia. The shared task is hosted as a competi"
P19-1085,D18-1185,0,0.0219262,"Missing"
P19-1085,N18-1074,0,0.605773,"the information for verifying claims. For example, by delivering the information “Los Angeles County is the most populous county in the USA” to “the Rodney King riots occurred in Los Angeles County” through the evidence graph, the synthetic information can support “The Rodney King riots took place in the most populous county in the USA”. Furthermore, we adopt an effective pretrained language representation model BERT (Devlin et al., 2019) to better grasp both evidence and claim semantics. We conduct experiments on the large-scale benchmark dataset for Fact Extraction and VERification (FEVER) (Thorne et al., 2018a). Experimental results show that the proposed framework outperforms recent state-of-the-art baseline systems. The further case study indicates that our framework could better leverage multi-evidence information and reason over the evidence for FV. 2 2.1 NLI predictions for final verification. Then, Hanselowski et al. (2018); Yoneda et al. (2018); Hidey and Diab (2018) adopt the enhanced sequential inference model (ESIM) (Chen et al., 2017b), a more effective NLI model, to infer the relevance between evidence and claims instead of DAM. As pre-trained language models have achieved great result"
P19-1085,W18-5501,0,0.12184,"Missing"
P19-1192,N18-2011,0,0.355763,"Missing"
P19-1192,D16-1126,0,0.14845,"eration in news systems. At the same time, poetry generation is of growing interest and has attained high levels of quality for classical Chinese poetry. Previously, Chinese poem composing research mainly focused on traditional Chinese poems. In light of the mostly short sentences and the metrical constraints of traditional Chinese poems, the majority of research attention focused on term selection to improve the thematic consistency (Wang et al., 2016). In contrast, modern Chinese poetry is more flexible and rich in rhetoric. Unlike sentimentcontrolled or topic-based text generation methods (Ghazvininejad et al., 2016), which have been widely used in poetry generation, existing research has largely disregarded the importance of rhetoric in poetry generation. Yet, to emulate humanwritten modern Chinese poems, it appears necessary to consider not only the topics but also the form of expression, especially with regard to rhetoric. In this paper, we propose a novel rhetorically controlled encoder-decoder framework inspired by the above sentiment-controlled and topic-based text generation methods, which can effectively generate poetry with metaphor and personification. Overall, the contributions of the paper are"
P19-1192,P17-4008,0,0.0599481,"ion, which play an essential role in enhancing the aesthetics of poetry. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a styl"
P19-1192,D10-1051,0,0.0828681,"methods, which can effectively generate poetry with metaphor and personification. Overall, the contributions of the paper are as follows: • We present the first work to generate modern Chinese poetry while controlling for the use of metaphor and personification, which play an essential role in enhancing the aesthetics of poetry. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines ("
P19-1192,P18-1139,0,0.0205024,"e a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 2015; Larsen et al., 2016) extend the traditional VAE model (Kingma and Welling, 2014) with an additional c"
P19-1192,N16-1014,0,0.060762,"results of human evaluation. F means Fluency. C stands for Coherence. M represents Meaningfulness while RA represents Rhetorical Aesthetics. how well the models fit the data. The RhetoricF1 score is used to measure the rhetorically controlled accuracy of the generated poem sentences. Specifically, if the rhetoric label of the generated sentence is consistent with the ground truth, the generated result is right, and wrong otherwise. The rhetoric label of each poem sentence is predicted by our rhetoric classifier mentioned above (see 4.1 for details about this classifier). Distinct1/Distinct-2 (Li et al., 2016) is used to evaluate the diversity of the generated poems. Human Evaluation. Following previous work (Yi et al., 2018b), we consider four criteria for human evaluation: • Fluency: Whether the generated poem is grammatically correct and fluent. • Coherence: Whether the generated poem is coherent with the topics and contexts. • Meaningfulness: Whether the generated poem contains meaningful information. • Rhetorical Aesthetics: Whether the generated rhetorical poem has some poetic and artistic beauty. Each criterion is scored on a 5-point scale ranging from 1 to 5. To build a test set for human e"
P19-1192,D18-1423,0,0.0467974,"Missing"
P19-1192,P18-1204,0,0.0279615,"he above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 2015; Larsen et al., 2016) extend the traditional VAE model (Kingma and Welling, 2014) with an additional conditioned label to gui"
P19-1192,C16-1100,0,0.0828726,"ral language processing (NLP), e.g., for response generation in dialogue (Le et al., 2018), answer or question generation in question answering, and headline generation in news systems. At the same time, poetry generation is of growing interest and has attained high levels of quality for classical Chinese poetry. Previously, Chinese poem composing research mainly focused on traditional Chinese poems. In light of the mostly short sentences and the metrical constraints of traditional Chinese poems, the majority of research attention focused on term selection to improve the thematic consistency (Wang et al., 2016). In contrast, modern Chinese poetry is more flexible and rich in rhetoric. Unlike sentimentcontrolled or topic-based text generation methods (Ghazvininejad et al., 2016), which have been widely used in poetry generation, existing research has largely disregarded the importance of rhetoric in poetry generation. Yet, to emulate humanwritten modern Chinese poems, it appears necessary to consider not only the topics but also the form of expression, especially with regard to rhetoric. In this paper, we propose a novel rhetorically controlled encoder-decoder framework inspired by the above sentimen"
P19-1192,D18-1430,0,0.0819122,"by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 2015; Larsen et al., 2016) extend the traditional VAE model (Kingma and Welling, 2014) with an additional conditioned label to guide the generation process. Whereas VAEs essentially directly store latent attributes as probability distributions, CVAEs model latent variables conditioned on random varia"
P19-1192,N16-1174,0,0.0364431,"song lyrics from a small set of online music websites. The sentence rhetoric label is required for our model training. To this end, we built a classifier to predict the rhetoric label automatically. We sampled about 15,000 sentences from the original poetry dataset and annotated the data manually with three categories, i.e., metaphor, personification, and other. This dataset was divided into a training set, validation set, and test set. Three classifiers, including LSTM, Bi-LSTM, and Bi-LSTM with a self-attention model, were trained on this dataset. The Bi-LSTM with self-attention classifier (Yang et al., 2016) outperforms the other models and achieves the best accuracy of 0.83 on the 1 2 https://github.com/Lucien-qiang/Rhetoric-Generator http://www.shigeku.com/ Models for Comparisons • HRED: A hierarchical encoder-decoder model for text generation (Serban et al., 2016), which employs a hierarchical RNN to model the sentences at both the sentence level and the context level. • WM: A recent Working Memory model for poetry generation (Yi et al., 2018b). • CVAE: A standard CVAE model without the specific decoder. We adopt the same architecture as that introduced in Zhao et al. (2017). 4.3 Evaluation De"
P19-1192,K18-1024,0,0.198924,"in enhancing the aesthetics of poetry. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above m"
P19-1192,P17-1125,0,0.014952,"hods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. F"
P19-1192,D14-1074,0,0.165013,"try. • We conduct extensive experiments showing that our model outperforms the state-of-theart both in automated and human evaluations. 2 2.1 Related Work Poetry Generation Poetry generation is a challenging task in NLP. Traditional methods (Gerv´as, 2001; Manurung, 2004; Greene et al., 2010; He et al., 2012) relied on grammar templates and custom semantic diagrams. In recent years, deep learning-driven methods have shown significant success in poetry generation, and topic-based poetry generation systems have been introduced (Ghazvininejad et al., 2017, 2018; Yi et al., 2018b). In particular, Zhang and Lapata (2014) propose to generate Chinese quatrains with Recurrent Neural Networks (RNNs), while Wang et al. (2016) obtain improved results by relying on a planning model for Chinese poetry generation. Recently, Memory Networks (Sukhbaatar et al., 2015) and Neural Turing Machines (Graves et al., 2014) have proven successful at certain tasks. The most relevant work for poetry generation is that of Zhang et al. (2017), which stores hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold t"
P19-1192,P17-1061,0,0.0451678,"ion network. Usually, we assume that both the prior and the recognition networks are multivariate Gaussian distributions, and their mean and log variance are estimated through multilayer perceptrons (MLP) as follows:   µ, σ 2 = MLPposterior (LSTM(Y ), c) h 0 0 i (6) µ , σ 2 = MLPprior (c) A single layer of the LSTM is used to encode the current lines, and obtain the hX component of c. The same LSTM structure is also used to encode the next line Y in the training stage. By using Eq. (6), we calculate the KL divergence between these distributions to optimize Eq. (5). Following the practice in Zhao et al. (2017), a reparameterization technique is used when sampling from the recognition and the prior network during training and testing. 3.3.2 Automatic Control(AC) CVAE In the ACCVAE model, we first predict the rhetorical mode of the next sentence using an MLP that is designed as follows: p(r|hX ) = softmax(MLPpredictor (hX )) r = arg max p(r |hX ) (7) In this case, the conditional variable c is also [hX ; e(r)], where hX is taken as the last hidden state of the encoder LSTM. The loss function is then defined as: 1995 L = LKL + LdecoderCE + LpredictorCE (8) In this paper, a two-layer MLP is used for Eq"
P19-1192,P18-1104,0,0.0247288,"es hundreds of human-authored poems in a static external memory to improve the generated quatrains and achieve a style transfer. The above models rely on an external memory to hold training data (i.e., external poems and articles). In contrast, Yi et al. (2018a) dynamically invoke a memory component by saving the writing history into memory. 2.2 Stylistic Language Generation The ability to produce diverse sentences in different styles under the same topics is an important characteristic of human writing. Some works have explored style control mechanisms for text generation tasks. For example, Zhou and Wang (2018) use naturally labeled emojis for large-scale emotional response generation in dialogue. Ke et al. (2018) and Wang et al. (2018) propose a sentence controlling function to generate interrogative, imperative, or declarative responses in dialogue. For the task of poetry generation, Yang et al. (2018) introduce an unsupervised style labeling to generate stylistic poetry, based on mutual information. Inspired by the above works, we regard rhetoric in 1993 poetry as a specific style and adopt a Conditional Variational Autoencoder (CVAE) model to generate rhetoric-aware poems. CVAEs (Sohn et al., 20"
P19-1194,P18-1139,0,0.0593011,"ed on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from them, we dynamically update the pseudo-parallel data via on-the-fly back-translation (Lample et al., 2018b) during training (Eq. 12). There are some other tasks of NLP also show interest in controlling the fine-grained attribute of text generation. For example, Zhang et al. (2018a) and Ke et al. (2018) propose to control the specificity and diversity in dialogue generation. We borrow ideas from these works but the motivation and proposed models of our work are a far cry from them. The main differences are: (1) Since sentiment is dependent on local context while specificity is independent of local context, there is a series of design in our model to take the local context (or previous generated words) st into consideration (e.g., Eq. 1, Eq. 3). (2) Due to the lack of parallel data, we propose a cycle reinforcement learning algorithm to train the proposed model (Section 2.3). 6 Conclusion In"
P19-1194,D14-1181,0,0.00290278,"ut is also important. Inspired by the Mean Reciprocal Rank metric which is widely used in the Information Retrieval area, we design a Mean Relative Reciprocal Rank (MRRR) metric to measure the relative ranking MRRR = N 1 X 1 N i=1 |rank(vi ) − rank(ˆ vi ) |+ 1 (13) In addition, we also compare our model with the coarse-grained sentiment transfer systems. In order to make the results comparable, we define the generated test samples of all baselines for reproducibility. sentiment intensity larger/smaller than 0.5 as positive/negative results. Then we use a pre-trained binary TextCNN classifier (Kim, 2014) to compute the classification accuracy. 3.4.2 Human Evaluation We also perform human evaluation to assess the quality of generated sentences more accurately. Each item contains the source input, the sampled target sentiment intensity value, and the output of different systems. Then 500 items are distributed to 3 evaluators, who are required to score the generated sentences from 1 to 5 based on the input and target sentiment intensity value in terms of three criteria: content, sentiment, fluency. Content evaluates the content preservation degree. Sentiment refers to how much the output matches"
P19-1194,D18-1549,0,0.0242572,"Missing"
P19-1194,D17-1230,0,0.0454095,"in Figure 2. By means of policy gradient method (Williams, 1992), for each training example, the expected gradient of Eq. 10 can be approximated as: K   1 X (k) ∇θ L(θ) &apos; − r − b ∇θ log pθ (yˆ(k) ) K k=1 (11) where K is the sample size and b is the greedy search decoding baseline that aims to reduce the variance of gradient estimate which is implemented in the same way as Paulus et al. (2017). Nevertheless, RL training strives to optimize a specific metric which may not guarantee the fluency of the generated text (Paulus et al., 2017), and 2016 usually faces the unstable training problems (Li et al., 2017). The most direct way is to expose the sentences which are from the training corpus to the decoder and trained via MLE (also called teacher-forcing). In order to expose the decoder to the original sentence from the training corpus, we borrow ideas from back-translation (Lample et al., 2018a,b). Specifically, the model first generates a sequence yˆ based on the input text x and the target sentiment intensity value vy , and then reconstructs the source input x based on yˆ and the source sentiment intensity value vx . Therefore, the gradient of the cycle reconstruction loss is defined as:   ∇θ"
P19-1194,N18-1169,0,0.0483598,"Missing"
P19-1194,D18-1420,0,0.19687,"ntences whose intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,D15-1166,0,0.0327,"0, 1]. Intuitively, in order to achieve fine-grained control of sentiment, words whose sentiment intensities are closer to the target sentiment intensity value vy should be assigned a higher probability. Take Figure 2 as an example, at the 5-th time-step, word “good” should be assigned a higher probability than word “bad”, thus the predicted intensity value g(“good”, s4 ) is closer to the target sentiment intensity than g(“bad”, s4 ). To favor words whose sentiment intensity is near vy , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around vy , inspired by Luong et al. (2015) and Zhang et al. (2018a). Specifically, the sentiment probability is formulated as: 2 ! g(Es , st ) − vy 1 s ot = √ exp − (4) 2σ 2 2πσ pst = softmax(ost ) (5) where σ is the standard deviation. To balance both sentiment transformation and content preservation, the final probability distribution pt over the entire vocabulary is defined as a mixture of two probability distributions: pt = γpst + (1 − γ)pct (6) where γ is the hyper-parameter that controls the trade-off between two generation probabilities. 2015 &&apos; Encoder Algorithm 1 The cycle reinforcement learning algorithm for training Seq2Se"
P19-1194,S18-1001,0,0.0359134,"Missing"
P19-1194,E17-1096,0,0.0651517,"Missing"
P19-1194,P02-1040,0,0.107261,"2 2.64 2.54 2.37 2.52 3.84 3.85 2.13 2.14 3.41 2.43 2.84 3.21 Seq2SentiSeq 32.5 10.3 0.13 0.78 35.1 3.62 4.09 4.17 3.96 Human Reference 100.0 100.0 0.07 0.83 31.2 4.51 4.36 4.75 4.54 Table 1: Automatic evaluation and human evaluation in three aspects: Content (BLUE-1, BLUE-2), Sentiment (MAE, MRRR) and Fluency (PPL). Avg shows the average human scores. ↑ denotes larger is better, and vice versa. Bold denotes the best results. review in the test dataset, crowd-workers are required to write five references with sentiment intensity value from V 0 = [0.1, 0.3, 0.5, 0.7, 0.9]. Therefore, the BLEU (Papineni et al., 2002) score between the human reference and the corresponding generated text of the same sentiment intensity can evaluate the content preservation performance. Fluency: To measure the fluency, we calculate the perplexity (PPL) of each generated sequence via a pre-trained bi-directional LSTM language model (Mousa and Schuller, 2017). Sentiment: In order to measure how close the sentiment intensity of outputs to the target intensity values, we define three metrics. Given an input sentence x and a list of target intensity values V = [v1 , v2 , ..., vN ], the corresponding outputs of the model are [yˆ1"
P19-1194,P18-2031,0,0.0321807,"results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer. 1 1 Target Sentiment Text sentiment transfer aims to rephrase the input to satisfy a given sentiment label (value) while preserving its original semantic content. It facilitates various NLP applications, such as automatically converting the attitude of review and fighting against offensive language in social media (dos Santos et al., 2018). Previous work (Shen et al., 2017; Li et al., 2018; Luo et al., 2019) on text sentiment transfer mainly focuses on the coarse-grained level: the reversal of Joint work between WeChat AI and Peking University. 0.1 Horrible food and terrible service! 0.3 Plain food, slow service. 0.5 Food and service need improvement. 0.7 Good food and service. 0.9 Amazing food and perfect service!! Figure 1: An example of the input and output of the fine-grained text sentiment transfer task. The output reviews describe the same content (e.g. food/service) as the input while expressing different sentiment inten"
P19-1194,P18-1080,0,0.0258986,"tly different. In the semantic embedding space, most of the positive words and negative words lie closely. On the contrary, in the sentiment embedding space, positive words are far from negative words. In conclusion, neighbors on semantic embedding space are semantically related, while neighbors on sentiment embedding space express a similar sentiment intensity. Related Work Recently, there is a growing literature on the task of unsupervised sentiment transfer. This task aims to reverse the sentiment polarity of a sentence but keep its content unchanged without parallel data (Fu et al., 2018; Tsvetkov et al., 2018; Li et al., 2018; Xu et al., 2018; Lample et al., 2019). However, there are few researches focus on the fine-grained control of sentiment. Liao et al. (2018) exploits pseudo-parallel data via heuristic rules, thus turns this task to a supervised setting. They then propose a model based on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from th"
P19-1194,P18-1090,1,0.78267,"e intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,P18-1102,0,0.162234,"t intensity2 , while keeping the semantic content unchanged. Taking Figure 1 as an example, given the same input and five sentiment intensity values ranging from 0 (most negative) to 1 (most positive), the system generates five different outputs that satisfy the corresponding sentiment intensity in a relative order. There are two main challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. t"
P19-1194,W17-5227,0,0.0201437,"challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. target sentiment intensity value is a real value, other than discrete labels. Second, parallel data3 is unavailable in practice. In other words, we can only access the corpora which are labeled with fine-grained sentiment ratings or intensity values. Therefore, in the FTST task, we can not train a generative model via ground truth outpu"
P19-1197,H05-1042,0,0.348902,"ain the performance of 9.71 BLEU score.1 1 Introduction Table-to-text generation is to generate a description from the structured table. It helps readers to summarize the key points in the table, and tell in the natural language. Figure 1 shows an example of table-to-text generation. The table provides some structured information about a person named “Denise Margaret Scott”, and the corresponding text describes the person with the key information in the table. Table-to-text generation can be applied in many scenarios, including weather report generation (Liang et al., 2009), NBA news writing (Barzilay and Lapata, 2005), biography generation (Dubou´e and McKeown, 2002; Lebret et al., 2016), and so on. Moreover, table-to-text genera1 The codes are available at https://github.com/ lancopku/Pivot. Denise Margaret Scott Born 24 April 1955 Melbourne, Victoria Nationality Australian Denise Margaret Scott 24 April 1955 Other names Scotty Occupation Comedian, actor, television and radio presenter Australian Known for Studio 10 Partner(s) John Lane Comedian, actor, television and radio presenter Children 2 Denise Margaret Scott (born 24 April 1955) is an Australian comedian, actor and television presenter. Surface Re"
P19-1197,E06-1040,0,0.0615103,"Missing"
P19-1197,P16-1185,0,0.0252088,"ates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where only limited parallel data is available. We separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data. Besides, We propose a me"
P19-1197,N16-1012,0,0.032569,"1: An example of table-to-text generation, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25"
P19-1197,W02-2112,0,0.029488,"n brings a significant improvement to the pivot models under both vanilla Seq2Seq and Transformer frameworks, which demonstrates the efficiency of the denoising data augmentation. 3.9 the PIVOT model has some problem in generating repeating words (such as “senator” in the example), it can select the correct key facts from the table, and produce a fluent description. 4 Related Work This work is mostly related to both table-to-text generation and low resource natural language generation. 4.1 Table-to-text Generation Table-to-text generation is widely applied in many domains. Dubou´e and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao e"
P19-1197,W18-6505,0,0.222032,"ance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25 20 15 10 5 0 10000 20000 30000 40000 50000 60000 Parallel Data Size Figure 2: The BLEU scores of the a table-to-text model trained with different number of parallel data under the encoder-decoder framework on the WIKIBIO dataset. lel data is available. Some previous work (Puduppully et al., 2018; Gehrmann et al., 2018) formulates the task as the combination of content selection and surface realization, and models them with an end-to-end model. Inspired by these work, we break up the table-to-text generation into two stages, each of which is performed by a model trainable with only a few annotated data. Specifically, it first predicts the key facts from the tables, and then generates the text with the key facts, as shown in Figure 1. The two-stage method consists of two separate models: a key fact prediction model and a surface realization model. The key fact prediction model is formulated as a sequence labe"
P19-1197,N18-1032,0,0.0158927,"r fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where on"
P19-1197,J82-2005,0,0.518536,"Missing"
P19-1197,D16-1128,0,0.566599,"g the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed s"
P19-1197,P09-1011,0,0.134087,"the example), it can select the correct key facts from the table, and produce a fluent description. 4 Related Work This work is mostly related to both table-to-text generation and low resource natural language generation. 4.1 Table-to-text Generation Table-to-text generation is widely applied in many domains. Dubou´e and McKeown (2002) proposed to generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model"
P19-1197,N03-1020,0,0.0677063,"Missing"
P19-1197,D15-1166,0,0.122996,"Realization Figure 1: An example of table-to-text generation, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext sam"
P19-1197,D18-1422,0,0.0332302,"y et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by utilizing pre-executed symbolic operations in a sequence-to-sequence model. Qualitative Analysis We provide an example to illustrate the improvement of our model more intuitively, as shown in Table 4. Under the low resource setting, the Transformer can not produce a fluent sentence, and also fails to select the proper fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, w"
P19-1197,P02-1040,0,0.103597,"Missing"
P19-1197,D18-1411,0,0.0221501,"generate the biography by matching the text with a knowledge base. Barzilay and Lapata (2005) presented an efficient method for automatically learning content selection rules from a corpus and its related database in the sports domain. Liang et al. (2009) introduced a system with a sequence of local decisions for the sportscasting and the weather forecast. Recently, thanks to the success of the neural network models, more work focused on the neural generative models in an endto-end style (Wiseman et al., 2017; Puduppully et al., 2018; Gehrmann et al., 2018; Sha et al., 2018; Bao et al., 2018; Qin et al., 2018). Lebret et al. (2016) constructed a dataset of biographies from Wikipedia, and built a neural model based on the conditional neural language models. Liu et al. (2018) introduced a structure-aware sequence-tosequence architecture to model the inner structure of the tables and the interaction between the tables and the text. Wiseman et al. (2018) focused on the interpretable and controllable generation process, and proposed a neural model using a hidden semi-markov model decoder to address these issues. Nie et al. (2018) attempted to improve the fidelity of neural table-to-text generation by ut"
P19-1197,W18-2205,0,0.0662726,"Missing"
P19-1197,W17-4503,0,0.0563362,"Missing"
P19-1197,K18-1003,0,0.0241831,"del more intuitively, as shown in Table 4. Under the low resource setting, the Transformer can not produce a fluent sentence, and also fails to select the proper fact from the table. Thanks to the unlabeled data, the SemiMT model can generate a fluent, human-like description. However, it suffers from the hallucination problem so that it generates some unseen facts, which is not faithful to the source input. Although 4.2 Low Resource Natural Language Generation The topic of low resource learning is one of the recent spotlights in the area of natural language generation (Tilk and Alum¨ae, 2017; Tran and Nguyen, 2018). More work focused on the task of neural machine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lam"
P19-1197,D17-1239,0,0.0786093,"Missing"
P19-1197,D18-1356,0,0.0931456,"Missing"
P19-1197,C18-1330,1,0.82569,"ion, and also a flow chart of our method. tion is a good testbed of a model’s ability of understanding the structured knowledge. Most of the existing methods for table-totext generation are based on the encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). They represent the source tables with a neural encoder, and generate the text word-byword with a decoder conditioned on the source table representation. Although the encoder-decoder framework has proven successful in the area of natural language generation (NLG) (Luong et al., 2015; Chopra et al., 2016; Lu et al., 2017; Yang et al., 2018), it requires a large parallel corpus, and is known to fail when the corpus is not big enough. Figure 2 shows the performance of a table-to-text model trained with different number of parallel data under the encoder-decoder framework. We can see that the performance is poor when the parallel data size is low. In practice, we lack the large parallel data in many domains, and it is expensive to construct a high-quality parallel corpus. This work focuses on the task of low resource table-to-text generation, where only limited paraltext samples. 35 30 2 BLEU 25 20 15 10 5 0 10000 20000 30000 40000"
P19-1197,D18-1138,1,0.847758,"achine translation, whose models can generalize to other tasks in natural language generation. Gu et al. (2018) proposed a novel universal machine translation which uses a transfer-learning approach to share lexical and sentence level representations across different languages. Cheng et al. (2016) proposed a semi-supervised approach that jointly train the sequence-to-sequence model with an auto-encoder, which reconstruct the monolingual corpora. More recently, some work explored the unsupervised methods to totally remove the need of parallel data (Lample et al., 2018b,a; Artetxe et al., 2017; Zhang et al., 2018). 5 Conclusions In this work, we focus on the low resource tableto-text generation, where only limited parallel data is available. We separate the generation into two stages, each of which is performed by a model trainable with only a few annotated data. Besides, We propose a method to construct a pseudo parallel dataset for the surface realization model, without the need of any structured table. Experiments show that our proposed model can achieve 27.34 BLEU score on a biography generation dataset with only 1, 000 parallel data. Acknowledgement We thank the anonymous reviewers for their thoug"
P19-1233,C18-1139,0,0.482507,"training and development data 4 . More notably, our GCDT surpasses the models that exploit additional task-specific resources or annotated corpora (Luo et al., 2015; Yang et al., 2017b; Chiu and Nichols, 2016). Additionally, we conduct experiments by leveraging the well-known BERT as an external resource for relatively fair comparison with models 4 We achieve F1 score of 92.18 when training on both training and development data without extra resources. 2435 Models (Rei, 2017) (Liu et al., 2017) (Peters et al., 2017)† (Peters et al., 2018) (Clark et al., 2018) (2018) BERTBASE (2018) BERTLARGE (Akbik et al., 2018)† GCDT + BERTLARGE F1 86.26 91.71 ± 0.10 91.93 ± 0.19 92.20 92.61 92.40 92.80 93.09 93.47 ± 0.03 # 0 1 2 3 F1 93.88 95.96 ± 0.08 96.37 ± 0.05 96.72 ± 0.05 97.00 97.30 ± 0.03 Table 4: F1 scores on the CoNLL2000 Chunking task by leveraging language model. We establish new stateof-the-art result on this task. that utilize external language models trained on massive corpora. Especially, Rei (2017) and Liu et al. (2017) build task-specific language models only on supervised data. Table 3 and Table 4 show that our GCDT outperforms previous state-of-theart results substantially at 93.47 (+0.38) on NE"
P19-1233,W17-4710,0,0.159173,"bulary. Formally, the label of word xt is predicted as the probabilistic equation (Eq. 13) st = DTde (ht , yt−1 ; θDTde ) (11) lt = st Wl + bl (12) P (yt = j|x) = sof tmax(lt )[j] (13) As we can see from the above procedures and Figure 1, our GCDT firstly encodes the global contextual representation along the sequential axis by Deep Transition RNN Deep transition RNNs extend conventional RNNs by increasing the transition depth of consecutive hidden states. Previous studies have shown the superiority of this architecture on both language modeling (Pascanu et al., 2014) and machine translation (Barone et al., 2017; Meng and Zhang, 2019). Particularly, Meng and Zhang (2019) propose to maintain a linear transformation path throughout the deep transition procedure with a linear gate to enhance the transition structure. Following Meng and Zhang (2019), the deep transition block in our hierarchical model is composed of two key components, namely Linear Transformation enhanced GRU (L-GRU) and Transition GRU (T-GRU). At each time step, LGRU first encodes each token with an additional linear transformation of the input embedding, then the hidden state of L-GRU is passed into a chain of 2433 T-GRU connected mer"
P19-1233,C02-1025,0,0.189985,"14; Xin et al., 2018), IntNet (Xin et al., 2018). The shallow connections between consecutive hidden states in those models inspire us to deepen the transition path for richer representation. More recently, there has been a growing body of work exploring to leverage language model trained on massive corpora in both character level (Peters et al., 2017, 2018; Akbik et al., 2018) and token level (Devlin et al., 2018). Inspired by the effectiveness of language model embeddings, we conduct auxiliary experiments by leveraging the well-known BERT as an additional feature. Exploit Global Information Chieu and Ng (2002) explore the usage of global feature in the whole document by the co-occurrence of each token, which is fed into a maximum entropy classifier. With the widespread application of distributed word representations (Mikolov et al., 2013) and neural networks (Collobert et al., 2011; Huang et al., 2015) in sequence labeling tasks, the global information is encoded into hidden states of BiRNNs. Specially, Yang et al. (2017a) leverage global sentence patterns for NER reranking. Inspired by the global sentence-level representation in S-LSTM (Zhang et al., 2018), we propose a more concise approach to ca"
P19-1233,Q16-1026,0,0.760702,"ndamental and challenging problems of Natural Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs (Hochreiter and Schmidhuber, 1997) have become a dominant method on ∗ This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/GCDT. multiple benchmark datasets (Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Peters et al., 2017). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has bee"
P19-1233,D18-1217,0,0.0548064,"Missing"
P19-1233,D17-1206,0,0.0543637,"Missing"
P19-1233,P82-1020,0,0.801678,"Missing"
P19-1233,N16-1030,0,0.867902,"g problems of Natural Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs (Hochreiter and Schmidhuber, 1997) have become a dominant method on ∗ This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/GCDT. multiple benchmark datasets (Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Peters et al., 2017). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful"
P19-1233,D15-1104,0,0.221855,"tly modeled at each position, as the nature of recurrent architecture makes RNN partial to the most recent input token. While our context-aware representation is incorporated with local word embeddings directly, which assists in capturing useful representations through combinatorial computing between diverse local word embeddings and the global contextual embedding. We further investigate the effects on positions where the global embedding is used. (Section 5.1) 2434 4 4.1 Models (Collobert et al., 2011)* (Huang et al., 2015)* (Passos et al., 2014)* (Lample et al., 2016) (Yang et al., 2016)* (Luo et al., 2015)* (Ma and Hovy, 2016) (Yang et al., 2017b)*† (Zhang et al., 2018) (Yang et al., 2017a) (Chiu and Nichols, 2016)*† (Xin et al., 2018) GCDT GCDT + BERTLARGE Experiments Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. Chunking The CoNLL2000 Chunking task (Sang and Buchholz, 2000) defines 11 syntactic chunk types (NP, VP, PP, etc.). Standard data includes train and test sets. Metric We adopt the BIOES tagging scheme for both tasks instead of the stand"
P19-1233,P16-1101,0,0.212471,"Missing"
P19-1233,W14-1609,0,0.0885275,"Missing"
P19-1233,P17-1161,0,0.39535,"Language Processing (NLP). Recently, neural models have become the de-facto standard for high-performance systems. Among various neural networks for sequence labeling, bi-directional RNNs (BiRNNs), especially BiLSTMs (Hochreiter and Schmidhuber, 1997) have become a dominant method on ∗ This work was done when Yijin Liu was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China † Jinan Xu is the corresponding author of the paper. 1 Code is available at: https://github.com/Adaxry/GCDT. multiple benchmark datasets (Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Peters et al., 2017). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful for model sequence (Z"
P19-1233,N18-1202,0,0.170779,"Missing"
P19-1233,W09-1119,0,0.228702,"and Nichols, 2016)*† (Xin et al., 2018) GCDT GCDT + BERTLARGE Experiments Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. Chunking The CoNLL2000 Chunking task (Sang and Buchholz, 2000) defines 11 syntactic chunk types (NP, VP, PP, etc.). Standard data includes train and test sets. Metric We adopt the BIOES tagging scheme for both tasks instead of the standard BIO2, since previous studies have highlighted meaningful improvements with this scheme (Ratinov and Roth, 2009). We take the official conlleval 3 as the token-level F1 metric. Since the data size if relatively small, we train each final model for 5 times with different parameter initialization and report the mean and standard deviation F1 value. 4.2 Table 1: F1 scores on CoNLL03. † refers to models trained on both training and development set. * refers to adopting external task-specific resources. Models (Collobert et al., 2011)* (Huang et al., 2015)* (Yang et al., 2017b) (Zhai et al., 2017) (Hashimoto et al., 2017) (Søgaard and Goldberg, 2016) (Xin et al., 2018) GCDT GCDT + BERTLARGE Implementation De"
P19-1233,P17-1194,0,0.0389681,"Missing"
P19-1233,W00-0726,0,0.826473,"Missing"
P19-1233,W03-0419,0,0.299564,"Missing"
P19-1233,P16-2038,0,0.0556517,"Missing"
P19-1233,D18-1279,0,0.311212,"Missing"
P19-1233,yang-etal-2017-neural-reranking,0,0.217434,"ure of recurrent architecture makes RNN partial to the most recent input token. While our context-aware representation is incorporated with local word embeddings directly, which assists in capturing useful representations through combinatorial computing between diverse local word embeddings and the global contextual embedding. We further investigate the effects on positions where the global embedding is used. (Section 5.1) 2434 4 4.1 Models (Collobert et al., 2011)* (Huang et al., 2015)* (Passos et al., 2014)* (Lample et al., 2016) (Yang et al., 2016)* (Luo et al., 2015)* (Ma and Hovy, 2016) (Yang et al., 2017b)*† (Zhang et al., 2018) (Yang et al., 2017a) (Chiu and Nichols, 2016)*† (Xin et al., 2018) GCDT GCDT + BERTLARGE Experiments Datasets and Metric NER The CoNLL03 NER task (Sang and De Meulder, 2003) is tagged with four linguistic entity types (PER, LOC, ORG, MISC). Standard data includes train, development and test sets. Chunking The CoNLL2000 Chunking task (Sang and Buchholz, 2000) defines 11 syntactic chunk types (NP, VP, PP, etc.). Standard data includes train and test sets. Metric We adopt the BIOES tagging scheme for both tasks instead of the standard BIO2, since previous studies have hi"
P19-1233,P18-1030,0,0.187121,"). However, there are several natural limitations of the BiLSTMs architecture. For example, at each time step, the BiLSTMs consume an incoming word and construct a new summary of the past subsequence. This procedure should be highly nonlinear, to allow the hidden states to rapidly adapt to the mutable input while still preserving a useful summary of the past (Pascanu et al., 2014). While in BiLSTMs, even stacked BiLSTMs, the transition depth between consecutive hidden states are inherently shallow. Moreover, global contextual information, which has been shown highly useful for model sequence (Zhang et al., 2018), is insufficiently captured at each token position in BiLSTMs. Subsequently, inadequate representations flow into the final prediction layer, which leads to the restricted performance of BiLSTMs. In this paper, we present a global context enhanced deep transition architecture to eliminate the mentioned limitations of BiLSTMs. In particular, we base our network on the deep transition (DT) RNN (Pascanu et al., 2014), which increases the transition depth between consecutive hidden states for richer representations. Furthermore, we assign each token an additional representation, which is a summat"
P19-1288,W17-4123,0,0.128847,"ty. Recently, the Transformer model (Vaswani et al., 2017) further enhances the translation performance on multiple language pairs, while suffering from the slow decoding procedure, which reJoint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. ? Corresponding Author stricts its application scenarios. The slow decoding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word according to the source sentence representations and the target translation history. Non-autoregressive Transformer model (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (Gu et al., 2017a), translation results from other systems (Lee et al., 2018; Guo et al., 2018) and latent variables (Kaiser et al., 2018) as decoder inputs. Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time"
P19-1288,D17-1210,0,0.0371484,"Missing"
P19-1288,P84-1044,0,0.302576,"Missing"
P19-1288,D16-1139,0,0.285399,"of predicted token fed to the fusion layer. 4 Related Work Gu et al. (2017a) introduced the nonautoregressive Transformer model to accelerate the translation. Lee et al. (2018) proposed a nonautoregressive sequence model based on iterative refinement, where the outputs of the decoder are fed back as inputs in the next iteration. Guo et al. (2018) proposed to enhance the decoder inputs with phrase-table lookup and embedding mapping. Kaiser et al. (2018) used a sequence of autoregressively generated discrete latent variables as inputs of the decoder. Knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) is a method for training a smaller and faster student network to perform better by learning from a teacher network, which is crucial in NAT models. Gu et al. (2017a) applied Sequence-level knowledge distillation to eliminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the s"
P19-1288,P02-1040,0,0.103733,"th Annual Meeting of the Association for Computational Linguistics, pages 3013–3024 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU (Papineni et al., 2002), GLEU (Wu et al., 2017), TER (Snover et al., 2006)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure. We conduct experiments on three machine transla"
P19-1288,P16-1162,0,0.0899481,"et al., 2017). Importance sampling estimates the properties of a particular distribution through sampling on a different proposal distribution. Complementary sum sampling reducdes the variance through suming over the important subset and estimating the rest via sampling. 5 5.1 Experiments Settings Dataset. We conduct experiments on three translation tasks3 : IWSLT16 En→De (196k pairs), WMT14 En↔De (4.5M pairs) and WMT16 En↔Ro (610k pairs). We use the preprocessed datasets released by Lee et al. (2018), where all sentences are tokenized and segmented into subword units using the BPE algorithm (Sennrich et al., 2016). For all tasks, source and target languages share the vocabulary with size 40k. For WMT14 En-De, we employ newstest-2013 and newstest-2014 as development and test sets. For WMT16 En-Ro, we take newsdev-2016 and newstest-2016 as development and test sets. For IWSLT16 En-De, we use the test2013 for validation. Baselines. We take the Transformer model (Vaswani et al., 2017) as the autoregressive baseline. The non-autoregressive model based on iterative refinement (Lee et al., 2018) is the nonautoregressive baseline, and we set the number of iterations to 2. Pre-train. To evaluate the sequence-le"
P19-1288,D18-1510,1,0.827268,"Missing"
P19-1288,P16-1159,0,0.121143,"search is fed into the decoder to guide the generation of the next word. The prominent feature of the autoregressive model is that it requires the target side historical information in the decoding procedure. Therefore target words are generated in the one-by-one style. Due to the autoregressive property, the decoding speed is limited, which restricts the application of the autoregressive model. 2.2 Reinforcement learning techniques (Sutton et al., 2000; Ng et al., 1999; Sutton, 1984) have been widely applied to improve the performance of the autoregressive NMT with sequence-level objectives (Shen et al., 2016; Ranzato et al., 2015; Bahdanau et al., 2016). As sequence-level objectives are usually non-differentiable, the loss function is defined as the negative expected reward: Lθ = − θ = arg max{L(θ)} θ L(θ) = M X T X m=1 t=1 m log(p(ytm |y<t , X m , θ)), (2) X p(Y|X, θ) · r(Y), (3) Y=y1:T where Y = y1:T denotes possible sequences generated by the model, and r(Y) is the corresponding reward such as BLEU, GLEU and TER for generating sequence Y. Enumerating all the possible target sequences is impossible due to the exponential search space, and REINFORCE (Williams, 1992) gives an elegant way to estim"
P19-1288,2006.amta-papers.25,0,0.0258753,"l Linguistics, pages 3013–3024 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this paper, we present two approaches to retrieve the target sequential information for NAT models to enhance their translation ability and meanwhile preserve the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. We leverage the sequence-level objectives (e.g., BLEU (Papineni et al., 2002), GLEU (Wu et al., 2017), TER (Snover et al., 2006)) instead of the cross-entropy objective to encourage NAT model to generate high quality sentences rather than the correct token for each position. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. The bottom layers of the FS-decoder run in parallel to keep the decoding speed and the top layer of the FS-decoder can exploit target sequential information to guide the target words generation procedure. We conduct experiments on three machine translation tasks (IWSLT16 En→De, WMT14 En↔De, WMT16 En→Ro"
P19-1288,D18-1149,0,0.507576,"ding problem of the Transformer model is caused by its autoregressive nature, which means that the target sentence is generated word by word according to the source sentence representations and the target translation history. Non-autoregressive Transformer model (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. Since the generation of target words is independent, NAT models utilize alternative information such as encoder inputs (Gu et al., 2017a), translation results from other systems (Lee et al., 2018; Guo et al., 2018) and latent variables (Kaiser et al., 2018) as decoder inputs. Without considering the target translation history, NAT models are weak to exploit the target words collocation knowledge and tend to generate repeated target words at adjacent time steps (Wang et al., 2019). Over-translation and undertranslation problems are aggravated and often occur due to the above reasons. Table 1 shows an inferior translation example generated by a NAT model. Compared to the autoregressive Transformer, NAT models achieve significant speedup while suffering from a large gap in translation qu"
P19-1288,P18-2053,0,0.0697674,"neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradient estimator. Recently, techniques for sequence-level training with continuous objectives have been explored, including deterministic policy gradient algorithms (Gu et al., 2017b), bag-of-words objective (Ma et al., 2018) and probabilistic n-gram matching (Shao et al., 2018). However, to the best of our knowledge, sequence-level training has not been applied to non-autoregressive models yet. The methods of variance reduction through focusing on the important parts of the distribution include importance sampling (Bengio et al., 2003; Glynn and Iglehart, 1989) and complementary sum sampling (Botev et al., 2017). Importance sampling estimates the properties of a particular distribution through sampling on a different proposal distribution. Complementary sum sampling reducdes the variance through suming over the i"
P19-1288,D18-1044,0,0.0803608,"et al., 2015; Kim and Rush, 2016) is a method for training a smaller and faster student network to perform better by learning from a teacher network, which is crucial in NAT models. Gu et al. (2017a) applied Sequence-level knowledge distillation to eliminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregress"
P19-1288,D18-1397,0,0.0355153,"e Y from the probability distribution and estimate the gradient with the gradient of log-probability weighted by the reward r(Y): ∇θ Lθ = t=1 where θ is a set of model parameters and y<t = {y1 , · · · , yt−1 } is the translation history. Given the training set D = {XM , YM } with M sentence pairs, the training objective is to maximize the loglikelihood of the training data as: Sequence-Level Training for Autoregressive NMT T X (4) ∇θ log(p(yt |y<t , X, θ)) · r(Y)]. − E[ Y t=1 Current reinforcement learning (RL) methods are designed for autoregressive models. Moreover, previous investigations (Wu et al., 2018; Weaver and Tao, 2013) show that the RL-based training procedure is unstable due to its high variance of gradient estimation. 3014 2.3 Non-Autoregressive Neural Machine Translation Non-autoregressive neural machine translation (Gu et al., 2017a) is proposed to accelerate the decoding process, which can simultaneously generate target words by discarding the autoregressive mechanism. The translation probability from X to Y is modeled as follows: P (Y |X, θ) = T Y p(yt |X, θ). (5) t=1 Given the training set D = {XM , YM } with M sentence pairs, the training objective is to maximize the log-likel"
P19-1288,1983.tc-1.13,0,0.575939,"Missing"
P19-1288,N18-1122,0,0.0997661,"Missing"
P19-1288,P18-1166,0,0.0194701,"liminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradi"
P19-1288,D18-1460,1,0.730675,"liminate the multimodality in the training corpus. Li et al. (2018) further proposed to improve non-autoregressive models through distilling knowledge from intermediary hidden states and attention weights of autoregressive models. Apart from non-autoregressive translation, there are works toward speeding up the translation from other perspectives. Wang et al. (2018) proposed the semi-autoregressive Transformer that generates a group of words in parallel at each time step. Press and Smith (2018) proposed the eager translation model that does not use the attention mechanism and has low latency. Zhang et al. (2018a) proposed the average attention network to accelerate decoding, which achieves significant speedup over the uncached Transformer. Zhang et al. (2018b) proposed cube pruning to speedup the beam search for neural machine translation without damaging the translation quality. Sequence-level training techniques have been widely explored in autoregressive neural machine translation, where most works (Ranzato et al., 2015; Shen et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) relied on reinforcement learning (Williams, 1992; Sutton et al., 2000) to build the gradi"
Q16-1027,buck-etal-2014-n,0,0.112276,"Missing"
Q16-1027,D14-1179,0,0.13632,"Missing"
Q16-1027,W14-3309,0,0.0984371,"can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source se"
Q16-1027,P15-1001,0,0.423363,"two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (Luong et al., 2015; Jean et al., 2015). However, a single neural model of either of the above types has not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on the WMT’14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) achieves 37.0. We focus on improving the single model perfor371 Transactions of the Association for Computational Linguistics, vol. 4, pp. 371–383, 2016. Action Editor: Holger Schwenk. Submission batch: 1/2016; Revision batch: 4/2016; Published 7/2016. c 20"
Q16-1027,D13-1176,0,0.134234,".2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies"
Q16-1027,N03-1017,0,0.0228491,"0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT’14 English-to-German task. 1 Introduction Neural machine translation (NMT) has attracted a lot of interest in solving the machine translation (MT) problem in recent years (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Unlike conventional statistical machine translation (SMT) systems (Koehn et al., 2003; Durrani et al., 2014) which consist of multiple separately tuned components, NMT models encode the source sequence into continuous representation space and generate the target sequence in an end-to-end fashon. Moreover, NMT models can also be easily adapted to other tasks such as dialog systems (Vinyals and Le, 2015), question answering systems (Yu et al., 2015) and image caption generation (Mao et al., 2015). In general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network r"
Q16-1027,N06-1014,0,0.0113091,"e+PosUnk Ensemble Ensemble+PosUnk Durrani, 2014 Ensemble+PosUnk Data 36M 36M 36M 36M 36M 36M 36M Voc 80K 80K 80K 80K 80K Full 80K BLEU 36.3 37.7 39.2 38.9 40.4 37.0 37.5 Table 8: BLEU scores of different models. The first two blocks are our results of two single models and models with post processing. In the last block we list two baselines of the best conventional SMT system and NMT system. Second, we recover the unknown words in the generated sequences with the Positional Unknown (PosUnk) model introduced in (Luong et al., 2015). The full parallel corpus is used to obtain the word mappings (Liang et al., 2006). We find this method provides an additional 1.5 BLEU points, which is consistent with the conclusion in Luong et al. (2015). We obtain the new BLEU score of 39.2 with a single Deep-Att model. For the ensemble models of Deep-Att, the BLEU score rises to 40.4. In the last two lines, we list the conventional SMT model (Durrani et al., 2014) and the previous best neural models based system Enc-Dec (Luong et al., 2015) for comparison. We find our best score outperforms the previous best score by nearly 3 points. 380 5 4 7 8 12 17 22 Sentences by Length 28 35 79 Figure 3: BLEU scores vs. source seq"
Q16-1027,P15-1002,0,0.151765,"general, there are two types of NMT topologies: the encoder-decoder network (Sutskever et al., 2014) and the attention network (Bahdanau et al., 2015). The encoder-decoder network represents the source sequence with a fixed dimensional vector and the target sequence is generated from this vector word by word. The attention network uses the representations from all time steps of the input sequence to build a detailed relationship between the target words and the input words. Recent results show that the systems based on these models can achieve similar performance to conventional SMT systems (Luong et al., 2015; Jean et al., 2015). However, a single neural model of either of the above types has not been competitive with the best conventional system (Durrani et al., 2014) when evaluated on the WMT’14 English-to-French task. The best BLEU score from a single model with six layers is only 31.5 (Luong et al., 2015) while the conventional method of (Durrani et al., 2014) achieves 37.0. We focus on improving the single model perfor371 Transactions of the Association for Computational Linguistics, vol. 4, pp. 371–383, 2016. Action Editor: Holger Schwenk. Submission batch: 1/2016; Revision batch: 4/2016; Pu"
Q16-1027,P15-1109,1,0.824274,"is the dropout operation (Hinton et al., 2012) which randomly sets an element of h to zero with a certain probability. The use of Half(·) is to reduce the parameter size and does not affect the performance. We observed noticeable performance degradation when using only the first third of the elements of “f”. ftk = Wfk · [Half(ftk−1 ), Dr(hk−1 )], k &gt; 1 (6) t With the F-F connections, we build a fast channel to propagate the gradients in the deep topology. F-F connections can accelerate the model convergence and while improving the performance. A similar idea was also used in (He et al., 2016; Zhou and Xu, 2015). Encoder: The LSTM layers are stacked following Eq. 5. We call this type of encoder interleaved bidirectional encoder. In addition, there are two similar columns (a1 and a2 ) in the encoder part. Each column consists of ne stacked LSTM layers. There is no connection between the two columns. The first layers of the two columns process the word representations of the source sequence in different directions. At the last LSTM layers, there are two groups of vectors representing the source sequence. The group size is the same as the length of the input sequence. Interface: Prior encoder-decoder mo"
W13-5708,C10-1011,0,0.109851,"Missing"
W13-5708,J08-4003,0,0.015643,"iments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the queue and the top word of the stack. We adopt the arc-standard system (Nivre, 2008), whose actions include: about an exponential number of semantically neighbouring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. The general process of neural language model based word embedding is as follows: • associate with each word in the vocabulary a distributed word feature vector (a real valued vector in Rm ); • express the joint probability function of word sequences in terms of the feature vectors of these words in the sequence; and, • sh"
W13-5708,W09-2307,0,0.0605908,"Missing"
W13-5708,Q13-1025,0,0.0225926,"Missing"
W13-5708,P12-2003,0,0.0222484,"Missing"
W13-5708,Q13-1012,0,0.0292666,"Missing"
W13-5708,P10-1110,0,0.397686,"who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the queue and the top word of the stack. We ado"
W13-5708,P08-1068,0,0.113017,"a correct tree if there are out-of-vocabulary (OOV) words (compared to the training data of the treebank) and/or the POS-tags are wrongly annotated? Words need to be generalized to solve this problem in a sense. Indeed, POS-tag itself is a way to generalize words into word classes. This is because POS-taggers can be trained on larger-scale data compared with treebanks. Annotating trees is far more difficult than annotating POS-tags. Considering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templat"
W13-5708,W02-2016,0,0.0433687,"is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the to"
W13-5708,C12-1103,0,0.287843,"Missing"
W13-5708,P13-2020,0,0.0212508,"Missing"
W13-5708,P10-1040,0,0.141137,"onsidering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 200"
W13-5708,N13-1063,1,0.760145,"ared with treebanks. Annotating trees is far more difficult than annotating POS-tags. Considering that unsupervised word clustering methods can make use of TB/PB-level Web data, these approaches have been shown to be helpful for dependency parsing (Koo et al., 2008). In this paper, we investigate the influence of generalization of words to the accuracies of Chinese dependency parsing. Specially, in our shift-reduce parser, we use a neural language model based word embedding method (Bengio et al., 2003) to generate distributed word feature vectors and then perform Kmeans based word clustering (Yu et al., 2013) to generate word classes. Our usage of word embedding is in line with Turian et al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce"
W13-5708,D08-1059,0,0.579374,"tion of word sequences in terms of the feature vectors of these words in the sequence; and, • shift, which removes the top word in the queue and pushes it onto the top of the stack; • left-arc, which pops the top item off the stack, and adds it as a modifier to the front of the queue; • right-arc, which removes the front of the queue, and adds it as a modifier to the top of the stack. In addition, the top of the stack is popped and added to the front of the queue. We follow Kudo and Matsumoto (2002) and use the Support Vector Machines (SVMs) for action classification training and beam search (Zhang and Clark, 2008) for decoding. 2 Neural Language Model Based Word Embedding Following (Bengio et al., 2003), we use a neural network with two hidden layers to learn distributed word feature vectors from large-scale training data. Recall that, the goal of statistical language modelling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training (so called OOV words and/or sequences). Ngram based a"
W13-5708,D12-1030,0,0.0295383,"Missing"
W13-5708,P11-2033,0,0.0280779,"periments. 3 Feature Templates At each step during shift-reducing, a parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;. We denote the top of stack with S0 , the front items from the queue with N0 , N1 , N2 , and N3 , the leftmost and rightmost modifiers of S0 (if any) with S0l and S0r , respectively, and the leftmost modifier of N0 (if any) with N0l (refer to Figure 1). The baseline feature templates without any word class level information (such as POS-tags) are shown in Table 1. These features are mostly taken from Zhang and Clark (2008), Huang and Sagae (2010), and Zhang and Nivre (2011). In this table, w, l and d represents the word, dependency label, and the distance between S0 and N0 , respectively. For example, S0 wN0 w represents the feature template that takes the word of S0 , and combines it with the word of N0 . In Table 1, (S/N )0l2 , (S/N )0r2 , and (S/N )0rn refer to the second leftmost modifier, the second rightmost modifier, and the right nearest modifier of (S/N )0 , respectively. It should be mentioned that, for the arc-standard algorithm used in this paper, S0 or N0 never contain a head word. The reason is that, once the head is found for a node, that node wil"
W13-5708,E06-1011,0,0.055922,"Missing"
W13-5708,nivre-etal-2006-maltparser,0,0.11463,"Missing"
W13-5708,W03-3017,0,0.058527,"al. (2010) and Yu et al. (2013), who study the effects of different clustering algorithms for POS tagging and named entity recognition (NER). We designed feature templates by making use of words, POS tags, CPOS tags, NLMWEbased word classes and their combinations. NLMWEbased word classes is shown to be an important supplement of POS-tags. Experiments on a Query treebank, CTB5 and CTB7 show that the combinations of features from CPOS-tags, POS-tags, and NLMWEbased word classes yield the best UASs. 1.1 Shift-reduce parsing We use a transition-based shift-reduce parser (Kudo and Matsumoto, 2002; Nivre, 2003; Nivre et al., 2006; Huang and Sagae, 2010) to 73 perform all the experiments in this paper. In a typical transition-based parsing process, the input words are stored in a queue and partially built dependency structures (e.g., sub-trees) are organized by a configuration (or state). A parser configuration (or, state) can be represented by a tuple &lt; S, N, A &gt;, where S is the stack, N is the queue of incoming words, and A is the set of dependency arcs that have been built. A set of shift-reduce actions are defined, which are used to construct new dependency arcs by connecting the top word of the"
