2016.gwc-1.22,W14-0130,1,0.822236,"kta as a separate character before search. 3.3.2 Morphological Analysis Before searching in the databases the word is first passed to a morphological analyzer to obtain its root form. We use Hindi Morph Analyzer (Bahuguna et al., 2014) to return the root form of the input word for Hindi language, since by principle, WordNet only contains root forms of the words. Due to non availability of other language Morphological Analyzers, we may not be able to include them in the search process. Though, in the future, we can use a fully automated version of the “Human mediated TRIE base generic stemmer”(Bhattacharyya et al., 2014) for obtaining root forms for other languages later. 3.3.3 Handling Multiple Root forms Figure 2: Devanagari Keyboard tion using Google Transliteration API5 , and a JavaScript based online keyboard (Figure: 2) for input of Hindi Unicode characters. Transliteration for a native user is very convenient. In case, the user does not know the right combination of keys then the keyboard for Devanagari is provided. These two methods ensure that all words can be easily entered for searching. Thereafter, by touching / clicking on “Search”, the synsets with all relevant information are retrieved. 3.3 Sea"
2016.gwc-1.23,W09-3401,0,0.0240783,"Princeton WordNet which form the basis of our query for the OpenClipArt API. We download the images via their URLs, and store them locally, to map them to Hindi WordNet5 (Dipak Narayan and Bhattacharyya, 2002) synset IDs later. The paper is organized as follows. In section 2, we describe our related work. In section 3 and 4, we describe our architecture, and the retrieval procedure along with the scoring algorithm. We describe the results obtained in Section 5. We describe the evaluation tool and qualitative analysis in sections 6 and 7, respectively. We conclude in section 8. 2 Related Work Bond et al. (2009) used OCAL to enhance the Japanese WordNet, and were able to mine 874 links for 541 synsets. On the basis of manual scoring they found 62 illustrations which were best suited for the sense, 642 illustrations to be a good representation, and 170 suitable, but imperfect illustrations. We extend their work for IndoWordNet, and use OCAL to mine the illustrations. Imagenet(Deng et al., 2009) is a similar project for Princeton WordNet which provides images/URLs for a concept. It contains 21841 synsets indexed with 14,197,122 images. We present a much simpler methodology of collecting images from the"
2016.gwc-1.57,D09-1048,1,0.758561,"Missing"
2018.gwc-1.31,bhattacharyya-2010-indowordnet,1,0.702734,"Italian, Spanish, German, French, Czech and Estonian. Each of these wordnets is structured in the same way as the Princeton WordNet for English (Miller et al., 1990) - synsets (sets of synonymous words) and semantic relations between them. Each wordnet separately captures a language-specific information. In addition, the wordnets are linked to an Inter-Lingual-Index, which uses Princeton WordNet as a base. This index enables one to go from concepts in one language to similar concepts in any other language. Such features make this resource helpful in crosslingual NLP applications. IndoWordNet (Bhattacharyya, 2010) is a linked wordnet comprising of wordnets for major Indian languages, viz, Assamese, Bengali, Bodo, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Sanskrit, Tamil, Telugu, and Urdu. These wordnets have been created using the expansion approach using Hindi WordNet as a pivot, which is partially linked to English WordNet. Previously, Joshi et al. (2012a) come up with a heuristic based measure where they use bilingual dictionaries to link two wordnets. They combine scores using various heuristics and generate a list of potential candidates for"
2018.gwc-1.31,W98-0705,0,0.247819,"ts. 1 Introduction Wordnets (Fellbaum, 1998) have been useful in different Natural Language Processing applications such as Word Sense Disambiguation (TufiS¸ et al., 2004; Sinha et al., 2006), Machine Translation (Knight and Luk, 1994) etc. Linked Wordnets are extensions of wordnets. In addition to language specific information captured in constituent wordnets, linked wordnets have a notion of an interlingual index, which connects similar concepts in different languages. Such linked wordnets have found their application in machine translation (Hovy, 1998), cross-lingual information retrieval (Gonzalo et al., 1998), etc. Given the extensive application of wordnets in different NLP applications, maintenance of wordnets involves expert involvement. Such involvement is costly both in terms of time and resources. This is further amplified in case of linked wordnets, where experts need to have knowledge of multiple languages. Thus, techniques that can help reduce the effort needed by experts are desirable. Recently, deep learning has been extremely successful in a wide array of NLP applications. This is primarily due to the development of word embeddings, which have become a crucial component in modern NLP."
2018.gwc-1.31,P12-1092,0,0.0480324,"linked wordnets, where experts need to have knowledge of multiple languages. Thus, techniques that can help reduce the effort needed by experts are desirable. Recently, deep learning has been extremely successful in a wide array of NLP applications. This is primarily due to the development of word embeddings, which have become a crucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information"
2018.gwc-1.31,C12-3030,1,0.772856,"Missing"
2018.gwc-1.31,D16-1104,1,0.828077,"ucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information about the structure of embedding spaces of different languages and the relation among them be used to aid linking of corresponding wordnets?” We demonstrate that this is true at least in the case of English and Hindi WordNets. We propose an approach to link them using word embeddings. Given a synset of the source language, the approa"
2018.gwc-1.31,D14-1181,0,0.00254133,"rily due to the development of word embeddings, which have become a crucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information about the structure of embedding spaces of different languages and the relation among them be used to aid linking of corresponding wordnets?” We demonstrate that this is true at least in the case of English and Hindi WordNets. We propose an approach to lin"
2018.gwc-1.31,P14-2050,0,0.0387504,"e knowledge of multiple languages. Thus, techniques that can help reduce the effort needed by experts are desirable. Recently, deep learning has been extremely successful in a wide array of NLP applications. This is primarily due to the development of word embeddings, which have become a crucial component in modern NLP. They are learned in an unsupervised manner from large amounts of raw corpora. Bengio et al. (2003) were the first to propose neural word embeddings. Many word embedding models have been proposed since then (Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013c; Levy and Goldberg, 2014). They have been efficiently utilized in many NLP applications: Part of Speech Tagging (Collobert and Weston, 2008), Named Entity Recognition (Collobert and Weston, 2008), Sentence Classification (Kim, 2014), Sentiment Analysis (Liu et al., 2015), Sarcasm Detection (Joshi et al., 2016) Mikolov et al. (2013a) made a particularly interesting observation about the structure of the embedding space of different languages. They noted that there is a linear mapping between such spaces. In this paper, we address the following question: “Can information about the structure of embedding spaces of differ"
2018.gwc-1.31,D15-1168,0,0.0741146,"Missing"
2018.gwc-1.31,2016.gwc-1.57,1,0.71929,"net comprising of wordnets for major Indian languages, viz, Assamese, Bengali, Bodo, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Sanskrit, Tamil, Telugu, and Urdu. These wordnets have been created using the expansion approach using Hindi WordNet as a pivot, which is partially linked to English WordNet. Previously, Joshi et al. (2012a) come up with a heuristic based measure where they use bilingual dictionaries to link two wordnets. They combine scores using various heuristics and generate a list of potential candidates for linked synsets. Singh et al. (2016) discuss a method to improve the current status of Hindi-English linkage and present a generic methodology i.e., manually creating bilingual mappings for concepts which are unavailable in either of the languages or not present as a synset in the target wordnet. Their method is beneficial for culture-specific synsets, or for non-existing concepts; but, it is cost and time inefficient, and requires a lot of manual effort on the part of a lexicographer. Our approach is mainly geared towards reducing effort on the part of the lexicographers. 3 Problem Statement Given wordnets of two different lang"
2018.gwc-1.31,C04-1192,0,0.177455,"Missing"
2018.gwc-1.37,J90-1003,0,0.219729,"uction A Wordnet is a large digital lexical database of a language in which information is organised around cognitive synonym sets or synsets (Fellbaum, 1998). The underlying basis of such organization are the word association studies in psycholinguistics, which proved that our mental lexicon is structured on associations, i.e. an appear1 http://www.cfilt.iitb.ac.in/wordnet/webhwn/index.php ance of one entity entails the appearance of the other in the mind. Thus, it was found that subjects respond quicker than normal to the word ‘nurse’ if it follows a highly associated word such as ‘doctor’ (Church and Hanks, 1990). This property of the mental lexicon is structurally built in the Wordnets and manifests itself in the lexical and semantic relations which are encoded in it. Thus, a Wordnet is a ready resource of vocabulary of a language, which captures associative learning in its structure. Conventional sources of vocabulary learning, such as the dictionaries and thesauri, do not have these relations due to the very nature of their composition. This is the motivation to present Hindi Wordnet as a tool for vocabulary learning and teaching. The second motivation is the fact that education is undergoing rapid"
2018.gwc-1.37,2016.gwc-1.23,1,0.68317,"ich is perceived only through eyes). Now, रंग (raMga, colour) is such an everyday word that it was quite tough to find an easy-tounderstand definition for it, hence its English translation (कलर / colour) has been provided. The English word is highly in use in the daily language and also occurs frequently in written form too, so it could be readily added to the Hindi wordnet data, thus solving the issue of all such words. 5.3 Picture depiction As rightly mentioned in a famous idiom ‘a picture is worth a thousand words’, a complex concept can be easily explained by a picture or an illustration. Kanojia et al. (2016) tried to automatically collect images for IndoWordNet5 , but due to the lack of tagged images openly available for use, enough images could not be collected. In Hindi Wordnet, there are several concepts which are hard to explain using the gloss. For example, the concept of a word ‘milk’ in Hindi is explained as वह सफेद तरल पदाथर जो सतनपायी जीव क मादा के सतन से िनकलता है (vaha sapheda tarala padaartha jo stanapaayii jiivoM kii maadaa ke stanoM se nikalataa hai, a white nutritious liquid secreted by mammals and used as food by human beings). This gloss seems to be difficult for level 1 and 2"
2018.gwc-1.37,2016.gwc-1.46,1,0.740729,"marking is carried out during word collection process as gloss is not simplified for higher levels. During the process, each word is marked with the grammatical properties corresponding to its POS category. Some of the grammatical features are as follows: Nouns are either countable or uncountable. They can belong to any of these categories: Proper Noun, Abstract Noun, Common Noun, Collective Noun. When a noun is a compound, it may belong to one of these categories: ततपु ष (tatpuruSha), कमर धारय (karmadhaaraya), ि गु (dvigu), अवययीभाव avyayiibhaava), ं (dvaMdva), or बहवरीही (bahuvriihii) (Redkar et al., 2016). The Verbs are either Transitive or Intransitive. The different types of verbs are: Simple verb, Conjunct verb, and Compound Verb. These verbs may also be Causative verb. Kinds of Adverbs that feature in this tool are of Manner, Place, Time and Quantity. Similarly, the Adjectives are categorized as Qualitative,Numeral, Quantitative, Pronominal. 5.5 Audio pronunciation Cognitive theories of multimedia learning (Mayer, 2002) indicate that audio cues are effective aids in a learning scenario, and also help in retaining the material learned (Bajaj et al., 2015). To help in more effective learning"
2018.gwc-1.47,W14-0111,0,0.0219074,"WordNet Library4 has been extensively used for research across various domains in NLP (Chauhan et al., 2013; Zesch et al., 2008; Gurevych et al., 2012). extJWNL5 extend JWNL and provides command-line support, and Maven6 support among many other features in their API. Emerging WordNets like Sinhala WordNet (Welgama et al., 2011) employ JWNL to create an API for their WordNet. Java API for WordNet Searching (JAWS) (Spell, 2009) is another such implementation. The MIT Java WordNet Interface (JWI)7 is also available for the same purposes and is available under the Creative Commons 4.0 License8 . Finlayson (2014) presents an extensive evaluation of the APIs available in Java for accessing Princeton WordNet. All of the work above has been done for Java, and is available for Princeton WordNet. A Python based toolkit, ESTNLTK (Orasmaa et al., 2016) includes Esto3 http://www.nltk.org/ http://jwordnet.sourceforge.net/handbook. html 5 http://extjwnl.sourceforge.net/ 6 https://maven.apache.org/ 7 https://projects.csail.mit.edu/jwi/ 8 https://creativecommons.org/licenses/by/4. 0/ 4 Figure 1: Basic flow of the pyiwn API nian WordNet developed under the EuroWordNet project (Vossen, 1998). Previously, efforts ha"
2018.gwc-1.47,W98-0705,0,0.105705,"d example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development. 1 Introduction WordNets are extensively used in many sub-tasks for Natural language Processing (NLP) (Knight and Luk, 1994; TufiŞ et al., 2004). They are a rich semantic lexicon which are accessible, freeto-use and fairly accurate. They have been used in cross-lingual information retrieval (Gonzalo et al., 1998), word sense disambiguation (Sinha et al., 2006), question answering (Pasca and Harabagiu, 2001) etc. Princeton WordNet (Fellbaum, 1998) or the English WordNet was the first to come into existence. EuroWordNet (Vossen, 1998) followed with a common structure for 12 European languages. Indian language WordNets originated with the advent of Hindi WordNet (Narayan et al., 2002) and based on an expansion approach, the rest of them were created. They form a common lexico-semantic resource called IndoWordNet (IWN) (Bhattacharyya, 2010). India has more than 22 languages and 18 of these have constituen"
2018.gwc-1.47,E12-1059,0,0.0213414,"PI with an aim that IndoWordNet data should also readily available in an easy-to-use framework. Python facilitates prebuilt libraries and datasets for NLP via NLTK. TensorFlow by Google (Abadi et al., 2016) is also built on Python, and other classic Machine Learning algorithms are available for use via the sci-kit learn (sklearn) library (Pedregosa et al., 2011). Hence, we choose Python for implementing the API and build a framework using it. 3 Related Work The Java WordNet Library4 has been extensively used for research across various domains in NLP (Chauhan et al., 2013; Zesch et al., 2008; Gurevych et al., 2012). extJWNL5 extend JWNL and provides command-line support, and Maven6 support among many other features in their API. Emerging WordNets like Sinhala WordNet (Welgama et al., 2011) employ JWNL to create an API for their WordNet. Java API for WordNet Searching (JAWS) (Spell, 2009) is another such implementation. The MIT Java WordNet Interface (JWI)7 is also available for the same purposes and is available under the Creative Commons 4.0 License8 . Finlayson (2014) presents an extensive evaluation of the APIs available in Java for accessing Princeton WordNet. All of the work above has been done for"
2018.gwc-1.47,L16-1390,0,0.0526102,"Missing"
2018.gwc-1.47,W12-5022,0,0.0177519,"g Princeton WordNet. All of the work above has been done for Java, and is available for Princeton WordNet. A Python based toolkit, ESTNLTK (Orasmaa et al., 2016) includes Esto3 http://www.nltk.org/ http://jwordnet.sourceforge.net/handbook. html 5 http://extjwnl.sourceforge.net/ 6 https://maven.apache.org/ 7 https://projects.csail.mit.edu/jwi/ 8 https://creativecommons.org/licenses/by/4. 0/ 4 Figure 1: Basic flow of the pyiwn API nian WordNet developed under the EuroWordNet project (Vossen, 1998). Previously, efforts had been made to create an API for IndoWordNet but they are not Python based. Prabhugaonkar et al. (2012) describe a two-layered architecture of a web-based API created using PHP. It requires one to download data separately and is inconvenient to deploy; we also come across hard-coded paths while trying to deploy their API. A Java-based API9 is available for download on the Hindi WordNet web interface, and also requires one to separately download the database for Hindi WordNet. Redkar et al. (2016) claim to have built an API for WordNets universally but their work is not publicly accessible, and no references to their implementation could be found. Hence, we work on an API which would contain NLT"
2018.gwc-1.47,C04-1192,0,0.0229762,"ke core functionalities in Python. Additionally, we use a pre-built speech synthesis system for Hindi language and augment Hindi data with audios for words, glosses, and example sentences. We provide a detailed usage of our API and explain the functions for ease of the user. Also, we package the IndoWordNet data along with the source code and provide it openly for the purpose of research. We aim to provide all our work as an open source framework for further development. 1 Introduction WordNets are extensively used in many sub-tasks for Natural language Processing (NLP) (Knight and Luk, 1994; TufiŞ et al., 2004). They are a rich semantic lexicon which are accessible, freeto-use and fairly accurate. They have been used in cross-lingual information retrieval (Gonzalo et al., 1998), word sense disambiguation (Sinha et al., 2006), question answering (Pasca and Harabagiu, 2001) etc. Princeton WordNet (Fellbaum, 1998) or the English WordNet was the first to come into existence. EuroWordNet (Vossen, 1998) followed with a common structure for 12 European languages. Indian language WordNets originated with the advent of Hindi WordNet (Narayan et al., 2002) and based on an expansion approach, the rest of them"
2018.gwc-1.49,2016.gwc-1.23,1,0.701514,"Indian Languages have also emerged in the recent past (Patil et al., 2013). Although these systems, which are already available, do not produce the most “natural” sounding output, but they are usable to an extent. Manual evaluations of the speech synthesis systems built for the Hindi Language show that there is still a need for better text processing and additional phonetic coverage (Kishore et al., 2003; Raj et al., 2007). Bengu et al. (2002) create an online context sensitive dictionary using Princeton WordNet and implement a Java based speech interface for the Text-to-Speech (TTS) engine. Kanojia et al. (2016) automatically collect images for IndoWordNet and augment them to the web interface, but due to the lack of tagged images openly available for use, they do not collect enough images. To the best of our knowledge, there has been no other work specifically in the direction of synthesizing audio for WordNet words or Synthesizing audio for Indian Language WordNets. 3 Our Approach Among the three main sub-types of concatenative synthesis, we choose to perform unit selection synthesis and build cluster units of the speech data recorded by a human voice. We use the Festival system to create a synthet"
2019.gwc-1.51,N09-3008,0,0.664538,"es (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). We also consider the method of Ciobanu and Dinu (2014), which employs dynamic programming based methods for sequence alignment. Among cognate sets common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007), XDice (Brew et al., 1996) or TF-IDF (Wu et al., 2008) could be used to measure similarities and validate the members of the set. 3 Datasets and Methodology We investigate language pairs for major Indian languages namely Marathi (Mr), Gujarati (Gu), Bengali (Bn), Punjabi (Pa), Sanskrit (Sa), Malayalam (Ml), Tamil (Ta), Telugu (Te), Nepali (Ne) and Urdu (Ur) with Hi"
2019.gwc-1.51,P14-2017,0,0.858587,"ide substantial proof that automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical similarity of word pairs and have been used extensively to detect cognates (Ciobanu and Dinu, 2014; Mulloni, 2007; Inkpen et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researchers have previously used consonant class matching (CCM) (Turchin et al., 2010), sound class-based alignment (SCA) (List, 2010) etc. to detect cognates in multilingual wordlists. The identification of cognates, here, is based on the comparison of words sound correspondences. Semantic similarity methods have also been deployed to detect cog"
2019.gwc-1.51,P15-2071,0,0.548536,"d segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). We also consider the method of Ciobanu and Dinu (2014), which employs dynamic programming based methods for sequence alignment. Among cognate sets common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007), XDice (Brew et al., 1996) or TF-IDF (Wu et al., 2008) could be used to measure similarities and validate the members of the set. 3 Datasets and Methodology We investigate language pairs for major Indian languages namely Marathi (Mr), Gujarati (Gu), Bengali (Bn), Punjabi (Pa), Sanskrit (Sa), Malayalam (Ml), Tamil (Ta), Telugu (Te"
2019.gwc-1.51,E17-1113,0,0.16851,"Missing"
2019.gwc-1.51,jha-2010-tdil,0,0.092144,"all words, in the concept space, in a comma-separated format. We, then, create word lists by combining all possible permutations of word pairs within each synset. For e.g., If synset ID X on the source side (Hindi) contains words S1 W1 and S1 W2 , and parallelly on the target side (other Indian languages), synset ID X contains T1 W1 and T1 W2 , we create a word list such as: S1 W1 , T1 W1 S1 W2 , T1 W1 S1 W1 , T1 W2 S1 W2 , T1 W2 To avoid redundancy, we remove duplicate word pairs from this list. Dataset 2: Parallel Corpora based dataset We use the ILCI parallel corpora for Indian languages (Jha, 2010) and create word pairs list by comparing all words in the source side sentence with all words on the target side sentence. Our hypothesis, here, is that words with high orthographic similarity which occur in the same context window (a sentence) would be cognates with a high probability. Due to the unavailability of ILCI parallel corpora for Sa and Ne, we download these corpora from Wikipedia and align it with the Hindi articles from Hindi Wikipedia. We calculate exact word matches to align articles to each other thus creating comparable corpora and discard unaligned lines from both sides. We,"
2019.gwc-1.51,mulloni-pekar-2006-automatic,0,0.565835,"ds, such as machine translation and bilingual terminology compilation. For e.g., the German - English cognates, Blume - bloom can be identified as cognates with orthographic similarity methods. Detection of cognates helps various NLP applications like IR (Pranav, 2018). Rama et al. (2018) study various cognate detection techniques and provide substantial proof that automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical similarity of word pairs and have been used extensively to detect cognates (Ciobanu and Dinu, 2014; Mulloni, 2007; Inkpen et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researche"
2019.gwc-1.51,P07-3005,0,0.0465822,"at automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical similarity of word pairs and have been used extensively to detect cognates (Ciobanu and Dinu, 2014; Mulloni, 2007; Inkpen et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researchers have previously used consonant class matching (CCM) (Turchin et al., 2010), sound class-based alignment (SCA) (List, 2010) etc. to detect cognates in multilingual wordlists. The identification of cognates, here, is based on the comparison of words sound correspondences. Semantic similarity methods have also been deployed to detect cognates among wor"
2019.gwc-1.51,W97-1102,0,0.859426,"this threshold to 0.5 for both datasets2 . Using 0.5 as threshold, we obtained the best training performance and hence chose to use this as the threshold for similarity calculation. The various similarity measures used are described in the next subsection. 3.4 Similarity Measures Normalized Edit Distance Method (NED) We calculate similarity scores for each word on the source side i.e., Hi by matching it with each word on the target side i.e., Sa, Bn, Gu, Pa, Mr, Ml, Ne, Ta, Te, and Ur. Since we match the words from the same conThe Normalized Edit Distance approach computes the edit distance (Nerbonne and Heeringa, 1997) for all word pairs in a synset/concept and then provides the output of probable cognate sets with distance and similarity scores. We assign labels for these sets based on the similarity score obtained from the NED method, where the similarity score is (1 - NED score). It is usually defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite). The score is normalized such that 0 equates to no similarity and 1 is an exact match. NED is equal to the minimum number of operations required to transform 1 htt"
2019.gwc-1.51,A00-2038,0,0.692763,"n of cognates for improving IR has already been explored for Indian languages (Makin et al., 2007). String similarity-based methods are often used as baseline methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usu"
2019.gwc-1.51,P18-3019,0,0.0288967,"m as well. 1 Introduction Cognates are words that have a common etymological origin (Crystal, 2008). They account for a considerable amount of unique words in many lexical domains, notably technical texts. The orthographic similarity of cognates can be exploited in different tasks involving recognition of translational equivalence between words, such as machine translation and bilingual terminology compilation. For e.g., the German - English cognates, Blume - bloom can be identified as cognates with orthographic similarity methods. Detection of cognates helps various NLP applications like IR (Pranav, 2018). Rama et al. (2018) study various cognate detection techniques and provide substantial proof that automatic cognate detection can help infer phylogenetic trees. In many NLP tasks, the orthographic similarity of cognates can compensate for the insufficiency of other kinds of evidence about the translational equivalency of words (Mulloni and Pekar, 2006). The detection of cognates in compiling bilingual dictionaries has proven to be helpful in Machine Translation (MT), and Information Retrieval (IR) tasks (Meng et al., 2001). Orthographic similarity-based methods have relied on the lexical simi"
2019.gwc-1.51,N01-1014,0,0.656344,"et al., 2005). These methods, generally, calculate the similarity score between two words and use the result to build training data for further classification. Cognate detection can also be performed using phonetic features and researchers have previously used consonant class matching (CCM) (Turchin et al., 2010), sound class-based alignment (SCA) (List, 2010) etc. to detect cognates in multilingual wordlists. The identification of cognates, here, is based on the comparison of words sound correspondences. Semantic similarity methods have also been deployed to detect cognates among word pairs (Kondrak, 2001). The measure of semantic similarity uses the context around both word pairs and helps in the identification of a cognate word pair by looking of similarity among the collected contexts. For our work, we can primarily divide words into four main categories viz. True Cognates, False Cognates, False Friends and NonCognates. In Figure 1, we present this classification with examples from various languages along with their meanings for better understanding. While some false friends are also false cognates, most of them are genuine cognates. Our primary goal is to be able to identify True Cognates."
2019.gwc-1.51,W12-0216,0,0.419883,"ften used as baseline methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Ko"
2019.gwc-1.51,N18-2063,0,0.583483,"Missing"
2019.gwc-1.51,C16-1097,0,0.522609,"methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. Although it performs well on the accuracy, it shows poor results with MRR. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on Orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). We also"
2019.gwc-1.51,J99-1003,0,0.669482,"ravidian languages like Malayalam, Tamil, Telugu, and Kannada borrow many words from Sanskrit. Although recently, Kanojia et al. (2019) perform cognate detection for a few Indian languages, but report results with manual verification of their output. Identification of cognates for improving IR has already been explored for Indian languages (Makin et al., 2007). String similarity-based methods are often used as baseline methods for cognate detection and the most commonly used among them is Edit distance based similarity measure. It is used as the baseline in the early cognate detection papers (Melamed, 1999). Essentially, it computes the number of operations required to transform from source to target cognate. Research in automatic cognate detection using phonetic aspects involves computation of similarity by decomposing phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), phonetic encodings (Rama et al., 2015), aligned segments of transcribed phonemes (List, 2012). We study Rama (2016)’s research, which employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved th"
2020.aacl-main.86,K18-1030,0,0.0211448,"d to a large body of work in psycholinguistic research that shows a relationship between text processing and gaze behaviour. Mishra and Bhattacharyya (2018) also describe some of the ways that eye-tracking can be used for multiple NLP tasks like translation complexity, sentiment analysis, etc. Research has been done on using gaze behaviour at run time to solve downstream NLP tasks like sentence simplification (Klerke et al., 2016), readability (Gonz´alez-Gardu˜no and Søgaard, 2018; Singh 859 et al., 2016), part-of-speech tagging (Barrett et al., 2016), sentiment analysis (Mishra et al., 2018; Barrett et al., 2018; Long et al., 2019), grammatical error detection (Barrett et al., 2018), hate speech detection (Barrett et al., 2018) and named entity recognition (Hollenstein and Zhang, 2019). Different strategies have been adopted to alleviate the need for gaze behaviour at run time. Barrett et al. (2016) use token level averages of gaze features at run time from the Dundee Corpus (Kennedy et al., 2003), to alleviate the need for gaze behaviour at run time. Singh et al. (2016) and Long et al. (2019) predict gaze behaviour at the tokenlevel prior to using it at run time. Mishra et al. (2018), Gonz´alez-Gard"
2020.aacl-main.86,P16-2094,0,0.03146,"e reads, that is what the mind processes. This hypothesis has led to a large body of work in psycholinguistic research that shows a relationship between text processing and gaze behaviour. Mishra and Bhattacharyya (2018) also describe some of the ways that eye-tracking can be used for multiple NLP tasks like translation complexity, sentiment analysis, etc. Research has been done on using gaze behaviour at run time to solve downstream NLP tasks like sentence simplification (Klerke et al., 2016), readability (Gonz´alez-Gardu˜no and Søgaard, 2018; Singh 859 et al., 2016), part-of-speech tagging (Barrett et al., 2016), sentiment analysis (Mishra et al., 2018; Barrett et al., 2018; Long et al., 2019), grammatical error detection (Barrett et al., 2018), hate speech detection (Barrett et al., 2018) and named entity recognition (Hollenstein and Zhang, 2019). Different strategies have been adopted to alleviate the need for gaze behaviour at run time. Barrett et al. (2016) use token level averages of gaze features at run time from the Dundee Corpus (Kennedy et al., 2003), to alleviate the need for gaze behaviour at run time. Singh et al. (2016) and Long et al. (2019) predict gaze behaviour at the tokenlevel prio"
2020.aacl-main.86,D14-1162,0,\N,Missing
2020.aacl-main.86,P13-2062,1,\N,Missing
2020.aacl-main.86,D16-1115,0,\N,Missing
2020.aacl-main.86,D16-1193,0,\N,Missing
2020.aacl-main.86,W17-5050,0,\N,Missing
2020.aacl-main.86,W16-4123,0,\N,Missing
2020.aacl-main.86,P18-1219,1,\N,Missing
2020.coling-main.119,P17-1042,0,0.0201093,"res (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular sim"
2020.coling-main.119,P18-1073,0,0.0179705,"imedia Dumps; as on April 22, 2020 Additional Monolingual Corpus JNU Sanskrit Proses Corpus Indic NLP Library FastText - GitHub 1387 uses the supervised method named MUSE (Conneau et al., 2017)10 which utilizes a manually curated bilingual lexicon11 for alignments. We use Hindi as a pivot language due to the ease of computation and availability of resources (Corpora and WordNet size). We use the monolingual models described above and train 13 cross-lingual word embedding models (thirteen language pairs over 100 dimensions) using this approach. The second cross-lingual methodology uses VecMap (Artetxe et al., 2018), which utilizes the monolingual models created above. VecMap uses an optional normalization feature while it builds the mappings between any two monolingual models. It performs orthogonal transformation and maps semantically related words, similar to MUSE, which was used in our first approach for building cross-lingual models. Additionally, it also reduces the dimensions of the embeddings models, which, is optional. We train it using the same hyperparameters as described above, for consistency while evaluating. We used the supervised approach for training these models as well, and the trainin"
2020.coling-main.119,D16-1162,0,0.0258029,"an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approximately around 1M tokens (Arthur et al., 2016). Our detected cognate list size varies from 930 cognates (Hi-Te) to 15834 (Hi-Mr). Due to the addition of more parallel instances to the corpus, the vocabulary size for NMT increases. Hence, we experiment further by varying the BPE merges, in a close range, to the optimal merge point obtained earlier. We report the results of the best optimal merge setting, for both NMT-BPE Baseline model and the cognate injected NMT-BPE model, in"
2020.coling-main.119,N09-3008,0,0.0125337,"nza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word re"
2020.coling-main.119,Q17-1010,0,0.0429109,"any other script to the Devanagari script. We perform Unicode transliteration using Indic NLP Library8 to convert scripts for Bn, As, Or, Gu, Pa, Ml, Ta, Kn and Te to Devanagari for standardization. Hi, Mr, Ko, Ne, and Sa are already based on the Devanagari script. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size o"
2020.coling-main.119,bojar-etal-2014-hindencorp,0,0.028968,"Missing"
2020.coling-main.119,D18-2029,0,0.0251243,"l dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular similarity distinguishes nearly parallel vectors much better as small changes in vector values yield considerable distances. For each word pair vector and its context vectors, we compute the ‘word-pair similarity’ and ‘contextual similarity’. We use arccos to obtain angular cosine similarity (asim) among vectors ‘u’ and ‘v’, as shown below:     u.v asim(u, v) = 1 − arccos /π (3) kukkvk Each candidate word-pair generates a score i.e., score1, and the average of scores among all words in the context dictionary generates another score i.e., s"
2020.coling-main.119,P14-2017,0,0.106733,"a. Link: Data, code and models This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information"
2020.coling-main.119,P15-2071,0,0.0199277,"(Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation e"
2020.coling-main.119,P19-4007,0,0.0420076,"Missing"
2020.coling-main.119,D18-1027,0,0.0152337,"e two similarity scores (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and thei"
2020.coling-main.119,E17-1113,0,0.0425306,"Missing"
2020.coling-main.119,jha-2010-tdil,0,0.0142001,"Missing"
2020.coling-main.119,N10-1103,0,0.043284,"be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or"
2020.coling-main.119,2019.gwc-1.51,1,0.808977,", Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic simil"
2020.coling-main.119,2020.lrec-1.378,1,0.798533,"ages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 17021 15726 14097 21710 9235 3363 936 3478 4103 11894 2560 11295 Non-Cognates 16119 15057 15983 15166 23029 8976 4005 1084 4101 3810 13027 1918 9826 Table 1: Number of cognates and non-cognates for each language pair in the dataset. Hi-Ne* and Hi-Ko* were generated via replicating their approach (Kanojia et al., 2020). Language Hi Bn Gu Mr Pa Sa Ml Ta Te Ne As Kn Ko Or Corpus Size 48142K 1564K 439K 520K 505K 553K 495K 909K 1023K 706K 504K 159K 214K 744K STTR (n=1000) 0.5821 0.5437 0.4587 0.6108 0.4314 0.5350 0.7339 0.6411 0.4950 0.4883 0.5968 0.5338 0.5614 0.4160 Table 2: Corpus Statistics where corpus size is the approximate number of lines, and STTR is the moving average type-token ratio on a windows of 1000 sentences. 3 Dataset and Experimental Setup In this section, we describe our primary dataset for the cognate detection task. We also describe the datasets used for building cross-lingual word embeddi"
2020.coling-main.119,P17-4012,0,0.0362403,"once the learning rate falls below 0.001. We perform our experiments with the feature sets (Orthographic (WLS), Phonetic (PVS), and three different cross-lingual embeddings based feature sets) described above for all the thirteen language pairs. We also perform an ablation test with various feature sets and report the results for the best feature combination in the next section. The results of our classification task can be seen in Table 3 and are discussed in the next section, in detail. 4.5 Cognate-aware Neural Machine Translation (NMT) Task For the NMT task, we use the OpenNMT-Py toolkit (Klein et al., 2017) to perform our experiments. We use a Bidirectional RNN Encoder-Decoder architecture with attention (Bahdanau et al., 2014). We choose three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best bas"
2020.coling-main.119,N03-2016,0,0.112806,"ginated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in French can translate to"
2020.coling-main.119,A00-2038,0,0.178355,"logies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (20"
2020.coling-main.119,N01-1014,0,0.277746,"Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual word embeddings. Cross-lingual w"
2020.coling-main.119,2005.mtsummit-papers.40,0,0.203812,"s that have originated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in Fr"
2020.coling-main.119,W12-0216,0,0.0171469,"tion methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Prev"
2020.coling-main.119,N01-1020,0,0.147349,"ative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language"
2020.coling-main.119,J99-1003,0,0.0606347,"ion 4 presents the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection"
2020.coling-main.119,K19-1011,0,0.0223535,"Missing"
2020.coling-main.119,mulloni-pekar-2006-automatic,0,0.0637471,"the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly i"
2020.coling-main.119,W97-1102,0,0.46133,"entations for each token. 4 Approaches We use various approaches to perform the cognate detection task viz. baseline cognate detection approaches like orthographic similarity-based, phonetic similarity-based, phonetic vectors with SiameseCNN based proposed by Rama (2016), and Recurrent neural network-based approach proposed by Kanojia et al. (2019b). We use the same hyperparameters and architectures, as discussed in these papers. We describe each of these feature sets in this section. 4.1 Weighted Lexical Similarity (WLS) The Normalized Edit Distance (NED) approach computes the edit distance (Nerbonne and Heeringa, 1997) for all word pairs in our dataset. Each of the operations has unit cost (except that substitution of a character by itself has zero cost), so NED is equal to the minimum number of operations to transform ‘word a’ to ‘word b’. We use a similarity score provided by NED, which is calculated as (1 - NED Score). We combine NED with q-gram distance (Shannon, 1948) for a better similarity score. The qgrams (‘n-grams’) are simply substrings of length q. This distance measure has been applied previously for various spelling correction approaches (Owolabi and McGregor, 1988; Kohonen, 1978). Kanojia et"
2020.coling-main.119,P02-1040,0,0.108173,"e three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best baseline model with an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approx"
2020.coling-main.119,N18-1202,0,0.0510587,"pt. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5"
2020.coling-main.119,P19-1493,0,0.0254159,"ransliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5 Link: Link: 7 Link: 8 Link: 9"
2020.coling-main.119,N18-2063,0,0.0356884,"Missing"
2020.coling-main.119,C16-1097,0,0.0946439,"work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual wor"
2020.coling-main.119,W99-0626,0,0.196426,"put. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for c"
2020.coling-main.119,W19-4720,0,0.0231209,"lingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic similarity arises for non-identical cognates, based on the reaction time from human annotators. Similarly, Merlo and Andueza Rodriguez (2019) show that cross-lingual models exhibit the semantic properties of for bilingual lexicons despite their structural simplicities, which leads us to perform our investigation for low-resource Indian languages. Uban et al. (2019) discuss the semantic change in languages by studying the change in cognate words across Romance languages using cross-lingual similarity. All of the previous approaches discussed above, lack the use of an appropriate cross-lingual similarity-based measure and do not work well for Indian languages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 1702"
2020.icon-main.23,P18-2080,0,0.0112667,"n training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. (2018) (who used string kernels and super word embeddings). In all of their works, they defined a source promp"
2020.icon-main.23,D16-1115,0,0.162933,"written in response to a completely different prompt. In order to solve this challenge of lack of training data, we use cognitive information learnt by gaze behaviour of readers to augment our training data and improve our model. Automatic essay grading has been around for over half a century ever since Page (1966)’s work (Beigman Klebanov and Madnani, 2020). While there have been a number of commercial systems like E-Rater (Attali and Burstein, 2006) from the Educational Testing Service (ETS), most modern-day systems use deep learning and neural networks, like convolutional neural networks (Dong and Zhang, 2016), recurrent neural networks (Taghipour and Ng, 2016), or both (Dong and Zhang, 2016). However, all these systems rely on the fact that their training and testing data is from the same prompt. Quite often, at run time, we may not have essays written in response to our target prompt (i.e. the prompt which our essay is written in response to). Because of the lack of training data, especially when training a model for essays written for a new prompt, many systems may fail at run time. To solve this problem, we propose a multi-task approach, similar to Mathias et al. (2020), where we learn a reader"
2020.icon-main.23,K17-1017,0,0.0143879,"se the properties of the target essay set in training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. (2018) (who used string kernels and super word embeddings). In all"
2020.icon-main.23,P18-1219,1,0.88028,"Missing"
2020.icon-main.23,2020.aacl-main.86,1,0.783741,"utional neural networks (Dong and Zhang, 2016), recurrent neural networks (Taghipour and Ng, 2016), or both (Dong and Zhang, 2016). However, all these systems rely on the fact that their training and testing data is from the same prompt. Quite often, at run time, we may not have essays written in response to our target prompt (i.e. the prompt which our essay is written in response to). Because of the lack of training data, especially when training a model for essays written for a new prompt, many systems may fail at run time. To solve this problem, we propose a multi-task approach, similar to Mathias et al. (2020), where we learn a reader’s gaze behaviour for helping our system grade new essays. In this paper, we look at a similar approach proposed by Mathias et al. (2020) to grade essays using cognitive information, which is learnt as an auxiliary task in a multi-task learning approach. Multi-task learning is a machine-learning approach, where the model tries to solve one or more auxiliary tasks to solve a primary task (Caruana, 1998). Similar to Mathias et al. (2020), the scoring of the essay is the primary task, while learning the gaze behaviour is the auxiliary task. Contribution. In this paper, we"
2020.icon-main.23,D14-1162,0,0.0849202,"viour attributes as described in Mathias et al. (2020). Binning is done to take into account the idiosyncracies of the gaze behaviour of individual readers (i.e. some people may read faster, others slower, etc.). Whenever we use gaze behaviour, we scale the value of the gaze behaviour bins to the range of [0, 1] as well. 5.5 Figure 1: Architecture of our gaze behaviour system, showing an input essay of n sentences, with the outputs being the gaze behaviour (whenever applicable), and the overall essay score. 5.3 Network Hyperparameters We use the 50 dimension GloVe pre-trained word embeddings (Pennington et al., 2014). We run our experiments over a batch size of 200, for 50 epochs. We set the learning rate as 0.001, and the dropout rate as 0.5. The word-level CNN layer has a kernel size of 5, with 100 filters. The sentence-level LSTM layer has 100 hidden units. We use the RMSProp Optimizer (Dauphin et al., 2015) with an initial learning rate of 0.001 and momentum of 0.9. Along with the network hyperparameters, we also weigh the loss functions of the different gaze behaviour attributes differently, using the same weights as Mathias et al. (2020), namely 0.05 for DT and FFD, 0.01 for IR and Experiment Config"
2020.icon-main.23,D15-1049,0,0.0284022,"pt. One drawback of this approach is that it would not be able to use the properties of the target essay set in training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. ("
2020.icon-main.23,D16-1193,0,0.0605172,"mpt. In order to solve this challenge of lack of training data, we use cognitive information learnt by gaze behaviour of readers to augment our training data and improve our model. Automatic essay grading has been around for over half a century ever since Page (1966)’s work (Beigman Klebanov and Madnani, 2020). While there have been a number of commercial systems like E-Rater (Attali and Burstein, 2006) from the Educational Testing Service (ETS), most modern-day systems use deep learning and neural networks, like convolutional neural networks (Dong and Zhang, 2016), recurrent neural networks (Taghipour and Ng, 2016), or both (Dong and Zhang, 2016). However, all these systems rely on the fact that their training and testing data is from the same prompt. Quite often, at run time, we may not have essays written in response to our target prompt (i.e. the prompt which our essay is written in response to). Because of the lack of training data, especially when training a model for essays written for a new prompt, many systems may fail at run time. To solve this problem, we propose a multi-task approach, similar to Mathias et al. (2020), where we learn a reader’s gaze behaviour for helping our system grade new e"
2020.icon-main.23,W15-0626,0,0.151979,"rmation, in the form of gaze behaviour. Our experiments show that using gaze behaviour helps in improving the performance of AEG systems, especially when we provide a new essay written in response to a new prompt for scoring, by an average of almost 5 percentage points of QWK. 1 Introduction One of the major challenges in machine learning is the requirement of a large amount of training data. AEG systems perform at their best when they are trained in a prompt-specific manner - i.e. the essays that they are tested on are written in response to the same prompt as the essays they are trained on (Zesch et al., 2015). These systems perform badly when they are tested against essays written in response to a different prompt. Zero-shot AEG is when our AEG system is used to grade essays written in response to a completely different prompt. In order to solve this challenge of lack of training data, we use cognitive information learnt by gaze behaviour of readers to augment our training data and improve our model. Automatic essay grading has been around for over half a century ever since Page (1966)’s work (Beigman Klebanov and Madnani, 2020). While there have been a number of commercial systems like E-Rater (A"
2020.icon-main.23,W18-0549,0,0.0146289,"f the target essay set in training the model. Therefore, as a way to alleviate this problem, we learn cognitive information, in the form of gaze behaviour, for the essays to help our automatic essay grading system grade the essays better. 3 Related Work While there has been work done on developing systems for automatic essay grading, all of them describe systems which use some of the essays the system is tested on as part of the training data (as well as validation data, where applicable) (Chen and He, 2013; Phandi et al., 2015; Taghipour and Ng, 2016; Dong and Zhang, 2016; Dong et al., 2017; Zhang and Litman, 2018; Cozma et al., 2018; Tay et al., 2018; Mathias et al., 2020). One of the solutions to solve the problem was using cross-domain AEG, where systems were trained using essays in a set of source prompt / prompts and tested on essays written in response to the target prompt. Some of the work done to study cross-domain AEG were Zesch et al. (2015) (who used task-independent features), Phandi et al. (2015) (who used domain adaptation), Dong and Zhang (2016) (who used a hierarchical CNN layers) and Cozma et al. (2018) (who used string kernels and super word embeddings). In all of their works, they de"
2020.lrec-1.378,D16-1250,0,0.0220935,"lingual word embeddings into a common space and thus should be able to decipher the ‘meaning’ or the ‘sense’ of two different words better, when they belong to different languages. Given the recent advancements in word representation models, cross-lingual word embedding based models should be employed for such a task. Please also note that we do not propose a new approach for the task of False friends’ detection and hence do not perform any experimentation with cross-lingual word embeddings. However, Merlo and Rodriguez (2019) show that cross-lingual word embeddings obtained using the VecMap (Artetxe et al., 2016) approach have shown promise and can be used to obtain a semantic comparison between two words from different languages. 6. aid the NLP tasks of Machine Translation, Cross-lingual Information Retrieval, and Computational Phylogenetics. We hope better approaches are developed for these tasks which can perform well on our challenge dataset. In the near future, we shall include partial cognates in our dataset creation approach and release another dataset on the same repository. Partial cognates mean different given different contexts and can confuse an NLP task. Hence, we believe it is also impor"
2020.lrec-1.378,N09-3008,0,0.691237,"ng based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Beng"
2020.lrec-1.378,W18-3903,0,0.0630431,"Missing"
2020.lrec-1.378,P14-2017,0,0.425837,", 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Ass"
2020.lrec-1.378,P15-2071,0,0.353185,"(Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gu"
2020.lrec-1.378,I11-1097,0,0.0276839,"ngst them is the Edit distance-based similarity measure (Melamed, 1999). Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) emplo"
2020.lrec-1.378,E17-1113,0,0.143587,"Missing"
2020.lrec-1.378,2019.gwc-1.51,1,0.582606,"bda Kosha” and its annotation with linked Wordnet IDs. With the help of a lexicographer, we perform the digitization of this dictionary. Further, we annotate the cognate sets from the dicCognate False Friend Hindi (Hi) Marathi (Mr) Hindi Meaning Marathi Meaning ank shikshA ank shikshA Number Education Number Punishment Table 1: An example each of a cognate pair and a false friend pair from the closely related Indian languages Hindi (Hi) and Marathi (Mr) tionary with Wordnet synset IDs based on manual validation, where the lexicographer checks each Wordnet in the existing linked sense.Based on Kanojia et al. (2019b)’s approach, we use linked Indian Wordnets to generate true cognate data and create another cognate dataset. Additionally, we use the same Wordnet data to produce a list of False Friends and release2 all the three datasets publicly. Our cognate sets can be utilized for lookup in phrase tables produced during Machine Translation to assess the quality of the translation system in question. They can be utilized as candidate translations for words, and our false friends’ list can be utilized by language learners to avoid pitfalls during the acquisition of a second language. False Friend and Cogn"
2020.lrec-1.378,C04-1137,0,0.122835,"True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language family and provide a methodology to complete the cognate chain for related languages. Our work releases similar data for Indian languages. Such a co"
2020.lrec-1.378,A00-2038,0,0.820934,"s are often used as baseline methods for cognate detection, and the most commonly used method amongst them is the Edit distance-based similarity measure (Melamed, 1999). Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu"
2020.lrec-1.378,W12-0216,0,0.467071,"Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment."
2020.lrec-1.378,N01-1020,0,0.505957,"l 1 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Roman"
2020.lrec-1.378,J99-1003,0,0.897363,"ics (Rama et al., 2018) as well as cross-lingual 1 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Wo"
2020.lrec-1.378,K19-1011,0,0.115603,"ngual embeddings may not be an appropriate feature. Cross-lingual word embeddings project monolingual word embeddings into a common space and thus should be able to decipher the ‘meaning’ or the ‘sense’ of two different words better, when they belong to different languages. Given the recent advancements in word representation models, cross-lingual word embedding based models should be employed for such a task. Please also note that we do not propose a new approach for the task of False friends’ detection and hence do not perform any experimentation with cross-lingual word embeddings. However, Merlo and Rodriguez (2019) show that cross-lingual word embeddings obtained using the VecMap (Artetxe et al., 2016) approach have shown promise and can be used to obtain a semantic comparison between two words from different languages. 6. aid the NLP tasks of Machine Translation, Cross-lingual Information Retrieval, and Computational Phylogenetics. We hope better approaches are developed for these tasks which can perform well on our challenge dataset. In the near future, we shall include partial cognates in our dataset creation approach and release another dataset on the same repository. Partial cognates mean different"
2020.lrec-1.378,W97-1102,0,0.888502,"from gold-standard translation candidates. Keeping the application of our dataset in mind, we ignore the inclusion of partial cognates from this dataset. 3.2. D2 - True Cognate Pairs via IndoWornet In their paper, Kanojia et al. (2019b) identify IndoWordnet (Bhattacharyya, 2017) as a potential resource for the task of cognate detection. They utilize deep neural network based approaches to validate their approach for cognate detection. We build this dataset using a simple orthographic similarity based approach from the IndoWordnet dataset. Our approach combines Normalized Edit Distance (NED) (Nerbonne and Heeringa, 1997) and Cosine Similarity (CoS) (Salton and Buckley, 1988) between words. We compare synset words from every language pair using NED and populate a list of cognate sets where NED score is 0.7 and above. Similarly, we populate another list of cognate sets from every language pair using a shingle (n-gram) based Cosine Similarity with the same threshold. Due to the different methods using which NED and CoS similarity techniques compute scores, both NED and CoS output a different number of word pairs. We choose a common intersection of cognate pairs from among both the lists, and populate a final ‘po"
2020.lrec-1.378,N18-2063,0,0.455793,"Missing"
2020.lrec-1.378,C16-1097,0,0.756328,"automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognat"
2020.lrec-1.378,C02-1002,0,0.191073,"st in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language fa"
2020.lrec-1.378,L18-1538,0,0.0193274,"nducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language family and provide a methodology to complete the cognate chain for related languages. Our work releases similar data for Indian languages. Such a cognate set data has not been released previously for Indian languages, to the best of our knowledge. Additionally, we release lists of false friends’ for language pairs. These cognates can be used to challenge the previously established cognate detection approaches further. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verifica"
2020.lrec-1.613,baccianella-etal-2010-sentiwordnet,0,0.0139889,"2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity (Cer et al., 2018). It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity dis"
2020.lrec-1.613,W06-1615,0,0.226339,"ch is based on the hypothesis that if source and target domains are similar, their CDSA accuracy should also be higher given all other conditions (such as data size) are the same. The rest of the paper is organized as follows. We describe related work in Section 2. We then introduce our sentiment classifier in Section 3. and the similarity metrics in Section 4. The results are presented in Section 5. followed by a discussion in Section 6. Finally, we conclude the paper in Section 7. 2. Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domai"
2020.lrec-1.613,Q17-1010,0,0.0320651,"ing D1 in D2 and ∆E indicates percentage change in entropy before and after mixing of source and target domains. Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is"
2020.lrec-1.613,D18-2029,0,0.0254947,"three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity (Cer et al., 2018). It was observed that cos"
2020.lrec-1.613,N19-1149,0,0.0182451,"int sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Recent work by Dai et al. (2019) uses similarity metrics to select the domain from which pre-trained embeddings should be obtained for named entity recognition. Similarly, Schultz et al. (2018) present a method for source domain selection as a weighted sum of similarity metrics. They use statistical classifiers such as logistic regression and support vector machines. However, the similarity measures used are computationally intensive. To the best of our knowledge, this is the first work at this scale that compares different costeffective similarity metrics with the performance of CDSA. 3. 1. Labelled Data: Here, each review"
2020.lrec-1.613,P07-1033,0,0.0418056,"Missing"
2020.lrec-1.613,L16-1041,0,0.0167249,"all our metrics in detail later in this section. These 11 metrics can also be classified into two categories: • Symmetric Metrics - The metrics which consider domain-pairs (D1 , D2 ) and (D2 , D1 ) as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings. Sentiment Classifier The core of this work is a sentiment classifier for different domains. We use the DRANZIERA benchmark dataset (Dragoni et al., 2016), which consists of Amazon reviews from 20 domains such as automatives, baby products, beauty products, etc. The detailed list can be seen in Table 1. To ensure that the datasets are balanced across all domains, we randomly select 5000 positive and 5000 negative reviews from each domain. The length of the reviews ranges from 5 words to 1654 words across all domains, with an average length ranging from 71 words to 125 words per domain. We point the reader to the original paper for detailed dataset statistics. We normalize the dataset by removing numerical values, punctuations, stop words, and c"
2020.lrec-1.613,P11-1013,0,0.0355759,"Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Re"
2020.lrec-1.613,P12-1043,0,0.0274583,"aume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been extracted using automatic methods (Li et al., 2012). Glorot et al. (2011) present a method for domain adaptation of sentiment classification that uses deep architectures. Our work differs from theirs in terms of computational intensity (deep architecture) and scale (4 domains only). In this paper, we compare similarity metrics with crossdomain adaptation for the task of sentiment analysis. This has been performed for several other tasks. Recent work by Dai et al. (2019) uses similarity metrics to select the domain from which pre-trained embeddings should be obtained for named entity recognition. Similarly, Schultz et al. (2018) present a metho"
2020.lrec-1.613,J81-4005,0,0.668614,"Missing"
2020.lrec-1.613,P18-2064,1,0.804252,"just need to search for them in the target domain to find out common significant words. LM2: Symmetric KL-Divergence (SKLD) KL Divergence can be used to compare the probabilistic distribution of polar words in two domains (Kullback and Leibler, 1951). A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains 4984 is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by Murthy et al. (2018) and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as ‘polar’ for a domain if, |P − N |>= 0.5 (2) where P is the probability of a word appearing in a review which is labelled positive and N is the probability of a word appearing in a review which is labelled negative. SKLD of a polar word for domain-pair (D1 , D2 ) is calculated as: ! ! P1 N1 + P1 ∗ log (3) A = N1 ∗ log N2 P2 N2 B = N2 ∗ log N1 ! P2 + P2 ∗ log P1 (4) A+B (5) 2 where Pi and Ni are probabilities of a word appearing under positively labelled and negatively labelle"
2020.lrec-1.613,D14-1162,0,0.0821559,"Missing"
2020.lrec-1.613,N18-1202,0,0.00906568,"tage change in entropy before and after mixing of source and target domains. Note that this metric offers the advantage of asymmetricity, unlike the other three metrics for labelled data. 2 We observe that any value of w does not change the relative ranking of domains. 4985 4.2. Metrics: Unlabelled Data For unlabelled target domain data, we utilize word and sentence embeddings-based similarity as a metric and use various embedding models. To train word embedding based models, we use Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and ELMo (Peters et al., 2018). We also exploit sentence vectors from models trained using Doc2Vec (Le and Mikolov, 2014), FastText, and Universal Sentence Encoder (Cer et al., 2018). In addition to using plain sentence vectors, we account for sentiment in sentences using SentiWordnet (Baccianella et al., 2010), where each review is given a sentiment score by taking harmonic mean over scores (obtained from SentiWordnet) of words in a review3 . ULM1: Word2Vec We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions4 where the context window is chosen to be 5. For each domain"
2020.lrec-1.613,I13-1076,1,0.806812,"main-pairs is a reason for poor performance. To mitigate this, we compute a confidence term for a domain-pair (D1 , D2 ) using the Jaccard Similarity Coefficient which is calculated as follows: C W1 + W2 − C |P1 − P2 |+ |N1 − N2 | (8) The overall distance is an average overall common polar words. Similar to SKLD, the confidence term based on Jaccard Similarity Coefficient is used to counter the imbalance of common polar word count between domain-pairs. (L1 Distance)avg + 1 J (9) Domain pairs are ranked in increasing order of final value. ! SKLD = J= which change their polarity across domains (Sharma and Bhattacharyya, 2013). The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc. For every common polar word between two domains, L1 Distance between two vectors [P1 , N1 ] and [P2 , N2 ] is calculated as; LM4: Entropy Change Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution"
2020.lrec-1.613,P18-1089,1,0.855125,"for each source domain can be highly intensive both in terms of time and resources. This makes it important to devise easy-to-compute metrics that use labelled data in the source and target domains. When target domain data is labelled, we use the following four metrics for comparing and ranking source domains for a particular target domain: LM1: Significant Words Overlap All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the ‘Clothing’ domain but not as significant in the ‘Movie’ domain. In this metric, we build upon existing work by Sharma et al. (2018) and extract significant words from 4983 D1 D2 D3 D4 D5 D6 D7 D8 D9 D10 D11 D12 D13 D14 D15 D16 D17 D18 D19 D20 D1 84.84 76.34 74.47 75.66 81.14 74.19 75.57 73.83 75.39 82.75 77.11 78.31 76.00 74.28 71.91 72.15 77.14 77.15 78.83 79.08 D2 70.15 83.24 75.00 74.32 69.81 73.87 77.93 73.29 72.90 72.69 65.46 79.11 79.46 77.31 72.34 75.18 77.22 80.04 71.26 70.15 D3 72.58 77.71 85.78 80.31 70.86 71.99 75.67 78.39 78.70 73.83 66.81 78.49 77.00 80.29 71.26 76.59 77.27 76.21 75.33 71.98 D4 73.94 77.83 80.16 84.49 73.09 76.37 75.08 79.82 76.93 73.59 72.53 78.69 78.42 78.66 75.29 75.44 77.06 79.09 76.18 73"
2020.lrec-1.613,P09-1043,0,0.0453219,"e and target domains are similar, their CDSA accuracy should also be higher given all other conditions (such as data size) are the same. The rest of the paper is organized as follows. We describe related work in Section 2. We then introduce our sentiment classifier in Section 3. and the similarity metrics in Section 4. The results are presented in Section 5. followed by a discussion in Section 6. Finally, we conclude the paper in Section 7. 2. Related Work Cross-domain adaptation has been reported for several NLP tasks such as part-of-speech tagging (Blitzer et al., 2006), dependency parsing (Zhang and Wang, 2009), and named entity recognition (Daume III, 2007). Early work in CDSA is by Denecke (2009). They show that lexicons such as SentiWordnet do not perform consistently for sentiment classification of multiple domains. Typical statistical approaches for CDSA use active learning (Li et al., 2013), 4982 co-training (Chen et al., 2011) or spectral feature alignment (Pan et al., 2010). In terms of the use of topic models for CDSA, He et al. (2011) adapt the joint sentiment tying model by introducing domain-specific sentiment-word priors. Similarly, cross-domain sentiment and topic lexicons have been ex"
2020.sltu-1.49,C18-1139,0,0.0255478,"st the quality of non-contextual word embeddings. The Named Entity Recognition task, collected from (Murthy et al., 2018), and FIRE 2014 workshop for NER, contains NER tagged data for 5 Indian languages, namely Hindi, Tamil, Bengali, Malayalam, and Marathi. We also use a Universal POS (UPOS), as well as an XPOS (language-specific PoS tags) tagged dataset, available from the Universal Dependency (UD) treebank (Nivre et al., 2016), which contains POS tagged data for 4 Indian languages, Hindi, Tamil, Telugu, and Marathi. For the tasks of NER, UPOS tagging, XPOS tagging, we use the Flair library (Akbik et al., 2018), which embeds our pre-trained embeddings as inputs for training the corresponding tagging models. The tagging models provided by Flair are vanilla BiLSTM-CRF sequence labellers. For the task of word analogy dataset, we simply use the vector addition and subtraction operators to check accuracy (i.e., v(France) − v(Paris) + v(Berlin) should be close to v(Germany)). For contextual word embeddings, we collect the statistics provided at the end of the pre-training phase to gauge the quality of the embeddings - perplexity scores for ELMo, masked language model accuracy for BERT, and so on. We repor"
2020.sltu-1.49,P18-1073,0,0.154385,"oblem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters as detailed further in the paper. Our key contributions are: (1) We acquire raw monolingual corpora for fourteen languages, includi"
2020.sltu-1.49,D18-1399,0,0.0872982,"oblem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters as detailed further in the paper. Our key contributions are: (1) We acquire raw monolingual corpora for fourteen languages, includi"
2020.sltu-1.49,P19-1019,0,0.0116312,"s to be extracted in a supervised manner, embeddings can be obtained in a completely unsupervised fashion. For Indian languages, there are little corpora and few datasets of appreciable size available for computational tasks. The wikimedia dumps which are used for generating pre-trained models are insufficient. Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Al"
2020.sltu-1.49,Q17-1010,0,0.311779,"ntroduced in (Y. Bengio, 2003) when it was realised that learning the joint probability of sequences was not feasible due to the ‘curse of dimensionality’, i.e., at that time, the value added by an additional dimension seemed much smaller than the overhead it added in terms of computational time, and space. Since then, several developments have occurred in this field. Word2Vec (Mikolov et al., 2013a) showed the way to train word vectors. The models introduced by them established new stateof-the-art on tasks such as Word Sense Disambiguation (WSD). GloVE (Pennington et al., 2014) and FastText (Bojanowski et al., 2017) further improved on results shown by Mikolov et al. (2013a), where GloVE used a co-occurrence matrix and FastText utilized the sub-word information to generate word vectors. Sent2Vec (Pagliardini et al., 2017) generates sentence vectors inspired by the same idea. Universal Sentence Embeddings (Cer et al., 2018), on the other hand, creates sentence vectors using two variants: transformers and DANs. Doc2Vec (Le and Mikolov, 2014) computes a feature vector for every document in the corpus. Similarly, Context2vec (Melamud et al., 2016) learns embedding for variable length sentential context for t"
2020.sltu-1.49,bojar-etal-2014-hindencorp,0,0.0478607,"Missing"
2020.sltu-1.49,D14-1067,0,0.0205245,"hine Learning wherein features have at times to be extracted in a supervised manner, embeddings can be obtained in a completely unsupervised fashion. For Indian languages, there are little corpora and few datasets of appreciable size available for computational tasks. The wikimedia dumps which are used for generating pre-trained models are insufficient. Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is"
2020.sltu-1.49,D18-2029,0,0.030248,"several developments have occurred in this field. Word2Vec (Mikolov et al., 2013a) showed the way to train word vectors. The models introduced by them established new stateof-the-art on tasks such as Word Sense Disambiguation (WSD). GloVE (Pennington et al., 2014) and FastText (Bojanowski et al., 2017) further improved on results shown by Mikolov et al. (2013a), where GloVE used a co-occurrence matrix and FastText utilized the sub-word information to generate word vectors. Sent2Vec (Pagliardini et al., 2017) generates sentence vectors inspired by the same idea. Universal Sentence Embeddings (Cer et al., 2018), on the other hand, creates sentence vectors using two variants: transformers and DANs. Doc2Vec (Le and Mikolov, 2014) computes a feature vector for every document in the corpus. Similarly, Context2vec (Melamud et al., 2016) learns embedding for variable length sentential context for target words. The drawback of earlier models was that the representation for each word was fixed regardless of the context in which it appeared. To alleviate this problem, contextual word embedding models were created. ELMo (Peters et al., 2018) used bidirectional LSTMs to improve on the previous works. Later, BE"
2020.sltu-1.49,P16-1035,0,0.0233854,"gs can be obtained in a completely unsupervised fashion. For Indian languages, there are little corpora and few datasets of appreciable size available for computational tasks. The wikimedia dumps which are used for generating pre-trained models are insufficient. Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Almost all of these models use Wikimedia corpus t"
2020.sltu-1.49,L18-1155,0,0.0172201,"paper. Our key contributions are: (1) We acquire raw monolingual corpora for fourteen languages, including Wikimedia dumps. (2) We train various embedding models and evaluate them. (3) We release these embedding models and evaluation data in a single repository2 . 2 Source Link 352 Repository Link The roadmap of the paper is as follows: in section 2, we discuss previous work; section 3 discusses the corpora and our evaluation datasets; section 4 briefs on the approaches used for training our models, section 5 discusses the resultant models and their evaluation; section 6 concludes the paper. Haider (2018) release word embeddings for the Urdu language, which is one of the Indian languages we do not cover with this work. To evaluate the quality of embeddings, they were tested on Urdu translations of English similarity datasets. 3. 2. Literature Survey Word embeddings were first introduced in (Y. Bengio, 2003) when it was realised that learning the joint probability of sequences was not feasible due to the ‘curse of dimensionality’, i.e., at that time, the value added by an additional dimension seemed much smaller than the overhead it added in terms of computational time, and space. Since then, s"
2020.sltu-1.49,K17-1034,0,0.0157085,"ut training data, the better the embedding models. Acquiring raw corpora to be used as input training data has been a perennial problem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters"
2020.sltu-1.49,K16-1006,0,0.175393,"ata, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Almost all of these models use Wikimedia corpus to train models which is insufficient 1 for Indian languages as Wikipedia itself lacks significant number of articles or text in these languages. Although there is no cap or minimum number of documents/lines which define a usable size of a corpus for training such models, it is general"
2020.sltu-1.49,K19-1011,0,0.0114122,"ing raw corpora to be used as input training data has been a perennial problem for NLP researchers who work with low resource languages. Given a raw corpus, monolingual word embeddings can be trained for a given language. Additionally, NLP tasks that rely on utilizing common linguistic properties of more than one language need cross-lingual word embeddings, i.e., embeddings for multiple languages projected into a common vector space. These cross-lingual word embeddings have shown to help the task of cross-lingual information extraction (Levy et al., 2017), False Friends and Cognate detection (Merlo and Rodriguez, 2019), and Unsupervised Neural Machine Translation (Artetxe et al., 2018b). With the recent advent of contextualized embeddings, a significant increase has been observed in the types of word embedding models. It would be convenient if a single repository existed for all such embedding models, especially for low-resource languages. Our work creates such a repository for fourteen Indian languages, keeping this in mind, by training and deploying 436 models with different training algorithms (like word2vec, BERT, etc.) and hyperparameters as detailed further in the paper. Our key contributions are: (1)"
2020.sltu-1.49,L16-1262,0,0.0471833,"Missing"
2020.sltu-1.49,N18-1049,0,0.0591286,"Missing"
2020.sltu-1.49,D14-1162,0,0.0850276,"Missing"
2020.sltu-1.49,N18-1202,0,0.604389,"Without sufficient data, it becomes difficult to train embeddings. NLP tasks that benefit from these pre-trained embeddings are very diverse. Tasks ranging from word analogy and spelling correction to more complex ones like Question Answering (Bordes et al., 2014), Machine Translation (Artetxe et al., 2019), and Information Retrieval (Diaz et al., 2016) have reported improvements with the use of well-trained embeddings models. The recent trend of transformer architecture based neural networks has inspired various language models that help train contextualized embeddings (Devlin et al., 2018; Peters et al., 2018; Melamud et al., 2016; Lample and Conneau, 2019). They report significant improvements over various NLP tasks and release pre-trained embeddings models for many languages. One of the shortcomings of the currently available pre-trained models is the corpora size used for their training. Almost all of these models use Wikimedia corpus to train models which is insufficient 1 for Indian languages as Wikipedia itself lacks significant number of articles or text in these languages. Although there is no cap or minimum number of documents/lines which define a usable size of a corpus for training such"
2021.emnlp-main.789,L16-1079,0,0.0279456,"scale (Likert, 1932). 2 Related Work Most of the previous work on computational humour has been towards the detection of humour. Smaller joke formats like one-liners which have just a single line of context, have been used (Hetzron, 1991). Language models like BERT are used for generating sentence embeddings, which have been shown to outperform other architectures in humour detection on short texts (Annamoradnejad, 2020). Since humour depends on how the speaker’s voice changes, the audio features, and language features have been used as inputs for machine learning models for humour detection. Bertero and Fung (2016) use audio and language features to detect humour in The Big Bang Theory sitcom dialogues. Park et al. (2018) passed audio and language features from a conversation dataset into an RNN to create a chatbot that can detect and respond to humour. Hasan et al. (2019) built a multi-modal dataset that uses text, audio, and video inputs for humour detection. There are existing datasets that rate the humour in tweets and Reddit posts, with the help of human annotators (Miller et al., 2020; Castro et al., 2018; Weller and Seppi, 2019). Creating human-annotated datasets is costly in terms of both time a"
2021.emnlp-main.789,W18-3502,0,0.0218029,"guage features have been used as inputs for machine learning models for humour detection. Bertero and Fung (2016) use audio and language features to detect humour in The Big Bang Theory sitcom dialogues. Park et al. (2018) passed audio and language features from a conversation dataset into an RNN to create a chatbot that can detect and respond to humour. Hasan et al. (2019) built a multi-modal dataset that uses text, audio, and video inputs for humour detection. There are existing datasets that rate the humour in tweets and Reddit posts, with the help of human annotators (Miller et al., 2020; Castro et al., 2018; Weller and Seppi, 2019). Creating human-annotated datasets is costly in terms of both time and money and has been one of the noted issues for creating humour datasets. Yang et al. (2019a,b) used time-aligned user comments for generating automated humour labels for multimodal humour identification tasks and found good agreement with manually annotated data. However, none of the previously existing datasets are created with standup comedy clips. 3 Dataset Acquisition and Pre-processing In this section, we describe the creation of our multi-modal dataset and the manual evaluation performed with"
2021.emnlp-main.789,2021.ccl-1.108,0,0.0320744,"Missing"
2021.emnlp-main.789,D19-1211,0,0.0340777,"Missing"
2021.emnlp-main.789,D14-1162,0,0.0852428,"d as input to separate Bi-LSTM layers followed by separate, Dense layers (Graves, Alex and Fernán- models can process sequences of token length 512; thus, we employ them for the entire transcript of dez, Santiago and Schmidhuber, Jürgen, 2005) as shown in Figure 1. The output from these two path- each ∼ 2 minute clip. We sum the output of the final 4 layers from these models to obtain a clip ways is then concatenated and fed to a classifier that outputs one-hot encoding of the 5-point rating. embedding (Alammar, 2018). As baseline textual features, we use GloVe em4.2 Muting Laughter beddings (Pennington et al., 2014). For obtaining Before extracting audio features, we remove the textual features, we experiment with BERTbase , audience laughter and isolate the speaker’s voice BERTlarge , XLM, DistilBERT, RoBERTabase and from each clip. Retaining the audience laughter RoBERTalarge to generate text embeddings (Demay enable a neural network to utilize it and predict vlin et al., 2018; Lample and Conneau, 2019; Sanh a score without using information from the text et al., 2019; Liu et al., 2019). 10075 4.5 Methodology The audio features and textual features are fed as input to the network for obtaining an outpu"
2021.emnlp-main.789,D19-1372,0,0.0205635,"een used as inputs for machine learning models for humour detection. Bertero and Fung (2016) use audio and language features to detect humour in The Big Bang Theory sitcom dialogues. Park et al. (2018) passed audio and language features from a conversation dataset into an RNN to create a chatbot that can detect and respond to humour. Hasan et al. (2019) built a multi-modal dataset that uses text, audio, and video inputs for humour detection. There are existing datasets that rate the humour in tweets and Reddit posts, with the help of human annotators (Miller et al., 2020; Castro et al., 2018; Weller and Seppi, 2019). Creating human-annotated datasets is costly in terms of both time and money and has been one of the noted issues for creating humour datasets. Yang et al. (2019a,b) used time-aligned user comments for generating automated humour labels for multimodal humour identification tasks and found good agreement with manually annotated data. However, none of the previously existing datasets are created with standup comedy clips. 3 Dataset Acquisition and Pre-processing In this section, we describe the creation of our multi-modal dataset and the manual evaluation performed with the help of human annota"
2021.findings-acl.256,D18-1178,0,0.0278593,"Missing"
2021.findings-acl.256,P98-1013,0,0.448964,"P ROTEST −−→ I NTENTIONALLY ACT • Using: the child frame presupposes the parent Uses frame, e.g., P ROTEST −−→ TAKING SIDES • Subframe: the child frame is a subevent of Subframe a complex parent event, e.g., T RIAL −−−−−→ V ERDICT Along with each frame relation, FrameNet also consists of relations between FEs of parent-child frames. Following are illustrative examples for FE relations for the above frame relations: • P ROTEST:P ROTESTER ALLY ACT:AGENT . • P ROTEST:P ROTESTER FrameNet RDP (Recoverable Deleted Predicates) https://framenet.icsi.berkeley.edu I NTENTION - Uses −−→ TAK - Subframe (Baker et al., 1998) is a taxonomy based on Fillmore’s theory of Frame Semantics. This theory claims that most words’ meanings can be inferred based on a semantic frame: a conceptual structure that denotes an abstract event, relation, or entity and the involved participants. For example, the concept of questioning involves a person asking a question (S PEAKER), person/people begin questioned A DDRESSEE, the content of the question M ESSAGE, and so on. In FrameNet, such a concept is 2 Is-A −−→ ING SIDES :C OGNIZER • T RIAL :J UDGE −−−−−→ V ERDICT:J UDGE FrameNet2 1 Relations 3.1.2 Mappings FrameNet data provides t"
2021.findings-acl.256,S13-2025,0,0.0468044,"Missing"
2021.findings-acl.256,P19-1568,0,0.0168413,"not be effective. We implement these models in PyTorch (Paszke et al., 2017). We initialize the word embedding layer with Google’s pre-trained embeddings3 and initialize the frame embedding layer with random values, in one case, for baseline, and pre-trained frame embedding, in another case. We use the same architecture to train another model for FE prediction, replace the frame embedding layer with an FE embedding layer, and candidates FEs are the FEs from all candidate frames. We take all FEs as a candidate set if no such mapping is found. 4.2 Frame and Frame Element Embeddings Inspired by Kumar et al. (2019)’s approach for the task of Word Sense Disambiguation (WSD), we propose a similar approach to perform NC interpretation. Our approach uses the definition of entities (along with the relations) to learn entity embeddings and relation embeddings. It uses an encoder (Bi-LSTM) to encode the definition of an entity and uses encoded representation as an embedding of the entity for ConvE. During the training, it also optimizes both: the encoder and ConvE. After the training, the encoding of definitions is taken as entity embeddings. We train ConvE twice to get frame and frame element embeddings separ"
2021.findings-acl.256,P98-1015,0,0.311648,"ed Work A relation between the components of a noun compound (say, chocolate cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forw"
2021.findings-acl.256,W17-2618,0,0.0118194,"tation is performed by one of the authors and hence does not warrant discussion on the interannotator agreement. However, please allow us to point out that our annotations are still manually performed by a human, which begets the consideration of these annotations to be gold-standard. The author chose the examples from Tratz and Hovy (2010)’s dataset randomly. During the annotation process, we found some difficulties because of the coverage issue of the FrameNet. The wordto-frame mapping in FrameNet has a coverage issue, and it has been widely reported in the literature (Pavlick et al., 2015; Botschen et al., 2017). We categorize the coverage issues into the following: No Candidate Frames: The word-to-frame mapping returned no candidate frame. In some cases, we could find a frame with manual effort (ref. Table 1). However, despite manual efforts, some cases, we could not find an appropriate frame all the time (e.g., star autograph, employee misconduct, etc.). No Suitable Frame in the Candidate Set: In this set, word-to-frame mapping retrieved candidate frames, but none of the candidates was found to be appropriate. For example, candidate frames for heat returned by the mapping are: C AUSE TEMPERATURE CH"
2021.findings-acl.256,W15-0122,0,0.0900705,"s of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forward neural network-based approach. This network takes concatenated embeddings of component nouns as an input and predicts one of the labels from the Tratz and Hovy (2010)’s label set. Fares et al. (2018) used a similar feed-forward network to predict two types of relations. This network, however, shares initial layers and separates output layers for each label type. NC interpretation via paraphrasing is another methodology that contains approaches such as prepositional and free paraphrasing"
2021.findings-acl.256,P16-3011,0,0.0135742,"e cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forward neural network-based approach. This network takes conca"
2021.findings-acl.256,N04-1016,0,0.159611,"dings of component nouns as an input and predicts one of the labels from the Tratz and Hovy (2010)’s label set. Fares et al. (2018) used a similar feed-forward network to predict two types of relations. This network, however, shares initial layers and separates output layers for each label type. NC interpretation via paraphrasing is another methodology that contains approaches such as prepositional and free paraphrasing. Prepositional paraphrasing, i.e., paraphrasing using a preposition, for example, student protest: “protest by student(s)”, is a relatively well-attended problem (Lauer, 1995; Lapata and Keller, 2004; Ponkiya et al., 2018b). All the above approaches proposed for prepositional paraphrasing use the fixed-set of eight prepositions proposed by Lauer (1995). The other set of approaches, i.e., free paraphrasing, however, has not received much attention. Apart from two SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013), it does not have much literature available. A recent study (Ponkiya et al., 2020) expresses paraphrasing as a “fill-in-theblank” problem, and utilizes pre-trained language models, for the task of noun-compound interpretation. 3 Foundations Levi (1978) performed a lingu"
2021.findings-acl.256,H94-1111,0,0.823659,"Missing"
2021.findings-acl.256,P15-2067,0,0.0385092,"Missing"
2021.findings-acl.256,2020.findings-emnlp.386,1,0.716195,"aphrasing. Prepositional paraphrasing, i.e., paraphrasing using a preposition, for example, student protest: “protest by student(s)”, is a relatively well-attended problem (Lauer, 1995; Lapata and Keller, 2004; Ponkiya et al., 2018b). All the above approaches proposed for prepositional paraphrasing use the fixed-set of eight prepositions proposed by Lauer (1995). The other set of approaches, i.e., free paraphrasing, however, has not received much attention. Apart from two SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013), it does not have much literature available. A recent study (Ponkiya et al., 2020) expresses paraphrasing as a “fill-in-theblank” problem, and utilizes pre-trained language models, for the task of noun-compound interpretation. 3 Foundations Levi (1978) performed a linguistic study to understand how noun compounds are generated. They call such compounds nominal compounds. This theory puts nominal compounds into two categories, 2902 based on the compounding process, as 1. Predicate Deletion: Here, a predicate between the components is dropped to create a compound. For example, apple pie is a “pie made from apple.” The predicate made from is dropped in this case. Similarly, fo"
2021.findings-acl.256,L18-1489,1,0.506606,"d from the semantics of the individual noun units present. 2901 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2901–2911 August 1–6, 2021. ©2021 Association for Computational Linguistics From a relation representation perspective, noun compounds are interpreted in two ways: via labelling and paraphrasing. Labelling involves assigning an abstract semantic relation from a predefined set, for example, orange juice: M ADE O F, hillside home: L OCATION, etc. There are many inventories of predefined semantic relations. We use the FrametNet based labels proposed by Ponkiya et al. (2018a). As per their convention, the head noun of a compound invokes the frame, and the modifier noun fits in one of the frame elements of the invoked frame, vide ‘board approval’ in the abstract. There are more than 11,000 FEs in FrameNet, and we have about 1900 training examples. Thus, the average number of examples for each label is quite small, and many labels do not have a training example. In summary, the contributions of this paper are three-fold: 1. We embed FrameNet entities in a continuous space, perform prediction in the continuous space to generalize over unseen labels, and show perfor"
2021.findings-acl.256,C18-1155,1,0.399286,"d from the semantics of the individual noun units present. 2901 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2901–2911 August 1–6, 2021. ©2021 Association for Computational Linguistics From a relation representation perspective, noun compounds are interpreted in two ways: via labelling and paraphrasing. Labelling involves assigning an abstract semantic relation from a predefined set, for example, orange juice: M ADE O F, hillside home: L OCATION, etc. There are many inventories of predefined semantic relations. We use the FrametNet based labels proposed by Ponkiya et al. (2018a). As per their convention, the head noun of a compound invokes the frame, and the modifier noun fits in one of the frame elements of the invoked frame, vide ‘board approval’ in the abstract. There are more than 11,000 FEs in FrameNet, and we have about 1900 training examples. Thus, the average number of examples for each label is quite small, and many labels do not have a training example. In summary, the contributions of this paper are three-fold: 1. We embed FrameNet entities in a continuous space, perform prediction in the continuous space to generalize over unseen labels, and show perfor"
2021.findings-acl.256,P10-1070,0,0.084251,"ompound (say, chocolate cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is similar to ours. Dima and Hinrichs (2015) proposed a feed-forward neural network-based approach. This networ"
2021.findings-acl.256,C94-2125,0,0.607722,"rom http://www.cfilt.iitb.ac.in/nc-dataset. 2 Related Work A relation between the components of a noun compound (say, chocolate cake) can be represented in one of the following two ways: (1) assigning a relation from a predefined set of semantic relations (M ADE O F), or (2) using a paraphrase to convey the underlying semantic relation (“cake made using chocolates” or “cake with chocolate flavor”). Noun-compound (NC) interpretation via labelling is the most commonly used methodology for NC interpretation. Scholars have proposed many inventories of semantic relations (Levi, 1978; Warren, 1978; Vanderwende, 1994; Lauer, 1995; ´ S´eaghdha, 2007; Barker and Szpakowicz, 1998; O Rosario et al., 2001; Tratz and Hovy, 2010; Fares, 2016; Ponkiya et al., 2018a). A recent FrameNetbased inventory by Ponkiya et al. (2018a) proposed FEs (Frame Elements) from FrameNet as labels (or, semantic relations). They released a dataset by annotating each noun compound with a frame and a frame element; and proposed this annotation for predicate ‘nominalization’. However, it also works for most of the cases of ‘predicate deletion’. For automatic labelling, Dima and Hinrichs (2015) and Fares et al. (2018)’s architecture is s"
2021.findings-acl.256,P07-3013,0,0.137977,"Missing"
K16-1016,C10-2005,0,0.0497006,"wn race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010). Introduction This paper addresses the task of Sentiment Analysis (SA) - automatic detection of the sentiment polarity as positive versus negative - of usergenerated short texts and sentences. Several sentiment analyzers exist in literature today (Liu and Zhang, 2012). Recent works, such as Kouloumpis et al. (2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content. Sentiment analysis remains a hard problem, due to the challenges it poses at the various levels, as summarized below. 1.1 Syntactic Challenges 1.3 Lexical Challenges Semantic and Pragmatic Challenges This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other Sentiment analyzers face the following three challe"
K16-1016,W15-2401,0,0.12339,"Missing"
K16-1016,W11-0705,0,0.0607671,"becomes an enemy to his own race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010). Introduction This paper addresses the task of Sentiment Analysis (SA) - automatic detection of the sentiment polarity as positive versus negative - of usergenerated short texts and sentences. Several sentiment analyzers exist in literature today (Liu and Zhang, 2012). Recent works, such as Kouloumpis et al. (2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content. Sentiment analysis remains a hard problem, due to the challenges it poses at the various levels, as summarized below. 1.1 Syntactic Challenges 1.3 Lexical Challenges Semantic and Pragmatic Challenges This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other Sentiment analyzers fac"
K16-1016,C14-1008,0,0.144854,"Missing"
K16-1016,D09-1020,0,0.168406,"Missing"
K16-1016,W14-3010,0,0.0174291,"task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below. 3.1 Dataset 2 Dataset 1 It contains 994 text snippets with 383 positive and 611 negative examples. Out of this, 350 are sarcastic or have other forms of irony. The snippets are a collection of reviews, normalized-tweets and 158 plained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014). Table 1 presents the accuracy of the three systems. The F-scores are not very high for all the systems (especially for dataset 1 that contains more sarcastic/ironic texts), possibly indicating that the snippets in our dataset pose challenges for existing sentiment analyzers. Hence, the selected datasets are ideal for our current experimentation that involves cognitive features. 4 verbs, nouns, adjectives and adverbs in the text. This is computed using NLTK1 . 6. Count of Named Entities (NE) i.e. Number of named entity mentions in the text. This is computed using NLTK. 7. Discourse connectors"
K16-1016,esuli-sebastiani-2006-sentiwordnet,0,0.0126573,"of gaze on a visual object (like characters, words etc. in text) (2) Saccades, corresponding to the transition of eyes between two fixations. Moreover, a saccade is called a Regressive Saccade or simply, Regression if it represents a phenomenon of going back to a pre-visited segment. A portion of a text is said to be skipped if it does not have any fixation. Figure 1 shows eye-movement behavior during annotation of the given sentence in dataset-1. The circles represent 3. Subjective scores (PosScore, NegScore) i.e. Scores of positive subjectivity and negative subjectivity using SentiWordNet (Esuli and Sebastiani, 2006). 4. Sentiment flip count (FLIP) i.e. Number of times words polarity changes in the text. Word polarity is determined using MPQA lexicon. 5. Part of Speech ratios (VERB, NOUN, ADJ, ADV) i.e. Ratios (proportions) of 1 159 http://www.nltk.org/ Figure 1: Snapshot of eye-movement behavior during annotation of an opinionated text. The circles represent fixations and lines connecting the circles represent saccades. Boxes represent Areas of Interest (AoI) which are words of the sentence in our case. also supported by von der Malsburg and Vasishth (2011). fixation and the line connecting the circles r"
K16-1016,D11-1100,1,0.957865,"given the context (e.g., His face fell when he was dropped from the team vs The boy fell from the bicycle, where the verb “fell” has to be disambiguated) (3) Domain Dependency, tackling words that change polarity across domains. (e.g., the word unpredictable being positive in case of unpredictable movie in movie domain and negative in case of unpredictable steering in car domain). Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006). Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classification using our enhanced feature set improves the performance (F-score) of"
K16-1016,C08-1031,0,0.0135792,"and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. Introd"
K16-1016,W14-2609,0,0.0333978,"g the paper in section 7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning b"
K16-1016,P11-1015,0,0.24822,"Missing"
K16-1016,I08-1039,0,0.0272598,"artineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation c"
K16-1016,maynard-greenwood-2014-cares,0,0.0273707,"our approach before concluding the paper in section 7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2"
K16-1016,N13-1088,1,0.88658,"Missing"
K16-1016,P14-2007,1,0.91123,"ties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. Introducing Cognitive Features We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate s"
K16-1016,P13-2062,1,0.885604,"Missing"
K16-1016,P15-2124,1,0.944174,"7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches exist"
K16-1016,W14-2623,1,0.798217,"lassification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. Introducing Cognitive Features We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate short-text with sentiment labels. Our hypothesis is that cognitive processes in the brain are related to eye-movement activities ("
K16-1016,N16-1179,0,0.12689,"Missing"
K16-1016,W04-3253,0,0.0369816,"eatures (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla an"
K16-1016,N10-1120,0,0.0655369,"a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to spa"
K16-1016,P06-2079,0,0.140611,"eatures alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators. Ikeda et al. (2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount"
K16-1016,W13-1605,0,0.0797154,"Missing"
K16-1016,P04-1035,0,0.0236932,"s Considering Dataset -1 and 2 as Test Data It is essential to check whether our selected datasets really pose challenges to existing sentiment analyzers or not. For this, we implement two statistical classifiers and a rule based classifier to check the test accuracy of Dataset 1 and Dataset 2. The statistical classifiers are based on Support Vector Machine (SVM) and N¨aive Bayes (NB) implemented using Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http://www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as exEye-tracking and Sentiment Analysis Datasets We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which"
K16-1016,H05-1044,0,0.330727,"ng Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http://www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as exEye-tracking and Sentiment Analysis Datasets We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which they use for the task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below. 3.1 Dataset 2 Dataset 1 It contains 994 text snippets with 383 positive and 6"
K16-1016,W02-1011,0,0.0277671,"and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011"
K16-1016,P13-1041,1,0.906044,"Missing"
K16-1016,P13-2149,1,0.93296,"nitive features, showing the effectiveness of cognitive features. In section 6, we discuss on the feasibility of our approach before concluding the paper in section 7. forms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.). Such challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone. 1.4 2 Related Work Sentiment classification has been a long standing NLP problem with both supervised"
K16-1016,D13-1066,0,0.0912234,"Missing"
K16-1016,I13-1076,1,0.926356,"Missing"
K16-1016,P06-1134,0,0.109259,"Missing"
L16-1349,P98-1004,0,0.030152,"chine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignmen"
L16-1349,P03-1012,0,0.038474,"t multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in l"
L16-1349,P08-1112,0,0.0233195,"oarse lexical resource using parallel topics obtained from multilingual topic models. We observe that for a machine translation system for English-Hindi, these coarse alignments do fine! In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A statistical machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover ali"
L16-1349,C04-1005,0,0.0538573,"ments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation lexical resource can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Sánchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the wo"
L16-1349,P10-1155,1,0.6771,"nlike so many one to one Cartesian product alignments, our approach keeps them in the same sentence, thus reducing the chances of the system learning non synonymous candidate translations. 2200 Figure 3: Parallel English-Hindi topics as generated by the topic model for the health dataset Thus, for T topics, and K top words, sentential approach results in a coarse lexical resource of T X K pseudo-parallel sentences. The coarse lexical resource for varying values of T is available freely for download. 3.1. Experiment Setup To generate the topics, we use corpora from health and tourism domain by Khapra et al. (2010). These datasets contain approximately 25000 parallel sentences for English - Hindi language pair. We implement the multilingual topic model in Java. Our implementation uses Gibbs sampling as described in the original paper. 3.3. Quantitative Evaluation Two human annotators evaluated the quality of the output obtained. Each word was marked as whether or not a translation in the other language was present in the same topic. The two annotators, A1 and A2, are native speakers of Hindi, and have had 15+ years of academic instruction in English. The inter-annotator agreement between them and their"
L16-1349,P07-2045,0,0.033916,"cal machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in noncomparable corpora. They then use MLTM to create bilingual lexicons for low resource language pairs, and provided candidate translations for more computationFigure 1: Our Sentent"
L16-1349,J00-2004,0,0.0962513,"the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation lexical resource can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data"
L16-1349,D09-1092,0,0.536026,"tem is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in noncomparable corpora. They then use MLTM to create bilingual lexicons for low resource language pairs, and provided candidate translations for more computationFigure 1: Our Sentential Approach to create pseudoparallel data ally intense alignment processes without the sentencealigned translations"
L16-1349,P00-1056,0,0.447221,"op terms for a text classification task. They observe that parallel topics perform better than topic words that are translated into the target language. Approaches that do not rely on parallel corpus have also been reported. Jagarlamudi and Daumé III (2010) use a bilingual lexical resource, and a comparable corpora to estimate a model called JointLDA. Boyd-Graber and Blei (2009) use unaligned corpus and extract multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Bar"
L16-1349,J03-1002,0,0.00587388,"approach to use a coarse lexical resource using parallel topics obtained from multilingual topic models. We observe that for a machine translation system for English-Hindi, these coarse alignments do fine! In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A statistical machine translation (SMT) system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. However, there is not a lot of parallel data available for these languages making it necessary for specialized techniques that improve alignment quality has been felt (Sanchis and Sánchez, 2008; Lee et al., 2006; Koehn et al., 2007). The existing baseline approach is called Cartesian product Approach. This approach was used by Mimno et al. (2009). In their work, they analyzed the characteristics of MLTM in comparison to monolingual LDA, and demonstrated that it is p"
L16-1349,tufis-barbu-2002-lexical,0,0.0758505,"nd Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy,"
L16-1349,J97-3002,0,0.0670303,"ment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufiş and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word se"
L16-1349,C98-1004,0,\N,Missing
L16-1686,E12-1060,0,0.0200859,") cannot be assigned a POS tag. We assign an ’ABR’ tag to such abbreviations. 2.4. The purpose of this resource is to keep up with the continuously updating language of the internet. For this reason, a static resource for slang would fail its very purpose once today’s slang goes out of use. We aim to create a web-crawler which runs continuously on various user forums and bulletins. The crawler would monitor the usage of words over these websites. A new word which appears poses the probability of being a slang word. Further we also aim to use methods such as described in Cook et al. (2014) and Lau et al. (2012), to monitor if a new sense seems to be emergent. We observe that very few of the definitions provided in Urban Dictionary are close to conventional. The primary 3 http://www.reddit.com 4 http://api.urbandictionary.com/ Dynamism 3. Validation We validate our claims of a structured, usable resource by performing experiments using WSD engines. We use the Lesk and Extended Lesk algorithms, implemented through 4330 Word Text Poster Follower Gloss / Definition A short message sent using a mobile device usually through a protocol such as a short messaging service (SMS) A person who uploads a message"
L16-1686,esuli-sebastiani-2006-sentiwordnet,0,0.00620249,"h can augment the English WordNet while dealing with neologisms and slang words on the internet. We show that the general accuracy obtained by using Lesk and extended Lesk is greater than when Urban Dictionary is directly used (Swerdfeger, Online). Currently, our resource holds 3000 slang words. However, this figure is constantly being updated as described in section 2.4. above. Our resource aims to mitigate the effect of non-conventional language on the internet. We notice that several of these slang words are sentiment bearing and annotating them with sentiment scores like the SentiWordNet (Esuli and Sebastiani, 2006) would help Sentiment Analysis for data on the web. We aim to do the same as a part of our future work. Also as future work, we aim to span out to different languages. We plan to identify slang from various languages and link semantically similar slang words. For example, the word LOL (Laughing Out Loud) is synonymous to the French MDR (Morte De 4331 Rire). Such linkages could be used to aid real-time Machine Translation in both text-to-text and speech-to-text scenarios. We aim to continuously update our resource with respect to adding new slang words to our repository. We also aim to make our"
L16-1686,P10-1023,0,0.0337969,"sable as a lexical resource. Princeton WordNet or the English WordNet (Fellbaum, 1998) is an online lexical resource which can be used for various NLP applications such as WSD, Machine Translation, Information retrieval, etc. Based on English WordNet, several other WordNets like the EuroWordNet(Vossen, 1998), IndoWordNet(Bhattacharyya, 2010) and MultiWordNet(Pianta et al., 2002) were created. We create a WordNet like structure which can be utilized for the aforementioned NLP applications. Other such works which are built upon a WordNet like concept and produce augmented resources are BabelNet(Navigli et al., 2010) and FrameNet(Baker et al., 1998). VerbNet(Schuler, 2005) is another such verb lexicon currently available for English. It is a hierarchical domain-independent, broad-coverage verb lexicon with mappings to the English WordNet. ConceptNet(Liu and Singh, 2004) is a semantic network, built from nodes (or “terms”), representing words or short phrases of natural language, that labels the relationships between them. Our method helps refine the data from Urban Dictionary; using several manual changes and mappings, we normalize this to a structured WordNet like resource. We validate the usability of o"
L16-1686,P98-1013,0,\N,Missing
L16-1686,C98-1013,0,\N,Missing
L18-1728,bhattacharyya-2010-indowordnet,1,0.554367,"Italian, Spanish, German, French, Czech and Estonian. Each of these wordnets is structured in the same way as the Princeton WordNet for English (Miller et al., 1990) - synsets (sets of synonymous words) and semantic relations between them. Each wordnet separately captures a language-specific information. In addition, the wordnets are linked to an Inter-Lingual-Index, which uses Princeton WordNet as a base. This index enables one to go from concepts in one language to similar concepts in any other language. Such features make this resource helpful in crosslingual NLP applications. IndoWordNet (Bhattacharyya, 2010) is a linked wordnet comprising of wordnets for major Indian languages, viz, Assamese, Bengali, Bodo, Gujarati, Hindi, Kannada, Kashmiri, Konkani, Malayalam, Manipuri, Marathi, Nepali, Oriya, Punjabi, Sanskrit, Tamil, Telugu, and Urdu. These wordnets have been created using the expansion approach with Hindi WordNet as a pivot, which is partially linked to English WordNet. We exploit these links to create mappings from English WordNet to wordnets of other languages. 3. Resources In this section, we describe the resources released with our work. We release two primary resources with our dataset"
L18-1728,W98-0705,0,0.362976,"1. Introduction 2. Wordnets (Fellbaum, 1998) have been useful in different Natural Language Processing applications such as Word Sense Disambiguation (TufiS¸ et al., 2004; Sinha et al., 2006), Machine Translation (Knight and Luk, 1994) etc. Linked Wordnets are extensions of wordnets. In addition to language-specific information captured in constituent wordnets, linked wordnets have a notion of an interlingual index, which connects similar concepts in different languages. Such linked wordnets have found their application in machine translation (Hovy, 1998), cross-lingual information retrieval (Gonzalo et al., 1998), etc. Given the extensive application of wordnets in different NLP applications, creation and maintenance of wordnets involve expert involvement. Such involvement is costly both in terms of time and resources. This is further amplified in case of linked wordnets, where experts need to have knowledge of multiple languages. India is a vast country with massive language diversity. According to a census in 2001, there are 122 major languages 1 , out of which, 29 have more than a million native speakers. The IndoWordNet project contains wordnets of 18 of these languages. These wordnets were create"
L18-1728,C12-3030,1,0.848256,"Missing"
L18-1728,2016.gwc-1.57,1,0.713756,"an culture. Thus, their corresponding variant is not available in the Princeton WordNet (and is not likely to be included anytime). Thus, one needs to maintain the translation/transliteration of such notions from Indian languages 3 https://wordnet.princeton.edu/wordnet/ man/wnstats.7WN.html 4 https://wordnet.princeton.edu/man/ sensemap.5WN.html 4605 to the English language as a separate bilingual mapping 5 . A similar issue arises in case of proper nouns, which should be present in an Indian lexicon but they are not present in Princeton WordNet. They are also handled using bilingual mappings (Singh et al., 2016). Some of the synsets in Indian languages are too fine-grained and have a common representation in the English language. This is why we use the principle of Hypernymy linkages for linking such concepts. We reserve a set of synset id numbers later for language specific concepts and create them to include in these wordnets, individually. These are not linked to the Princeton WordNet and hence are not included in our resource. 5. Conclusion and Future Work In this paper, we describe two resources released along with this paper. We discussed the Indian language wordnets that are part of the IndoWo"
L18-1728,C04-1192,0,0.0469016,"Missing"
N13-1088,de-melo-etal-2012-empirical,0,0.0236507,"Missing"
N13-1088,I11-1078,1,0.837435,"xplicitly in the literature. NLP researchers define it according to their convenience. In our current work, we strive to unravel In this paper, we delve deep into the cognitive roles associated with sense disambiguation through the means of an eye-tracking device capturing the gaze patterns of lexicographers, during the annotation process. In-depth discussions with trained lexicographers indicate that there are multiple cognitive sub-processes driving the sense disambiguation task. The eye movement paths available from the screen recordings done during sense annotation conform to this theory. Khapra et al. (2011) points out that the accuracy of various WSD algorithms is poor on certain 733 Proceedings of NAACL-HLT 2013, pages 733–738, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Part-of-speech (POS) categories, particularly, verbs. It is also a general observation for lexicographers involved in sense annotation that there are different levels of difficulties associated with various classes of words. This fact is also reflected in our analysis on sense annotation. The data available after the eye-tracking experiments gave us the fixation times and saccades pertaini"
N13-1088,passonneau-etal-2010-word,0,0.02543,"locations during sentence reading. Their study indicates that local lexical predictability influences in decisions but not where the initial fixation lands in a word. In another work based on word grouping hypothesis and eye 734 movements during reading by Drieghe et al. (2008), the distribution of landing positions and durations of first fixations in a region containing a noun preceded by either an article or a high-frequency three-letter word were compared. Recently, some work is done on the study of sense annotation. A study of sense annotations done on 10 polysemous words was conducted by Passonneau et al. (2010). They opined that the word meanings, contexts of use, and individual differences among annotators gives rise to inter-annotation variations. De Melo et al. (2012) present a study with a focus on MASC (Manually-Annotated SubCorpus) project, involving annotations done using WordNet sense identifiers as well as FrameNet lexical units. In our current work we use eye-tracking as a tool to make findings regarding the cognitive processes connected to the human sense disambiguation procedure, and to gain a better understanding of “contextual evidence” which is of paramount importance for human annota"
N13-1088,P10-1154,0,0.0344902,"Missing"
P16-1104,W14-2609,0,0.240048,"implicated one. Giora (1995), on the other hand, define sarcasm as a mode of indirect negation that requires processing of both negated and implicated messages. Ivanko and Pexman (2003) define sarcasm as a six tuple entity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic"
P16-1104,W10-2914,0,0.661187,"a mode of indirect negation that requires processing of both negated and implicated messages. Ivanko and Pexman (2003) define sarcasm as a six tuple entity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features alone. With the advent of sophisticated eyetrac"
P16-1104,P15-2124,1,0.683305,"(1995), on the other hand, define sarcasm as a mode of indirect negation that requires processing of both negated and implicated messages. Ivanko and Pexman (2003) define sarcasm as a six tuple entity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features a"
P16-1104,W13-1605,0,0.0496852,"Missing"
P16-1104,maynard-greenwood-2014-cares,0,0.108701,"ntity consisting of a speaker, a listener, Context, Utterance, Literal Proposition and Intended Proposition and study the cognitive aspects of sarcasm processing. Computational linguists have previously addressed this problem using rule based and statistical techniques, that make use of : (a) Unigrams and Pragmatic features (Carvalho et al., 2009; Gonz´alez-Ib´anez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag 1096 interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Most of the previously done work on sarcasm detection uses distant supervision based techniques (ex: leveraging hashtags) and stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). But, detecting sarcasm in linguistically well-formed structures, in absence of explicit cues or information (like emoticons), proves to be hard using such linguistic/stylistic features alone. With the advent of sophisticated eyetrackers and electro/magneto-encephalographic (EEG/MEG) devices, it has been possible to delve deep into the cognitive underpinnings of sarcasm understanding. Fil"
P16-1104,P04-1035,0,0.0220621,"4 141 t 14.1 14.0 9.5 13.9 11.9 13.2 15.3 p 5.84E-39 1.71E-38 3.74E-20 1.89E-37 2.75E-28 6.79E-35 3.96E-43 Table 1: T-test statistics for average fixation duration time per word (in ms) for presence of sarcasm (represented by S) and its absence (NS) for participants P1-P7. information. 3.1 Document Description The database consists of 1,000 short texts, each having 10-40 words. Out of these, 350 are sarcastic and are collected as follows: (a) 103 sentences are from two popular sarcastic quote websites3 , (b) 76 sarcastic short movie reviews are manually extracted from the Amazon Movie Corpus (Pang and Lee, 2004) by two linguists. (c) 171 tweets are downloaded using the hashtag #sarcasm from Twitter. The 650 non-sarcastic texts are either downloaded from Twitter or extracted from the Amazon Movie Review corpus. The sentences do not contain words/phrases that are highly topic or culture specific. The tweets were normalized to make them linguistically well formed to avoid difficulty in interpreting social media lingo. Every sentence in our dataset carries positive or negative opinion about specific “aspects”. For example, the sentence “The movie is extremely well cast” has positive sentiment about the a"
P16-1104,P11-2102,0,0.127089,"Missing"
P16-1104,D13-1066,0,0.525184,"Missing"
P18-1219,P16-1068,0,0.021795,"died by Rayner (1998), where he found that unpredictable words are less likely to be skipped than predictable words. Shermis and Burstein (2013) gives a brief overview of how text-based features are used in multiple aspects of essay grading, including grammatical error detection, sentiment analysis, shortanswer scoring, etc. Their work also describes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap"
P18-1219,P05-1018,0,0.306238,"stems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzal"
P18-1219,D16-1115,0,0.137915,"ctable words are less likely to be skipped than predictable words. Shermis and Burstein (2013) gives a brief overview of how text-based features are used in multiple aspects of essay grading, including grammatical error detection, sentiment analysis, shortanswer scoring, etc. Their work also describes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discours"
P18-1219,W17-5050,0,0.150285,"Missing"
P18-1219,P14-2007,1,0.850171,"allenge in NLP. It has been studied since Page’s seminal work on automatic essay grading in the mid-1960s (Page, 1966). This is due to the dependence of quality on different aspects such as the overall structure of the text, clarity, etc. that are highly qualitative in nature, and whose scoring can vary from person to person (Person, 2013). Scores for such qualitative aspects cannot be inferred solely from the text and would benefit from psycholinguistic information, such as gaze behaviour. Gaze based features have been used for co-reference resolution (Ross et al., 2016), sentiment analysis (Joshi et al., 2014) and translation annotation complexity estimation (Mishra et al., 2013). They could also be very useful for education applications, like evaluating readability (Mishra et al., 2017) and in automatic essay grading. In this paper, we consider the following qualitative properties of text: Organization, Coherence and Cohesion. A text is well-organized if it begins with an introduction, has a body and ends with a conclusion. One of the other aspects of organization is the fact that it takes into account how the content of the text is split into paragraphs, with each paragraph denoting a single idea"
P18-1219,W15-1814,0,0.0271384,"ricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015), etc.. Our work uses eyetracking to predict the score given by a reader to a complete piece of text (rather than just a sentence as done by Klerke et al. (2015)) and show that the scoring is more reliable if the reader has understood the text. 4 Features In order to predict the scores of the different properties of the text, we use the following text and gaze features. 4.1 Text-based Features We use a set of text-based features to come up with a baseline system to predict the scores for different properties. The first set of features that we use are length and count-based features, such as wo"
P18-1219,W02-0109,0,0.0151885,"features, namely the degree of polysemy, coreference distance, and the Flesch Reading Ease Score (FRES) (Flesch, 1948). These features help in normalizing the gaze features for text complexity. These features were extracted using Stanford CoreNLP (Manning et al., 2014), and MorphAdorner (Burns, 2013). The third set of features that we use are stylistic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK7 (Loper and Bird, 2002). 6 https://writing.wisc.edu/Handbook/ Transitions.html 7 http://www.nltk.org/ The fourth set of features that we use are word embedding features. We use the average of word vectors of each word in the essay, using Google News word vectors (Mikolov et al., 2013). The word embeddings are 300 dimensions. We also calculate the mean and maximum similarities between the word vectors of the content words in adjacent sentences of the text, using GloVe word embeddings8 (Pennington et al., 2014). The fifth set of features that we use are language modeling features. We use the count of words that are ab"
P18-1219,P14-5010,0,0.00257896,"based features to come up with a baseline system to predict the scores for different properties. The first set of features that we use are length and count-based features, such as word length, word count, sentence length, count of transition phrases6 etc. (Persing and Ng, 2015; Zesch et al., 2015). The next set of features that we use are complexity features, namely the degree of polysemy, coreference distance, and the Flesch Reading Ease Score (FRES) (Flesch, 1948). These features help in normalizing the gaze features for text complexity. These features were extracted using Stanford CoreNLP (Manning et al., 2014), and MorphAdorner (Burns, 2013). The third set of features that we use are stylistic features such as the ratios of the number of adjectives, nouns, prepositions, and verbs to the number of words in the text. These features are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK7 (Loper and Bird, 2002). 6 https://writing.wisc.edu/Handbook/ Transitions.html 7 http://www.nltk.org/ The fourth set of features that we use are word embedding features. We use the average of word vectors of each word in the essay, using Google News word vectors (Mikolov"
P18-1219,P13-2062,1,0.916596,"atic essay grading in the mid-1960s (Page, 1966). This is due to the dependence of quality on different aspects such as the overall structure of the text, clarity, etc. that are highly qualitative in nature, and whose scoring can vary from person to person (Person, 2013). Scores for such qualitative aspects cannot be inferred solely from the text and would benefit from psycholinguistic information, such as gaze behaviour. Gaze based features have been used for co-reference resolution (Ross et al., 2016), sentiment analysis (Joshi et al., 2014) and translation annotation complexity estimation (Mishra et al., 2013). They could also be very useful for education applications, like evaluating readability (Mishra et al., 2017) and in automatic essay grading. In this paper, we consider the following qualitative properties of text: Organization, Coherence and Cohesion. A text is well-organized if it begins with an introduction, has a body and ends with a conclusion. One of the other aspects of organization is the fact that it takes into account how the content of the text is split into paragraphs, with each paragraph denoting a single idea. If the text is too long, and not split into paragraphs, one could con"
P18-1219,D14-1162,0,0.0830811,"atures are used to model the distributions of PoS tags in good and bad texts. These were extracted using NLTK7 (Loper and Bird, 2002). 6 https://writing.wisc.edu/Handbook/ Transitions.html 7 http://www.nltk.org/ The fourth set of features that we use are word embedding features. We use the average of word vectors of each word in the essay, using Google News word vectors (Mikolov et al., 2013). The word embeddings are 300 dimensions. We also calculate the mean and maximum similarities between the word vectors of the content words in adjacent sentences of the text, using GloVe word embeddings8 (Pennington et al., 2014). The fifth set of features that we use are language modeling features. We use the count of words that are absent in Google News word vectors and misspelled words using the PyEnchant9 library. In order to check the grammaticality of the text, we construct a 5-gram language model, using the Brown Corpus (Francis and Kucera, 1979). The sixth set of features are sequence features. These features are particularly useful in modeling organization (sentence and paragraph sequence similarity) (Persing et al., 2010), coherence and cohesion (PoS and lemma similarity). Pitler et al. (2010) showed that co"
P18-1219,D10-1023,0,0.06521,"Missing"
P18-1219,P15-1053,0,0.0195901,"l. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015), etc.. Our work uses eyetracking to predict the score given by a reader to a complete piece of text (rather than just a sentence as done by Klerke et al. (2015)) and show that the scoring is more reliable if the reader has understood the text. 4 Features In order to pr"
P18-1219,W16-1904,1,0.815823,"e quality of a text is an interesting challenge in NLP. It has been studied since Page’s seminal work on automatic essay grading in the mid-1960s (Page, 1966). This is due to the dependence of quality on different aspects such as the overall structure of the text, clarity, etc. that are highly qualitative in nature, and whose scoring can vary from person to person (Person, 2013). Scores for such qualitative aspects cannot be inferred solely from the text and would benefit from psycholinguistic information, such as gaze behaviour. Gaze based features have been used for co-reference resolution (Ross et al., 2016), sentiment analysis (Joshi et al., 2014) and translation annotation complexity estimation (Mishra et al., 2013). They could also be very useful for education applications, like evaluating readability (Mishra et al., 2017) and in automatic essay grading. In this paper, we consider the following qualitative properties of text: Organization, Coherence and Cohesion. A text is well-organized if it begins with an introduction, has a body and ends with a conclusion. One of the other aspects of organization is the fact that it takes into account how the content of the text is split into paragraphs, w"
P18-1219,C14-1090,0,0.0243358,"scribes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain a"
P18-1219,P06-2103,0,0.0478763,"years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015),"
P18-1219,D16-1193,0,0.156078,"re he found that unpredictable words are less likely to be skipped than predictable words. Shermis and Burstein (2013) gives a brief overview of how text-based features are used in multiple aspects of essay grading, including grammatical error detection, sentiment analysis, shortanswer scoring, etc. Their work also describes a number of current essay grading systems that are R (Attali and available in the market like E-rater Burstein, 2004). In recent years, there has been a lot of work done on evaluating the holistic scores of essays, using deep learning techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for"
P18-1219,W15-0626,0,0.0909798,"ng techniques (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong and Zhang, 2016). There has been little work done to model text organization, such as Persing et al. (2010) (using machine learning) and Taghipour (2017) (using neural networks). However, there has been a lot of work done to model coherence and cohesion, using methods like lexical chains (Somasundaran et al., 2014), an entity grid (Barzilay and Lapata, 2005), etc. An interesting piece of work to model coherence was done by Soricut and Marcu (2006) 2354 where they used a machine translation-based approach to model coherence. Zesch et al. (2015) use topical overlap to model coherence for essay grading. Discourse connectors are used as a heuristic to model cohesion by Zesch et al. (2015) and Persing and Ng (2015). Our work is novel because it makes use of gaze behaviour to model and predict coherence and cohesion in text. In recent years, there has been some work in using eye-tracking to evaluate certain aspects of the text, like readability (Gonzalez-Gardu˜no and Søgaard, 2017; Mishra et al., 2017), grammaticality (Klerke et al., 2015), etc.. Our work uses eyetracking to predict the score given by a reader to a complete piece of text"
P18-1219,P10-1056,0,0.0813857,"Missing"
W14-0126,N13-1088,1,0.885924,"Missing"
W14-0126,N03-1032,0,0.0718943,"Missing"
W14-0126,C12-3033,1,\N,Missing
W14-5126,W04-2215,0,0.0323745,"ously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP tasks such as POS tagging and chunking, but not much work has gone in the direction of corpora generation aid for Machine Translation (MT). The few similar works that we did find are noted below. PolyPhraZ (Hajlaoui and Boitet, 2004) is one such tool which helps in visualizing, editing and evaluating MT systems on parallel corpora. CasualConc (Imao, 2008) is a parallel concordancer which generates keyword in context concordance lines, word clusters, collocation analysis, and word counts. MemoQ (Kilgray, 2006) and Trados (SDL, 2007) are also Computer Aided Translation (CAT) D S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 162–166, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen syst"
W14-5126,P07-2045,0,0.00501907,"to ensure time and cost effectiveness of the process. Traditional method of parallel corpus creation 162 involves manual translation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text"
W14-5126,J03-1002,0,0.00655842,"slation of every sentence by inputting a monolingual corpus and translating its each sentence. But, strict quality checks and skilled translators need to be employed to ensure correctness and, usually, the process of translation is followed by a validation phase to ensure quality and reliability. The process of parallel corpora generation can be divided into the following phases: translation, validation and sentence alignment. Furthermore, to help SMT tools like Moses (Koehn et al., 2007), it would be desirable to manually correct word alignments generated by an automatic tool such as GIZA++ (Och and Ney, 2003). We present a comprehensive workbench to streamline the process of corpora creation for SMT. This common workbench allows for corpora generation, validation, evaluation, alignment and management simultaneously. We aim to simplify the laborious manual task of corpora generation for all language pairs, and provide with aides at each step. 2 Related Work There are a wide class of document management solutions and products which fall under the category of “corpora and text mining”. We find that though a lot of effort has gone into creating tools to aid in corpora generation for lower level NLP ta"
W14-5126,E03-1016,0,0.0455033,"Goa, India. December 2014. 2014 NLP Association of India (NLPAI) Figure 1: Snapshot of PaCMan on validation / translation screen systems which are commercially available with features like Translation memory, and Term Extraction. Wordfast is CAT system having just one free version “WordFast Anywhere”. We studied and used the system, but found the interface less intuitive, and hard to use. “WordFast Anywhere” also has an integrated MT system which provides translations via Microsoft Bing and an integrated MT system. Another system we came across is a web based text corpora development system (Yablonsky, 2003) that focuses on the development of UML-specifications, architecture and implementations of DBMS tools. None of the above mentioned systems provide a word alignment visualization, which can be corrected manually, and saved to provide perfect phrase tables later. 3 Parallel Corpora Management System Parallel Corpora Management System (PaCMan) (Figure: 1) is a platform-independent web-based workbench for managing all the processes involved in the generation of good quality parallel corpora. Along with covering the procedural / managerial aspects of the parallel corpora generation process, this t"
W14-5126,E03-1063,0,\N,Missing
W14-5126,kunchukuttan-etal-2014-shata,1,\N,Missing
W15-5916,W14-3348,0,0.0151644,"cy score of the chat output and Adequacy score of the chat output. Fluency indicates the grammatical correctness where as adequacy indicates how appropriate the machine translation is, when it comes to semantic transfer. We then compute the average Inter Annotator Agreement (IAA) between their scores. Table 6 presents the results. We have obtained a substantial IAA via Fliess’ Kappa evaluation. To justify the second point, we input the un-normalized chat messages to three differ110 ent translators viz. Google, Bing and SataAnuvaadak. We then obtain the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) translation scores. We then compute the evaluation scores after applying the pre- and post-processing steps explained in section 2. Table 4 and 5 present the scores for processed and un-processed inputs. As we can see, applying pre- and post-processing steps help us achieve better evaluation scores. We observe a slight inconsistency between BLEU and METEOR scores for some cases. BLEU has been shown to be less effective for systems involving Indian Languages due to language divergence, free-word order and morphological richness (Ananthakrishnan et al., 2007). On the other hand, our METEOR modu"
W15-5916,W14-3308,1,0.899573,"the pre- and post-processing steps explained in section 2. Table 4 and 5 present the scores for processed and un-processed inputs. As we can see, applying pre- and post-processing steps help us achieve better evaluation scores. We observe a slight inconsistency between BLEU and METEOR scores for some cases. BLEU has been shown to be less effective for systems involving Indian Languages due to language divergence, free-word order and morphological richness (Ananthakrishnan et al., 2007). On the other hand, our METEOR module has been modified to support stemming, paraphrasing and lemmatization (Dungarwal et al., 2014) for Indian Languages, tackling such nuances to some extent. This may have accounted for the score differences. 5 Conclusion and Future Work In today’s era of short messaging and chatting, there is a great need to make available a multilingual chat system where users can interact despite the language barrier. We present such an Indian language IM application that facilitates cross lingual text based communication. Our success in developing such an application for English to Indian languages is a small step in providing people with easy access to such a chat system. We plan to make efforts towa"
W15-5916,2000.tc-1.6,0,0.0626886,"Missing"
W15-5916,P07-2045,0,0.0059458,"MS and Chatting) to plain English. Use of such language (often referred as Chatting Language) induces noise which poses additional processing challenges. While dictionary lookup based methods6 are popular for Normalization, they can not make use of context and domain knowledge. For example, yr can have multiple translations like year, your. We tackle this by implementing a normal6 http://www.lingo2word.com 108 ization system7 (Raghunathan and Krawczyk, 2009) as a Phrase Based Machine Translation System, that learns normalization patterns from a large number of training examples. We use Moses (Koehn et al., 2007), a statistical machine translation system that allows training of translation models. Training process requires a Language Model of the target language and a parallel corpora containing aligned un-normalized and normalized word pairs. Our language model consists of 15000 English words taken from the web. Parallel corpora was collected from the following sources : 1. Stanford Normalization Corpora which consists of 9122 pair of un-normalized and normalized words / phrases. 2. The above corpora, however, lacks acronyms and short hand texts like 2mrw, l8r, b4 which are frequently used in chattin"
W15-5916,kunchukuttan-etal-2014-shata,1,0.853522,"t time or a farmer from Punjab trying to get tips from a Tamil speak1 http://www.cfilt.iitb.ac.in/transchat/ 106 ing professor on modern agricultural tools and techniques, communication is often hindered by language barrier. This problem has been recognized and well-studied by computational linguists as a result of which a large number of automatic translation systems have been proposed and modified in the last 30 years. Some of the notable Indian Language translation systems include Anglabharati (Sinha et al., 1995), Anusaraka (Padmanathrao, 2009), Sampark (Anthes, 2010) and Sata-Anuvaadak2 (Kunchukuttan et al., 2014). Popular organizations like Google and Microsoft also provide translation solutions for Indian languages through Google- and BingTranslation systems. Stymne (2011) demonstrate techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation, which can be utilized in a chat scenario. But even after so many years of MT research, one can still claim that these systems have not been able to attract a lot of users. This can be attributed to factors like(a) poor user experience in terms of UI design, (b) systems being highly computationalresource intensive and slow i"
W15-5916,P02-1040,0,0.108661,"the system, Usability score, Fluency score of the chat output and Adequacy score of the chat output. Fluency indicates the grammatical correctness where as adequacy indicates how appropriate the machine translation is, when it comes to semantic transfer. We then compute the average Inter Annotator Agreement (IAA) between their scores. Table 6 presents the results. We have obtained a substantial IAA via Fliess’ Kappa evaluation. To justify the second point, we input the un-normalized chat messages to three differ110 ent translators viz. Google, Bing and SataAnuvaadak. We then obtain the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) translation scores. We then compute the evaluation scores after applying the pre- and post-processing steps explained in section 2. Table 4 and 5 present the scores for processed and un-processed inputs. As we can see, applying pre- and post-processing steps help us achieve better evaluation scores. We observe a slight inconsistency between BLEU and METEOR scores for some cases. BLEU has been shown to be less effective for systems involving Indian Languages due to language divergence, free-word order and morphological richness (Ananthakrishnan et al., 20"
W15-5916,W11-2159,0,0.0191858,"nication is often hindered by language barrier. This problem has been recognized and well-studied by computational linguists as a result of which a large number of automatic translation systems have been proposed and modified in the last 30 years. Some of the notable Indian Language translation systems include Anglabharati (Sinha et al., 1995), Anusaraka (Padmanathrao, 2009), Sampark (Anthes, 2010) and Sata-Anuvaadak2 (Kunchukuttan et al., 2014). Popular organizations like Google and Microsoft also provide translation solutions for Indian languages through Google- and BingTranslation systems. Stymne (2011) demonstrate techniques for replacement of unknown words and data cleaning for Haitian Creole SMS translation, which can be utilized in a chat scenario. But even after so many years of MT research, one can still claim that these systems have not been able to attract a lot of users. This can be attributed to factors like(a) poor user experience in terms of UI design, (b) systems being highly computationalresource intensive and slow in terms of response time, and (c) bad quality translation output. Moreover, the current MT interfaces do not provide a natural environment to attract more number of"
W15-5945,P98-1004,0,0.0494915,"chine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignment"
W15-5945,J93-2003,0,0.0480692,"for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment quality has been felt (Sanchis and Snchez, 20"
W15-5945,P03-1012,0,0.0503326,"t multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in lo"
W15-5945,P08-1112,0,0.0229563,", statistical machine translation (SMT), and automatic construction of bilingual text. Statistical Machine Translation (SMT) is a technology for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently"
W15-5945,C04-1005,0,0.051151,"nments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Snchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the word may"
W15-5945,I13-2006,1,0.856431,"form word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanchis and Snchez, 2008; Lee et al., 2006) give valuable information about words and their neighbors, thus identifying a class to which the word may belong. This helps in disambiguation and thus selecting word correspondences but can also give rise to increased vocabulary thus making the training data more sparse. Joshi et al. (2013) use in domain parallel data to inject additional alignment mappings for the news headline domain. Finally, Koehn et al. (2007) propose a factored translation model that can incorporate any linguistic factors including POS information in phrase-based SMT. It provides a generalized representation of a translation model, because it can map multiple source and target factors. It may help to effectively handle out-of-vocabulary (OOV) by incorporating many linguistic factors, but it still crucially relies on the initial quality of word alignment that will dominate the translation probabilities. In"
W15-5945,P10-1155,1,0.670319,"y the topic model for the tourism dataset 5 Experiment Setup 2. Cartesian product Approach: In this approach the pseudo parallel data was created using MLTM approach described earlier, and added to the training data before training the MT systems. We added the pseudo parallel data to training data using the approach indicated in Figure 3. Thus, for 50 topics and 11 top words, we add 550 pseudo-parallel sentences, each of length 1. In this section, we describe the dataset, and setup for the experiments conducted. 5.1 Dataset For our experiments, we use corpora from health and tourism domain by Khapra et al. (2010). These datasets contain approximately 25000 parallel sentences for English - Hindi language pair. We use these for both the creation of pseudo parallel data, and training Machine translation systems. We separate 500 sentences each for testing and tuning purposes. We ensure that they are not present in the training corpus. 5.2 3. Sentential Approach: We added the pseudo parallel data created using MLTM approach to the training data using the approach indicated in Figure 4. Thus, for 50 topics and 11 top words, we add 50 pseudo-parallel sentences, each of length 11. Setup We implemented the mul"
W15-5945,P07-2045,0,0.0298051,"dely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between words that infrequently occur in the training data. Hence, a need for specialized techniques that improve alignment quality has been felt (Sanchis and Snchez, 2008; Lee et al., 2006; Koehn et al., 2007). Mimno et al. (2009) present a multilingual topic model called PolyLDA, and apply it for Machine Translation for European and other languages such as Danish, German, Greek, English, Spanish, etc. Since multilingual topic models generate parallel topics: parallel clusters of words that are likely to be about the same theme, these topics provide coarse alignment that a Moses-like translation system can leverage on. The idea is to not rely on any external ontology such as WordNet and to rely purely on a parallel corpus to create such coarse alignments. The focus of our paper is to improve word a"
W15-5945,J00-2004,0,0.0224318,"the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word sense disambiguation and select appropriate translations while a translation dictionary can only list all translations for each word or phrase. Some researchers use Part-of-speeches (POS), which represent morphological classes of words, tagging on bilingual training data (Sanc"
W15-5945,D09-1092,0,0.0304376,"Missing"
W15-5945,P00-1056,0,0.750202,"the top terms for a text classification task. They observe that parallel topics perform better than topic words that are translated into the target language. Approaches that do not rely on parallel corpus have also been reported. Jagarlamudi and Daum´e III (2010) use a bilingual dictionary, and a comparable corpora to estimate a model called JointLDA. Boyd-Graber and Blei (2009) use unaligned corpus and extract multilingual topics using a multilingual topic model called MuTo. The second area that our work is related to is improvement of alignment between words/phrases for machine translation. Och and Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barb"
W15-5945,J03-1002,0,0.00636946,"ense disambiguation, statistical machine translation (SMT), and automatic construction of bilingual text. Statistical Machine Translation (SMT) is a technology for the automatic translation of text in one 308 natural language into another. In a country like India where more than 22 official languages are spoken across 29 states, the task of translation becomes immensely important. A SMT system typically uses two modules: alignment and reordering. The quality of an SMT system is dependent on the alignments discovered. The initial quality of word alignment is known to impact the quality of SMT (Och and Ney, 2003; Ganchev et al., 2008). Many SMT based systems are evaluated in terms of the information gained from the word alignment results. IBM models (Brown et al., 1993) are among the most widely used models for statistical word alignment. For these models, having a large parallel dataset can result in good alignment, and hence, facilitate a good quality SMT system. However, there is not a lot of parallel data available for English to Indian Languages, or for one Indian Language to another. Without sufficient amount of parallel corpus, it is very difficult to learn the correct correspondences between"
W15-5945,tufis-barbu-2002-lexical,0,0.0496959,"nd Ney (2000) describe improved alignment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy,"
W15-5945,J97-3002,0,0.384589,"nment models for statistical machine translation. They use both the phrase based and word based approaches to extend the baseline alignment models. Their results show that this method improved precision without loss of recall in English to German alignments. However, if the same unit is aligned to two different target units, this method is unlikely to make a selection. Cherry and Lin (2003) model the alignments directly given the sentence pairs whereas some researchers use similarity and association measures to build alignment links (Ahrenberg et al., 1998; Tufi and Barbu, 2002). In addition, Wu (1997) use a stochastic inversion transduction grammar to simultaneously parse the sentence pairs to get the word or phrase alignments. Some researchers use preprocessing steps to identity multi-word units for word alignment (Ahrenberg et al., 1998; Tiedemann, 1999; Melamed, 2000). These methods obtain multi-word candidates, but are unable to handle separated phrases and multiwords in low frequencies. Hua and Haifeng (2004) use a rule based translation system to improve the results of statistical machine translation. It can translate multiword alignments with higher accuracy, and can perform word se"
W15-5945,C98-1004,0,\N,Missing
W17-7518,P15-2124,1,0.83101,"admissions process. A well written SOP is a must for an applicant to ensure their admission in any university, and more so for elite universities. Their thoughts and ideas should be organized in their statement. University guidelines1,2 , Alumni blogs3 , and Admission consultancy blogs4 recommend spending ample time on each SOP and tailoring it to perfection. They also recommend stylometry for writing an essay i.e. word limit, active voice, coherence, and continuity. Various NLP applications like Essay grading (Larkey, 1998), Text Summarization (Gupta and Lehal, 2010) and Sentiment Analysis (Joshi et al., 2015) utilize these features. Hence, we believe that an application that evaluates their statement is crucial. The key question that this paper attempts to answer is: ‘ Can information gained from an SOP be used to predict the outcome of a candidate application for graduate school admissions? ’ 3 Related Work Ward (2006) discuss a qualitative model for Graduate Admissions to Computer Science programs but do not use any Machine Learning or Deep Learning based techniques for estimating a likelihood. According to them, other factors which affect the decision of the committee reviewing the applications"
W17-7518,W02-0109,0,0.241608,"Missing"
W17-7518,D14-1162,0,0.0807167,"olysemy - Average number of WordNet (Fellbaum, 2010) senses per word. 4.5.3 Document Similarity Score and Error based Features 1. Cosine Similarity - Cosine Similarity Score of an SOP with the corpus of accepted essays dataset, where we ensure that the SOP being compared is not a part of the accepted essay corpus. 8 http://www.cfilt.iitb.ac.in/ cognitive-nlp/ 2. Similarity-based features using GloVe The similarity between every pair of content words in adjacent sentences. The similarity is computed as the cosine similarity between their word vectors from the pre-trained GloVe word embeddings (Pennington et al., 2014). We calculate the mean and maximum similarity values. 3. Spell Check Errors - We use PyEnchant9 to embed a spell checker and count the number of errors in each document. The count is then used as another feature for training classifier. 4. Out of Vocabulary Words - We use the pretrained Google news word embeddings and find out word vectors for every token in the document. The tokens which do not return any vector are either rare words or in all probability out of vocabulary words. We use the count of such tokens as another feature set. 5 Results We perform the experiments detailed in section"
W19-7511,Q17-1010,0,0.0950375,"Missing"
W19-7511,D18-2029,0,0.0476738,"Missing"
W19-7511,P06-1035,0,0.0485466,"Missing"
W19-7511,W97-1102,0,0.0664099,"Missing"
W19-7511,N18-1202,0,0.0519215,"Missing"
W19-7511,R09-1064,0,0.0640674,"Missing"
W19-7511,N18-2063,0,0.0206677,"Missing"
W19-7511,C16-1097,0,0.0337873,"Missing"
W19-7511,W07-1306,0,0.0722626,"Missing"
W19-7511,W06-1109,0,0.131246,"Missing"
W19-7512,D18-2029,0,0.0214687,"Missing"
W19-7512,W14-0147,1,0.862478,"Missing"
W19-7512,P10-1023,0,0.0177927,"Missing"
W19-7512,W97-1102,0,0.156064,"Missing"
W19-7512,W13-5616,0,0.0179109,"Missing"
