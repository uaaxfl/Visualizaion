C10-1065,W08-0312,0,0.0183476,"Missing"
C10-1065,W97-0703,0,0.0194392,"is found to achieve the highest correlation with human annotations. We also provide evidence that the degree of semantic similarity varies with the location of the partially-matching component words. 1 Introduction Keyphrases are noun phrases (NPs) that are representative of the main content of documents. Since they represent the key topics in documents, extracting good keyphrases benefits various natural language processing (NLP) applications such as summarization, information retrieval (IR) and question-answering (QA). Keyphrases can also be used in text summarization as semantic metadata (Barzilay and Elhadad, 1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases supplement full-text indexing and assist users in creating good queries. In the past, a large body of work on keyphrases has been carried out as an extraction task, utilizing three types of cohesion: (1) document cohesion, i.e. cohesion between documents and keyphrases (Frank et al., 1999; Witten et al., 1999; Despite recent successes in keyphrase extraction (Frank et al., 1999; Turney, 2003; Park et al., 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007), current work is hampered by the inflexibility of stan"
C10-1065,S10-1004,1,0.888563,"Missing"
C10-1065,N03-1020,0,0.0479685,"etween the reference and candidate translations. The difference is that it allows for more match flexibility, including stem variation and WordNet synonymy. The basic metric is based on the number of mapped unigrams found between the two strings, the total number of unigrams in the translation, and the total number of unigrams in the reference. NIST (Martin and Przybocki, 1999) is once again similar to BLEU, but integrates a proportional difference in the co-occurrences for all ngrams while weighting more heavily n-grams that occur less frequently, according to their information value. ROUGE (Lin and Hovy, 2003) — and its variants including ROUGE-N and ROUGE-L — is similarly based on n-gram overlap between the candidate and reference summaries. For example, ROUGE-N is based on co-occurrence statistics, using higher-order n-grams (n > 1) to estimate the fluency of summaries. ROUGE-L uses longest common subsequence (LCS)-based statistics, based on the assumption that the longer the substring overlap between the two strings, the greater the similar Saggion et al. (2002). ROUGEW is a weighted LCS-based statistic that prioritizes consecutive LCSes. In this research, we experiment exclusively with the basi"
C10-1065,W09-0404,0,0.0450082,"Missing"
C10-1065,2001.mtsummit-papers.68,0,0.0658224,"Missing"
C10-1065,C08-2021,0,0.031443,"Missing"
C10-1065,C02-1073,0,0.0347311,"-occurrences for all ngrams while weighting more heavily n-grams that occur less frequently, according to their information value. ROUGE (Lin and Hovy, 2003) — and its variants including ROUGE-N and ROUGE-L — is similarly based on n-gram overlap between the candidate and reference summaries. For example, ROUGE-N is based on co-occurrence statistics, using higher-order n-grams (n > 1) to estimate the fluency of summaries. ROUGE-L uses longest common subsequence (LCS)-based statistics, based on the assumption that the longer the substring overlap between the two strings, the greater the similar Saggion et al. (2002). ROUGEW is a weighted LCS-based statistic that prioritizes consecutive LCSes. In this research, we experiment exclusively with the basic ROUGE metric, and unigrams (i.e. ROUGE-1). 3.2 R-precision In order to analyze near-misses in keyphrase extraction evaluation, Zesch and Gurevych (2009) proposed R-precision, an n-gram-based evaluation metric for keyphrase evaluation.3 R-precision contrasts with the majority of previous work on keyphrase extraction evaluation, which has used semantic similarity based on external resources 3 Zesch and Gurevych’s R-precision has nothing to do with the informat"
C10-1065,C08-1122,0,0.0279896,"Missing"
C10-1065,R09-1086,0,0.579699,"rithm. Also, computing algorithm is closer than effective grid to the same goldstandard keyphrase. From these observations, we infer that n-gram-based evaluation metrics can be applied to evaluating keyphrase extraction, but also that candidates with the same relative n-gram overlap are not necessarily equally good. Our primary goal is to test the utility of n-gram based evaluation metrics to the task of keyphrase extraction evaluation. We test the following evaluation metrics: (1) evaluation metrics from MT and multi-document summarization (BLEU, NIST, METEOR and ROUGE); and (2) R-precision (Zesch and Gurevych, 2009), an n-gram-based evaluation metric developed specifically for keyphrase extraction evaluation which has yet to be evaluated against humans at the extraction task. Secondarily, we attempt to shed light on the bigger question of whether it is feasible to expect that n-gram-based metrics without access to external resources should be able to capture subtle semantic differences in keyphrase candidates. To this end, we experimentally verify the impact of lexical overlap of different types on keyphrase similarity, and use this as the basis for proposing a variant of R-precision. In the next section"
C10-1065,W04-3252,0,\N,Missing
C10-1065,C02-1142,0,\N,Missing
C10-1065,P02-1040,0,\N,Missing
C12-1167,P08-1081,0,0.0243099,"arsing as a joint link and dialogue act classification task, by using CRFSGD (Bottou, 2011) and MaltParser (Nivre et al., 2007). They also demonstrated that the methods they use for thread discourse structure parsing are able to perform equally well over partial threads as complete threads, by experimenting with “in situ” classification of evolving threads. There is also research focusing on particular types of dialogue acts, such as question–answer pairs in emails (Shrestha and McKeown, 2004) and forum threads (Cong et al., 2008), question– context–answer in forum threads (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al.,"
C12-1167,D10-1084,1,0.902682,"inux Information Access by Data Mining) dataset of Baldwin et al. (2007). In this thread, Post1 and Post3 are both from the thread’s initiator UserA. Post1 asks a question, and Post3 asks for more information about an answer provided by UserB in Post2. In response to Post3, UserB adds more information to his/her original answer, and Post5 provides another independent answer. In threads like this, it is important to identify whether the problem is solved or not, and also where solution(s) are likely to be found. This research proposes to use information derived from thread discourse structure (Kim et al., 2010b; Wang et al., 2011) to help predict Solvedness of threads, without validating the answers provided in the threads. The discourse structure of the thread is modelled as a rooted Directed Acyclic Graph (DAG), and each post in the thread is represented as a node in this DAG. The reply-to relations between posts are then denoted as direct edges (Links) between nodes in the DAG, and the type of a reply-to relation is defined as Dialogue Act (DA). The Link between two connected posts (i.e. having a reply-to relation) is represented as the distance between the two posts in their chronological order"
C12-1167,W10-2923,1,0.877252,"inux Information Access by Data Mining) dataset of Baldwin et al. (2007). In this thread, Post1 and Post3 are both from the thread’s initiator UserA. Post1 asks a question, and Post3 asks for more information about an answer provided by UserB in Post2. In response to Post3, UserB adds more information to his/her original answer, and Post5 provides another independent answer. In threads like this, it is important to identify whether the problem is solved or not, and also where solution(s) are likely to be found. This research proposes to use information derived from thread discourse structure (Kim et al., 2010b; Wang et al., 2011) to help predict Solvedness of threads, without validating the answers provided in the threads. The discourse structure of the thread is modelled as a rooted Directed Acyclic Graph (DAG), and each post in the thread is represented as a node in this DAG. The reply-to relations between posts are then denoted as direct edges (Links) between nodes in the DAG, and the type of a reply-to relation is defined as Dialogue Act (DA). The Link between two connected posts (i.e. having a reply-to relation) is represented as the distance between the two posts in their chronological order"
C12-1167,U08-1009,0,0.0882953,"Missing"
C12-1167,C04-1128,0,0.0258603,"as Kim et al. (2010b), but different parsing approaches. Specifically, Wang et al. (2011) approached thread discourse structure parsing as a joint link and dialogue act classification task, by using CRFSGD (Bottou, 2011) and MaltParser (Nivre et al., 2007). They also demonstrated that the methods they use for thread discourse structure parsing are able to perform equally well over partial threads as complete threads, by experimenting with “in situ” classification of evolving threads. There is also research focusing on particular types of dialogue acts, such as question–answer pairs in emails (Shrestha and McKeown, 2004) and forum threads (Cong et al., 2008), question– context–answer in forum threads (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level"
C12-1167,C10-2133,0,0.0271101,"g et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). Moreover Wang and Rosé (2010) demonstrated that initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) from online forums have the potential to enhance thread summarisation and automatically generate knowledge bases for Community Question Answering (cQA) services such as Yahoo! Answers. Furthermore, Kim et al. (2006) showed that dialogue acts can be used to classify student online discussions in web-enhanced courses. Specifically, they use dialogue acts to identify discussion threads that may have"
C12-1167,D11-1002,1,0.802154,"cess by Data Mining) dataset of Baldwin et al. (2007). In this thread, Post1 and Post3 are both from the thread’s initiator UserA. Post1 asks a question, and Post3 asks for more information about an answer provided by UserB in Post2. In response to Post3, UserB adds more information to his/her original answer, and Post5 provides another independent answer. In threads like this, it is important to identify whether the problem is solved or not, and also where solution(s) are likely to be found. This research proposes to use information derived from thread discourse structure (Kim et al., 2010b; Wang et al., 2011) to help predict Solvedness of threads, without validating the answers provided in the threads. The discourse structure of the thread is modelled as a rooted Directed Acyclic Graph (DAG), and each post in the thread is represented as a node in this DAG. The reply-to relations between posts are then denoted as direct edges (Links) between nodes in the DAG, and the type of a reply-to relation is defined as Dialogue Act (DA). The Link between two connected posts (i.e. having a reply-to relation) is represented as the distance between the two posts in their chronological ordering. In the annotated"
C12-1167,N10-1097,0,0.030598,"trated that the methods they use for thread discourse structure parsing are able to perform equally well over partial threads as complete threads, by experimenting with “in situ” classification of evolving threads. There is also research focusing on particular types of dialogue acts, such as question–answer pairs in emails (Shrestha and McKeown, 2004) and forum threads (Cong et al., 2008), question– context–answer in forum threads (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009), initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) in forum threads (Wang and Rosé, 2010), as well as request and commitment in emails (Lampert et al., 2007, 2008a,b, 2010). Thread discourse structure can be used to facilitate different tasks in web user forums. For example, threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). Moreover Wang and Rosé (2010) demonstrated that initiation–response pairs (e.g."
C12-1167,C00-2137,0,0.0123633,"xperiments over the full 250-thread ILIAD dataset. In both cases, various combinations of the features introduced in Section 4 are used. To generate these features, both the gold-standard LinkDAs and the automatically predicted ones are used. All our Solvedness classification experiments were carried out based on stratified 10-fold crossvalidation. The results are evaluated using classification accuracy (AC C). As our baselines, we use a majority classifier (ZeroR), as well as the best Solvedness classifier provided by Baldwin et al. (2007) (ADCS). As mentioned earlier, randomised estimation (Yeh, 2000) (at a significance level of p < 0.05) is used throughout the paper for statistical significance testing. 2746 Feature Category Baseline DA-only LinkDA-based System/feature(s) ZeroR ADCS LastPostDA LastNonInitDA HasResolution LastPostDA +LastNonInitDA LastPostDA +HasResolution LastNonInitDA +HasResolution AllDAFeat LastPairDA LastSubthreadDA AllLinkDAFeat AllDAFeat +AllLinkDAFeat AC C g old AC Caut o .779 .788 .833∗ .775 .766 .792 .779 .779 .834∗ .779 .883∗ .775 .874∗ .792 .883∗ .779 .851∗ .792 .833∗ .779 .833∗ .792 .865∗ .792 Table 3: Results over ILIAD222 , using discourse structure features"
C12-1167,N10-1142,0,\N,Missing
D10-1084,P02-1048,0,0.0245783,"icipant’s message(s) in a turn. 863 al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spoken and written dialogue—and tested them using the Map Task Corpus (Anderson et al., 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1–3 utterances in order to classify dialogue acts for the target utterance. There has been substantially less effort on classifying dialogue acts in written dialogue: Wu et al. (2002) and Forsyth (2007) have used keyword-based approaches for classifying online chats; Ivanovic (2008) tested the use of n-gram features for 1-on-1 live chats with MSN Online Shopping assistants. Various machine learning techniques have been investigated for the dialogue classification task. Samuel et al. (1998) used transformation-based learning to classify spoken dialogues, incorporating Monte Carlo sampling for training efficiency. Stolcke et al. (2000) used Hidden Markov Models (HMMs) to account for the structure of spoken dialogues, while Wu et al. (2002) also used transformation- and rule-"
D10-1084,W98-0319,0,0.661934,"Missing"
D10-1084,W10-2923,1,0.878643,". Previously, Ivanovic (2008) explored Boolean 1- 864 Our motivation for using structural information as a feature is that the location of an utterance can be a strong predictor of the dialogue act. That is, dialogues are sequenced, comprising turns (i.e. a given user is sending text), each of which is made up of one or more messages (i.e. strings sent by the user). Structured classification methods which make use of this sequential information have been applied to related tasks such as tagging semantic labels of key sentences in biomedical domains (Chung, 2009) and post labels in web forums (Kim et al., 2010). Based on the nature of live chats, we observed that the utterance position in the chat, as well as in a turn, plays an important role when identifying its dialogue act. For example, an utterance such as Hello will occur at the beginning of a chat while an utterance such as Have a nice day will typically appear at the end. The position of utterances in a turn can also help identify the dialogue act; i.e. when there are several utterances in a turn, utterances are related to each other, and thus examining the previous utterances in the same turn can help correctly predict the target utterance."
D10-1084,N04-3002,0,0.0102816,"lassifying Dialogue Acts in One-on-one Live Chats Su Nam Kim,♠ Lawrence Cavedon♥ and Timothy Baldwin♠ ♠ Dept of Computer Science and Software Engineering, University of Melbourne ♥ School of Computer Science and IT, RMIT University sunamkim@gmail.com, lcavedon@gmail.com, tb@ldwin.net Abstract linguistic research on the topic. There has been substantially more work done on dialogue and dialogue corpora, mostly in spoken dialogue (e.g. Stolcke et al. (2000)) but also multimodal dialogue systems in application areas such as telephone support service (Bangalore et al., 2006) and tutoring systems (Litman and Silliman, 2004). Spoken dialogue analysis introduces many complications related to the error inherent in current speech recognition technologies. As an instance of written dialogue, an advantage of live chats is that recognition errors are not such an issue, although the nature of language used in chat is typically ill-formed and turn-taking is complicated by the semi-asynchronous nature of the interaction (e.g. Werry (1996)). We explore the task of automatically classifying dialogue acts in 1-on-1 online chat forums, an increasingly popular means of providing customer service. In particular, we investigate"
D10-1084,P98-2188,0,0.105999,"n derived from utterances (Sections 4.2 and 4.3). 2 Related Work While there has been significant work on classifying dialogue acts, the bulk of this has been for spoken dialogue. Most such work has considered: (1) defining taxonomies of dialogue acts; (2) discovering useful features for the classification task; and (3) experimenting with different machine learning techniques. We focus here on (2) and (3); we return to (1) in Section 3. For classifying dialogue acts in spoken dialogue, various features such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. Jurafsky et al. (1998) used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et 1 An utterance is the smallest unit to deliver a participant’s message(s) in a turn. 863 al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features—which could be adapted to both spo"
D10-1084,J00-3003,0,\N,Missing
D10-1084,P06-1026,0,\N,Missing
D10-1084,C98-2183,0,\N,Missing
D11-1002,W04-3240,0,0.462342,"Missing"
D11-1002,D09-1047,0,0.0158785,"on dialogue act boundaries were fed into an n-gram language model, which was used for the joint segmentation and classification of dialogue acts. Sutton and McCallum (2005) performed joint parsing and semantic role labelling (SRL), using the results of a probabilistic SRL system to improve the accuracy of a probabilistic parser. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-of-the-art results over both tasks. There has been a recent growth in user-level research over forums. Lui and Baldwin (2009) explored a range of user-level features, including replies-to and co-participation graph analysis, for post quality classification. Lui and Baldwin (2010) introduced a novel user classification task where each user is classified against four attributes: clarity, proficiency, positivity and effort. User communication roles i"
D11-1002,P08-1081,0,0.0130061,"ove the classification accuracy for postlevel tasks. Initiation–response pairs (e.g. question–answer, assessment–agreement, and blame–denial) from online forums have the potential to enhance thread summarisation or automatically generate knowledge bases for Community Question Answering (cQA) services such as Yahoo! Answers. While initiation– response pair identification has been explored as a pairwise ranking problem (Wang and Ros´e, 2010), question–answer pair identification has been approached via the two separate sub-tasks of question classification and answer detection (Cong et al., 2008; Ding et al., 2008; Cao et al., 2009). Our thread discourse structure prediction task includes joint classification of post roles (i.e. dialogue acts) and links, and could potentially be performed at the sub-post sentence level to extract initiation–response pairs. 3 Task Description and Data Set The main task performed in this research is joint classification of inter-post links (Link) and dialogue acts (DA) within forum threads. In this, we assume that a post can only link to an earlier post (or a virtual root node), and that dialogue acts are labels on edges. It is possible for there to be multiple edges fro"
D11-1002,W05-1504,0,0.0477988,"Missing"
D11-1002,P08-1095,0,0.0229703,"010a), demonstrating the generalisability of the original method. In both cases, however, we tackled only a single task, either link classification (optionally given dialogue act tags) or dialogue act classification, but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of c"
D11-1002,N09-1037,0,0.0208178,"different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their approach, the predictions of a multi-layer perceptron classifier on dialogue act boundaries were fed into an n-gram language model, which was used for the joint segmentation and classification of dialogue acts. Sutton and McCallum (2005) performed joint parsing and semantic role labelling (SRL), using the results of a probabilistic SRL system to improve the accuracy of a probabilistic parser. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-of-the-art results over both tasks. There has been a recent growth in user-level research over forums. Lui and Baldwin (2009) explored a range of user-level features, including replies-"
D11-1002,J86-3001,0,0.379024,", but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivan"
D11-1002,D10-1084,1,0.517285,"assessment (Lui and Baldwin, 2009). We aim to move beyond simple threading, to predict not only the links between posts, but also show the manner of each link, in the form of the discourse structure of the thread. In doing so, we hope to be able to perform richer visualisation of thread structure (e.g. highlighting the key posts which appear to have led to a successful resolution to a problem), and more finegrained weighting of posts in threads for search purposes. To illustrate the task, we use an example thread, made up of 5 posts from 4 distinct participants, from the CNET forum dataset of Kim et al. (2010b), as shown in Figure 1. The discourse structure of the thread is modelled as a rooted directed acyclic graph 13 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 13–25, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics full threads, indicating that the method is applicable to in-situ thread classification. Finally, we investigate the role of user-level features in discourse structure analysis. Ø 0+Question-Question User A Post 1 HTML Input Code ...Please can someone tell me how to create an input box that a"
D11-1002,W10-2923,1,0.839716,"Missing"
D11-1002,W10-1915,0,0.0189845,"Missing"
D11-1002,W02-0216,0,0.0351975,"neralisability of the original method. In both cases, however, we tackled only a single task, either link classification (optionally given dialogue act tags) or dialogue act classification, but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication includin"
D11-1002,U10-1009,1,0.684369,"tion (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-of-the-art results over both tasks. There has been a recent growth in user-level research over forums. Lui and Baldwin (2009) explored a range of user-level features, including replies-to and co-participation graph analysis, for post quality classification. Lui and Baldwin (2010) introduced a novel user classification task where each user is classified against four attributes: clarity, proficiency, positivity and effort. User communication roles in web forums have also been studied (Chan and Hayes, 2010; Chan et al., 2010). Threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). These results su"
D11-1002,D07-1013,1,0.806323,"the joint classification predictions, and performed a similar breakdown of posts 20 for Link and DA; the results are presented in Figure 3. It is clear that the anomaly for CRFSGD comes from the DA component, due to there being greater predictability in the dialogue for final posts in a thread (users tend to confirm a successful resolution of the problem, or report on successful external reproduction of the solution). MaltParser seems less adept at identifying that a post is at the end of a thread, and predicting the dialogue act accordingly. This observation is congruous with the findings of McDonald and Nivre (2007) that errors propagate, due to MaltParser’s greedy inference strategy. The higher results for Link are to be expected, as throughout the thread, most posts tend to link locally. XXX B/down XXX XXX Test [1, 2] [1, 4] [1, 6] [1, 8] [All] [1, 2] [1, 4] [1, 6] [1, 8] [All] .947/.947 .946/.947 .946/.947 .946/.947 .946/.946 — .836/.841 .840/.841 .840/.841 .840/.838 — — .800/.794 .800/.794 .800/.791 — — — .780/.769 .776/.767 — — — .756/.738 Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of different lengths (indicated in the rows), broken d"
D11-1002,E06-1011,0,0.053421,"Missing"
D11-1002,H05-1066,0,0.17872,"Missing"
D11-1002,N06-1047,0,0.022453,"-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different su"
D11-1002,W03-3017,1,0.730692,". One feature of MaltParser that makes it well suited to our task is that it is possible to define feature models of arbitrary complexity for each token. In presenting the thread data to MaltParser, we represent the nulllink from the initial post of each thread, as well as any disconnected posts, as the root. To the best of our knowledge, there is no past work on using dependency parsing to learn thread discourse structure. Based on extensive experimentation, we determined that the MaltParser configuration that obtains the best results for our task is the Nivre algorithm in arc-standard mode (Nivre, 2003; Nivre, 2004), using LIBSVM (Chang and Lin, 2011) with a linear kernel as the learner, and a feature model with exhaustive combinations of features relating to the features and predictions of the first/top 17 three tokens from both “Input” and “Stack”.3 As such, MaltParser is actually unable to predict any non-projective structures, as experiments with algorithms supporting non-projective structures invariably led to lower results. In our choice of parsing algorithm, we are also unable to detect posts with multiple heads, but can potentially detect disconnected sub-graphs. 4.2 Features The fe"
D11-1002,W04-0308,1,0.0531983,"of MaltParser that makes it well suited to our task is that it is possible to define feature models of arbitrary complexity for each token. In presenting the thread data to MaltParser, we represent the nulllink from the initial post of each thread, as well as any disconnected posts, as the root. To the best of our knowledge, there is no past work on using dependency parsing to learn thread discourse structure. Based on extensive experimentation, we determined that the MaltParser configuration that obtains the best results for our task is the Nivre algorithm in arc-standard mode (Nivre, 2003; Nivre, 2004), using LIBSVM (Chang and Lin, 2011) with a linear kernel as the learner, and a feature model with exhaustive combinations of features relating to the features and predictions of the first/top 17 three tokens from both “Input” and “Stack”.3 As such, MaltParser is actually unable to predict any non-projective structures, as experiments with algorithms supporting non-projective structures invariably led to lower results. In our choice of parsing algorithm, we are also unable to detect posts with multiple heads, but can potentially detect disconnected sub-graphs. 4.2 Features The features used in"
D11-1002,P95-1005,0,0.304332,"Missing"
D11-1002,C08-1095,0,0.0412607,"Missing"
D11-1002,W09-3813,0,0.0170508,"et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their approach, the p"
D11-1002,W04-2319,0,0.0142908,"o a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interaction"
D11-1002,C10-2133,0,0.0513622,"ies-to and co-participation graph analysis, for post quality classification. Lui and Baldwin (2010) introduced a novel user classification task where each user is classified against four attributes: clarity, proficiency, positivity and effort. User communication roles in web forums have also been studied (Chan and Hayes, 2010; Chan et al., 2010). Threading information has been shown to enhance retrieval effectiveness for post-level retrieval (Xi et al., 2004; Seo et al., 2009), thread-level retrieval (Seo et al., 2009; Elsas and Carbonell, 2009), sentence-level shallow information extraction (Sondhi et al., 2010), and near-duplicate thread detection (Muthmann et al., 2009). These results suggest that the thread structural representation used in this research, which includes both linking struc15 ture and the dialogue act associated with each link, could potentially provide even greater leverage in these retrieval tasks. Another related research area is post-level classification, such as general post quality classification (Weimer et al., 2007; Weimer and Gurevych, 2007; Wanas et al., 2008; Lui and Baldwin, 2009), and post descriptiveness in particular domains (e.g. medical forums: Leaman et al. (2010))"
D11-1002,N03-1030,0,0.021924,"c graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 2000; Shriberg et al., 2004; Murray et al., 2006), email (Cohen et al., 2004; Carvalho and Cohen, 2005; Lampert et al., 2008), instant messaging (Ivanovic, 2008; Kim et al., 2010a), edited documents (Soricut and Marcu, 2003; Sagae, 2009) and online forums (Xi et al., 2004; Weinberger and Fischer, 2006; Wang et al., 2007; Fortuna et al., 2007; Kim et al., 2010b). For a more complete review of models for discourse disentanglement and dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their a"
D11-1002,J00-3003,0,0.332415,"Missing"
D11-1002,W05-0636,0,0.0238895,"nd dialogue act tagging, see Kim et al. (2010b). Joint classification has been applied in a number of different contexts, based on the intuition that it should be possible to harness interactions between different sub-tasks to the mutual benefit of both. Warnke et al. (1997) jointly performed segmentation and dialogue act classification over a German spontaneous speech corpus. In their approach, the predictions of a multi-layer perceptron classifier on dialogue act boundaries were fed into an n-gram language model, which was used for the joint segmentation and classification of dialogue acts. Sutton and McCallum (2005) performed joint parsing and semantic role labelling (SRL), using the results of a probabilistic SRL system to improve the accuracy of a probabilistic parser. Finkel and Manning (2009) built a joint, discriminative model for parsing and named entity recognition (NER), addressing the problem of inconsistent annotations across the two tasks, and demonstrating that NER benefited considerably from the interaction with parsing. Dahlmeier et al. (2009) proposed a joint probabilistic model for word sense disambiguation (WSD) of prepositions and SRL of prepositional phrases (PPs), and achieved state-o"
D11-1002,A97-1011,0,0.0698392,"Missing"
D11-1002,N10-1097,0,0.161975,"Missing"
D11-1002,P07-2019,0,0.0814572,"ation predictions, and performed a similar breakdown of posts 20 for Link and DA; the results are presented in Figure 3. It is clear that the anomaly for CRFSGD comes from the DA component, due to there being greater predictability in the dialogue for final posts in a thread (users tend to confirm a successful resolution of the problem, or report on successful external reproduction of the solution). MaltParser seems less adept at identifying that a post is at the end of a thread, and predicting the dialogue act accordingly. This observation is congruous with the findings of McDonald and Nivre (2007) that errors propagate, due to MaltParser’s greedy inference strategy. The higher results for Link are to be expected, as throughout the thread, most posts tend to link locally. XXX B/down XXX XXX Test [1, 2] [1, 4] [1, 6] [1, 8] [All] [1, 2] [1, 4] [1, 6] [1, 8] [All] .947/.947 .946/.947 .946/.947 .946/.947 .946/.946 — .836/.841 .840/.841 .840/.841 .840/.838 — — .800/.794 .800/.794 .800/.791 — — — .780/.769 .776/.767 — — — .756/.738 Table 5: Post-level Link-DA F-score for CRFSGD/MaltParser, based on in situ classification over sub-threads of different lengths (indicated in the rows), broken d"
D11-1002,P07-2032,0,0.0159489,"Missing"
D11-1002,J05-2005,0,0.0318554,"n both cases, however, we tackled only a single task, either link classification (optionally given dialogue act tags) or dialogue act classification, but never the two together. In this paper, we take the obvious step of exploring joint classification of post link and dialogue act tags, to generate full thread discourse structures. Discourse disentanglement (i.e. link classification) and dialogue act tagging have been studied largely as independent tasks. Discourse disentanglement is the task of dividing a conversation thread (Elsner and Charniak, 2008; Lemon et al., 2002) or document thread (Wolf and Gibson, 2005) into a set of distinct sub-discourses. The disentangled discourse is sometimes assumed to take the form of a tree structure (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), an acyclic graph structure (Ros´e et al., 1995; Schuth et al., 2007; Elsner and Charniak, 2008; Wang et al., 2008; Lin et al., 2009), or a more general cyclic chain graph structure (Wolf and Gibson, 2005). Dialogue acts are used to describe the function or role of an utterance in a discourse, and have been applied to the analysis of mediums of communication including conversational speech (Stolcke et al., 20"
D11-1002,C00-2137,0,0.0329904,"Missing"
D11-1060,P98-1015,0,0.0789602,"deletion, the modifier is derived from the object of the underlying relative clause; however, the first three verbs also allow for it to be derived from the subject. Levi expresses the distinction using indexes. For example, music box is M AKE1 (object-derived), i.e., the box makes music, while chocolate bar is M AKE2 (subject-derived), i.e., the bar is made of chocolate (note the passive). Due to time constraints, we focused on one relation of Levi’s, M AKE2 , which is among the most frequent relations an NC can express and is present in some form in many relation inventories (Warren, 1978; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001; Nastase and Szpakowicz, 2003; Girju et al., 2005; Girju et al., 2007; Girju et al., 2009; Hendrickx et al., 2010; Tratz and Hovy, 2010). In Levi’s theory, M AKE2 means that the head of the noun compound is made up of or is a product of its modifier. There are three subtypes of this relation (we do not attempt to distinguish between them): 2. we select the top 20 most frequent patterns; (a) the modifier is a unit and the head is a configuration, e.g., root system; 3. we filter out all patterns that were extracted less than N times (we tried 5 and 10) and with less th"
D11-1060,P99-1008,0,0.0239839,"e (2008) adopted a similar fine-grained verbcentered approach to NC semantics. Using a distribution over verbs as a semantic interpretation was also carried out in a recent challenge: SemEval-2010 Task 9 (Butnariu et al., 2009; Butnariu et al., 2010). In noun compound interpretation, verbs and prepositions can be seen as patterns connecting the two nouns in a paraphrase. Similar pattern-based approaches have been popular in information extraction and ontology learning. For example, Hearst (1992) extracted hyponyms using patterns such as X, Y, and/or other Zs, where Z is a hypernym of X and Y. Berland and Charniak (1999) used similar patterns to extract meronymy (part-whole) relations, e.g., parts/NNS of/IN wholes/NNS matches basements of buildings. Unfortunately, matches are rare, which makes it difficult to build large semantic inventories. In order to overcome data sparseness, pattern-based approaches are often combined with bootstrapping. For example, Riloff and Jones (1999) used a multi-level bootstrapping algorithm to learn both a semantic lexicon and extraction patterns, e.g., owned by X extracts C OMPANY and facilities in X extracts L OCATION. That is, they learned semantic lexicons using extraction p"
D11-1060,C08-1011,0,0.467425,"ce National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interprete"
D11-1060,W09-2416,1,0.878239,"Missing"
D11-1060,fillmore-etal-2002-seeing,0,0.0251555,"odifier instead of gold. The problem also arose on Step 1, where we used WordNet to check whether the NC candidates were composed of two nouns. Since words like clear, friendly, and single are listed in WordNet as nouns (which is possible in some contexts), we extracted wrong NCs such as clear cube, friendly team, and single chain. There were similar issues with verbparticle constructions since some particles can be used as nouns as well, e.g., give back, break down. Some errors were due to semantic transparency issues, where the syntactic and the semantic head of a target NP were mismatched (Fillmore et al., 2002; Fontenelle, 1999). For example, from the sentence “This wine is made from a range of white grapes.”, we would extract range rather than grapes as the potential modifier of wine. In some cases, the NC-pattern pair was correct, but the NC did not express the target relation, e.g., while contain is a good paraphrase for toy box, the noun compound itself is not an instance of M AKE2 . 656 60 ’Acc.i1’ ’Acc.i2’ ’Acc.i3’ 50 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure 2: NC accuracy vs. collocation strength. 9 Conclusion and Future Work We have presented a framework for building a very large dat"
D11-1060,S07-1003,1,0.824769,"Missing"
D11-1060,P07-1072,0,0.0713507,"ion Using Bootstrapping and the Web as a Corpus Su Nam Kim Computer Science & Software Engineering University of Melbourne Melbourne, VIC 3010 Australia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize th"
D11-1060,C92-2082,0,0.049182,"ng conjunctions; they also used this distribution to predict coarse-grained abstract relations. Butnariu and Veale (2008) adopted a similar fine-grained verbcentered approach to NC semantics. Using a distribution over verbs as a semantic interpretation was also carried out in a recent challenge: SemEval-2010 Task 9 (Butnariu et al., 2009; Butnariu et al., 2010). In noun compound interpretation, verbs and prepositions can be seen as patterns connecting the two nouns in a paraphrase. Similar pattern-based approaches have been popular in information extraction and ontology learning. For example, Hearst (1992) extracted hyponyms using patterns such as X, Y, and/or other Zs, where Z is a hypernym of X and Y. Berland and Charniak (1999) used similar patterns to extract meronymy (part-whole) relations, e.g., parts/NNS of/IN wholes/NNS matches basements of buildings. Unfortunately, matches are rare, which makes it difficult to build large semantic inventories. In order to overcome data sparseness, pattern-based approaches are often combined with bootstrapping. For example, Riloff and Jones (1999) used a multi-level bootstrapping algorithm to learn both a semantic lexicon and extraction patterns, e.g.,"
D11-1060,S10-1006,1,0.865178,"Missing"
D11-1060,P06-2064,1,0.932221,"csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one"
D11-1060,P10-1150,0,0.103813,"uce a large number of semantically interpreted noun compounds from a small number of seeds. In each iteration, the method replaced one component of an NC with its synonyms, hypernyms and hyponyms to generate a new NC. These new NCs were further filtered based on their semantic similarity with the original NC. While the method acquired a large number of noun compounds without significant semantic drifting, its accuracy degraded rapidly after each iteration. More importantly, the variation of the sense pairs was limited since new NCs had to be semantically similar to the original NCs. Recently, Kozareva and Hovy (2010) combined patterns and bootstrapping to learn the selectional restrictions for various semantic relations. They used patterns involving the coordinating conjunction and, e.g., “* and John fly to *”, and learned arguments such as Mary/Tom and France/New York. Unlike in NC interpretation, it is not necessary for their arguments to form an NC, e.g., Mary France and France Mary are not NCs. Rather, they were interested in building a semantic ontology with a predefined set of semantic relations, similar to YAGO (Suchanek et al., 2007), where the pattern work for would have arguments like a company/"
D11-1060,J02-3004,0,0.0192778,"stralia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, w"
D11-1060,P09-1045,0,0.0892433,"s (verbs and prepositions) that interpret them for a given abstract relation. First, we extract NCs using a small number of seed patterns from a given abstract relation. Then, using the extracted NCs, we harvest more patterns. This is repeated until no new NCs and patterns can be extracted or for a pre-specified number of iterations. Our approach combines pattern-based extraction and bootstrapping, which is novel for NC interpretation; however, such combinations have been used in other areas, e.g., named entity recognition (Riloff and Jones, 1999; Thelen and Riloff, 2002; Curran et al., 2007; McIntosh and Curran, 2009). The remainder of the paper is organized as follows: Section 2 gives an overview of related work, Section 3 motivates our semantic representation, Sections 4, 5, and 6 explain our method, dataset and experiments, respectively, Section 7 discusses the results, Section 8 provides error analysis, and Section 9 concludes with suggestions for future work. 2 Related Work As we mentioned above, the implicit relation between the two nouns forming a noun compound can often be expressed overtly using verbal and prepositional paraphrases. For example, student loan is “loan given to a student”, while mor"
D11-1060,W04-2609,0,0.0904546,"Missing"
D11-1060,P08-1052,1,0.947622,"rtment of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number"
D11-1060,N04-3012,0,0.0116789,"stantial agreement (Landis and Koch, 1977). The accuracy for NC-only strict bootstrapping is a bit higher than for strict bootstrapping, but the actual differences are probably smaller since the evaluation of the former on iteration 2 was done for the most frequent NCs, which are more accurate. 7 As a comparison, we implemented the method of Kim and Baldwin (2007), which generates new semantically interpreted NCs by replacing either the head or the modifier of a seed NC with suitable synonyms, hypernyms and sister words from WordNet, followed by similarity filtering using WordNet::Similarity (Pedersen et al., 2004). Discussion Tables 2 and 3 show that fixing one of the two nouns in the pattern, as in strict bootstrapping and NC-only strict bootstrapping, yields significantly higher accuracy (χ2 test) for both NC and NC-pattern pair extraction compared to loose bootstrapping. 655 Note that the number of extracted NCs is much higher with the strict methods because of the higher number of possible instantiations of the generalized query patterns. For NC-only strict bootstrapping, the number of extracted NCs grows exponentially since the number of patterns does not diminish as in the other two methods. The"
D11-1060,W01-0511,0,0.0395006,"rived from the object of the underlying relative clause; however, the first three verbs also allow for it to be derived from the subject. Levi expresses the distinction using indexes. For example, music box is M AKE1 (object-derived), i.e., the box makes music, while chocolate bar is M AKE2 (subject-derived), i.e., the bar is made of chocolate (note the passive). Due to time constraints, we focused on one relation of Levi’s, M AKE2 , which is among the most frequent relations an NC can express and is present in some form in many relation inventories (Warren, 1978; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001; Nastase and Szpakowicz, 2003; Girju et al., 2005; Girju et al., 2007; Girju et al., 2009; Hendrickx et al., 2010; Tratz and Hovy, 2010). In Levi’s theory, M AKE2 means that the head of the noun compound is made up of or is a product of its modifier. There are three subtypes of this relation (we do not attempt to distinguish between them): 2. we select the top 20 most frequent patterns; (a) the modifier is a unit and the head is a configuration, e.g., root system; 3. we filter out all patterns that were extracted less than N times (we tried 5 and 10) and with less than M NCs per pattern (we t"
D11-1060,P02-1032,0,0.0370902,"Missing"
D11-1060,N09-2060,0,0.0352601,"Missing"
D11-1060,H05-1047,0,0.0151203,"(Downing, 1977), and they can capture fine-grained aspects of the meaning. For example, while both wrinkle treatment and migraine treatment express the same abstract relation T REATMENT-F OR -D ISEASE, fine-grained differences can be revealed using verbs, e.g., smooth can paraphrase the former, but not the latter. In many theories, verbs play an important role in NC derivation (Levi, 1978). Moreover, speakers often use verbs to make the hidden relation between the noun in a noun compound overt. This allows for simple extraction and for straightforward use in NLP tasks like textual entailment (Tatu and Moldovan, 2005) and machine translation (Nakov, 2008a). Finally, a single verb is often not enough, and the meaning is better approximated by a collection of verbs. For example, while malaria mosquito expresses C AUSE (and is paraphrasable using cause), further aspects of the meaning can be captured with more verbs, e.g., carry, spread, be responsible for, be infected with, transmit, pass on, etc. 4 Method We harvest noun compounds expressing some target abstract semantic relation (in the experiments below, this is Levi’s M AKE2 ), starting from a small number of initial seed patterns: paraphrasing verbs and"
D11-1060,W02-1028,0,0.0311195,"p algorithm to jointly harvest NCs and patterns (verbs and prepositions) that interpret them for a given abstract relation. First, we extract NCs using a small number of seed patterns from a given abstract relation. Then, using the extracted NCs, we harvest more patterns. This is repeated until no new NCs and patterns can be extracted or for a pre-specified number of iterations. Our approach combines pattern-based extraction and bootstrapping, which is novel for NC interpretation; however, such combinations have been used in other areas, e.g., named entity recognition (Riloff and Jones, 1999; Thelen and Riloff, 2002; Curran et al., 2007; McIntosh and Curran, 2009). The remainder of the paper is organized as follows: Section 2 gives an overview of related work, Section 3 motivates our semantic representation, Sections 4, 5, and 6 explain our method, dataset and experiments, respectively, Section 7 discusses the results, Section 8 provides error analysis, and Section 9 concludes with suggestions for future work. 2 Related Work As we mentioned above, the implicit relation between the two nouns forming a noun compound can often be expressed overtly using verbal and prepositional paraphrases. For example, stu"
D11-1060,P10-1070,0,0.255759,"e Web as a Corpus Su Nam Kim Computer Science & Software Engineering University of Melbourne Melbourne, VIC 3010 Australia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphras"
D11-1060,C94-2125,0,0.712245,"bourne, VIC 3010 Australia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In"
D11-1060,I05-1082,1,\N,Missing
D11-1060,S10-1007,1,\N,Missing
D11-1060,C98-1015,0,\N,Missing
I05-1082,C02-1011,0,0.0223591,"modifier and head noun for all NCs in the training data; we experiment with two methods for calculating the combined similarity. The third step is to choose the NC in the training data which is most similar to the test instance, and tag the test instance according to the semantic relation associated with that training instance. Formally, SA is the similarity between NCs (Ni,1 , Ni,2 ) and (Bj,1 , Bj,2 ): SA ((Ni,1 , Ni,2 ), (Bj,1 , Bj,2 )) = ((αS1 + S1) × ((1 − α)S2 + S2)) 2 (1) where S1 is the modifier similarity (i.e. S(Ni,1 , Bj1 )) and S2 is head noun similarity (i.e. S(Ni,2 , Bj2 )); α ∈ [0, 1] is a weighting factor. SB is an analogous similarity function, based on the F-score: N11 N12 N21 N22 NN RELATION B11 B12 B21 B22 B31 B32 Relation3 Relation19 Relation3 Bj1 Bj2 Relation_k Ni1 Ni2 Similarity in detail S(Ni1,B11) S(Ni2,B12) S(Ni1,B21) S(Ni2,B22) S(Ni1,Bj1) S(Ni2,Bj2) S(Ni1,Bm1) S(Ni2,Bm2) Nn1 Nn2 Bm1 Bm2 Relation2 Fig. 2. Similarity between the ith NC in the test data and j th NC in the training data Automatic Interpretation of Noun Compounds SB ((Ni,1 , Ni,2 ), B(j,1 , Bj,2 )) = 2 × (S1 + αS1) × (S2 + (1 − α)S2) (S1 + αS1) + (S2 + (1 − αS2)) 951 (2) The semantic relation is det"
I05-1082,W04-0404,1,0.643147,"Missing"
I05-1082,J02-3004,0,0.0686548,"also investigated the relative contribution of the modifier and the head noun in noun compounds of different semantic types. 1 Introduction ¯ made up of two or more nouns, such as golf club or A noun compound (NC) is an N paper submission; we will refer to the rightmost noun as the head noun and the remainder of nouns in the NC as modifiers. The interpretation of noun compounds is a well-researched area in natural language processing, and has been applied in applications such as question answering and machine translation [1,2,3]. Three basic properties make the interpretation of NCs difficult [4]: (1) the compounding process is extremely productive; (2) the semantic relationship between head noun and modifier in the noun compounds is implicit; and (3) the interpretation can be influenced by contextual and pragmatic factors. In this paper, we are interested in recognizing the semantic relationship between the head noun and modifier(s) of noun compounds. We introduce a method based on word similarity between the component nouns in an unseen test instance NC and annotated training instance NCs. Due to its simplicity, our method is able to interpret NCs with significantly reduced cost. We"
I05-1082,C94-2125,0,0.230991,", pp. 945–956, 2005. c Springer-Verlag Berlin Heidelberg 2005  946 S.N. Kim and T. Baldwin distinguish semantic relations from semantic roles. The semantic relation in an NC is the underlying relation between the head noun and its modifier, whereas its semantic role is an indication of its relation to the governing verb and other constituents in the sentence context. There is a significant body of closely-related research on interpreting semantic relations in NCs which relies on hand-written rules. [5] examined the problem of interpretation of NCs and constructed a set of hand-written rules. [6] automatically extracted semantic information from an on-line dictionary and manipulated a set of hand-written rules to assign weights to semantic relations. Recently, there has been work on the automatic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is based on a simplifying assumption as to the scope of semantic relations or the domain of interpretation, making it difficult to compare the performance of NC interpretation in a broader context. In the remainder of the paper, we detail the motivation for our work (Section 2), introduce the WordNet::Similarity sys"
I05-1082,W01-0511,0,0.891828,"its semantic role is an indication of its relation to the governing verb and other constituents in the sentence context. There is a significant body of closely-related research on interpreting semantic relations in NCs which relies on hand-written rules. [5] examined the problem of interpretation of NCs and constructed a set of hand-written rules. [6] automatically extracted semantic information from an on-line dictionary and manipulated a set of hand-written rules to assign weights to semantic relations. Recently, there has been work on the automatic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is based on a simplifying assumption as to the scope of semantic relations or the domain of interpretation, making it difficult to compare the performance of NC interpretation in a broader context. In the remainder of the paper, we detail the motivation for our work (Section 2), introduce the WordNet::Similarity system which we use to calculate word similarity (Section 3), outline the set of semantic relations used (Section 4), detail how we collected the data (Section 5), introduce the proposed method (Section 6), and describe experimental results (Section 7). 2 M"
I05-1082,W04-2609,0,0.740155,"its semantic role is an indication of its relation to the governing verb and other constituents in the sentence context. There is a significant body of closely-related research on interpreting semantic relations in NCs which relies on hand-written rules. [5] examined the problem of interpretation of NCs and constructed a set of hand-written rules. [6] automatically extracted semantic information from an on-line dictionary and manipulated a set of hand-written rules to assign weights to semantic relations. Recently, there has been work on the automatic (or semi-automatic) interpretation of NCs [4,7,8]. However, most of this work is based on a simplifying assumption as to the scope of semantic relations or the domain of interpretation, making it difficult to compare the performance of NC interpretation in a broader context. In the remainder of the paper, we detail the motivation for our work (Section 2), introduce the WordNet::Similarity system which we use to calculate word similarity (Section 3), outline the set of semantic relations used (Section 4), detail how we collected the data (Section 5), introduce the proposed method (Section 6), and describe experimental results (Section 7). 2 M"
I05-1082,P98-1015,0,0.783066,"xperimental results (Section 7). 2 Motivation Most work related to interpreting NCs depends on hand-coded rules [5]. The first attempt at automatic interpretation by [6] showed that it was possible to successfully interpret NCs. However, the system involved costly hand-written rules involving manual intervention. [9] estimated the amount of world knowledge required to interpret NCs and claimed that the high cost of data acquisition offsets the benefits of automatic interpretation of NCs. Recent work [4,7,8] has investigated methods for interpreting NCs automatically with minimal human effort. [10] introduced a semi-automatic method for recognizing noun–modifier relations. [4] examined nominalizations (a proper subset of NCs) in terms of whether the modifier is a subject or object of the verb the head noun is derived from (e.g. language understanding = understand language). [7] assigned hierarchical tags to nouns in medical texts and classified them according to their semantic relations using neural networks. [8] used the word senses of nouns to classify the semantic relations of NCs. However, in all this work, there has been some underlying simplifying assumption, in terms of the domai"
I05-1082,W04-0816,0,0.0240189,"medical texts and classified them according to their semantic relations using neural networks. [8] used the word senses of nouns to classify the semantic relations of NCs. However, in all this work, there has been some underlying simplifying assumption, in terms of the domain or range of interpretations an NC can occur with, leading to questions of scalability and portability to novel domains/NC types. In this paper, we introduce a method which uses word similarity based on WordNet. Word similarity has been used previously in various lexical semantic tasks, including word sense disambiguation [11,12]. [11] showed that term-to-term similarity in a context space can be used to disambiguate word senses. [12] measured the relatedness of concepts using similarity based on WordNet. [13] examined the task of disambiguating noun groupings with respect to word senses using similarity between nouns in NCs. Our research uses similarities between nouns in the training and test data to interpret the semantic relations of novel NCs. Automatic Interpretation of Noun Compounds MATERIAL apple juice s11 947 TIME morning milk s12 s21 s22 chocolate milk Fig. 1. Similarity between test NC chocolate milk and t"
I05-1082,W95-0105,0,0.0274609,"this work, there has been some underlying simplifying assumption, in terms of the domain or range of interpretations an NC can occur with, leading to questions of scalability and portability to novel domains/NC types. In this paper, we introduce a method which uses word similarity based on WordNet. Word similarity has been used previously in various lexical semantic tasks, including word sense disambiguation [11,12]. [11] showed that term-to-term similarity in a context space can be used to disambiguate word senses. [12] measured the relatedness of concepts using similarity based on WordNet. [13] examined the task of disambiguating noun groupings with respect to word senses using similarity between nouns in NCs. Our research uses similarities between nouns in the training and test data to interpret the semantic relations of novel NCs. Automatic Interpretation of Noun Compounds MATERIAL apple juice s11 947 TIME morning milk s12 s21 s22 chocolate milk Fig. 1. Similarity between test NC chocolate milk and training NCs apple juice and morning milk Table 1. WordNet-based similarities for component nouns in the training and test data Training noun Test noun Sij t1 apple chocolate 0.71 t2 ju"
I05-1082,P94-1019,0,0.399544,"chocolate 0.71 t2 juice milk 0.83 t1 morning chocolate 0.27 t2 milk milk 1.00 Figure 1 shows the correspondences between two training NCs, apple juice and morning milk, and a test NC, chocolate milk; Table 1 lists the noun pairings and noun– noun similarities based on WordNet. Each training noun is a component noun from the training data, each test noun is a component noun in the input, and Sij provides a measure of the noun–noun similarity in training and test, where t1 is the modifier and t2 is the head noun in the NC in question. The similarities in Table 1 were computed by the WUP method [14] as implemented in WordNet::Similarity (see Section 3). The simple product of the individual similarities (of each modifier and head noun, respectively) gives the similarity of the NC pairing. For example, the similarity between chocolate milk and apple juice is 0.60, while that between chocolate milk and morning milk is 0.27. Note that although milk in the input NC also occurs in a training exemplar, the semantic relations for the individual NCs differ. That is, while apple juice is juice made from apples (MATERIAL), morning milk is milk served in the morning (TIME). By comparing the similari"
I05-1082,O97-1002,0,\N,Missing
I05-1082,C98-1015,0,\N,Missing
I08-1074,P98-1015,0,0.0283656,"= BENEFICIARY), while student protest conventionally means a student undertaking a protest (SR = AGENT).2 NCs are formed from simplex nouns with high productivity. The huge number of possible NCs and potentially large number of SRs makes NC interpretation a very difficult problem. In the past, much NC interpretation work has been carried out which targets particular NLP applications such as information extraction, question-answering and machine translation. Unfortunately, much of it has not gained 1 The 4th International Workshop on Semantic Evaluation SRs used in the examples are taken from Barker and Szpakowicz (1998). 2 The first step in NC interpretation is to define a set of SRs. Levi (1979), for example, proposed a system of 9 SRs, while others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation"
I08-1074,P07-1072,0,0.0135112,"le others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (cov"
I08-1074,I05-1082,1,0.775674,"a system of 9 SRs, while others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in differen"
I08-1074,P06-2064,1,0.677213,"duced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (coverage of SRs over the NCs is another issue). Our aim in this paper is to compare and analyze existing NC interpretation methods over a common, publicly availab"
I08-1074,W04-2609,0,0.350338,"tation a very difficult problem. In the past, much NC interpretation work has been carried out which targets particular NLP applications such as information extraction, question-answering and machine translation. Unfortunately, much of it has not gained 1 The 4th International Workshop on Semantic Evaluation SRs used in the examples are taken from Barker and Szpakowicz (1998). 2 The first step in NC interpretation is to define a set of SRs. Levi (1979), for example, proposed a system of 9 SRs, while others have proposed classifications with 20-30 SRs (Finin, 1980; Barker and Szpakowicz, 1998; Moldovan et al., 2004). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Van"
I08-1074,W07-1108,0,0.0678177,"Missing"
I08-1074,W01-0511,0,0.0345516,"Missing"
I08-1074,C94-2125,0,0.042037,"04). Smaller sets tend to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (coverage of SRs over the NCs is another issue). Our aim in this paper is to compare and analyze existing NC interpretation me"
I08-1074,P94-1019,0,0.0238202,"Method The SENSE COLLOCATION method of Moldovan et al. (2004) is based on the pair of word senses of NC constituents. The basic idea is that NCs which have the same or similar sense collocation tend to have the same SR. As an example, car factory and auton(r, fij ) n(fij ) (3) where S1 is the modifier similarity (i.e. S(Ni,1 , Bj1 )) and S2 is the head noun similarity (i.e. S(Ni,2 , Bj2 )); α ∈ [0, 1] is a weighting factor. The similarity scores are calculated across the bag of WordNet senses (without choosing between 570 Method SC OLL SC OLLCT CS IM CS IM +SC OLLCT them) using the method of Wu and Palmer (1994) as implemented in WordNet::Similarity (Patwardhan et al., 2003). This is done for each pairing of WordNet senses of the two words in question, and the overall lexical similarity is calculated as the average across the pairwise sense similarities. HYBRID CS IMCT Description sense collocation sense collocation + SC OLL co-training constituent similarity constituent similarity + SC OLL co-training SC OLL + CS IM + SC OLLCT constituent similarity + CS IM co-training Table 1: Systems used in our experiments 2.4 Co-Training by Sense Collocation Co-training by sense collocation (SC OLL CO TRAINING )"
I08-1074,J02-3004,0,0.0303976,"end to have reduced coverage due to coarse granularity, whereas larger sets tend to be too fine grained and suffer from low inter-annotator agreement. Additionally pragmatic/contextual differentiation leads to difficulties in defining and interpreting SRs (Downing, 1977; SparckJones, 1983). Recent attempts in the area of NC interpretation have taken two basic approaches: analogy-base interpretation (Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007) and semantic disambiguation relative to an underlying predicate or semantically-unambiguous paraphrase (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Most methods employ rich ontologies and ignore the context of use, supporting the claim by Fan (2003) that axioms and ontological distinctions are more important than detailed knowledge of specific nouns for NC interpretation. Additionally, most approaches use supervised learning, raising questions about the generality of the test and 569 training data sets and the effectiveness of the algorithms in different domains (coverage of SRs over the NCs is another issue). Our aim in this paper is to compare and analyze existing NC interpretation methods over a c"
I08-1074,S07-1003,0,\N,Missing
I08-1074,C98-1015,0,\N,Missing
I08-2108,P01-1004,1,0.808695,"ally expensive. We thus adopt a cheaper scoring mechanism which normalises relative to the length of w and di,j , but ignores the length of substring matches. Namely, we use the Dice coefficient. 4.2 Tokenisation Tokenisation is particularly important in Japanese because it is a non-segmenting language with a logographic orthography (kanji). As such, we can chose to either word tokenise via a word splitter such as ChaSen, or character tokenise. Character and word tokenisation have been compared in the context of Japanese information retrieval (Fujii and Croft, 1993) and translation retrieval (Baldwin, 2001), and in both cases, characters have been found to be the superior representation overall. Orthogonal to the question of whether to tokenise into words or characters, we adopt an n-gram segment representation, in the form of simple unigrams and simple bigrams. In the case of word tokenisation and simple bigrams, e.g., example (1) would be represented as { おとなしい犬 , 犬を , を飼いたい }. 4.3 Extended Glosses The main direction in which Banerjee and Pedersen (2002) successfully extended the Lesk algorithm was in including hierarchically-adjacent glosses (i.e. hyponyms and hypernyms). We take this a step"
I08-2108,J98-1006,0,0.0481197,"is applicable to all-words with minimal effort. Banerjee and Pedersen (2002) extended the Lesk method for WordNetbased WSD tasks, to include hierarchical data from the WordNet ontology (Fellbaum, 1998). They observed that the hierarchical relations significantly enhance the basic model. Both these methods will be described extensively in Section 3.1, as our approach is based on them. Other notable unsupervised and semi-supervised approaches are those of McCarthy et al. (2004), who combine ontological relations and untagged corpora to automatically rank word senses in relation to a corpus, and Leacock et al. (1998) who use untagged data to build sense-tagged data automatically based on monosemous words. Parallel corpora have also been used to avoid the need for hand-tagged data, e.g. by Chan and Ng (2005). 3 Background As background to our work, we first describe the basic and extended Lesk algorithms that form the core of our approach. Then we present the Lexeed lexical resource we have used in our experiments, and finally we outline aspects of Japanese relevant for this work. 3.1 Basic and Extended Lesk The original Lesk algorithm (Lesk, 1986) performs WSD by calculating the relative word overlap betw"
I08-2108,P04-1036,0,0.226288,"l definitions. In our experiments, we will make clear when hand-tagged sense information is being used. Unsupervised methods rely on different knowledge sources to build their models. Primarily the following types of lexical resources have been used for WSD: MRDs, lexical ontologies, and untagged corpora (monolingual corpora, second language corpora, and parallel corpora). Although early approaches focused on exploiting a single resource (Lesk, 1986), recent trends show the benefits of combining different knowledge sources, such as hierarchical relations from an ontology and untagged corpora (McCarthy et al., 2004). In this summary, we will focus on a few representative systems that make use of different resources, noting that this is an area of very active research which we cannot do true justice to within the confines of this paper. The Lesk method (Lesk, 1986) is an MRD-based system that relies on counting the overlap between the words in the target context and the dictionary definitions of the senses. In spite of its simplicity, it has been shown to be a hard baseline for unsupervised methods in Senseval, and it is applicable to all-words with minimal effort. Banerjee and Pedersen (2002) extended th"
I08-2108,W03-2408,0,0.0276544,"o the knowledge sources they use to build their models. A top-level distinction is made between supervised and unsupervised systems. The former rely on training instances that have been hand-tagged, while the latter rely on other types of knowledge, such as lexical databases or untagged corpora. The Senseval evaluation tracks have shown that supervised systems perform better when sufficient training data is available, but they do not scale well to all words in context. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques (Mihalcea and Chklovski, 2003). In this paper, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator. The resource in question is the Lexeed Sensebank (Tanaka et al., 2006) and consists of the 28,000 most familiar words of Japanese, each of which has one or more basic senses. The senses take the form of a dictionary definition composed from the closed vocabulary of the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Lexeed also has a semi-automatically constructed ontology. Through the Lexee"
I08-2108,shirai-2002-construction,0,0.48783,"the POS tag of the target word should match the word class of the word sense, and this provides a coarse-grained filter for discriminating homographs with different word classes. We also experiment with a stop word-based filter which ignores a closed set of 18 lexicographic markers commonly found in definitions (e.g. 略 [ryaku] “an abbreviation for ...”), in line with those used by Nichols et al. (2005) in inducing the ontology. 5 Evaluation We evaluate our various extensions over two datasets: (1) the example sentences in the Lexeed sensebank, and (2) the Senseval-2 Japanese dictionary task (Shirai, 2002). All results below are reported in terms of simple precision, following the conventions of Senseval evaluations. For all experiments, precision and recall are identical as our systems have full coverage. For the two datasets, we use two baselines: a random baseline and the first-sense baseline. Note that the first-sense baseline has been shown to be hard to beat for unsupervised systems (McCarthy et al., 2004), and it is considered supervised when, as in this case, the first-sense is the most frequent sense from hand-tagged corpora. 5.1 Lexeed Example Sentences The goal of these experiments i"
I08-2108,W06-0608,1,0.912613,"hile the latter rely on other types of knowledge, such as lexical databases or untagged corpora. The Senseval evaluation tracks have shown that supervised systems perform better when sufficient training data is available, but they do not scale well to all words in context. This is known as the knowledge acquisition bottleneck, and is the main motivation behind research on unsupervised techniques (Mihalcea and Chklovski, 2003). In this paper, we aim to exploit an existing lexical resource to build an all-words Japanese word-sense disambiguator. The resource in question is the Lexeed Sensebank (Tanaka et al., 2006) and consists of the 28,000 most familiar words of Japanese, each of which has one or more basic senses. The senses take the form of a dictionary definition composed from the closed vocabulary of the 28,000 words contained in the dictionary, each of which is further manually sense annotated according to the Lexeed sense inventory. Lexeed also has a semi-automatically constructed ontology. Through the Lexeed sensebank, we investigate a number of areas of general interest to the WSD community. First, we test extensions of the Lesk algorithm (Lesk, 1986) over Japanese, focusing specifically on th"
I13-1026,W11-2002,0,0.348728,"U module and expert annotators are addressees. Gold standard interpretations for these descriptions are produced by annotators on the basis of their understanding of what was said, e.g., an ambiguous utterance has more than one correct interpretation. The SLU system’s performance is evaluated on the basis of the rank of the correct interpretations. 1 Examples from our trials are marked with asterisks (∗). 225 International Joint Conference on Natural Language Processing, pages 225–233, Nagoya, Japan, 14-18 October 2013. utterances in terms of ASR Word Error Rate (WER), e.g., (Hirschman, 1998; Black et al., 2011). Möller (2008) provides a comprehensive collection of interaction parameters for evaluating telephone-based spoken dialogue services, which pertain to different aspects of an interaction, viz communication, cooperativity, task success, and spoken input. Our characterization of spoken utterances along the accuracy and knowledge dimensions is related to Möller’s task success category. However, in our case, these features pertain to the context, rather than the task. In addition, our characterization is linked to system development effort, i.e., how much effort should be invested to address utte"
I13-1026,W01-1614,0,0.0559482,"focuses on the interpretation of descriptions of household objects (Zukerman et al., 2008). Our contributions pertain to (1) the characterization of spoken utterances, (2) experimental design, and (3) quantitative evaluation metrics for an N-best list. Characterization of spoken utterances. According to (Jokinen and McTear, 2010), “in diagnostic-type evaluations, a representative test suite is used so as to produce a system’s performance profile with respect to a taxonomy of possible inputs”. In addition, one of the typical aims of an evaluation is to identify components that can be improved (Paek, 2001). These two factors in combination motivate a characterization of input utterances along two dimensions: accuracy and knowledge (Section 4). • Accuracy indicates whether an utterance describes an intended object precisely and unambiguously. For instance, when intending a blue plate, “the blue plate” is an accurate description if there is only one such plate in the room, while “the green plate” is inaccurate. • Knowledge indicates how much the SLU module knows about different factors of the interpretation process, e.g., vocabulary or geometric • In the Interpretive experiment, trial subjects an"
I13-1026,W09-3902,0,0.194332,"be invested to address utterances with certain characteristics; and to evaluation metrics, in the sense that the assessment of an interpretation depends on the accuracy of an utterance, and takes into account the capabilities of an SLU system. These two experiments, in combination with our characterization of spoken utterances, enable the comparison of system and human interpretations under different conditions. Quantitative evaluation metrics. Automatic Speech Recognizers (ASRs) and parsers often return N-best hypotheses to SLU modules, while many SLU systems return only one interpretation (DeVault et al., 2009; Jokinen and McTear, 2010; Black et al., 2011). However, maintaining N-best interpretations at the semantic and pragmatic level enables a Dialogue Manager (DM) to examine more than one interpretation, and discover features that guide appropriate responses and support error recovery. This ranking requirement, together with our experimental design, motivates the following metrics (Section 6). • For Interpretive experiments, we propose correlation measures, such as Spearman rank or Pearson correlation coefficient, to compare participants’ ratings of candidate interpretations with the scores give"
I13-1026,I13-1027,1,0.710784,"Missing"
I13-1026,W01-0902,0,\N,Missing
I13-1026,J09-2005,0,\N,Missing
I13-1027,P98-1013,0,0.00915833,"ese actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser failed to produce a parse tree. In contrast, our shallow semantic parser is part of a noisy channel model that post-processes the output of an ASR. 3.1 Word error classifier We investigated three classifiers to determine whether a word in the ASR textual output is correct: the Weka implementation of Decision Trees (Quinlan, 1993) and Naïve Bayes classifiers (Domingos a"
I13-1027,P00-1037,0,0.442042,"c words). Our mechanism was evaluated on a corpus of 295 spoken referring expressions, improving interpretation performance. 1 the stool to the left of the storm the left of Object Prep Specifier the plate in to play it in Object Noise Prep the table the table Landmark the microwave the microwave Landmark The idea of the noisy channel model is that a message is sent through a channel that introduces errors, and the receiver endeavours to reconstruct the original message by taking into account the characteristics of the noisy channel and of the transmitted information (Ringger and Allen, 1996; Brill and Moore, 2000; Zwarts et al., 2010). The system described in this paper handles three types of errors: noise (which is removed), missing prepositions (which are inserted), and mis-heard words (which are replaced). Table 1 shows two descriptions that illustrate these errors. The first row for each description displays what was spoken, the second row displays what was heard by the ASR, and the third row shows the semantic labels assigned to each segment in the description by a shallow semantic parser (Section 3.2). Specifically, in the first example, the preposition “to” is missing, and the object “stool” is"
I13-1027,N09-2022,0,0.0137607,"nd grammatical numbers to better match grammatical expectations. However, these actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser failed to produce a parse tree. In contrast, our shallow semantic parser is part of a noisy channel model that post-processes the output of an ASR. 3.1 Word error classifier We investigated three classifiers to determine whether a word in the ASR textual output is correct: the Weka implementatio"
I13-1027,W09-3729,0,0.0262894,"to better match grammatical expectations. However, these actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser failed to produce a parse tree. In contrast, our shallow semantic parser is part of a noisy channel model that post-processes the output of an ASR. 3.1 Word error classifier We investigated three classifiers to determine whether a word in the ASR textual output is correct: the Weka implementation of Decision Tre"
I13-1027,W09-3907,1,0.854975,"Missing"
I13-1027,P04-1005,0,0.0988166,"d Research This research combines three main elements: correction of ASR output, noisy channel models and shallow semantic parsing. López-Cózar and Griol (2010) used lexical approaches to replace, insert or delete words in a textual ASR output, and syntactic approaches to modify tenses of verbs and grammatical numbers to better match grammatical expectations. However, these actions make ad hoc changes. The noisy channel model has been employed for various NLP tasks, such as ASR output correction (Ringger and Allen, 1996), spelling correction (Brill and Moore, 2000), and disfluency correction (Johnson and Charniak, 2004; Zwarts et al., 2010). Our approach differs from the traditional noisy channel approach in that it uses a word-error classifier to model the noisy channel, and semantic information to model the input characteristics. Shallow semantic parsers for SDSs have been used in (Coppola et al., 2009; Geertzen, 2009). Coppola et al. (2009) used FrameNet (Baker et al., 1998) to detect and filter the frames for target words, and employed a Support Vector Machine (SVM) classifier to perform semantic labeling. Geertzen (2009) used a shallow parser to detect semantic units only when a dependency parser faile"
I13-1027,C10-1154,0,0.0972094,"was evaluated on a corpus of 295 spoken referring expressions, improving interpretation performance. 1 the stool to the left of the storm the left of Object Prep Specifier the plate in to play it in Object Noise Prep the table the table Landmark the microwave the microwave Landmark The idea of the noisy channel model is that a message is sent through a channel that introduces errors, and the receiver endeavours to reconstruct the original message by taking into account the characteristics of the noisy channel and of the transmitted information (Ringger and Allen, 1996; Brill and Moore, 2000; Zwarts et al., 2010). The system described in this paper handles three types of errors: noise (which is removed), missing prepositions (which are inserted), and mis-heard words (which are replaced). Table 1 shows two descriptions that illustrate these errors. The first row for each description displays what was spoken, the second row displays what was heard by the ASR, and the third row shows the semantic labels assigned to each segment in the description by a shallow semantic parser (Section 3.2). Specifically, in the first example, the preposition “to” is missing, and the object “stool” is mis-heard as “storm”;"
I13-1027,I13-1026,1,0.711099,"Missing"
I13-1027,C98-1013,0,\N,Missing
P06-2064,briscoe-carroll-2002-robust,0,0.0184929,"we can identify the matching relation forms of semantic relations to decide the semantic relation for NCs. 3 Noun Compound Get sentences with H,M S({have, own, possess} V , M SUBJ , H OBJ ) (5) S({belong to} V , H SUBJ , M OBJ ) RASP parser Collect Subj, Obj, PP, PPN, V, T 4 Method Figure 1 outlines the system architecture of our approach. We used three corpora: the Brown corpus (as contained in the Penn Treebank), the Wall Street Journal corpus (also taken from the Penn treebank), and the written component of the British National Corpus (BNC). We ﬁrst parsed each of these corpora using RASP (Briscoe and Carroll, 2002), and identiﬁed for each verb token the voice, head nouns of the subject and object, and also, for each PP attached to that verb, the head preposition and head noun of the Semantic Relations in Compound Nouns While there has been wide recognition of the need for a system of semantic relations with which to classify NCs, there is still active debate as to what the composition of that set should be, or indeed 493 As mentioned earlier, we built our supervised classiﬁer using TiMBL. NP (hereafter, PPN). Next, for our test NCs, we identiﬁed all verbs for which the modiﬁer and head noun co-occur as"
P06-2064,C02-1011,0,0.0108296,"Missing"
P06-2064,I05-1082,1,0.383251,"t performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations. We interpret semantic relations relative to a ﬁxed set of constructions involving the modiﬁer and head noun and a set of seed verbs for each semantic relation: e.g. (the) family owns (a) car is taken as evidence for family car being an instance of the POSSESSOR relation. We then attempt to map all instances of the modiﬁer and head noun as the heads of NPs in a transitive sen"
P06-2064,J02-3004,0,0.0730099,"tware Engineering University of Melbourne, Victoria 3010 Australia and ‡ NICTA Victoria Research Lab University of Melbourne, Victoria 3010 Australia {snkim,tim}@csse.unimelb.edu.au Abstract rules. Vanderwende (1994) attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method"
P06-2064,W04-2609,0,0.676526,"e automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations. We interpret semantic relations relative to a ﬁxed set of constructions involving the modiﬁer and head noun and a set of seed verbs for each semantic relation: e.g. (the) family owns (a) c"
P06-2064,W04-0404,1,0.883662,"Missing"
P06-2064,W01-0511,0,0.884588,"ity of Melbourne, Victoria 3010 Australia {snkim,tim}@csse.unimelb.edu.au Abstract rules. Vanderwende (1994) attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NCs with training instances. The aim of this paper is to develop an automatic method for interpreting NCs based on semantic relations. We interpret semantic relations relative to a ﬁxed set of constructio"
P06-2064,C94-2125,0,0.19583,"b−Mapping map verbs onto seed verbs Modified Sentences Match modified sentences wrt relation forms WordNet::Similarity Moby’s Thesaurus Classifier Final Sentences Semantic Relation Classifier:Timbl Figure 1: System Architecture whether it is reasonable to expect that all NCs should be interpretable with a ﬁxed set of semantic relations. Based on the pioneering work on Levi (1979) and Finin (1980), there have been efforts in computational linguistics to arrive at largely taskspeciﬁc sets of semantic relations, driven by the annotation of a representative sample of NCs from a given corpus type (Vanderwende, 1994; Barker and Szpakowicz, 1998; Rosario and Marti, 2001; Moldovan et al., 2004). In this paper, we use the set of 20 semantic relations deﬁned by Barker and Szpakowicz (1998), rather than deﬁning a new set of relations. The main reasons we chose this set are: (a) that it clearly distinguishes between the head noun and modiﬁers, and (b) there is clear documentation of each relation, which is vital for NC annotation effort. The one change we make to the original set of 20 semantic relations is to exclude the PROPERTY relation since it is too general and a more general form of several other relati"
P06-2064,P98-1015,0,0.854863,"in Noun Compounds via Verb Semantics Su Nam Kim† and Timothy Baldwin†‡ † Computer Science and Software Engineering University of Melbourne, Victoria 3010 Australia and ‡ NICTA Victoria Research Lab University of Melbourne, Victoria 3010 Australia {snkim,tim}@csse.unimelb.edu.au Abstract rules. Vanderwende (1994) attempted the automatic interpretation of NCs using hand-written rules, with the obvious cost of manual intervention. Fan et al. (2003) estimated the knowledge required to interpret NCs and claimed that performance was closely tied to the volume of data acquired. In more recent work, Barker and Szpakowicz (1998) used a semi-automatic method for NC interpretation in a ﬁxed domain. Lapata (2002) developed a fully automatic method but focused on nominalizations, a proper subclass of NCs.1 Rosario and Marti (2001) classiﬁed the nouns in medical texts by tagging hierarchical information using neural networks. Moldovan et al. (2004) used the word senses of nouns based on the domain or range of interpretation of an NC, leading to questions of scalability and portability to novel domains/NC types. Kim and Baldwin (2005) proposed a simplistic general-purpose method based on the lexical similarity of unseen NC"
P06-2064,P94-1019,0,0.0538751,"loyed in our method. As our parser, we used RASP, generating a dependency representation for the most probable parse for each sentence. Note that RASP also lemmatises all words in a POS-sensitive manner. To map actual verbs onto seed verbs, we experimented with two resources: WordNet::Similarity and Moby’s thesaurus. WordNet::Similarity2 is an open source software package that allows the user to measure the semantic similarity or relatedness between two words (Patwardhan et al., 2003). Of the many methods implemented in WordNet::Similarity, we report on results for one path-based method (WUP, Wu and Palmer (1994)), one content-information based method (JCN, Jiang and Conrath (1998)) and two semantic relatedness methods (LESK, Banerjee and Pedersen (2003), and VECTOR, (Patwardhan, 2003)). We also used a random similarity-generating method as a baseline (RANDOM). The second semantic resource we use for verbmapping method is Moby’s thesaurus. Moby’s thesaurus is based on Roget’s thesaurus, and contains 30K root words, and 2.5M synonyms and related words. Since the direct synonyms of seed verbs have limited coverage over the set of sentences used in our experiment, we also experimented with using second-l"
P06-2064,O97-1002,0,\N,Missing
P06-2064,C98-1015,0,\N,Missing
S07-1049,P98-1015,0,0.116023,"ven semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the C ONTENTC ONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-deﬁned inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are M AKE,1 , as exempliﬁed in apple pie where the pie is made from apple(s), and P OSSES SOR , as exempliﬁed in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: C AUSE -E FFECT, I NSTRUMENT-AGENCY, P RODUCT-P RODUCER, O RIGIN -E NTITY, T HEME T OOL, PART-W HOLE and C ONTENT-C ONTAINER. Our approach to the task was to: (1) naively trea"
S07-1049,I05-1082,1,0.800034,"of general-purpose SR interpretation over the nominal classiﬁcation task, and establish a new baseline for the task. The remainder of this paper is structured as follows. We present our methods in Section 2 and depict the system architectures in Section 4. We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6. 2 Approach We used two basic NC interpretation methods. The ﬁrst method uses sense collocations as proposed by Moldovan et al. (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Note that neither method uses the context of usage of the NC, i.e. the only features are the words contained in the NC. 2.1 Sense Collocation Method Moldovan et al. (2004) proposed a method called semantic scattering for interpreting NCs. The intuition behind this method is that when the sense collocation of NCs is the same, their SR is most likely the same. For example, the sense collocation of automobile factory is the same as that of car factory, because the senses of automobile and car, and factory 232 in the two instances, are identical. As a result, the two NCs have the semantic relati"
S07-1049,W04-2609,0,0.121186,"(and word sense) is a strong determinant of the SR in practice. Our aim in this paper is to demonstrate the effectiveness of general-purpose SR interpretation over the nominal classiﬁcation task, and establish a new baseline for the task. The remainder of this paper is structured as follows. We present our methods in Section 2 and depict the system architectures in Section 4. We then describe and discuss the performance of our methods in Section 5 and conclude the paper in Section 6. 2 Approach We used two basic NC interpretation methods. The ﬁrst method uses sense collocations as proposed by Moldovan et al. (2004), and the second method uses the lexical similarity of the component words in the NC as proposed by Kim and Baldwin (2005). Note that neither method uses the context of usage of the NC, i.e. the only features are the words contained in the NC. 2.1 Sense Collocation Method Moldovan et al. (2004) proposed a method called semantic scattering for interpreting NCs. The intuition behind this method is that when the sense collocation of NCs is the same, their SR is most likely the same. For example, the sense collocation of automobile factory is the same as that of car factory, because the senses of"
S07-1049,C94-2125,0,0.0109686,"mpatibility of a given semantic relation with each of a set of test nominal pairs, e.g. between climate and forest in the fragment the climate in the forest with respect to the C ONTENTC ONTAINER relation. Semantic relations (or SRs) in nominals represent the underlying interpretation of the nominal, in the form of the directed relation between the two nominals. The proposed task is a generalisation of the more conventional task of interpreting noun compounds (NCs), in which we take a NC such as cookie jar and interpret it according to a pre-deﬁned inventory of semantic relations (Levi, 1979; Vanderwende, 1994; Barker and Szpakowicz, 1998). Examples of semantic relations are M AKE,1 , as exempliﬁed in apple pie where the pie is made from apple(s), and P OSSES SOR , as exempliﬁed in family car where the car is possessed by a family. In the SemEval-2007 task, SR interpretation takes the form of a binary decision for a given nominal pair in context and a given SR, in judging whether that nominal pair conforms to the SR. Seven relations were used in the task: C AUSE -E FFECT, I NSTRUMENT-AGENCY, P RODUCT-P RODUCER, O RIGIN -E NTITY, T HEME T OOL, PART-W HOLE and C ONTENT-C ONTAINER. Our approach to the"
S07-1049,P94-1019,0,0.0074081,"s .71 and .27, and .83 and 1.00 respectively. We would then add these up to derive the overall similarity for a given NC and ﬁnd that apple juice is a better match. From this, we would assign the SR of M AKE from apple juice to chocolate milk. Formally, SA is the similarity between NCs (Ni,1 , Ni,2 ) and (Bj,1 , Bj,2 ): SA ((Ni,1 , Ni,2 ), (Bj,1 , Bj,2 )) = ((αS1 + S1) × ((1 − α)S2 + S2)) (3) 2 where S1 is the modiﬁer similarity (i.e. S(Ni,1 , Bj1 )) and S2 is head noun similarity (i.e. S(Ni,2 , Bj2 )); α ∈ [0, 1] is a weighting factor. The similarity scores are calculated using the method of Wu and Palmer (1994) as implemented in WordNet::Similarity (Patwardhan et al., 2003). This is done for each pairing of WordNet senses of each of the two words in question, and the overall lexical similarity is calculated as the average across the pairwise sense similarities. The ﬁnal classiﬁcation is derived from the training instance which has the highest lexical similarity with the test instance in question. 3 Co-Training As with many semantic annotation tasks, SR tagging is a time-consuming and expensive process. At the same time, due to the inherent complexity of the SR interpretation task, we require large a"
S07-1049,C98-1015,0,\N,Missing
S07-1050,agirre-de-lacalle-2004-publicly,0,0.0691583,"Missing"
S07-1050,P98-2127,0,0.246172,"Missing"
S07-1050,U06-1008,1,0.898149,"Missing"
S07-1050,C98-2122,0,\N,Missing
S10-1004,W03-1028,0,0.955241,"g, National University of Singapore, Singapore sunamkim@gmail.com, medelyan@gmail.com, kanmy@comp.nus.edu.sg, tb@ldwin.net Abstract have showcased the potential benefits of keyphrase extraction to downstream NLP applications. In light of these developments, we felt that this was an appropriate time to conduct a shared task for keyphrase extraction, to provide a standard assessment to benchmark current approaches. A second goal of the task was to contribute an additional public dataset to spur future research in the area. Currently, there are several publicly available data sets.2 For example, Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. The data set contains keyphrases (i.e. controlled and uncontrolled terms) assigned by professional indexers — 1,000 for training, 500 for validation and 500 for testing. Nguyen and Kan (2007) collected a dataset containing 120 computer science articles, ranging in length from 4 to 12 pages. The articles contain author-assigned keyphrases as well as reader-assigned keyphrases contributed by undergraduate CS students. In the general newswire domain, Wan and Xiao (2008) developed a dataset of 308 d"
S10-1004,N04-4005,0,0.0188339,"tions such as summarization, information retrieval and question-answering. In summarization, keyphrases can be used as a form of semantic metadata (Barzilay and Elhadad, 1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases can supplement full-text indexing and assist users in formulating queries. Recently, a resurgence of interest in keyphrase extraction has led to the development of several new systems and techniques for the task (Frank et al., 1999; Witten et al., 1999; Turney, 1999; Hulth, 2003; Turney, 2003; Park et al., 2004; Barker and Corrnacchia, 2000; Hulth, 2004; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007; Wan and Xiao, 2008; Liu et al., 2009; Medelyan, 2009; Nguyen and Phan, 2009). These 1 We use “keyphrase” and “keywords” interchangeably to refer to both single words and phrases. ♦ Min-Yen Kan’s work was funded by National Research Foundation grant “Interactive Media Search” (grant # R-252000-325-279). 2 All data sets listed below are available for download from http://github.com/snkim/ AutomaticKeyphraseExtraction 3 http://bit.ly/maui-datasets 21 Proceedings of the 5th International Worksho"
S10-1004,C08-1122,0,0.704332,"available data sets.2 For example, Hulth (2003) contributed 2,000 abstracts of journal articles present in Inspec between the years 1998 and 2002. The data set contains keyphrases (i.e. controlled and uncontrolled terms) assigned by professional indexers — 1,000 for training, 500 for validation and 500 for testing. Nguyen and Kan (2007) collected a dataset containing 120 computer science articles, ranging in length from 4 to 12 pages. The articles contain author-assigned keyphrases as well as reader-assigned keyphrases contributed by undergraduate CS students. In the general newswire domain, Wan and Xiao (2008) developed a dataset of 308 documents taken from DUC 2001 which contain up to 10 manually-assigned keyphrases per document. Several databases, including the ACM Digital Library, IEEE Xplore, Inspec and PubMed provide articles with authorassigned keyphrases and, occasionally, readerassigned ones. Medelyan (2009) automatically generated a dataset using tags assigned by the users of the collaborative citation platform CiteULike. This dataset additionally records how many people have assigned the same keyword to the same publication. In total, 180 full-text publications were annotated by over 300"
S10-1004,R09-1086,0,0.312048,"tual F-scores 2.2 Evaluation Method and Baseline Traditionally, automatic keyphrase extraction systems have been assessed using the proportion of top-N candidates that exactly match the goldstandard keyphrases (Frank et al., 1999; Witten et al., 1999; Turney, 1999). In some cases, inexact matches, or near-misses, have also been considered. Some have suggested treating semanticallysimilar keyphrases as correct based on similarities computed over a large corpus (Jarmasz and Barriere, 2004; Mihalcea and Tarau, 2004), or using semantic relations defined in a thesaurus (Medelyan and Witten, 2006). Zesch and Gurevych (2009) compute near-misses using an ngram based approach relative to the gold standard. For our shared task, we follow the traditional exact match evaluation metric. That is, we match the keyphrases in the answer set with those the systems provide, and calculate micro-averaged precision, recall and F-score (β = 1). In the evaluation, we check the performance over the top 5, 10 and 15 candidates returned by each system. We rank the participating systems by F-score over the top 15 candidates. Participants were required to extract existing phrases from the documents. Since it is theoretically possible"
S10-1004,P09-2046,0,0.0298245,"1997; Lawrie et al., 2001; D’Avanzo and Magnini, 2005). In search engines, keyphrases can supplement full-text indexing and assist users in formulating queries. Recently, a resurgence of interest in keyphrase extraction has led to the development of several new systems and techniques for the task (Frank et al., 1999; Witten et al., 1999; Turney, 1999; Hulth, 2003; Turney, 2003; Park et al., 2004; Barker and Corrnacchia, 2000; Hulth, 2004; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Medelyan and Witten, 2006; Nguyen and Kan, 2007; Wan and Xiao, 2008; Liu et al., 2009; Medelyan, 2009; Nguyen and Phan, 2009). These 1 We use “keyphrase” and “keywords” interchangeably to refer to both single words and phrases. ♦ Min-Yen Kan’s work was funded by National Research Foundation grant “Interactive Media Search” (grant # R-252000-325-279). 2 All data sets listed below are available for download from http://github.com/snkim/ AutomaticKeyphraseExtraction 3 http://bit.ly/maui-datasets 21 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21–26, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics search areas in each case. Note that the trial d"
S10-1004,W04-3252,0,\N,Missing
S10-1004,W97-0703,0,\N,Missing
S10-1004,D09-1027,0,\N,Missing
S10-1004,C02-1142,0,\N,Missing
S10-1006,W09-1401,0,0.0287746,"for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations.1 Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to “aboutness” annotation such as semantic roles (Carreras and M`arquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., “Smoking may/may not have caused cancer in this case.” We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities ar"
S10-1006,W05-0620,0,\N,Missing
S10-1007,P07-1072,0,0.0565426,"ited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose a"
S10-1007,I05-1082,1,0.151111,"a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected dire"
S10-1007,P06-2064,1,0.272527,"(MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This pa"
S10-1007,E03-1073,0,0.0115181,"ample, malaria mosquito is a ‘mosquito that carries malaria’. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems. 1 Introduction Noun compounds (NCs) are sequences of two or more nouns that act as a single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Bald´ S´eaghdha, 2008). As a win and Tanaka, 2004;"
S10-1007,C94-2125,0,0.271982,"Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edit"
S10-1007,W04-2609,0,0.051487,"ing, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popular"
S10-1007,P08-1052,1,0.415224,"e, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird’s-eye view of the task. Section 2"
S10-1007,W06-3813,1,0.819134,"lex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-bas"
S10-1007,W07-1108,1,0.889705,"Missing"
S10-1007,D08-1027,0,0.00993176,"Missing"
S10-1007,C08-1011,1,\N,Missing
S10-1007,W03-1803,0,\N,Missing
S10-1007,W04-0404,0,\N,Missing
S10-1007,P84-1109,0,\N,Missing
S10-1007,W01-0511,0,\N,Missing
U07-1009,P07-1072,0,0.0126833,"o enhance NC interpretation; the noun components that comprise the NCs are disambiguated using these WSD techniques (Sparck Jones, 1983; Kim and Baldwin, 2007). Kim and Baldwin (2007) carried out experiments on automatically modeling WSD and attested the usefulness of conducting word sense analysis of an NC in determining its SR. 2.2 Previous Approaches to NC Interpretation A majority of research undertaken in interpreting NCs has been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; Kim and Baldwin, 2007) and SEMANTIC INTER PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity method. A signiﬁcant contribution to this area is by Moldovan et al. (2004), who used the sense collocation (i.e. pair of word senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual informat"
U07-1009,S07-1003,0,0.0305768,"Missing"
U07-1009,P06-2064,1,0.702941,"WSD techniques (Sparck Jones, 1983; Kim and Baldwin, 2007). Kim and Baldwin (2007) carried out experiments on automatically modeling WSD and attested the usefulness of conducting word sense analysis of an NC in determining its SR. 2.2 Previous Approaches to NC Interpretation A majority of research undertaken in interpreting NCs has been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; Kim and Baldwin, 2007) and SEMANTIC INTER PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity method. A signiﬁcant contribution to this area is by Moldovan et al. (2004), who used the sense collocation (i.e. pair of word senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from 5 European languages as features to"
U07-1009,J02-3004,0,0.154449,"elated. As English noun phrases are rightheaded, the head noun occurs after all modifying 49 Research on NCs can be categorised into four main groups: deﬁning SRs, disambiguating the syntax of NCs, disambiguating the semantics of NCs, and interpreting NCs via SRs. Each task is detailed in Section 2.1. Interpreting NCs has received much attention of late, and the problem has been addressed in areas of machine translation (MT), information extraction (IE), and applications such as questionanswering (QA). NCs pose a considerable challenge to computational linguistics due to the following issues (Lapata, 2002): (1) NCs are extremely productive; (2) the semantic relationship between the head noun and its modiﬁer(s) is implicit; and (3) the interpretation of an NC can vary due to contextual and pragmatic factors. Due to these challenges, current NC interpretation methods are too error-prone to be employed directly in NLP applications without human intervention or preprocessing. In this paper, we investigate the task of NC interpretation based on sense collocation. It has been shown that NCs with semantically similar components share the same SR (Kim and Baldwin, 2007); this is encapsulated by the phr"
U07-1009,W04-2609,0,0.0660329,"elb.edu.au Abstract nouns. For example, brick house is interpreted as a house that is modiﬁed by the word brick, which exhibits a P RODUCT-P RODUCER relationship between the two nouns in the compound. In contrast, the modiﬁer and head in house brick exhibits a PARTW HOLE relationship, which is interpreted as a brick from a house, rather than the former interpretation of a house made of bricks. The set of SRs that we are concerned with in this paper is deﬁned in Section 5.1. This paper investigates the task of noun compound interpretation, building on the sense collocation approach proposed by Moldovan et al. (2004). Our primary task is to evaluate the impact of similar words on the sense collocation method, and decrease the sensitivity of the classiﬁers by expanding the range of sense collocations via different semantic relations. Our method combines hypernyms, hyponyms and sister words of the component nouns, based on W ORD N ET. The data used in our experiments was taken from the nominal pair interpretation task of S EM E VAL -2007 (4th International Workshop on Semantic Evaluation 2007). In our evaluation, we test 7-way and 2-way class data, and show that the inclusion of hypernyms improves the perfo"
U07-1009,W05-0603,0,0.074031,"Missing"
U07-1009,W01-0511,0,0.0450922,"Missing"
U07-1009,W07-1108,0,0.0635081,"Missing"
U07-1009,C94-2125,0,0.0419458,"Cs are disambiguated using these WSD techniques (Sparck Jones, 1983; Kim and Baldwin, 2007). Kim and Baldwin (2007) carried out experiments on automatically modeling WSD and attested the usefulness of conducting word sense analysis of an NC in determining its SR. 2.2 Previous Approaches to NC Interpretation A majority of research undertaken in interpreting NCs has been based on two statistical methods: SEMANTIC SIMILARITY (Barker and Szpakowicz, 1998; Rosario, 2001; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase, 2006; Girju, 2007; Kim and Baldwin, 2007) and SEMANTIC INTER PRETABILITY (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov, 2006). Our work, based on an extension of the sense collocation approach, corresponds to the semantic similarity method. A signiﬁcant contribution to this area is by Moldovan et al. (2004), who used the sense collocation (i.e. pair of word senses) as their primary feature in disambiguating NCs. Many subsequent studies have been based on this sense collocation method, with the addition of other performance-improving features. For example, Girju (2007) added contextual information (e.g. the grammatical role and POS) and cross-lingual information from"
U07-1009,I05-1082,1,\N,Missing
U07-1009,P98-1015,0,\N,Missing
U07-1009,C98-1015,0,\N,Missing
U09-1013,drouin-2004-detection,0,0.0319108,"the Reuters document collection using term frequency and inverse document frequency. 1 Introduction Automatic domain-specific term extraction is a categorization/classification task where terms are categorized into a set of predefined domains. It has been employed in tasks such as keyphrase extraction (Frank et al., 1999; Witten et al., 1999), word sense disambiguation (Magnini et al., 2002), and query expansion and cross-lingual text categorization (Rigutini et al., 2005). Even though the approach shows promise, relatively little research has been carried out to study its effects in detail (Drouin, 2004; Milne et al., 2006; Rigutini et al., 2006; Kida et al., 2007; Park et al., 2008). Most of the research to date on domainspecific term extraction has employed supervised machine learning, within the fields of term categorization and text mining. However, to date, the only research to approach the task in an unsupervised manner is that of Park et al. (2008). Unsupervised methods have the obvious advantage that they circumvent the need for laborious manual classification of training instances, and are thus readily applicable to arbitrary sets of domains, tasks and languages. In this paper, we p"
U09-1013,P06-1068,0,0.0754223,"Missing"
U09-1013,S01-1027,0,0.0880807,"Missing"
U10-1006,W10-0508,1,0.600811,"line users to share information on the Internet. Users post their questions or problems onto online forums and get possible solutions from other users. Through this simple mechanism, great volumes of data with customised answers to highly specialised domain-specific questions are created on a daily basis. However, it is not an easy job to extract the information latent in the threads. The aim of our research is to help users to more easily access existing information in forums which relate to their questions, by text mining troubleshooting-oriented, computer-related technical user forum data (Baldwin et al., 2010). An example thread from a real-world forum is shown in Figure 1, which is made up of 4 posts with 3 distinct participants. Our proposed strategy is to model the “content structure” of forum threads by analysing requests for information and provision of solutions in the thread data. We devise an ontology of problem sources and solution types with which to analyse individual threads, paving the way for users to spell out the general nature of their support need in their queries. The main contributions of this paper are: (1) designing a modular thread-level class set; (2) constructing and publis"
U10-1006,P05-1037,0,0.0204605,"evel class set; (2) constructing and publishing an annotated dataset; and (3) performing preliminary threadlevel experiments over the dataset. 2 Related Work There is very little work that is specifically targeted at the thread-level analysis of web user forum data. The most closely-related work is that performed by Baldwin et al. (2007), and our thread class set was created based on this original work. Another research line that relates to the thread classification is discussion summarisation. For example, technical online IRC (Internet Relay Chat) discussions are summarised and segmented in Zhou and Hovy (2005)’s research. The message segments are then clustered to find the most relevant information to users using machine learning models. There has also been work on email summarisation, concentrating primarily on summarising and organising email archives by extracting overview sentences to help users find the most useful email threads (Nenkova and Bagga, 2004; Li Wang, Su Nam Kim and Timothy Baldwin. 2010. Thread-level Analysis over Technical User Forum Data. In Proceedings of Australasian Language Technology Association Workshop, pages 27−31 Rambow et al., 2004; Wan and McKeown, 2004). 3 The crawle"
U10-1006,N04-4027,0,0.02075,"ions are summarised and segmented in Zhou and Hovy (2005)’s research. The message segments are then clustered to find the most relevant information to users using machine learning models. There has also been work on email summarisation, concentrating primarily on summarising and organising email archives by extracting overview sentences to help users find the most useful email threads (Nenkova and Bagga, 2004; Li Wang, Su Nam Kim and Timothy Baldwin. 2010. Thread-level Analysis over Technical User Forum Data. In Proceedings of Australasian Language Technology Association Workshop, pages 27−31 Rambow et al., 2004; Wan and McKeown, 2004). 3 The crawled threads were then preprocessed. Only the title and sub-forum information of each thread, and the body, title, and author information of each post were preserved. Finally, we randomly selected 500 threads from 4 sub-forums of the CNET forums: Operating Systems, Software, Hardware, and Web Development. Two annotators performed a pilot annotation using a seed set of 150 threads and a dedicated web annotation tool. The κ value for the pilot annotation (indicating the relative agreement between the two annotators) was 0.43. The annotators sat down together to"
U10-1006,C04-1079,0,0.0195437,"nd segmented in Zhou and Hovy (2005)’s research. The message segments are then clustered to find the most relevant information to users using machine learning models. There has also been work on email summarisation, concentrating primarily on summarising and organising email archives by extracting overview sentences to help users find the most useful email threads (Nenkova and Bagga, 2004; Li Wang, Su Nam Kim and Timothy Baldwin. 2010. Thread-level Analysis over Technical User Forum Data. In Proceedings of Australasian Language Technology Association Workshop, pages 27−31 Rambow et al., 2004; Wan and McKeown, 2004). 3 The crawled threads were then preprocessed. Only the title and sub-forum information of each thread, and the body, title, and author information of each post were preserved. Finally, we randomly selected 500 threads from 4 sub-forums of the CNET forums: Operating Systems, Software, Hardware, and Web Development. Two annotators performed a pilot annotation using a seed set of 150 threads and a dedicated web annotation tool. The κ value for the pilot annotation (indicating the relative agreement between the two annotators) was 0.43. The annotators sat down together to go over every thread wh"
U10-1006,C00-2137,0,\N,Missing
U11-1009,I08-2108,1,0.915731,"d labels would be more usable (e.g. document classification using coarse-grained labels of terms), thus, we used 9 super-labels in this work. Table 2 shows the final domain concepts we used 3 4 http://foldoc.org/ http://www.onelook.com/ 60 4 Methodology 4.1 Feature Set I: Bag-of-Words n-gram-based bag-of-words (BoW) features are one of the most broadly applied features to measure the semantic similarity between two terms/texts. This has been used in various tasks such as document classification (Joachims, 1998), dialogue act classification (Ivanovic, 2005) and term classification (Lesk, 1986; Baldwin et al., 2008). As shown in (Hulth and Megyesi, 2006), keywords along the contextual features (i.e., simple 1grams) are useful in identifying semantic similarity. However, keywords are often multi-grams such as 2-grams (e.g. Fast Ethernet, optical mouse) and 3-grams (e.g. 0/1 knapsack problem, Accelerated Graphics Port). Sharing the same intuition, some previous work (Ivanovic, 2005) employed not only 1-grams but also 2-grams for the classification task. Similarly, we also observed that terms are often multi-grams. Thus, in this work, we also explored various n-grams. In evaluation, we tested 1- and 2grams"
U11-1009,drouin-2004-detection,0,0.203178,"nspecific terms are further categorized in terms of their domain concepts (i.e., semantic labels/classes). Lawrence Cavedon School of CS IT RMIT University Melbourne, Australia lawrence.cavedon@rmit.edu.au For example, Firefox belongs to the domain concept Software, while Prolog is associated with the domain concept Programming. In this paper, we use the term domain concept for consistency. Note that in previous work, the meaning of the domainspecificity is associated with either word senses (e.g. (Magnini et al., 2002; Rigutini et al., 2005)) or the statistical use of terms in context (e.g. (Drouin, 2004; Milne et al., 2006; Kida et al., 2007; Park et al., 2008; Kim et al., 2009; Vivaldi and Rodrguez, 2010)). In WordNet, the domain concept is assigned based on the word senses. Similarly, WordNet Domain has terms with domain concepts per sense. However, most work previously conducted work used domain-specificity is based on statistical use. In this paper, we follow the latter definition, i.e., domain-specificity associated with the statistical use of the term. Domain-specificity of terms has been leveraged in various natural language processing (NLP) and related tasks, such as word sense disam"
U11-1009,P06-1068,0,0.0294409,"document classification using coarse-grained labels of terms), thus, we used 9 super-labels in this work. Table 2 shows the final domain concepts we used 3 4 http://foldoc.org/ http://www.onelook.com/ 60 4 Methodology 4.1 Feature Set I: Bag-of-Words n-gram-based bag-of-words (BoW) features are one of the most broadly applied features to measure the semantic similarity between two terms/texts. This has been used in various tasks such as document classification (Joachims, 1998), dialogue act classification (Ivanovic, 2005) and term classification (Lesk, 1986; Baldwin et al., 2008). As shown in (Hulth and Megyesi, 2006), keywords along the contextual features (i.e., simple 1grams) are useful in identifying semantic similarity. However, keywords are often multi-grams such as 2-grams (e.g. Fast Ethernet, optical mouse) and 3-grams (e.g. 0/1 knapsack problem, Accelerated Graphics Port). Sharing the same intuition, some previous work (Ivanovic, 2005) employed not only 1-grams but also 2-grams for the classification task. Similarly, we also observed that terms are often multi-grams. Thus, in this work, we also explored various n-grams. In evaluation, we tested 1- and 2grams individually as well as the combination"
U11-1009,P05-2014,0,0.203053,"uru and Chen, 2007). Since some resources are already publicly available (despite shortcomings), utilizing these resources reduces the time for manually developing training data, and should lead to robust systems due to consistent labeling. In addition, such resources are reusable for enlarging the existing resources or creating new semantic resources. Our basic approach is to use semantic similarity between domain-specific terms. Contextual features have often been employed for semantic similarity in various tasks, such as text categorization (Joachims, 1998) and dialogue act classification (Ivanovic, 2005). Thus, we also explore using context as base features. Furthermore, we explore the use of rich semantic features. That is, we employ the domain concepts and topics derived from known domain-specific terms over the same resource as additional features. We detail our rich semantic features in Section 4.2 and 4.3. In evaluation, we applied our approaches to the domain Computing, as the interest in this domain is growing due to the large volume of web corpora, including social media such as web forums and blogs. In the following sections, we describe related work and the existing resources in Sec"
U11-1009,D07-1073,0,0.0384271,"2009; Vivaldi and Rodrguez, 2010)). In WordNet, the domain concept is assigned based on the word senses. Similarly, WordNet Domain has terms with domain concepts per sense. However, most work previously conducted work used domain-specificity is based on statistical use. In this paper, we follow the latter definition, i.e., domain-specificity associated with the statistical use of the term. Domain-specificity of terms has been leveraged in various natural language processing (NLP) and related tasks, such as word sense disambiguation (WSD) (Magnini et al., 2002), named entity recognition (NER) (Kazama and Torisawa, 2007), and query expansion (Rigutini et al., 2005). Resources containing domain information fall into two groups: the list of domain-specific terms without domain concepts (e.g. Agrivoc, EUROVOC, ASFA Thesaurus); and with domain concepts (e.g. WordNet, WordNet Domain, Unified Medical Language System (UMLS)). Although there have been efforts developing such knowledge resources, the task has been generally carried out by hand, requiring high cost and time. Further, even hand-crafted resources are often limited in terms of quality and quantity. Su Nam Kim and Lawrence Cavedon. 2011. Classifying Domain"
U11-1009,J04-2002,0,0.0488264,"ves finding named entities and assigning each a tag from a predefined set of categories, such as LOCATION or PERSON. The difference with our task in this paper is that in both WSD and NER, the target terms are generally in some use context (i.e., the correct word sense of target term depends on that context), while our targets are isolated, i.e., appear out of context. In this paper, our approach is closer to corpus-based WSD which normally uses co-occurrence of terms between two corpora. In recent years, there have been systems proposed to extract terms and to assign semantic labels to them (Navigli and Velard, 2004; Cimiano and Vlker, 2005; Nicola et al., 2009). OntoLearn (Navigli and Velard, 2004) has three components. First, it extracts a domain terminology from Web sites. It then assigns the domain concepts in order to build a hierarchical structure of ontologies. The system uses semantic similarity between WordNet concepts for component words in a candidate and conceptual relations among the concept components based on word senses. Finally, ontologies in WordNet are trimmed and enriched with the extracted domain concepts. Text2Onto (Cimiano and Vlker, 2005) is another ontology builder and includes t"
U11-1009,P98-2181,0,0.0461999,"utomatic ontology builders (e.g. OntoLearn, Text2Onto) that work by extracting domain-specific terms and tagging domain-concepts to build such resources. Building a domain-specific ontology requires two main tasks — extracting domain-specific terms and assigning the domain concept(s). There have been several methods proposed for each task and also as a complete ontology builder—we describe such work in Section 2. In this paper, our interest lies on assigning domain concepts to the existing domain-specific resources. In one sense our task can be viewed as building a taxonomy from dictionaries (Rigau et al., 1998) and/or a semantic class labelling task (Punuru and Chen, 2007). Since some resources are already publicly available (despite shortcomings), utilizing these resources reduces the time for manually developing training data, and should lead to robust systems due to consistent labeling. In addition, such resources are reusable for enlarging the existing resources or creating new semantic resources. Our basic approach is to use semantic similarity between domain-specific terms. Contextual features have often been employed for semantic similarity in various tasks, such as text categorization (Joach"
U11-1009,vivaldi-rodriguez-2010-finding,0,0.0784153,"labels/classes). Lawrence Cavedon School of CS IT RMIT University Melbourne, Australia lawrence.cavedon@rmit.edu.au For example, Firefox belongs to the domain concept Software, while Prolog is associated with the domain concept Programming. In this paper, we use the term domain concept for consistency. Note that in previous work, the meaning of the domainspecificity is associated with either word senses (e.g. (Magnini et al., 2002; Rigutini et al., 2005)) or the statistical use of terms in context (e.g. (Drouin, 2004; Milne et al., 2006; Kida et al., 2007; Park et al., 2008; Kim et al., 2009; Vivaldi and Rodrguez, 2010)). In WordNet, the domain concept is assigned based on the word senses. Similarly, WordNet Domain has terms with domain concepts per sense. However, most work previously conducted work used domain-specificity is based on statistical use. In this paper, we follow the latter definition, i.e., domain-specificity associated with the statistical use of the term. Domain-specificity of terms has been leveraged in various natural language processing (NLP) and related tasks, such as word sense disambiguation (WSD) (Magnini et al., 2002), named entity recognition (NER) (Kazama and Torisawa, 2007), and q"
U11-1009,C00-2137,0,0.0253025,"icted labels. This is the system using TF·IDF valued 1-grams from all terms with frequency ≥ 2, as this was our bestperforming system. Overall, we found that many domain concepts are mislabeled with Programming and Documentation since they are most often used concepts and could be a border concept for terms labeled with other domain concepts. For example, CS and Documentation are often labeled as Programming, while Networking is mislabeled as Documentation. Finally, we used randomized estimation to calculate whether any performance differences between 63 methods are statistically significant (Yeh, 2000) and found all systems exceeding the baseline system had p-value ≤ 0.05, which indicates significant improvement. 5.2 Semi-supervised Learning For semi-supervised learning, we evaluated the impact of the size of training data. We observed that despite increasing training data, performance does not significantly improve. However, to compare the performance between supervised and semisupervised systems, we simulated semi-supervised system with unused training data from FOLDOC. Table 9 shows the performance of semi-supervised learning using two groups of features: 1-gram with frequency ≥ 1, and 1"
U11-1009,S01-1027,0,\N,Missing
U11-1009,C98-2176,0,\N,Missing
U13-1014,W11-2002,0,0.0678862,"Missing"
U13-1014,I13-1026,1,0.882388,"Missing"
U14-1007,W07-2307,0,0.0448355,"Missing"
U14-1007,I13-1026,1,0.8064,"Missing"
U14-1007,J12-1006,0,0.0261313,"Missing"
U14-1007,W11-2808,0,0.0319289,"Missing"
U14-1007,J06-2002,0,\N,Missing
W06-2110,W02-2001,1,0.869882,"vant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs can determine the likelihood of their occurrence with 1 The learning of lexical items in a form that can be fed directly into a de"
W06-2110,W03-1812,1,0.860051,"to be used with objects with the semantics of object and prepositional phrases containing NPs with the semantics of place. Also, as observed above, the valence of a VPC can differ from that of the head verb. (3) and (4) illustrate two different senses of take off with intransitive and transitive syntax, respectively. Note that take cannot occur as a simplex intransitive verb. wearing (3) take off = lift off The airplane takes off. ARGS : When verbs co-occur with particles to form VPCs, their meaning can be significantly different from the semantics of the head verb in isolation. According to Baldwin et al. (2003), divergences in VPC and head verb semantics are often reflected in differing selectional preferences, as manifested in patterns of noun co-occurrence. In one example cited in the paper, the cosine similarity between cut and cut out, based on word co-occurrence vectors, was found to be greater than that between cut and cut off, mirroring the intuitive compositionality of these VPCs. (1) and (2) illustrate the difference in the selectional preferences of the verb put in isolation as compared with the VPC put on.3 book OBJ = book, publication, object ANALYSIS : EX : 2 Put the book on the table."
W06-2110,W03-1809,1,0.923199,"lay lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) o"
W06-2110,briscoe-carroll-2002-robust,0,0.0226196,"off co-occurs with a subject of the class airplane, aeroplane. In (4), on the other hand, take off = remove and the corresponding object noun is of class garment or clothing. From the above, we can see that head nouns in the subject and object argument positions can be used to distinguish VPCs from simplex verbs with prepositional phrases (i.e. verb-PPs). 3 Approach Our goal is to distinguish VPCs from verb-PPs in corpus data, i.e. to take individual inputs such as Kim handed the paper in today and tag each as either a VPC or a verb-PP. Our basic approach is to parse each sentence with RASP (Briscoe and Carroll, 2002) to obtain a first-gloss estimate of the VPC and verb-PP token instances, and also identify the head nouns of the arguments of each VPC and simplex verb. For the head noun of each subject and object, as identified by RASP, we use WordNet 2.1 (Fellbaum, 1998) to obtain the word sense. Finally we build a supervised classifier using TiMBL 5.1 (Daelemans et al., 2004). raw text corpus RASP parser Preprocessing Verbs Particles v+p with Semantics e.g. take_off := [.. put_on := [.. look_after := [.. Subjects Objects WordNet Word Senses TiMBL Classifier 3.1 Method Compared to the method proposed by Ba"
W06-2110,calzolari-etal-2002-towards,0,0.0448145,"Missing"
W06-2110,P97-1018,0,0.0360332,"multiple simplex words and display lexical, syntactic and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He"
W06-2110,P03-1065,0,0.05032,"that of the head verb (e.g. walk off ) or alternatively diverge (e.g. lift off ). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of the compositionality/productivity of VPCs. There is a modest body of research on the identification and extraction of VPCs. Note that in the case of VPC identification we seek to detect individual VPC token instances in corpus data, whereas in the case of VPC extraction we seek to arrive at an inventory of VPC types/lexical items based on analysis of token instances in corpus data. Li et al. (2003) identify English VPCs (or “phrasal verbs” in their parlance) using handcoded regular expressions. Baldwin and Villavicencio (2002) extract a simple list of VPCs from corpus data, while Baldwin (2005) extracts VPCs with valence information under the umbrella of deep lexical acquisition.1 The method of Baldwin (2005) is aimed at VPC extraction and takes into account only the syntactic features of verbs. In this paper, our interest is in VPC identification, and we make use of deeper semantic information. In Fraser (1976) and Villavicencio (2006) it is argued that the semantic properties of verbs"
W06-2110,J93-2004,0,0.0273478,"Missing"
W06-2110,W03-1810,0,0.357159,"and/or semantic idiosyncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put th"
W06-2110,P04-1036,0,0.0526714,"Missing"
W06-2110,W03-0411,0,0.241644,"Missing"
W06-2110,W03-1808,0,0.329858,"the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order. In the case that the object is pronominal, however, the VPC must occur in split word order (c.f. *He handed in it) (Huddleston and Pullum, 2002; Villavicencio, 2003). The semantics of the VPC can either derive transparently from the semantics of the head verb and particle (e.g. walk off ) or be significantly removed from the semantics of the head verb and/or particle (e.g. look up); analogously, the selectional preferences of VPCs can mirror those of their head verbs or alternatively diverge markedly. The syntax of the VPC can also coincide with that of the head verb (e.g. walk off ) or alternatively diverge (e.g. lift off ). In the following, we review relevant past research on VPCs, focusing on the extraction/identification of VPCs and the prediction of"
W06-2110,W05-1006,0,0.0129273,"ncracies (Sag et al., 2002; Calzolari et al., 2002). In the case of English, MWEs are conventionally categorised syntacticosemantically into classes such as compound nominals (e.g. New York, apple juice, GM car), verb particle constructions (e.g. hand in, battle on), non-decomposable idioms (e.g. a piece of cake, kick the bucket) and light-verb constructions (e.g. make a mistake). MWE research has focussed largely on their implications in language understanding, fluency and robustness (Pearce, 2001; Sag et al., 2002; Copestake and Lascarides, 1997; Bannard et al., 2003; McCarthy et al., 2003; Widdows and Dorow, 2005). In this paper, our goal is to identify individual token instances of English verb particle constructions (VPCs hereafter) in running text. For the purposes of this paper, we follow Baldwin (2005) in adopting the simplifying assumption that VPCs: (a) consist of a head verb and a unique prepositional particle (e.g. hand in, walk off); and (b) are either transitive (e.g. hand in, put on) or intransitive (e.g. battle on). A defining characteristic of transitive VPCs is that they can generally occur with either joined (e.g. He put on the sweater) or split (e.g. He put the sweater on) word order."
W06-2605,J03-4003,0,0.0211176,"Missing"
W06-2605,C94-1042,0,0.0430237,"components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX’s existing lexicon to incorporate VerbNet and CoreLex. The lexicon is based on COMLEX (Grishman et al., 1994). Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic roles are mapped to the thematic roles, which are defined as variables in the ontology types. For more details on the parser see (Terenzi and Di Eugenio, 2003). Each of the 10,084 EDUs was parsed using the parser. The parser generates both a syntactic tree and the associated semantic representation – for the purpose of this paper, we only focus on the latter. Figure 2 shows"
W06-2605,J05-1004,0,0.0601932,"Missing"
W06-2605,J91-4003,0,0.0101383,"Lavie, 2000), a robust left-corner parser, with VerbNet (Kipper et al., 2000) and CoreLex (Buitelaar, 1998). Our interest in decompositional theories of lexical semantics led us to base our semantic representation on VerbNet. VerbNet operationalizes Levin’s work and accounts for 4962 distinct verbs classified into 237 main classes. Moreover, VerbNet’s strong syntactic components allow it to be easily coupled with a parser in order to automatically generate a semantically annotated corpus. To provide semantics for nouns, we use CoreLex (Buitelaar, 1998), in turn based on the generative lexicon(Pustejovsky, 1991). CoreLex defines basic types such as art (artifact) or com (communication). Nouns that share the same bundle of basic types are grouped in the same Systematic Polysemous Class (SPC). The resulting 126 SPCs cover about 40,000 nouns. We modified and augmented LCFLEX’s existing lexicon to incorporate VerbNet and CoreLex. The lexicon is based on COMLEX (Grishman et al., 1994). Verb and noun entries in the lexicon contain a link to a semantic type defined in the ontology. VerbNet classes (including subclasses and frames) and CoreLex SPCs are realized as types in the ontology. The deep syntactic ro"
W06-2605,N03-1030,0,0.268361,"ns. Also of relevance to the topic of this workshop, is that discourse structure is inherently highly structured, since discourse structure is generally described in hierarchical terms: basic units of analysis, generally clauses, are related by discourse relations, resulting in more complex units, which in turn can be related via discourse relations. At the moment, we do not yet address the problem of parsing at higher levels of discourse. We intend to build on the work we present in this paper to achieve that goal. The task of discourse parsing can be divided into two disjoint sub-problems ((Soricut and Marcu, 2003) and (Polanyi et al., 2004)). The two sub-problems are automatic identification of segment boundaries and the labeling of rhetorical relations. Though we consider the problem of automatic segmentation to be an important part in discourse parsing, we have focused entirely on the latter problem of automatically labeling rhetorical 33 Figure 1: SemDP System Architecture (Discourse Parser) relations only. Our approach uses rich verb semantics1 of elementary discourse units (EDUs)2 based on VerbNet(Kipper et al., 2000) as background knowledge and manually annotated rhetorical relations as training"
W06-2605,N03-2034,1,0.707027,"Missing"
W06-2605,P02-1047,0,0.13861,"Missing"
W06-2605,kingsbury-palmer-2002-treebank,0,\N,Missing
W06-2605,W01-1605,0,\N,Missing
W06-2605,W04-0211,0,\N,Missing
W09-2415,W04-3205,0,0.0639941,"unrelated semantic roles. There is a rudimentary frame hierarchy that defines mappings between roles of individual frames,5 but it is far from complete. The situation is similar in PropBank. PropBank does use a small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization. In contrast, all of the semantic relation inventories discussed in Section 1 contain fewer than 50 types of semantic relations. More generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (Chklovski and Pantel, 2004) and include relations that are not verbcentered (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Using the same labels for similar semantic relations facilitates supervised learning. For example, a model trained with examples of sell relations should be able to transfer what it has learned to give relations. This has the potential of adding 5 For example, it relates the B UYER role of the C OM frame (verb sell ) to the R ECIPIENT role of the G IVING frame (verb give). MERCE SELL 97 1. People in Hawaii might be feeling &lt;e1>aftershocks&lt;/e1> from that powerful &lt;e2>earthquake&lt;/e2> for weeks"
W09-2415,P08-1027,0,0.0924452,"tical NLP settings, where any relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, som"
W09-2415,J02-3001,0,0.0386032,"This is motivated by modelling considerations. Presumably, the data for OTHER will be very nonhomogeneous. By including it, we force any model of the complete data set to correctly identify the decision boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of mu"
W09-2415,S07-1003,1,0.384989,"Missing"
W09-2415,C92-2082,0,0.060649,"tion will take place in two rounds. In the first round, we will do a coarse-grained search for positive examples for each relation. We will collect data from the Web using a semi-automatic, pattern-based search procedure. In order to ensure a wide variety of example sentences, we will use several dozen patterns per relation. We will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the OTHER relation with realistic near-miss negative examples of the other relations. The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators. In order to maintain exclusivity of relations, only examples that are negative for all relations but R will be included as positive and only examples that are negative for all nine relations will be included as OTHER. Next, the annotators will compare their decisions and assess inter-annotator agreement. Consensus will be sought; if the annotators cannot agree on an example it will not be included in the data set, but it will be recorded for future analysis. Finally, two othe"
W09-2415,P08-2047,0,0.0143881,"relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, some subsequent publications tri"
W09-2415,I05-1082,1,0.187851,"nds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relat"
W09-2415,W04-2609,0,0.0615549,"fy noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes c"
W09-2415,P08-1052,1,0.611835,"medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is wh"
W09-2415,C08-1082,1,0.339838,"Missing"
W09-2415,J05-1004,0,0.0280101,"boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary. The second major difference is the syntactic context. Theories of semantic roles usually d"
W09-2415,P06-1015,1,0.178213,"m of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes clear that context does indeed play a role. Consider, for example, the noun compound wood shed : it may refer either to a shed made of wood, or to a shed of any material used to store wood. This ambiguity is likely to be resolved in particular contexts. In fact, most NLP appli"
W09-2415,D07-1075,0,0.0368294,"annotation, we define a nominal as a noun or a base noun phrase. A base noun phrase is a noun and its pre-modifiers (e.g., nouns, adjectives, determiners). We do not include complex noun phrases (e.g., noun phrases with attached prepositional phrases or relative clauses). For example, lawn is a noun, lawn mower is a base noun phrase, and the engine of the lawn mower is a complex noun phrase. We focus on heads that are common nouns. This emphasis distinguishes our task from much work in IE, which focuses on named entities and on considerably more fine-grained relations than we do. For example, Patwardhan and Riloff (2007) identify categories like Terrorist organization as participants in terror-related semantic relations, which consists predominantly of named entities. We feel that named entities are a specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization. Figure 1 shows two examples of annotated sentences. The XML tags &lt;e1> and &lt;e2> mark the target nominals. Since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into acco"
W09-2415,W01-0511,0,0.0250487,"CL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one"
W09-2415,P02-1032,0,0.00907258,"stics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and dat"
W09-2415,W09-1401,0,\N,Missing
W09-2415,J02-3004,0,\N,Missing
W09-2415,S10-1006,1,\N,Missing
W09-2415,W04-2412,0,\N,Missing
W09-2416,W04-0404,0,0.485888,"Missing"
W09-2416,C08-1011,1,0.300292,"he problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability distribution over a range of candidates. For example, highly probable paraphrases for chocolate bar are bar made of chocolate and bar that tastes like chocolate, while bar that eats chocolate is very unlikely. As described in Section 3.3, a set of goldstandard paraphrase distributions can be constructed by collating responses from a large number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we"
W09-2416,P07-1072,0,0.421183,"Missing"
W09-2416,P84-1109,0,0.450496,"araphrasing models, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a gi"
W09-2416,W96-0309,0,0.0699356,"ls, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability d"
W09-2416,P06-2064,1,0.371927,"rge number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for detai"
W09-2416,E03-1073,0,0.0568415,"Missing"
W09-2416,P08-1052,1,0.883976,"jects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for details.3 The most popular par"
W09-2416,W07-1108,1,0.886065,"Missing"
W09-2416,W01-0511,0,0.105028,"ONSTITUTE into SOURCE-RESULT, RESULT-SOURCE and COPULA; COPULA is then further subdivided at two additional levels. 101 In computational linguistics, popular inventories of semantic relations have been proposed by Nastase and Szpakowicz (2003) and Girju et al. (2005), among others. The former groups 30 finegrained relations into five coarse-grained supercategories, while the latter is a flat list of 21 relations. Both schemes are intended to be suitable for broad-coverage analysis of text. For specialized applications, however, it is often useful to use domain-specific relations. For example, Rosario and Hearst (2001) propose 18 abstract relations for interpreting NCs in biomedical text, e.g., DEFECT, MATERIAL, PERSON AFFILIATED, ATTRIBUTE OF CLINICAL STUDY. Inventory-based analyses offer significant advantages. Abstract relations such as ‘location’ and ‘possession’ capture valuable generalizations about NC semantics in a parsimonious framework. Unlike paraphrase-based analyses (Section 2.2), they are not tied to specific lexical items, which may themselves be semantically ambiguous. They also lend themselves particularly well to automatic interpretation methods based on multi-class classification. On the"
W09-2416,D08-1027,0,0.0137166,"Missing"
W09-2416,W03-1803,0,0.0693929,"Missing"
W09-2902,P06-1068,0,0.186446,"Missing"
W09-2902,C02-1142,0,0.0679156,"Missing"
W09-2902,C08-2021,0,0.0639913,"Missing"
W09-2902,W97-0703,0,0.286606,"Missing"
W09-2902,C08-1122,0,0.0506999,"erve as a representative summary of the document and also serve as high quality index terms. It is thus no surprise that keyphrases have been utilized to acquire critical information as well as to improve the quality of natural language processing (NLP) applications such as document summarizer(D´avanzo and Magnini, 2005), information retrieval (IR)(Gutwin et al., 1999) and document clustering(Hammouda et al., 2005). In the past, various attempts have been made to boost automatic keyphrase extraction performance based primarily on statistics(Frank et al., 1999; Turney, 2003; Park et al., 2004; Wan and Xiao, 2008) and a rich set of heuristic features(Barker and Corrnacchia, 2000; Medelyan and Witten, 9 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9–16, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP Textract (Park et al., 2004) also ranks the candidate keyphrases by its judgment of keyphrases’ degree of domain specificity based on subjectspecific collocations(Damerau, 1993), in addition to term cohesion using Dice coefficient(Dice, 1945). Recently, Wan and Xiao (2008) extracts automatic keyphrases from single documents, utilizing document clustering informati"
W09-2902,P89-1010,0,0.175281,"Missing"
W09-2902,councill-etal-2008-parscit,1,0.793074,"rget feature. + indicates an improvement, - indicates a performance decline, and ? indicates no effect or unconfirmed due to small changes of performances. Again, supervised denotes Maximum Entropy training and Unsupervised is our unsupervised approach. To assess the performance of the proposed candidate selection rules and features, we implemented a keyphrase extraction pipe line. We start with raw text of computer science articles converted from PDF by pdftotext. Then, we partitioned the into section such as title and sections via heuristic rules and applied sentence segmenter 2 , ParsCit3 (Councill et al., 2008) for reference collection, part-of-speech tagger4 and lemmatizer5 (Minnen et al., 2001) of the input. After preprocessing, we built both supervised and unsupervised classifiers using Naive Bayes from the WEKA machine learning toolkit(Witten and Frank, 2005), Maximum Entropy6 , and simple weighting. In evaluation, we collected 250 papers from four different categories7 of the ACM digital library. Each paper was 6 to 8 pages on average. In author assigned keyphrase, we found many were missing or found as substrings. To remedy this, we collected reader assigned keyphrase by hiring senior year und"
W09-2905,W02-2001,0,0.108882,"Missing"
W09-2905,P01-1025,0,0.0384995,"annotated in (Baldwin, 2005) and 464 annotated LVC candidates used in (Tan et al., 2006). Both sets of annotations give both positive and negative examples. Our final VPC and LVC evaluation datasets were then constructed by intersecting the goldstandard datasets with our corresponding sets of extracted candidates. We also concatenated both sets of evaluation data for composite evaluation. This set is referred to as “Mixed”. Statistics of our three evaluation datasets are summarized in Table 2. VPC data LVC data Total (freq ≥ 6) Positive instances (the n-best method). However, as discussed in Evert and Krenn (2001), this method depends heavily on the choice of n. In this paper, we opt for average precision (AP), which is the average of precisions at all possible recall values. This choice also makes our results comparable to those of Pecina and Schlesinger (2006). 3.3 Figure 1(a, b) gives the two average precision profiles of the 82 AMs presented in Pecina and Schlesinger (2006) when we replicated their experiments over our English VPC and LVC datasets. We observe that the average precision profile for VPCs is slightly concave while the one for LVCs is more convex. This can be interpreted as VPCs being"
W09-2905,W06-1203,0,0.067321,"Missing"
W09-2905,pearce-2002-comparative,0,0.561572,"Missing"
W09-2905,P06-2084,0,0.597394,"onal University of Singapore hoanghuu@comp.nus.edu.sg snkim@csse.unimelb.edu.au kanmy@comp.nus.edu.sg ratio and the T score. In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated. Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher’s exact tests, various coefficients such as Dice and Jaccard. Later, Ramisch et al. (2008) evaluated MI, 2 Pearson’s χ and Permutation Entropy. Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assembled and evaluated over Czech collocations. These collocations contained a mix of idiomatic expressions, technical terms, light verb constructions and stock phrases. In their work, the best combination of AMs was selected using machine learning. While the previous works have evaluated AMs, there have been few details on why the AMs perform as they do. A detailed analysis of why these AMs perform as they do is needed in order to explain their identification performance, and to help us recommend AMs for future tasks. This weakness of previous works motivated us to addres"
W09-2905,J93-1007,0,0.18954,". We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process. For VPCs, we first identify particles using a pre-compiled set of 38 particles based on Baldwin (2005) and Quirk et al. (1985) (Appendix A). Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in Baldwin (2005). Next, we search to the left of the located particle for the nearest verb. As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to Baldwin and Villavicencio (2002) and Smadja (1993), since longer NPs tend to be located after particles. 32 AM Name M1. Joint Probability Formula AM Name M2. Mutual Information f ( xy ) / N Formula 1 ∑f N log ij i, j M3. Log likelihood ratio M5. Local-PMI M7. PMI 2∑ i, j M4. Pointwise MI (PMI) fij fij log fˆ ij M6. PMI f ( xy ) × PMI 2 log Nf ( xy ) M8. Mutual Dependency 2 M10. Normalized expectation a ( a + b)( a + c ) M12. First Kulczynski a a+b+c M13. Second Sokal-Sneath M17. Hamann M19. Yule’s ω a + 2(b + c ) M16. Rogers-Tanimoto a+d a+b+c+d ( a + d ) − (b + c ) M18. Odds ratio a+b+c+d M20. Yule’s Q ad − bc ad + bc M21. BrawnBlanquet M23."
W09-2905,W06-2407,1,0.816489,"Missing"
W10-0508,P95-1005,0,0.516276,"Missing"
W10-0508,J05-2005,0,0.0605956,"ce IR effectiveness. 3 Conclusions This paper provides an outline of the ILIAD project, focusing on the tasks of crawling, thread-level analysis, post-level analysis, user-level analysis and IR reranking. We have designed a series of class sets for the component tasks, and carried out experimentation over a range of data sources, achieving encouraging results. 2.3 Post-level analysis Acknowledgements We automatically analyse the post-to-post discourse structure of each thread, in terms of which (preceding) post(s) each post relates to, and how, building off the work of Ros´e et al. (1995) and Wolf and Gibson (2005). For example, a given post may refute the solution proposed in an earlier post, and also propose a novel solution in response to the initiating post. Separately, we are developing techniques for identifying whether a new post to a given forum is sufficiently similar to other (ideally resolved) threads that the author should be prompted to first check the existing threads for redundancy before a new thread is initiated. Our experiments on post-level analysis are, once again, based on data from LinuxQuestions and CNET. NICTA is funded by the Australian Government as represented by the Departmen"
W10-2923,W10-0508,1,0.824462,"Missing"
W10-2923,P08-1095,0,0.032955,"s for post dependency linking and labelling, which achieve strong results for the respective tasks. In this work, we draw on existing work (esp. Xi et al. (2004)) in proposing a novel DA tag set customised to the analysis of troubleshootingoriented web user forums (Section 3), and compare a range of text classification and structured classification methods for post-level DA classification. Discourse disentanglement is the process of automatically identifying coherent sub-discourses in a single thread (in the context of user forums/mailing lists), chat session (in the context of IRC chat data: Elsner and Charniak (2008)), system interaction (in the context of HCI: Lemon et al. (2002)) or document (Wolf and Gibson, 2005). The exact definition of what constitutes a subdiscourse varies across domains, but for our purposes, entails an attempt to resolve the informa2 Related Work Related work exists in the broad fields of dialogue processing, discourse analysis and information retrieval, and can be broken down into the following tasks: (1) dialogue act tagging; (2) discourse “disentanglement”; (3) community question answering; and (4) newsgroup/user forum search. 193 ority of this method over a model which ignore"
W10-2923,N09-1035,0,0.0122661,"indicator of which post a given post responds (links) to, and can aid in DA tagging. We use simple cosine similarity to find the post with the most-similar title, and represent its relative location to the current post. We built machine learners using a conventional Maximum Entropy (ME) learner,2 as well as two structural learners, namely: (1) SVM-HMMs (Joachims et al., 2009), as implemented in SVMstruct3 , with a linear kernel; and (2) conditional random fields (CRFs) using CRF++.4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006). Both are discriminative models which capture structural dependencies, which is highly desirable in terms of modelling sequential preferences between post labels (e.g. A-C ONF typically following a A-A). SVMHMM has the additional advantage of scaling to large numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is c"
W10-2923,J86-3001,0,0.188004,"structure was superior to that using a monolithic document representation. The observations and results of Xi et al. (2004) and Seo et al. (2009) that threading information (or in our case “disentangled” DAG structure) enhances IR effectiveness is a core motivator for this research. tion need of the initiator by a particular approach; if there are competing approaches proposed in a single thread, multiple sub-discourses will necessarily arise. The data structure used to represent the disentangled discourse varies from a simple connected sub-graph (Elsner and Charniak, 2008), to a stack/tree (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), to a full directed acyclic graph (DAG: Ros´e et al. (1995), Wolf and Gibson (2005), Schuth et al. (2007)). Disentanglement has been carried out via analysis of direct citation/user name references (Schuth et al., 2007; Seo et al., 2009), topic modelling (Lin et al., 2009), and clustering over content-based features for pairs of posts, optionally incorporating various constraints on post recency (Elsner and Charniak, 2008; Wang et al., 2008; Seo et al., 2009). 3 Post Label Set Our post label set contains 12 categories, intended to capture the typical int"
W10-2923,W01-1605,0,0.0604213,"is task of enhanced support sharing, in the form of text mining over troubleshootingoriented web user forum data (Baldwin et al., to appear). One facet of our proposed strategy for enhancing information access to troubleshooting-oriented web user forum data is to preprocess threads to uncover the “content structure” of the thread, in the form of its post-to-post discourse structure. Specifically, we identify which earlier post(s) a given post responds to (linking) and in what manner (tagging), in an amalgam of dialogue act tagging (Stolcke et al., 2000) and coherence-based discourse analysis (Carlson et al., 2001; Wolf and Gibson, 2005). The reason we do this is gauge the relative role/import of individual posts, to index and weight component terms accordingly, ultimately in an attempt to enhance information access. Evidence to suggest that this structure can enhance information retrieval effectiveness comes from Xi et al. (2004) and Seo et al. (2009) (see Section 2). To illustrate the task, consider the thread from the CNET forum shown in Figure 1, made up of 5 posts (Post 1, ..., Post 5) with 4 distinct participants (A, B, C, D). In the first post, A initiates the thread by requesting assistance in"
W10-2923,W04-3240,0,0.540286,"Missing"
W10-2923,W02-0216,0,0.171406,"s for the respective tasks. In this work, we draw on existing work (esp. Xi et al. (2004)) in proposing a novel DA tag set customised to the analysis of troubleshootingoriented web user forums (Section 3), and compare a range of text classification and structured classification methods for post-level DA classification. Discourse disentanglement is the process of automatically identifying coherent sub-discourses in a single thread (in the context of user forums/mailing lists), chat session (in the context of IRC chat data: Elsner and Charniak (2008)), system interaction (in the context of HCI: Lemon et al. (2002)) or document (Wolf and Gibson, 2005). The exact definition of what constitutes a subdiscourse varies across domains, but for our purposes, entails an attempt to resolve the informa2 Related Work Related work exists in the broad fields of dialogue processing, discourse analysis and information retrieval, and can be broken down into the following tasks: (1) dialogue act tagging; (2) discourse “disentanglement”; (3) community question answering; and (4) newsgroup/user forum search. 193 ority of this method over a model which ignores thread structure. Finally, Seo et al. (2009) automatically deri"
W10-2923,P08-1081,0,0.0291932,"05) in adopting a DAG representation of discourse structure, and draw on the wide set of features used in discourse entanglement to model coherence. Community question answering (cQA) is the task of identifying question–answer pairs in a given thread, e.g. for the purposes of thread summarisation (Shrestha and McKeown, 2004) or automated compilation of resources akin to Yahoo! Answers. cQA has been applied to both mailing list and user forum threads, conventionally based on question classification, followed by ranking of candidate answers relative to each question (Shrestha and McKeown, 2004; Ding et al., 2008; Cong et al., 2008; Cao et al., 2009). The task is somewhat peripheral to our work, but relevant in that it involves the implicit tagging of certain posts as containing questions/answers, as well as linking the posts together. Once again, we draw on the features used in cQA in this research. There has been a spike of recent interest in newsgroup/user forum search. Xi et al. (2004) proposed a structured information retrieval (IR) model for newsgroup search, based on author features, thread structure (based on the tree defined by the reply-to structure), thread “topology” features and content-b"
W10-2923,N06-1047,0,0.0138883,"ser A Thank You! Post 4 Thanks a lot for that . . . I have Microsoft Visual Studio 6, what program should I do this in? Lastly, how do I actually include this in my site?. . . User D A little more help Post 5 . . . You would simply do it this way: . . . You could also just . . . An example of this is:. . . Figure 1: Snippeted posts in a CNET thread 53EBF2753EBF2 1234567123456 123457 123456 Dialogue act (DA) tagging is a means of capturing the function of a given utterance relative to an encompassing discourse, and has been proposed variously as a means of enhancing dialogue summarisation (Murray et al., 2006), and tracking commitments and promises in email (Cohen et al., 2004; Lampert et al., 2008), as well as being shown to improve speech recognition accuracy (Stolcke et al., 2000). A wide range of DA tag sets have been proposed, usually customised to a particular medium such as speech dialogue (Stolcke et al., 2000; Shriberg et al., 2004), taskfocused email (Cohen et al., 2004; Wang et al., 2007; Lampert et al., 2008) or instant messaging (Ivanovic, 2008). The most immediately relevant DA-based work we are aware of is that of Xi et al. (2004), who proposed a 5-way classification for newsgroup da"
W10-2923,J00-3003,0,0.486125,"Missing"
W10-2923,P07-2019,0,0.094754,"information (or in our case “disentangled” DAG structure) enhances IR effectiveness is a core motivator for this research. tion need of the initiator by a particular approach; if there are competing approaches proposed in a single thread, multiple sub-discourses will necessarily arise. The data structure used to represent the disentangled discourse varies from a simple connected sub-graph (Elsner and Charniak, 2008), to a stack/tree (Grosz and Sidner, 1986; Lemon et al., 2002; Seo et al., 2009), to a full directed acyclic graph (DAG: Ros´e et al. (1995), Wolf and Gibson (2005), Schuth et al. (2007)). Disentanglement has been carried out via analysis of direct citation/user name references (Schuth et al., 2007; Seo et al., 2009), topic modelling (Lin et al., 2009), and clustering over content-based features for pairs of posts, optionally incorporating various constraints on post recency (Elsner and Charniak, 2008; Wang et al., 2008; Seo et al., 2009). 3 Post Label Set Our post label set contains 12 categories, intended to capture the typical interactions that take place in troubleshooting-oriented threads on technical forums. There are 2 super-categories (Q UESTION, A NSWER) and 3 single"
W10-2923,P95-1005,0,0.167737,"Missing"
W10-2923,P98-2188,0,0.0229291,"Missing"
W10-2923,P07-2032,0,0.0226497,"Missing"
W10-2923,J05-2005,0,0.34868,"pport sharing, in the form of text mining over troubleshootingoriented web user forum data (Baldwin et al., to appear). One facet of our proposed strategy for enhancing information access to troubleshooting-oriented web user forum data is to preprocess threads to uncover the “content structure” of the thread, in the form of its post-to-post discourse structure. Specifically, we identify which earlier post(s) a given post responds to (linking) and in what manner (tagging), in an amalgam of dialogue act tagging (Stolcke et al., 2000) and coherence-based discourse analysis (Carlson et al., 2001; Wolf and Gibson, 2005). The reason we do this is gauge the relative role/import of individual posts, to index and weight component terms accordingly, ultimately in an attempt to enhance information access. Evidence to suggest that this structure can enhance information retrieval effectiveness comes from Xi et al. (2004) and Seo et al. (2009) (see Section 2). To illustrate the task, consider the thread from the CNET forum shown in Figure 1, made up of 5 posts (Post 1, ..., Post 5) with 4 distinct participants (A, B, C, D). In the first post, A initiates the thread by requesting assistance in creating a web form. In"
W10-2923,P04-1088,0,0.0273801,"Missing"
W10-2923,C00-2137,0,0.0155274,"arge numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is contained in the test data for a given iteration, all other posts in that same thread are also in the test data (or more pertinently, not in the training data). We evaluate using micro-averaged precision, recall and Fscore (β = 1). We test the statistical significance of all above-baseline results using randomised estimation (p &lt; 0.05; Yeh (2000)), and present all such results in bold in our results tables. In our experiments, we first look at the post classification task in isolation (i.e. we predict which labels to associate with each post, underspecifying which posts those labels relate to). We then move on to look at the link classification task, again in isolation (i.e. we predict which previous posts each post links to, underspecifying the nature of the link). Finally, we perform preliminary investigation of the joint task of DA and link classification, by incorporating DA class features into the link classification task. Post S"
W10-2923,N03-1028,0,0.0134673,"sponds (links) to, and can aid in DA tagging. We use simple cosine similarity to find the post with the most-similar title, and represent its relative location to the current post. We built machine learners using a conventional Maximum Entropy (ME) learner,2 as well as two structural learners, namely: (1) SVM-HMMs (Joachims et al., 2009), as implemented in SVMstruct3 , with a linear kernel; and (2) conditional random fields (CRFs) using CRF++.4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006). Both are discriminative models which capture structural dependencies, which is highly desirable in terms of modelling sequential preferences between post labels (e.g. A-C ONF typically following a A-A). SVMHMM has the additional advantage of scaling to large numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is contained in the test data for a given i"
W10-2923,W06-0127,0,0.0154486,"We use simple cosine similarity to find the post with the most-similar title, and represent its relative location to the current post. We built machine learners using a conventional Maximum Entropy (ME) learner,2 as well as two structural learners, namely: (1) SVM-HMMs (Joachims et al., 2009), as implemented in SVMstruct3 , with a linear kernel; and (2) conditional random fields (CRFs) using CRF++.4 SVMHMMs and CRFs have been successfully applied to a range of sequential tagging tasks such as syllabification (Bartlett et al., 2009), chunk parsing (Sha and Pereira, 2003) and word segmentation (Zhao et al., 2006). Both are discriminative models which capture structural dependencies, which is highly desirable in terms of modelling sequential preferences between post labels (e.g. A-C ONF typically following a A-A). SVMHMM has the additional advantage of scaling to large numbers of features (namely the lexical features). As such, we only experiment with lexical features for SVM-HMM and ME. All of our evaluation is based on stratified 10fold cross-validation, stratifying at the thread level to ensure that if a given post is contained in the test data for a given iteration, all other posts in that same thr"
W10-2923,C04-1128,0,0.042657,"post C should exist only in the case that the link between them is not inferrable transitively. Detailed definitions of each post tag are given below. Note that initiator refers to the user who started the thread with the first post. In this work, we follow Ros´e et al. (1995) and Wolf and Gibson (2005) in adopting a DAG representation of discourse structure, and draw on the wide set of features used in discourse entanglement to model coherence. Community question answering (cQA) is the task of identifying question–answer pairs in a given thread, e.g. for the purposes of thread summarisation (Shrestha and McKeown, 2004) or automated compilation of resources akin to Yahoo! Answers. cQA has been applied to both mailing list and user forum threads, conventionally based on question classification, followed by ranking of candidate answers relative to each question (Shrestha and McKeown, 2004; Ding et al., 2008; Cong et al., 2008; Cao et al., 2009). The task is somewhat peripheral to our work, but relevant in that it involves the implicit tagging of certain posts as containing questions/answers, as well as linking the posts together. Once again, we draw on the features used in cQA in this research. There has been"
W10-2923,W04-2319,0,\N,Missing
W10-2923,C98-2183,0,\N,Missing
Y12-1021,W03-1028,0,0.942919,"in the form of either simplex nouns (e.g. library) or noun phrases (e.g. social issue). They have been studied in the past to provide topic-related information for many applications such as text summarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summa´ rizers. DAvanzo and Magnini (2005) used them to 1 In this work, we use the term keywords for consistency, while noting that it can be used to refer to multiword terms. 199 There has been much research on automatic keyword extraction (Frank et al., 1999; Turney, 1999; Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction (Kim et al., 2010b). A small minority of researchers have used different sources of data such as email (Dredze et al., 2008) and HTML documents (Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure). In this paper, our aim is to automat"
Y12-1021,W98-0319,0,0.141465,"Congress. The live chats contain 33 online discussions that the Library’s Educational Outreach team hosted for teachers between 2002 and 2006. To define dialogue acts that suit this data, we investigated existing sets of dialogue acts from both spoken dialogues and live chats. Many can be found in both spoken and written dialogues based on the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997). For live chats, Wu et al. (2002) and Forsyth (2007) defined 15 dialogue acts for casual online conversations based on previous sets (Samuel et al., 1998; Shriberg et al., 1998; Jurafsky et al., 1998; Stolcke et al., 2000). Ivanovic (2008) proposed 12 dialogue acts applying DAMSL for customer service chats. Given the fact that our live chat forum data is closer to customer service chats in terms of the nature of the data (e.g. question, request, gratitude etc.), we decided to adopt the set from Ivanovic 201 (2008) and added two more dialogue acts – BACKGROUND and OTHER. The list of dialogue acts and examples can be found in Table 1. We selected 15 forums containing at least 200 utterances. The data was first segmented into discourse units, and sentence tokenized. Then, we cleaned the data"
Y12-1021,D10-1084,1,0.745951,"ummarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summa´ rizers. DAvanzo and Magnini (2005) used them to 1 In this work, we use the term keywords for consistency, while noting that it can be used to refer to multiword terms. 199 There has been much research on automatic keyword extraction (Frank et al., 1999; Turney, 1999; Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction (Kim et al., 2010b). A small minority of researchers have used different sources of data such as email (Dredze et al., 2008) and HTML documents (Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure). In this paper, our aim is to automatically extract keywords for multi-party live chats. Live chats are essentially text-based dialogues, with less disfluencies than spoken dialogues but greater scope for overlapping utterances and out-of"
Y12-1021,S10-1004,1,0.815205,"ummarizers, search engines and indexers. For example, Barzilay and Elhadad (1997) used keywords as semantic meta-information for summa´ rizers. DAvanzo and Magnini (2005) used them to 1 In this work, we use the term keywords for consistency, while noting that it can be used to refer to multiword terms. 199 There has been much research on automatic keyword extraction (Frank et al., 1999; Turney, 1999; Hulth, 2003, inter alia). The majority of work has been done over specific domains such as scientific articles and newspapers, including the recent SemEval-2010 shared task on keyword extraction (Kim et al., 2010b). A small minority of researchers have used different sources of data such as email (Dredze et al., 2008) and HTML documents (Mori et al., 2004), as outlined in Section 2. However, existing approaches tend not to work well when applied to different target sources, and are often susceptible to domain-specific features of the target documents (e.g. structure). In this paper, our aim is to automatically extract keywords for multi-party live chats. Live chats are essentially text-based dialogues, with less disfluencies than spoken dialogues but greater scope for overlapping utterances and out-of"
Y12-1021,P10-2055,0,0.0297937,"ion that POS patterns such as (NN NN) and (JJ NN) are more frequent among keywords. Nguyen and Kan (2007) extracted keywords using structural information such as the document title and section headings derived from scientific articles. Wan and Xiao (2008) used a document clustering method to extract salient words, then utilized those to rank the candidates. Liu et al. (2009) developed an unsupervised method using TF·IDF and variants thereof. The main approach is to cluster the terms with respect to the sub-topics, rank candidates in each cluster, then select top-ranked candidates as keywords. Li et al. (2010) proposed a method based on semantic similarity among n-ary phrases, based on Wikipedia entities and links, and used the weighted GirvanNewman algorithm for candidate ranking. More recently, Kim et al. (2010b) proposed a keyword extraction shared task over scientific articles. Participants used a broad range of features based on document structure, semantic similarity and various document and term heuristics. Keyword extraction has also been carried out on various types of documents. Scientific articles and news articles are often the target of keyword extraction (Hulth, 2003; Nguyen and Kan,"
Y12-1021,N09-1070,0,0.208282,"Missing"
Y12-1021,P98-2188,0,0.13797,"Missing"
Y12-1021,J00-3003,0,0.291157,"Missing"
Y12-1021,C08-1122,0,0.64964,"ent. Hereafter, we will refer to this term as first appearance. The GenEx system (Turney, 1999) employed nine heuristic features based exclusively on morphosyntax, such as word length and phrase frequency. Hulth (2003) used TF·IDF, first appearance and keyphraseness2 as the basis of his method, and added POS tags assigned to candidate terms based on the observation that POS patterns such as (NN NN) and (JJ NN) are more frequent among keywords. Nguyen and Kan (2007) extracted keywords using structural information such as the document title and section headings derived from scientific articles. Wan and Xiao (2008) used a document clustering method to extract salient words, then utilized those to rank the candidates. Liu et al. (2009) developed an unsupervised method using TF·IDF and variants thereof. The main approach is to cluster the terms with respect to the sub-topics, rank candidates in each cluster, then select top-ranked candidates as keywords. Li et al. (2010) proposed a method based on semantic similarity among n-ary phrases, based on Wikipedia entities and links, and used the weighted GirvanNewman algorithm for candidate ranking. More recently, Kim et al. (2010b) proposed a keyword extraction"
Y12-1021,W97-0703,0,\N,Missing
Y12-1021,C98-2183,0,\N,Missing
Y12-1050,P06-1026,0,0.29257,"Missing"
Y12-1050,W04-3240,0,0.672503,"Missing"
Y12-1050,P08-1095,0,0.0280354,"th maximum TF. 3.4 percentage a dialogue act Interaction among Utterances Finally, we investigated the interaction between features proposed in Bangalore et al. (2006) and Kim et al. (2010a). Bangalore et al. (2006) used sentences to provide dialogue act information of previous utterances over spoken dialogues; Kim et al. (2010a) used predicted dialogue acts directly. A major point of difference for us is that our data contains multiple participants; thus, the interactions among utterances tend to be more indirect. Moreover, due to difficulties in utterance disentanglement similarily shown in Elsner and Charniak (2008)), we expect reduced effectiveness over our data of such information (although some degree of interaction exists). However, to partly help with disentanglement, we noticed that some users mentioned the user name(s) of the users they are responding to in their posts, which allows us to identify the utterances they link to. Based on these observations, we tested the five interaction features listed below: • Prev1, Prev2, Prev3: dialogue act(s) or sentence(s) from 1 ∼ 3 previous utterances; • User: a dialogue act or sentence from 1 previous utterance in which the user is the same as the author in"
Y12-1050,W98-0319,0,0.150135,"section describes the data and dialogue act categories in detail. 2.1 Dataset 1: Live Forum Chats tween 2002 and 2006. To define dialogue acts suitable for this data, we investigated existing sets of dialogue acts from both spoken dialogues and live chats. Many have been based on the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997), initially applied to the TRAIN corpus of transcribed spoken task-oriented dialogues. In live chats, Wu et al. (2002) and Forsyth (2007) defined 15 dialogue acts for casual online conversations based on previous sets (Samuel et al., 1998; Jurafsky et al., 1998; Stolcke et al., 2000) and characteristics of conversations. Ivanovic (2008) proposed 12 dialogue acts applying DAMSL to customer service chats. We found that forum chats are not dissimilar to customer service chats in terms of the nature of conversations (e.g. question, request, thanking, etc.), and so decided to adopt the DA set defined by Ivanovic (2008). To the 12 dialogue acts from Ivanovic (2008), we added two further dialogue acts — BACKGROUND and OTHER. BACKGROUND is designed to cover contributions containing information about the participants themselves, which often occurs before dis"
Y12-1050,N09-1072,0,0.0138524,"ialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have distinct features which make it difficult to reuse existing methods for live chats. For example, spoken dialogue introduces difficulties due to errors"
Y12-1050,D10-1084,1,0.888648,"g online forums from the USA Library of Congress. We found that, for multi-party dialogues, features based on 1-gram and keywords produced best performance, while features exploiting structure and interaction did not perform as well as previously reported results over 1-to-1 chats. 1 Introduction Dialogue Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has rece"
Y12-1050,W10-2923,1,0.644224,"g online forums from the USA Library of Congress. We found that, for multi-party dialogues, features based on 1-gram and keywords produced best performance, while features exploiting structure and interaction did not perform as well as previously reported results over 1-to-1 chats. 1 Introduction Dialogue Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has rece"
Y12-1050,N06-1047,0,0.0320268,"e Acts (or DAs) are discourse units (or utterances) that represent the semantics of contributions to a dialogue at the level of illocutionary force. Dialogue acts have been studied in various types of conversations — spoken/written dialogue contributions (Stolcke et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have"
Y12-1050,P98-2188,0,0.357409,"he remainder of this section describes the data and dialogue act categories in detail. 2.1 Dataset 1: Live Forum Chats tween 2002 and 2006. To define dialogue acts suitable for this data, we investigated existing sets of dialogue acts from both spoken dialogues and live chats. Many have been based on the Dialogue Act Markup in Several Layers (DAMSL) scheme (Allen and Core, 1997), initially applied to the TRAIN corpus of transcribed spoken task-oriented dialogues. In live chats, Wu et al. (2002) and Forsyth (2007) defined 15 dialogue acts for casual online conversations based on previous sets (Samuel et al., 1998; Jurafsky et al., 1998; Stolcke et al., 2000) and characteristics of conversations. Ivanovic (2008) proposed 12 dialogue acts applying DAMSL to customer service chats. We found that forum chats are not dissimilar to customer service chats in terms of the nature of conversations (e.g. question, request, thanking, etc.), and so decided to adopt the DA set defined by Ivanovic (2008). To the 12 dialogue acts from Ivanovic (2008), we added two further dialogue acts — BACKGROUND and OTHER. BACKGROUND is designed to cover contributions containing information about the participants themselves, which"
Y12-1050,J00-3003,0,0.357584,"Missing"
Y12-1050,D11-1002,1,0.846245,"et al., 2000; Wu et al., 2002; Kim et al., 2010a), sentence-level (Lampert et al., 2008), paragraph-level (Cong et al., 2008), or complete messages consisting of several paragraphs (Cohen et al., 2004). Authors have argued that automatic dialogue act identification could help in a range of applications, such as meeting summarisation (Murray et al., 2006), email summarisation, conversational agents, speech recognition (Stolcke et al., 2000), 463 or human social intention detection (Jurafsky et al., 2009). They can also be useful in informationsharing chats in online forums (Kim et al., 2010b; Wang et al., 2011). Recently, live chat has received growing attention since chat services and similar applications have gained popularity as a communication method. However, the majority of previous work on dialogue act classification for dialogue has been carried out over spoken dialogue. Although spoken and written dialogue have similarities, they have distinct features which make it difficult to reuse existing methods for live chats. For example, spoken dialogue introduces difficulties due to errors inherent in speech recognition output, but allows acoustic and prosodic features to be leveraged (e.g. Stolck"
Y12-1050,C00-2137,0,0.0120644,"ine are bold-faced. MENT) caused confusion. Tables 7 and 8 show the the performance of each label produced by stemmed unigram, keywords, TextUserL features. We observed that some dialogue acts, such as EXPRESSION, OPENING, THANKING, are relatively easy to detect; others, such as NO-ANSWER, REQUEST, RESPONSE-ACK are hard to predict accurately. We also noticed that the lower recall produced the lower F-score for those dialogue acts which are hard to detect. Finally, we conducted randomized estimation to calculate whether any performance differences between methods are statistically significant (Yeh, 2000). We found that the keyword features led to statistically significant improvements over the base470 line system (p < 0.05). 6 Conclusion We have investigated the task of classifying dialogue acts in multi-party chats, and proposed features to automatically classify dialogue acts based on context, structure, keyword, and interactions among utterances. We found that the system using contextual and keyword features performed the best. Further, we have shown that features from structure and interactions did not perform well, unlike their effectiveness over 1-on-1 live chats in Kim et al. (2010a)."
Y12-1050,C98-2183,0,\N,Missing
