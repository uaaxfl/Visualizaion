2021.semeval-1.63,{HITSZ}-{HLT} at {S}em{E}val-2021 Task 5: Ensemble Sequence Labeling and Span Boundary Detection for Toxic Span Detection,2021,-1,-1,5,0,1809,qinglin zhu,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"This paper presents the winning system that participated in SemEval-2021 Task 5: Toxic Spans Detection. This task aims to locate those spans that attribute to the text{'}s toxicity within a text, which is crucial for semi-automated moderation in online discussions. We formalize this task as the Sequence Labeling (SL) problem and the Span Boundary Detection (SBD) problem separately and employ three state-of-the-art models. Next, we integrate predictions of these models to produce a more credible and complement result. Our system achieves a char-level score of 70.83{\%}, ranking 1/91. In addition, we also explore the lexicon-based method, which is strongly interpretable and flexible in practice."
2021.repl4nlp-1.28,Box-To-Box Transformations for Modeling Joint Hierarchies,2021,-1,-1,2,0,2527,shib dasgupta,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"Learning representations of entities and relations in structured knowledge bases is an active area of research, with much emphasis placed on choosing the appropriate geometry to capture the hierarchical structures exploited in, for example, isa or haspart relations. Box embeddings (Vilnis et al., 2018; Li et al., 2019; Dasgupta et al., 2020), which represent concepts as n-dimensional hyperrectangles, are capable of embedding hierarchies when training on a subset of the transitive closure. In Patel et al., (2020), the authors demonstrate that only the transitive reduction is required and further extend box embeddings to capture joint hierarchies by augmenting the graph with new nodes. While it is possible to represent joint hierarchies with this method, the parameters for each hierarchy are decoupled, making generalization between hierarchies infeasible. In this work, we introduce a learned box-to-box transformation that respects the structure of each hierarchy. We demonstrate that this not only improves the capability of modeling cross-hierarchy compositional edges but is also capable of generalizing from a subset of the transitive reduction."
2021.naacl-main.68,Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning,2021,-1,-1,5,0,3438,xuelu chen,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however, existing embedding methods only model triple-level uncertainty, and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts."
2021.naacl-main.104,Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization,2021,-1,-1,4,0,3558,anshuman mishra,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths."
2021.gem-1.16,Decoding Methods for Neural Narrative Generation,2021,-1,-1,3,0,244,alexandra delucia,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",0,"Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters{---}specifically, maximum mutual information{---}analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric."
2021.ccl-1.63,"èåæ\
æåæçéå¼åé®å¥è¯å«æ¨¡å(Implicit Rhetorical Questions Recognition Model Combined with Sentiment Analysis)",2021,-1,-1,1,1,1813,xiang li,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}åé®æ¯ç°ä»£æ±è¯­ä¸­ä¸ç§å¸¸ç¨çä¿®è¾ææ³,æ ¹æ®æ¯å¦å«æåé®æ è®°å¯åä¸ºæ¾å¼åé®å¥ä¸éå¼åé®å¥ãå
¶ä¸­éå¼åé®å¥è¡¨è¾¾çæ
ææ´ä¸ºä¸°å¯,è¡¨ç°å½¢å¼ä¹ååå¤æ,å¯¹éå¼åé®å¥çè¯å«æ´å
·æææ§ãæ¬æé¦å
æ©å

äºæ±è¯­åé®å¥è¯­æåº,è¯­æåºè§æ¨¡è¾¾å°10000ä½å¥,æ¥çéå¯¹éå¼åé®å¥çç¹ç¹,æåºäºä¸ç§èåæ
æåæçéå¼åé®å¥è¯å«æ¨¡åãæ¨¡åèèäºå¥å­çè¯­ä¹ä¿¡æ¯,ä¸ä¸æä¿¡æ¯,å¹¶åå©æ
æåæä»»å¡è¾
å©è¯å«éå¼åé®å¥ãå®éªç»æè¡¨æ,æ¬ææåºçæ¨¡åå¨éå¼åé®å¥è¯å«ä»»å¡ä¸åå¾äºè¯å¥½çæ§è½ã{''}"
2021.acl-long.353,Prefix-Tuning: Optimizing Continuous Prompts for Generation,2021,-1,-1,1,1,1813,xiang li,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training."
2021.acl-long.480,Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation,2021,-1,-1,4,0,9177,zhiyong wu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"A neural multimodal machine translation (MMT) system is one that aims to perform better translation by extending conventional text-only translation models with multimodal information. Many recent studies report improvements when equipping their models with the multimodal module, despite the controversy of whether such improvements indeed come from the multimodal part. We revisit the contribution of multimodal information in MMT by devising two interpretable MMT models. To our surprise, although our models replicate similar gains as recently developed multimodal-integrated systems achieved, our models learn to ignore the multimodal information. Upon further investigation, we discover that the improvements achieved by the multimodal models over text-only counterparts are in fact results of the regularization effect. We report empirical findings that highlight the importance of MMT models{'} interpretability, and discuss how our findings will benefit future research."
2020.starsem-1.2,Reading Comprehension as Natural Language Inference:A Semantic Analysis,2020,-1,-1,4,0,3558,anshuman mishra,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"In the recent past, Natural language Inference (NLI) has gained significant attention, particularly given its promise for downstream NLP tasks. However, its true impact is limited and has not been well studied. Therefore, in this paper, we explore the utility of NLI for one of the most prominent downstream tasks, viz. Question Answering (QA). We transform one of the largest available MRC dataset (RACE) to an NLI form, and compare the performances of a state-of-the-art model (RoBERTa) on both these forms. We propose new characterizations of questions, and evaluate the performance of QA and NLI models on these categories. We highlight clear categories for which the model is able to perform better when the data is presented in a coherent entailment form, and a structured question-answer concatenation form, respectively."
2020.iwslt-1.18,Xiaomi{'}s Submissions for {IWSLT} 2020 Open Domain Translation Task,2020,-1,-1,3,0,18833,yuhui sun,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes the Xiaomi{'}s submissions to the IWSLT20 shared open domain translation task for Chinese{\textless}-{\textgreater}Japanese language pair. We explore different model ensembling strategies based on recent Transformer variants. We also further strengthen our systems via some effective techniques, such as data filtering, data selection, tagged back translation, domain adaptation, knowledge distillation, and re-ranking. Our resulting Chinese-{\textgreater}Japanese primary system ranked second in terms of character-level BLEU score among all submissions. Our resulting Japanese-{\textgreater}Chinese primary system also achieved a competitive performance."
2020.emnlp-main.85,{P}roto{QA}: A Question Answering Dataset for Prototypical Common-Sense Reasoning,2020,40,0,2,1,2528,michael boratko,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Given questions regarding some prototypical situation {---} such as Name something that people usually do before they leave the house for work? {---} a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international trivia game show {--} Family Feud. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task."
2020.autosimtrans-1.5,Modeling Discourse Structure for Document-level Neural Machine Translation,2020,-1,-1,2,0,22314,junxuan chen,Proceedings of the First Workshop on Automatic Simultaneous Translation,0,"Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the English-to-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN."
2020.acl-main.243,Posterior Control of Blackbox Generation,2020,-1,-1,1,1,1813,xiang li,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Text generation often requires high-precision output that obeys task-specific rules. This fine-grained control is difficult to enforce with off-the-shelf deep learning models. In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach. Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model. This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models. Experiments consider applications of this approach for text generation. We find that this method improves over standard benchmarks, while also providing fine-grained control."
2020.acl-demos.19,"{C}onv{L}ab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems",2020,25,2,4,0.75,5876,qi zhu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab, ConvLab-2 inherits ConvLab{'}s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides an user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component."
Q19-1023,A Generative Model for Punctuation in Dependency Trees,2019,54,0,1,1,1813,xiang li,Transactions of the Association for Computational Linguistics,0,"Treebanks traditionally treat punctuation marks as ordinary words, but linguists have suggested that a tree{'}s {``}true{''} punctuation marks are not observed (Nunberg, 1990). These latent {``}underlying{''} marks serve to delimit or separate constituents in the syntax tree. When the tree{'}s yield is rendered as a written sentence, a string rewriting mechanism transduces the underlying marks into {``}surface{''} marks, which are part of the observed (surface) string but should not be regarded as part of the tree. We formalize this idea in a generative model of punctuation that admits efficient dynamic programming. We train it without observing the underlying marks, by locally maximizing the incomplete data likelihood (similarly to the EM algorithm). When we use the trained model to reconstruct the tree{'}s underlying punctuation, the results appear plausible across 5 languages, and in particular are consistent with Nunberg{'}s analysis of English. We show that our generative model can be used to beat baselines on punctuation restoration. Also, our reconstruction of a sentence{'}s underlying punctuation lets us appropriately render the surface punctuation (via our trained underlying-to-surface mechanism) when we syntactically transform the sentence."
P19-3011,{C}onv{L}ab: Multi-Domain End-to-End Dialog System Platform,2019,24,4,6,0,2972,sungjin lee,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We present ConvLab, an open-source multi-domain end-to-end dialog system platform, that enables researchers to quickly set up experiments with reusable components and compare a large set of different approaches, ranging from conventional pipeline systems to end-to-end neural models, in common environments. ConvLab offers a set of fully annotated datasets and associated pre-trained reference models. As a showcase, we extend the MultiWOZ dataset with user dialog act annotations to train all component models and demonstrate how ConvLab makes it easy and effortless to conduct complicated experiments in multi-domain end-to-end dialog settings."
D19-1276,Specializing Word Embeddings (for Parsing) by Information Bottleneck,2019,0,6,1,1,1813,xiang li,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction."
P18-1025,Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures,2018,0,14,2,0,29086,luke vilnis,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model."
L18-1494,Sound Signal Processing with {S}eq2{T}ree Network,2018,0,0,5,0,662,weicheng ma,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1041,Few-Shot Charge Prediction with Discriminative Legal Attributes,2018,0,26,2,0,13231,zikun hu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Automatic charge prediction aims to predict the final charges according to the fact descriptions in criminal cases and plays a crucial role in legal assistant systems. Existing works on charge prediction perform adequately on those high-frequency charges but are not yet capable of predicting few-shot charges with limited cases. Moreover, these exist many confusing charge pairs, whose fact descriptions are fairly similar to each other. To address these issues, we introduce several discriminative attributes of charges as the internal mapping between fact descriptions and charges. These attributes provide additional information for few-shot charges, as well as effective signals for distinguishing confusing charges. More specifically, we propose an attribute-attentive charge prediction model to infer the attributes and charges simultaneously. Experimental results on real-work datasets demonstrate that our proposed model achieves significant and consistent improvements than other state-of-the-art baselines. Specifically, our model outperforms other baselines by more than 50{\%} in the few-shot scenario. Our codes and datasets can be obtained from https://github.com/thunlp/attribute{\_}charge."
P16-1137,Commonsense Knowledge Base Completion,2016,48,39,1,1,1813,xiang li,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion (KBC). Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set. However, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases. We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones. We find strong performance from a bilinear model using a simple additive architecture to model phrases. We manually evaluate our trained modelxe2x80x99s ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as mediumconfidence tuples from ConceptNet."
W15-4502,Improving Event Detection with {A}bstract {M}eaning {R}epresentation,2015,25,16,1,1,1813,xiang li,Proceedings of the First Workshop on Computing News Storylines,0,"Event Detection (ED) aims to identify instances of specified types of events in text, which is a crucial component in the overall task of event extraction. The commonly used features consist of lexical, syntactic, and entity information, but the knowledge encoded in the Abstract Meaning Representation (AMR) has not been utilized in this task. AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. In this paper, we demonstrate the effectiveness of AMR to capture and represent the deeper semantic contexts of the trigger words in this task. Experimental results further show that adding AMR features on top of the traditional features can achieve 67.8% (with 2.1% absolute improvement) F-measure (F1), which is comparable to the state-of-the-art approaches."
R15-1010,Improving Event Detection with Active Learning,2015,15,4,2,1,30065,kai cao,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Event Detection (ED), one aspect of Information Extraction, involves identifying instances of specified types of events in text. Much of the research on ED has been based on the specifications of the 2005 ACE [Automatic Content Extraction] event task 1 , and the associated annotated corpus. However, as the event instances in the ACE corpus are not evenly distributed, some frequent expressions involving ACE events do not appear in the training data, adversely affecting performance. In this paper, we demonstrate the effectiveness of a Pattern Expansion technique to import frequent patterns extracted from external corpora to boost ED performance. The experimental results show that our pattern-based system with the expanded patterns can achieve 70.4% (with 1.6% absolute improvement) F-measure over the baseline, an advance over current state-of-the-art systems."
R15-1011,Improving Event Detection with Dependency Regularization,2015,9,2,2,1,30065,kai cao,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"Event Detection (ED) is an Information Extraction task which involves identifying instances of specified types of events in text. Most recent research on Event Detection relies on pattern-based or featurebased approaches, trained on annotated corpora, to recognize combinations of event triggers, arguments, and other contextual information. These combinations may each appear in a variety of linguistic forms. Not all of these event expressions will have appeared in the training data, thus adversely affecting ED performance. In this paper, we demonstrate the effectiveness of Dependency Regularization techniques to generalize the patterns extracted from the training data to boost ED performance. The experimental results on the ACE 2005 corpus show that our pattern-based system with the expanded patterns can achieve 70.49% (with 2.57% absolute improvement) F-measure over the baseline, which advances the state-of-the-art for such systems."
P15-2103,"Tackling Sparsity, the Achilles Heel of Social Networks: Language Model Smoothing via Social Regularization",2015,24,2,2,0,3650,rui yan,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Online social networks nowadays have the worldwide prosperity, as they have revolutionized the way for people to discover, to share, and to diffuse information. Social networks are powerful, yet they still have Achilles Heel: extreme data sparsity. Individual posting documents, (e.g., a microblog less than 140 characters), seem to be too sparse to make a difference under various scenarios, while in fact they are quite different. We propose to tackle this specific weakness of social networks by smoothing the posting document language model based on social regularization. We formulate an optimization framework with a social regularizer. Experimental results on the Twitter dataset validate the effectiveness and efficiency of our proposed model."
R13-1051,Confidence Estimation for Knowledge Base Population,2013,15,4,1,1,1813,xiang li,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of83.5%, Pearson coefficient of54.2%, and2.3%absolute improvement in F-measure score through a weighted voting strategy."
R13-1052,Towards Fine-grained Citation Function Classification,2013,15,16,1,1,1813,xiang li,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"We look into the problem of recognizing citation functions in scientific literature, trying to reveal authorsxe2x80x99 rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%."
P13-2105,Iterative Transformation of Annotation Guidelines for Constituency Parsing,2013,28,1,1,1,1813,xiang li,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only."
W11-1215,Cross-lingual Slot Filling from Comparable Corpora,2011,32,3,2,0,44205,matthew snover,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"This paper introduces a new task of crosslingual slot filling which aims to discover attributes for entity queries from crosslingual comparable corpora and then present answers in a desired language. It is a very challenging task which suffers from both information extraction and machine translation errors. In this paper we analyze the types of errors produced by five different baseline approaches, and present a novel supervised rescoring based validation approach to incorporate global evidence from very large bilingual comparable corpora. Without using any additional labeled data this new approach obtained 38.5% relative improvement in Precision and 86.7% relative improvement in Recall over several state-of-the-art approaches. The ultimate system outperformed monolingual slot filling pipelines built on much larger monolingual corpora."
Y10-1027,Domain-Independent Novel Event Discovery and Semi-Automatic Event Annotation,2010,24,11,2,0,20232,hao li,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"Information Extraction (IE) is becoming increasingly useful, but it is a costly task to discover and annotate novel events, event arguments, and event types. We exploit both monolingual texts and bilingual sentence-aligned parallel texts to cluster event triggers and discover novel event types. We then generate event argument annotations semiautomatically, framed as a sentence ranking and semantic role labeling task. Experiments on three different corpora -ACE, OntoNotes and a collection of scientific literature -have demonstrated that our domain-independent methods can significantly speed up the entire event discovery and annotation process while maintaining high quality."
ji-etal-2010-annotating,Annotating Event Chains for Carbon Sequestration Literature,2010,16,2,2,0,716,heng ji,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper we present a project of annotating event chains for an important scientific domain â carbon sequestration. This domain aims to reduce carbon emissions and has been identified by the U.S. National Academy of Engineering (NAE) as a grand challenge problem for the 21st century. Given a collection of scientific literature, we identify a set of centroid experiments; and then link and order the observations and events centered around these experiments on temporal or causal chains. We describe the fundamental challenges on annotations and our general solutions to address them. We expect that our annotation efforts will produce significant advances in inter-operability through new information extraction techniques and permit scientists to build knowledge that will provide better understanding of important scientific challenges in this domain, share and re-use of diverse data sets and experimental results in a more efficient manner. In addition, the annotations of metadata and ontology for these literature will provide important support for data lifecycle activities."
