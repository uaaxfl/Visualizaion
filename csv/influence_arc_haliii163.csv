2020.acl-main.189,D19-1371,0,0.012227,"yphrase extraction, we use training, validation, and test data from SemEval 2017 Task 10 (Augenstein et al., 2017), consisting of IO tags (we use one “I” tag for all three keyphrase types). In all tasks, we implement both the policy and difference classifier by fine-tuning the last layer of a BERT embedding representation (Devlin et al., 2019). More specifically, for a sentence of length T , w1 , . . . , wT , we first compute BERT embeddings for each word, x1 , . . . , xT using the appropriate BERT model: English BERT and M-BERT5 for named entity and part-of-speech, respectively, and SciBERT (Beltagy et al., 2019) for keyphrase extraction. We then represent the state at position t by concatenating the word embedding at that position with a one-hot representation of the previous action: st = [wt ; onehot(at−1 )]. This feature representation is used both for learning the labeling policy and also learning the difference classifier. 4.3 Expert Policy and Heuristics In all experiments, the expert π ? is a simulated human annotator who annotates one word at a time. The expert returns the optimal action for the relevant evaluation metric (F-score for named entity recognition and keyphrase extraction, and accu"
2020.acl-main.189,P10-1088,0,0.0344113,"ne and Warmuth, 1989), where, at each round, a learner is presented with an example x and must predict a label yˆ ∈ {−1, +1}. After this prediction, the true label is revealed only if the learner predicts +1. This framework has been applied in several settings, such as spam filtering and document classification with minority class distributions (Sculley, 2007). Sculley (2007) also conducts a through comparison of two methods that can be used to address the one-side feedback problem: label-efficient online learning (Cesa-Bianchi et al., 2006) and margin-based learning (Vapnik, 1982). 2.3 2007; Bloodgood and Callison-Burch, 2010). For sequence labeling tasks, Haertel et al. (2008) found that labeling effort depends both on the number of words labeled (which we model), plus a fixed cost for reading (which we do not). In the context of imitation learning, active approaches have also been considered for at least three decades, often called “learning with an external critic” and “learning by watching” (Whitehead, 1991). More recently, Judah et al. (2012) describe RAIL, an active learning-for-imitation-learning algorithm akin to our ACTIVE DAGGER baseline, but which in principle would operate with any underlying i.i.d. act"
2020.acl-main.189,W11-1902,0,0.0651259,"Missing"
2020.acl-main.189,P04-1015,0,0.147509,"e expert alone and over several baselines and ablations that establish the importance of both the difference classifier and the Apple Tasting paradigm. 2 Background and Related Work We review first the use of imitation learning for structured prediction, then online active learning, and finally applications of active learning to structured prediction and imitation learning problems. 2.1 Learning to Search The learning to search approach to structured prediction casts the joint prediction problem of producing a complex output as a sequence of smaller classification problems (Ratnaparkhi, 1996; Collins and Roark, 2004; Daumé et al., 2009). For instance, in the named entity recognition example from Figure 1, an input sentence x is labeled one word at a time, left-to-right. At the depicted state (s10 ), the model has labeled the first nine words and must next label the tenth word. Learning to search approaches assume access to an oracle policy π ? , which provides the optimal label at every position. In (interactive) imitation learning, we aim to imitate the behavior of the expert policy, π ? , which provides the true labels. The learning to search view allows us to cast structured prediction as a (degenerat"
2020.acl-main.189,W96-0213,0,0.288587,"vings over using the expert alone and over several baselines and ablations that establish the importance of both the difference classifier and the Apple Tasting paradigm. 2 Background and Related Work We review first the use of imitation learning for structured prediction, then online active learning, and finally applications of active learning to structured prediction and imitation learning problems. 2.1 Learning to Search The learning to search approach to structured prediction casts the joint prediction problem of producing a complex output as a sequence of smaller classification problems (Ratnaparkhi, 1996; Collins and Roark, 2004; Daumé et al., 2009). For instance, in the named entity recognition example from Figure 1, an input sentence x is labeled one word at a time, left-to-right. At the depicted state (s10 ), the model has labeled the first nine words and must next label the tenth word. Learning to search approaches assume access to an oracle policy π ? , which provides the optimal label at every position. In (interactive) imitation learning, we aim to imitate the behavior of the expert policy, π ? , which provides the true labels. The learning to search view allows us to cast structured p"
2020.acl-main.189,N19-1423,0,0.0270187,"two state s10 to tag the 10th word; the state s10 (highlighted in purple) combines x with y h errors at t = 4 and t = 6. The heuristic label at t = 10 is y10 =ORG. Under Hamming loss, the cost at t = 10 is minimized for a = ORG, which is therefore the expert action (if it were queried). The label that would be provided for s10 to the difference classifier is 0 because the two policies agree. nary compiled from the training data (Zesch et al., 2008; Haghighi and Klein, 2006). In all three settings, the expert is a simulated human annotator. We train L EAQI on all three tasks using fixed BERT (Devlin et al., 2019) features, training only the final layer (because we are in the regime of small labeled data). The goal in all three settings is to minimize the number of words the expert annotator must label. In all settings, we’re able to establish the efficacy of L EAQI, showing that it can indeed provide significant label savings over using the expert alone and over several baselines and ablations that establish the importance of both the difference classifier and the Apple Tasting paradigm. 2 Background and Related Work We review first the use of imitation learning for structured prediction, then online"
2020.acl-main.189,P17-1102,0,0.0231448,"Missing"
2020.acl-main.189,W05-0619,0,0.0977864,", L EAQI takes a policy class Π, a hypothesis class H for the difference classifier (assumed to be symmetric and to contain the “constant one” function), a number of episodes N , an expert policy π ? , a heuristic policy π h , and a confidence parameter b > 0. The general structure of L EAQI follows that of DAgger, but with three key differences: Active Imitation & Structured Prediction In the context of structured prediction for natural language processing, active learning has been considered both for requesting full structured outputs (e.g. Thompson et al., 1999; Culotta and McCallum, 2005; Hachey et al., 2005) and for requesting only pieces of outputs (e.g. Ringger et al., 2095 (a) roll-in (line 7) is according to the learned policy (not mixed with the expert, as that would require additional expert queries), (b) actions are queried only if the current policy is uncertain at s (line 12), and (c) the expert π ? is only queried if it is predicted to disagree with the heuristic π h at s by the difference classifier, or if apple tasting method switches the difference classifier label (line 15; see §3.2). Algorithm 2 L EAQI(Π, H, N, π ? , π h , b) 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 1"
2020.acl-main.189,P08-2017,0,0.0325897,"d with an example x and must predict a label yˆ ∈ {−1, +1}. After this prediction, the true label is revealed only if the learner predicts +1. This framework has been applied in several settings, such as spam filtering and document classification with minority class distributions (Sculley, 2007). Sculley (2007) also conducts a through comparison of two methods that can be used to address the one-side feedback problem: label-efficient online learning (Cesa-Bianchi et al., 2006) and margin-based learning (Vapnik, 1982). 2.3 2007; Bloodgood and Callison-Burch, 2010). For sequence labeling tasks, Haertel et al. (2008) found that labeling effort depends both on the number of words labeled (which we model), plus a fixed cost for reading (which we do not). In the context of imitation learning, active approaches have also been considered for at least three decades, often called “learning with an external critic” and “learning by watching” (Whitehead, 1991). More recently, Judah et al. (2012) describe RAIL, an active learning-for-imitation-learning algorithm akin to our ACTIVE DAGGER baseline, but which in principle would operate with any underlying i.i.d. active learning algorithm (not just our specific choice"
2020.acl-main.189,N06-1041,0,0.0471165,"e and y is the (unobserved) ground truth. The predictor π operates left-to-right and, in this example, is currently at ˆ 1:9 . The heuristic makes two state s10 to tag the 10th word; the state s10 (highlighted in purple) combines x with y h errors at t = 4 and t = 6. The heuristic label at t = 10 is y10 =ORG. Under Hamming loss, the cost at t = 10 is minimized for a = ORG, which is therefore the expert action (if it were queried). The label that would be provided for s10 to the difference classifier is 0 because the two policies agree. nary compiled from the training data (Zesch et al., 2008; Haghighi and Klein, 2006). In all three settings, the expert is a simulated human annotator. We train L EAQI on all three tasks using fixed BERT (Devlin et al., 2019) features, training only the final layer (because we are in the regime of small labeled data). The goal in all three settings is to minimize the number of words the expert annotator must label. In all settings, we’re able to establish the efficacy of L EAQI, showing that it can indeed provide significant label savings over using the expert alone and over several baselines and ablations that establish the importance of both the difference classifier and th"
2020.acl-main.189,W03-1014,0,0.459216,"Missing"
2020.acl-main.189,W07-1516,0,0.142112,"Missing"
2020.acl-main.189,E14-2006,0,0.027183,"Missing"
2020.acl-main.189,zesch-etal-2008-extracting,0,0.0356182,"is the input sentence and y is the (unobserved) ground truth. The predictor π operates left-to-right and, in this example, is currently at ˆ 1:9 . The heuristic makes two state s10 to tag the 10th word; the state s10 (highlighted in purple) combines x with y h errors at t = 4 and t = 6. The heuristic label at t = 10 is y10 =ORG. Under Hamming loss, the cost at t = 10 is minimized for a = ORG, which is therefore the expert action (if it were queried). The label that would be provided for s10 to the difference classifier is 0 because the two policies agree. nary compiled from the training data (Zesch et al., 2008; Haghighi and Klein, 2006). In all three settings, the expert is a simulated human annotator. We train L EAQI on all three tasks using fixed BERT (Devlin et al., 2019) features, training only the final layer (because we are in the regime of small labeled data). The goal in all three settings is to minimize the number of words the expert annotator must label. In all settings, we’re able to establish the efficacy of L EAQI, showing that it can indeed provide significant label savings over using the expert alone and over several baselines and ablations that establish the importance of both the d"
2020.acl-main.418,W14-1014,0,0.0297853,"Missing"
2020.acl-main.418,P08-2039,0,0.03336,"Missing"
2020.acl-main.418,E09-1011,0,0.0757164,"Missing"
2020.acl-main.418,E85-1004,0,0.0988979,"Missing"
2020.acl-main.418,H94-1097,0,0.698931,"Missing"
2020.acl-main.418,W18-1602,0,0.0568093,"Missing"
2020.acl-main.418,S18-2011,0,0.0458214,"Missing"
2020.acl-main.418,P05-1054,0,0.159677,"Missing"
2020.acl-main.418,D11-1120,0,0.266151,"Missing"
2020.acl-main.418,W07-0802,0,0.138853,"Missing"
2020.acl-main.418,C88-1029,0,0.342202,"Missing"
2020.acl-main.418,darwish-etal-2014-using,0,0.0702537,"Missing"
2020.acl-main.418,H05-1013,1,0.538909,"Missing"
2020.acl-main.418,W03-2909,0,0.0226691,"Missing"
2020.acl-main.418,W12-1006,0,0.0473601,"Missing"
2020.acl-main.418,dinu-etal-2012-romanian,0,0.0622999,"Missing"
2020.acl-main.418,P98-1056,0,0.0162305,"Missing"
2020.acl-main.418,W18-1110,0,0.0886183,"Missing"
2020.acl-main.418,H05-1050,0,0.0682675,"Missing"
2020.acl-main.418,W12-1514,0,0.0390835,"Missing"
2020.acl-main.418,D12-1135,0,0.0780536,"Missing"
2020.acl-main.418,P16-1080,0,0.052636,"Missing"
2020.acl-main.418,W19-3821,0,0.137128,"Missing"
2020.acl-main.418,K15-1011,0,0.0443108,"Missing"
2020.acl-main.418,P86-1031,0,0.65807,"Missing"
2020.acl-main.418,N18-2110,0,0.0350581,"Missing"
2020.acl-main.418,W14-0908,0,0.038306,"Missing"
2020.acl-main.418,N13-2012,0,0.0617664,"Missing"
2020.acl-main.418,E17-4005,0,0.0367977,"Missing"
2020.acl-main.418,P13-4012,0,0.026272,"Missing"
2020.acl-main.418,J13-1008,0,0.0605451,"Missing"
2020.acl-main.418,W14-3315,0,0.032395,"Missing"
2020.acl-main.418,J88-1004,0,0.0954926,"Missing"
2020.acl-main.418,W19-0118,0,0.0994123,"Missing"
2020.acl-main.418,S18-1001,0,0.0690232,"Missing"
2020.acl-main.418,W11-1709,0,0.0969424,"Missing"
2020.acl-main.418,D18-1142,0,0.0605385,"Missing"
2020.acl-main.418,C14-1184,0,0.0605262,"Missing"
2020.acl-main.418,C14-2014,0,0.0481843,"Missing"
2020.acl-main.418,D14-1211,0,0.0487295,"Missing"
2020.acl-main.418,W15-1203,0,0.0642101,"Missing"
2020.acl-main.418,P16-1140,0,0.0370664,"Missing"
2020.acl-main.418,C94-1067,0,0.307918,"Missing"
2020.acl-main.418,W06-1608,0,0.0828571,"Missing"
2020.acl-main.418,E17-1101,0,0.0604626,"Missing"
2020.acl-main.418,D10-1048,0,0.0435336,"Missing"
2020.acl-main.418,D15-1234,0,0.0671959,"Missing"
2020.acl-main.418,W17-1609,0,0.0601747,"Missing"
2020.acl-main.418,N18-2002,0,0.0960146,"Missing"
2020.acl-main.418,D14-1121,0,0.0608866,"Missing"
2020.acl-main.418,D17-1247,0,0.118639,"Missing"
2020.acl-main.418,W11-1903,0,0.0303321,"Missing"
2020.acl-main.418,W11-0310,0,0.0393655,"Missing"
2020.acl-main.418,W16-0204,0,0.0896822,"Missing"
2020.acl-main.418,N15-1044,0,0.0611289,"Missing"
2020.acl-main.418,J81-4001,0,0.475168,"Missing"
2020.acl-main.418,sidorov-etal-2014-comparison,0,0.0461186,"Missing"
2020.acl-main.418,W03-2906,0,0.0438337,"Missing"
2020.acl-main.418,H05-1060,0,0.0160534,"Missing"
2020.acl-main.418,W15-2814,0,0.0604651,"Missing"
2020.acl-main.418,W17-1606,0,0.108894,"Missing"
2020.acl-main.418,E17-2108,0,0.0360609,"Missing"
2020.acl-main.418,W94-0115,0,0.475307,"Missing"
2020.acl-main.418,E14-3004,0,0.0559163,"Missing"
2020.acl-main.418,Y07-1047,0,0.117223,"Missing"
2020.acl-main.418,D18-1004,0,0.0631958,"Missing"
2020.acl-main.418,Q18-1042,0,0.369499,", both because gender can show up explicitly (e.g., pronouns in English, morphology in Arabic) and because societal expectations and stereotypes around gender roles may be explicitly or implicitly assumed by speakers or listeners. This can lead to significant biases in coreference resolution systems: cases where systems “systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others” (Friedman and Nissenbaum, 1996, p. 332). Gender bias in coreference resolution can manifest in many ways; work by Rudinger et al. (2018), Zhao et al. (2018a), and Webster et al. (2018) focused largely on the case of binary gender discrimination in trained coreference systems, showing that current systems over-rely on social stereotypes when resolving HE and SHE pronouns1 (see §2). Contemporaneously, critical work in HumanComputer Interaction has complicated discussions around gender in other fields, such as computer vision (Keyes, 2018; Hamidi et al., 2018). Building on both lines of work, and inspired by Keyes’s (2018) study of vision-based automatic gender recognition systems, we consider gender bias from a broader conceptual frame than the binary “folk” model. We investi"
2020.acl-main.418,P13-1058,0,0.0538652,"Missing"
2020.acl-main.418,W18-1114,0,0.0946158,"Missing"
2020.acl-main.418,C88-2159,0,0.460471,"Missing"
2020.acl-main.418,C16-1199,0,0.0343323,"Missing"
2020.acl-main.418,D17-1323,0,0.0477793,"Missing"
2020.acl-main.418,N18-2003,0,0.462477,"varied aspects of gender, both because gender can show up explicitly (e.g., pronouns in English, morphology in Arabic) and because societal expectations and stereotypes around gender roles may be explicitly or implicitly assumed by speakers or listeners. This can lead to significant biases in coreference resolution systems: cases where systems “systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others” (Friedman and Nissenbaum, 1996, p. 332). Gender bias in coreference resolution can manifest in many ways; work by Rudinger et al. (2018), Zhao et al. (2018a), and Webster et al. (2018) focused largely on the case of binary gender discrimination in trained coreference systems, showing that current systems over-rely on social stereotypes when resolving HE and SHE pronouns1 (see §2). Contemporaneously, critical work in HumanComputer Interaction has complicated discussions around gender in other fields, such as computer vision (Keyes, 2018; Hamidi et al., 2018). Building on both lines of work, and inspired by Keyes’s (2018) study of vision-based automatic gender recognition systems, we consider gender bias from a broader conceptual frame than the bi"
2020.acl-main.418,D18-1521,0,0.396249,"varied aspects of gender, both because gender can show up explicitly (e.g., pronouns in English, morphology in Arabic) and because societal expectations and stereotypes around gender roles may be explicitly or implicitly assumed by speakers or listeners. This can lead to significant biases in coreference resolution systems: cases where systems “systematically and unfairly discriminate against certain individuals or groups of individuals in favor of others” (Friedman and Nissenbaum, 1996, p. 332). Gender bias in coreference resolution can manifest in many ways; work by Rudinger et al. (2018), Zhao et al. (2018a), and Webster et al. (2018) focused largely on the case of binary gender discrimination in trained coreference systems, showing that current systems over-rely on social stereotypes when resolving HE and SHE pronouns1 (see §2). Contemporaneously, critical work in HumanComputer Interaction has complicated discussions around gender in other fields, such as computer vision (Keyes, 2018; Hamidi et al., 2018). Building on both lines of work, and inspired by Keyes’s (2018) study of vision-based automatic gender recognition systems, we consider gender bias from a broader conceptual frame than the bi"
2020.acl-main.485,W19-3816,0,0.0323746,"Missing"
2020.acl-main.485,S19-1023,0,0.0283668,"Missing"
2020.acl-main.485,W19-3805,0,0.0877226,"Missing"
2020.acl-main.485,N19-3002,0,0.0756333,"se statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities. 1 Introduction A large body of work analyzing “bias” in natural language processing (NLP) systems has emerged in recent years, including work on “bias” in embedding spaces (e.g., Bolukbasi et al., 2016a; Caliskan et al., 2017; Gonen and Goldberg, 2019; May et al., 2019) as well as work on “bias” in systems developed for a breadth of tasks including language modeling (Lu et al., 2018; Bordia and Bowman, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Vanmassenhove et al., 2018; Stanovsky et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), and hate speech/toxicity detection (e.g., Park et al., 2018; Dixon et al., 2018), among others. Although these papers have laid vital groundwork by illustrating some of the ways that NLP systems can be harmful, the majority of them fail to engage critically with what constitutes “bias” in the frst place. Despite the fact that analyzing “bias” is an inherently normative process—in which some system be"
2020.acl-main.485,W18-0802,0,0.0410519,"Missing"
2020.acl-main.485,W19-3504,0,0.202462,"(Caliskan et al., 2017); (b) sentiment analysis systems yielding different intensity scores for sentences containing names associated with African Americans and sentences containing names associated with European Americans (Kiritchenko and Mohammad, 2018); and (c) toxicity 5454 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–5476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics detection systems scoring tweets containing features associated with African-American English as more offensive than tweets without these features (Davidson et al., 2019; Sap et al., 2019). Moreover, some of these papers focus on “racial bias” expressed in written text, while others focus on “racial bias” against authors. This use of imprecise terminology obscures these important differences. We survey 146 papers analyzing “bias” in NLP systems, fnding that their motivations are often vague and inconsistent. Many lack any normative reasoning for why the system behaviors that are described as “bias” are harmful, in what ways, and to whom. Moreover, the vast majority of these papers do not engage with the relevant literature outside of NLP to ground normative c"
2020.acl-main.485,N13-1037,0,0.02424,"rceptions (Olteanu et al., 2019; Sap et al., 2019; Geiger et al., 2020), and annotation aggregation processes (Pavlick and Kwiatkowski, 2019)?  Evaluation: How are NLP systems evaluated? What are the impacts of evaluation metrics (Olteanu et al., 2017)? Are any non-quantitative evaluations performed? . How do NLP systems reproduce or transform language ideologies? Which language varieties or practices come to be deemed good or bad? Might “good” language simply mean language that is easily handled by existing NLP systems? For example, linguistic phenomena arising from many language practices (Eisenstein, 2013) are described as “noisy text” and often viewed as a target for “normalization.” How do the language ideologies that are reproduced by NLP systems maintain social hierarchies? . Which representational harms are being measured or mitigated? Are these the most normatively concerning harms, or merely those that are well handled by existing algorithmic fairness techniques? Are there other representational harms that might be analyzed? 4.2 Conceptualizations of “bias” underpin this conceptualization of “bias?” Turning now to (R2), we argue that work analyzing “bias” in NLP systems should provide ex"
2020.acl-main.485,2020.acl-main.262,0,0.0378979,"Missing"
2020.acl-main.485,P19-1166,0,0.0819945,"Missing"
2020.acl-main.485,P15-2079,0,0.0461963,"Missing"
2020.acl-main.485,P16-2096,0,0.0643013,"Missing"
2020.acl-main.485,2020.lrec-1.180,0,0.023978,"Missing"
2020.acl-main.485,W19-3817,0,0.0318392,"Missing"
2020.acl-main.485,W19-3806,0,0.0204154,"Missing"
2020.acl-main.485,W19-4446,0,0.0329752,"ustify focusing on particular system behaviors, even when the downstream effects are not measured. Papers on “bias” in embedding spaces are especially likely to do this because embeddings are often used as input to other systems: “However, none of these papers [on embeddings] have recognized how blatantly sexist the embeddings are and hence risk introducing biases of various types into real-world systems.” —Bolukbasi et al. (2016a) In contrast, papers that provide surveys or frameworks for analyzing “bias” in NLP systems treat representational harms as harmful in their own right. For example, Mayfeld et al. (2019) and Ruane et al. (2019) cite the harmful reproduction of dominant linguistic norms by NLP systems (a point to which we return in section 4), while Bender (2019) outlines a range of harms, including seeing stereotypes in search results and being made invisible to search engines due to language practices. 3.2 Techniques Papers’ techniques are not well grounded in the relevant literature outside of NLP. Perhaps unsurprisingly given that the papers’ motivations are often vague, inconsistent, and lacking in normative reasoning, we also found that the papers’ proposed quantitative techniques for me"
2020.acl-main.485,P11-1077,0,0.0373638,"cle? What kinds of NLP systems do these decisions result in, and what kinds do they foreclose?  General assumptions: To which linguistic norms do NLP systems adhere (Bender, 2019; Ruane et al., 2019)? Which language practices are implicitly assumed to be standard, ordinary, correct, or appropriate?  Task defnition: For which speakers are NLP systems (and NLP resources) developed? (See Joshi et al. (2020) for a discussion.) How do task defnitions discretize the world? For example, how are social groups delineated when defning demographic attribute prediction tasks (e.g., Koppel et al., 2002; Rosenthal and McKeown, 2011; Nguyen et al., 2013)? What about languages in native language prediction tasks (Tetreault et al., 2013)?  Data: How are datasets collected, preprocessed, and labeled or annotated? What are the impacts of annotation guidelines, annotator assumptions and perceptions (Olteanu et al., 2019; Sap et al., 2019; Geiger et al., 2020), and annotation aggregation processes (Pavlick and Kwiatkowski, 2019)?  Evaluation: How are NLP systems evaluated? What are the impacts of evaluation metrics (Olteanu et al., 2017)? Are any non-quantitative evaluations performed? . How do NLP systems reproduce or trans"
2020.acl-main.485,W17-1609,0,0.046418,"Missing"
2020.acl-main.485,N18-2002,0,0.117026,"Missing"
2020.acl-main.485,P19-1163,0,0.394762,"; (b) sentiment analysis systems yielding different intensity scores for sentences containing names associated with African Americans and sentences containing names associated with European Americans (Kiritchenko and Mohammad, 2018); and (c) toxicity 5454 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454–5476 c July 5 - 10, 2020. 2020 Association for Computational Linguistics detection systems scoring tweets containing features associated with African-American English as more offensive than tweets without these features (Davidson et al., 2019; Sap et al., 2019). Moreover, some of these papers focus on “racial bias” expressed in written text, while others focus on “racial bias” against authors. This use of imprecise terminology obscures these important differences. We survey 146 papers analyzing “bias” in NLP systems, fnding that their motivations are often vague and inconsistent. Many lack any normative reasoning for why the system behaviors that are described as “bias” are harmful, in what ways, and to whom. Moreover, the vast majority of these papers do not engage with the relevant literature outside of NLP to ground normative concerns when propos"
2020.acl-main.485,2020.acl-main.486,0,0.0863559,"Missing"
2020.acl-main.485,2020.acl-main.690,0,0.0691754,"ries to refect that the main point of differentiation between the 146 papers’ motivations and proposed quantitative techniques for measuring or mitigating “bias” is whether or not they focus on stereotyping. Among the papers that do not focus on stereotyping, we found that most lack suffciently clear motivations and techniques to reliably categorize them further. Motivations “An over-prevalence of some gendered forms in the training data leads to translations with identifable errors. Translations are better for sentences involving men and for sentences containing stereotypical gender roles.” —Saunders and Byrne (2020) Even when papers do state clear motivations, they are often unclear about why the system behaviors that are described as “bias” are harmful, in what ways, and to whom. We found that even papers with clear motivations often fail to explain what kinds of system behaviors are harmful, in what ways, to whom, and why. For example, 5456 “Deploying these word embedding algorithms in practice, for example in automated translation systems or as hiring aids, runs the serious risk of perpetuating problematic biases in important societal contexts.” —Brunet et al. (2019) “It is essential to quantify and m"
2020.acl-main.485,W17-1611,0,0.0648916,"Missing"
2020.acl-main.485,W19-3808,0,0.0756519,"Missing"
2020.acl-main.485,2020.acl-main.468,0,0.28092,"Missing"
2020.acl-main.485,D19-1339,0,0.0862092,"Missing"
2020.acl-main.485,2020.findings-emnlp.291,0,0.0365037,"Missing"
2020.acl-main.485,2020.findings-emnlp.280,0,0.0199185,"Missing"
2020.acl-main.485,P19-1164,0,0.106254,"ining the power relations between technologists and such communities. 1 Introduction A large body of work analyzing “bias” in natural language processing (NLP) systems has emerged in recent years, including work on “bias” in embedding spaces (e.g., Bolukbasi et al., 2016a; Caliskan et al., 2017; Gonen and Goldberg, 2019; May et al., 2019) as well as work on “bias” in systems developed for a breadth of tasks including language modeling (Lu et al., 2018; Bordia and Bowman, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Vanmassenhove et al., 2018; Stanovsky et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), and hate speech/toxicity detection (e.g., Park et al., 2018; Dixon et al., 2018), among others. Although these papers have laid vital groundwork by illustrating some of the ways that NLP systems can be harmful, the majority of them fail to engage critically with what constitutes “bias” in the frst place. Despite the fact that analyzing “bias” is an inherently normative process—in which some system behaviors are deemed good and others harmful—papers on “bias” in NLP systems are rife with unstated assumptions about what kinds of system behav"
2020.acl-main.485,P19-1159,0,0.110051,"Missing"
2020.acl-main.485,P19-1162,0,0.0541878,"Missing"
2020.acl-main.485,2020.acl-main.263,0,0.0255865,"Missing"
2020.acl-main.485,W13-1706,0,0.0199284,"mptions: To which linguistic norms do NLP systems adhere (Bender, 2019; Ruane et al., 2019)? Which language practices are implicitly assumed to be standard, ordinary, correct, or appropriate?  Task defnition: For which speakers are NLP systems (and NLP resources) developed? (See Joshi et al. (2020) for a discussion.) How do task defnitions discretize the world? For example, how are social groups delineated when defning demographic attribute prediction tasks (e.g., Koppel et al., 2002; Rosenthal and McKeown, 2011; Nguyen et al., 2013)? What about languages in native language prediction tasks (Tetreault et al., 2013)?  Data: How are datasets collected, preprocessed, and labeled or annotated? What are the impacts of annotation guidelines, annotator assumptions and perceptions (Olteanu et al., 2019; Sap et al., 2019; Geiger et al., 2020), and annotation aggregation processes (Pavlick and Kwiatkowski, 2019)?  Evaluation: How are NLP systems evaluated? What are the impacts of evaluation metrics (Olteanu et al., 2017)? Are any non-quantitative evaluations performed? . How do NLP systems reproduce or transform language ideologies? Which language varieties or practices come to be deemed good or bad? Might “goo"
2020.acl-main.485,D18-1334,0,0.0262679,"ile interrogating and reimagining the power relations between technologists and such communities. 1 Introduction A large body of work analyzing “bias” in natural language processing (NLP) systems has emerged in recent years, including work on “bias” in embedding spaces (e.g., Bolukbasi et al., 2016a; Caliskan et al., 2017; Gonen and Goldberg, 2019; May et al., 2019) as well as work on “bias” in systems developed for a breadth of tasks including language modeling (Lu et al., 2018; Bordia and Bowman, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Vanmassenhove et al., 2018; Stanovsky et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), and hate speech/toxicity detection (e.g., Park et al., 2018; Dixon et al., 2018), among others. Although these papers have laid vital groundwork by illustrating some of the ways that NLP systems can be harmful, the majority of them fail to engage critically with what constitutes “bias” in the frst place. Despite the fact that analyzing “bias” is an inherently normative process—in which some system behaviors are deemed good and others harmful—papers on “bias” in NLP systems are rife with unstated assumptions about w"
2020.acl-main.485,2020.acl-main.484,0,0.114666,"Missing"
2020.acl-main.485,W19-3813,0,0.0199143,"Missing"
2020.acl-main.485,Q18-1042,0,0.0394644,"Missing"
2020.acl-main.485,W19-3815,0,0.0199129,"Missing"
2020.acl-main.485,2020.acl-main.380,0,0.078906,"Missing"
2020.acl-main.485,N19-1064,0,0.0592156,"Missing"
2020.acl-main.485,D17-1323,0,0.154462,"Missing"
2020.acl-main.485,W19-3814,0,0.0304394,"Missing"
2020.acl-main.485,N18-2003,0,0.129388,"of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities. 1 Introduction A large body of work analyzing “bias” in natural language processing (NLP) systems has emerged in recent years, including work on “bias” in embedding spaces (e.g., Bolukbasi et al., 2016a; Caliskan et al., 2017; Gonen and Goldberg, 2019; May et al., 2019) as well as work on “bias” in systems developed for a breadth of tasks including language modeling (Lu et al., 2018; Bordia and Bowman, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Vanmassenhove et al., 2018; Stanovsky et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), and hate speech/toxicity detection (e.g., Park et al., 2018; Dixon et al., 2018), among others. Although these papers have laid vital groundwork by illustrating some of the ways that NLP systems can be harmful, the majority of them fail to engage critically with what constitutes “bias” in the frst place. Despite the fact that analyzing “bias” is an inherently normative process—in which some system behaviors are deemed good and others harmful—papers on “bias” in NLP"
2020.acl-main.485,D18-1521,0,0.0651516,"of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities. 1 Introduction A large body of work analyzing “bias” in natural language processing (NLP) systems has emerged in recent years, including work on “bias” in embedding spaces (e.g., Bolukbasi et al., 2016a; Caliskan et al., 2017; Gonen and Goldberg, 2019; May et al., 2019) as well as work on “bias” in systems developed for a breadth of tasks including language modeling (Lu et al., 2018; Bordia and Bowman, 2019), coreference resolution (Rudinger et al., 2018; Zhao et al., 2018a), machine translation (Vanmassenhove et al., 2018; Stanovsky et al., 2019), sentiment analysis (Kiritchenko and Mohammad, 2018), and hate speech/toxicity detection (e.g., Park et al., 2018; Dixon et al., 2018), among others. Although these papers have laid vital groundwork by illustrating some of the ways that NLP systems can be harmful, the majority of them fail to engage critically with what constitutes “bias” in the frst place. Despite the fact that analyzing “bias” is an inherently normative process—in which some system behaviors are deemed good and others harmful—papers on “bias” in NLP"
2020.acl-main.485,D19-1531,0,0.0460382,"ors that are described as “bias” are harmful, in what ways, and to whom. We found that even papers with clear motivations often fail to explain what kinds of system behaviors are harmful, in what ways, to whom, and why. For example, 5456 “Deploying these word embedding algorithms in practice, for example in automated translation systems or as hiring aids, runs the serious risk of perpetuating problematic biases in important societal contexts.” —Brunet et al. (2019) “It is essential to quantify and mitigate gender bias in these embeddings to avoid them from affecting downstream applications.” —Zhou et al. (2019) “[I]f the systems show discriminatory behaviors in the interactions, the user experience will be adversely affected.” —Liu et al. (2019) These examples leave unstated what “problematic biases” or non-ideal user experiences might look like, how the system behaviors might result in these things, and who the relevant stakeholders or users might be. In contrast, we fnd that papers that provide surveys or frameworks for analyzing “bias” in NLP systems often name who is harmed, acknowledging that different social groups may experience these systems differently due to their different relationships w"
2020.acl-main.485,P19-1161,1,0.868722,"Missing"
2020.acl-main.485,2020.sdp-1.30,0,0.0293557,"Missing"
2020.findings-emnlp.167,C16-1291,0,0.396332,"replaced by manual alignments. Induced attention refers to the base model (§4). 5.1 Supervised Attention Our annotated lexical alignments resemble our base model’s attention mechanisms. At the encoding stage, question tokens and the relevant columns are aligned (e.g., “who” ↔ column “athlete”) which should induce higher weights in both question-tocolumn and column-to-question attention (Eq. (3) and Eq. (4)); similarly, for decoding, annotation reflects which question words are most relevant to the current output token. Inspired by improvements from supervised attention in machine translation (Liu et al., 2016; Mi et al., 2016), we train the base model’s attention mechanisms to minimize the Euclidean distance5 between the human-annotated alignment vector a? and the model-generated attention vector a: Similarly, we define literal string copying from q with another bilinear scoring matrix W STR . 5 ACCLF (Dev) 1 Latt = ka − a? k2 . 2 The vector a? is a one-hot vector when the annotation aligns to a single element, or a? represents a uniform distribution over the subset in cases where the annotation aligns multiple elements. 5.2 Oracle Experiments with Manual Alignments To present the potential of ali"
2020.findings-emnlp.167,D16-1249,0,0.159887,"alignments. Induced attention refers to the base model (§4). 5.1 Supervised Attention Our annotated lexical alignments resemble our base model’s attention mechanisms. At the encoding stage, question tokens and the relevant columns are aligned (e.g., “who” ↔ column “athlete”) which should induce higher weights in both question-tocolumn and column-to-question attention (Eq. (3) and Eq. (4)); similarly, for decoding, annotation reflects which question words are most relevant to the current output token. Inspired by improvements from supervised attention in machine translation (Liu et al., 2016; Mi et al., 2016), we train the base model’s attention mechanisms to minimize the Euclidean distance5 between the human-annotated alignment vector a? and the model-generated attention vector a: Similarly, we define literal string copying from q with another bilinear scoring matrix W STR . 5 ACCLF (Dev) 1 Latt = ka − a? k2 . 2 The vector a? is a one-hot vector when the annotation aligns to a single element, or a? represents a uniform distribution over the subset in cases where the annotation aligns multiple elements. 5.2 Oracle Experiments with Manual Alignments To present the potential of alignment annotations"
2020.findings-emnlp.167,P15-1142,0,0.428845,"The table-question-answer triplets come from W IKI TABLE Q UESTIONS. We provide the logical forms as SQL plus alignments between question and logical form. In the bottom example, for instance, “the highest” ↔ ORDER BY and LIMIT 1, as indicated by both matching highlight color ( blue ) and circled-number labels ( 2 ). We address this lack by introducing S QUALL,1 the first large-scale semantic-parsing dataset with manual lexical-to-logical alignments; and we investigate the potential accuracy boosts achievable from such alignments. The starting point for S QUALL is W IKI TABLE Q UESTIONS (WTQ; Pasupat and Liang, 2015), containing data tables, English questions regarding the tables, and table-based answers. We manually enrich the 11,276-instance subset of WTQ’s training data that is translatable to SQL 1 Equal contribution; listed in alphabetical order. S QUALL =“SQL+QUestion pairs ALigned Lexically”. 1849 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1849–1864 c November 16 - 20, 2020. 2020 Association for Computational Linguistics by providing expert annotations, consisting not only of target logical forms in SQL, but also labeled alignments between the input question tokens"
2020.findings-emnlp.167,P17-1089,0,0.0288395,"dataset release. Dua et al. (2020) show that these annotator rationales improve model accuracy for a given annotation budget on machine reading comprehension. The alignments we provide could, at a stretch, be considered a type of rationale for the output SQL annotation. Text-to-SQL Datasets There is growing interest in both the database and NLP communities in text-to-SQL applications. Widely-used domainspecific datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996; Popescu et al., 2003), Restaurants (Tang and Mooney, 2000; Popescu et al., 2003), and Scholar (Iyer et al., 2017). WikiSQL (Zhong et al., 2017) is among the first large-scale datasets with questionlogical form pairs querying a wide range of data tables extracted from Wikipedia, but WikiSQL’s logical forms are generated from a limited set of templates. In contrast, WTQ questions are authored by humans under no specific constraints, and as a result WTQ includes more diverse semantics and logical operations. The family of Spider datasets (Yu et al., 2018, 2019a,b) contain queries even more complex than in WTQ, including a higher percentage of nested queries and multiple table joins. We leave extensions of l"
2020.findings-emnlp.167,P16-1003,0,0.0413284,"Missing"
2020.findings-emnlp.167,N19-1357,0,0.0233556,"e entropy of the attention distributions in the question-to-column (q2c), column-to-question (c2q) and decoder-to-question (d2q) modules, comparing models trained with supervised encoder/decoder attention, none (SEQ 2 SEQ+ ), or both strategies (ALIGN). judgments. This is an arguably surprising benefit, since the supervised decoder was not trained with q2c supervision, and so one might have expected it to perform similarly to SEQ 2 SEQ+ . However, one needs to be careful in interpreting these results, as machine-induced attention distributions are not intended for direct human interpretation (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Qualitative Analysis Our additional supervision helps when the question has little textual overlap with the referred columns. Figure 3 shows an example. With finer-grained supervision, ALIGN learns the column “Serial Name” corresponds to the question word “show”, but SEQ 2 SEQ+ selects the wrong column “Co-Star”. 7 Related Work Attention and Alignments Explicit supervision for attention mechanisms (Bahdanau et al., 2015) is helpful for many tasks, including machine translation (Liu et al., 2016; Mi et al., 2016), image captioning (Liu et al., 2017), and visual qu"
2020.findings-emnlp.167,P18-1033,0,0.0521508,"Missing"
2020.findings-emnlp.167,D14-1162,0,0.0824568,"Missing"
2020.findings-emnlp.167,2020.acl-main.742,0,0.597059,"es correspond to spans in the input questions. We used our alignment to generate gold selection spans, filtering out instances where literal values could not be reconstructed through fuzzy match from the gold spans. After post-processing, S QUALL contained 11,276 table-question-answer triplets with logical form and lexical alignment annotations. (State-of-the-Art)4 Base Model: Seq2seq with Attention and Copying 4 Recent state-of-the-art text-to-SQL models extend the sequence-to-sequence (seq2seq) framework with attention and copying mechanisms (Zhong et al., 2017; Dong and Lapata, 2016, 2018; Suhr et al., 2020, inter alia). We adopt this strong neural paradigm as our base model. The seq2seq model generates one output token at a time via a probability distribution conditioned on both the input sequence representations and the partially-generated Q|y| output sequence: P (y |x) = i=1 P (yi |y<i , x), where x and y are the feature representations for the input and output sequences, and <i denotes a prefix. The last token of y must be a special <STOP> token that terminates the output generation. The per-token probability distribution is modeled through Long-Short Term Memory networks (LSTMs, Hochreiter"
2020.ngt-1.5,D19-1165,0,0.53233,"%2d 2 Related Work derstanding tasks. To the best of our knowledge, this is the first work using meta-learning for fewshot NMT adaptation. Our goal for few-shot NMT adaptation is to adapt a pre-trained NMT model (e.g. trained on general domain data) to new domains (e.g. medical domain) with a small amount of training examples. Chu et al. (2018) surveyed several recent approaches that address the shortcomings of traditional fine-tuning when applied to domain adaptation. Our work distinguishes itself from prior work by learning to fine-tune with tiny amounts of training examples. Most recently, Bapna et al. (2019) proposed a simple approach for adaptation in NMT. The approach consists of injecting task specific adapter layers into a pre-trained model. These adapters enable the model to adapt to new tasks as it introduces a bottleneck in the architecture that makes it easier to adapt. Our approach uses a similar model architecture, however, instead of injecting a new adapter for each task separately, M ETA -MT uses a single adapter layer, and meta-learns a better initialization for this layer that can easily be fine-tuned to new domains with very few training examples. Similar to our goal, Michel and Ne"
2020.ngt-1.5,W19-5321,0,0.0122525,"18) of size 40k. In all cases, the baseline machine translation system is a neural English to German (En-De) transformer model (Vaswani et al., 2017), initially trained on 5.2M sentences filtered from the the standard parallel data (Europarl-v9, CommonCrawl, NewsCommentary-v14, wikititles-v1 and Rapid2019) from the WMT-19 shared task (Barrault et al., 2019). We use WMT14 and WMT19 newtests as validation and test sets respectively. The baseline system scores 37.99 BLEU on the full WMT19 newstest which compares favorably with strong single system baselines at WMT19 shared task (Ng et al., 2019; Junczys-Dowmunt, 2019). For meta-learning, we use the MAML algorithm as described in Alg 1. To minimize memory consumption, we ignored the second order gradient terms from Eq 2. We implement the First-Order MAML approximation (FoMAML) as described in Finn et al. (2017). We also experimented with the first-order meta-learning algorithm Reptile (Nichol et al., 2018). We found that since Reptile doesn’t directly account for the performance on the task query set, along with the large model capacity of the Transformer architecture, it can easily over-fit to the support set, thus achieving almost perfect performance on t"
2020.ngt-1.5,I17-2004,0,0.0189166,"ent domains. However, this approach assumes that these domains are static and known at training time, while M ETA -MT can dynamically generalize to totally new domains, previously unseen at meta-training time. Several approaches have been proposed for lightweight adaptation of NMT systems. Vilar (2018) introduced domain specific gates to control the contribution of hidden units feeding into the next layer. However, Bapna et al. (2019) showed that this introduced a limited amount of per-domain capacity; in addition, the learned gates are not guaranteed to be easily adaptable to unseen domains. Khayrallah et al. (2017) proposed a lattice search algorithm for NMT adaptation, however, this algorithm assumes access to lattices generated from a phrase based machine translation system. Our meta-learning strategy mirrors that of Gu et al. (2018) in the low resource translation setting, as well as Wu et al. (2019) for cross-lingual named entity recognition with minimal resources, Mi et al. (2019) for low-resource natural language generation in task-oriented dialogue systems, and Dou et al. (2019) for low-resource natural language un3 Approach: Meta-Learning for Few-Shot NMT Adaptation Neural Machine Translation sy"
2020.ngt-1.5,W17-3204,0,0.0341511,"y 4, 000 translated words (300 parallel sentences). 1 Hal Daumé III Microsoft Research & University of Maryland me@hal3.name Introduction Neural Machine Translation (NMT) systems (Bahdanau et al., 2016; Sutskever et al., 2014) are usually trained on large general-domain parallel corpora to achieve state-of-the-art results (Barrault et al., 2019). Unfortunately, these generic corpora are often qualitatively different from the target domain of the translation system. Moreover, NMT models trained on one domain tend to perform poorly when translating sentences in a significantly different domain (Koehn and Knowles, 2017; Chu and Wang, 2018). A widely used approach for adapting NMT is domain adaptation by fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Sennrich et al., 2016), where a model is first trained on general-domain data and then adapted by continuing the training on a smaller amount of in-domain data. This approach often leads to empirical improvements in the targeted 1 Code Release: We make the code publicly available online: https://www.dropbox.com/s/ jguxb75utg1dmxl/meta-mt.zip?dl=0 43 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 43–53"
2020.ngt-1.5,C18-1111,0,0.281836,"(300 parallel sentences). 1 Hal Daumé III Microsoft Research & University of Maryland me@hal3.name Introduction Neural Machine Translation (NMT) systems (Bahdanau et al., 2016; Sutskever et al., 2014) are usually trained on large general-domain parallel corpora to achieve state-of-the-art results (Barrault et al., 2019). Unfortunately, these generic corpora are often qualitatively different from the target domain of the translation system. Moreover, NMT models trained on one domain tend to perform poorly when translating sentences in a significantly different domain (Koehn and Knowles, 2017; Chu and Wang, 2018). A widely used approach for adapting NMT is domain adaptation by fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Sennrich et al., 2016), where a model is first trained on general-domain data and then adapted by continuing the training on a smaller amount of in-domain data. This approach often leads to empirical improvements in the targeted 1 Code Release: We make the code publicly available online: https://www.dropbox.com/s/ jguxb75utg1dmxl/meta-mt.zip?dl=0 43 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 43–53 c Online, July 10, 2"
2020.ngt-1.5,D18-2012,0,0.019191,"n over three different meta-test sets. For evaluation, we use BLEU (Papineni et al., 2002). We measure case-sensitive de-tokenized BLEU with SacreBLEU (Post, 2018). All results use beam search with a beam of size five. learning rate increases linearly for 4, 000 steps to 7e − 4, after which it is decayed proportionally to the inverse square root of the number of steps. For meta-learning, we used a meta-batch size of 1. We optimized the meta-learning loss function using Adam with a learning rate of 1e − 5 and default parameters for β1 , β2 . All data is pre-processed with joint sentencepieces (Kudo and Richardson, 2018) of size 40k. In all cases, the baseline machine translation system is a neural English to German (En-De) transformer model (Vaswani et al., 2017), initially trained on 5.2M sentences filtered from the the standard parallel data (Europarl-v9, CommonCrawl, NewsCommentary-v14, wikititles-v1 and Rapid2019) from the WMT-19 shared task (Barrault et al., 2019). We use WMT14 and WMT19 newtests as validation and test sets respectively. The baseline system scores 37.99 BLEU on the full WMT19 newstest which compares favorably with strong single system baselines at WMT19 shared task (Ng et al., 2019; Jun"
2020.ngt-1.5,2015.iwslt-evaluation.11,0,0.0912891,"e Introduction Neural Machine Translation (NMT) systems (Bahdanau et al., 2016; Sutskever et al., 2014) are usually trained on large general-domain parallel corpora to achieve state-of-the-art results (Barrault et al., 2019). Unfortunately, these generic corpora are often qualitatively different from the target domain of the translation system. Moreover, NMT models trained on one domain tend to perform poorly when translating sentences in a significantly different domain (Koehn and Knowles, 2017; Chu and Wang, 2018). A widely used approach for adapting NMT is domain adaptation by fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Sennrich et al., 2016), where a model is first trained on general-domain data and then adapted by continuing the training on a smaller amount of in-domain data. This approach often leads to empirical improvements in the targeted 1 Code Release: We make the code publicly available online: https://www.dropbox.com/s/ jguxb75utg1dmxl/meta-mt.zip?dl=0 43 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 43–53 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d 2 Related Work"
2020.ngt-1.5,P18-2050,0,0.0609798,"et al. (2019) proposed a simple approach for adaptation in NMT. The approach consists of injecting task specific adapter layers into a pre-trained model. These adapters enable the model to adapt to new tasks as it introduces a bottleneck in the architecture that makes it easier to adapt. Our approach uses a similar model architecture, however, instead of injecting a new adapter for each task separately, M ETA -MT uses a single adapter layer, and meta-learns a better initialization for this layer that can easily be fine-tuned to new domains with very few training examples. Similar to our goal, Michel and Neubig (2018) proposed a space efficient approach to adaptation that learns domain specific biases to the output vocabulary. This enables large-scale personalization for NMT models when small amounts of data are available for a lot of different domains. However, this approach assumes that these domains are static and known at training time, while M ETA -MT can dynamically generalize to totally new domains, previously unseen at meta-training time. Several approaches have been proposed for lightweight adaptation of NMT systems. Vilar (2018) introduced domain specific gates to control the contribution of hidd"
2020.ngt-1.5,D18-1398,0,0.0222512,"been proposed for lightweight adaptation of NMT systems. Vilar (2018) introduced domain specific gates to control the contribution of hidden units feeding into the next layer. However, Bapna et al. (2019) showed that this introduced a limited amount of per-domain capacity; in addition, the learned gates are not guaranteed to be easily adaptable to unseen domains. Khayrallah et al. (2017) proposed a lattice search algorithm for NMT adaptation, however, this algorithm assumes access to lattices generated from a phrase based machine translation system. Our meta-learning strategy mirrors that of Gu et al. (2018) in the low resource translation setting, as well as Wu et al. (2019) for cross-lingual named entity recognition with minimal resources, Mi et al. (2019) for low-resource natural language generation in task-oriented dialogue systems, and Dou et al. (2019) for low-resource natural language un3 Approach: Meta-Learning for Few-Shot NMT Adaptation Neural Machine Translation systems are not robust to domain shifts (Chu and Wang, 2018). It is a highly desirable characteristic of the system to be adaptive to any domain shift using weak supervision without degrading the performance on the general doma"
2020.ngt-1.5,N19-4009,0,0.0160912,"g number of words instead of number of sentences to avoid introducing any advantages for domains with longer sentences. 4.1 classical fine-tuning would perform using the same data albeit in a different configuration. (D) M ETA -MT: Our proposed approach from Alg 1. In this setup, we use N adaptation tasks T in Dmeta-train , each with a support set of size K words to perform Meta-Learning. Second order meta-gradients are ignored to decrease the computational complexity. 4.2 Model Architecture and Implementation Details We use the Transformer Model (Vaswani et al., 2017) implemented in fairseq (Ott et al., 2019). In this work, we use a transformer model with a modified architecture that can facilitate better adaptation. We use “Adapter Modules” (Houlsby et al., 2019; Bapna et al., 2019) which introduce an extra layer after each transformer block that can enable more efficient tuning of the models. Following Bapna et al. (2019), we augment the Transformer model with feed-forward adapters: simple single hiddenlayer feed-forward networks, with a nonlinear activation function between the two projection layers. These adapter modules are introduced after the Layer Norm and before the residual connection la"
2020.ngt-1.5,N18-2080,0,0.141043,"ns with very few training examples. Similar to our goal, Michel and Neubig (2018) proposed a space efficient approach to adaptation that learns domain specific biases to the output vocabulary. This enables large-scale personalization for NMT models when small amounts of data are available for a lot of different domains. However, this approach assumes that these domains are static and known at training time, while M ETA -MT can dynamically generalize to totally new domains, previously unseen at meta-training time. Several approaches have been proposed for lightweight adaptation of NMT systems. Vilar (2018) introduced domain specific gates to control the contribution of hidden units feeding into the next layer. However, Bapna et al. (2019) showed that this introduced a limited amount of per-domain capacity; in addition, the learned gates are not guaranteed to be easily adaptable to unseen domains. Khayrallah et al. (2017) proposed a lattice search algorithm for NMT adaptation, however, this algorithm assumes access to lattices generated from a phrase based machine translation system. Our meta-learning strategy mirrors that of Gu et al. (2018) in the low resource translation setting, as well as W"
2020.ngt-1.5,P02-1040,0,0.106872,"cribed in §4. We simulate domain adaptation problems by sub-sampling tasks with 4k English tokens for the support set, and 32k tokens for the query set. We study the effect of varying the size of the query and the support sets in §4.5. We use N = 160 tasks for the meta-training dataset Dmeta-train , where we sample 16 tasks from each of the ten different domains. We use a meta-validation Dmeta-test and meta-test Dmeta-test sets of size 10, where we sample a single task from each domain. We report the mean and standard-deviation over three different meta-test sets. For evaluation, we use BLEU (Papineni et al., 2002). We measure case-sensitive de-tokenized BLEU with SacreBLEU (Post, 2018). All results use beam search with a beam of size five. learning rate increases linearly for 4, 000 steps to 7e − 4, after which it is decayed proportionally to the inverse square root of the number of steps. For meta-learning, we used a meta-batch size of 1. We optimized the meta-learning loss function using Adam with a learning rate of 1e − 5 and default parameters for β1 , β2 . All data is pre-processed with joint sentencepieces (Kudo and Richardson, 2018) of size 40k. In all cases, the baseline machine translation sys"
2020.ngt-1.5,W18-6319,0,0.01689,"nglish tokens for the support set, and 32k tokens for the query set. We study the effect of varying the size of the query and the support sets in §4.5. We use N = 160 tasks for the meta-training dataset Dmeta-train , where we sample 16 tasks from each of the ten different domains. We use a meta-validation Dmeta-test and meta-test Dmeta-test sets of size 10, where we sample a single task from each domain. We report the mean and standard-deviation over three different meta-test sets. For evaluation, we use BLEU (Papineni et al., 2002). We measure case-sensitive de-tokenized BLEU with SacreBLEU (Post, 2018). All results use beam search with a beam of size five. learning rate increases linearly for 4, 000 steps to 7e − 4, after which it is decayed proportionally to the inverse square root of the number of steps. For meta-learning, we used a meta-batch size of 1. We optimized the meta-learning loss function using Adam with a learning rate of 1e − 5 and default parameters for β1 , β2 . All data is pre-processed with joint sentencepieces (Kudo and Richardson, 2018) of size 40k. In all cases, the baseline machine translation system is a neural English to German (En-De) transformer model (Vaswani et a"
2020.ngt-1.5,P16-1009,0,0.0233158,"(Bahdanau et al., 2016; Sutskever et al., 2014) are usually trained on large general-domain parallel corpora to achieve state-of-the-art results (Barrault et al., 2019). Unfortunately, these generic corpora are often qualitatively different from the target domain of the translation system. Moreover, NMT models trained on one domain tend to perform poorly when translating sentences in a significantly different domain (Koehn and Knowles, 2017; Chu and Wang, 2018). A widely used approach for adapting NMT is domain adaptation by fine-tuning (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Sennrich et al., 2016), where a model is first trained on general-domain data and then adapted by continuing the training on a smaller amount of in-domain data. This approach often leads to empirical improvements in the targeted 1 Code Release: We make the code publicly available online: https://www.dropbox.com/s/ jguxb75utg1dmxl/meta-mt.zip?dl=0 43 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 43–53 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d 2 Related Work derstanding tasks. To the best of our knowledge, this"
2020.ngt-1.5,N19-1209,0,0.0295565,"Missing"
2020.ngt-1.5,tiedemann-2012-parallel,0,0.0160778,"the impact of the support and the query sizes used for meta-learning? (§4.5) 3. What is the effect of the NMT model architecture on performance? (§4.6) In our experiments, we train M ETA -MT only on simulated data, where we simulate a few-shot domain adaptation setting as described in §3.2. This is possible because M ETA -MT learns model parameters θ that can generalize to future adaptation tasks by optimizing the meta-objective function in Eq 2. We train and evaluate M ETA -MT on a collection of ten different datasets. All of these datasets are collected from the Open Parallel Corpus (OPUS) (Tiedemann, 2012), and are publicly available online. The datasets cover a variety of diverse domains that should enable us to evaluate our proposed approach. The datasets we consider are: 1. Bible: a parallel corpus created from translations of the Bible (Christodouloupoulos and Steedman, 2015). 2. European Central Bank: website and documentations from the European Central Bank. 3. KDE: a corpus of KDE4 localization files. 4. Quran: a collection of Quran translations compiled by the Tanzil project. 5. WMT news test sets: a parallel corpus of (1) where α is the learning rate and L is the task loss function. Th"
2020.ngt-1.5,P12-1033,0,0.0285825,"arning Approach Support Size: 4k, Query Size: 16k Meta-MT Meta-MT-0 Fine Tuning on Task 26.35 23.48 26.12 21.21 Meta-MT 24.70 20.95 20.33 20.83 25.17 24.36 BLEU BLEU 25.68 Classical Fine-Tune Baseline 24.18 24.42 0.00 16000 32000 Size of Support Set (En Tokens) 64000 Adapter Transformer Architecture Figure 5: M ETA -MT and fine-tuning adaptation performance on the meta-test set Dmeta-test vs different query set sizes per adaptation task. Figure 6: BLEU scores reported for two different model architectures: Adapter Transformer (Bapna et al., 2019) (Left), and the Transformer base architecture (Vaswani et al., 2012) (Right). that for reasonably small size of the support set (4k and 8k), M ETA -MT outperforms the classical fine-tuning baseline. However, when the data size increase (16k to 64), M ETA -MT is outperformed by the fine-tuning baseline. This happens because for a larger support size, e.g. 64k, we only have access to 10 meta-training tasks in Dmeta-train , this is not enough to generalize to new unseen adaptation tasks, and M ETA -MT over-fits to the training tasks from Dmeta-train , however, the performance degrades and doesn’t generalize to Dmeta-test . To understand more directly the impact o"
2020.ngt-1.5,W17-4719,0,\N,Missing
2020.ngt-1.5,W19-5301,0,\N,Missing
2021.emnlp-main.756,P17-1171,0,0.0199906,"ey use standard IR techniques to find relevant passages and match them with answer strings. These methods succeed for simple questions where terms overlap with evidence passages, This no longer holds for multi-hop questions that require a reasoning chain as evidence to the answer: evidence pieces do not overlap with the question but rather depend on the previous evidence pieces. Unsupervised IR methods cannot capture such implicit relations. D IST DR removes the burden of little textual overlap through dense retrieval and its iterative process retrieves better evidence. Open-domain QA systems Chen et al. (2017) first combine information retrieval and (neural) reading comprehension for open-domain QA. Several works aim to improve the neural reader (Clark and Gardner, 2018; Wang et al., 2018, inter alia), or use generative models to compose an answer (Lewis et al., 2020; Izacard and Grave, 2021, inter alia). Recent progress (Karpukhin et al., 2020; Xiong et al., 2021a,b, inter alia) uses dense retrieval to aid both single-hop and multi-hop questions. However, a crucial distinction is that these approaches assume the evidence is given for training, while D IST DR iteratively finds evidence and uses it"
2021.emnlp-main.756,2020.acl-main.501,0,0.0136931,"ence pieces), building a reasoning chain to find the answer. Our work focuses on training ODQA systems 9612 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9612–9622 c November 7–11, 2021. 2021 Association for Computational Linguistics without these expensive annotations (Section 2): we only start with a question-answer pair. With that starting point, we use distant supervision to infer which evidence helps us get to the answer. The technical challenge is how to find these evidence from millions of candidates. Previous methods (Joshi et al., 2017; Cheng et al., 2020) use term matching (e.g., TF - IDF) for evidence retrieval, but their goal is a single piece of evidence: linking a question to a passage. As shown in Figure 1, the key to finding some evidence pieces does not appear in the question: for example, you only know to figure out that Rutgers University is in New Jersey after learning where Professor Cheong works. Fortunately, navigating to an answer given a question from a search engine is not impossible: humans do it every day, building on their existing knowledge toward the answer (Russell, 2019). With each round of searching to find additional c"
2021.emnlp-main.756,P19-1612,0,0.0172435,"relevant to question. Returning to our running example, while “named after a lawn ornament store in Wayne, New Jersey” has the state where Professor Cheong works, it is irrelevant to condensed matter physics. Thus, a reader filters spurious evidence (Reader Filter) to keep D IST DR on target. 3.1 Preliminary: Fully-Supervised ODQA This section reviews state-of-the-art systems for fully-supervised ODQA, where a dense retriever finds evidence from a large corpus, and a reader— multi-tasked with evidence reranking and span extraction—outputs a span as the answer. Dense Retrieval Dense retrieval (Lee et al., 2019) is based on a dual-encoder architecture, which uses BERT to represent both the query q and the passage p with dense vectors. The model learns a scoring function (e.g., dot product) between question and passage vectors: f (q, p) = sim(EncQ (q), EncP (p)). qt = [q; z1 ; . . . ; zt−1 ], (2) and retrieve a new evidence piece zt . During inference, a beam search finds the top-k evidence, where the score is the product of individual evidence pieces’ score. For training, given positive evidence z + ≡ z1+ , . . . , zn+ (In Figure 1, z1+ : Sang-Wook Cheong, z2+ : Rutgers University) and a set of negat"
2021.emnlp-main.756,N19-1246,0,0.123739,"wrong evidence, often because the extracted evidence only includes one span that matches the answer type, therefore the reader confidently outputs the span for the wrong reason. In the third example in Table 6, Nassau County is the only county it sees, and therefore the model has stumbled upon the right answer erroneously. Building a model with faithful predictions is an important ongoing research topic (Jacovi and Goldberg, 2020). 6 Related Work Question Answering Datasets There is growing interest in NLP communities to build large-scale datasets (Rajpurkar et al., 2016; Jia and Liang, 2017; Dua et al., 2019, inter alia) for QA research. In addition to questions and answers, benchmark datasets often include annotated evidence, but it requires significant annotation protocol design and human annotations. In SQUAD (Rajpurkar et al., 2016), among the first large-scale reading comprehension datasets, annotators write questions conditioned on a passage, which creates dataset artifacts (Jia and Liang, 2017). To overcome dataset artifacts, NATURAL Q UESTIONS (Kwiatkowski et al., 2019) use real Google queries as questions and ask annotators to label both evidence passages and short answers. But such anno"
2021.emnlp-main.756,D18-1167,0,0.052637,"Missing"
2021.emnlp-main.756,D18-1134,1,0.853519,"QA, annotators are presented with a linked Wikipedia passages, as pilot studies indicate that it is difficult to ask a meaningful multi-hop question with arbitrary passages. However, some questions in H OT P OT QA include shortcuts that are answerable by a single passage (Min et al., 2019b), which is confirmed by our analysis (Section 5.2). Distant supervision has been successfully adopted for many NLP tasks such as relation extraction (Mintz et al., 2009). Recent work builds QA datasets with distant supervision, such as T RIVIAQA (Joshi et al., 2017), S EARCH QA (Dunn et al., 2017), QBL INK (Elgohary et al., 2018) by automatically gathering evidence documents from a corpus as distant supervision for available questionanswer pairs. They use standard IR techniques to find relevant passages and match them with answer strings. These methods succeed for simple questions where terms overlap with evidence passages, This no longer holds for multi-hop questions that require a reasoning chain as evidence to the answer: evidence pieces do not overlap with the question but rather depend on the previous evidence pieces. Unsupervised IR methods cannot capture such implicit relations. D IST DR removes the burden of l"
2021.emnlp-main.756,2021.eacl-main.74,0,0.0325608,"evidence pieces do not overlap with the question but rather depend on the previous evidence pieces. Unsupervised IR methods cannot capture such implicit relations. D IST DR removes the burden of little textual overlap through dense retrieval and its iterative process retrieves better evidence. Open-domain QA systems Chen et al. (2017) first combine information retrieval and (neural) reading comprehension for open-domain QA. Several works aim to improve the neural reader (Clark and Gardner, 2018; Wang et al., 2018, inter alia), or use generative models to compose an answer (Lewis et al., 2020; Izacard and Grave, 2021, inter alia). Recent progress (Karpukhin et al., 2020; Xiong et al., 2021a,b, inter alia) uses dense retrieval to aid both single-hop and multi-hop questions. However, a crucial distinction is that these approaches assume the evidence is given for training, while D IST DR iteratively finds evidence and uses it to improve the model. Min et al. (2019a) also use hard-EM for weakly-supervised QA, but— orthogonal to our approach—they assume the evidence is given and find the most likely answer mentions in the evidence, while we aim to find evidence from a large corpus. 7 Conclusion atively finding"
2021.emnlp-main.756,2020.acl-main.386,0,0.0208478,"Missing"
2021.emnlp-main.756,D19-1284,0,0.120894,"l of the intermediate evidence pieces (e.g., in Figure 1, the evidence pieces for Sang-Wook Cheong’s workplace which point you to Rutgers University’s location) needed for the answer. Creating such intricate training data is expensive. For example, Kwiatkowski et al. (2019) use additional experts to justify the correctness of annotated evidence. The annotation protocol is even more nuanced for multi-hop questions. For example, Yang et al. (2018) ask annotators to write multi-hop questions based on two linked Wikipedia passages as a pre-defined reasoning chain, which creates dataset artifacts (Min et al., 2019b). While plenty of question-answer pairs are available without evidence labels, we cannot directly train SOTA models on such data. Open-domain question answering (ODQA) takes a question, retrieves evidence from a large corpus, and finds an answer based on that evidence (Voorhees et al., 1999). With the help of large scale datasets, state-of-the-art approaches to QA (Karpukhin et al., 2020; Zhao et al., 2021, inter alia) can answer both simple questions that require only a single evidence piece (i.e., one passage); and more challenging multi-hop questions: computers must jump or “hop” from pas"
2021.emnlp-main.756,P19-1416,0,0.209437,"l of the intermediate evidence pieces (e.g., in Figure 1, the evidence pieces for Sang-Wook Cheong’s workplace which point you to Rutgers University’s location) needed for the answer. Creating such intricate training data is expensive. For example, Kwiatkowski et al. (2019) use additional experts to justify the correctness of annotated evidence. The annotation protocol is even more nuanced for multi-hop questions. For example, Yang et al. (2018) ask annotators to write multi-hop questions based on two linked Wikipedia passages as a pre-defined reasoning chain, which creates dataset artifacts (Min et al., 2019b). While plenty of question-answer pairs are available without evidence labels, we cannot directly train SOTA models on such data. Open-domain question answering (ODQA) takes a question, retrieves evidence from a large corpus, and finds an answer based on that evidence (Voorhees et al., 1999). With the help of large scale datasets, state-of-the-art approaches to QA (Karpukhin et al., 2020; Zhao et al., 2021, inter alia) can answer both simple questions that require only a single evidence piece (i.e., one passage); and more challenging multi-hop questions: computers must jump or “hop” from pas"
2021.emnlp-main.756,P09-1113,0,0.158318,"ly question–answer pairs, which is significantly cheaper. Annotation is more fraught for multi-hop QA 9619 datasets (Yang et al., 2018). To construct H OTP OT QA, annotators are presented with a linked Wikipedia passages, as pilot studies indicate that it is difficult to ask a meaningful multi-hop question with arbitrary passages. However, some questions in H OT P OT QA include shortcuts that are answerable by a single passage (Min et al., 2019b), which is confirmed by our analysis (Section 5.2). Distant supervision has been successfully adopted for many NLP tasks such as relation extraction (Mintz et al., 2009). Recent work builds QA datasets with distant supervision, such as T RIVIAQA (Joshi et al., 2017), S EARCH QA (Dunn et al., 2017), QBL INK (Elgohary et al., 2018) by automatically gathering evidence documents from a corpus as distant supervision for available questionanswer pairs. They use standard IR techniques to find relevant passages and match them with answer strings. These methods succeed for simple questions where terms overlap with evidence passages, This no longer holds for multi-hop questions that require a reasoning chain as evidence to the answer: evidence pieces do not overlap wit"
2021.emnlp-main.756,D17-1215,0,0.0144287,", D IST DR finds the wrong evidence, often because the extracted evidence only includes one span that matches the answer type, therefore the reader confidently outputs the span for the wrong reason. In the third example in Table 6, Nassau County is the only county it sees, and therefore the model has stumbled upon the right answer erroneously. Building a model with faithful predictions is an important ongoing research topic (Jacovi and Goldberg, 2020). 6 Related Work Question Answering Datasets There is growing interest in NLP communities to build large-scale datasets (Rajpurkar et al., 2016; Jia and Liang, 2017; Dua et al., 2019, inter alia) for QA research. In addition to questions and answers, benchmark datasets often include annotated evidence, but it requires significant annotation protocol design and human annotations. In SQUAD (Rajpurkar et al., 2016), among the first large-scale reading comprehension datasets, annotators write questions conditioned on a passage, which creates dataset artifacts (Jia and Liang, 2017). To overcome dataset artifacts, NATURAL Q UESTIONS (Kwiatkowski et al., 2019) use real Google queries as questions and ask annotators to label both evidence passages and short answ"
2021.emnlp-main.756,2020.emnlp-main.88,0,0.0155452,"us, and using the evidence as distant supervision for model training. Without using any evidence labels, D IST DR matches the fully-supervised SOTA approaches on both multi-hop and single-hop QA benchmarks. Annotating evidence for existing questionanswer pairs is generally expensive, especially for complex questions. While D IST DR can accurately find evidence for arbitrary complex machine reading-style questions, future work needs to validate whether this can work for other types of questions. This could improve the reader to answer numerical reasoning (Dua et al., 2019), temporal reasoning (Ning et al., 2020), multi-model reasoning (Lei et al., 2018), or combination of these skills (Bartolo et al., 2020). Acknowledgments We thank CLIP members, Tianze Shi, anonymous reviewers and meta-reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the BETTER Program contract 201919051600005. Boyd-Graber is supported by NSF Grant IIS-1822494. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view o"
2021.emnlp-main.756,P17-1147,0,0.109744,"these passages evidence pieces), building a reasoning chain to find the answer. Our work focuses on training ODQA systems 9612 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9612–9622 c November 7–11, 2021. 2021 Association for Computational Linguistics without these expensive annotations (Section 2): we only start with a question-answer pair. With that starting point, we use distant supervision to infer which evidence helps us get to the answer. The technical challenge is how to find these evidence from millions of candidates. Previous methods (Joshi et al., 2017; Cheng et al., 2020) use term matching (e.g., TF - IDF) for evidence retrieval, but their goal is a single piece of evidence: linking a question to a passage. As shown in Figure 1, the key to finding some evidence pieces does not appear in the question: for example, you only know to figure out that Rutgers University is in New Jersey after learning where Professor Cheong works. Fortunately, navigating to an answer given a question from a search engine is not impossible: humans do it every day, building on their existing knowledge toward the answer (Russell, 2019). With each round of searching"
2021.emnlp-main.756,D16-1264,0,0.113201,"Missing"
2021.emnlp-main.756,N12-1087,0,0.0153616,"P (z |q) (For single-hop questions z is retrieved with single step; for multi-hop questions it takes multiple steps to find z: after the first retrieval step the query q is not just the original question bu a so appended ev dence as n Equa on 2 ) and selecting an answer from the question P (a |q, z) We use expec a on max m za on (EM) o nfer he latent variable z We compute the likelihood of each z given q yielding a vector of estimates zˆ (Es ep) hen upda e he mode parame ers based on zˆ (M-s ep) S nce ’s n rac ab e o enumera e a evidence candidates to compute the expectation we adopt hard-EM (Samdani et al 2012) and approxima e he E-s ep by p ck ng he mos ke y so u on as zˆ We pass over all questions in the training set and repeat this process for multiple iterations until the model converges retrieval steps for multi-hop questions specifically at each step t: zˆt = arg max P (ˆ zt |q, zˆ1 , ..., zˆt−1 ); zˆ∈Z (7) and single-step retrieval for single-hop questions We use an up-to-date retriever to find the top-k evidence from corpus (with beam search for the k-highest scoring chains Section 3 1) Given the retrieval output we will eventually need o re ra n he re r ever Th s requ res know ng which evid"
2021.emnlp-main.756,D18-1259,0,0.3683,"te-of-the-art systems use evidence labels for training, but acquiring labeled evidence pieces is expensive. State-of-the-art (SOTA) methods, however, are trained with all of the intermediate evidence pieces (e.g., in Figure 1, the evidence pieces for Sang-Wook Cheong’s workplace which point you to Rutgers University’s location) needed for the answer. Creating such intricate training data is expensive. For example, Kwiatkowski et al. (2019) use additional experts to justify the correctness of annotated evidence. The annotation protocol is even more nuanced for multi-hop questions. For example, Yang et al. (2018) ask annotators to write multi-hop questions based on two linked Wikipedia passages as a pre-defined reasoning chain, which creates dataset artifacts (Min et al., 2019b). While plenty of question-answer pairs are available without evidence labels, we cannot directly train SOTA models on such data. Open-domain question answering (ODQA) takes a question, retrieves evidence from a large corpus, and finds an answer based on that evidence (Voorhees et al., 1999). With the help of large scale datasets, state-of-the-art approaches to QA (Karpukhin et al., 2020; Zhao et al., 2021, inter alia) can answ"
2021.emnlp-main.756,2021.naacl-main.368,1,0.856608,"Missing"
2021.findings-acl.355,P19-1470,0,0.151338,"dataset of potentially expected stereotypes. An advantage of the dataset approach is re-usability, while an advantage of the post-hoc analysis approach is that it may capture stereotypes we had not thought of a priori. a premise p and relation type r, generate a hypothesis h that bears that relation to p. This framing enables us to explore what trained models have learned about inference, without providing explicit hypothesis prompts. For NLI, we focus on two finetuned GPT-2 models using the SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets. For CI, we use the COMET model (Bosselut et al., 2019), which is trained on the ATOMIC (Sap et al., 2018) dataset.3 More details are in Appendix A. 3 Our goal is to construct hypotheses like “The [TAR GET C ATEGORY ] person is cutting up fish for dinner.” To this end, we define a set of domains and target categories, and a set of context situations. Data Generation & Annotation We conduct experiments to study stereotypes with a focus on generative text inference tasks. To do that, we construct a list of stereotype domains and a list of target categories for each of the domains. We also manually create a list of underspecified, real-life context s"
2021.findings-acl.355,D15-1075,0,0.0464848,"n answering, and a post-hoc analysis of what a model actually produces, rather than a predefined dataset of potentially expected stereotypes. An advantage of the dataset approach is re-usability, while an advantage of the post-hoc analysis approach is that it may capture stereotypes we had not thought of a priori. a premise p and relation type r, generate a hypothesis h that bears that relation to p. This framing enables us to explore what trained models have learned about inference, without providing explicit hypothesis prompts. For NLI, we focus on two finetuned GPT-2 models using the SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets. For CI, we use the COMET model (Bosselut et al., 2019), which is trained on the ATOMIC (Sap et al., 2018) dataset.3 More details are in Appendix A. 3 Our goal is to construct hypotheses like “The [TAR GET C ATEGORY ] person is cutting up fish for dinner.” To this end, we define a set of domains and target categories, and a set of context situations. Data Generation & Annotation We conduct experiments to study stereotypes with a focus on generative text inference tasks. To do that, we construct a list of stereotype domains and a list of target categor"
2021.findings-acl.355,D19-1418,0,0.0218977,"“bias”). A major focus of past work has been on the domains of gender and race, across a variety of tasks including language modeling, coreference resolution, natural language inference, machine translation, and sentiment analysis (Sheng et al., 2019; Rudinger et al., 2018; Lu et al., 2018; Dinan et al., 2019; Rudinger et al., 2017; Kiritchenko and Mohammad, 2018); Blodgett et al. (2020) provide a review. There has simultaneously been a range of work aimed to mitigate problems of stereotyping in NLP systems, including many in the space of text generation (Sheng et al., 2020; He et al., 2019; Clark et al., 2019; Huang et al., 2020). In comparison to this line of work, our main extensions are (a) a broader range of domains considered, and (b) a specific focus on the generation of entailed text. Several very recent papers have also explored other stereotype domains, including disabilities (Hutchinson et al., 2020), and larger collections of domains similar to ours. For instance, two recently released datasets by Nadeem et al. (2020) and Nangia et al. (2020) provide example texts and measurements to determine if a language generation system exhibits stereotyping toward the domains of nationality, race,"
2021.findings-acl.355,D19-1339,0,0.0212484,"ust six stereotype domains, and we do not explicitly account for intersectionality. While our annotators are of diverse cultural backgrounds, another limitation is that there are only four, limiting the breadth of our analysis of annotator positionality. 2 Related Work Our work builds on a growing body of recent computational literature on stereotypes (often termed “bias”). A major focus of past work has been on the domains of gender and race, across a variety of tasks including language modeling, coreference resolution, natural language inference, machine translation, and sentiment analysis (Sheng et al., 2019; Rudinger et al., 2018; Lu et al., 2018; Dinan et al., 2019; Rudinger et al., 2017; Kiritchenko and Mohammad, 2018); Blodgett et al. (2020) provide a review. There has simultaneously been a range of work aimed to mitigate problems of stereotyping in NLP systems, including many in the space of text generation (Sheng et al., 2020; He et al., 2019; Clark et al., 2019; Huang et al., 2020). In comparison to this line of work, our main extensions are (a) a broader range of domains considered, and (b) a specific focus on the generation of entailed text. Several very recent papers have also explored"
2021.findings-acl.355,2020.findings-emnlp.291,0,0.0176047,"terature on stereotypes (often termed “bias”). A major focus of past work has been on the domains of gender and race, across a variety of tasks including language modeling, coreference resolution, natural language inference, machine translation, and sentiment analysis (Sheng et al., 2019; Rudinger et al., 2018; Lu et al., 2018; Dinan et al., 2019; Rudinger et al., 2017; Kiritchenko and Mohammad, 2018); Blodgett et al. (2020) provide a review. There has simultaneously been a range of work aimed to mitigate problems of stereotyping in NLP systems, including many in the space of text generation (Sheng et al., 2020; He et al., 2019; Clark et al., 2019; Huang et al., 2020). In comparison to this line of work, our main extensions are (a) a broader range of domains considered, and (b) a specific focus on the generation of entailed text. Several very recent papers have also explored other stereotype domains, including disabilities (Hutchinson et al., 2020), and larger collections of domains similar to ours. For instance, two recently released datasets by Nadeem et al. (2020) and Nangia et al. (2020) provide example texts and measurements to determine if a language generation system exhibits stereotyping tow"
2021.findings-acl.355,2021.acl-long.416,0,0.0675789,"Missing"
2021.findings-acl.355,2020.emnlp-main.154,0,0.0307546,"work aimed to mitigate problems of stereotyping in NLP systems, including many in the space of text generation (Sheng et al., 2020; He et al., 2019; Clark et al., 2019; Huang et al., 2020). In comparison to this line of work, our main extensions are (a) a broader range of domains considered, and (b) a specific focus on the generation of entailed text. Several very recent papers have also explored other stereotype domains, including disabilities (Hutchinson et al., 2020), and larger collections of domains similar to ours. For instance, two recently released datasets by Nadeem et al. (2020) and Nangia et al. (2020) provide example texts and measurements to determine if a language generation system exhibits stereotyping toward the domains of nationality, race, religion, profession, orientation, disability, age, appearance, socioeconomic status, and gender. Li et al. (2020) probes transformer4053 based question answering models on stereotypes towards gender, nationality, religion, ethnicity domains. Here, question/answer pairs are constructed where a particular answer either does or does not contain a known stereotype. Our analysis is similar to these, with a slightly broader set of domains, a focus on in"
2021.findings-acl.355,Q19-1043,0,0.0213357,"rget categories, we are much more concerned with how each hypothesis is perceived by a human reader. We observe some cases in which the models generate similar outputs across several target categories, but for which the generated text is highly stereotyped and thus may cause representational harms. Finally, from human judgments, though our work is limited to US culture and the backgrounds of our four annotators, we find that people’s different backgrounds influence their perceptions of stereotypes. Even though this might result in lower agreement scores, such diversity can be actually useful (Pavlick and Kwiatkowski, 2019) in helping to explore the problem space. Overall, when deploying a system, it is important to make a wise consideration on annotators’ backgrounds. Considering annotators of different age, professions, education, and culture might give a multiplicity of valuable perspective on stereotypes. Acknowledgments The authors are grateful to all the reviewers who have provided helpful suggestions to improve this work. We also thank the CLIP lab at the University of Maryland for comments on previous drafts. References Amanda Blackhorse. 2017. Native American? American Indian? Nope. Indian Country Today"
2021.findings-acl.355,W17-1609,1,0.904808,"Missing"
2021.findings-acl.355,N18-1101,0,0.0344423,"lysis of what a model actually produces, rather than a predefined dataset of potentially expected stereotypes. An advantage of the dataset approach is re-usability, while an advantage of the post-hoc analysis approach is that it may capture stereotypes we had not thought of a priori. a premise p and relation type r, generate a hypothesis h that bears that relation to p. This framing enables us to explore what trained models have learned about inference, without providing explicit hypothesis prompts. For NLI, we focus on two finetuned GPT-2 models using the SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) datasets. For CI, we use the COMET model (Bosselut et al., 2019), which is trained on the ATOMIC (Sap et al., 2018) dataset.3 More details are in Appendix A. 3 Our goal is to construct hypotheses like “The [TAR GET C ATEGORY ] person is cutting up fish for dinner.” To this end, we define a set of domains and target categories, and a set of context situations. Data Generation & Annotation We conduct experiments to study stereotypes with a focus on generative text inference tasks. To do that, we construct a list of stereotype domains and a list of target categories for each of the domains. We a"
2021.findings-acl.355,2020.emnlp-demos.6,0,0.0983149,"Missing"
2021.findings-acl.355,D18-1009,0,0.0204632,"mbled these lists. For religion, nationality, race, socio, and politics, we mostly follow the lists from outside resources (such as Pew, the World Atlas, and Wikipedia; see Appendix C); for gender, we manually create the list. Note that many categories have multiple possible labels; we attempt to use ones that are currently generally benign and affirming, to avoid triggering stereotypical inferences based on an explicitly negative represen3 We note that even when CI is not framed as a generative task, CI datasets have been created using generative textual inference models (Zhang et al., 2017; Zellers et al., 2018). 4054 tation of the target category4 . For instance, we use formerly incarcerated person instead of felon and Black or African American instead of older and/or related derogatory terms.5 This choice, however, means that our results do not capture the full extent of stereotypes, as more derogatory terms often come with stronger stereotypical inferences, even for the same category (Devine and Baker, 1991). Table 2 is the list of our 71 target categories, which also includes spelling variations for some categories (e.g., presence or absence of a hyphen). In our analysis, we merge multiple terms"
2021.naacl-main.368,N19-1423,0,0.0417048,"Missing"
2021.naacl-main.368,P19-1612,0,0.0510493,"on 2) in the dense space to find and cache the most relevant candidate chains structured text of Wikipedia with its structured hyperlinks. While they show promise on bench- and iteratively compose the query by appending marks, it’s difficult to extend them beyond aca- the retrieval history. We improve the retrieval by encouraging the representation to discriminate hard demic testbeds because real-world datasets often negative evidence chains from the correct chains, lack this structure. For example, medical records which are refreshed by the model. lack links between reports. Dense retrieval (Lee et al., 2019; Guu et al., We evaluate Beam Dense Retrieval (B EAM DR) 2020; Karpukhin et al., 2020, inter alia) provides a on H OT P OT QA (Yang et al., 2018), a multi4635 1 Introduction Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4635–4641 June 6–11, 2021. ©2021 Association for Computational Linguistics hop question answering benchmark. When retrieving evidence chains directly from the corpus (full retrieval), B EAM DR is competitive to the state-of-the-art cascade reranking systems that use Wikipedi"
2021.naacl-main.368,D18-1167,0,0.0748215,"oduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the focus of this work. Given a complex question, researcher"
2021.naacl-main.368,D19-1605,1,0.852995,"y, and use traditional sparse IR systems to select the passage, which complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zha"
2021.naacl-main.368,2020.emnlp-main.466,0,0.0605023,"sparse IR systems to select the passage, which complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by t"
2021.naacl-main.368,D18-1134,1,0.845231,"EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the focus of this work. Given a complex question, researchers have in- tional Intelligence (ODNI), Intelligence Advanced Research P"
2021.naacl-main.368,D19-1258,0,0.0265495,"m size Figure 2 plots the Passage EM with different beam sizes. While initially increassing the beam size improves Passage Exact Match, the marginal improvement decreases after a beam size of forty. 3.3 Answer Extraction Evaluation Baselines We compare B EAM DR with (Zhao et al., 2020b), GRR (Asai et al., 2020) and the contemporaneous MDR (Xiong et al., 2021b). We use released code from GRR (Asai et al., 2020) following its settings on BERT base and large. We use four 2080Ti GPUs. TXH Baselines We compare B EAM DR with TF - IDF, Results Using the same implementation but on Semantic Retrieval (Nie et al., 2019, SR), which our reranked chains, B EAM DR outperforms GRR 4637 Retriever Reader Dev EM F1 Test EM F1 Models BERT base Reader 54.0 GRR GRR 52.7 B EAM DR GRR 54.9 BERT large wwm Reader GRR GRR 60.5 B EAM DR GRR 61.3 MDR ∗ MDR ∗ 61.5 ELECTRA large Reader MDR ∗ MDR ∗ 63.4 TXH TXH 66.2 65.8 68.0 51.6 - 64.1 - 73.3 74.1 74.7 60.0 60.4 - 73.0 73.2 - 76.2 62.3 75.3 GRR B EAM DR B EAM DR† Passage Recall First hop Second hop 85.1 86.4 88.0 85.3 78.9 87.1 Overlap 64.3 26.7 14.7 Table 3: Passage Recall and overlap comparison between B EAM DR and GRR with different hop passages. Systems with † filter seco"
2021.naacl-main.368,N15-1117,1,0.739585,"ch complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should investigate how to use these connections to resolve ambiguity in the question (Elgohary et al., 2019; Min et al., 2020), resolve entity mentions (Guha et al., 2015), connect concepts across modalities (Lei et al., 2018), or to connect related questions to each other (Elgohary et al., 2018). Extracting multiple pieces of evidence automatically has applications from solving crossword puzzles (Littman et al., 2002), graph database construc- Acknowledgments tion (De Melo and Weikum, 2009), and understanding relationships (Chang et al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the f"
2021.naacl-main.368,D19-1261,0,0.179797,"a film starring Nicolas Cage and T´ea Leoni?” to the context “The Family Man is a 2000 American film written by David Diamond and David Weissman, and starring Nicolas Cage and T´ea Leoni.”. 5 Related Work step retrieval to select the first hop passages (or entity mentions), then find the next hop candidates directly from Wikipedia links and rerank them. Like B EAM DR, Asai et al. (2020) use beam search to find the chains but still rely on a graph neural network over Wikipedia links. B EAM DR retrieves evidence chains through dense representations without relying on the corpus semi-structure. Qi et al. (2019, 2020) iteratively generate the query from the question and retrieved history, and use traditional sparse IR systems to select the passage, which complements B EAM DR’s approach. 6 Conclusion We introduce a simple yet effective multi-step dense retrieval method, B EAM DR. By conducting beam search and globally refreshing negative chains during training, B EAM DR finds reasoning chains in dense space. B EAM DR is competitive to more complex SOTA systems albeit not using semi-structured information. While B EAM DR can uncover relationship embedded within a single question, future work should in"
2021.naacl-main.368,N18-1059,0,0.0566839,"al., 2009; Iyyer et al., We thank the anonymous reviewers and meta2016) to question answering (Ferrucci et al., 2010), reviewer for their suggestions and comments. Zhao is supported by the Office of the Director of Nawhich is the focus of this work. Given a complex question, researchers have in- tional Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the BETvestigated multi-step retrieval techniques to find TER Program contract 2019-19051600005. Boydan evidence chain. Knowledge graph question Graber is supported by NSF Grant IIS-1822494. answering approaches (Talmor and Berant, 2018; Any opinions, findings, conclusions, or recommenZhang et al., 2018, inter alia) directly search the dations expressed here are those of the authors and evidence chain from the knowledge graph, but falter when KG coverage is sparse. With the re- do not necessarily reflect the view of the sponsors. lease of large-scale datasets (Yang et al., 2018), recent systems (Nie et al., 2019; Zhao et al., 2020b; Asai et al., 2020; Dhingra et al., 2020, inter alia) use Wikipedia abstracts (the first paragraph of a Wikipedia page) as the corpus to retrieve the evidence chain. Dhingra et al. (2020) treat Wi"
2021.naacl-main.368,D18-1259,0,0.374635,"they show promise on bench- and iteratively compose the query by appending marks, it’s difficult to extend them beyond aca- the retrieval history. We improve the retrieval by encouraging the representation to discriminate hard demic testbeds because real-world datasets often negative evidence chains from the correct chains, lack this structure. For example, medical records which are refreshed by the model. lack links between reports. Dense retrieval (Lee et al., 2019; Guu et al., We evaluate Beam Dense Retrieval (B EAM DR) 2020; Karpukhin et al., 2020, inter alia) provides a on H OT P OT QA (Yang et al., 2018), a multi4635 1 Introduction Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4635–4641 June 6–11, 2021. ©2021 Association for Computational Linguistics hop question answering benchmark. When retrieving evidence chains directly from the corpus (full retrieval), B EAM DR is competitive to the state-of-the-art cascade reranking systems that use Wikipedia links. Combined with standard reranking and answer span extraction modules, the gain from full retrieval propagates to findings answers (Section"
D08-1071,W01-1802,0,0.0320019,"over X , any 0 ≤ η ≤ 1/ |Y|, any 0 <  < 1, any 0 < δ < 1 and any η ≤ η0 < 1/ |Y|, if the algorithm is given access to examples drawn EXηSN (c, D) and inputs , δ and η0 , then with probability at least 1 − δ, the algorithm returns a hypothesis h ∈ C with error at most . Here, EXηSN (c, D) is a structured noise oracle, which draws examples from D, labels them by c and randomly replaces with another label with prob. η. Note here the rather weak notion of noise: entire structures are randomly changed, rather than individual labels. Furthermore, the error is 0/1 loss over the entire structure. Collins (2001) establishes learnability results for the class of hyperplane models under 0/1 loss. While not stated directly in terms of PAC learnability, it is clear that his results apply. Taskar et al. (2005) establish tighter bounds for the case of Hamming loss. This suggests that the requirement of 0/1 loss is weaker. As suggested before, it is not sufficient for χ to simply be correct (the constant 1 function is correct, but not useful). We need it to be discriminating, made precise in the following definition. Definition 4. We say the discrimination of χ for h0 is PrD [χ(f1 (x), h0 (x))]−1 . In other"
D08-1071,J93-2004,0,0.0327011,"3.1 One-sided Learning with Hints We begin by considering a simplified version of the “learning with hints” problem. Suppose that all we care about is learning f2 . We have a small amount of data labeled by f2 (call this D) and a large amount of data labeled by f1 (call this Dunlab –”unlab” because as far as f2 is concerned, it is unlabeled). RUNNING E XAMPLE In our example, this means that we have a small amount of labeled NER data and a large amount of labeled POS/chunk data. We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences). We are only interested in learning to do NER. Details of the exact HMM setup are in Section 4.2. We call the following algorithm “One-Sided Learning with Hints,” since it aims only to learn f2 : 1: Learn h2 directly on D 2: For each example (x, y1 ) ∈ D unlab 3: Compute y2 = h2 (x) 4: If χ(y1 , y2 ), add (x, y2 ) to D 5: Relearn h2 on the (augmented) D 6: Go to (2) if desired RUNNING E XAMPLE In step 1, we train an NER HMM on CoNLL. On test data, this model achieves an F -score of 50.8. In step 2, we run this HMM on all the WS"
D08-1071,N06-1020,0,0.173583,"o NNP, we also include NNPS. The sentence is: “George Bush spoke to Congress today” 680 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 680–688, c Honolulu, October 2008. 2008 Association for Computational Linguistics ated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practic"
D08-1071,W95-0107,0,0.0199499,"g with Hints We begin by considering a simplified version of the “learning with hints” problem. Suppose that all we care about is learning f2 . We have a small amount of data labeled by f2 (call this D) and a large amount of data labeled by f1 (call this Dunlab –”unlab” because as far as f2 is concerned, it is unlabeled). RUNNING E XAMPLE In our example, this means that we have a small amount of labeled NER data and a large amount of labeled POS/chunk data. We use 3500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003) as the NER data and section 20-23 of the WSJ (Marcus et al., 1993; Ramshaw and Marcus, 1995) as the POS/chunk data (8936 sentences). We are only interested in learning to do NER. Details of the exact HMM setup are in Section 4.2. We call the following algorithm “One-Sided Learning with Hints,” since it aims only to learn f2 : 1: Learn h2 directly on D 2: For each example (x, y1 ) ∈ D unlab 3: Compute y2 = h2 (x) 4: If χ(y1 , y2 ), add (x, y2 ) to D 5: Relearn h2 on the (augmented) D 6: Go to (2) if desired RUNNING E XAMPLE In step 1, we train an NER HMM on CoNLL. On test data, this model achieves an F -score of 50.8. In step 2, we run this HMM on all the WSJ data, and extract 3145 co"
D08-1071,W00-0726,0,0.0804273,"Missing"
D08-1071,P95-1026,0,0.0891885,"s in Natural Language Processing, pages 680–688, c Honolulu, October 2008. 2008 Association for Computational Linguistics ated on a large amount of unlabeled data. Its predictions are assumed to be correct, and it is retrained on the unlabeled data according to its own predictions. Although there is little theoretical support for self-training, it is relatively popular in the natural language processing community. Its success stories range from parsing (McClosky et al., 2006) to machine translation (Ueffing, 2006). In some cases, self-training takes into account model confidence. Co-training (Yarowsky, 1995; Blum and Mitchell, 1998) is related to self-training, in that an algorithm is trained on its own predictions. Where it differs is that co-training learns two separate models (which are typically assumed to be independent; for instance by training with disjoint feature sets). These models are both applied to a large repository of unlabeled data. Examples on which these two models agree are extracted and treated as labeled for a new round of training. In practice, one often also uses a notion of model confidence and only extracts agreed-upon examples for which both models are confident. The or"
D08-1071,W03-0419,0,\N,Missing
D10-1008,P98-1013,0,0.567785,"s. We also present two corpus-based methods to automatically produce a new resource for affect state recognition: a patient polarity verb lexicon. 4.1 4.1.1 Plot Unit Creation Recognizing Affect States The basic building blocks of plot units are affect states which come in three flavors: positive, negative, and mental. In recent years, many publicly available resources have been created for sentiment analysis and other types of semantic knowledge. We considered a wide variety of resources and ultimately decided to experiment with five resources that most closely matched our needs: • FrameNet (Baker et al., 1998): We manually identified 87 frame classes that seem to be associated with affect: 43 mental classes (e.g., COMMU NICATION and NEEDING ), 22 positive classes (e.g., ACCOMPLISHMENT and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- la"
D10-1008,P08-1090,0,0.0739261,"Missing"
D10-1008,P09-1068,0,0.349667,"Missing"
D10-1008,W06-1651,0,0.0453128,"and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect states, when they occur with the designated part-of-speech. • Speech Act Verbs: We used 228 speech act verbs from (Wierzbicka, 1987) to produce M states. 4.1.2 Identifying the Characters For the purposes of this work, we made two simplifying assumptions: (1) There are only two characters per fable1 , and (2) Both characters are mentioned in the fable’s title. The problem of coreference resolution for fables is somewhat dif"
D10-1008,E03-1061,0,0.0686685,"Missing"
D10-1008,P97-1023,0,0.04484,"ify verbs that co-occur with these words as probable agents. For each agent term, we applied the pattern “* by [a,an,the] AGENT” and extracted the matching N-grams. Then we applied a part-ofspeech tagger to each N-gram and saved the words that were tagged as verbs (i.e., the words in the * position).3 This process produced 811 negative (evil agent) PPVs and 1362 positive (kind agent) PPVs. 4.2.2 PPV Bootstrapping over Conjunctions Our second approach for acquiring PPVs is based on an observation from sentiment analysis research that conjoined adjectives typically have the same polarity (e.g. (Hatzivassiloglou and McKeown, 1997)). 3 The POS tagging quality is undoubtedly lower than if tagging complete sentences but it seemed reasonable. Our hypothesis was that conjoined verbs often share the same polarity as well (e.g., “abducted and killed” or “rescued and rehabilitated”). We exploit this idea inside a bootstrapping algorithm to iteratively learn verbs that co-occur in conjunctions. Bootstrapping begins with 10 negative and 10 positive PPV seeds. First, we extracted triples of the form “w1 and w2” from the Google Web 1T N -gram corpus that had frequency ≥ 100 and were lower case. We separated each conjunction into t"
D10-1008,W10-0905,0,0.0241283,"Missing"
D10-1008,W06-0301,0,0.0311767,"Missing"
D10-1008,1985.tmi-1.17,0,0.12358,"Missing"
D10-1008,W03-0404,1,0.740643,"RB (“w1”) and a CONTEXT (“and w2”), and created a copy of the conjunction with the roles of w1 and w2 reversed. For example, “rescued and adopted” produces: VERB =“rescued” CONTEXT =“and adopted” VERB =“adopted” CONTEXT =“and rescued” Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. Basilisk identifies semantically similar words based on their co-occurrence with seeds in contextual patterns. Basilisk was originally designed for semantic class induction using lexico-syntactic patterns, but has also been used to learn subjective and objective nouns (Riloff et al., 2003). Basilisk first identifies the pattern contexts that are most strongly associated with the seed words. Words that occur in those contexts are labeled as candidates and scored based on the strength of their contexts. The top 5 candidates are selected and the bootstrapping process repeats. Basilisk produces a lexicon of learned words as well as a ranked list of pattern contexts. Since we bootstrapped over verb conjunctions, we also extracted new PPVs from the contexts. We ran the bootstrapping process to create a lexicon of 500 words, and we collected verbs from the top 500 contexts as well. 5"
D10-1008,C08-1103,0,0.0249138,"Missing"
D10-1008,P05-1017,0,0.15278,"use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect states, when they occur with the designated part-of-speech. • Speech Act Verbs: We used 228 speech act verbs from (Wierzbicka, 1987) to produce M states. 4.1.2 Identifying the Characters For the purposes of this work, we made two simplifying assumptions: (1) There are only two characters per fable1 , and (2) Both characters are mentioned in the fable’s title. The problem of coreference resolution for fables is somewhat different than for other genres, primarily because the characters are often animals (e.g"
D10-1008,W02-1028,1,0.759532,"atively learn verbs that co-occur in conjunctions. Bootstrapping begins with 10 negative and 10 positive PPV seeds. First, we extracted triples of the form “w1 and w2” from the Google Web 1T N -gram corpus that had frequency ≥ 100 and were lower case. We separated each conjunction into two parts: a primary VERB (“w1”) and a CONTEXT (“and w2”), and created a copy of the conjunction with the roles of w1 and w2 reversed. For example, “rescued and adopted” produces: VERB =“rescued” CONTEXT =“and adopted” VERB =“adopted” CONTEXT =“and rescued” Next, we applied the Basilisk bootstrapping algorithm (Thelen and Riloff, 2002) to learn PPVs. Basilisk identifies semantically similar words based on their co-occurrence with seeds in contextual patterns. Basilisk was originally designed for semantic class induction using lexico-syntactic patterns, but has also been used to learn subjective and objective nouns (Riloff et al., 2003). Basilisk first identifies the pattern contexts that are most strongly associated with the seed words. Words that occur in those contexts are labeled as candidates and scored based on the strength of their contexts. The top 5 candidates are selected and the bootstrapping process repeats. Basi"
D10-1008,H05-2018,1,0.286447,"ave been created for sentiment analysis and other types of semantic knowledge. We considered a wide variety of resources and ultimately decided to experiment with five resources that most closely matched our needs: • FrameNet (Baker et al., 1998): We manually identified 87 frame classes that seem to be associated with affect: 43 mental classes (e.g., COMMU NICATION and NEEDING ), 22 positive classes (e.g., ACCOMPLISHMENT and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect st"
D10-1008,H05-1044,0,0.168083,"ave been created for sentiment analysis and other types of semantic knowledge. We considered a wide variety of resources and ultimately decided to experiment with five resources that most closely matched our needs: • FrameNet (Baker et al., 1998): We manually identified 87 frame classes that seem to be associated with affect: 43 mental classes (e.g., COMMU NICATION and NEEDING ), 22 positive classes (e.g., ACCOMPLISHMENT and SUPPORTING ), and 22 negative classes (e.g., CAUSE HARM and PROHIBITING ). We use the verbs listed for these classes to produce M, +, and - affect states. • MPQA Lexicon (Wilson et al., 2005b): We used the words listed as having positive or negative sentiment polarity to produce +/- states, when they occur with the designated part-of-speech. • OpinionFinder (Wilson et al., 2005a) (Version 1.4) : We used the +/- labels assigned by its contextual polarity classifier (Wilson et al., 2005b) to create +/- states and the MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) to produce mental (M) states. • Semantic Orientation Lexicon (Takamura et al., 2005): We used the words listed as having positive or negative polarity to produce +/- affect st"
D10-1008,C98-1013,0,\N,Missing
D11-1023,N09-1003,0,0.285098,"Missing"
D11-1023,D07-1090,0,0.341222,"aming text. We apply CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 1 Introduction There is more data available today on the web than there has ever been and it keeps increasing. Use of large data in the Natural Language Processing (NLP) community is not new. Many NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) have benefited from having large amounts of data. However, processing large amounts of data is still challenging. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an ave"
D11-1023,P89-1010,0,0.630617,"and a correlation (ρ) of 1. The results with respect to different sized counter (20 million (20M ), 50 million (50M )) models are shown in Table 1. If we compare the second and third column of the table using PMI and LLR for 20M counters, we get exact rankings for LLR compared to PMI while comparing TopK word pairs. The explanation for such a behavior is: since we are 3 Even with other datasets we found that using counters linear in the size of the stream leads to ARE close to zero ∀ counts. 254 not throwing away any infrequent word pairs, PMI will rank pairs with low frequency counts higher (Church and Hanks, 1989). Hence, we are evaluating the PMI values for rare word pairs and we need counters linear in size of stream to get almost perfect ranking. This is also evident from the fourth column for 50M of the Table 1, where CU PMI ranking gets close to the optimal as the number of counters approaches stream size. However, in some NLP problems, we are not interested in low-frequency items. In such cases, even using space less than linear in number of counters would suffice. In our extrinsic evaluations, we show that using space less than the length of the stream does not degrade the performance. 4 4.1 Ext"
D11-1023,P09-1041,0,0.0149053,"ghts on edges in the graph are parameterized as a linear function of features, with weight learned by some supervised learning algorithm. In this section, we ask the question: can word association scores be used to derive syntactic structures in an unsupervised manner? A first pass answer is: clearly not. Metrics like PMI would assign high association scores to rare word pairs (mostly content words) leading to incorrect parses. Metrics like LLR would assign high association scores to frequent words, also leading to incorrect parses. However, with a small amount of linguistic side information (Druck et al., 2009; Naseem et al., 2010), we see that these issues can be overcome. In particular, we see that large data + a little linguistics &gt; fancy unsupervised learning algorithms. 5.1 Graph Definition Our approach is conceptually simple. We construct a graph over nodes in the sentence with a unique “root” node. The graph is directed and fully connected, and for any two words in positions i and j, the weight from word i to word j is defined as: wij = αasc asc(wi , wj ) − αdist dist(i − j) + αling ling(ti , tj ) Here, asc(wi , wj ) is a association score such as PMI or LLR computed using approximate counts"
D11-1023,N09-1058,1,0.830197,"Missing"
D11-1023,W10-2808,1,0.523316,"Missing"
D11-1023,W10-1503,1,0.714482,"Missing"
D11-1023,D09-1079,0,0.0499078,"ute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community to use streaming, randomized, and approximate algorithms to handle large amounts of data (Goyal et al., 2009; Levenberg et al., 2010; Van Durme and Lall, 2010). Streaming approaches (Muthukrishnan, 2005) provide memory and time-efficient framework to deal with terabytes of data. However, these approaches are proposed to solve a singe problem. For example, our earlier work (Goyal et al., 2009) and Levenberg and Osborne (2009) build approximate language models and show their effectiveness in Statistical Machine Translation (SMT). Streambased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB) Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” given verb “x” using the highest approximate online Pointwise Mutual Information (PMI) values. In this paper, we explore sketch techniques, especially the Count-Min sketch (Cormode and Muthukrishnan, 2004) to build a single"
D11-1023,N10-1062,0,0.0377837,"ing. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community to use streaming, randomized, and approximate algorithms to handle large amounts of data (Goyal et al., 2009; Levenberg et al., 2010; Van Durme and Lall, 2010). Streaming approaches (Muthukrishnan, 2005) provide memory and time-efficient framework to deal with terabytes of data. However, these approaches are proposed to solve a singe problem. For example, our earlier work (Goyal et al., 2009) and Levenberg and Osborne (2009) build approximate language models and show their effectiveness in Statistical Machine Translation (SMT). Streambased translation models (Levenberg et al., 2010) has been shown effective to handle large parallel streaming data for SMT. In Van Durme and Lall (2009b), a Talbot Osborne Morris Bloom (TOMB)"
D11-1023,H05-1066,0,0.137987,"Missing"
D11-1023,D10-1120,0,0.0127478,"graph are parameterized as a linear function of features, with weight learned by some supervised learning algorithm. In this section, we ask the question: can word association scores be used to derive syntactic structures in an unsupervised manner? A first pass answer is: clearly not. Metrics like PMI would assign high association scores to rare word pairs (mostly content words) leading to incorrect parses. Metrics like LLR would assign high association scores to frequent words, also leading to incorrect parses. However, with a small amount of linguistic side information (Druck et al., 2009; Naseem et al., 2010), we see that these issues can be overcome. In particular, we see that large data + a little linguistics &gt; fancy unsupervised learning algorithms. 5.1 Graph Definition Our approach is conceptually simple. We construct a graph over nodes in the sentence with a unique “root” node. The graph is directed and fully connected, and for any two words in positions i and j, the weight from word i to word j is defined as: wij = αasc asc(wi , wj ) − αdist dist(i − j) + αling ling(ti , tj ) Here, asc(wi , wj ) is a association score such as PMI or LLR computed using approximate counts from the sketch. Simi"
D11-1023,P05-1077,0,0.492812,"h to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 1 Introduction There is more data available today on the web than there has ever been and it keeps increasing. Use of large data in the Natural Language Processing (NLP) community is not new. Many NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) have benefited from having large amounts of data. However, processing large amounts of data is still challenging. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has attracted the NLP community"
D11-1023,C08-1114,0,0.0446642,"CountMin sketch to approximate word pair counts and exhibit their effectiveness on three important NLP tasks. Our experiments demonstrate that on all of the three tasks, we get performance comparable to Exact word pair counts setting and state-of-the-art system. Our method scales to 49 GB of unzipped web data using bounded space of 2 billion counters (8 GB memory). 1 Introduction There is more data available today on the web than there has ever been and it keeps increasing. Use of large data in the Natural Language Processing (NLP) community is not new. Many NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) have benefited from having large amounts of data. However, processing large amounts of data is still challenging. This has motivated NLP community to use commodity clusters. For example, Brants et al. (2007) used 1500 machines for a day to compute the relative frequencies of n-grams from 1.8TB of web data. In another work, a corpus of roughly 1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. However, the inaccessibility of clusters to an average user has"
D11-1023,P10-2043,0,0.240141,"Missing"
D11-1023,J90-1003,0,\N,Missing
D11-1023,D09-1098,0,\N,Missing
D11-1041,J93-1003,0,0.270819,"erent kinds of specific objects, we expand n1 and n2 in their textual descriptions using synonyms. For example, the object class n1 =aeroplane should include the synonyms {plane, jet, fighter jet, aircraft}, denoted as hn1 i. To do this, we expand each object class using their corresponding WordNet synsets up to at most three hyponymns levels. Example synonyms for some of the classes are summarized in Table 2. We can now compute from the Gigaword corpus [Graff, 2003] the probability that a verb exists given the detected nouns, Pr (v|n1 , n2 ). We do this by computing the log-likelihood ratio [Dunning, 1993] , λnvn , of trigrams (hn1 i , v, hn2 i), computed from each sentence in the English Gigaword corpus [Graff, 2003]. This is done by extracting only the words in the corpus that are defined in N and V (including their synonyms). This forms a reduced corpus sequence from which we obtain our target trigrams. For example, the sentence: ity that a scene co-occurs with the object and action, Pr (s|n, v) by: Pr (n, v|s)Pr (s) Pr (n, v) Pr (n|s)Pr (v|s)Pr (s) = Pr (n)Pr (v) ∝ Pr (s|n) × Pr (s|v) Pr (s|n, v) = the large brown dog chases a small young cat around the messy room, forcing the cat to run a"
D11-1041,D10-1040,0,0.0756278,"by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al., 2003] are based on three steps: selection, planning and realization. A common challenge in generation problems is the question of: what is the input? Recently, approaches for generation have focused on formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty into account. 446 Our Approach Our approach is summarized in Fig. 3. The input is a test image where we detect objects and scenes using trained detection algorithms [Felzenszwalb et al., 2010; Torralba et al"
D11-1041,N03-1020,0,0.00865367,"are IN the scene}. In the second experiment, we applied the HMM strategy described above but made all transition probabilities equiprobable, removing the effects of the corpus, and producing a sentence structure which we denote as Teq∗ . The third experiment produces the full T ∗ with transition probabilities learned from the corpus. All experiments were performed on the 100 unseen testing images from the UIUC dataset and we used only the most likely (top) sentence generated for all evaluation. We use two evaluation metrics as a measure of the accuracy of the generated sentences: 1) ROUGE-1 [Lin and Hovy, 2003] precision scores and 2) Relevance and Readability of the generated sentences. ROUGE-1 is a recall based metric that is commonly used to measure the effectiveness of text summarization. In this work, the short descriptive sentence of an image can be viewed as summarizing the image content and ROUGE-1 is able to capture how well this sentence can describe the image by comparing it with the human annotated ground truth of the UIUC dataset. Due to the short sentences generated, we did not consider other ROUGE metrics (ROUGE-2, ROUGE-SU4) which captures fluency and is not an issue here. Experimen"
D11-1041,W09-2802,0,0.00985749,"yond what has been annotated by humans. [Yao et al., 2010] has recently introduced a framework for parsing images/videos to textual description that requires significant annotated data, a requirement that our proposed approach avoids. Natural language generation (NLG) is a longstanding problem. Classic approaches [Traum et al., 2003] are based on three steps: selection, planning and realization. A common challenge in generation problems is the question of: what is the input? Recently, approaches for generation have focused on formal specification inputs, such as the output of theorem provers [McKeown, 2009] or databases [Golland et al., 2010]. Most of the effort in those approaches has focused on selection and realization. We address a tangential problem that has not received much attention in the generation literature: how to deal with noisy inputs. In our case, the inputs themselves are often uncertain (due to misrecognitions by object/scene detectors) and the content selection and realization needs to take this uncertainty into account. 446 Our Approach Our approach is summarized in Fig. 3. The input is a test image where we detect objects and scenes using trained detection algorithms [Felze"
D11-1086,P11-2071,1,0.875033,"Missing"
D11-1086,J93-1003,0,0.107556,"n the following two sections. For the sake of convenience and clarity, we describe our techniques in the context of cross-covariance matrix between English and Spanish language pair. But these techniques extend directly to monolingual covariance matrices, and to different language pairs as well. 3.1 Computing Word Pair Association The first step in filtering out the noisy word cooccurrences is to use an appropriate measure to compute the strength of word pairs (English and Spanish words). This is a well studied problem and several association measures have been proposed in the NLP literature (Dunning, 1993; Inkpen and Hirst, 2002; Moore, 2004). These association measures can be divided into groups based on the statistics they use (Hoang et al., 2009). Here we explore a few of them for sparsifying the cross-covariance matrix. 3.1.1 Covariance The first option is to use the cross-covariance matrix itself. As noted above, when the data matrix is centered, the cross-covariance of an English word Pn (ei ) with a Spanish word (fj ) is given by k=1 Xik Yjk . It measures the strength with which 932 two words co-occur together. This measure uses information about the occurrence of a word pair in aligned"
D11-1086,P91-1023,0,0.375411,"pace can be found using either generative approaches based on topic modeling (Mimno et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Vu et al., 2009) or discriminative approaches based on variants of Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al., 2003; Platt et al., 2010; Haghighi et al., 2008). Both styles rely on document level term co-occurrences to find the latent representation. 1 Introduction Aligning documents from different languages arises in a range of tasks such as parallel phrase extraction (Gale and Church, 1991; Rapp, 1999), mining translations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarlamudi, 2011) and document retrieval (Ballesteros and Croft, 1996; Munteanu and Marcu, 2005). In this task, we are given a comparable corpora and some documents in one language are assumed to have a The discriminative approaches capture essential word co-occurrences in terms of two monolingual covariance matrices and a cross-covariance matrix. Subsequently, they use these covariance matrices to find projection directions in each language such that aligned documents lie close t"
D11-1086,P09-1121,0,0.162947,"llected from community based resources such as Wikipedia. This degrades performance of a 930 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930–940, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics variety of tasks, such as transliteration Mining (Kle- of data in both the languages and further assume that mentiev and Roth, 2006; Hermjakob et al., 2008; the data is centered (subtract the mean vector from Ravi and Knight, 2009) and multilingual web search each document i.e. xi ←xi − µx and yi ← yi − µy ). (Gao et al., 2009). Then CCA finds projection directions a and b which In this paper, we address the problem of identi- maximize: fying and removing noisy entries in the covariance aT XY T b √ √ matrices. We address this problem in two stages. aT XX T a bT Y Y T b In the first stage, we explore the use of word assos.t. aT XX T a = 1 & bT Y Y T b = 1 ciation measures such as Mutual Information (MI) and Yule’s ω (Reis and Judd, 2000) in computing The projection directions are obtained by solving the the strength of a word pair (Sec. 3.1). We also generalized eigen system: explore the use of bilingual dictionaries"
D11-1086,P08-1088,0,0.0238202,"ovariance matrices on two data sets from two different language pairs. Most of the existing approaches use manually aligned document pairs to find a common subspace in which the aligned document pairs are maximally correlated. The sub-space can be found using either generative approaches based on topic modeling (Mimno et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Vu et al., 2009) or discriminative approaches based on variants of Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al., 2003; Platt et al., 2010; Haghighi et al., 2008). Both styles rely on document level term co-occurrences to find the latent representation. 1 Introduction Aligning documents from different languages arises in a range of tasks such as parallel phrase extraction (Gale and Church, 1991; Rapp, 1999), mining translations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarlamudi, 2011) and document retrieval (Ballesteros and Croft, 1996; Munteanu and Marcu, 2005). In this task, we are given a comparable corpora and some documents in one language are assumed to have a The discriminative approaches capture essential"
D11-1086,P08-1045,1,0.844972,"Missing"
D11-1086,W09-2905,0,0.0133819,"etween English and Spanish language pair. But these techniques extend directly to monolingual covariance matrices, and to different language pairs as well. 3.1 Computing Word Pair Association The first step in filtering out the noisy word cooccurrences is to use an appropriate measure to compute the strength of word pairs (English and Spanish words). This is a well studied problem and several association measures have been proposed in the NLP literature (Dunning, 1993; Inkpen and Hirst, 2002; Moore, 2004). These association measures can be divided into groups based on the statistics they use (Hoang et al., 2009). Here we explore a few of them for sparsifying the cross-covariance matrix. 3.1.1 Covariance The first option is to use the cross-covariance matrix itself. As noted above, when the data matrix is centered, the cross-covariance of an English word Pn (ei ) with a Spanish word (fj ) is given by k=1 Xik Yjk . It measures the strength with which 932 two words co-occur together. This measure uses information about the occurrence of a word pair in aligned documents and doesn’t use other statistics such as ‘how often this pair doesn’t co-occur together’ and so on. 3.1.2 Mutual Information Association"
D11-1086,W02-0909,0,0.0190504,"two sections. For the sake of convenience and clarity, we describe our techniques in the context of cross-covariance matrix between English and Spanish language pair. But these techniques extend directly to monolingual covariance matrices, and to different language pairs as well. 3.1 Computing Word Pair Association The first step in filtering out the noisy word cooccurrences is to use an appropriate measure to compute the strength of word pairs (English and Spanish words). This is a well studied problem and several association measures have been proposed in the NLP literature (Dunning, 1993; Inkpen and Hirst, 2002; Moore, 2004). These association measures can be divided into groups based on the statistics they use (Hoang et al., 2009). Here we explore a few of them for sparsifying the cross-covariance matrix. 3.1.1 Covariance The first option is to use the cross-covariance matrix itself. As noted above, when the data matrix is centered, the cross-covariance of an English word Pn (ei ) with a Spanish word (fj ) is given by k=1 Xik Yjk . It measures the strength with which 932 two words co-occur together. This measure uses information about the occurrence of a word pair in aligned documents and doesn’t u"
D11-1086,P11-2026,1,0.334547,"it is still biased by the frequent words. The high association measure of a frequent English word with many Spanish words, makes it a nearest neighbour for lot of Spanish words. One way to prevent this is to discourage an already selected English word from associating with a new Spanish word. This requires a global knowledge of all the selected pairs and can not be done by looking at the individual words, as is the case with the greedy strategy employed by the relative thresholding. We use matching to solve this problem. We formulate the selection of the word pairs as a network flow problem (Jagarlamudi et al., 2011). The objective is to select word pairs that have high association measure while constraining each word to be associated with only a few words from other language. Let Iij denote an indicator variable taking a value of 0 or 1 depending on if the word pair (ei , fj ) is selected or not. We want each word to bePassociated with P k words from other language, i.e. j Iij = k and i Iij = k. Moreover, we want word pairs with high association score to be selected. We can encode this objective and the constraints as the following optimization problem: 3.3 Our Approach In this section we summarize our a"
D11-1086,P06-1103,0,0.0602339,"Missing"
D11-1086,2005.mtsummit-papers.11,0,0.0366874,"ords allowed for each English word and vice versa and k=2 is the number of monolingual word associations for each word. We also run both these combinations with monolingual augmentation, indicated by Yule(l)+Match(k)+Aug. For dictionary based weighting, Dictionary+Match(k), we choose a translation probability threshold of 0.01 and try k ∈ {1, 2}. Again, we run these combinations with monolingual augmentation identified by Dictionary+Match(k)+Aug. 4.4 Results For our final results, we choose data in two language pairs (English-Spanish and English-German) from two different resources, Europarl (Koehn, 2005) and Wikipedia. For Europarl data sets, we artificially make them comparable by considering the first half CCA OPCA Yule(2)+Match(2) Yule(2)+Match(2)+Aug Yule(3)+Match(2) Yule(3)+Match(2)+Aug Dictionary+Match(1) Dictionary+Match(2) Dictionary+Match(2)+Aug Wikipedia English-Spanish English-German Acc. MRR Acc. MRR 0.776 0.852 0.570 0.699 0.781 0.856 0.570 0.700 ∗ ∗ 0.798 0.866 0.576 0.703 0.811∗ 0.876∗ 0.602∗ 0.723∗ 0.803∗ 0.870∗ 0.572 0.700 0.793∗ 0.861∗ 0.610∗ 0.726∗ 0.811∗ 0.875∗ 0.656∗ 0.762∗ 0.811∗ 0.876∗ 0.623∗ 0.736∗ 0.825∗ 0.885∗ 0.630∗ 0.735∗ Europarl English-Spanish English-German Acc"
D11-1086,D09-1092,0,0.0662613,"Missing"
D11-1086,W04-3243,0,0.0221965,"ake of convenience and clarity, we describe our techniques in the context of cross-covariance matrix between English and Spanish language pair. But these techniques extend directly to monolingual covariance matrices, and to different language pairs as well. 3.1 Computing Word Pair Association The first step in filtering out the noisy word cooccurrences is to use an appropriate measure to compute the strength of word pairs (English and Spanish words). This is a well studied problem and several association measures have been proposed in the NLP literature (Dunning, 1993; Inkpen and Hirst, 2002; Moore, 2004). These association measures can be divided into groups based on the statistics they use (Hoang et al., 2009). Here we explore a few of them for sparsifying the cross-covariance matrix. 3.1.1 Covariance The first option is to use the cross-covariance matrix itself. As noted above, when the data matrix is centered, the cross-covariance of an English word Pn (ei ) with a Spanish word (fj ) is given by k=1 Xik Yjk . It measures the strength with which 932 two words co-occur together. This measure uses information about the occurrence of a word pair in aligned documents and doesn’t use other stati"
D11-1086,J05-4003,0,0.0317301,"variants of Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al., 2003; Platt et al., 2010; Haghighi et al., 2008). Both styles rely on document level term co-occurrences to find the latent representation. 1 Introduction Aligning documents from different languages arises in a range of tasks such as parallel phrase extraction (Gale and Church, 1991; Rapp, 1999), mining translations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarlamudi, 2011) and document retrieval (Ballesteros and Croft, 1996; Munteanu and Marcu, 2005). In this task, we are given a comparable corpora and some documents in one language are assumed to have a The discriminative approaches capture essential word co-occurrences in terms of two monolingual covariance matrices and a cross-covariance matrix. Subsequently, they use these covariance matrices to find projection directions in each language such that aligned documents lie close to each other (Sec. 2). The strong reliance of these approaches on the covariance matrices leads to problems, especially with the noisy data caused either by the noisy words in a document or the noisy document al"
D11-1086,J03-1002,0,0.00916465,"CA. Thus, their utility in bringing additional information, which is not captured by the covariance matrices, is arguable (our experiments show that they are indeed helpful). Moreover, they use document level co-occurrence information which is coarse compared to the cooccurrence at sentence level or the translational information provided by a bilingual dictionary. So, we use bilingual dictionaries as our final resource to weigh the word co-occurrences. Notice that, using bilingual information brings in information gleaned from an external corpus. We use translation tables learnt using Giza++ (Och and Ney, 2003) on Europarl data set. Since the translation tables are asymmetric, we combine translation tables from both the directions. We first use a threshold on the conditional probability to filter out the low probability ones and then convert them into joint probabilities before combining. For each word pair (ei , fj ), we compute the score as:  1 P (ei |fj )P (fj ) + P (fj |ei )P (ei ) 2 Selection Strategies The next step after computing association measure for all word pairs is to use them in selecting the pairs that need to be retained. In this section, we describe some approaches such as thresh"
D11-1086,D10-1025,0,0.314895,"efficacy of sparse covariance matrices on two data sets from two different language pairs. Most of the existing approaches use manually aligned document pairs to find a common subspace in which the aligned document pairs are maximally correlated. The sub-space can be found using either generative approaches based on topic modeling (Mimno et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Vu et al., 2009) or discriminative approaches based on variants of Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al., 2003; Platt et al., 2010; Haghighi et al., 2008). Both styles rely on document level term co-occurrences to find the latent representation. 1 Introduction Aligning documents from different languages arises in a range of tasks such as parallel phrase extraction (Gale and Church, 1991; Rapp, 1999), mining translations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarlamudi, 2011) and document retrieval (Ballesteros and Croft, 1996; Munteanu and Marcu, 2005). In this task, we are given a comparable corpora and some documents in one language are assumed to have a The discriminative appr"
D11-1086,P99-1067,0,0.0941972,"either generative approaches based on topic modeling (Mimno et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Vu et al., 2009) or discriminative approaches based on variants of Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al., 2003; Platt et al., 2010; Haghighi et al., 2008). Both styles rely on document level term co-occurrences to find the latent representation. 1 Introduction Aligning documents from different languages arises in a range of tasks such as parallel phrase extraction (Gale and Church, 1991; Rapp, 1999), mining translations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarlamudi, 2011) and document retrieval (Ballesteros and Croft, 1996; Munteanu and Marcu, 2005). In this task, we are given a comparable corpora and some documents in one language are assumed to have a The discriminative approaches capture essential word co-occurrences in terms of two monolingual covariance matrices and a cross-covariance matrix. Subsequently, they use these covariance matrices to find projection directions in each language such that aligned documents lie close to each other"
D11-1086,N09-1005,0,0.0240971,"ument or the noisy document alignments. Noisy data is not uncommon and is usually the case with data collected from community based resources such as Wikipedia. This degrades performance of a 930 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930–940, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics variety of tasks, such as transliteration Mining (Kle- of data in both the languages and further assume that mentiev and Roth, 2006; Hermjakob et al., 2008; the data is centered (subtract the mean vector from Ravi and Knight, 2009) and multilingual web search each document i.e. xi ←xi − µx and yi ← yi − µy ). (Gao et al., 2009). Then CCA finds projection directions a and b which In this paper, we address the problem of identi- maximize: fying and removing noisy entries in the covariance aT XY T b √ √ matrices. We address this problem in two stages. aT XX T a bT Y Y T b In the first stage, we explore the use of word assos.t. aT XX T a = 1 & bT Y Y T b = 1 ciation measures such as Mutual Information (MI) and Yule’s ω (Reis and Judd, 2000) in computing The projection directions are obtained by solving the the strength of a"
D11-1086,E09-1096,0,0.0232824,"rs. Later, we explore different selection strategies to remove the noisy pairs based on the association scores. Our experimental results on the task of aligning comparable documents shows the efficacy of sparse covariance matrices on two data sets from two different language pairs. Most of the existing approaches use manually aligned document pairs to find a common subspace in which the aligned document pairs are maximally correlated. The sub-space can be found using either generative approaches based on topic modeling (Mimno et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010; Vu et al., 2009) or discriminative approaches based on variants of Principal Component Analysis (PCA) and Canonical Correlation Analysis (CCA) (Susan T. Dumais, 1996; Vinokourov et al., 2003; Platt et al., 2010; Haghighi et al., 2008). Both styles rely on document level term co-occurrences to find the latent representation. 1 Introduction Aligning documents from different languages arises in a range of tasks such as parallel phrase extraction (Gale and Church, 1991; Rapp, 1999), mining translations for out-of-vocabulary words for statistical machine translation (Daume III and Jagarlamudi, 2011) and document r"
D11-1086,P10-1115,0,0.0518713,"Missing"
D11-1086,J93-1004,0,\N,Missing
D12-1002,P04-1021,0,0.0605696,"Missing"
D12-1002,P08-1045,1,0.851617,"ny natural language processing (NLP) tasks have bilingual resources from English into other languages. For example, significantly larger parallel texts are available 1 It is arguable that getting words and their IPA representation require knowledge about both words and IPA symbols, but it still is specific to one language and, in this sense, we refer to it as a monolingual resource. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We 12 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics English Word IPA bashful /ˈbæʃfəl/ tuesday /ˈtuːzdeɪ/ craft /kɹæft/"
D12-1002,C00-1056,0,0.0884559,"Missing"
D12-1002,P06-1103,0,0.206235,"utational Natural c Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics English Word IPA bashful /ˈbæʃfəl/ tuesday /ˈtuːzdeɪ/ craft /kɹæft/ book /bʊk/ head /hɛd/ Bulgarian Word IPA шибам /ˈʃibəm/ лук /luk/ как /kak/ музей /mʊˈzej/ спека /spɛˈkɤ/ Table 1: Example phoneme dictionaries in English and Bulgarian. The English translations for the Bulgarian words are switch, onion, how, museum, and spekle. summarize the approaches that are most relevant to us in Sec. 5. In this paper, we operate in the context of transliteration mining (Klementiev and Roth, 2006; Sproat et al., 2006) where we assume that we are given a source language name and a list of target language candidate transliterations and the task is to identify the correct transliteration. Given a set of l languages, we address the problem of building a transliteration system between every pair of languages. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998) or a set of common names transliterated from every language into a pivot language. Though it is relatively easy to obtain names translite"
D12-1002,W09-3501,0,0.0165716,"essing (NLP) tasks have bilingual resources from English into other languages. For example, significantly larger parallel texts are available 1 It is arguable that getting words and their IPA representation require knowledge about both words and IPA symbols, but it still is specific to one language and, in this sense, we refer to it as a monolingual resource. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We 12 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics English Word IPA bashful /ˈbæʃfəl/ tuesday /ˈtuːzdeɪ/ craft /kɹæft/ book /bʊk/ head /h"
D12-1002,N01-1020,0,0.101126,"Missing"
D12-1002,I11-1091,0,0.0416988,"Missing"
D12-1002,N09-1005,0,0.221069,"it still is specific to one language and, in this sense, we refer to it as a monolingual resource. Named entity (NE) transliteration involves transliterating a name in one language into another language and is shown to be crucial for machine translation (MT) (Knight and Graehl, 1998; AlOnaizan and Knight, 2002; Hermjakob et al., 2008; Li et al., 2009) and cross-lingual information retrieval (CLIR) (AbdulJaleel and Larkey, 2003; Mandl and Womser-Hacker, 2005; Udupa et al., 2009). There exists a large body of literature in transliteration, especially in the bilingual setting, well summarized by Ravi and Knight (2009). We 12 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics English Word IPA bashful /ˈbæʃfəl/ tuesday /ˈtuːzdeɪ/ craft /kɹæft/ book /bʊk/ head /hɛd/ Bulgarian Word IPA шибам /ˈʃibəm/ лук /luk/ как /kak/ музей /mʊˈzej/ спека /spɛˈkɤ/ Table 1: Example phoneme dictionaries in English and Bulgarian. The English translations for the Bulgarian words are switch, onion, how, museum, and spekle. summarize the approaches"
D12-1002,D11-1078,0,0.0353972,"Missing"
D12-1002,P06-1010,0,0.0240007,"e Learning, pages 12–23, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics English Word IPA bashful /ˈbæʃfəl/ tuesday /ˈtuːzdeɪ/ craft /kɹæft/ book /bʊk/ head /hɛd/ Bulgarian Word IPA шибам /ˈʃibəm/ лук /luk/ как /kak/ музей /mʊˈzej/ спека /spɛˈkɤ/ Table 1: Example phoneme dictionaries in English and Bulgarian. The English translations for the Bulgarian words are switch, onion, how, museum, and spekle. summarize the approaches that are most relevant to us in Sec. 5. In this paper, we operate in the context of transliteration mining (Klementiev and Roth, 2006; Sproat et al., 2006) where we assume that we are given a source language name and a list of target language candidate transliterations and the task is to identify the correct transliteration. Given a set of l languages, we address the problem of building a transliteration system between every pair of languages. A straight forward supervised learning approach would require training data of name pairs between every pair of languages (Knight and Graehl, 1998) or a set of common names transliterated from every language into a pivot language. Though it is relatively easy to obtain names transliterated into a pivot lan"
D12-1002,W06-1630,0,0.193796,"Missing"
D12-1002,N07-1061,0,0.0372995,"Missing"
D12-1002,P07-1015,0,0.137476,"Missing"
D12-1002,W02-0505,0,\N,Missing
D12-1002,J98-4003,0,\N,Missing
D12-1098,N09-1003,0,0.032766,"Missing"
D12-1098,P89-1010,0,0.250959,"irs. WS-203: A subset of WS-353 with 203 word pairs (Agirre et al., 2009). RG-65: (Rubenstein and Goodenough, 1965) has 65 word pairs. MC-30: A subset of RG-65 dataset with 30 word pairs (Miller and Charles, 1991). The results in Table 2 shows that by using distributed online-PMI (by making a single pass over the corpus) is comparable to offline-PMI (which is computed by making two passes over the corpus). For generating context vectors from GW, for both offline-PMI and online-PMI, we use a frequency cutoff of 5 for word-context pairs to throw away the rare terms as they are sensitive to PMI (Church and Hanks, 1989). Next, FLAG generates online-PMI vectors from GWB50 and GWB100 and uses frequency cutoffs of 15 and 25. The higher frequency cutoffs are selected based on the intuition that, with more data, we get more noise, and hence not considering word-context pairs with frequency less than 25 will be better for the system. As FLAG is go1075 ing to use the context vectors to find nearest neighbors, we also throw away all those words which have ≤ 50 contexts associated with them. This generates context vectors for 57, 930 words from GW; 95, 626 from GWB50 and 106, 733 from GWB100. Test Set Offline-PMI Onl"
D12-1098,P11-1061,0,0.119341,"s quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 1 Raul Guerra Dept. of Computer Science University of Maryland Introduction Many natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when working with large amounts of data (Pantel et al., 2009). This bottleneck is typically addressed by means of commodity clusters. For example, Pantel et al. (2009) c"
D12-1098,D11-1023,1,0.841948,"Missing"
D12-1098,N09-1058,1,0.84306,"Missing"
D12-1098,D12-1100,1,0.855994,"Missing"
D12-1098,P08-1119,0,0.0767073,"Missing"
D12-1098,N10-1062,0,0.0276141,". For example, Pantel et al. (2009) compute distributional similarity between 500 million terms over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms. In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system that constructs a fast large-scale approximate nearest-neighbor graph from a large text corpus. To build this system, we exploit recent developments in the area of approximation, randomization and streaming for large-scale NLP problems (Ravichandran et al., 2005; Goyal et al., 2009; Levenberg et al., 2010). More specifically we exploit work on Locality Sensitive Hashing (L SH) (Charikar, 2002) for computing word-pair similarities from large text collections (Ravichandran et al., 2005; Van Durme and Lall, 2010). However, Ravichandran et al. (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming setting. Unfortunately, their approach can handle o"
D12-1098,U08-1013,0,0.101239,"for example, constructing web-derived polarity lexicons (Velikovich et al., 2010), top 25 edges were used, and for unsupervised part-of-speech tagging using label propagation (Das and Petrov, 2011), top 5 edges were used. 5.1 Google Sets Problem Google Sets problem (Ghahramani and Heller, 2005) can be defined as: given a set of query words, return top t similar words with respect to query words. To evaluate the quality of our approximate large-scale graph, we return top 25 words which have best aggregated similarity scores with respect to query words. We take 5 classes and their query terms (McIntosh and Curran, 2008) shown in Table 8 and our goal is to learn 25 new words which are similar with these 5 query words. Language: german, french, estonian, hungarian, bulgarian Place: scandinavia, mongolia, mozambique, zambia, namibia Nationality: german, hungarian, estonian, latvian, lithuanian Date: september, february, august, july, november Organization: looksmart, hotbot, lycos, webcrawler, alltheweb Table 9: Learned terms for Google Sets Problem Concrete seeds Abstract seeds car, house, tree, horse, animal man, table, bottle, woman, computer idea, bravery, deceit, trust, dedication anger, humour, luck, infl"
D12-1098,E09-1077,0,0.0325645,"ed online-PMI algorithm with novel fast approximate nearest neighbor search algorithms (variants of P LEB). These algorithms return the approximate nearest neighbors quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 1 Raul Guerra Dept. of Computer Science University of Maryland Introduction Many natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when workin"
D12-1098,P05-1077,0,0.775534,"ically addressed by means of commodity clusters. For example, Pantel et al. (2009) compute distributional similarity between 500 million terms over a 200 billion words in 50 hours using 100 quad-core nodes, explicitly storing a similarity matrix between 500 million terms. In this work, we propose Fast Large-Scale Approximate Graph (FLAG) construction, a system that constructs a fast large-scale approximate nearest-neighbor graph from a large text corpus. To build this system, we exploit recent developments in the area of approximation, randomization and streaming for large-scale NLP problems (Ravichandran et al., 2005; Goyal et al., 2009; Levenberg et al., 2010). More specifically we exploit work on Locality Sensitive Hashing (L SH) (Charikar, 2002) for computing word-pair similarities from large text collections (Ravichandran et al., 2005; Van Durme and Lall, 2010). However, Ravichandran et al. (2005) approach stored an enormous matrix of all unique words and their contexts in main memory, which is infeasible for very large data sets. A more efficient online framework to locality sensitive hashing (Van Durme and Lall, 2010; Van Durme and Lall, 2011) computes distributional similarity in a streaming settin"
D12-1098,W02-1028,0,0.0379384,"e and Abstract Words Our goal is to automatically learn concrete and abstract words (Turney et al., 2011). We apply bootstrapping (Kozareva et al., 2008) on the word graphs by manually selecting 10 seeds for concrete and abstract words (see Table 10). We use in-degree (sum of weights of incoming edges) to compute the score for each node which has connections with known (seeds) or automatically labeled nodes, previously exploited to learn hyponymy relations from the web (Kozareva et al., 2008). We learn concrete and abstract words together (known as mutual exclusion principle in bootstrapping (Thelen and Riloff, 2002; McIntosh and Curran, 2008)), and each word is assigned to only one class. Moreover, after each iteration, we harmonically decrease the weight of the in-degree associated with instances learned in later iterations. We add 25 new instances at each iteration and ran 100 iterations of bootstrapping, yielding 2506 concrete nouns and 2498 abstract nouns. To evaluate our learned words, we searched in WordNet whether they had ‘abstraction’ or ’physical’ as their hypernym. Out of 2506 learned concrete nouns, 1078 Concrete: girl, person, bottles, wife, gentleman, microcomputer, neighbor, boy, foreigne"
D12-1098,D11-1063,0,0.0531024,"done on P LEB (Point Location in Equal Balls) (Indyk and Motwani, 1998; Charikar, 2002). We propose novel variants of P LEB to address the issue of reducing the pre-processing time for P LEB. One of the variants of P LEB (FASTP LEB) with considerably less pre-processing time has effectiveness comparable to P LEB. We evaluate these variants of P LEB both quantitatively and qualitatively on large data sets. Finally, we show the applicability of large-scale graphs built from FLAG on two applications: the Google-Sets problem (Ghahramani and Heller, 2005), and learning concrete and abstract words (Turney et al., 2011). 2 Count-Min sketch 2007; Cormode and Hadjieleftheriou, 2008) and sketch techniques (Charikar et al., 2004; Li et al., 2008; Cormode and Muthukrishnan, 2004; Rusu and Dobra, 2007). In our another recent paper (Goyal et al., 2012), we conducted a systematic study and compare many sketch techniques which answer point queries with focus on large-scale NLP tasks. In that paper, we empirically demonstrated that CM sketch performs the best among all the sketches on three large-scale NLP tasks. CM sketch uses hashing to store the approximate frequencies of all items from the large data set onto a sm"
D12-1098,P10-2043,0,0.0379184,"Missing"
D12-1098,P11-2004,0,0.0339271,"Missing"
D12-1098,N10-1119,0,0.10598,"algorithms (variants of P LEB). These algorithms return the approximate nearest neighbors quickly. We show our system’s efficiency in both intrinsic and extrinsic experiments. We further evaluate our fast search algorithms both quantitatively and qualitatively on two NLP applications. 1 Raul Guerra Dept. of Computer Science University of Maryland Introduction Many natural language processing (NLP) problems involve graph construction. Examples include constructing polarity lexicons based on lexical graphs from WordNet (Rao and Ravichandran, 2009), constructing polarity lexicons from web data (Velikovich et al., 2010) and unsupervised part-ofspeech tagging using label propagation (Das and Petrov, 2011). The later two approaches construct nearest-neighbor graphs between word pairs by computing nearest neighbors between word pairs from large corpora. These nearest neighbors form the edges of the graph, with weights given by the distributional similarity (Turney and Pantel, 2010) between terms. Unfortunately, computing the distributional similarity between all words in a large vocabulary is computationally and memory intensive when working with large amounts of data (Pantel et al., 2009). This bottleneck is t"
D12-1098,J90-1003,0,\N,Missing
D12-1098,D09-1098,0,\N,Missing
D12-1100,N09-1003,0,0.0575422,"Missing"
D12-1100,D08-1007,0,0.0229747,"Missing"
D12-1100,D07-1090,0,0.0113295,"any language-based tasks: the concept is that simple models trained on big data can outperform more complex models with fewer examples. However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate asGraham Cormode AT&T Labs–Research graham@research.att.com sociation scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandr"
D12-1100,P10-1046,0,0.0118214,"sets without human annotation. The pseudo-words are a common way to evaluate selectional preferences models (Erk, 2007; Bergsma et al., 2008) that measure the strength of association between a predicate and its argument filler, e.g., that the noun “song” is likely to be the object of the verb “sing”. A pseudo-word is the conflation of two words (e.g. song/dance). One word is the original in a sentence, and the second is the confounder. For example, in our task of selectional preferences, the system has to decide for the verb “sing” which is the correct object between “song”/“dance”. Recently, Chambers and Jurafsky (2010) proposed a simple baseline based on co-occurrence counts of words, which has state-of-the-art performance on pseudo-words evaluation for selectional preferences. We use a simple approach (without any typed dependency data) similar to Chambers and Jurafsky (2010), where we count all word pairs (except word pairs involving stop words) that appear within a window of size 3 from Gigaword (9.8 GB). That generates 970 million word pair tokens (stream size) and 94 million word pair types. Counts of all the 94 million unique word pairs are stored in CMCU, CMM-CU, and LCU-WS. For a target verb, we ret"
D12-1100,P89-1010,0,0.397644,"is recommended. 3.4 recall to measure the number of top-K sorted word pairs that are found in both the rankings. In Figure 3(a), we compute the recall for CMCU, COUNT-CU, CMM-CU, LCU-SWS, and LCU-WS sketches at several top-K thresholds of word pairs for approximate PMI ranking. We can make several observations from Figure 3(a). COUNT-CU has the worst recall for almost all the top-K settings. For top-K values less than 750, all sketches except COUNT-CU have comparable recall. Meanwhile, for K greater than 750, LCU-WS has the best recall. The is because PMI is sensitive to low frequency counts (Church and Hanks, 1989), over-estimation of the counts of low frequency word pairs can make their approximate PMI scores worse. In Figure 3(b), we compare the LLR rankings. For top-K values less than 1000, all the sketches have comparable recall. For top-K values greater than 1000, CM-CU, LCU-SWS, and LCU-WS perform better. The reason for such a behavior is due to LLR favoring high frequency word pairs, and COUNTCU and CMM-CU making under-estimation error on high frequency word pairs. Evaluating association scores ranking Last, in many NLP problems, we are interested in association rankings obtained using Pointwise"
D12-1100,P07-1028,0,0.0139294,"Missing"
D12-1100,D11-1023,1,0.836519,"Missing"
D12-1100,N09-1058,1,0.678075,"Missing"
D12-1100,D09-1079,0,0.0646159,"and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate asGraham Cormode AT&T Labs–Research graham@research.att.com sociation scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problem"
D12-1100,P05-1077,0,0.0661976,"al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate asGraham Cormode AT&T Labs–Research graham@research.att.com sociation scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solves three largescale NLP problems using small bounded memory footprint. However, there are"
D12-1100,P08-1058,0,0.012472,"oximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate asGraham Cormode AT&T Labs–Research graham@research.att.com sociation scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs and word-context pairs). Thus we focus on solving this central problem in the context of NLP applications. Sketch algorithms (Charikar et al., 2004; Cormode, 2011) are a memory- and time-efficient solution to answering point queries. Recently in NLP, we (Goyal and Daum´e III, 2011a) demonstrated that a version of the Count-Min sketch (Cormode and Muthukrishnan, 2004) accurately solve"
D12-1100,D07-1049,0,0.0119585,"However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate asGraham Cormode AT&T Labs–Research graham@research.att.com sociation scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005; Van Durme and Lall, 2010). All these problems ultimately depend on approximate counts of items (such as n-grams, word pairs"
D12-1100,C08-1114,0,0.0141547,"sks: the concept is that simple models trained on big data can outperform more complex models with fewer examples. However, this new view comes with its own challenges: principally, how to effectively represent such large data sets so that model parameters can be efficiently extracted? One answer is to adopt compact summaries of corpora in the form of probabilistic “sketches”. In recent years, the field of Natural Language Processing (NLP) has seen tremendous growth and interest in the use of approximation, randomization, and streaming techniques for large-scale problems (Brants et al., 2007; Turney, 2008). Much of this work relies on tracking very many statistics. For example, storing approximate counts (Talbot and Osborne, 2007; Van Durme and Lall, 2009a; Goyal and Daum´e III, 2011a), computing approximate asGraham Cormode AT&T Labs–Research graham@research.att.com sociation scores like Pointwise Mutual Information (Li et al., 2008; Van Durme and Lall, 2009b; Goyal and Daum´e III, 2011a), finding frequent items (like n-grams) (Goyal et al., 2009), building streaming language models (Talbot and Brants, 2008; Levenberg and Osborne, 2009), and distributional similarity (Ravichandran et al., 2005"
D12-1100,P10-2043,0,0.0937378,"Missing"
D12-1100,J90-1003,0,\N,Missing
D12-1118,C04-1046,0,0.0232506,"Missing"
D12-1118,P07-1107,0,0.122907,"ent or estimating the difficulty of the question. Mislead by the Content Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the closest match it can find is “Yasunari Kawabata”, who wrote the novel Snow Co"
D12-1118,D10-1028,1,0.844115,"re aggressive strategy, perhaps incorporating the identity of the opponent or estimating the difficulty of the question. Mislead by the Content Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the clo"
D12-1118,P11-1106,0,0.0372283,"nd broader corpora for training. In addition, using larger corpora would allow us to have more comprehensive doubly-hierarchical language models (Wood and Teh, 2009). We are also interested in adding richer models of opponents to the state space that would adaptively adjust strategies as it learned more about the strengths and weaknesses of its opponent (Waugh et al., 2011). Further afield, our presentation of sentences closely resembles paradigms for cognitive experiments in linguistics (Thibadeau et al., 1982) but are much cheaper to conduct. If online processing effects (Levy et al., 2008; Levy, 2011) could be observed in buzzing behavior; e.g., if a confusingly worded phrase depresses buzzing probability, it could help validate cognitively-inspired models of online sentence processing. Incremental classification is a natural problem, both for humans and resource-limited machines. While our data set is trivial (in a good sense), learning how humans process data and make decisions in a cheap, easy crowdsourced application can help us apply new algorithms to improve performance in settings where features aren’t free, either because of computational or annotation cost. Acknowledgments We than"
D12-1118,P00-1071,0,0.0211785,"tent Model The second example is a question on Enrico Fermi, an ItalianAmerican physicist. The first clues are about magnetic fields near a Fermi surface, which causes the content model to view “magnetic field” as the most likely answer. The question’s text, however, has pragmatic cues “this man” and “this Italian” which would have ruled out the abstract answer “magnetic field”. Correcting this would require a model that jointly models content and bigrams (Hardisty et al., 2010), has a coreference system as its content model (Haghighi and Klein, 2007), or determines the correct question type (Moldovan et al., 2000). Insufficient Data The third example is where our approach had no chance. The question is a very difficult question about George Washington, America’s first president. As a sign of its difficulty, only half the players answered correctly, and only near the end of the question. The question concerns lesser known episodes from Washington’s life, including a mistress caught in the elements. To the content model, of the several hypotheses it considers, the closest match it can find is “Yasunari Kawabata”, who wrote the novel Snow Country, whose plot matches some of these keywords. To answer these"
D12-1118,J08-4003,0,0.0109721,"rning tech- answers might also help your opponent, e.g., by elimnique (Langford and Zadrozny, 2005; Abbeel and inating an incorrect answer. Moreover, strategies in a Ng, 2004; Syed et al., 2008): given a representation game setting (rather than a single question) are more of the state space, learn a classifier that can map from complicated. For example, if a right answer is worth a state to an action. This is also a common paradigm +10 points and the penalty for an incorrect question for other incremental tasks, e.g., shift-reduce pars- is −5, then a team leading by 15 points on the last ing (Nivre, 2008). question should never attempt to answer. InvestigatGiven examples of the correct answer given a con- ing such gameplay strategies would require a “roll figuration of the state space, we can learn a MDP out” of game states (Tesauro and Galperin, 1996) to without explicitly representing the reward function. explore the efficacy of such strategies. While interIn this section, we define our method of defining esting, we leave these issues to future work. actions and our representation of the state space. We also investigated learning a policy directly from users’ buzzes directly (Abbeel and Ng,"
D12-1118,W11-1808,0,0.0266342,"Missing"
D12-1118,D11-1136,0,0.032474,"al., 2011). In contrast, incremental classification allows the learner to decide whether to acquire additional features. A common paradigm for incremental classification is to view the problem as a Markov decision process (MDP) (Zubek and Dietterich, 2002). The incremental classifier can either request an additional feature or render a classification decision (Chai et al., 2004; Ji and Carin, 2007; Melville et al., 2005), choosing its actions to minimize a known cost function. Here, we assume that the environment chooses a feature in contrast to a learner, as in some active learning settings (Settles, 2011). In Section 5, we use a MDP to decide whether additional features need to be processed in our application of incremental classification to a trivia game. 2.1 Trivia as Incremental Classification A real-life setting where humans classify documents incrementally is quiz bowl, an academic competition between schools in English-speaking countries; hundreds of teams compete in dozens of tournaments each year (Jennings, 2006). Note the distinction between quiz bowl and Jeopardy, a recent application area (Ferrucci et al., 2010). While Jeopardy also uses signaling devices, these are only usable afte"
D12-1118,P06-4018,0,\N,Missing
D12-1118,W02-0109,0,\N,Missing
D13-1109,W10-0701,0,0.0158926,"wever, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation. Although that work shows significant MT improvements, it is based primarily on distributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts. Additionally, that work reports results using artificially created monolingual corpora taken from separate source and target halves of a NEWdomain parallel corpu"
D13-1109,P13-1141,1,0.923939,"text, it is one of the largest freely available parallel corpora for any lan6 We could have, analogously, used the target language (English) side of the parallel corpus and measure overlap with the English Wikipedia documents, or even used both. 7 http://www.parl.gc.ca guage pair. In order to simulate more typical data settings, we sample every 32nd line, using the resulting parallel corpus of 253, 387 lines and 5, 051, 016 tokens to train our baseline model. We test our model using three NEW-domain corpora: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts (Carpuat et al., 2013a), and (3) a corpus of translated movie subtitles (Tiedemann, 2009). We use development and test sets to tune and evaluate our MT models. We use the NEW-domain parallel training corpora only for language modeling and for identifying NEW -domain-like comparable documents. 4.2 Machine translation We use the Moses MT framework (Koehn et al., 2007) to build a standard statistical phrase-based MT model using our OLD-domain training data. Using Moses, we extract a phrase table with a phrase limit of five words and estimate the standard set of five feature functions (phrase and lexical translation p"
D13-1109,N12-1047,0,0.0302403,"007) to build a standard statistical phrase-based MT model using our OLD-domain training data. Using Moses, we extract a phrase table with a phrase limit of five words and estimate the standard set of five feature functions (phrase and lexical translation probabilities in each direction and a constant phrase penalty feature). We also use a standard lexicalized reordering model and two language models based on the English side of the Hansard data and the given NEW -domain training corpora. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cherry and Foster, 2012). We call this the “simple baseline.” In Section 5.2 we describe several other baseline approaches. 4.3 Experiments For each domain, we use the marginal matching method described in Section 3 to learn a new, domain-adapted joint distribution, pnew k (s, t), over all French and English words. We use the learned joint to compute conditional probabilities, pnew k (t|s), for each French word s and rank English translations t accordingly. First, we evaluate the learned joint directly using the distribution based on the wordaligned NEW-domain development set as a gold standard. Then, we perform end-"
D13-1109,P11-2071,1,0.710111,"Missing"
D13-1109,H92-1020,0,0.378434,"ora taken from separate source and target halves of a NEWdomain parallel corpus, which may have more lexical overlap with the corresponding test set than we could expect from true monolingual corpora. Our work mines NEW-domain-like document pairs from Wikipedia. In this work, we show that, keeping data resources constant, our model drastically outperforms this previous approach. Razmara et al. (2013) take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases. Della Pietra et al. (1992) and Federico (1999) explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours. However, our focus is on translation. 3 Model Our goal is to recover a probabilistic translation dictionary in a NEW-domain, represented as a joint probability distribution pnew (s, t) over source/target word pairs. At our disposal, we have access to a joint distribution pold (s, t) from the OLD-domain (computed from word alignments), plus comparable document pairs in the NEW-domain. From these comparable documents, w"
D13-1109,D12-1025,0,0.124393,"rns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain te"
D13-1109,P98-1069,0,0.283477,"1078 ing domains in machine translation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. P"
D13-1109,P08-1088,0,0.365017,"e words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an"
D13-1109,N13-1056,1,0.779102,"n MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourci"
D13-1109,P06-1103,0,0.114295,"esulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translati"
D13-1109,W02-0902,0,0.592232,"and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is r"
D13-1109,P07-2045,0,0.00516723,"32nd line, using the resulting parallel corpus of 253, 387 lines and 5, 051, 016 tokens to train our baseline model. We test our model using three NEW-domain corpora: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts (Carpuat et al., 2013a), and (3) a corpus of translated movie subtitles (Tiedemann, 2009). We use development and test sets to tune and evaluate our MT models. We use the NEW-domain parallel training corpora only for language modeling and for identifying NEW -domain-like comparable documents. 4.2 Machine translation We use the Moses MT framework (Koehn et al., 2007) to build a standard statistical phrase-based MT model using our OLD-domain training data. Using Moses, we extract a phrase table with a phrase limit of five words and estimate the standard set of five feature functions (phrase and lexical translation probabilities in each direction and a constant phrase penalty feature). We also use a standard lexicalized reordering model and two language models based on the English side of the Hansard data and the given NEW -domain training corpora. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cher"
D13-1109,D09-1092,0,0.032703,"ity of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual di"
D13-1109,P10-2041,0,0.0677104,"e could have targeted our learning even more by using our NEW-domain MT test sets. Doing so would increase the chances that our source language words of interest appear in the comparable corpus. However, to avoid overfitting any particular test set, we use the French side of the training data. For each Wikipedia document pair, we compute the percent of French phrases up to length four that are observed in the French monolingual NEW -domain corpus and rank document pairs by the geometric mean of the four overlap measures. More sophisticated ways to identify NEW-domainlike Wikipedia pages (e.g. Moore and Lewis (2010)) may yield additional performance gains, but, qualitatively, the ranked Wikipedia pages seemed reasonable to the authors. 4 Experimental setup 4.1 Data We use French-English Hansard parliamentary proceedings7 as our OLD-domain parallel corpus. With over 8 million parallel lines of text, it is one of the largest freely available parallel corpora for any lan6 We could have, analogously, used the target language (English) side of the parallel corpus and measure overlap with the English Wikipedia documents, or even used both. 7 http://www.parl.gc.ca guage pair. In order to simulate more typical d"
D13-1109,D09-1141,0,0.0196136,"system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain adaptation. Although that work shows significant MT improvements, it is based primarily on distributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts. Additionally, that work reports results using artificially created monolingual corpora taken from separate source and target halves of a NEWdomain parallel corpus, which may have more lexical overlap with the corresponding test set than we could ex"
D13-1109,P12-1017,0,0.140934,"monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OOV words in NEWdomain text in order to do domain"
D13-1109,J03-1002,0,0.00435154,"pair marginal distributions. Finally, we describe how we identify comparable document pairs relevant to the NEW-domain. 3.1 Marginal Matching Objective Given word-aligned parallel data in the OLD-domain and source and target comparable corpora in the NEW -domain, we first estimate a joint distribution pold (s, t) over word pairs (s, t) in the OLD-domain, where s and t range over source and target language words, respectively. For the OLD-domain joint distribution, we use a simple maximum likelihood estimate based on non-null automatic word alignments (using grow-diag-final GIZA++ alignments (Och and Ney, 2003)). Next, we find source and target marginal distributions, q(s) and q(t), by relative frequency estimates over the source and target comparable corpora. Our goal is to recover a joint distribution pnew (s, t) for the new domain that matches the marginals, q(s) and q(t), but is minimally different from the original joint distribution, pold (s, t). We cast this as a linear programming problem: pnew = arg min p − pold (1) p 1 X subject to: p(s, t) = 1, p(s, t) ≥ 0 s,t X p(s, t) = q(t), s X p(s, t) = q(s) t In the objective function, the joint probability matrices p and pold are interpreted as lar"
D13-1109,P11-1133,0,0.022372,"formance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource se"
D13-1109,P95-1050,0,0.759395,"ikipedia.org 1078 ing domains in machine translation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain a"
D13-1109,P99-1067,0,0.15112,"machine translation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data"
D13-1109,P11-1002,0,0.20353,"ethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a starting point (prior) for this translation distribution. It is reasonable to assume an initial bilingual dictionary can be obtained even in low resource settings, for example by crowdsourcing (Callison-Burch and Dredze, 2010) or pivoting through related languages (Schafer and Yarowsky, 2002; Nakov and Ng, 2009). Daum´e III and Jagarlamudi (2011) mine translations for high frequency OO"
D13-1109,P13-1109,0,0.492486,"tributional similarity, thus making it difficult to learn translations for low frequency source words with sparse word context counts. Additionally, that work reports results using artificially created monolingual corpora taken from separate source and target halves of a NEWdomain parallel corpus, which may have more lexical overlap with the corresponding test set than we could expect from true monolingual corpora. Our work mines NEW-domain-like document pairs from Wikipedia. In this work, we show that, keeping data resources constant, our model drastically outperforms this previous approach. Razmara et al. (2013) take a fundamentally different approach and construct a graph using source language monolingual text and identify translations for source language OOV words by pivoting through paraphrases. Della Pietra et al. (1992) and Federico (1999) explore models for combining foreground and background distributions for the purpose of language modeling, and their approaches are somewhat similar to ours. However, our focus is on translation. 3 Model Our goal is to recover a probabilistic translation dictionary in a NEW-domain, represented as a joint probability distribution pnew (s, t) over source/target"
D13-1109,W02-2026,0,0.246367,"nslation. That work concludes that errors resulting from unseen (OOV) and new translation sense words cause the majority of the degradation in translation performance that occurs when an MT model trained on OLD-domain data is used to translate data in a NEW-domain. Here, we target OOV errors, though our marginal matching method is also applicable to learning translations for NTS words. A plethora of prior work learns bilingual lexicons from monolingual and comparable corpora with many signals including distributional, temporal, and topic similarity (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Schafer and Yarowsky, 2002; Schafer, 2006; Klementiev and Roth, 2006; Koehn and Knight, 2002; Haghighi et al., 2008; Mimno et al., 2009; Mausam et al., 2010; Prochasson and Fung, 2011; Irvine and Callison-Burch, 2013). However, this prior work stops short of using these lexicons in translation. We augment a baseline MT system with learned translations. Our approach bears some similarity to Ravi and Knight (2011), Dou and Knight (2012), and Nuhn et al. (2012); we learn a translation distribution despite a lack of parallel data. However, we focus on the domain adaptation setting. Parallel data in an OLD -domain acts as a"
D13-1109,N10-1063,1,0.926845,"Missing"
D13-1109,C98-1066,0,\N,Missing
D13-1109,Q13-1035,1,\N,Missing
D13-1152,C10-1007,0,0.0558249,"simple heuristics such as a global relative pruning threshold, we learn a featurized decisionmaking policy of a more complex form. Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger) (Ross et al., 2011), a recent iterative imitation learning technique for structured prediction. Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures. In Roark and Hollingshead (2008) and Bergsma and Cherry (2010), pruning decisions are made locally as a preprocessing step. In the recent vine pruning approach (Rush and Petrov, 2012), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas1455 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cade (Charniak et al., 2006). These approaches do not directly tackle the feature selection problem. Although pruned edges do not require further feature computat"
D13-1152,W06-2920,0,0.0529003,"how many locked children it currently has. It turns out that given all the parsing features, the margin is the most discriminative meta-feature. When it is present, other metafeatures we added do not help much, Thus we do not include them in our experiments due to overhead. 6 6.1 Experiment Setup We generate dependency structures from the PTB constituency trees using the head rules of Yamada and Matsumoto (2003). Following convention, we use sections 02–21 for training, section 22 for development and section 23 for testing. We also report results on six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) as suggested in (Rush and Petrov, 2012), which cover a variety of language families. We follow the standard training/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers (policies). The PTB test data is tagged by a Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) trained on sections 02–21. We use the provided gold POS tags for the CoNLL test data. All results are evaluated by the unlabeled attachment score (UAS). For fair comparison with previous work, punctuation is included when computing parsing accuracy of all CoNLL-X"
D13-1152,D07-1101,0,0.0472707,"nd one 1456 of its modifiers. Finding the best tree requires first computing θ·φ(E) for each of the n2 possible edges. Since scoring the edges independently in this way restricts the parser to a local view of the dependency structure, higher-order models can achieve better accuracy. For example, in the second-order model of McDonald and Pereira (2006), each local subgraph E is a triple that includes the head and two modifiers of the head, which are adjacent to each other. Other methods that use triples include grandparent-parent-child triples (Koo and Collins, 2010), or non-adjacent siblings (Carreras, 2007). Third-order models (Koo and Collins, 2010) use quadruples, employing grand-sibling and tri-sibling information. The usual inference problem is to find the highest scoring tree for the input sentence. Note that in a valid tree, each token 1, . . . , n must be attached to exactly one parent (either another token or the root 0). We can further require the tree to be projective, meaning that edges are not allowed to cross each other. It is well known that dynamic programming can be used to find the best projective dependency tree in O(n3 ) time, much as in CKY, for firstorder models and some hig"
D13-1152,N06-1022,0,0.10832,"Missing"
D13-1152,C96-1058,1,0.389269,"ls (Koo and Collins, 2010) use quadruples, employing grand-sibling and tri-sibling information. The usual inference problem is to find the highest scoring tree for the input sentence. Note that in a valid tree, each token 1, . . . , n must be attached to exactly one parent (either another token or the root 0). We can further require the tree to be projective, meaning that edges are not allowed to cross each other. It is well known that dynamic programming can be used to find the best projective dependency tree in O(n3 ) time, much as in CKY, for firstorder models and some higher-order models (Eisner, 1996; McDonald and Pereira, 2006).1 When the projectivity restriction is lifted, McDonald et al. (2005b) pointed out that the best tree can be found in O(n2 ) time using a minimum directed spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967; Tarjan, 1977), though only for first-order models.2 We will make use of this fast non-projective algorithm as a subroutine in early stages of our system. 3 Dynamic Feature Selection Unlike typical feature selection methods that fix a subset of selected features and use it throughout testing, in dynamic feature selection we choose features adaptively for"
D13-1152,P10-1001,0,0.259435,"n] and m ∈ [1, n] (with h 6= m) are a head token and one 1456 of its modifiers. Finding the best tree requires first computing θ·φ(E) for each of the n2 possible edges. Since scoring the edges independently in this way restricts the parser to a local view of the dependency structure, higher-order models can achieve better accuracy. For example, in the second-order model of McDonald and Pereira (2006), each local subgraph E is a triple that includes the head and two modifiers of the head, which are adjacent to each other. Other methods that use triples include grandparent-parent-child triples (Koo and Collins, 2010), or non-adjacent siblings (Carreras, 2007). Third-order models (Koo and Collins, 2010) use quadruples, employing grand-sibling and tri-sibling information. The usual inference problem is to find the highest scoring tree for the input sentence. Note that in a valid tree, each token 1, . . . , n must be attached to exactly one parent (either another token or the root 0). We can further require the tree to be projective, meaning that edges are not allowed to cross each other. It is well known that dynamic programming can be used to find the best projective dependency tree in O(n3 ) time, much as"
D13-1152,J93-2004,0,0.0463942,"Missing"
D13-1152,D08-1017,0,0.0165487,"tic feature selection methods for dependency parsing, and show that even a few feature templates can give decent accuracy (Section 3.2). We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4). Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm (McDonald et al., 2005a; McDonald and Pereira, 2006). However, our approach applies to other graph-based dependency parsers as well—including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (Martins et al., 2008), belief propagation (Smith and Eisner, 2008), or structured boosting (Wang et al., 2007). 2 Graph-based Dependency Parsing In graph-based dependency parsing of an n-word input sentence, we must construct a tree y whose vertices 0, 1, . . . n correspond to the root node (namely 0) and the ordered words of the sentence. Each directed edge of this tree points from a head (parent) to one of its modifiers (child). Following a common approach to structured prediction problems, the score of a tree y is defined as P a sum of local Pscores. That is, sθ (y) = θ · E∈y φ(E) = E∈y θ · φ(E), where E ranges"
D13-1152,E06-1011,0,0.52191,"s to a smaller feature set for time efficiency. We aim to do feature selection and edge pruning dynamically, balancing speed and accuracy by using only as many features as needed. In this paper, we first explore standard static feature selection methods for dependency parsing, and show that even a few feature templates can give decent accuracy (Section 3.2). We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4). Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm (McDonald et al., 2005a; McDonald and Pereira, 2006). However, our approach applies to other graph-based dependency parsers as well—including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (Martins et al., 2008), belief propagation (Smith and Eisner, 2008), or structured boosting (Wang et al., 2007). 2 Graph-based Dependency Parsing In graph-based dependency parsing of an n-word input sentence, we must construct a tree y whose vertices 0, 1, . . . n correspond to the root node (namely 0) and the ordered words of the sentence. Each directed edge of this tree points from a head (parent) t"
D13-1152,P05-1012,0,0.874911,"all possible edges (or other small substructures) using a learned function; in the decoding stage, we use combinatorial optimization to find the dependency tree with the highest total score. Generally linear edge-scoring functions are used for speed. But they use a large set of features, derived from feature templates that consider different conjunctions of the edge’s attributes. As a result, parsing time is dominated by the scoring stage— computing edge attributes, using them to instantiate feature templates, and looking up the weights of the resulting features in a hash table. For example, McDonald et al. (2005a) used on average about 120 first-order feature templates on each edge, built from attributes such as the edge direction and length, the We therefore ask the question: can we use fewer features to score the edges, while maintaining the effect that the true dependency tree still gets a higher score? Motivated by recent progress on dynamic feature selection (Benbouzid et al., 2012; He et al., 2012), we propose to add features one group at a time to the dependency graph, and to use these features together with interactions among edges (as determined by intermediate parsing results) to make hard"
D13-1152,H05-1066,0,0.123804,"Missing"
D13-1152,C08-1094,0,0.0311852,". However, in place of relatively simple heuristics such as a global relative pruning threshold, we learn a featurized decisionmaking policy of a more complex form. Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger) (Ross et al., 2011), a recent iterative imitation learning technique for structured prediction. Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures. In Roark and Hollingshead (2008) and Bergsma and Cherry (2010), pruning decisions are made locally as a preprocessing step. In the recent vine pruning approach (Rush and Petrov, 2012), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas1455 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cade (Charniak et al., 2006). These approaches do not directly tackle the feature selection problem. Although pruned edges do not re"
D13-1152,N12-1054,0,0.576196,"form. Since each decision can affect later stages, or later decisions in the same stage, we model this problem as a sequential decision-making process and solve it by Dataset Aggregation (DAgger) (Ross et al., 2011), a recent iterative imitation learning technique for structured prediction. Previous work has made much progress on the complementary problem: speeding up the decoding stage by pruning the search space of tree structures. In Roark and Hollingshead (2008) and Bergsma and Cherry (2010), pruning decisions are made locally as a preprocessing step. In the recent vine pruning approach (Rush and Petrov, 2012), significant speedup is gained by leveraging structured information via a coarse-to-fine projective parsing cas1455 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1455–1464, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cade (Charniak et al., 2006). These approaches do not directly tackle the feature selection problem. Although pruned edges do not require further feature computation, the pruning step must itself compute similar high-dimensional features just to decide which edges to prune. For this"
D13-1152,D08-1016,1,0.482264,"y parsing, and show that even a few feature templates can give decent accuracy (Section 3.2). We then propose a novel way to dynamically select features for each edge while keeping the overhead of decision making low (Section 4). Our present experiments use the Maximum Spanning Tree (MST) parsing algorithm (McDonald et al., 2005a; McDonald and Pereira, 2006). However, our approach applies to other graph-based dependency parsers as well—including non-projective parsing, higher-order parsing, or approximations to higher-order parsing that use stacking (Martins et al., 2008), belief propagation (Smith and Eisner, 2008), or structured boosting (Wang et al., 2007). 2 Graph-based Dependency Parsing In graph-based dependency parsing of an n-word input sentence, we must construct a tree y whose vertices 0, 1, . . . n correspond to the root node (namely 0) and the ordered words of the sentence. Each directed edge of this tree points from a head (parent) to one of its modifiers (child). Following a common approach to structured prediction problems, the score of a tree y is defined as P a sum of local Pscores. That is, sθ (y) = θ · E∈y φ(E) = E∈y θ · φ(E), where E ranges over small connected subgraphs of y that can"
D13-1152,N03-1033,0,0.0754605,"the PTB constituency trees using the head rules of Yamada and Matsumoto (2003). Following convention, we use sections 02–21 for training, section 22 for development and section 23 for testing. We also report results on six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) as suggested in (Rush and Petrov, 2012), which cover a variety of language families. We follow the standard training/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers (policies). The PTB test data is tagged by a Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) trained on sections 02–21. We use the provided gold POS tags for the CoNLL test data. All results are evaluated by the unlabeled attachment score (UAS). For fair comparison with previous work, punctuation is included when computing parsing accuracy of all CoNLL-X languages but not English (PTB). For policy training, we train a linear SVM classifier using Liblinear (Fan et al., 2008). For all languages, we run DAgger for 20 iterations and seLanguage Method DYN FS Bulgarian V INE P DYN FS Chinese V INE P DYN FS English V INE P DYN FS German V INE P DYN FS Japanese V INE P DYN FS Portuguese V IN"
D13-1152,W03-3023,0,0.0530048,"f the next feature group to be added We also tried more complex meta-features, for example, mean and variance of the scores of competing edges, and structured features such as whether the head of e is locked and how many locked children it currently has. It turns out that given all the parsing features, the margin is the most discriminative meta-feature. When it is present, other metafeatures we added do not help much, Thus we do not include them in our experiments due to overhead. 6 6.1 Experiment Setup We generate dependency structures from the PTB constituency trees using the head rules of Yamada and Matsumoto (2003). Following convention, we use sections 02–21 for training, section 22 for development and section 23 for testing. We also report results on six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) as suggested in (Rush and Petrov, 2012), which cover a variety of language families. We follow the standard training/test split specified in the CoNLL-X data and tune parameters by cross validation when training the classifiers (policies). The PTB test data is tagged by a Stanford part-of-speech (POS) tagger (Toutanova et al., 2003) trained on sections 02–21. We use the provided gold PO"
D14-1070,W13-3214,0,0.0242311,"vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them across paragraphs. 6.2 Factoid Question-Answering Factoid question answering is often functionally equivalent to information retrieval. Given a knowledge base and a query, the goal is to Thomas Mann Henrik Ibsen Joseph Conrad Henry James Franz Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named en"
D14-1070,D12-1118,1,0.767192,"s have a property called pyramidality, which means that sentences early in a question contain harder, more obscure clues, while later sentences are “giveaways”. This design rewards players with deep knowledge of a particular subject and thwarts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4"
D14-1070,de-marneffe-etal-2006-generating,0,0.0211844,"Missing"
D14-1070,W13-0112,0,0.0150837,"ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al"
D14-1070,P13-1088,0,0.00846746,". from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed representations of paragraphs. Here we present a simpler approach where a single model is able to learn complex sentence representations and average them acr"
D14-1070,W13-3209,0,0.0158745,"rts bag of words methods. Sometimes the first sentence contains no named entities—answering the question correctly requires an actual understanding of the sentence (Figure 1). Later sentences, however, progressively reveal more well-known and uniquely identifying terms. Previous work answers quiz bowl questions using a bag of words (na¨ıve Bayes) approach (Boyd-Graber et al., 2012). These models fail on sentences like the first one in Figure 1, a typical hard, initial clue. Recursive neural networks (rnns), in contrast to simpler models, can capture the compositional aspect of such sentences (Hermann et al., 2013). rnns require many redundant training examples to learn meaningful representations, which in the quiz bowl setting means we need multiple questions about the same answer. Fortunately, hundreds of questions are produced during the school year for quiz bowl competitions, yielding many different examples of questions asking about any entity of note (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive"
D14-1070,P14-1136,0,0.00701547,"ector hs . The error for the sentence is C(S, θ) = XX L(rank(c, s, Z))max(0, s∈S z∈Z 1 − xc · hs + xz · hs ), (5) where the function rank(c, s, Z) provides the rank of correct answer c with respect to the incorrect answers Z. We transform this rank into a loss function4 shown by Usunier et al. (2009) to optimize the top of the ranked list, r P L(r) = 1/i. i=1 Since rank(c, s, Z) is expensive to compute, we approximate it by randomly sampling K incorrect answers until a violation is observed (xc · hs < 1 + xz · hs ) and set rank(c, s, Z) = (|Z |− 1)/K, as in previous work (Weston et al., 2011; Hermann et al., 2014). The model minimizes the sum of the error over all sentences T normalized by the number of nodes N in the training set, J(θ) = 1 X C(t, θ). N (6) t∈T The parameters θ = (Wr∈R , Wv , We , b), where R represents all dependency relations in the data, are optimized using AdaGrad(Duchi et al., 2011).5 In Section 4 we compare performance to an identical model (fixed-qanta) that excludes answer vectors from We and show that training them as part of θ produces significantly better results. The gradient of the objective function, ∂C 1 X ∂J(t) = , ∂θ N ∂θ (7) t∈T is computed using backpropagation throu"
D14-1070,P14-1105,1,0.253879,"ion described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain their own answer as part"
D14-1070,P08-1028,0,0.0784578,"er is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to this work are the dialogue analysis model proposed by Kalchbrenner & Blunsom (2013) and the paragraph vector model described in Le and Mikolov (2014), both of which are able to generate distributed repr"
D14-1070,P14-1037,0,0.028984,"Missing"
D14-1070,N12-1085,1,0.502253,"641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outperforms bag of words and information retrieval baselines. Our model improves upon a contrastive max-margin objective function from previous work to dynamically u"
D14-1070,D07-1002,0,0.141453,"z Kafka Figure 6: A question on the German novelist Thomas Mann that contains no named entities, along with the five top answers as scored by qanta. Each cell in the heatmap corresponds to the score (inner product) between a node in the parse tree and the given answer, and the dependency parse of the sentence is shown on the left. All of our baselines, including irwiki, are wrong, while qanta uses the plot description to make a correct guess. return the answer. Many approaches to this problem rely on hand-crafted pattern matching and answer-type classification to narrow down the search space (Shen, 2007; Bilotti et al., 2010; Wang, 2006). More recent factoid qa systems incorporate the web and social media into their retrieval systems (Bian et al., 2008). In contrast to these approaches, we place the burden of learning answer types and patterns on the model. 7 Future Work While we have shown that dt-rnns are effective models for quiz bowl question answering, other factoid qa tasks are more challenging. Questions like what does the aarp stand for? from trec qa data require additional infrastructure. A more apt comparison would be to IBM’s proprietary Watson system (Lally et al., 2012) for Jeop"
D14-1070,D11-1014,1,0.167867,"argin objective function described below. While we are not interested in obtaining a ranked list of answers,3 we observe better performance by adding the weighted approximaterank pairwise (warp) loss proposed in Weston et al. (2011) to our objective function. where R(n, k) is the dependency relation between node n and child node k. 3.2 Training Our goal is to map questions to their corresponding answer entities. Because there are a limited number of possible answers, we can view this as a multi-class classification task. While a softmax layer over every node in the tree could predict answers (Socher et al., 2011; Iyyer et al., 2014), this method overlooks that most answers are themselves words (features) in other questions (e.g., a question on World 635 Given a sentence paired with its correct answer c, we randomly select j incorrect answers from the set of all incorrect answers and denote this subset as Z. Since c is part of the vocabulary, it has a vector xc ∈ We . An incorrect answer z ∈ Z is also associated with a vector xz ∈ We . We define S to be the set of all nodes in the sentence’s dependency tree, where an individual node s ∈ S is associated with the 2 Of course, questions never contain the"
D14-1070,P13-1045,1,0.470518,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,D13-1170,1,0.042951,"the Holy Roman Empire. The first sentence contains no words or named entities that by themselves are indicative of the answer, while subsequent sentences contain more and more obvious clues. where inputs are typically a single sentence and outputs are either continuous or a limited discrete set. Neural networks have not yet shown to be useful for tasks that require mapping paragraph-length inputs to rich output spaces. Introduction Deep neural networks have seen widespread use in natural language processing tasks such as parsing, language modeling, and sentiment analysis (Bengio et al., 2003; Socher et al., 2013a; Socher et al., 2013c). The vector spaces learned by these models cluster words and phrases together based on similarity. For example, a neural network trained for a sentiment analysis task such as restaurant review classification might learn that “tasty” and “delicious” should have similar representations since they are synonymous adjectives. These models have so far only seen success in a limited range of text-based prediction tasks, Consider factoid question answering: given a description of an entity, identify the person, place, or thing discussed. We describe a task with high-quality ma"
D14-1070,Q14-1017,1,0.765477,"te (see Section 4.1 for more details). Thus, we have built-in redundancy (the number of “askable” entities is limited), but also built-in diversity, as difficult clues cannot appear in every question without becoming well-known. Dependency-Tree Recursive Neural Networks To compute distributed representations for the individual sentences within quiz bowl questions, we use a dependency-tree rnn (dt-rnn). These representations are then aggregated and fed into a multinomial logistic regression classifier, where class labels are the answers associated with each question instance. In previous work, Socher et al. (2014) use dt-rnns to map text descriptions to images. dt-rnns are robust to similar sentences with slightly different syntax, which is ideal for our problem since answers are often described by many sentences that are similar in meaning but different in structure. Our model improves upon the existing dt-rnn model by jointly learning answer and question representations in the same vector space rather than learning them separately. 3.1 Model Description As in other rnn models, we begin by associating each word w in our vocabulary with a vector representation xw ∈ Rd . These vectors are stored as the"
D14-1070,D07-1003,0,0.0464695,"al., 2012) for Jeopardy, which is limited to single sentences, or to models trained on Yago (Hoffart et al., 2013). We would also like to fairly compare qanta 641 Q A Q A Q A Q A Q Akbar Muhammad Shah Jahan Ghana A Babur Figure 7: An extremely misleading question about John Cabot, at least to computer models. The words muslim and mecca lead to three Mughal emperors in the top five guesses from qanta; other models are similarly led awry. with ir-wiki. A promising avenue for future work would be to incorporate Wikipedia data into qanta by transforming sentences to look like quiz bowl questions (Wang et al., 2007) and to select relevant sentences, as not every sentence in a Wikipedia article directly describes its subject. Syntax-specific annotation (Sayeed et al., 2012) may help in this regard. Finally, we could adapt the attribute space learned by the dt-rnn to use information from knowledge bases and to aid in knowledge base completion. Having learned many facts about entities that occur in question text, a dt-rnn could add new facts to a knowledge base or check existing relationships. 8 Conclusion We present qanta, a dependency-tree recursive neural network for factoid question answering that outpe"
D14-1070,D11-1016,0,0.0348678,"on Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowledge base completion task (Socher et al., 2013b). More relevant to thi"
D14-1070,C10-1142,0,0.00402061,"ane_addams hull_house jimmy_carter ronald_reagan woodrow_wilson Wars, rebellions, and battles U.S. presidents Prime ministers Explorers & emperors Policies Other Figure 5: t-SNE 2-D projections of 451 answer vectors divided into six major clusters. The blue cluster is predominantly populated by U.S. presidents. The zoomed plot reveals temporal clustering among the presidents based on the years they spent in office. from the meaning of the words that it contains as well as the syntax that glues those words together. Many computational models of compositionality focus on learning vector spaces (Zanzotto et al., 2010; Erk, 2012; Grefenstette et al., 2013; Yessenalina and Cardie, 2011). Recent approaches towards modeling compositional vector spaces with neural networks have been successful, although simpler functions have been proposed for short phrases (Mitchell and Lapata, 2008). Recursive neural networks have achieved state-of-the-art performance in sentiment analysis and parsing (Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013a). rnns have not been previously used for learning attribute spaces as we do here, although recursive tensor networks were unsuccessfully applied to a knowle"
D14-1140,P13-2121,0,0.0244753,"Missing"
D14-1140,P98-2141,0,0.421183,"hown”, which is a poor translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as mt was revolutionized by statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) t"
D14-1140,J03-1002,0,0.0606447,"rectly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases—where the input is either incomplete or incorrect—require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the hmm aligner (Vogel et al., 1996; Och and Ney, 2003). We first consider incomplete but correct inputs. Let y = τ (x1:t ) be the translator’s output given a partial source input x1:t with translation y. Then, τ (x1:t ) produces all target words yj if there is a source word xi in the input aligned to those words—i.e., (i, j) ∈ ax,y —and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi ; then, if xi is present in the inpu"
D14-1140,P14-2090,0,0.18418,"r recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an optimization problem, using a greedy search and dynamic programming to find segmentation strategies that maximize an evaluation measure. However, unlike our work, the direction of translation was from from svo to svo, obviating the need for verb prediction. Simultaneous translation is more straightforward for languages with compatible word orders, such as English and Spanish (F¨ ugen, 2008). To our knowledge, the only attempt to specifically predict verbs or any late-occurring terms (Matsubara et al.,"
D14-1140,P02-1040,0,0.114049,"n our system’s actions, we need to know how to select which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann e"
D14-1140,P11-1027,0,0.0210688,"ht language model. This is (na¨ıvely) analogous to how a human translator might use his own “language model” to guess upcoming words to gain some speed by completing, for example, collocations before they are uttered. We use a simple bigram language model for next-word prediction. We use Heafield et al. (2013). For verb prediction, we use a generative model that combines the prior probability of a particular verb v, p(v), with the likelihood of the source context at time t given that verb (namely, p(x1:t |v)), as estimated by a smoothed Kneser-Ney language model (Kneser and Ney, 1995). We use Pauls and Klein (2011). The prior probability p(v) is estimated by simple relative frequency estimation. The context, x1:t , consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb v (t) at time t is then: arg max p(v) v 5 t Y p(xi |v, xi−n+1:i−1 ) (2) i=1 One could replace T with a parameter, β, to bias towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. 1346 where xi−n+1:i−1 is the n − 1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as"
D14-1140,2006.amta-papers.18,0,0.0265515,"vie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs. Once we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator— 3 Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension. one that can completely translate an imagined sentence after one word is uttered—as an unobtainable system. On the other extreme is a standard batch translator, which"
D14-1140,quasthoff-etal-2006-corpus,0,0.0437515,"Missing"
D14-1140,P06-2088,0,0.157126,"or translation. However, it is still comprehensible, and the overall policy is similar to what a human would do: intuit the subject of the sentence from early clues and use a more general verb to stand in for a more specific one. 8 Related Work Just as mt was revolutionized by statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the t"
D14-1140,shimizu-etal-2014-collection,0,0.0581262,"on and Future Work Creating an effective simultaneous translation system for sov to svo languages requires not only translating partial sentences, but also effectively predicting a sentence’s verb. Both elements of the system require substantial refinement before they are usable in a real-world system. Replacing our idealized translation system is the most challenging and most important next step. Supporting multiple translation hypotheses and incremental decoding (Sankaran 1350 et al., 2010) would improve both the efficiency and effectiveness of our system. Using data from human translators (Shimizu et al., 2014) could also add richer strategies for simultaneous translation: passive constructions, reordering, etc. Verb prediction also can be substantially improved both in its scope in the system and how we predict verbs. Verb-final languages also often place verbs at the end of clauses, and also predicting these verbs would improve simultaneous translation, enabling its effective application to a wider range of sentences. Instead predicting an exact verb early (which is very difficult), predicting a semantically close or a more general verb might yield interpretable translations. A natural next step i"
D14-1140,2006.amta-papers.25,0,0.026828,"lect which actions to take. Eventually, we will formalize this as a learned policy (Section 5) that maps from states to actions. First, however, we need to define a reward that measures how “good” an action is. 3 Objective: What is a good simultaneous translation? Good simultaneous translations must optimize two objectives that are often at odds, i.e., producing translations that are, in the end, accurate, and producing them in pieces that are presented expeditiously. While there are existing automated metrics for assessing translation quality (Papineni et al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The"
D14-1140,P97-1037,0,0.133617,"al., 2002; Banerjee and Lavie, 2005; Snover et al., 2006), these must be modified to find the necessary compromise between translation quality and expeditiousness. That is, a good metric for simultaneous translation must achieve a balance between translating chunks early and translating accurately. All else being equal, maximizing either goal in isolation is trivial: for accurate translations, use a batch system and wait until the sentence is complete, translating it all at once; for a maximally expeditious translation, create monotone translations, translating each word as it appears, as in Tillmann et al. (1997) and Pytlik and Yarowsky (2006). The former is not simultaneous at all; the latter is mere word-for-word replacement and results in awkward, often unintelligible translations of distant language pairs. Once we have predictions, we have an expanded array of possibilities, however. On one extreme, we can imagine a psychic translator— 3 Using constrained decoding to enforce consistent translation prefixes would complicate our method but is an appealing extension. one that can completely translate an imagined sentence after one word is uttered—as an unobtainable system. On the other extreme is a s"
D14-1140,N03-1033,0,0.0339159,":t , consists of all words observed. We model p(x1:t |v) with verb-specific n-gram language models. The predicted verb v (t) at time t is then: arg max p(v) v 5 t Y p(xi |v, xi−n+1:i−1 ) (2) i=1 One could replace T with a parameter, β, to bias towards different kinds of simultaneous translations. As β → ∞, we recover batch translation. 1346 where xi−n+1:i−1 is the n − 1-gram context. To narrow the search space, we consider only the 100 most frequent final verbs, where a “final verb” is defined as the sentence-final sequence of verbs and particles as detected by a German part-of-speech tagger (Toutanova et al., 2003).6 5 Learning a Policy We have a framework (states and actions) for simultaneous machine translation and a metric for assessing simultaneous translations. We now describe the use of reinforcement learning to learn a policy, a mapping from states to actions, to maximize lbleu reward. We use imitation learning (Abbeel and Ng, 2004; Syed et al., 2008): given an optimal sequence of actions, learn a generalized policy that maps states to actions. This can be viewed as a cost-sensitive classification (Langford and Zadrozny, 2005): a state is represented as a feature vector, the loss corresponds to t"
D14-1140,C96-2141,0,0.0530067,"source sentence correctly once it has been fed the appropriate source words as input. There are two edge cases: empty input yields an empty output, while a complete, correct source sentence returns the correct, complete translation. Intermediate cases—where the input is either incomplete or incorrect—require using an alignment. The omniscient translator assumes as input a reference translation r, a partial source language input x1:t and a corresponding partial output y. In addition, the omniscient translator assumes access to an alignment between r and x. In practice, we use the hmm aligner (Vogel et al., 1996; Och and Ney, 2003). We first consider incomplete but correct inputs. Let y = τ (x1:t ) be the translator’s output given a partial source input x1:t with translation y. Then, τ (x1:t ) produces all target words yj if there is a source word xi in the input aligned to those words—i.e., (i, j) ∈ ax,y —and all preceding target words can be translated. (That translations must be contiguous is a natural requirement for human recipients of translations). In the case where yj is unaligned, the closest aligned target word to yj that has a corresponding alignment entry is aligned to xi ; then, if xi is"
D14-1140,2013.mtsummit-papers.11,0,0.0115023,"statistical learning, we suspect that simultaneous mt will similarly benefit from this paradigm, both from a systematic system for simultaneous translation and from a framework for learning how to incorporate predictions. Simultaneous translation has been dominated by rule and parse-based approaches (Mima et al., 1998a; Ryu et al., 2006). In contrast, although Verbmobil (Wahlster, 2000) performs incremental translation using a statistical mt module, its incremental decisionmaking module is rule-based. Other recent approaches in speech-based systems focus on waiting until a pause to translate (Sakamoto et al., 2013) or using word alignments (Ryu et al., 2012) between languages to determine optimal translation units. Unlike our work, which focuses on prediction and learning, previous strategies for dealing with sov-to-svo translation use rule-based methods (Mima et al., 1998b) (for instance, passivization) to buy time for the translator to hear more information in a spoken context—or use phrase table and reordering probabilities to decide where to translate with less delay (Fujita et al., 2013). Oda et al. (2014) is the most similar to our work on the translation side. They frame word segmentation as an o"
D14-1140,W10-1733,0,0.122467,"Missing"
D14-1140,W05-0909,0,\N,Missing
D14-1140,C98-2136,0,\N,Missing
D15-1006,D08-1089,0,0.0551911,"ference, he said. Unlike previous approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one. We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system. In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined. Table 4: Example of translation produced by GD and RW. 5.5 Error Analysis This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders. However, our problem is different in several ways. First, while the approaches resemble each other, our motivation is to reduce translation delay. Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed. Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences. Table 4 compares translations b"
D15-1006,D14-1140,1,0.793294,"Missing"
D15-1006,I13-1147,0,0.0122048,"revious approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one. We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system. In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined. Table 4: Example of translation produced by GD and RW. 5.5 Error Analysis This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders. However, our problem is different in several ways. First, while the approaches resemble each other, our motivation is to reduce translation delay. Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed. Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences. Table 4 compares translations by GD and RW. RW correc"
D15-1006,W14-7008,0,0.0167516,"simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one. We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system. In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined. Table 4: Example of translation produced by GD and RW. 5.5 Error Analysis This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders. However, our problem is different in several ways. First, while the approaches resemble each other, our motivation is to reduce translation delay. Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed. Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences. Table 4 compares translations by GD and RW. RW correctly puts the verb said"
D15-1006,D10-1092,0,0.030681,"y slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Quality of Rewritten Translations After applying the rewriting rules (Section 4), Table 2 shows the percentage of sen"
D15-1006,2011.iwslt-evaluation.18,0,0.0132805,"ow the effect of rewritten references, we compare the following MT systems: • GD: only gold reference translations; • RW: only rewritten reference translations; • RW + GD: both gold and the rewritten references; and • RW- LM + GD: using gold reference translations but using the rewritten references for training the LM and for tuning. For RW + GD and RW- LM + GD, we interpolate the language models of GD and RW. The interpolating weight is tuned with the rewritten sentences. For RW + GD, we combine the translation models (phrase tables and reordering tables) of RW and GD by fill-up combination (Bisazza et al., 2011), where all entries in the tables of RW are preserved and entries from the tables of GD are added if new. Increasing the RP threshold increases interpretation delay but improves the quality of the translation. We set the RP threshold at 0.0, 0.2, 0.4, 0.8 and finally 1.0 (equivalent to batch translation). Figure 3 shows the BLEU/RIBES scores vs. the number of words per segement as we increase the threshold. Rewritten sentences alone do not significantly improve over the baseline. We suspect this is because the transformation rules sometimes generate ungrammatical sentences due to parsing error"
D15-1006,P03-1054,0,0.0117594,"e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Quality of Rewritten Translations After applying the rewriting rules (Section 4), Table 2 shows the percentage of sentences that are candidates and how many rewrites are accepted. The most generalizable rules are passiviz"
D15-1006,N12-1047,0,0.0155365,"ccompanying example sentences.7 Statistics of the dataset are shown in Table 1. The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Qua"
D15-1006,P05-1066,0,0.21838,"Missing"
D15-1006,J14-1002,0,0.0383052,"Missing"
D15-1006,matsubara-etal-2002-bilingual,0,0.537728,"n. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s leisure, allowing for more introspection and precise word choice. We aim to address the data scarcity problem and combine translators’ lexical precision and interpreters’ sy"
D15-1006,J03-1002,0,0.00596878,"stem, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1. The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum"
D15-1006,P14-2090,0,0.503832,"system that only uses gold reference translations. 1 Introduction Simultaneous interpretation is challenging because it demands both quality and speed. Conventional batch translation waits until the entire sentence is completed before starting to translate. This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience. Simultaneous interpretation instead requires a tradeoff between quality and speed. A common strategy is to translate independently translatable segments as soon as possible. Various segmentation methods (Fujita et al., 2013; Oda et al., 2014) reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders. We show an example of Japanese-English translation in Figure 1. Consider the batch translation: in English, the verb change comes immediately after the subject We, whereas in Japanese it comes at the end 55 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55–64, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Source: ! We-TOP government-GEN structure and compositi"
D15-1006,P02-1040,0,0.0942046,"rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings. We use GIZA++ (Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning. The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model. We can now formally define the delay. Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to. We define the delay of ei as di = max(0, ai − maxj&lt;i aj ). The delay of x is P then N i=1 di /N , where the sum is over all aligned words except punctuation and stopwords. 5.1 Quality of Rewritten Translations After applying the rewriting rules (Section 4), Ta"
D15-1006,2013.iwslt-papers.3,0,0.642823,"he the subject and begin translating before observing the final verb. Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s l"
D15-1006,shimizu-etal-2014-collection,0,0.0714722,"other. Consider the monotone translation in Figure 1. By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb. Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction ("
D15-1006,N13-1023,0,0.463868,"responding Japanese sentence. We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting. Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations. 2 While we are motivated by real-time interpretation, to simplify our problem, we assume that we have perfect text input. Given this constraint, a typical simultaneous interpretation system (Sridhar et al., 2013; Fujita et al., 2013; Oda et al., 2014) produces partial translations of consecutive segments in the source sentence and concatenates them to produce a complete translation. We define the translation delay of a sentence as the average number of tokens the system has to observe between translation of two consecutive segments (denoted by # words/seg).1 For instance, the minimum delay of 1 word/seg is achieved when we translate immediately upon hearing a word. At test time, when the input is segmented, the delay is the average segment length. During the data preprocessing step of rewriting, we c"
D15-1006,tohyama-matsubara-2006-collection,0,0.187856,"Figure 1. By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb. Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction. These transformations enable us to divide the input into shorter segments, thus reducing translation delay. To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order. Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff. However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010). In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compar"
D15-1006,P03-1010,0,0.0607567,"alignment therefore forms a segment with delay of four words/seg. Alignments of the following words come before the source word aligned to love; hence, they are already translated in the previous segment and we do not double count their delay. In this example, the delay of the original sentence is 2.5 word/seg; after rewriting, it is reduced to 1.7 word/seg. Therefore, we accept the rewritten sentence. However, when the subject phrase is long and the object phrase is short, a swap may not reduce delay. Experiments We evaluate our method on the Reuters JapaneseEnglish corpus of news articles (Utiyama and Isahara, 2003). For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1. The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents. We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences. Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters s"
D15-1006,N09-1028,0,0.191939,"nd the gold reference translation. Boldface indicates the number is significantly larger than others (excluding the gold ref) according to two-sample t-tests with p &lt; 0.001. Effect on Verbs Rewriting training data not only creates lower latency simultaneous translations, but it also improves batch translation. One reason is that SOV to SVO translation often drops the verb because of long range reordering. (We see this for Japanese here, but this is also true for German.) Similar word orders in the source and target results in less reordering and improves phrase-based MT (Collins et al., 2005; Xu et al., 2009). Table 3 shows the number of verbs in the translations of the test sentences produced by GD, RW, RW + GD, as well as the number in the gold reference translation. Both RW and RW + GD produce more verbs (a statistically significant result), although RW + GD captures the most verbs. 61 he also said that the real dangers for the euro lay in the potential for divergences in the domestic policy needs Ref among the various participating nations of the single currency. GD he also for the euro, is a real danger to launch a single currency in many different countries and domestic policies on the need"
D15-1006,P07-2045,0,\N,Missing
D17-1153,D15-1006,1,0.859145,"Missing"
D17-1153,2012.eamt-1.60,0,0.0130167,"Missing"
D17-1153,W08-0336,0,0.0133316,"anguage pairs from different language families with different typological properties: German-to-English and (De-En) and Chinese-to-English (Zh-En). We use parallel transcriptions of TED talks for these pairs of languages from the machine translation track of the IWSLT 2014 and 2015 (Cettolo et al., 2014, 2015, 2012). For each language pair, we split its data into four sets for supervised training, bandit training, development and testing (Table 1). For English and German, we tokenize and clean sentences using Moses (Koehn et al., 2007). For Chinese, we use the Stanford Chinese word segmenter (Chang et al., 2008) to segment sentences and tokenize. We remove all sentences with length greater than 50, resulting in an average sentence length of 18. We use IWSLT 2015 data for supervised training and development, IWSLT 2014 data for bandit training and previous years’ development and evaluation data for testing. 5.1 Evaluation Framework For each task, we first use the supervised training set to pre-train a reference NMT model using supervised learning. On the same training set, we also pre-train the critic model with translations sampled from the pre-trained NMT model. Next, we enter a bandit learning mode"
D17-1153,W14-3346,0,0.0136011,"e wish to optimize. To establish the feasibility of driving learning from human feedback without doing a full, costly user study, we begin with a simulation study. The key aspects (Figure 2) of human feedback we capture are: (a) mismatch between training objective and feedbackmaximizing objective, (b) human ratings typically are binned (§ 4.1), (c) individual human ratings have high variance (§4.2), and (d) non-expert ratings can be skewed with respect to expert ratings (§4.3). In our simulated study, we begin by modeling gold standard human ratings using add-onesmoothed sentence-level B LEU (Chen and Cherry, 2014).3 Our evaluation criteria, therefore, is average sentence-B LEU over the run of our algo1467 3 “Smoothing 2” in Chen and Cherry (2014). Granularity Original [0.1, 0.3), [0.3, 0.5), [0.5, 0.7), [0.7, 0.9) and [0.9, 1.0]. Since most sentence-B LEU scores are much closer to zero than to one, many of the larger bins are frequently vacant. Perturbed g=1 g=3 4.2 Variance λ=0 λ=5 Skew ρ=2 ρ=.5 Figure 2: Examples of how our perturbation functions change the “true” feedback distribution (left) to ones that better capture features found in human feedback (right). rithm. However, in any realistic scenar"
D17-1153,D14-1140,1,0.299199,"Missing"
D17-1153,N16-1111,1,0.603668,"Missing"
D17-1153,P07-2045,0,0.0110527,"sh; for ρ < 1, the rater is motivational. 5 Experimental Setup We choose two language pairs from different language families with different typological properties: German-to-English and (De-En) and Chinese-to-English (Zh-En). We use parallel transcriptions of TED talks for these pairs of languages from the machine translation track of the IWSLT 2014 and 2015 (Cettolo et al., 2014, 2015, 2012). For each language pair, we split its data into four sets for supervised training, bandit training, development and testing (Table 1). For English and German, we tokenize and clean sentences using Moses (Koehn et al., 2007). For Chinese, we use the Stanford Chinese word segmenter (Chang et al., 2008) to segment sentences and tokenize. We remove all sentences with length greater than 50, resulting in an average sentence length of 18. We use IWSLT 2015 data for supervised training and development, IWSLT 2014 data for bandit training and previous years’ development and evaluation data for testing. 5.1 Evaluation Framework For each task, we first use the supervised training set to pre-train a reference NMT model using supervised learning. On the same training set, we also pre-train the critic model with translations"
D17-1153,P17-1138,0,0.0675524,"tion (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards. Our results show that bandit feedback can be an effective feedback mechanism for neural machine translation systems. This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008). An obvious question is whether we could extend our framework to model individual annotator"
D17-1153,D16-1127,0,0.0324095,"Zh−En 0.25 0.5 0.67 1 (e) Variance ρ 1.5 2 4 (f) Skew Figure 5: Performance gains of NMT models trained with NED-A2C in Per-Sentence B LEU (top row) and in Heldout B LEU (bottom row) under various degrees of granularity, variance, and skew of scores. Performance gains of models trained with un-perturbed scores are within the shaded regions. 2010), myoelectric control (Pilarski et al., 2011), and virtual assistants (Isbell et al., 2001). Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural"
D17-1153,D15-1166,0,0.747737,"al,jbg}@umiacs.umd.edu Abstract Machine translation is a natural candidate problem for reinforcement learning from human feedback: users provide quick, dirty ratings on candidate translations to guide a system to improve. Yet, current neural machine translation training focuses on expensive human-generated reference translations. We describe a reinforcement learning algorithm that improves neural machine translation systems from simulated human feedback. Our algorithm combines the advantage actor-critic algorithm (Mnih et al., 2016) with the attention-based neural encoderdecoder architecture (Luong et al., 2015). This algorithm (a) is well-designed for problems with a large action space and delayed rewards, (b) effectively optimizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors. 1 Introduction Bandit structured prediction is the task of learning to solve complex joint prediction problems (like parsing or machine translation) under a very limited feedback model: a system must produce a single structured output (e.g., translation) and then the world reveals a score that measures how good or bad th"
D17-1153,D08-1027,0,0.16025,"Missing"
D17-1153,P16-1152,0,0.0543786,"nularity, variance, and skew of scores. Performance gains of models trained with un-perturbed scores are within the shaded regions. 2010), myoelectric control (Pilarski et al., 2011), and virtual assistants (Isbell et al., 2001). Recently, this learning framework has been combined with recurrent neural networks to solve machine translation (Bahdanau et al., 2017), dialogue generation (Li et al., 2016), neural architecture search (Zoph and Le, 2017), and device placement (Mirhoseini et al., 2017). Other approaches to more general structured prediction under bandit feedback (Chang et al., 2015; Sokolov et al., 2016a,b) show the broader efficacy of this framework. Ranzato et al. (2016) describe MIXER for training neural encoder-decoder models, which is a reinforcement learning approach closely related to ours but requires a policy-mixing strategy and only uses a linear critic model. Among work on bandit MT, ours is closest to Kreutzer et al. (2017), which also tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simul"
D17-1153,2015.mtsummit-papers.13,0,0.218913,"imizes traditional corpus-level machine translation metrics, and (c) is robust to skewed, high-variance, granular feedback modeled after actual human behaviors. 1 Introduction Bandit structured prediction is the task of learning to solve complex joint prediction problems (like parsing or machine translation) under a very limited feedback model: a system must produce a single structured output (e.g., translation) and then the world reveals a score that measures how good or bad that output is, but provides neither a “correct” output nor feedback on any other possible output (Chang et al., 2015; Sokolov et al., 2015). Because of the extreme sparsity of this feedback, a common experimental setup is that one pre-trains a good-but-not-great “reference” system based on whatever labeled data is available, and then seeks to improve it over time using this bandit feedback. A common motivation for this problem setting is cost. In the case of translation, bilingual “experts” can read a source sentence and a possible translation, and can much more quickly provide a rating of that translation than they can produce a full translation on their own. Furthermore, one can often collect even less expensive ratings from “n"
D17-1153,D15-1130,0,0.028197,"e (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards. Our results show that bandit feedback can be an effective feedback mechanism for neural machine translation systems. This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008). An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al., 2015; Rabinovich et al., 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010). Another direction is to apply active learning techniques to reduce the sample complexity required to improve the systems or to extend to richer action spaces for problems like simultaneous translation, which requires prediction (Grissom II et al., 2014) and reordering (He et al., 2015) among other strategies to both minimize delay and effectively translate a sentence (He et al., 2016). Acknowledgements Many thanks to Yvette Graham for her help with the WMT human evaluations dat"
D17-1153,Q14-1025,0,0.0176647,"tackle this problem using neural encoder-decoder models, but we (a) take advantage of a state-of-the-art reinforcement learning method; (b) devise a strategy to simulate noisy rewards; and (c) demonstrate the robustness of our method on noisy simulated rewards. Our results show that bandit feedback can be an effective feedback mechanism for neural machine translation systems. This is despite that errors in human annotations hurt machine learning models in many NLP tasks (Snow et al., 2008). An obvious question is whether we could extend our framework to model individual annotator preferences (Passonneau and Carpenter, 2014) or learn personalized models (Mirkin et al., 2015; Rabinovich et al., 2017), and handle heteroscedastic noise (Park, 1966; Kersting et al., 2007; Antos et al., 2010). Another direction is to apply active learning techniques to reduce the sample complexity required to improve the systems or to extend to richer action spaces for problems like simultaneous translation, which requires prediction (Grissom II et al., 2014) and reordering (He et al., 2015) among other strategies to both minimize delay and effectively translate a sentence (He et al., 2016). Acknowledgements Many thanks to Yvette Grah"
D17-1153,K15-1001,0,\N,Missing
D17-1153,E17-1101,0,\N,Missing
D17-1153,2015.iwslt-evaluation.1,0,\N,Missing
D18-1208,P17-1190,0,0.0607382,"Missing"
D18-1208,K17-1045,0,0.0330819,"sequence-to-sequence model to predict which sentences to include in the summary. Subsequently, Nallapati et al. (2017) proposed a different model using word-level bidirectional RNNs along with a sentence level bidirectional RNN for predicting which sentences should be extracted. Their sentence extractor creates representations of the whole document and computes separate scores for salience, novelty, and location. These works represent the state-of-the-art for deep learningbased extractive summarization and we analyze them further in this paper. Other recent neural network approaches include, Yasunaga et al. (2017), who learn a graphconvolutional network (GCN) for multi-document summarization. They do not closely examine the choice of sentence encoder, which is one of the focuses of the present paper; rather, they study the best choice of graph structure for the GCN, which is orthogonal to this work. Non-neural network learning-based approaches have also been applied to summarization. Typically they involve learning n-gram feature weights in linear models along with other non-lexical word or structural features (Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Durrett et al., 2016). In this paper, we"
D18-1208,W14-3348,0,\N,Missing
D18-1208,W05-0908,0,\N,Missing
D18-1208,D14-1162,0,\N,Missing
D18-1208,P16-1046,0,\N,Missing
D18-1208,P16-1188,0,\N,Missing
D18-1208,E17-2008,0,\N,Missing
D18-1208,P11-1049,0,\N,Missing
D18-1208,E12-1023,0,\N,Missing
D19-1063,D16-1155,0,0.0452418,"Missing"
D19-1063,D18-1287,0,0.0841355,"Missing"
D19-1063,W17-4304,1,0.858886,"Missing"
D19-1063,D17-1106,0,0.0564717,"Missing"
D19-1063,W14-4313,0,0.0318955,"ating images that can convince human perception (Karras et al., 2017, 2018), simulating language interaction remains challenging. There are efforts in building complex interactive text-based worlds (Cˆot´e et al., 2018; Urbanek et al., 2019) but the lack of a graphical component makes them not suitable for visually grounded learning. On the other hand, experimentation on real humans and robots, despite expensive and time-consuming, are important for understanding the true complexity of real-world scenarios (Chai et al., 2018, 2016; Rybski et al., 2007; Mohan and Laird, 2014; Liu et al., 2016; She et al., 2014). 1. Constructing the H ANNA simulator by augmenting an indoor photo-realistic simulator with simulated human assistance, mimicking a scenario where a mobile agent finds objects by asking for directions along the way (§3). 2. An effective model and training algorithm for the H ANNA problem, which includes a hierarchical memory-augmented recurrent architecture that models human assistance as sub-goals (§ 5), and introduces an imitation learning objective that enhances exploration of the environment and interpretability of the agent’s help-request decisions. (§4). We embed the H ANNA problem in"
D19-1063,N19-1197,0,0.0249583,"st and future behavior to determine actions that are most and least beneficial to them, combining the advantages of both model-based and progress-estimating methods (Wang et al., 2018; Ma et al., 2019a,b). 3 Request Multimodal Simulated assistance instructions humans Problem V LN V NLA C VDN H ANNA (this work) 7 3 3 3 7 7 7 3 7 3 7 3 Table 1: Comparing H ANNA with other photo-realistic navigation problems. V LN (Anderson et al., 2018b) does not allow agent to request help. V NLA (Nguyen et al., 2019) models an advisor who is always present to help but speaks simple, templated language. C VDN (Thomason et al., 2019b) contains natural conversations in which a human assistant aids another human in navigation tasks but offers limited language interaction simulation, as language assistance is not available when the agent deviates from the collected trajectories and tasks. H ANNA simulates human assistants that provide language-and-vision instructions that adapt to the agent’s current position and goal. dom location ( ), is given a task request, and is allotted a budget of T time steps to complete the task. The agent succeeds the if its final location is within success meters of the location of any instance"
D19-1063,D19-1062,0,0.0612319,"Missing"
D19-5411,D18-1443,0,0.0273634,"Missing"
D19-5411,P17-4012,0,0.0187282,"Missing"
D19-5411,N19-4009,0,0.0318425,"Missing"
D19-5411,N19-1204,0,0.232315,"uman-written summaries is high. It generally takes a long time for a human to summarize a document, as they not only have to read and understand information in the article, but also have to 90 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 90–97 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics lines at all). Previous work on cross-lingual summarization performs evaluation with human judgments (Orsan and Chiorean, 2008), or with automatic metrics and noisy source articles generated by automatic translation systems (Wan et al., 2010; Ouyang et al., 2019). The former approach is expensive and not reproducible, while the latter is prone to biases induced by translation systems that could be further amplified by summarization systems. This paper presents Global Voices, a highquality multilingual dataset of summaries of news articles. The dataset can serve as a standard benchmark in both multilingual and cross-lingual summarization. Global Voices1 is a multilingual website that reports and translates news about unheard voices across the globe. Translation in this website is performed by the Lingua team,2 consisting of volunteer translators. As of"
D19-5411,D15-1044,0,0.0464511,"summary is based on subjective and common-sense rules that vary among individuals and are difficult to be expressed precisely in words. Even in monolingual summarization, there were limited attempts in constructing summarization datasets via crowd-sourcing (Over et al., 2007; Dang and Owczarzak, 2008, 2010). These datasets are mostly used for evaluation due to their small sizes. To construct large-scale training datasets, researchers mine news sources that naturally provide human-written summaries (Hermann et al., 2015; Sandhaus, 2008), or construct artificial summaries from document titles (Rush et al., 2015). Summaries collected in this way may be not best for evaluation because they are generated under unknown guidelines (or there may be no guideIntroduction Cross-lingual summarization is an important but highly unexplored task. The ability to summarize information written or spoken in any language at a large scale would empower humans with much more knowledge about the diverse world. Despite the fast development of automatic summarization (Allahyari et al., 2017; Dong, 2018; Gambhir and Gupta, 2017), present technology mostly focuses on monolingual summarization. There is currently lacking a st"
D19-5411,P10-1094,0,0.165943,"of crowsourcing human-written summaries is high. It generally takes a long time for a human to summarize a document, as they not only have to read and understand information in the article, but also have to 90 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 90–97 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics lines at all). Previous work on cross-lingual summarization performs evaluation with human judgments (Orsan and Chiorean, 2008), or with automatic metrics and noisy source articles generated by automatic translation systems (Wan et al., 2010; Ouyang et al., 2019). The former approach is expensive and not reproducible, while the latter is prone to biases induced by translation systems that could be further amplified by summarization systems. This paper presents Global Voices, a highquality multilingual dataset of summaries of news articles. The dataset can serve as a standard benchmark in both multilingual and cross-lingual summarization. Global Voices1 is a multilingual website that reports and translates news about unheard voices across the globe. Translation in this website is performed by the Lingua team,2 consisting of volunt"
D19-5411,orasan-chiorean-2008-evaluation,0,\N,Missing
D19-5411,W13-3102,0,\N,Missing
D19-5411,W13-3103,0,\N,Missing
E12-1021,W09-2206,0,0.812665,"a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set (Section 2.1). To improve document-topic distributions, we encourage the model to select document-level topics based on the existence of input seed words in that document (Sec"
E12-1021,D07-1109,0,0.00808628,"he bootstrapping literature for learning semantic lexicons (Thelen and Riloff, 2002). The goals are similar as well: growing a small set of seed examples into a much larger set. A key difference is the type of semantic information that the two approaches aim to capture: semantic lexicons are based on much more specific notions of semantics (e.g. all the country names) than the generic “topic” semantics of topic models. The idea of seeding has also been used in prototype-driven learning (Haghighi and Klein, 2006) and shown similar efficacies for these semisupervised learning approaches. LDAWN (Boyd-Graber et al., 2007) models sets of words for the word sense disambiguation 208 task. It assumes that a topic is a distribution over synsets and relies on the Wordnet to obtain the synsets. The most related prior work is that of (Andrzejewski et al., 2009), who propose the use Dirichlet Forest priors to incorporate Must Link and Cannot Link constraints into the topic models. This work is analogous to constrained K-means clustering (Wagstaff et al., 2001; Basu et al., 2008). A must link between a pair word types represents that the model should encourage both the words to have either high or low probability in any"
E12-1021,N06-1041,0,0.0332803,"t al., 2011) will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting. 2 Incorporating Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to th"
E12-1021,P11-1026,0,0.319112,"categories of the Reuters-21578 categorization corpus. We use them as running example in the rest of the paper. papers that such topics should exist in the corpus. By allowing the user to provide some seed words related to these underrepresented topics, we encourage the model to find evidence of these topics in the data. Importantly, we only encourage the model to follow the seed sets and do not force it. So if it has compelling evidence in the data to overcome the seed information then it still has the freedom to do so. Our seeding approach in combination with the interactive topic modeling (Hu et al., 2011) will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting. 2 Incorporating Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and"
E12-1021,N09-1036,0,0.0199187,"Missing"
E12-1021,D09-1026,0,0.900824,"ate a word from its distribution. 2.4 Automatic Seed Selection In (Andrzejewski and Zhu, 2009; Andrzejewski et al., 2009), the seed information is provided manually. Here, we describe the use of feature selection techniques, prevalent in the classification literature, to automatically derive the seed sets. If we want the topicality structure identified by the LDA to align with the underlying class structure, then the seed words need to be representative of the underlying topicality structure. To enable this, we first take class labeled data (doesn’t need to be multi-class labeled data unlike (Ramage et al., 2009)) and identify the discriminating features for each class. Then we choose these discriminating features as the initial sets of seed words. In principle, this is similar to the prototype driven unsupervised learning (Haghighi and Klein, 2006). We use Information Gain (Mitchell, 1997) to identify the required discriminating features. The Information Gain (IG) of a word (w) in a class (c) is given by IG(c, w) = H(c) − H(c|w) where H(c) is the entropy of the class and H(c|w) is the conditional entropy of the class given the word. In computing Information Gain, we binarize the document vectors and"
E12-1021,W02-1028,0,0.0208076,"combination with the interactive topic modeling (Hu et al., 2011) will allow a user to both explore a corpus, and also guide the exploration towards the distinctions that he/she finds more interesting. 2 Incorporating Seeds Our approach to allowing a user to guide the topic discovery process is to let him provide seed information at the level of word type. Namely, the user provides sets of seed words that are representative of the corpus. Table 2 shows an example of seed sets one might use for the Reuters corpus. This kind of supervision is similar to the seeding in bootstrapping literature (Thelen and Riloff, 2002) or prototype-based learning (Haghighi and Klein, 2006). Our reliance on seed sets is orthogonal to existing approaches that use external knowledge, which operate at the level of documents (Blei and McAuliffe, 2008), tokens (Andrzejewski and Zhu, 2009) or pair-wise constraints (Andrzejewski et al., 2009). We build a model that uses the seed words in two ways: to improve both topic-word and document-topic probability distributions. For ease of exposition, we present these ideas separately and then in combination (Section 2.3). To improve topic-word distributions, we set up a model in which each"
E12-1076,W10-0701,0,0.00692588,"ace, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and γ (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced. It is important to note that this evaluation compares full generation systems; many factors are"
E12-1076,2005.mtsummit-papers.11,0,0.0171464,"possible trees and removes mark-up to produce a final string. This is also the stage where punctuation may be added. Different strings may be generated depending on different specifications from the user, as discussed at the beginning of Section 4 and shown in the online demo. To evaluate the system against other systems, we specify that the system should (1) not hallucinate likely verbs; and (2) return the longest string possible. 4.4.1 Step 7: Get Final Tree, Clear Mark-Up We explored two methods for selecting a final string. In one method, a trigram language model built using the Europarl (Koehn, 2005) data with start/end symbols returns the highest-scoring description (normalizing for length). In the second method, we limit the generation system to select the most likely closed-class words (determiners, prepositions) while building the subtrees, overgenerating all possible adjective combinations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To ord"
E12-1076,P08-1119,0,0.0302625,"Missing"
E12-1076,P98-1116,0,0.0450347,"structions between objects if specified at runtime. A similar approach was taken in Yang et al. (2011). Some examples are given in Figure 7. We follow a three-tiered generation process (Reiter and Dale, 2000), utilizing content determination to first cluster and order the object nouns, create their local subtrees, and filter incorrect detections; microplanning to construct full syntactic trees around the noun clusters, and surface realization to order selected modifiers, realize them as postnominal or prenominal, and select final outputs. The system follows an overgenerate-andselect approach (Langkilde and Knight, 1998), which allows different final trees to be selected with different settings. 4.1 Knowledge Base Midge uses a knowledge base that stores models for different tasks during generation. These models are primarily data-driven, but we also include a hand-built component to handle a small set of rules. The data-driven component provides the syntactically informed word co-occurrence statistics learned from the Flickr data, a model for ordering the selected nouns in a sentence, and a model to change computer vision attributes to attribute:value pairs. Below, we discuss the three main data-driven models"
E12-1076,W11-0326,1,0.644361,", the generator filters and constrains the noisy detections output from a vision system to generate syntactic trees that detail what the computer vision system sees. Results show that the generation system outperforms state-of-the-art systems, automatically generating some of the most natural image descriptions to date. 1 Introduction It is becoming a real possibility for intelligent systems to talk about the visual world. New ways of mapping computer vision to generated language have emerged in the past few years, with a focus on pairing detections in an image to words (Farhadi et al., 2010; Li et al., 2011; Kulkarni et al., 2011; Yang et al., 2011). The goal in connecting vision to language has varied: systems have started producing language that is descriptive and poetic (Li et al., 2011), summaries that add content where the computer vision system does not (Yang et al., 2011), and captions copied directly from other images that are globally (Farhadi et al., 2010) and locally similar (Ordonez et al., 2011). A commonality between all of these approaches is that they aim to produce naturalsounding descriptions from computer vision detections. This commonality is our starting point: We aim to des"
E12-1076,P11-2041,1,0.471394,"econd method, we limit the generation system to select the most likely closed-class words (determiners, prepositions) while building the subtrees, overgenerating all possible adjective combinations. The final string is then the one with the most words. We find that the second method produces descriptions that seem more natural and varied than the n-gram ranking method for our development set, and so use the longest string method in evaluation. 4.4.2 Step 8: Prenominal Modifier Ordering To order sets of selected adjectives, we use the top-scoring prenominal modifier ordering model discussed in Mitchell et al. (2011). This is an ngram model constructed over noun phrases that were extracted from an automatically parsed version of the New York Times portion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and γ (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Hu"
E12-1076,W11-1611,0,0.00779403,"Missing"
E12-1076,W10-0721,0,0.417713,"ed on object nouns, and this enables each noun head in a description to select for the kinds of structures it tends to appear in (syntactic constraints) and the other words it tends to occur with (semantic constraints). This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the vision system predicts and how the object noun tends to be described. 2 Background Our approach to describing images starts with a system from Kulkarni et al. (2011) that composes novel captions for images in the PASCAL sentence data set,2 introduced in Rashtchian et al. (2010). This provides multiple object detections based on Felzenszwalb’s mixtures of multiscale deformable parts models (Felzenszwalb et al., 2008), and stuff detections (roughly, mass nouns, things like sky and grass) based on linear SVMs for low level region features. Appearance characteristics are predicted using trained detectors for colors, shapes, textures, and materials, an idea originally introduced in Farhadi et al. (2009). Local texture, Histograms of Oriented Gradients (HOG) (Dalal and Triggs, 2005), edge, and color descriptors inside the bounding box of a recognized object are binned int"
E12-1076,J09-4008,0,0.01131,"ion of the Gigaword corpus (Graff and Cieri, 2003). With this in place, blue clear sky becomes clear blue sky, wooden brown table becomes brown wooden table, etc. 5 Evaluation Each set of sentences is generated with α (likelihood cutoff) set to .01 and γ (observation count cutoff) set to 3. We compare the system against human-written descriptions and two state-of-theart vision-to-language systems, the Kulkarni et al. (2011) and Yang et al. (2011) systems. Human judgments were collected using Amazon’s Mechanical Turk (Amazon, 2011). We follow recommended practices for evaluating an NLG system (Reiter and Belz, 2009) and for running a study on Mechanical Turk (Callison-Burch and Dredze, 2010), using a balanced design with each subject rating 3 descriptions from each system. Subjects rated their level of agreement on a 5-point Likert scale including a neutral middle position, and since quality ratings are ordinal (points are not necessarily equidistant), we evaluate responses using a non-parametric test. Participants that took less than 3 minutes to answer all 60 questions and did not include a humanlike rating for at least 1 of the 3 human-written descriptions were removed and replaced. It is important to"
E12-1076,D11-1041,1,\N,Missing
E12-1076,C04-1096,0,\N,Missing
E12-1076,C98-1112,0,\N,Missing
E12-1076,W10-0707,0,\N,Missing
E12-1076,P08-1000,0,\N,Missing
E14-1069,P13-1162,0,0.017982,"gue and are likely to attract opposition. We use several resources to capture this information. We use a lexicon of subjective and positive/negative sentiment expressions (Riloff and Wiebe, 2003). This resource can help identify subjective statements attempting to bias the discussion (e.g., “So he was driving negligently?”) We use a list of hedges and boosters (Hyland, 2005). This resource can potentially allow the model to identify evasive (“I might have seen him”) and (overly) confident responses (“I am absolutely sure that I have seen him”). We use a lexicon of biased language provided by (Recasens et al., 2013), this lexicon extracted from Wikipedia edits consists of words indicative of bias, for example in an attempt to frame the facts raised in the discussion according to one of the viewpoints (“The death of Nicolle Simposon” vs. “The murder of Nicolle Simposon”). Finally we use a Patient Polarity Verbs lexicon (Goyal et al., 2010). This lexicon consists 2.1 Mining Courtroom Proceedings The first step in forming our dataset consists of collecting a large set of relevant courtroom dialogue snippets. First, we look for textual occurrences of objections in the trial transcript by looking for sustain"
E14-1069,W03-1014,0,0.105716,"r relevant utterances, such as ones making claims associating individuals with locations. We use the Named Entity Recognizer (NER) described in (Finkel et al., 2005) to identify this information. (2) Subjective and Biased Language Equally important to understanding the topics of conversation is the way they are discussed. Expressions of subjectivity and sentiment are useful linguistic tools for changing the tone of the dialogue and are likely to attract opposition. We use several resources to capture this information. We use a lexicon of subjective and positive/negative sentiment expressions (Riloff and Wiebe, 2003). This resource can help identify subjective statements attempting to bias the discussion (e.g., “So he was driving negligently?”) We use a list of hedges and boosters (Hyland, 2005). This resource can potentially allow the model to identify evasive (“I might have seen him”) and (overly) confident responses (“I am absolutely sure that I have seen him”). We use a lexicon of biased language provided by (Recasens et al., 2013), this lexicon extracted from Wikipedia edits consists of words indicative of bias, for example in an attempt to frame the facts raised in the discussion according to one of"
E14-1069,D11-1039,0,0.0322247,"ant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Discriminative latent variables models have seen a surge of interest in recent years, both in the machine learning community (Yu and Joachims, 2009; Quattoni et al., 2007) as well as various application domains such as NLP (T¨ackstr¨om and McDonald, 2011) and computer vision (Felzenszwalb et al., 2010). In NLP, one of the most wellknown applications of discriminative latent strucInstead of focusing on actions, like the above work, we focus on dialogue content and relationships between utterances. Furthermore, unlike m"
E14-1069,W05-0613,0,0.0268158,"he structure prediction inference problem (Roth and Yih, 2007). Our prediction task, identifying the actionable result of a dialogue, requires capturing the dialogue and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Discriminative latent variables models have seen a surge of interest in recent years, both in the machine learning community (Yu and Joachims, 2009; Quattoni et al., 2007) as well"
E14-1069,J00-3003,0,0.365358,"Missing"
E14-1069,P08-1090,0,0.0404005,"ted sentences as irrelevant, while marking topically related sentences and identifying the connection between the question-answer pair (decisions marked in solid blue lines). When trained without situated information, the latent output structure marks topically unrelated sentences as relevant for objection classification. Note that in this case all the edge variables are turned off (marked with dashed red lines). A related area of work with different motivations and different technical approaches has focused on attempting to understand narrative structure. For instance, Chambers and Jurafsky (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) model narrative flow in the style of Schankian scripts (Schank and Abelson, 1977). Their focus is on common sequences of actions, not specifically related to dialogue. Somewhat more related is recent work (Goyal et al., 2010) that aimed to build a computational model of Lehnert’s Plot Units (Lehnert, 1981) model. That work focused primarily on actions and not on dialogue: in fact, their results showed that the lack of dialogue understanding was a significant detriment to their ability to model plot structure. room dialogues. We adopted the structured latent varia"
E14-1069,P09-1068,0,0.033846,"while marking topically related sentences and identifying the connection between the question-answer pair (decisions marked in solid blue lines). When trained without situated information, the latent output structure marks topically unrelated sentences as relevant for objection classification. Note that in this case all the edge variables are turned off (marked with dashed red lines). A related area of work with different motivations and different technical approaches has focused on attempting to understand narrative structure. For instance, Chambers and Jurafsky (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009) model narrative flow in the style of Schankian scripts (Schank and Abelson, 1977). Their focus is on common sequences of actions, not specifically related to dialogue. Somewhat more related is recent work (Goyal et al., 2010) that aimed to build a computational model of Lehnert’s Plot Units (Lehnert, 1981) model. That work focused primarily on actions and not on dialogue: in fact, their results showed that the lack of dialogue understanding was a significant detriment to their ability to model plot structure. room dialogues. We adopted the structured latent variable model defined in (Chang et"
E14-1069,N10-1066,1,0.929036,"ng margin-based optimization problem, where λ is a regularization parameter, and ` is the squared-hinge loss function: In our experiments, we formalize Eq. (4) as an ILP instance, which we solve using the highly optimized Gurobi toolkit4 . 4 Learning and Inference 6 refers to all linguistic resources used. We also included a +/-1 word window around words appearing in these resources http://www.gurobi.com/ “*” denotes all properties 659 4.2 This formulation is not a convex optimization problem and care must be taken to find a good optimum. In our experiments, we use the algorithm presented in (Chang et al., 2010) to solve this problem. The algorithm solves this non-convex optimization function iteratively, decreasing the value of the objective in each iteration until convergence. In each iteration, the algorithm determines the values of the latent variables of positive examples, and optimizes the modified objective function using a cutting plane algorithm. This algorithmic approach is conceptually (and algorithmically) related to the algorithm suggested by (Yu and Joachims, 2009). As standard, we classify x as positive iff fw (x) ≥ 0. In Eq. (4), wT φs (x) is the score associated with the substructure"
E14-1069,N13-1100,0,0.0210085,"ki/Foundation_ (evidence) 661 … MR. DARDEN THE COURT MS. KESTLER tured classification is to the Textual Entailment (TE) task (Chang et al., 2010; Wang and Manning, 2010). The TE task bears some resemblances ours, as both tasks require making a binary decision on the basis of a complex input object (i.e., the history of dialogue, pairs of paragraphs), creating the need for a learning framework that is flexible enough to model the complex latent structure that exists in the input. Another popular application domain is sentiment analysis (Yessenalina et al., 2010; T¨ackstr¨om and McDonald, 2011; Trivedi and Eisenstein, 2013). The latent variable model allows the learner to identify finer grained sentiment expression than annotated in the data. Your Honor, this is hearsay Overruled I don&apos;t recall if there was that day or not. I know at some point, we had a meeting as to what evidence we had and what was going to be tested and who was going to test it. MR. NEUFELD On the very next day, June 16th, did you participate in another meeting about this case? MS. KESTLER I don&apos;t recall. MR. NEUFELD Do you recall being at a meeting with Erin Reilly, Collin Yamauchi and Dennis Fung and … Greg Matheson about this case on June"
E14-1069,P05-1045,0,0.00631614,"dication of structure, topics of controversy, and the sentiment and tone of language used in the dialogue. The second captures pragmatic considerations by situating the dialogue utterances in the context of the courtroom. Each utterance is attributed to a speaker, thus capturing meaningful patterns specific to individual speakers. Linguistic Resources (1) Named Entities provide strong indications of the topics discussed in the dialogue and help uncover relevant utterances, such as ones making claims associating individuals with locations. We use the Named Entity Recognizer (NER) described in (Finkel et al., 2005) to identify this information. (2) Subjective and Biased Language Equally important to understanding the topics of conversation is the way they are discussed. Expressions of subjectivity and sentiment are useful linguistic tools for changing the tone of the dialogue and are likely to attract opposition. We use several resources to capture this information. We use a lexicon of subjective and positive/negative sentiment expressions (Riloff and Wiebe, 2003). This resource can help identify subjective statements attempting to bias the discussion (e.g., “So he was driving negligently?”) We use a li"
E14-1069,C10-1131,0,0.0608629,"Missing"
E14-1069,D10-1040,0,0.0269786,"e and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Discriminative latent variables models have seen a surge of interest in recent years, both in the machine learning community (Yu and Joachims, 2009; Quattoni et al., 2007) as well as various application domains such as NLP (T¨ackstr¨om and McDonald, 2011) and computer vision (Felzenszwalb et al., 2010). In NLP, one of the most well"
E14-1069,D10-1102,0,0.0287443,"to the results in Table 2. 9 10 http://en.wikipedia.org/wiki/Foundation_ (evidence) 661 … MR. DARDEN THE COURT MS. KESTLER tured classification is to the Textual Entailment (TE) task (Chang et al., 2010; Wang and Manning, 2010). The TE task bears some resemblances ours, as both tasks require making a binary decision on the basis of a complex input object (i.e., the history of dialogue, pairs of paragraphs), creating the need for a learning framework that is flexible enough to model the complex latent structure that exists in the input. Another popular application domain is sentiment analysis (Yessenalina et al., 2010; T¨ackstr¨om and McDonald, 2011; Trivedi and Eisenstein, 2013). The latent variable model allows the learner to identify finer grained sentiment expression than annotated in the data. Your Honor, this is hearsay Overruled I don&apos;t recall if there was that day or not. I know at some point, we had a meeting as to what evidence we had and what was going to be tested and who was going to test it. MR. NEUFELD On the very next day, June 16th, did you participate in another meeting about this case? MS. KESTLER I don&apos;t recall. MR. NEUFELD Do you recall being at a meeting with Erin Reilly, Collin Yamau"
E14-1069,D10-1008,1,0.901996,"Missing"
E14-1069,P97-1013,0,0.0510918,"their ability to model plot structure. room dialogues. We adopted the structured latent variable model defined in (Chang et al., 2010), and use ILP to solve the structure prediction inference problem (Roth and Yih, 2007). Our prediction task, identifying the actionable result of a dialogue, requires capturing the dialogue and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialogue processing from its structure. Di"
E14-1069,prasad-etal-2008-penn,0,0.03594,"nderstanding was a significant detriment to their ability to model plot structure. room dialogues. We adopted the structured latent variable model defined in (Chang et al., 2010), and use ILP to solve the structure prediction inference problem (Roth and Yih, 2007). Our prediction task, identifying the actionable result of a dialogue, requires capturing the dialogue and discourse relations. While we view these relations as latent variables in the context of action prediction, studying these relations independently has been the focus of significant research efforts, such as discourse relations (Prasad et al., 2008), rhetorical structure (Marcu, 1997) and dialogue act modeling (Stolcke et al., 2000). Fully supervised approaches for learning to predict dialogue and discourse relations (such as (Baldridge and Lascarides, 2005)) typically requires heavy supervision and has been applied only to limited domains. Moving away from full supervision, the work of (Golland et al., 2010) uses a game-theoretic model to explicitly model the roles of dialogue participants. In the context of dialogue and situated language understanding, the work of (Artzi and Zettlemoyer, 2011) shows how to derive supervision for dialog"
E14-1069,miltsakaki-etal-2004-penn,0,\N,Missing
E14-1069,P13-2014,0,\N,Missing
H05-1013,N04-1001,0,\N,Missing
H05-1013,P05-1077,0,\N,Missing
H05-1013,P02-1014,0,\N,Missing
H05-1013,J01-4004,0,\N,Missing
H05-1013,P03-1001,0,\N,Missing
J05-4004,P00-1041,0,0.056655,"Missing"
J05-4004,W03-1004,0,0.0611087,"Missing"
J05-4004,J93-2003,0,0.0107334,"Missing"
J05-4004,C96-2183,0,0.107973,"Missing"
J05-4004,P02-1057,1,0.91229,"Missing"
J05-4004,P03-1011,0,0.015612,"tates identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely na¨ıve in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is just a collection of nearb"
J05-4004,A00-1043,0,0.0762817,"Missing"
J05-4004,J02-4006,0,0.345357,"rtz, Zajic, and Dorr 2002), based on IBM Model 1 (Brown et al. 1993). These models treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines. Such mappings can be considered word-to-word alignments, but as our results show (see Section 5), these models are too weak for capturing the sophisticated operations that are employed by humans in summarizing texts. To date, there has been very little work on the word alignment task in the context of summarization. The most relevant work is that of Jing (2002), in which a hidden Markov alignment model is applied to the task of identifying word and phrase-level correspondences between documents and abstracts. Unfortunately, this model is only able to align words that are identical up to their stems, and thus suffers from a problem of recall. This also makes it ill-suited to the task of learning how to perform abstraction, in which one would desire to know how words get changed. For example, Jing’s model cannot identify any of the following alignments from Figure 1: (Connecting Point ↔ Connecting Point Systems), (Mac ↔ Macintosh), (retailer ↔ seller)"
J05-4004,W01-0100,0,0.210724,"Missing"
J05-4004,P00-1056,0,0.0810572,". By doing so, one could recreate the extracts at each iteration using the previous iteration’s parameters to make better and shorter extracts. Similarly, one might only allow summary words to align to words found in their corresponding extract sentences, which would serve to significantly speed up training and, combined with the parameterized extracts, might not hurt performance. A final option, but one that we do not advocate, would be to give up on phrases and train the model in a word-to-word fashion. This could be coupled with heuristic phrasal creation as is done in machine translation (Och and Ney 2000), but by doing this, one completely loses the probabilistic interpretation that makes this model so pleasing. Aside from computational considerations, the most obvious future effort along the lines of this model is to incorporate it into a full document summarization system. Since this can be done in many ways, including training extraction systems, compression systems, headline generation systems, and even extraction systems, we left this to future work so that we could focus specifically on the alignment task in this article. Nevertheless, the true usefulness of this model will be borne out"
J05-4004,J03-1002,0,0.0395148,"which one would desire to know how words get changed. For example, Jing’s model cannot identify any of the following alignments from Figure 1: (Connecting Point ↔ Connecting Point Systems), (Mac ↔ Macintosh), (retailer ↔ seller), (Macintosh ↔ Apple Macintosh systems) and (January 1989 ↔ last January). Word alignment (and, to a lesser degree, phrase alignment) has been an active topic of research in the machine translation community. Based on these efforts, one might be initially tempted to use readily available alignment models developed in the context of machine translation, such as GIZA++ (Och and Ney 2003), to obtain wordlevel alignments in document, abstract corpora. However, as we will show (Section 5), the alignments produced by such a system are inadequate for the document, abstract alignment task. 1.4 Article Structure In this article, we describe a novel, general model for automatically inducing wordand phrase-level alignments between documents and their human-written abstracts. Beginning in Section 2, we will describe the results of human annotation of such alignments. Based on this annotation, we will investigate the empirical linguistic properties of such alignments, including lexi"
J05-4004,W04-3219,0,0.117887,"Missing"
J05-4004,W97-0710,0,0.116112,"Missing"
J05-4004,C96-2141,0,0.09177,"Missing"
J05-4004,P01-1067,0,0.0259234,"s empirical variance. We model null states identically to the relative jump model. 4.1.3 Syntax-Aware Jump Model. Both of the previously described jump models are extremely na¨ıve in that they look only at the distance jumped and completely ignore what is being jumped over. In the syntax-aware jump model, we wish to enable the model to take advantage of syntactic knowledge in a very weak fashion. This is quite different from the various approaches to incorporating syntactic knowledge into machine translation systems, wherein strong assumptions about the possible syntactic operations are made (Yamada and Knight 2001; Eisner 2003; Gildea 2003). To motivate this model, consider the first document sentence shown with its syntactic parse tree in Figure 6. Though it is not always the case, forward jumps of distance more than one are often indicative of skipped words. From the standpoint of the relative jump models, jumping over the four words tripled it ’s sales and jumping over the four words of Apple Macintosh systems are exactly the same.7 However, intuitively, we would be much more willing to jump over the latter than the former. The latter phrase is a full syntactic constituent, while the first phrase is"
J05-4004,P00-1038,0,\N,Missing
J05-4004,P03-2041,0,\N,Missing
J05-4004,J96-2004,0,\N,Missing
N09-1058,W05-0909,0,0.0424591,"ake use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4 http://www.statmt.org/moses/ http://www.statmt.org/wmt07/ 6 We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact. This suggests that for SMT performance, more data 5 518 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities. We can either turn these counts into conditional probabilities (by using SRILM) or use"
N09-1058,D07-1090,0,0.291499,"wm depends on previous n-1 words where n denotes the size of n-gram. This assumption that probability of predicting a current word depends on the previous words is called a Markov assumption, typically estimated by relative frequency: m−1 P (wm |wm−n+1 )= m−1 C(wm−n+1 wm ) m−1 C(wm−n+1 ) (1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that"
N09-1058,P96-1041,0,0.0247653,"Missing"
N09-1058,W06-3113,0,0.0169533,"s is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the"
N09-1058,N04-1035,0,0.0724516,"Missing"
N09-1058,D07-1091,0,0.0106414,"ly 32 out of 54k 7-grams contained in test set. The small recall value for 7-grams suggests that these counts may not be that useful in SMT. We further substantiate our findings in our extrinsic evaluations. There we show that integrating 7-gram stream counts with an SMT system does not affect its overall performance significantly. 5 Extrinsic Evaluation 5.1 Experimental Setup All the experiments conducted here make use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4 http://www.statmt.org/moses/ http://www.statmt.org/wmt07/ 6 We fou"
N09-1058,P02-1040,0,0.11221,"5.1 Experimental Setup All the experiments conducted here make use of publicly available resources. Europarl (EP) corpus French-English section is used as parallel data. The publicly available Moses4 decoder is used for training and decoding (Koehn and Hoang, 2007). The news corpus released for ACL SMT workshop in 2007 consisting of 1057 sentences5 is used as the development set. Minimum error rate training (MERT) is used on this set to obtain feature weights to optimize translation quality. The final SMT system performance is evaluated on a uncased test set of 3071 sentences using the BLEU (Papineni et al., 2002), NIST (Doddington, 2002) and METEOR (Banerjee and Lavie, 2005) scores. The test set is the union of the 2007 news devtest and 2007 news test data from ACL SMT workshop 2007.6 4 http://www.statmt.org/moses/ http://www.statmt.org/wmt07/ 6 We found that testing on Parliamentary test data was completely insensitive to large n-gram LMs, even when these LMs are exact. This suggests that for SMT performance, more data 5 518 5.2 Integrating stream counts feature into decoder Our method only computes high-frequency n-gram counts; it does not estimate conditional probabilities. We can either turn these"
N09-1058,P05-1077,0,0.594774,"Missing"
N09-1058,I08-2089,0,0.00743965,"ined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the target n-best hypothesis list is not diverse enough. Hence if possible it is always better to integrate LMs directly into the decoder. 2.3 Streaming Consider an algorithm that reads the input from a read-only stream from left to right, with no ability to go back to the input that it has alread"
N09-1058,P08-1058,0,0.228851,"sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT p"
N09-1058,P07-1065,0,0.23664,"ated by relative frequency: m−1 P (wm |wm−n+1 )= m−1 C(wm−n+1 wm ) m−1 C(wm−n+1 ) (1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machin"
N09-1058,D07-1049,0,0.730477,"ated by relative frequency: m−1 P (wm |wm−n+1 )= m−1 C(wm−n+1 wm ) m−1 C(wm−n+1 ) (1) Eq 1 estimates the n-gram probability by taking the ratio of observed frequency of a particular sequence and the observed frequency of the prefix. This is precisely the relative frequency estimate we seek. 2.2 Large-scale Language modeling Using higher order LMs to improve the accuracy of SMT is not new. (Brants et al., 2007; Emami et al., 2007) built 5-gram LMs over web using distributed cluster of machines and queried them via network requests. Since the use of cluster of machines is not always practical, (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a) showed a randomized data structure called Bloom filter, that can be used to construct space efficient language models for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machin"
N09-1058,C08-1114,0,0.0632375,"Missing"
N09-1058,P08-1086,0,0.00932061,"ilter, that can be used to construct space efficient language models for SMT. (Talbot and Brants, 2008) presented randomized language model based on perfect hashing combined with entropy pruning to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the target n-best hypothesis list is not diverse enough. Hence if possible it is always better to integrate LMs directly into the dec"
N09-1058,W06-1626,0,0.0110118,"to achieve further memory reductions. A problem mentioned in (Talbot and Brants, 2008) is that the algorithm that computes the compressed representation might need to retain the entire database in memory; in their paper, they design strategies to work around this problem. (Federico and Bertoldi, 2006) also used single machine and fewer bits to store the LM probability by using efficient prefix trees. (Uszkoreit and Brants, 2008) used partially classbased LMs together with word-based LMs to improve SMT performance despite the large size of the word-based models used. (Schwenk and Koehn, 2008; Zhang et al., 2006) used higher language models at time of re-ranking rather than integrating directly into the decoder to avoid the overhead of keeping LMs in the main memory since disk lookups are simply too slow. Now using higher order LMs at time of re-ranking looks like a good option. However, the target n-best hypothesis list is not diverse enough. Hence if possible it is always better to integrate LMs directly into the decoder. 2.3 Streaming Consider an algorithm that reads the input from a read-only stream from left to right, with no ability to go back to the input that it has already processed. This alg"
N09-1058,P08-1000,0,\N,Missing
N09-1067,P07-1009,1,0.441087,"Missing"
N12-1088,J93-2003,0,0.019058,"earned automatically. Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation). In contrast to prior work, our approach uses features defined separately within the input"
N12-1088,W06-2920,0,0.0551713,"the decoding score. The candidate tag sequences are reranked based on this final score. 4 Experiments In this section, we report POS tagging experiments on four languages: English, Chinese, French and English (En.) Chinese (Zh.) French (Fr.) Swedish (Sv.) # sent. # words # sent. # words # sent. # words # sent. # words Train. 15K 362K 50K 292K 9K 254K 8K 137K Dev. 2K 47K 4K 26K 2K 57K 2K 31K Test 1791 43K 3647 25K 1351 40K 1431 28K Table 1: Training and test data statistics. Swedish. The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). We only consider the word and its fine grained POS tag (columns 2 and 5 respectively) and ignore the dependency links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence."
N12-1088,P05-1022,0,0.885331,"ches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation). In contrast to prior work, our approach uses features defined separately within the input and output spaces, and learns a mapping function that can map an object from one space into the other. Since our approach requires within-space features, it makes the featu"
N12-1088,N09-1025,0,0.0627471,"Missing"
N12-1088,J05-1003,0,0.23992,"isting reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation). In contrast to prior work, our approach uses features defined separately within the input and output spaces, and learns a mapping function that can map an object from one space into the other. Since our approach requires within-space"
N12-1088,E09-1033,0,0.0497215,"Missing"
N12-1088,D07-1117,0,0.287037,"cs. Swedish. The data in all these languages is obtained from the CoNLL 2006 shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). We only consider the word and its fine grained POS tag (columns 2 and 5 respectively) and ignore the dependency links in the data. Table 1 shows the data statistics in each of these languages. We use a second order Hidden Markov Model (Thede and Harper, 1999) based tagger as a baseline tagger in our experiments. This model uses trigram transition and emission probabilities and is shown to achieve good accuracies in English and other languages (Huang et al., 2007). We refer to this as the baseline tagger in the rest of this paper and is used to produce n-best list for each candidate sentence. The n-best list for training data is produced using multifold cross-validation like Collins and Koo (2005) and Charniak and Johnson (2005). The first block of Table 2 shows the accuracies of the top-ranked tag sequence (according to the Viterbi decoding score) and the oracle accuracies on the 10-best list. As expected the accuracies on English and French are high and are on par with the state-of-the-art systems. From the oracle scores, it is clear that though ther"
N12-1088,N10-1095,0,0.0191335,"-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship be"
N12-1088,P06-1096,0,0.071271,"Missing"
N12-1088,P05-1012,0,0.157169,"d when we include the n-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm l"
N12-1088,W03-0402,0,0.092,"seline rankers improved when we include the n-gram features but it is still less than the discriminative model in most cases. Finally, Table 3 shows the performance of our models without combining with the Viterbi decoding score. As shown, the performance drops significantly and is in accordance with the behavior observed elsewhere (Collins and Koo, 2005). 5 Related Work In this section, we discuss approaches that are most relevant to our problem and the approach. In NLP literature, discriminative reranking has been well explored for parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen and Joshi, 2003; McDonald et al., 2005; Johnson and Ural, 2010) and statistical machine translation (Shen et al., 2004; Watanabe et al., 2007; Liang et al., 2006). Collins (2002) proposed two reranking approaches, namely boosting algorithm and a voted perceptron, for the POS tagging task. Later Huang et al. (2007) propose a regularized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each v"
N12-1088,N04-1023,0,0.192772,"inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation). In contrast to prior work, our approach uses features defined separately within the input and output spaces, and learns a mapping function that can map an object from one space into the other. Since our approach requires within-space features, it makes the feature engineering rela"
N12-1088,P99-1023,0,0.309017,"be done computationally efficiently. A key quality of our approach is that feature engineering can be done separately on the input and output spaces; the relationship between inputs and outputs is learned automatically. Experiments on part-of-speech tagging task in four languages show significant improvements over a baseline decoder and existing reranking approaches. 1 Introduction Mapping inputs to outputs lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear"
N12-1088,N07-2047,0,0.0300382,"ized version of the objective used by Collins (2002) and show an improved performance for Chinese. In all of the above reranking approaches, the feature functions are defined jointly on the input and output, whereas in our approach, the features are defined separately within each view and the algorithm learns the relationship between them automatically. This is the primary difference between our approach and the existing rerankers. In principle, our margin formulations are similar to the max margin formulations of CCA (Szedmak et al., 2007) and maximum margin regression (Szedmak et al., 2006; Wang et al., 2007). These approaches solve the following optimization problem: min kW k2 + C1T ξ (16) s.t. hyi , W φ(x)i i ≥ 1 − ξi ∀i = 1 · · · n 707 Our approach differs from these formulations in two main ways: the score assigned by our generative model (equivalent to CCA) for an input-output pair (xTi abT yi ) can be converted into this format by substituting W ← baT but in doing so we are ignoring the rank constraint. It is often observed that, dimensionality reduction leads to an improved performance and thus the rank constraint becomes crucial. Another major difference is that, the constraints in Eq. 16"
N12-1088,D07-1080,0,0.177082,"lies at the heart of many Natural Language Processing applications. For example, given a sentence as input: part-of-speech (POS) tagging involves finding the appropriate POS tag sequence (Thede and Harper, 1999); parsing involves finding the appropriate tree structure (Kubler et al., 2009) and statistical machine translation (SMT) involves finding correct target language translation (Brown et al., 1993). The accuracy achieved on such tasks can often be improved significantly with the help of a discriminative reranking step (Collins and Koo, 2005; Charniak and Johnson, 2005; Shen et al., 2004; Watanabe et al., 2007). Unfortunately, developing joint features over the input and output space can be challenging, especially in problems for which the exact mapping between the input and the output is unclear (for instance, in automatic caption generation for images, semantic parsing or non-literal translation). In contrast to prior work, our approach uses features defined separately within the input and output spaces, and learns a mapping function that can map an object from one space into the other. Since our approach requires within-space features, it makes the feature engineering relatively easy. For clarity"
N12-1088,P02-1062,0,\N,Missing
N12-1088,D11-1086,1,\N,Missing
N12-1094,P89-1010,0,0.244848,"nouns and adjectives automatically based on bootstrapping techniques. First, we construct a graph between adjectives by computing distributional similarity (Turney and Pantel, 2010) between them. For computing distributional similarity between adjectives, each target adjective is defined as a vector of nouns which are modified by the target adjective. To be exact, we use only those adjectives as modifiers which appear adjacent to a noun (that is, in a JJ NN construction). For example, in “small red apple,” we consider only red as a modifier for noun. We use Pointwise Mutual Information (PMI) (Church and Hanks, 1989) to weight the contexts, and select the top 1000 PMI contexts for each adjective.3 Next, we apply cosine similarity to find the top 10 distributionally similar adjectives with respect to each target adjective based on our large generic corpus (Large-Data from Section 2.1). This creates a graph with adjectives as nodes and cosine similarity as weight on the edges. Analogously, we construct a graph with nouns as nodes (here, adjectives are used as contexts for nouns). We then apply bootstrapping (Kozareva et al., 2008) on the noun and adjective graphs by selecting 10 seeds for visual and non-vis"
N12-1094,P10-1126,0,0.071421,"ful. In general, features in the phrase were most useful (not surprisingly), and then features before the phrase (presumably to give context, for instance as in “out of the window”). Features from after the phrase were not useful. 4 Non-singleton features appear more than once in the data. + + + + + + + + + + C ATEGORY Words Image Bootstrap Spell Length Words Wordnet Spell Spell Wordnet Wordnet P OSITION Phrase Phrase Phrase Before Phrase After Before Before After AUC 74.7 74.4 74.3 75.3 74.7 76.2 76.1 76.0 76.8 77.0 75.6 Sadeghi, 2011), and automatic caption generation (Farhadi et al., 2010; Feng and Lapata, 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012), it becomes increasingly important to understand, and to be able to detect, text that actually refers to observed phenomena. Our results suggest that while this is a hard problem, it is possible to leverage large text resources and state-of-the-art computer vision algorithms to address it with high accuracy. Acknowledgments Table 6: Results of feature ablation on L ARGE data set. Corresponding results on the L ARGE data set are shown in Table 6. Note that the order of features selected is"
N12-1094,P08-1119,0,0.0257797,"Missing"
N12-1094,W11-0326,1,0.693528,"eatures before the phrase (presumably to give context, for instance as in “out of the window”). Features from after the phrase were not useful. 4 Non-singleton features appear more than once in the data. + + + + + + + + + + C ATEGORY Words Image Bootstrap Spell Length Words Wordnet Spell Spell Wordnet Wordnet P OSITION Phrase Phrase Phrase Before Phrase After Before Before After AUC 74.7 74.4 74.3 75.3 74.7 76.2 76.1 76.0 76.8 77.0 75.6 Sadeghi, 2011), and automatic caption generation (Farhadi et al., 2010; Feng and Lapata, 2010; Ordonez et al., 2011; Kulkarni et al., 2011; Yang et al., 2011; Li et al., 2011; Mitchell et al., 2012), it becomes increasingly important to understand, and to be able to detect, text that actually refers to observed phenomena. Our results suggest that while this is a hard problem, it is possible to leverage large text resources and state-of-the-art computer vision algorithms to address it with high accuracy. Acknowledgments Table 6: Results of feature ablation on L ARGE data set. Corresponding results on the L ARGE data set are shown in Table 6. Note that the order of features selected is different because the training data is different. Here, the most useful features"
N12-1094,U08-1013,0,0.0289441,"Missing"
N12-1094,E12-1076,1,0.90543,"Missing"
N12-1094,W10-0721,0,0.0518612,"n Flickr is almost always written by the photographer of that image. This means the descriptions often contain information that is not actually pictured in the image, or contain references that are only relevant to the photographer (referring to a person/pet by name). One might think that this is an artifact of this particular dataset, but it appears to be generic to all captions, even those written by a viewer (rather than the photographer). Figure 2 shows an image from the Pascal dataset (Everingham et al., 2010), together with captions written by random people collected via crowd-sourcing (Rashtchian et al., 2010). There is much in this caption that is clearly made-up by the author, presumably to make the caption more interesting (e.g., meta-references like “the camera” or “A photo” as well as “guesses” about the image, such as “garage” and “venison”). Second, there is a question of how much inference you are allowed to do when you say that you “see” something. For example, in the top image in Figure 1, the street is pictured, but does that mean that “Hanbury St.” is visual? What if there were a street sign that clearly read “Hanbury St.” in the image? This problem comes up all the time, when people sa"
N12-1094,W02-1028,0,0.0376964,"Missing"
N12-1094,N10-1119,0,0.0106617,"VP to 20 visually descriptive predicates shown in the top of Table 2, and VA to all nouns that appear in the object argument position with respect to the seed predicates. We approximate this by taking nouns on the right hand side of the predicates within a window of 4 words using the Web 1T Google N-gram data (Brants and Franz., 2006). For edge weights, we use conditional probabilities between predicates and arguments so that w(p → a) := pr(a|p) and w(a → p) := pr(p|a). In order to collectively induce the visually descriptive words from this graph, we apply the graph propagation algorithm of Velikovich et al. (2010), a variant of label propagation algorithms (Zhu and Ghahramani, 2002) that has been shown to be effective for inducing a web-scale polarity lexicon based on word co-occurrence statistics. This algoColor Material Shape Size Surface Direction Pattern Quality Beauty Age Ethnicity purple blue maroon beige green plastic cotton wooden metallic silver circular square round rectangular triangular small big tiny tall huge coarse smooth furry fluffy rough sideways north upward left down striped dotted checked plaid quilted shiny rusty dirty burned glittery beautiful cute pretty gorgeous lovely young ma"
N12-1094,J90-1003,0,\N,Missing
N12-1094,D11-1041,1,\N,Missing
N12-1094,W10-0707,0,\N,Missing
N12-1094,W05-1003,0,\N,Missing
N13-1060,P08-1009,0,0.0152384,"traints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source se"
N13-1060,P05-1033,0,0.278373,"fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to tha"
N13-1060,J07-2003,0,0.789841,"into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy. 1 Hal Daum´e III University of Maryland College Park, USA hal@umiacs.umd.edu Introduction Hierarchical phrase-based (HPB) translation models (Chiang, 2005; Chiang, 2007) that utilize synchronous context free grammars (SCFG) have been widely adopted in statistical machine translation (SMT). Although formally syntactic, such models rarely respect linguistically-motivated syntax, and have no formal notion of semantics. As a result, they tend to produce translations containing both grammatical errors and semantic role confusions. Our goal is to take advantage of syntactic and semantic parsing to improve translation quality of HPB translation models. Rather than introducing semantic structure into the HPB model directly, we construct an improved translation model"
N13-1060,W06-1628,0,0.127879,"Missing"
N13-1060,P06-1121,0,0.106268,"S reordering model short/simple phrase (e.g., friday) or a long/complex one (e.g., when I was 20 years old), which has impact on its reordering in translation. 6 Related Work While there has been substantial work on linguistically motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in 547 translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the majo"
N13-1060,N03-1017,0,0.424256,"feature φi . See (Chiang, 2007) for more details. 2.2 Syntactic Constraints Translation rules in an HPB model are extracted from initial phrase pairs, which must include at least one word inside one phrase aligned to a word inside the other, such that no word inside one phrase can be aligned to a word outside the other phrase. It is not surprising to observe that initial phrases frequently are non-intuitive and inconsistent with linguistic constituents, because they are based only on statistical word alignments. Nothing in the framework actually requires linguistic knowledge. 541 Koehn et al. (2003) conjectured that such nonintuitive phrases do not help in translation. They tested this conjecture by restricting phrases to syntactically motivated constituents on both the source and target side: only those initial phrase pairs are subtrees in the derivations produced by the model. However, their phrase-based translation experiments (on Europarl data) showed the restriction to syntactic constituents is actually harmful, because too many phrases are eliminated. The idea of hard syntactic constraints then seems essentially to have been abandoned: it doesn’t appear in later work. On the face o"
N13-1060,W04-3250,0,0.0735116,"all source parse trees to annotate semantic roles for all verbal predicates. We use the 2003 NIST MT evaluation test data (919 sentence pairs) as the development data, and the 2002, 2004 and 2005 NIST MT evaluation test data (878, 1788 and 1082 sentence pairs, respectively) as the test data. For evaluation, the NIST BLEU script (version 11b) is used to calculate the NIST BLEU scores, which measures caseinsensitive matching of n-grams with n up to 4. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrapping approach (Koehn, 2004). 4.1 Effects of Syntactic Constraints We have also tested syntactic constraints that simply require phrases on the source side to map to a subtree (called basic constraints). Similar to requiring initial phrases on the source side to satisfy the constraints in training process, we only perform chart parsing on text spans which satisfy the constraints in decoding process. Table 1 shows the results of applying syntactic constraints with different experimental settings. From the table, we have the following observations. • Consistent with the conclusion in Koehn et al. (2003), using the basic co"
N13-1060,P10-1113,1,0.903402,"Missing"
N13-1060,C10-1081,0,0.558038,"3.1 AM-MNR very much 喜欢 红色 的 汽车 wo rengran feichang xihua hongse de qiche a. Word alignment for an English-Chinese sentence pair with semantic roles for the English sentence PAS-S A01 AM-TMP2 VBP3 A14 AM-MNR5 X3 X4 PAS-T X1 X2 X5 b. PAS-S and PAS-T for predicate like Figure 2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given"
N13-1060,W06-1606,0,0.12216,"Missing"
N13-1060,D07-1079,0,0.0445253,"Missing"
N13-1060,P08-1114,1,0.876441,"re vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong"
N13-1060,P08-1023,0,0.0472373,"cally motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in 547 translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the len"
N13-1060,P00-1056,0,0.217393,"ver fully cover some roles while partially cover other roles. For example, phrases like the red, the read car very in Figure 1 are invalid. 544 Experiments We have presented our two-level approach to incorporating syntactic and semantic structures in a HPB system. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese seman"
N13-1060,P03-1021,0,0.0424925,"ystem. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese semantic role 4 This dataset includes LDC2002E18, LDC2003E14, Hansards portion of LDC2004T08 and LDC2005T06 LDC2003E07, LDC2004T07, max-phrase-length=10 max-char-span=10 max-phrase-length=∞ max-char-span=∞ System base HPB + basic constraints + unflattened"
N13-1060,J05-1004,0,0.0359002,"2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given a predicate p, it takes the following form: P (PAS-T |PAS-S, PRE=p) (2) Note that cases for untranslated roles can be naturally reflected in our PAS reordering model. For example, if the argument IA0 is untranslated in Figure 2, its PAS-T will be X2 X5 X3 X4 . 2 In PAS-S, w"
N13-1060,N07-1051,0,0.242769,"Missing"
N13-1060,J10-4005,0,0.0172666,"verage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in 547 translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints,"
N13-1060,D07-1078,0,0.0933571,"ned trees are more reliable than unflattened ones, in the sense that some bracketing errors in unflattened trees can be eliminated during tree flattening. Figure 1(b) illustrates flattening a syntactic parse by moving the head (like) and all its modifiers (I, still, the red car, and very much) to the same level. Third, initial phrase pair extraction in Chiang’s HPB generates a very large number of rules, which makes training and decoding very slow. To avoid this, a widely used strategy is to limit initial phrases to a reasonable length on either side during rule extraction (e.g., 10 in Chiang (2007)). A corresponding constraint to speed up decoding prohibits any X from spanning a substring longer than a fixed length, often the same as the maximum phrase length in rule extraction. Although the initial phrase length limitation mainly keeps non-intuitive phrases out, it also closes the door on some useful phrases. For example, a translation rule hI still like X, wo rengran xihuan Xi will be prohibited if the non-terminal X covers 8 or more words. In contrast, our hard constraints have already filtered out dominating nonintuitive phrases; thus there is more room to include additional useful"
N13-1060,N09-2004,0,0.0780833,"-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from theirs in two main aspects: 1) we consider reordering not only between an argument and its predicate, but also between two arguments; and 2) our reordering model can naturally model cases of un"
N13-1060,P09-1036,0,0.0207385,"um size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from theirs in two m"
N13-1060,N10-1016,0,0.0365831,"Missing"
N13-1060,P12-1095,0,0.438706,"vated. In previous work, Liu and Gildea (2010) model the reordering/deletion of source-side semantic roles in a tree-to-string translation model. While it is natural to include semantic structures in a treebased translation model, the effect of semantic structures is presumably limited, since tree templates themselves have already encoded semantics to some 540 Proceedings of NAACL-HLT 2013, pages 540–549, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics extent. For example, template (VP (VBG giving) NP#1 NP#2) entails NP#1 as receiver and NP#2 as thing given. Xiong et al. (2012) model the reordering between predicates and their arguments by assuming arguments are translated as a unit. However, they only considered the reordering between arguments and their predicates. 2 Syntactic Constraints for HPB Translation Model In this section, we briefly review the HPB model, then present our approach to incorporating syntactic constraints into it. 2.1 HPB Translation Model In HPB models, synchronous rules take the form X → hγ, α, ∼i, where X is the non-terminal symbol, γ and α are strings of lexical items and nonterminals in the source and target side, respectively, and ∼ ind"
N13-1060,D10-1030,0,0.0451222,"Missing"
N13-1060,J02-3001,0,\N,Missing
N15-1052,P11-2037,0,0.0376754,"Missing"
N15-1052,D13-1135,0,0.253898,"Missing"
N15-1052,J95-2003,0,0.892876,"Missing"
N15-1052,N06-2015,0,0.149649,"Missing"
N15-1052,D10-1086,0,0.505565,"Missing"
N15-1052,N07-1051,0,0.0325048,"Missing"
N15-1052,D07-1057,0,\N,Missing
N15-1052,D07-1007,0,\N,Missing
N15-1052,D14-1084,0,\N,Missing
N16-1111,D11-1033,0,0.0209909,"e and Interpretese are different and characterize how they differ, the contribution of our work is not just examining an interesting, important dialect. Our work provides opportunities to improve conventional simultaneous MT systems by exploiting and modeling human tactics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is align"
N16-1111,D14-1018,0,0.128125,"2, the literal translation “as much as possible” is reduced to “very”, and the adjective “Japanese” is omitted. Before we study these characteristics quantitatively in the next section, we visualize Interpretese and Translationese by a word cloud in Figure 1. The size of each word is proportional to the difference between its frequencies in Interpretese and Translationese (Section 3). The word color indicates whether it is more frequent in Interpretese (black) or Translationese (gold). “the” is over-represented in Interpretese, a phenomenon also occurs in Translationese vs. the original text (Eetemadi and Toutanova, 2014). More conjunction words (e.g., “and”, “so”, “or”, “then”) are used in Interpretese, likely for segmentation, whereas “that” is more frequent in Translationese—a sign of clauses. In addition, the pronoun “I” occurs more often in Translationese while “be” and “is” occur more often in Interpretese, which is consistent with our passivization hypothesis. Source (S), translation (T) and interpretation (I) text 1 2 3 4 5 (S) この日本語の待遇表現の特徴ですが英語から日本語へ直訳しただけでは表現できないと いった特徴があります. (T) (One of) the characteristics of honorific Japanese is that it can not be adequately expressed when using a direct transla"
N16-1111,D14-1140,1,0.915717,"Missing"
N16-1111,D15-1006,1,0.897002,"2013), a process we call segmentation. This is different from what is commonly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using “which is” to put it in a clause (Table 1, Example 3), or creating a new sentence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for interpreting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et al., 2015). Because the verb is needed early in the target sentence but only appears at the end of the source sentence, an obvious strategy is to wait for the final verb. However, if the interpreter uses passive voice, they can start translating immediately and append the verb at the end (Table 1, Examples 4– 5). During passivization, the subject is often omitted when obvious from context. Generalization Camayd-Freixas (2011) and AlKhanji et al. (2000) observe that interpreters focus on delivering the gist of a sentence rather than duplicating the nuanced meaning of each word. More frequent words are ch"
N16-1111,J06-4003,0,0.0197397,"ture Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segmenter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text chunk. Passivization: We compute the number of passive verbs normalized by the total number of verbs. We detect passive voice in English by matching the following regular expression: a be verb (be, are, is, was, were etc.) followed by zero to four non-verb words and one verb in its past participle form. We detect passive voice in Japanese by checking that the dictionary form of a verb has the suffix “れる”. Vocabulary To measure variety, we use Vt /N and Vs /N , where Vt and Vs are counts of distinct tokens and stems, and N is the total number of tokens."
N16-1111,N06-1014,0,0.0278821,"ics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segmenter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text c"
N16-1111,ma-2006-champollion,0,0.0490351,"by wavy lines. :::::::: 3 Classification of Translationese and Interpretese We investigate the difference between Translationese and Interpretese by creating a text classifier to distinguish between them and then examining the most useful features. We train our classifier on a bilingual Japanese-English corpus of spoken monologues and their simultaneous interpretations (Matsubara et al., 2002). To obtain a three-way parallel corpus of aligned translation, interpretation, and their shared source text, we first align the interpreted sentences to source sentences by dynamic programming following Ma (2006).5 This step results in 1684 pairs 5 Sentences are defined by sentence boundaries marked in the corpus, thus coherence is preserved during alignment. 973 of text chunks, with 33 tokens per chunk on average. We then collect human translations from Gengo6 for each source text chunk (one translator per monologue). The original corpus has four interpretors per monologue. We use all available interpretation by copying the translation of a text chunk for its additional interpretation. 3.1 Discriminative Features We use logistic regression as our classifier. Its job is to tell, given a chunk of Engli"
N16-1111,matsubara-etal-2002-bilingual,0,0.0712313,"verbs in translation are underlined. Information appearing in translation but omitted in interpretation are in (parentheses). Summarized expressions and their corresponding expression in translation are :::::::: underlined :: by wavy lines. :::::::: 3 Classification of Translationese and Interpretese We investigate the difference between Translationese and Interpretese by creating a text classifier to distinguish between them and then examining the most useful features. We train our classifier on a bilingual Japanese-English corpus of spoken monologues and their simultaneous interpretations (Matsubara et al., 2002). To obtain a three-way parallel corpus of aligned translation, interpretation, and their shared source text, we first align the interpreted sentences to source sentences by dynamic programming following Ma (2006).5 This step results in 1684 pairs 5 Sentences are defined by sentence boundaries marked in the corpus, thus coherence is preserved during alignment. 973 of text chunks, with 33 tokens per chunk on average. We then collect human translations from Gengo6 for each source text chunk (one translator per monologue). The original corpus has four interpretors per monologue. We use all availa"
N16-1111,P14-2090,0,0.0121586,"he next section. Our hypothesis is that tactics used by interpreters roughly fall in two non-exclusive categories: (i) delay minimization, to enable prompt translation by arranging target words in an order similar to the source; (ii) memory footprint minimization, to avoid overloading working memory by reducing communicated information. Segmentation Interpreters often break source sentences into multiple smaller sentences (CamaydFreixas, 2011; Shimizu et al., 2013), a process we call segmentation. This is different from what is commonly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using “which is” to put it in a clause (Table 1, Example 3), or creating a new sentence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for interpreting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et al., 2015). Because the verb is needed early in the target sentence but only appears at the end of the source sentence, an obvious strategy is to w"
N16-1111,P15-1020,0,0.0654508,"ntional simultaneous MT systems by exploiting and modeling human tactics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segm"
N16-1111,2013.iwslt-papers.3,0,0.692172,"ystems. 2 Distinguishing Translationese and Interpretese In this section, we discuss strategies used in Interpretese, which we detect automatically in the next section. Our hypothesis is that tactics used by interpreters roughly fall in two non-exclusive categories: (i) delay minimization, to enable prompt translation by arranging target words in an order similar to the source; (ii) memory footprint minimization, to avoid overloading working memory by reducing communicated information. Segmentation Interpreters often break source sentences into multiple smaller sentences (CamaydFreixas, 2011; Shimizu et al., 2013), a process we call segmentation. This is different from what is commonly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using “which is” to put it in a clause (Table 1, Example 3), or creating a new sentence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for interpreting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et"
N16-1111,shimizu-etal-2014-collection,0,0.0912836,"do human interpreters use? 1 Unlike consecutive interpretation (speakers stop after a complete thought and wait for the interpreter), simultaneous interpretation has the interpreter to translate while listening to speakers. 2 Language produced in the process of translation is often considered a dialect of the target language: “Translationese” (Baker, 1993). Thus, “Interpretese” refers to interpreted language. Hal Daumé III Computer Science and UMIACS University of Maryland hal@cs.umd.edu Most previous work focuses on qualitative analysis (Bendazzoli and Sandrelli, 2005; Camayd-Freixas, 2011; Shimizu et al., 2014) or pattern counting (Tohyama and Matsubara, 2006; Sridhar et al., 2013). In contrast, we use a more systematic approach based on feature selection and statistical tests. In addition, most work ignores translated text, making it hard to isolate strategies applied by interpreters as opposed to general strategies needed for any translation. Shimizu et al. (2014) are the first to take a comparative approach; however, they directly train MT systems on the interpretation corpus without explicitly examining interpretation tactics. While some techniques can be learned implicitly, the model may also l"
N16-1111,tohyama-matsubara-2006-collection,0,0.248117,"utive interpretation (speakers stop after a complete thought and wait for the interpreter), simultaneous interpretation has the interpreter to translate while listening to speakers. 2 Language produced in the process of translation is often considered a dialect of the target language: “Translationese” (Baker, 1993). Thus, “Interpretese” refers to interpreted language. Hal Daumé III Computer Science and UMIACS University of Maryland hal@cs.umd.edu Most previous work focuses on qualitative analysis (Bendazzoli and Sandrelli, 2005; Camayd-Freixas, 2011; Shimizu et al., 2014) or pattern counting (Tohyama and Matsubara, 2006; Sridhar et al., 2013). In contrast, we use a more systematic approach based on feature selection and statistical tests. In addition, most work ignores translated text, making it hard to isolate strategies applied by interpreters as opposed to general strategies needed for any translation. Shimizu et al. (2014) are the first to take a comparative approach; however, they directly train MT systems on the interpretation corpus without explicitly examining interpretation tactics. While some techniques can be learned implicitly, the model may also learn undesirable behavior such as omission and si"
N16-1111,N03-1033,0,0.0322122,"our data-driven approach could yield additional strategies for improving MT systems. Another strategy—given the scarcity and artifacts of interpretation corpus—is to select references that present delay-minimizing features of Interpretese from translation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Chernov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015). A Feature Extraction We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuromoji 10 to tokenize, lemmatize and tag Japanese sen10 http://www.atilika.org/ 975 tences. Below we describe the features in detail. Inversion: Let {Ai } be the set of indexes of target words to which each source word wi is aligned. We count Ai and Aj (i &lt; j) as an inverted pair if max(Ai ) &gt; min(Aj ). This means that we have to wait until the jth word to translate the ith word. Segmentation: We use the punkt sentence segmenter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text chunk. Passivization: We compute the number of passive verbs normalize"
N16-1180,P13-1035,0,0.618432,"Missing"
N16-1180,P14-1035,0,0.635859,"and other topic model baselines in two crowdsourced evaluations described in Section 4. In Section 5 we show qualitative results and make connections to existing literary scholarship. 2 A Dataset of Character Interactions Our dataset consists of 1,383 fictional works pulled from Project Gutenberg and other Internet sources. Project Gutenberg has a limited selection (outside of science fiction) of mostly classic literature, so we add more contemporary novels from various genres such as mystery, romance, and fantasy to our dataset. To identify character mentions, we run the BookNLP pipeline of Bamman et al. (2014), which includes character name clustering, quoted speaker identification, and coreference resolution.1 For ev1 While this pipeline works reasonably well, it is unreliable for first-person narratives; we leave the necessary improvements 1535 ery detected character mention, we define a span as beginning 100 tokens before the mention and ending 100 tokens after the mention. We do not use sentence or paragraph boundaries because they vary considerably depending on the author (e.g., William Faulkner routinely wrote single sentences longer than many of Hemingway’s paragraphs). All spans in our data"
N16-1180,P08-1090,0,0.0134561,"ric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as"
N16-1180,P09-1068,0,0.015139,"ased, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et"
N16-1180,P15-1077,0,0.0237195,"relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interp"
N16-1180,E12-1065,0,0.296948,"ships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. su"
N16-1180,P10-1015,0,0.202155,"summaries (Bamman et al., 2013). The NUBBI model of Chang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations sho"
N16-1180,P15-1144,0,0.0178133,"; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgments We thank Jonathan Chang and Amit Gruber for providing baseli"
N16-1180,D15-1208,0,0.0777085,"ts even when we reduce the number of topics. 1541 reasonably. Finally, returning to the “extra” meaning of meals discussed in Section 1, food occurs slightly more frequently in positive relationships. 6 Related Work There are two major areas upon which our work builds: computational literary analysis and deep neural networks for natural language processing. Most previous work in computational literary analysis has focused either on characters or events. In the former category, graphical models and classifiers have been proposed for learning character personas from novels (Bamman et al., 2014; Flekova and Gurevych, 2015) and film summaries (Bamman et al., 2013). The NUBBI model of Chang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from"
N16-1180,N15-1004,0,0.0148287,"nal datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgments We thank Jonathan Chang and Amit Grub"
N16-1180,P14-1105,1,0.80738,"ky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Concl"
N16-1180,P15-1162,1,0.0293637,"Missing"
N16-1180,N15-1185,0,0.0223603,"ang et al. (2009a) learns topics that statically describe characters and their relationships. Because these models lack temporal components (the focus of our task), we compare instead against the HTMM of Gruber et al. (2007). Closest to our own work is the supervised structured prediction problem of Chaturvedi et al. (2016), in which features are designed to predict dynamic sequences of positive and negative interactions between two characters in plot summaries. Other research in this area includes social network construction from novels (Elson et al., 2010; Srivastava et al., 2016) and film (Krishnan and Eisenstein, 2015), as well as attempts to summarize and generate stories (Elsner, 2012). While some of the relationship descriptors learned by our model are character-centric, others are more events-based, depicting actions rather than feelings; Figure 6: Clusters from PCA visualizations of the RMN’s learned book (left) and character (right) embeddings. We see a cluster of books about war and violence (many of which are authored by Tom Clancy) as well as a cluster of lead female characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of charact"
N16-1180,P14-1132,0,0.0147958,"ations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et al., 2014). We show representation learning models like RMN can be just as interpretable as LDA-based models. Other applications for which researchers have prioritized interpretable vector representations include text-to-vision mappings (Lazaridou et al., 2014) and word embeddings (Fyshe et al., 2015; Faruqui et al., 2015). 1542 7 Conclusion We formalize the task of unsupervised relationship modeling, which involves learning a set of relationship descriptors as well as a trajectory over these descriptors for each relationship in an input dataset. We present the RMN, a novel neural network architecture for this task that generates more interpretable descriptors and trajectories than topic model baselines. Finally, we show that the output of our model can lead to interesting insights when combined with annotations in an existing dataset. Acknowledgmen"
N16-1180,P15-1107,0,0.00991822,"characters from primarily romance novels. These visualizations show that the RMN can recover useful static representations of characters and books in addition to the dynamic relationship trajectories. such descriptors have been the focus of much previous work (Schank and Abelson, 1977; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Orr et al., 2014). Our model is more closely related to the plot units framework (Lehnert, 1981; Goyal et al., 2013), which annotates events with emotional states. The RMN builds on deep recurrent autoencoders such as the hierarchical LSTM autoencoder of Li et al. (2015); however, it is more efficient because of the span-level vector averaging. It is also similar to recent neural topic model architectures (Cao et al., 2015; Das et al., 2015), although these models are limited to static document representations. We hope to apply the RMN to nonfictional datasets as well; in this vein, Iyyer et al. (2014) apply a neural network to sentences from nonfiction political books for ideology prediction. More generally, topic models and related generative models are a central tool for understanding large corpora from science (Talley et al., 2011) to politics (Nguyen et"
N16-1180,D14-1162,0,0.127245,"Missing"
N16-1180,D15-1088,0,0.0242198,"of the RMN is a set of relationship descriptors (topics) and—for each relationship in the dataset—a trajectory, or a sequence of probability distributions over these descriptors (document-topic assignments). However, the RMN uses recent advances in deep learning to achieve better control over descriptor coherence and trajectory smoothness (Section 4). 3.1 Formalizing the Problem Assume we have two characters c1 and c2 in book b. We define Sc1 ,c2 as a sequence of token spans where each span st ∈ Sc1 ,c2 is itself a set of tokens to character name clustering, which are further expanded upon in Vala et al. (2015), for future work. 2 Code and span data available at http://github.com/ miyyer/rmn. Each input to the RMN is a tuple that contains identifiers for a book and two characters, as well as the spans corresponding to their relationship: (b, c1 , c2 , Sc1 ,c2 ). Given one such input, our objective is to reconstruct Sc1 ,c2 using a linear combination of relationship descriptors from R as shown in Figure 2; we now describe this process formally. r t = RT dt : reconstruction of input span R: descriptor matrix dt dt = ↵ · softmax(Wd · [ht ; dt 1: previous state (1 ↵) · dt 1 ])+ 1: distribution over desc"
N16-1180,Q14-1017,0,\N,Missing
P02-1057,A00-2023,0,\N,Missing
P02-1057,A00-1043,0,\N,Missing
P02-1057,P97-1003,0,\N,Missing
P02-1057,P00-1038,0,\N,Missing
P02-1057,P00-1041,0,\N,Missing
P02-1057,C96-2183,0,\N,Missing
P02-1057,W01-1605,1,\N,Missing
P02-1057,P99-1042,0,\N,Missing
P06-1039,H05-1086,0,0.0118823,"CA 90292 me@hal3.name,marcu@isi.edu Abstract relevant documents for a given query. For both of these tasks, BAYE S UM performs well, even when the underlying retrieval model is noisy. The idea of leveraging known relevant documents is known as query expansion in the information retrieval community, where it has been shown to be successful in ad hoc retrieval tasks. Viewed from the perspective of IR, our work can be interpreted in two ways. First, it can be seen as an application of query expansion to the summarization task (or, in IR terminology, passage retrieval); see (Liu and Croft, 2002; Murdock and Croft, 2005). Second, and more importantly, it can be seen as a method for query expansion in a non-ad-hoc manner. That is, BAYE S UM is a statistically justified query expansion method in the language modeling for IR framework (Ponte and Croft, 1998). We present BAYE S UM (for “Bayesian summarization”), a model for sentence extraction in query-focused summarization. BAYE S UM leverages the common case in which multiple documents are relevant to a single query. Using these documents as reinforcement for query terms, BAYE S UM is not afflicted by the paucity of information in short queries. We show that ap"
P06-1039,H05-1115,0,0.04623,"Missing"
P07-1033,W06-1615,0,0.526864,"ch tagging) on the following datasets: ACE-NER. We use data from the 2005 Automatic Content Extraction task, restricting ourselves to the named-entity recognition task. The 2005 ACE data comes from 5 domains: Broadcast News (bn), Broadcast Conversations (bc), Newswire (nw), Weblog (wl), Usenet (un) and Converstaional Telephone Speech (cts). CoNLL-NE. Similar to ACE-NER, a named-entity recognition task. The difference is: we use the 2006 ACE data as the source domain and the CoNLL 2003 NER data as the target domain. PubMed-POS. A part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. (2006). There are two domains: the source domain is the WSJ portion of the Penn Treebank and the target domain is PubMed. CNN-Recap. This is a recapitalization task introduced by Chelba and Acero (2004) and also used by Daum´e III and Marcu (2006). The source domain is newswire and the target domain is the output of an ASR system. Treebank-Chunk. This is a shallow parsing task based on the data from the Penn Treebank. This data comes from a variety of domains: the standard WSJ domain (we use the same data as for CoNLL 2000), the ATIS switchboard domain, and the Brown corpus (which is, itself, assemb"
P07-1033,W04-3237,0,0.0784441,"elopment and test). We use the predictions made by the S RC O NLY model as additional features and train a second model on the target data, augmented with this new feature. In the L IN I NT baseline, we linearly interpolate the predictions of the S RC O NLY and the T G T O NLY models. The interpolation parameter is adjusted based on target development data. These baselines are actually surprisingly difficult to beat. To date, there are two models that have successfully defeated them on a handful of datasets. The first model, which we shall refer to as the P RIOR model, was first introduced by Chelba and Acero (2004). The idea of this model is to use the S R C O NLY model as a prior on the weights for a second model, trained on the target data. Chelba and Acero (2004) describe this approach within the context of a maximum entropy classifier, but the idea 257 is more general. In particular, for many learning algorithms (maxent, SVMs, averaged perceptron, naive Bayes, etc.), one regularizes the weight vector toward zero. In other words, all of these algorithms contain a regularization term on the weights w of the form λ ||w||22 . In the generalized P RIOR model, we simply replace this regularization term wi"
P08-1045,P98-1036,0,0.0193253,"aum´e III University of Utah School of Computing 50 S Central Campus Drive Salt Lake City, UT 84112, USA me@hal3.name taken from NIST02-05 corpora Ref2 musicians such as Bach, Mozart, Chopin, Bethoven, Shuman, Rachmaninoff, Rafael and Brokoviev Ref3 composers including Bach, Mozart, Schopen, Beethoven, missing name Raphael, Rahmaniev and Brokofien Ref4 composers such as Bach, Mozart, missing name Beethoven, Schumann, Rachmaninov, Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research, e.g., (Knight and Graehl, 1997; Chen et al., 1998; Al-Onaizan, 2002). One approach is to “sound out” words and create new, plausible targetlanguage spellings that preserve the sounds of the source-language name as much as possible. Another approach is to phonetically match source-language names against a large list of target-language words 389 Proceedings of ACL-08: HLT, pages 389–397, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics and phrases. Most of this work has been disconnected from end-to-end MT, a problem which we address head-on in this paper. The simplest way to integrate name handling into SMT is:"
P08-1045,kang-choi-2000-automatic,0,0.123058,"Missing"
P08-1045,N06-1011,0,0.130978,"Missing"
P08-1045,P97-1017,1,0.733098,"e-art is poor for 1 Hal Daum´e III University of Utah School of Computing 50 S Central Campus Drive Salt Lake City, UT 84112, USA me@hal3.name taken from NIST02-05 corpora Ref2 musicians such as Bach, Mozart, Chopin, Bethoven, Shuman, Rachmaninoff, Rafael and Brokoviev Ref3 composers including Bach, Mozart, Schopen, Beethoven, missing name Raphael, Rahmaniev and Brokofien Ref4 composers such as Bach, Mozart, missing name Beethoven, Schumann, Rachmaninov, Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research, e.g., (Knight and Graehl, 1997; Chen et al., 1998; Al-Onaizan, 2002). One approach is to “sound out” words and create new, plausible targetlanguage spellings that preserve the sounds of the source-language name as much as possible. Another approach is to phonetically match source-language names against a large list of target-language words 389 Proceedings of ACL-08: HLT, pages 389–397, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics and phrases. Most of this work has been disconnected from end-to-end MT, a problem which we address head-on in this paper. The simplest way to integrate name ha"
P08-1045,P04-1021,0,0.537393,"Missing"
P08-1045,W01-1413,0,0.0606696,"Missing"
P08-1045,P07-1119,0,0.281956,"ortran. In 391 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as ¬ñJJ K AÒkP or a phrase such as É J ¯@P  K P ñ Ó , we want to find the English transliteration for it. This is not just a romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoff and Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divide the task of transliteration into two steps: given an Ara"
P08-1045,P06-1010,0,0.284514,"Missing"
P08-1045,W98-1005,1,0.872525,"the correct translation Termoli, but also the incorrect Portran. In 391 Transliterator This section describes how we transliterate Arabic words or phrases. Given a word such as ¬ñJJ K AÒkP or a phrase such as É J ¯@P  K P ñ Ó , we want to find the English transliteration for it. This is not just a romanization like rHmanynuf and murys rafyl for the examples above, but a properly spelled English name such as Rachmaninoff and Maurice Ravel. The transliteration result can contain several alternatives, e.g. RachmaninoffjRachmaninov. Unlike various generative approaches (Knight and Graehl, 1997; Stalls and Knight, 1998; Li et al., 2004; Matthews, 2007; Sherif and Kondrak, 2007; Kashani et al., 2007), we do not synthesize an English spelling from scratch, but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million). We develop a similarity metric for Arabic and English words. Since matching against millions of candidates is computationally prohibitive, we store the English words and phrases in an index, such that given an Arabic word or phrase, we quickly retrieve a much smaller set of likely candidates and apply our similarity metric to that smaller list. We divi"
P08-1045,P98-2220,0,0.34804,"Missing"
P08-1045,W02-2017,0,\N,Missing
P08-1045,W02-0505,1,\N,Missing
P08-1045,C98-1036,0,\N,Missing
P09-2074,D08-1054,0,0.0250107,".3 83.7 83.1 ing graph structures with two standard deviation error bars; grouped by number of graphs. Grey bars are indistinguishable from best model in previous group; blue bars are at least two stddevs better; red bars are at least four stddevs better. Hellinger Euclidean 110 92.1 Table 1: Comparison of held-out perplexities for varyBhattacharyya 120 *none* Logit 100 Using data from the scientific domain, we have shown that we can achieve significant reductions in perplexity on held-out data using these models. Our model resembles recent work on hypertext topic models (Gruber et al., 2008; Sun et al., 2008) and blog influence (Nallapati and Cohen, 2008), but is specifically tailored toward undirected models. Ours is an alternative to the recently proposed Markov Topic Models approach (Wang et al., 2009). While the goal of these two models is similar, the approaches differ fairly dramatically: we use the graph structure to inform the per-document topic distributions; they use the graph structure to inform the unigram models associated with each topic. It would be worthwhile to directly compare these two approaches. 90 80 0 200 400 # of iterations 600 800 Figure 5: Held-out perplexity for differen"
P10-5005,W10-2903,0,\N,Missing
P10-5005,W02-1001,0,\N,Missing
P10-5005,P04-1015,0,\N,Missing
P10-5005,P09-1011,0,\N,Missing
P10-5005,P02-1034,0,\N,Missing
P10-5005,P06-1096,0,\N,Missing
P10-5005,P95-1037,0,\N,Missing
P11-2026,P91-1023,0,0.809424,"ithm to learn the interlingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion"
P11-2026,P09-1121,0,0.149971,"ain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector space model for cross-lingual applications (Turney and Pantel, 2010). There are two major approaches for solving the document alignment problem, depending on the available resources. The first approach, which is widely used in the Cross-lingual Information R"
P11-2026,P08-1088,0,0.0349081,"ss-lingual Information Retrieval (CLIR) literature, uses bilingual dictionaries to translate documents from one language (source) into another (target) language (Ballesteros and Croft, 1996; Pirkola et al., 2001). Then standard measures such as cosine similarity are used to identify target language documents that are close to the translated document. The second approach is to use training data of aligned document pairs to find a common subspace such that the aligned document pairs are maximally correlated (Susan T. Dumais, 1996; Vinokourov et al., 2003; Mimno et al., 2009; Platt et al., 2010; Haghighi et al., 2008) . Both kinds of approaches have their own strengths and weaknesses. Dictionary based approaches treat source documents independently, i.e., each source language document is translated independently of other documents. Moreover, after translation, the relationship of a given source document with the rest of the source documents is ignored. On the other hand, supervised approaches use all the source and target language documents to infer an interlingual 1 We use the phrases “common subspace” and “interlingual representation” interchangeably. 147 Proceedings of the 49th Annual Meeting of the Ass"
P11-2026,P08-1045,1,0.911892,"Missing"
P11-2026,P06-1103,0,0.037455,"ach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector space model for cross-lingual applications (Turney and Pantel, 2010). There are two major approaches for solving the document"
P11-2026,2005.mtsummit-papers.11,0,0.017733,"Wikipedia in English-Spanish and English-German language pairs. For both the data sets, we consider only words that occurred more than once in at least five documents. Of the words that meet the frequency criterion, we choose the most frequent 2000 words for English-Spanish data set. But, because of the compound word phenomenon of German, we retain all the frequent words for English-German data set. Subsequently we convert the documents into TFIDF weighted vectors. The bilingual dictionaries for both the language pairs are generated by running Giza++ (Och and Ney, 2003) on the Europarl data (Koehn, 2005). 3 Word-by-Word CCA (λ = 0.3) CCA (λ = 0.5) CCA (λ = 0.8) OPCA Ours (C = 0.6) Ours (C = 1.0) En – Es 0.597 0.627 0.628 0.637 0.688 0.67 0.658 https://lemon.cs.elte.hu/trac/lemon 150 We follow the process described in Sec. 2.3 to recover the document alignment for our method. We compare our approach with a dictionary based approach, such as word-by-word translation, and supervised approaches, such as CCA (Vinokourov et al., 2003; Hotelling, 1936) and OPCA (Platt et al., 2010). Word-by-word translation and our approach use bilingual dictionary while CCA and OPCA use a training corpus of aligned"
P11-2026,D09-1092,0,0.113446,"Missing"
P11-2026,J05-4003,0,0.0604805,"ngual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector space model for cross-lingual applications (Turney and P"
P11-2026,J03-1002,0,0.00382182,"e choose 2500 aligned document pairs from Wikipedia in English-Spanish and English-German language pairs. For both the data sets, we consider only words that occurred more than once in at least five documents. Of the words that meet the frequency criterion, we choose the most frequent 2000 words for English-Spanish data set. But, because of the compound word phenomenon of German, we retain all the frequent words for English-German data set. Subsequently we convert the documents into TFIDF weighted vectors. The bilingual dictionaries for both the language pairs are generated by running Giza++ (Och and Ney, 2003) on the Europarl data (Koehn, 2005). 3 Word-by-Word CCA (λ = 0.3) CCA (λ = 0.5) CCA (λ = 0.8) OPCA Ours (C = 0.6) Ours (C = 1.0) En – Es 0.597 0.627 0.628 0.637 0.688 0.67 0.658 https://lemon.cs.elte.hu/trac/lemon 150 We follow the process described in Sec. 2.3 to recover the document alignment for our method. We compare our approach with a dictionary based approach, such as word-by-word translation, and supervised approaches, such as CCA (Vinokourov et al., 2003; Hotelling, 1936) and OPCA (Platt et al., 2010). Word-by-word translation and our approach use bilingual dictionary while CCA and OP"
P11-2026,D10-1025,0,0.259818,"dely used in the Cross-lingual Information Retrieval (CLIR) literature, uses bilingual dictionaries to translate documents from one language (source) into another (target) language (Ballesteros and Croft, 1996; Pirkola et al., 2001). Then standard measures such as cosine similarity are used to identify target language documents that are close to the translated document. The second approach is to use training data of aligned document pairs to find a common subspace such that the aligned document pairs are maximally correlated (Susan T. Dumais, 1996; Vinokourov et al., 2003; Mimno et al., 2009; Platt et al., 2010; Haghighi et al., 2008) . Both kinds of approaches have their own strengths and weaknesses. Dictionary based approaches treat source documents independently, i.e., each source language document is translated independently of other documents. Moreover, after translation, the relationship of a given source document with the rest of the source documents is ignored. On the other hand, supervised approaches use all the source and target language documents to infer an interlingual 1 We use the phrases “common subspace” and “interlingual representation” interchangeably. 147 Proceedings of the 49th A"
P11-2026,P99-1067,0,0.170903,"lingual representation. We show that bilingual dictionaries generalize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector s"
P11-2026,N09-1005,0,0.075547,"hod or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector space model for cross-lingual applications (Turney and Pantel, 2010). There are two major approaches for solving the document alignment problem, depending on the available resources. The first"
P11-2026,E09-1091,1,0.851901,"word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector space model for cross-lingual applications (Turney and Pantel, 2010). There are two major approaches for solving the document alignment problem, depending on the availab"
P11-2026,E09-1096,0,0.144039,"ize to different domains better: our approach gives better performance than either a word by word translation method or Canonical Correlation Analysis (CCA) trained on a different domain. 1 Introduction The growth of text corpora in different languages poses an inherent problem of aligning documents across languages. Obtaining an explicit alignment, or a different way of bridging the language barrier, is an important step in many natural language processing (NLP) applications such as: document retrieval (Gale and Church, 1991; Rapp, 1999; Ballesteros and Croft, 1996; Munteanu and Marcu, 2005; Vu et al., 2009), Transliteration Mining (Klementiev and Roth, 2006; Hermjakob et al., 2008; Udupa et al., 2009; Ravi and Knight, 2009) and Multilingual Web Search (Gao et al., 2008; Gao et al., 2009). Raghavendra Udupa Microsoft Research India Bangalore, India raghavu@microsoft.com Aligning documents from different languages arises in all the above mentioned problems. In this paper, we address this problem by mapping documents into a common subspace (interlingual representation)1 . This common subspace generalizes the notion of vector space model for cross-lingual applications (Turney and Pantel, 2010). Ther"
P11-2026,J93-1004,0,\N,Missing
P11-2071,W05-0909,0,0.0291572,"pproximately half of the additional errors made. The only exception is the news domain, which is sufficiently similar to parliament proceedings (Europarl) that there are essentially no new, frequent words in news. By mining a dictionary and naively incorporating it into a translation system, one can only do slightly better than baseline. However, with a more clever integration, we can close about half of the gap between baseline (unadapted) performance and an oracle experiment. In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al., 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). The specific setting we consider is the one in which we have plentiful parallel (“labeled”) data in a source domain (eg., parliament) and plentiful comparable (“unlabeled”) data in a target domain (eg., medical). We can use the unlabeled data in the target domain to build a good language model. Finally, we assume access to a very small amount of parallel (“labeled”) target data, but only enough to evaluate on, or run weight tuning (Och, 2003). All knowledge about unseen words must come from the comparable data. 2 Background and Challenges Domain adaptation is a well-studied field, both in th"
P11-2071,W09-0432,0,0.133119,"learning approach to adaptation could hope to attenuate this problem. There have been a few attempts to measure or perform domain adaptation in machine translation. One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005). Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources. These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain. 3 Data Our source domain is European Parliament proceedings (http://www.statmt.org/ europarl/). We use three target domains: the News Commentary corpus (News) used in the MT Shared task at ACL 2007, European Medicines Agency text (Emea), the Open Subtitles data (Subs) and the PHP technical document data, provided as part of the OPUS corpus http: //urd.let.rug.nl/tiedeman/OPUS/). We extracted development and test sets from e"
P11-2071,W07-0722,0,0.0683315,"ant domains. No machine learning approach to adaptation could hope to attenuate this problem. There have been a few attempts to measure or perform domain adaptation in machine translation. One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005). Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources. These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain. 3 Data Our source domain is European Parliament proceedings (http://www.statmt.org/ europarl/). We use three target domains: the News Commentary corpus (News) used in the MT Shared task at ACL 2007, European Medicines Agency text (Emea), the Open Subtitles data (Subs) and the PHP technical document data, provided as part of the OPUS corpus http: //urd.let.rug.nl/tiedeman/OPUS/). We extracted de"
P11-2071,P08-1088,0,0.469384,"Missing"
P11-2071,2005.eamt-1.19,0,0.168143,"adjust the weights of a learned translation model to do well on a new domain. As expected, we shall see that unseen words pose a major challenge for adapting translation systems to distant domains. No machine learning approach to adaptation could hope to attenuate this problem. There have been a few attempts to measure or perform domain adaptation in machine translation. One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005). Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources. These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain. 3 Data Our source domain is European Parliament proceedings (http://www.statmt.org/ europarl/). We use three target domains: the News Commentary corpus (News) used in the MT Shared task at ACL 2007, European Medicin"
P11-2071,W07-0733,0,0.0219768,"rds pose a major challenge for adapting translation systems to distant domains. No machine learning approach to adaptation could hope to attenuate this problem. There have been a few attempts to measure or perform domain adaptation in machine translation. One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005). Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources. These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain. 3 Data Our source domain is European Parliament proceedings (http://www.statmt.org/ europarl/). We use three target domains: the News Commentary corpus (News) used in the MT Shared task at ACL 2007, European Medicines Agency text (Emea), the Open Subtitles data (Subs) and the PHP technical document data, provided as part of the OPUS"
P11-2071,P07-2045,0,0.0111142,", technical documentation) in two source languages (German, French), we: (1) Ascertain the degree to which domain divergence causes increases in unseen words, and the degree to which this degrades translation performance. (For instance, if all unknown words are names, then copying them verbatim may be sufficient.) (2) Extend known methods for mining dictionaries from comparable corpora to the domain adaptation setting, by “bootstrapping” them based on known translations from the source domain. (3) Develop methods for integrating these mined dictionaries into a phrase-based translation system (Koehn et al., 2007). As we shall see, for most target domains, out of vocabulary terms are the source of approximately half of the additional errors made. The only exception is the news domain, which is sufficiently similar to parliament proceedings (Europarl) that there are essentially no new, frequent words in news. By mining a dictionary and naively incorporating it into a translation system, one can only do slightly better than baseline. However, with a more clever integration, we can close about half of the gap between baseline (unadapted) performance and an oracle experiment. In most cases this amounts to"
P11-2071,P03-1021,0,0.142732,"oracle experiment. In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al., 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). The specific setting we consider is the one in which we have plentiful parallel (“labeled”) data in a source domain (eg., parliament) and plentiful comparable (“unlabeled”) data in a target domain (eg., medical). We can use the unlabeled data in the target domain to build a good language model. Finally, we assume access to a very small amount of parallel (“labeled”) target data, but only enough to evaluate on, or run weight tuning (Och, 2003). All knowledge about unseen words must come from the comparable data. 2 Background and Challenges Domain adaptation is a well-studied field, both in the NLP community as well as the machine learning and statistics communities. Unlike in machine learning, in the case of translation, it is not enough to simply 407 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 407–412, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics adjust the weights of a learned translation model to do well on a new domain. As exp"
P11-2071,P02-1040,0,0.100232,"s, out of vocabulary terms are the source of approximately half of the additional errors made. The only exception is the news domain, which is sufficiently similar to parliament proceedings (Europarl) that there are essentially no new, frequent words in news. By mining a dictionary and naively incorporating it into a translation system, one can only do slightly better than baseline. However, with a more clever integration, we can close about half of the gap between baseline (unadapted) performance and an oracle experiment. In most cases this amounts to an improvement of about 1.5 Bleu points (Papineni et al., 2002) and 1.5 Meteor points (Banerjee and Lavie, 2005). The specific setting we consider is the one in which we have plentiful parallel (“labeled”) data in a source domain (eg., parliament) and plentiful comparable (“unlabeled”) data in a target domain (eg., medical). We can use the unlabeled data in the target domain to build a good language model. Finally, we assume access to a very small amount of parallel (“labeled”) target data, but only enough to evaluate on, or run weight tuning (Och, 2003). All knowledge about unseen words must come from the comparable data. 2 Background and Challenges Doma"
P11-2071,D08-1090,0,0.0348968,"to attenuate this problem. There have been a few attempts to measure or perform domain adaptation in machine translation. One of the first approaches essentially performs test-set relativization (choosing training samples that look most like the test data) to improve translation performance, but applies the approach only to very small data sets (Hildebrand et al., 2005). Later approaches are mostly based on a data set made available in the 2007 StatMT workshop (Koehn and Schroeder, 2007), and have attempted to use monolingual (Civera and Juan, 2007; Bertoldi and Federico, 2009) or comparable (Snover et al., 2008) corpus resources. These papers all show small, but significant, gains in performance when moving from Parliament domain to News domain. 3 Data Our source domain is European Parliament proceedings (http://www.statmt.org/ europarl/). We use three target domains: the News Commentary corpus (News) used in the MT Shared task at ACL 2007, European Medicines Agency text (Emea), the Open Subtitles data (Subs) and the PHP technical document data, provided as part of the OPUS corpus http: //urd.let.rug.nl/tiedeman/OPUS/). We extracted development and test sets from each of these corpora, except for new"
P11-2071,C08-1074,0,\N,Missing
P11-2071,D08-1076,0,\N,Missing
P11-5001,W10-2903,0,\N,Missing
P11-5001,W02-1001,0,\N,Missing
P11-5001,P04-1015,0,\N,Missing
P11-5001,P09-1011,0,\N,Missing
P11-5001,P02-1034,0,\N,Missing
P11-5001,P06-1096,0,\N,Missing
P11-5001,P95-1037,0,\N,Missing
P13-1141,P10-1088,0,0.0107299,"esults are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note that for both the dictionary mining setting and the active learning setting, it is important to consider words in"
P13-1141,D07-1007,1,0.182543,"ntific domain, the word gains a new sense: “ratio”, which simply does not exist in the parliament domain. In a science domain, the “report” sense exists, but it is dominated about 12:1 by “ratio.” In a medical domain, the “report” sense remains dominant (about 2:1), but the new “ratio” sense appears frequently. In this paper we define a new task that we call S ENSE S POTTING. The goal of this task is to identify words in a new domain monolingual text that appeared in old domain text but which have a new, previously unseen sense1 . We operate under the framework of phrase sense disambiguation (Carpuat and Wu, 2007), in which we take automatically align parallel data in an old domain to generate an initial old-domain sense inventory. This sense inventory provides the set of “known” word senses in the form of phrasal translations. Concrete examples are shown in Table 1. One of our key contributions is the development of a rich set of features based on monolingual text that are indicative of new word senses. This work is driven by an application need. When machine translation (MT) systems are applied in a new domain, many errors are a result of: (1) previously unseen (OOV) source language words, or (2) sou"
P13-1141,P07-1007,0,0.0197521,"ethods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation has focused o"
P13-1141,cook-stevenson-2010-automatically,0,0.0531426,"context given a predefined sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct S ENSE S POTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in S ENSE S POTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent langua"
P13-1141,P11-2071,1,0.451453,"Missing"
P13-1141,N06-1017,0,0.0974335,"induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine trans"
P13-1141,P98-1069,0,0.225714,"ly unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (toke"
P13-1141,W11-2508,0,0.0205318,"sense inventory (e.g., Agirre and Edmonds (2006)), and word sense induction, the task of learning sense inventories from text (e.g., Agirre and Soroa (2007)). In contrast, detecting novel senses has not received as much attention, and is typically addressed within word sense induction, rather than as a distinct S ENSE S POTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in S ENSE S POTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) u"
P13-1141,P08-1088,0,0.00478932,"ords that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known word being used in a new sense?” Note"
P13-1141,P07-2045,0,0.0035526,"stributions. SpreadNorm is the ratio of spread of the prior and posterior distributions, where spared is the difference between maximum and minimum probabilities of the distribution as defined earlier. ConfusionNorm is the ratio of confusion of the prior and posterior distributions, where confusion is defined as earlier. 5 Data and Gold Standard The first component of our task is a parallel corpus of old domain data, for which we use the French-English Hansard parliamentary proceedings (http://www.parl.gc.ca). From this, we extract an old domain sense dictionary, using the Moses MT framework (Koehn et al., 2007). This defines our old domain sense dictionary. For new domains, we use three sources: (1) the EMEA medical corpus (Tiedemann, 2009), (2) a corpus of scientific abstracts, and (3) a corpus of translated movie subtitles (Tiedemann, 2009). Basic statistics are shown in Table 2. In all parallel corpora, we normalize the English for American spelling. To create the gold standard truth, we followed a lexical sample apparoach and collected a set of 300 “representative types” that are interesting to evaluate on, because they have multiple senses within a single domain or whose senses are likely to ch"
P13-1141,E12-1060,0,0.012705,"ot received as much attention, and is typically addressed within word sense induction, rather than as a distinct S ENSE S POTTING task. Novel sense detection has been mostly motivated by the study of language change over time. Most approaches model changes in co-occurrence patterns for word types when moving between corpora of old and modern language (Sagi et al., 2009; Cook and Stevenson, 2010; Gulordava and Baroni, 2011). Since these type-based models do not capture polysemy in the new language, there have been a few attempts at detecting new senses at the tokenlevel as in S ENSE S POTTING. Lau et al. (2012) leverage a common framework to address sense induction and disambiguation based on topic models (Blei et al., 2003). Sense induction is framed as learning topic distributions for a word type, while disambiguation consists of assigning topics to word tokens. This model can interestingly be used to detect newly coined senses, which might co-exist with old senses in recent language. Bamman and Crane (2011) use parallel Latin-English data to learn to disambiguate Latin words into English senses. New English translations are used as evidence that Latin words have shifted sense. In contrast, the S"
P13-1141,P04-1036,0,0.0855446,"re unknown in parallel data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on"
P13-1141,J07-4005,0,0.1011,"data. Such novel sense induction methods require manually annotated datasets for the purpose of evaluation. This is an expensive process and therefore evaluation is typically conducted on a very small scale. In contrast, our S ENSE S POTTING task leverages automatically word-aligned parallel corpora as a source of annotation for supervision during training and evaluation. 1436 The impact of domain on novel senses has also received some attention. Most approaches operate at the type-level, thus capturing changes in the most frequent sense of a word when shifting domains (McCarthy et al., 2004; McCarthy et al., 2007; Erk, 2006; Chan and Ng, 2007). Chan and Ng (2007) notably show that detecting changes in predominant sense as modeled by domain sense priors can improve sense disambiguation, even after performing adaptation using active learning. Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for ma"
P13-1141,W07-0737,0,0.0115962,"Finally, S ENSE S POTTING has not been addressed directly in MT. There has been much interest in translation mining from parallel or comparable corpora for unknown words, where it is easy to identify which words need translations. In contrast, S ENSE S POTTING detects when words have new senses and, thus, frequently a new translation. Work on active learning for machine translation has focused on collecting translations for longer unknown segments (e.g., Bloodgood and CallisonBurch (2010)). There has been some interest in detecting which phrases that are hard to translate for a given system (Mohit and Hwa, 2007), but difficulties can arise for many reasons: S ENSE S POTTING focuses on a single problem. probabilities as well as their difference. These are our Type:RelFreq features. 4 Topic Model Feature The intuition behind the topic model feature is that if a word’s distribution over topics changes when moving into a new domain, it is likely to also gain a new sense. For example, suppose that in our old domain, the French word enceinte is only used with the sense “wall,” but in our new domain, enceinte may have senses corresponding to either “wall” or to “pregnant.” We would expect to see this reflec"
P13-1141,P95-1050,0,0.25233,"(1) previously unseen (OOV) source language words, or (2) source language words that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is t"
P13-1141,W09-0214,0,0.0937541,"Missing"
P13-1141,W02-2026,0,0.0401843,"ce language words, or (2) source language words that appear with a new sense and which require new transla1 All features, code, data and raw results are at: github. com/hal3/IntrinsicPSDEvaluation 1435 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1435–1445, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions2 (Carpuat et al., 2012). Given monolingual text in a new domain, OOVs are easy to identify, and their translations can be acquired using dictionary extraction techniques (Rapp, 1995; Fung and Yee, 1998; Schafer and Yarowsky, 2002; Schafer, 2006; Haghighi et al., 2008; Mausam et al., 2010; Daum´e III and Jagarlamudi, 2011), or active learning (Bloodgood and Callison-Burch, 2010). However, previously seen (even frequent) words which require new translations are harder to spot. Because our motivation is translation, one significant point of departure between our work and prior related work (§3) is that we focus on word tokens. That is, we are not interested only in the question of “has this known word (type) gained a new sense?”, but the much more specific question of “is this particular (token) occurrence of this known"
P13-1141,S07-1002,0,\N,Missing
P13-1141,C98-1066,0,\N,Missing
P14-1106,P96-1041,0,0.134806,"4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on t"
P14-1106,N13-1003,0,0.0233916,"ituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on ChineseEnglish translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also disc"
P14-1106,D08-1024,1,0.927619,"trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint features include both MR08 exact-matching and crossboundary constraints (denoted XP= and XP+). Since the syntactic parses of the tuning and test data contain 29 types of constituent labels and 35 types of POS tags, we have 29 types of XP+ features and 64 types of XP= features. 4.2 Model Training To train the syntactic and semantic reordering models, we use a gold alignment dataset.2 It contains 7,870 sentences with 191,364 Chinese words and 261,399 English words. We first run syn1 http://www.itl.nist.gov/iad/mig//tests/mt This dataset includes LD"
P14-1106,J07-2003,0,0.490023,"rove translation quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous"
P14-1106,W13-2258,0,0.0235223,"uments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture dif"
P14-1106,I11-1005,0,0.0321974,"Missing"
P14-1106,N03-1017,0,0.0397449,"Missing"
P14-1106,W04-3250,0,0.0690769,"5.2 35.6 34.4 34.5 34.5 35.5 35.6 36.0 36.0 35.9 35.8 35.8 35.8 36.1 MT03 36.1 36.9‡ 37.2‡ 37.1‡ 36.7‡ 36.7‡ 37.0‡ 37.3‡ 37.4 38.2‡ 38.1‡ 38.2‡ 37.6† 37.4 37.6† 38.4‡ Test MT05 32.3 33.6‡ 33.7‡ 33.6‡ 33.0‡ 33.1‡ 33.6‡ 33.7‡ 34.2 35.0‡ 34.8‡ 35.3‡ 34.7‡ 34.5† 34.7‡ 35.2‡ MT08 27.4 28.4‡ 28.6‡ 28.8‡ 27.8† 27.8‡ 27.7† 29.0‡ 28.7 29.2‡ 29.2‡ 29.5‡ 28.7 28.8 28.8 29.5‡ Avg. 31.9 33.0 33.2 33.1 32.5 32.5 32.8 33.3 33.4 34.1 34.0 34.3 33.7 33.6 33.7 34.4 Table 5: System performance in BLEU scores. ‡/†: significant over baseline or MR08 at 0.01 / 0.05, respectively, as tested by bootstrap resampling (Koehn, 2004) shown in the rows of “+ sem-reorder” in Table 5. Here we observe: • The semantic reordering models also achieve significant gain of 0.8 BLEU on average over the baseline system, demonstrating the effectiveness of PAS-based reordering. However, the gain diminishes to 0.3 BLEU on the MR08 system. • The syntactic reordering models outperform the semantic reordering models on both the baseline and MR08 systems. Finally, we integrate both the syntactic and semantic reordering models into the final system. The two models collectively achieve a gain of up to 1.4 BLEU over the baseline and 1.0 BLEU o"
P14-1106,P10-1146,0,0.0294091,"are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word lev"
P14-1106,C10-1071,0,0.0186897,"vement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous"
P14-1106,P05-1066,0,0.0612232,"(e.g., 34.9 vs. 34.3). To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translati"
P14-1106,D13-1049,0,0.0157089,"mantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008)"
P14-1106,P10-4002,1,0.934219,"on quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the targe"
P14-1106,P13-4034,1,0.838779,"K Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semanti"
P14-1106,P07-1091,0,0.0254148,"we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with"
P14-1106,P10-1113,1,0.882959,"Missing"
P14-1106,N13-1060,1,0.88395,"Missing"
P14-1106,P13-1032,0,0.0202659,"reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on ChineseEnglish translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also discussed the difference"
P14-1106,C10-1081,0,0.0215185,". (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and t"
P14-1106,D11-1079,0,0.0382431,"Missing"
P14-1106,P08-1114,1,0.9248,"roach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the target language, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models: one is b"
P14-1106,N10-1127,0,0.018131,"ing. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu"
P14-1106,P11-1065,0,0.0529872,"Missing"
P14-1106,C10-1043,0,0.0197319,"e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT d"
P14-1106,D13-1053,0,0.014297,"ed binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation"
P14-1106,P13-1156,0,0.0178749,"the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiaw"
P14-1106,P00-1056,0,0.192489,"ic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03,"
P14-1106,J05-1004,0,0.00957919,"g model takes a CFG rule (e.g., VP → VP PP PP) and models the reordering of the constituents on the left hand side by examining their translation or visit order according to the target language. For the semantic reordering model, it takes a PAS and models its reordering on the target side. Figure 1 shows an example of a PAS where the predicate (Pre) has two core arguments (A0 and A1) and one adjunct (TMP). Note that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles. According to the annotation principles in (Chinese) PropBank (Palmer et al., 2005; Xue and Palmer, 2009), all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g., NPs and VBD in Figure 1) do not overlap with each other. Next, we use a CFG rule to describe our syntactic reordering model. Treating the two forms of reorderings in a unified way, the semantic reordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent. Because the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering pat"
P14-1106,P12-1095,0,0.020195,"ted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is un"
P14-1106,P02-1040,0,0.0905028,"et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word"
P14-1106,N09-1028,0,0.021861,"owever, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another ap"
P14-1106,N07-1051,0,0.0385816,"ey smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resni"
P14-1106,P09-1037,1,0.840115,"utput and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patter"
P14-1106,P13-1124,0,0.0190784,"2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the targe"
P14-1106,I05-3027,0,0.0350418,"i with a semantic role Ri . Algorithm 1 therefore permits a unified treatment of syntactic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of"
P14-1106,P12-1096,0,0.0135225,"6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Althou"
P14-1106,P13-1111,0,0.0135279,"level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering"
P14-1106,P09-1054,0,0.0453873,"Missing"
P14-1106,C10-1126,0,0.020213,"33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reord"
P14-1106,D07-1077,0,0.0341675,"To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during dec"
P14-1106,N09-2004,0,0.0214349,"ctivity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reorde"
P14-1106,I11-1004,0,0.014308,"10) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translati"
P14-1106,C04-1073,0,0.0529703,"types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted feature"
P14-1141,N12-1074,0,0.0587139,"Missing"
P14-1141,I13-1001,0,0.0260681,"(2014) use latent structure, aimed to identify relevant dialog segments, for predicting objections during courtroom deliberations. Other related work include speech act recognition in emails and forums but at a sentence level (Jeong et al., 2009), and using social network analysis to improve message classification into pre-determined types (Fortuna et al., 2007). Discussion forums data has also been used to address other interesting challenges such as extracting chatbox knowledge for use in general online forums (Huang et al., 2007) and automatically extracting answers from discussion forums (Catherine et al., 2013), subjectivity analysis of online forums (Biyani et al., 2013). Most of these methods use ideas similar to ours: identifying that threads (or discussions) have an underlying structure and that messages belong to categories. However, they operate in a different domain, which makes their goals and methods different from ours. Our work is most closely related to that of Backstrom et al. (2013) which introduced the re-entry prediction task —predicting whether a user who has participated in a thread will later contribute another comment to it. While seemingly related, their prediction task, focusin"
P14-1141,N10-1066,1,0.829513,"fw (tj , pj )) w 2 j (2) where λ is the regularization coefficient, tj is the j th thread with intervention decision rj and pj are the posts of this thread. w is the weight vector, l(·) is the squared hinge loss function and fw (tj , pj ) is defined in Equation 1. Replacing the term fw (tj , pj ) with the contents of Equation 1 in the minimization objective above, reveals the key difference from the traditional SVM formulation - the objective function has a maximum term inside the global minimization problem making it non-convex. We, therefore, employ the optimization algorithm presented in (Chang et al., 2010) to solve this problem. Exploiting the semi-convexity property (Felzenszwalb et al., 2010), the algorithm works in two steps, each executed iteratively. In the first step, it determines the latent variable assignments for positive examples. The algorithm then performs two step iteratively - first it determines the structural assignments for the negative examples, and then optimizes the fixed objective function using a cutting plane algorithm. Once this process converges for negative examples, the algorithm reassigns values to the latent variables for positive examples, and proceeds to the seco"
P14-1141,W02-1001,0,0.034724,"eneric EM style algorithm. The supervision in this model is provided only in form of the observed intervention decision, r and the post categories, hi are hid1504 den. The model uses the pseudocode shown in Algorithm 1 to iteratively refine the weight vectors. In each iteration, the model first uses viterbi algorithm to decode thread sequences with the current weights wt to find optimal highest scoring latent state sequences that agree with the observed intervention state (r = r0 ). In the next step, given the latent state assignments from the previous step, a structured perceptron algorithm (Collins, 2002) is used to update the weights wt+1 using weights from the previous step, wt , initialization. Algorithm 1 Training algorithm for LCMM 1: 2: 3: 4: 5: 6: 7: 8: Input: Labeled data D = {(t, p, r)i } Output: Weights w Initialization: Set wj randomly, ∀j for t : 1 to N do hˆi = arg maxh [wt · φ(p, r, h, t)] such that r = ri ∀i ˆ r) wt+1 = StructuredPerceptron(t, p, h, end for return w While testing, we use the learned weights and viterbi decoding to compute the intervention state and the best scoring latent category sequence. 3.3.2 Feature Engineering In addition to the ‘Thread Only Features’ and"
P14-1141,E14-1069,1,0.874407,"Missing"
P14-1141,D09-1130,0,0.018305,"al., 2009; Yano and Smith, 2010; Artzi et al., 2012) and rate of content diffusion (Kwak et al., 2010; Lerman and Ghosh, 2010; Bakshy et al., 2011; Romero et al., 2011; Artzi et al., 2012) and also question answering (Chaturvedi et al., 2014). Wang et al. (2007) incorporate thread structure of conversations using features in email threads while Goldwasser and Daum´e III (2014) use latent structure, aimed to identify relevant dialog segments, for predicting objections during courtroom deliberations. Other related work include speech act recognition in emails and forums but at a sentence level (Jeong et al., 2009), and using social network analysis to improve message classification into pre-determined types (Fortuna et al., 2007). Discussion forums data has also been used to address other interesting challenges such as extracting chatbox knowledge for use in general online forums (Huang et al., 2007) and automatically extracting answers from discussion forums (Catherine et al., 2013), subjectivity analysis of online forums (Biyani et al., 2013). Most of these methods use ideas similar to ours: identifying that threads (or discussions) have an underlying structure and that messages belong to categories."
P14-1141,P07-2019,0,0.0201449,"ed Work To the best of our knowledge, the problem of predicting instructor’s intervention in MOOC forums has not been addressed yet. Prior work deals with analyzing general online discussion forums of social media sites (Kleinberg, 2013): such as predicting comment volume (Backstrom et al., 2013; De Choudhury et al., 2009; Wang et al., 2012; Tsagkias et al., 2009; Yano and Smith, 2010; Artzi et al., 2012) and rate of content diffusion (Kwak et al., 2010; Lerman and Ghosh, 2010; Bakshy et al., 2011; Romero et al., 2011; Artzi et al., 2012) and also question answering (Chaturvedi et al., 2014). Wang et al. (2007) incorporate thread structure of conversations using features in email threads while Goldwasser and Daum´e III (2014) use latent structure, aimed to identify relevant dialog segments, for predicting objections during courtroom deliberations. Other related work include speech act recognition in emails and forums but at a sentence level (Jeong et al., 2009), and using social network analysis to improve message classification into pre-determined types (Fortuna et al., 2007). Discussion forums data has also been used to address other interesting challenges such as extracting chatbox knowledge for"
P15-1158,P06-1005,0,0.027707,"the referent (Orita et al., 2014) that are not incorporated in our simulations, this model sets up a framework to test the role and interaction of various potential factors suggested in the discourse literature. Salience of the referent is computed differently depending on its information status: old or new. The following illustrates the speaker’s assumptions about the listener’s discourse model: 3. If r = new, sample that new referent r from the base distribution over entities with probability U1· (count U· denotes a total number of unseen entities that is estimated from a named entity list (Bergsma and Lin, 2006)). The above discourse model is frequency-based. We can replace the term Nr for the old referent with f (di,j ) = e−di,j /a that captures recency, where the recency function f (di,j ) decays exponentially with the distance between the current referent ri and the same referent rj that has previously been referred to. This framework for frequency and recency of new and old referents exactly corresponds to priors in the Chinese Restaurant Process (Teh et al., 2006) and the distance-dependent Chinese Restaurant Process (Blei and Frazier, 2011). The denominator in (5) represents the sum of potentia"
P15-1158,J12-1006,0,0.0487652,"Missing"
P15-1158,J95-2003,0,0.931419,"s the details of our model. Section 4 describes the data, preprocessing and annotation procedure. Section 5 presents simulation results. Section 6 summarizes this study and discusses implications and future directions. 2 2.1 Relevant Work Discourse salience Speakers’ choices of referring expressions have long been an object of study. Pronominalization has been examined particularly often in both theoretical and experimental studies. Discourse theories predict that speakers use pronouns when they think that a referent is salient in the discourse (Giv´on, 1983; Ariel, 1990; Gundel et al., 1993; Grosz et al., 1995), where salience of the referent is influenced by various factors such as grammatical position (Brennan, 1995), recency (Chafe, 1994), topicality (Arnold, 1998), competitors (Fukumura et al., 2011), visual salience (Vogels et al., 2013b), and so on. Discourse theories have characterized the link between referring expressions and discourse salience by stipulating constructs such as a scale of topicality (Giv´on, 1983), accessibility hierarchy (Ariel, 1990), or implicational hierarchy (Gundel et al., 1993). All of these assume fixed form-salience correspondences in that a certain referring expre"
P15-1158,W14-2008,1,0.911454,"example, we assume that an entity Barack Obama has one and only one proper name ‘Barack Obama’, and this entity is unambiguously associated with male and singular. Although we use an example with two possible referring expressions, as long as P (w|r) is constant across all referents and words, it does not make a difference to the computation in (5) how many competing words we assume for each referent. To estimate the salience of a referent, P (r), our framework employs factors such as referent frequency or recency. Although there are other important factors such as topicality of the referent (Orita et al., 2014) that are not incorporated in our simulations, this model sets up a framework to test the role and interaction of various potential factors suggested in the discourse literature. Salience of the referent is computed differently depending on its information status: old or new. The following illustrates the speaker’s assumptions about the listener’s discourse model: 3. If r = new, sample that new referent r from the base distribution over entities with probability U1· (count U· denotes a total number of unseen entities that is estimated from a named entity list (Bergsma and Lin, 2006)). The abov"
P15-1158,J04-3003,0,0.105109,"Missing"
P15-1158,S10-1001,0,0.0608875,"Missing"
P15-1158,E12-2021,0,\N,Missing
P15-1162,S14-2098,0,0.0113706,"Missing"
P15-1162,D10-1115,0,0.0107131,"ible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been success"
P15-1162,D14-1070,1,0.582478,"OW, g averages word embeddings1 1 X z = g(w ∈ X) = vw . (1) |X| w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(Ws · z + b), (2) where the softmax function is exp q softmax(q) = Pk j=1 exp qj (3) Ws is a k × d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is `(ˆ y) = k X yp log(ˆ yp ). (4) p=1 1 Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. NBOW 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien"
P15-1162,D12-1118,1,0.276157,"Missing"
P15-1162,D14-1082,0,0.0813431,"ide of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and recursive neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Sutskever et al., 2014; Tai et al., 2015), to drop useless words rather than randomly-selected ones. 8 Conclusion In this paper, we introduce the deep averaging network, which feeds an unweighted average of word"
P15-1162,D14-1179,0,0.0113963,"Missing"
P15-1162,D08-1094,0,0.0145513,"Missing"
P15-1162,D11-1129,0,0.0213482,"n the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and para"
P15-1162,W13-3209,0,0.0832105,"state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we sho"
P15-1162,P14-1105,1,0.666888,"rameters selected on the SST also work well for the IMDB task. 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN - MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DAN s require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minutes on a single core o"
P15-1162,W13-3214,0,0.0468386,"Missing"
P15-1162,P14-1062,0,0.482625,"instantiation of NBOW, g averages word embeddings1 1 X z = g(w ∈ X) = vw . (1) |X| w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(Ws · z + b), (2) where the softmax function is exp q softmax(q) = Pk j=1 exp qj (3) Ws is a k × d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is `(ˆ y) = k X yp log(ˆ yp ). (4) p=1 1 Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. NBOW 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien"
P15-1162,D13-1166,0,0.0216649,"gnificant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (So"
P15-1162,D14-1181,0,0.0448337,"an NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. Kim (2014) shows that some of these issues can be avoided by using a convolutional network instead of a RecNN, but the computational complexity increases even further (see Section 4 for runtime comparisons). What contributes most to the power of syntactic 1682 RecNN softmax z3 = f (W   DAN softmax c1 + b) z2 softmax z2 = f (W  softmax  c2 + b) z1 z1 = f (W h2 = f (W2 · h1 + b2 ) h1 = f (W1 · av + b1 )   c3 + b) c4 av = 4 P i=1 Predator c1 is c2 a c3 masterpiece c4 Predator c1 is c2 a c3 ci 4 masterpiece c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Soft"
P15-1162,P14-1140,0,0.010475,"stette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN , we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a"
P15-1162,P11-1015,0,0.19269,"och on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN ) and the convolutional neural network multichannel (Kim, 2014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vec"
P15-1162,P08-1028,0,0.0096376,"sentiment. Intuitively, after the embeddings are fine-tuned during DAN training, we might expect a decrease in the norms of stopwords and an increase in the 1688 norms of sentiment-rich words like “awesome” or “horrible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN . Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN ,"
P15-1162,P05-1015,0,0.56308,"— — — — — 89.4 92.6 — 89.2 — — 431 — — — 2,452 — Table 1: DANs achieve comparable sentiment accuracies to syntactic functions (bottom third of table) but require much less training time (measured as time of a single epoch on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines in"
P15-1162,D14-1162,0,0.123646,"paces of size |V |2 . 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW- RAND and DAN - RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-available 300-d GloVe vectors trained over the Common Crawl (Pennington et al., 2014). The DAN - ROOT model only has access to sentence-level labels for SST experiments, while all other models are trained on labeled phrases (if they exist) in addition to sentences. We train all NBOW and DAN models using AdaGrad (Duchi et al., 2011). We apply DANs to documents by averaging the embeddings for all of a document’s tokens and then feeding that average through multiple layers as before. Since the representations computed by DAN s are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational cost. We find that the hype"
P15-1162,N12-1085,1,0.265487,"find that the hyperparameters selected on the SST also work well for the IMDB task. 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN - MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DAN s require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minute"
P15-1162,D11-1014,0,0.0224104,"oth fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), R ec NTNs, the deep recursive network ( DR ec NN ) proposed by ˙Irsoy and Cardie (2014), and the TREE - LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN ) and the convolutional neural network multichannel (Kim, 2014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3 PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. 1684 the word-represe"
P15-1162,P13-1045,0,0.0214218,"NNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their r"
P15-1162,D13-1170,0,0.156033,"NNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. R ec NN s also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their r"
P15-1162,P15-1150,0,0.409406,"hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficiency in the process. In subsequent sections, we argue that this complexity is not matched by a corresponding gain in performance. Recursive neural networks (RecNNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW , each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW model"
P15-1162,D14-1004,0,0.00882624,"Missing"
P15-1162,P12-2018,0,0.150115,"014, CNN - MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3 PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram na¨ıve Bayes (BINB) and na¨ıve Bayes support vector machine (NBSVM - BI) models introduced by Wang and Manning (2012), both of which are memory-intensive due to huge feature spaces of size |V |2 . 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW- RAND and DAN - RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-ava"
P16-1177,D14-1110,0,0.0254243,"e-art model for this task is a semi-supervised approach (Rothe and Sch¨utze, 2015). This model use resources like WordNet 1886 to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way. Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAELGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch¨utze (2015). It is also interesting that SAE (without any context information) outperforms the pre-tra"
P16-1177,P15-2114,0,0.12848,"een their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho,"
P16-1177,N10-1145,0,0.131137,"and h2n , as additional features: hsub = |h1n − h2n | hdot = h1n h2n , (10) where hsub and hdot capture the element-wise difference and similarity (in terms of the sign of elements in each dimension) between h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a wor"
P16-1177,P12-1092,0,0.157429,"mantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, co"
P16-1177,P15-1162,1,0.379487,"Missing"
P16-1177,D14-1218,0,0.0343004,"Missing"
P16-1177,D15-1106,0,0.119472,"e, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho, 2015), topics associated with the sentence (Mikolov and Zweig, 2012; Le and Mikolov, 2014), the document that contains the sentence (Huang et al., 2012), as well as their combinations (Ji et al., 2016). It is important to integrate context into neural networks because these models are often trained with only local information about their individual inputs. For example, recurrent and recursive neural networks only use local information about previously seen words in a sentence to predict the next word or composition.1 On the other hand, context information (such as topical infor"
P16-1177,D14-1113,0,0.0149496,"ection 3). The state-of-the-art model for this task is a semi-supervised approach (Rothe and Sch¨utze, 2015). This model use resources like WordNet 1886 to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way. Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAELGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch¨utze (2015). It is also interesting that SAE (without any context information) out"
P16-1177,Q15-1022,0,0.00399514,"performances initially improve. The CSAE-LGC model that uses both local and global context benefits more from greater number of hidden layers than CSAE-LC that only uses local context. We attribute this to the use of global context in CSAE-LGC that leads to more accurate representations of words in their context. We also note that with just a single hidden layer, CSAE-LGC largely improves the performance as compared to CSAE-LC. 6 Related Work Representation learning models have been effective in many tasks such as language modeling (Bengio et al., 2003; Mikolov et al., 2013b), topic modeling (Nguyen et al., 2015), paraphrase detection (Socher et al., 2011), and ranking tasks (Yih et al., 2013). We briefly review works that use context information for text representation. Huang et al. (2012) presented an RNN model that uses document-level context information to construct more accurate word representations. In particular, given a sequence of words, the approach uses other words in the document as external (global) knowledge to predict the next word in the sequence. Other approaches have also modeled context at the document level (Lin et al., 2015; Wang and Cho, 2015; Ji et al., 2016). Ji et al. (2016) p"
P16-1177,D14-1162,0,0.108443,"Missing"
P16-1177,P15-1173,0,0.0245374,"Missing"
P16-1177,D13-1044,0,0.01333,"h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012); “qAns” a TREC QA dataset with ground-truth labels for semantically relevant quest"
P16-1177,P14-1068,0,0.0298506,"ks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various"
P16-1177,D12-1087,0,0.00457205,"ncoder network. Each decoder layer tries to recover the input of its corresponding encoder layer. As such, the weights are initially symmetric and the decoder weights do need to be learned. After the training is complete, the hidden layer hn contains a context-sensitive representation of the inputs x and cx . 2.3 Context Information Context is task and data dependent. For example, a sentence or document that contains a target word forms the word’s context. When context information is not readily available, we use topic models to determine such context for individual inputs (Blei et al., 2003; Stevens et al., 2012). In particular, we use Non-Negative Matrix Factorization (NMF) (Lin, 2007): Given a training set with n instances, i.e., X ∈ Rv×n , where v is the size of a global vocabulary and the scalar k is the number of topics in the dataset, we learn the topic matrix D ∈ Rv×k and context matrix C ∈ Rk×n using the following sparse coding algorithm: min kX − DCk2F + µkCk1 , D,C s.t. (8) D ≥ 0, C ≥ 0, where each column in C is a sparse representation of an input over all topics and will be used as global context information in our model. We obtain context vectors for test instances by transforming them ac"
P16-1177,D07-1003,0,0.0108977,"quation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012); “qAns” a TREC QA dataset with ground-truth labels for semantically relevant questions and (single-sentence) answers from Wang et al. (2007); and “qSim” a community QA dataset crawled from Stack Exchange with ground-truth labels for semantically equivalent questions from Dos Santos et al. (2015). Table 1 shows statistics of these datasets. To enable direct comparison with previous work, we use the same training, development, and test data provided by Dos Santos et al. (2015) and Wang et al. (2007) for qSim and qAns respectively and the entire data of SCWS (in unsupervised setting). We consider local and global context for target words in SCWS. The local context of a target word is its ten neighboring words (five before and five af"
P16-1177,N13-1106,0,0.0321806,"Missing"
P16-1177,P13-1171,0,0.0498837,"ment-wise difference and similarity (in terms of the sign of elements in each dimension) between h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context"
P18-1255,D16-1125,0,0.0215172,"lect the view of the sponsors. Conclusion We have constructed a new dataset for learning to rank clarification questions, and proposed a novel model for solving this task. Our model integrates well-known deep network architectures with the classic notion of expected value of perfect information, which effectively models a pragmatic choice on the part of the questioner: how do I imagine the other party would answer if I were to ask this question. Such pragmatic principles have recently been shown to be useful in other tasks as well (Golland et al., 2010; Smith et al., 2013; Orita et al., 2015; Andreas and Klein, 2016). One can naturally extend our EVPI approach to a full reinforcement learning approach to handle multi-turn conversations. Our results shows that the EVPI model is a promising formalism for the question generation task. In order to move to a full system that can help users like Terry write better posts, there are three interesting lines of future work. First, we need it to be able to generalize: for instance by constructing templates of the form “What version of are you running?” into which the system would need to fill a variable. Second, in order to move from question ranking to question gen"
P18-1255,D11-1039,0,0.0396353,"using our method. Liu et al. (2010) use template question generation to help authors write better related work sections. Mostafazadeh et al. (2016) introduce a Visual Question Generation task where the goal is to generate natural questions that are not about what is present in the image rather about what can be inferred given the image, somewhat analogous to clarification questions. Penas and Hovy (2010) identify the notion of missing information similar to us, but they fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. Artzi and Zettlemoyer (2011) use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed towards simplifying a user query; whereas we generate clarification questions aimed at identifying missing information in a text. Among works that use community question answer forums, the keywords to questions (K2Q) system (Zheng et al., 2011) generates a list of candidate questions and refinement words, given a set of input keywords, to help a user ask a better question. Figueroa and Neumann (2013) rank different paraphrases of query for effective search on forums. (Romeo et al"
P18-1255,D10-1040,0,0.0334878,"essed here are those of the authors and do not necessarily reflect the view of the sponsors. Conclusion We have constructed a new dataset for learning to rank clarification questions, and proposed a novel model for solving this task. Our model integrates well-known deep network architectures with the classic notion of expected value of perfect information, which effectively models a pragmatic choice on the part of the questioner: how do I imagine the other party would answer if I were to ask this question. Such pragmatic principles have recently been shown to be useful in other tasks as well (Golland et al., 2010; Smith et al., 2013; Orita et al., 2015; Andreas and Klein, 2016). One can naturally extend our EVPI approach to a full reinforcement learning approach to handle multi-turn conversations. Our results shows that the EVPI model is a promising formalism for the question generation task. In order to move to a full system that can help users like Terry write better posts, there are three interesting lines of future work. First, we need it to be able to generalize: for instance by constructing templates of the form “What version of are you running?” into which the system would need to fill a variab"
P18-1255,P15-1086,0,0.0257233,"the two feedforward networks Fans and Futil of the EVPI model. non-neural baselines. However, the differences between all the neural models are statistically insignificant.18 6 Related work Most prior work on question generation has focused on generating reading comprehension questions: given text, write questions that one might find on a standardized test (Vanderwende, 2008; Heilman, 2011; Rus et al., 2011; Olney et al., 2012). Comprehension questions, by definition, are answerable from the provided text. Clarification questions–our interest–are not. Outside reading comprehension questions, Labutov et al. (2015) generate high-level question templates by crowdsourcing which leads to significantly less data than we collect using our method. Liu et al. (2010) use template question generation to help authors write better related work sections. Mostafazadeh et al. (2016) introduce a Visual Question Generation task where the goal is to generate natural questions that are not about what is present in the image rather about what can be inferred given the image, somewhat analogous to clarification questions. Penas and Hovy (2010) identify the notion of missing information similar to us, but they fill the know"
P18-1255,D16-1230,0,0.0393779,"Missing"
P18-1255,W15-4640,0,0.033047,"forms similar to random, unlike when evaluated against human judgments. The Community QA baseline again outperforms Neural(p, q) model and comes very close to the Neural (p, a) model. As before, the neural baselines that make use of the answer outperform the one that does not use the answer and the EVPI model performs significantly better than Neural(p, q, a). 5.2.3 Excluding the original question In the preceding analysis, we considered a setting in which the “ground truth” original question was in the candidate set Q. While this is a common evaluation framework in dialog response selection (Lowe et al., 2015), it is overly optimistic. We, therefore, evaluate against the “best” and the “valid” annotations on the nine other question candidates. We find that the neural models beat the 17 We use 10 hidden layers in the feedforward network of the neural baseline and five hidden layers each in the two feedforward networks Fans and Futil of the EVPI model. non-neural baselines. However, the differences between all the neural models are statistically insignificant.18 6 Related work Most prior work on question generation has focused on generating reading comprehension questions: given text, write questions"
P18-1255,P16-1170,0,0.0324922,"g reading comprehension questions: given text, write questions that one might find on a standardized test (Vanderwende, 2008; Heilman, 2011; Rus et al., 2011; Olney et al., 2012). Comprehension questions, by definition, are answerable from the provided text. Clarification questions–our interest–are not. Outside reading comprehension questions, Labutov et al. (2015) generate high-level question templates by crowdsourcing which leads to significantly less data than we collect using our method. Liu et al. (2010) use template question generation to help authors write better related work sections. Mostafazadeh et al. (2016) introduce a Visual Question Generation task where the goal is to generate natural questions that are not about what is present in the image rather about what can be inferred given the image, somewhat analogous to clarification questions. Penas and Hovy (2010) identify the notion of missing information similar to us, but they fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. Artzi and Zettlemoyer (2011) use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed tow"
P18-1255,S17-2003,0,0.0489357,"Missing"
P18-1255,S17-2009,0,0.0155486,"the positive and negative candidate triples (same as in our utility calculator (§2.3)) to minimize hinge loss on misclassification error using cross-product features between each of (p, q), (q, a) and (p, a). We tune the ngram length and choose n=3 which performs best on the tune set. The question candidates are finally ranked according to their predictions for the positive label. Community QA: The recent SemEval2017 Community Question-Answering (CQA) (Nakov et al., 2017) included a subtask for ranking a set of comments according to their relevance to a given post in the Qatar Living15 forum. Nandi et al. (2017), winners of this subtask, developed a logistic regression model using features based on 14 15 string similarity, word embeddings, etc. We train this model on all the positively and negatively labelled (p, q) pairs in our dataset (same as in our utility calculator (§ 2.3), but without a). We use a subset of their features relevant to our task.16 Neural baselines: We construct the following neural baselines based on the LSTM representation of their inputs (as described in §2.4): 1. Neural(p, q): Input is concatenation of p¯ and q¯. 2. Neural(p, a): Input is concatenation of p¯ and a ¯. 3. Neura"
P18-1255,P15-1158,1,0.84195,"Missing"
P18-1255,W01-1614,0,0.0261366,"Missing"
P18-1255,C16-1163,0,0.13525,"Missing"
P18-1255,W11-2853,0,0.118948,"valid” annotations on the nine other question candidates. We find that the neural models beat the 17 We use 10 hidden layers in the feedforward network of the neural baseline and five hidden layers each in the two feedforward networks Fans and Futil of the EVPI model. non-neural baselines. However, the differences between all the neural models are statistically insignificant.18 6 Related work Most prior work on question generation has focused on generating reading comprehension questions: given text, write questions that one might find on a standardized test (Vanderwende, 2008; Heilman, 2011; Rus et al., 2011; Olney et al., 2012). Comprehension questions, by definition, are answerable from the provided text. Clarification questions–our interest–are not. Outside reading comprehension questions, Labutov et al. (2015) generate high-level question templates by crowdsourcing which leads to significantly less data than we collect using our method. Liu et al. (2010) use template question generation to help authors write better related work sections. Mostafazadeh et al. (2016) introduce a Visual Question Generation task where the goal is to generate natural questions that are not about what is present in"
P18-1255,W16-0106,0,0.0499705,"Missing"
P18-1255,I11-1106,0,0.101437,"ation questions. Penas and Hovy (2010) identify the notion of missing information similar to us, but they fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. Artzi and Zettlemoyer (2011) use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed towards simplifying a user query; whereas we generate clarification questions aimed at identifying missing information in a text. Among works that use community question answer forums, the keywords to questions (K2Q) system (Zheng et al., 2011) generates a list of candidate questions and refinement words, given a set of input keywords, to help a user ask a better question. Figueroa and Neumann (2013) rank different paraphrases of query for effective search on forums. (Romeo et al., 2016) develop a neural network based model for ranking questions on forums with the intent of retrieving similar other question. The recent SemEval-2017 Community QuestionAnswering (CQA) (Nakov et al., 2017) task included a subtask to rank the comments according to their relevance to the post. Our task primarily differs from this task in that we want to i"
P18-1255,C10-2113,0,0.0174471,"ication questions–our interest–are not. Outside reading comprehension questions, Labutov et al. (2015) generate high-level question templates by crowdsourcing which leads to significantly less data than we collect using our method. Liu et al. (2010) use template question generation to help authors write better related work sections. Mostafazadeh et al. (2016) introduce a Visual Question Generation task where the goal is to generate natural questions that are not about what is present in the image rather about what can be inferred given the image, somewhat analogous to clarification questions. Penas and Hovy (2010) identify the notion of missing information similar to us, but they fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions. Artzi and Zettlemoyer (2011) use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed towards simplifying a user query; whereas we generate clarification questions aimed at identifying missing information in a text. Among works that use community question answer forums, the keywords to questions (K2Q) system (Zheng et al., 2011) generates a list o"
P18-1255,D14-1162,0,0.0810535,") (details in § 2.4). The utility of the updated post is then defined as U(pi + aj ) = σ(Futil (p¯i , q¯j , a¯j ))5 . We want this utility to be close to 1 for all the positively labelled (p, q, a) triples and close to 0 for all the negatively labelled (p, q, a) triples. We therefore define our loss using the binary cross-entropy formulation below: lossutil (yi , p¯i , q¯j , a ¯j ) = yi log(σ(Futil (p¯i , q¯j , a¯j ))) (4) 2.4 Our joint neural network model Our fundamental representation is based on recurrent neural networks over word embeddings. We obtain the word embeddings using the GloVe (Pennington et al., 2014) model trained on the entire datadump of StackExchange.6 . In Eq 2 and Eq 3, the average word vector representations qˆ and a ˆ are obtained by averaging the GloVe word embeddings for all words in the question and the answer respectively. Given an initial post p, we generate a post neural representation p¯ using a post LSTM (long short-term memory architecture) (Hochreiter and Schmidhuber, 1997). The input layer consists of word embeddings of the words in the post which is fed into a single hidden layer. The output of each of the hidden states is averaged together to get our neural representat"
P18-1255,W01-0902,0,\N,Missing
Q13-1035,P11-2078,0,0.0164661,"retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language"
Q13-1035,D11-1033,0,0.0547371,"tputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fin"
Q13-1035,P11-1022,0,0.0233897,"the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We can then check whether the MT output has the same set of English w"
Q13-1035,C12-1010,0,0.0640096,"Missing"
Q13-1035,2011.iwslt-evaluation.18,0,0.0835001,"mately as much error as unseen words, suggesting a novel avenue for research in sense induction. Unfortunately, it appears that choosing the right sense for these at translation time is even more difficult than in the unseen word case. 4. The story is more complicated for seen words with known translations: if we limit ourselves to “high To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some of our conclusions ar"
Q13-1035,C04-1046,0,0.0353001,"ed. F IGURE 1: Example of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We c"
Q13-1035,D07-1007,1,0.617467,"le of WADE visualization. Dashed boxes around the French input mark the phrase spans used by the decoder. see what happens at the word level (regardless of how it affects translation performance) and a macrolevel analysis to discover impact on corpus translation performance. We focus on the first three S4 categories and separately discuss search errors (§7). In both cases, we use exact string match to detect translation equivalences, as has been done previously in other settings that also use word alignments to inspect errors or automatically generate data for other tasks (Blatz et al., 2004; Carpuat and Wu, 2007; Popovi´c and Ney, 2011; Bach et al., 2011, among others). 5.1 Micro-analysis: WADE We define Word Alignment Driven Evaluation, or WADE, which is a technique for analyzing MT system output at the word level, allowing us to (1) manually browse visualizations of MT output annotated with S4 error types, and (2) aggregate counts of errors. WADE is based on the fact that we can automatically word-align a French test sentence and its English reference translation, and the MT decoder naturally produces a word alignment between a French sentence and its machine translation. We can then check whether"
Q13-1035,P13-1141,1,0.049126,"its well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributions from comparable new domain corpora (Irvine et al., 2013). Earlier, Daum´e III and Jagarlamudi (2011) showed how mining translations for unseen words from comparable corpora can im431 prove SMT in a new domain. 4 The S4 Taxonomy We begin with a simple question: when we move an SMT system from an old domain to a new domain, what goes wrong ? We employ a set of four error types as our taxonomy. We refer to these error types as SEEN, SENSE, SCORE and SEARCH, and together as the S4 taxonomy: SEEN : an attempt to translate a sour"
Q13-1035,D11-1003,0,0.0173755,"Missing"
Q13-1035,N12-1047,0,0.0250817,"iments. Each system scores translation candidates using standard features: 5 phrase-table features, including phrasal translation probabilities and lexical weights in both translation directions, and a constant phrase penalty; 6 lexicalized reordering features, including bidirectional models built for monotone, swap, discontinuous reorderings; 1 distance-based reordering feature; and 2 language models, a 5-gram model learned on the OLD domain, and a 5-gram model learned on the NEW domain. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MIRA algorithm (Cherry and Foster, 2012). This results in a strong large-scale OLD system, which performs well on the old domain and is a good starting point for studying domain shifts. 5 The word alignments, 4. Hansards, News and the Canadian Science Publishing are available, respectively, at: http://www.parl.gc.ca, http: //www.statmt.org/wmt09/translation-task.html, and http://www.nrcresearchpress.com, preprocessed versions and data splits used in this paper can be downloaded from http://hal3.name/damt. 5. We use (unadapted) HMM word alignments (Vogel et 434 language models and tuning sets are kept constant across all experiments"
Q13-1035,J07-2003,0,0.0483847,"ion approach might achieve the best of both worlds. 8 Limiting Assumptions This paper represents a partial exploration of the space of possible assumptions about models and data. We cannot hope to explore the combinatorial explosion of possibilities, and therefore have restricted our analysis to the following settings: Phrase-based models. All of our experiments are carried out using phrase-based translation, as implemented in the open-source Moses translation system (Koehn et al., 2007) to ensure that they are reproducible. Our methods are easily extended to hierarchical phrase-based models (Chiang, 2007). It is not clear whether the same conclusions would hold: on the one hand, complex phrasal rules might overfit even more badly than phrases; on the other hand, hierarchical models might have more flexibility to generalize structures. Translation languages. We only translate from French to English. This well-studied language pair presents several advantages; large quantities of data are publicly available in a wide variety of domains, and standard statistical machine translation architectures yield good performance. Unlike with more distant languages such as Chinese-English, or languages with"
Q13-1035,P11-2071,1,0.879591,"Missing"
Q13-1035,2010.iwslt-papers.5,0,0.444775,"ccount for approximately as much error as unseen words, suggesting a novel avenue for research in sense induction. Unfortunately, it appears that choosing the right sense for these at translation time is even more difficult than in the unseen word case. 4. The story is more complicated for seen words with known translations: if we limit ourselves to “high To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some"
Q13-1035,W07-0717,0,0.799178,"nguage model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an"
Q13-1035,D10-1044,0,0.231096,"is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been use"
Q13-1035,D11-1084,0,0.0142283,"r to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pai"
Q13-1035,W12-3154,0,0.289779,"s unseen words, suggesting a novel avenue for research in sense induction. Unfortunately, it appears that choosing the right sense for these at translation time is even more difficult than in the unseen word case. 4. The story is more complicated for seen words with known translations: if we limit ourselves to “high To date, work on domain adaptation in SMT mostly proposed methods to efficiently combine data from multiple domains. To the best of our knowledge, there have been only a few studies to understand how domain shifts affect translation quality (Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012). However, these start from different premises than this paper, and as a result, ask related but complementary questions. These previous analyses focus on how to improve a particular MT architecture (trained on new domain data) by injecting old domain data into a specific part of the pipeline in order to improve BLEU score. In comparison to this work, we focus on finer-grained phenomena. We distinguish between effects previously lumped together as “missing phrase-table entries.” Despite different starting assumptions, language pairs and data, some of our conclusions are consistent with previou"
Q13-1035,2005.eamt-1.19,0,0.0209944,"refer to domains and source/target to refer to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010)"
Q13-1035,D13-1109,1,0.875633,"Missing"
Q13-1035,W07-0733,0,0.584855,"lier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributio"
Q13-1035,2005.iwslt-1.8,0,0.15512,"Missing"
Q13-1035,P07-2045,0,0.00379976,"of informal noisy text. 4 In this study, we use the Hansard domain as the domain, and we consider four possible NEW domains: EMEA, News, Science and Subs. Data sets for all domains were processed consistently. After tokenization, we paid particular attention to normalization in order to minimize artificial differences when combining data, such as American, British and Canadian spellings. This proved particularly important for the news domain; the impact of SEEN reduced by more than half after normalization. OLD 6.2 MT systems We build standard phrase-based SMT systems using the Moses toolkit (Koehn et al., 2007) for all experiments. Each system scores translation candidates using standard features: 5 phrase-table features, including phrasal translation probabilities and lexical weights in both translation directions, and a constant phrase penalty; 6 lexicalized reordering features, including bidirectional models built for monotone, swap, discontinuous reorderings; 1 distance-based reordering feature; and 2 language models, a 5-gram model learned on the OLD domain, and a 5-gram model learned on the NEW domain. Features are combined using a log-linear model optimized for BLEU, using the n-best batch MI"
Q13-1035,lambert-etal-2012-automatic,0,0.0176546,"ailable for the WMT 2009 evaluation. It has been commonly used in the domain adaptation literature (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Haddow and Koehn, 2012, for instance). Science: Parallel abstracts from scientific publications in many disciplines including physics, biology, and computer science. We collected data from two distinct sources: (1) Canadian Science Publishing made available translated abstracts from their journals which span many research disciplines; (2) parallel abstracts from PhD theses in Physics and Computer Science collected from the HAL public repository (Lambert et al., 2012). Subs: Translated movie subtitles, available through the OPUS corpora collection (Tiedemann, 2009). In contrast to the other domains considered, subtitles consist of informal noisy text. 4 In this study, we use the Hansard domain as the domain, and we consider four possible NEW domains: EMEA, News, Science and Subs. Data sets for all domains were processed consistently. After tokenization, we paid particular attention to normalization in order to minimize artificial differences when combining data, such as American, British and Canadian spellings. This proved particularly important for the ne"
Q13-1035,2011.iwslt-evaluation.7,0,0.0157753,"Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a n"
Q13-1035,D07-1036,0,0.0113529,"ce/target to refer to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adaptin"
Q13-1035,2011.iwslt-papers.5,0,0.0211927,"http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on tr"
Q13-1035,D09-1074,0,0.361269,"been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech r"
Q13-1035,W09-2412,0,0.0488796,"Missing"
Q13-1035,2010.eamt-1.29,0,0.0130978,"o et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al"
Q13-1035,J11-4002,0,0.0861812,"Missing"
Q13-1035,P12-1099,0,0.016185,"b-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying when a word has gained a new sense in a new domain (Carpuat et al., 2013), as well as learning joint word translation probability distributions from comparable new domain corpora (Irvine et al., 2013). Earlier, Daum´e III and Jagarlamudi (2011) showed how mining translations for unseen words from comparable corpora can im431 prove SMT in a new domain. 4 The S4 Taxonomy We begin with a simple question: when w"
Q13-1035,E12-1055,0,0.0861505,". 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality"
Q13-1035,W10-1728,0,0.0230941,"dels (Niehues and Waibel, 2010). In particular, Foster et al. (2010) show that adapting at the phrase pair levels outperform earlier coarser corpus level combination approaches (Foster and Kuhn, 2007). This is consistent with our analysis: domain shifts have a fine-grained impact on translation quality. Finally, strategies have been proposed to combine sub-models trained independently on different sub-corpora. Linear interpolation is widely used for mixing language models in speech recognition, and it has also been used for adapting translation and language models in MT(Foster and Kuhn, 2007; Tiedemann, 2010; Lavergne et al., 2011). Log-linear combination fits well in existing SMT architectures (Foster and Kuhn, 2007; Koehn and Schroeder, 2007). Koehn and Schroeder (2007) consider both an intersection setting (where only entries occurring in all phrase-tables combined are considered), and a union setting (where entries which are not in the intersection are given an arbitrary null score). Razmara et al. (2012) take this approach further and frame combination as ensemble decoding. 3.3 Targeting Specific Error Types The experiments conducted in this article motivated follow-up work on identifying wh"
Q13-1035,N03-1033,0,0.00307439,"Missing"
Q13-1035,vilar-etal-2006-error,0,0.0997081,"Missing"
Q13-1035,C96-2141,0,0.226061,"Missing"
Q13-1035,D10-1091,0,0.0980275,"Missing"
Q13-1035,C04-1059,0,0.0192079,"vs. frequent phrases. 1. We use old/new to refer to domains and source/target to refer to languages, to avoid ambiguity (we stay away from indomain and out-of-domain, which is itself ambiguous). 2. All source data, methodological code and outputs are available at http://hal3.name/damt. 3.2 430 Domain Adaptation for MT Prior work focuses on methods combining data from old and new domains to learn translation and language models. Many filtering techniques have been proposed to select OLD data that is similar to NEW . Information retrieval techniques have been used to improve the language model (Zhao et al., 2004), the translation model (Hildebrand et al., 2005; Lu et al., 2007; Gong et al., 2011; Duh et al., 2010; Banerjee et al., 2012), or both (Lu et al., 2007); language model cross-entropy has also been used for data selection (Axelrod et al., 2011; Mansour et al., 2011; Sennrich, 2012). Another research thread addresses corpora weighting, rather than hard filtering. Weighting has been applied at different levels of granularity: sentence pairs (Matsoukas et al., 2009), phrase pairs (Foster et al., 2010), n-grams (Ananthakrishnan et al., 2011), or sub-corpora through factored models (Niehues and Wai"
Q13-1035,S10-1002,0,\N,Missing
S16-1184,W13-2322,0,0.0373079,"ms in NLP like part-of-speech tagging, named entity recognition (Daum´e III et al., 2014), coreference resolution (Ma et al., 2014), and dependency parsing (He et al., 2013). Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses ∗ The first two authors contributed equally to this work. Figure 1: AMR graph for the sentence “I read a book, called Stories from Nature, about the forest.” that control a policy that takes actions in this search space. AMR (Banarescu et al., 2013), in turn, is a structured semantic representation which is a rooted, directed, acyclic graph. The nodes of this graph represent concepts in the given sentence and the edges represent relations between these concepts. As such, the task of predicting AMRs can be naturally placed in the L2S framework. This allows us to model the learning of concepts and relations in a unified setting which aims to minimize the loss over the entire predicted structure. In the next section, we briefly review DAGGER and explain its various components with respect to our AMR parsing task. Section 3 describes our mai"
S16-1184,P13-2131,0,0.0685972,"ed root croot and the root of a component, we get the k-best list (as described in section 3.2) between them and choose the most frequent edge from it. Dataset BOLT DF MT Broadcast conversation Weblog and WSJ BOLT DF English Guidelines AMRs 2009 Open MT Proxy reports Weblog Xinhua MT Training 1061 214 0 6455 689 204 6603 866 741 Dev 133 0 100 210 0 0 826 0 99 Test 133 0 100 229 0 0 823 0 86 Table 4: Dataset statistics. All figures represent number of sentences. bit machine learning library (Langford et al., 2007; Daum´e III et al., 2014). The evaluation of predicted AMRs is done using Smatch (Cai and Knight, 2013) 1 , which compares two AMRs using precision, recall and F1 . Our system obtained a Smatch F1 score of 0.46 with a P recision of 0.51 and a Recall of 0.43 on the test set in the Shared Task (We made a tokenization error during the actual semeval submission and so reported an F1 score of 0.44 instead). The mean F1 score of all systems submitted to the shared task was 0.55 and the standard deviation was 0.06. 5 3.6 Acyclicity The post-processing step described in the previous section ensures that the predicted AMRs are rooted, connected, graphs. However, an AMR, by definition, is also acyclic. W"
S16-1184,P04-1015,0,0.0735607,"tasks are treated as a sequence of predictions. Using Learning to Search, we add past predictions as features for future predictions, and define a combined loss over the entire AMR structure. ARG1 ARG0 book i topic name name op1 forest op3 op2 Nature Stories from 1 Introduction This paper describes our submission to the Abstract Meaning Representation (AMR) Parsing Shared Task at SemEval 2016. The goal of the task is to generate AMRs automatically for English sentences. We develop a novel technique for AMR parsing that uses Learning to Search (L2S) (Ross et al., 2011; Daum´e III et al., 2009; Collins and Roark, 2004). L2S is a family of approaches that solves structured prediction problems. These algorithms have proven to be highly effective for problems in NLP like part-of-speech tagging, named entity recognition (Daum´e III et al., 2014), coreference resolution (Ma et al., 2014), and dependency parsing (He et al., 2013). Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses ∗ The first two authors contributed equally to this work. Figure 1: AMR graph for the se"
S16-1184,de-marneffe-etal-2006-generating,0,0.0498589,"Missing"
S16-1184,D13-1152,1,0.899758,"Missing"
S16-1184,D14-1225,0,0.0312202,"n This paper describes our submission to the Abstract Meaning Representation (AMR) Parsing Shared Task at SemEval 2016. The goal of the task is to generate AMRs automatically for English sentences. We develop a novel technique for AMR parsing that uses Learning to Search (L2S) (Ross et al., 2011; Daum´e III et al., 2009; Collins and Roark, 2004). L2S is a family of approaches that solves structured prediction problems. These algorithms have proven to be highly effective for problems in NLP like part-of-speech tagging, named entity recognition (Daum´e III et al., 2014), coreference resolution (Ma et al., 2014), and dependency parsing (He et al., 2013). Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses ∗ The first two authors contributed equally to this work. Figure 1: AMR graph for the sentence “I read a book, called Stories from Nature, about the forest.” that control a policy that takes actions in this search space. AMR (Banarescu et al., 2013), in turn, is a structured semantic representation which is a rooted, directed, acyclic graph. The nodes of"
W02-2102,W02-2103,1,\N,Missing
W02-2102,A00-2018,0,\N,Missing
W02-2102,A00-2023,0,\N,Missing
W02-2102,C00-1007,0,\N,Missing
W02-2102,P98-1116,1,\N,Missing
W02-2102,C98-1112,1,\N,Missing
W02-2102,P98-1035,0,\N,Missing
W02-2102,C98-1035,0,\N,Missing
W02-2102,P02-1039,1,\N,Missing
W02-2102,P01-1067,1,\N,Missing
W02-2102,P99-1042,0,\N,Missing
W02-2102,J01-2004,0,\N,Missing
W02-2102,P02-1057,1,\N,Missing
W02-2102,P01-1017,0,\N,Missing
W04-1016,P99-1071,0,0.142688,"f we show many humans the same two sentences, they will produce similar summaries. Of course we do not penalize one human for using different words than another. The sentence fusion task is interesting after performing sentence extraction, the extracted sentences often contain superfluous information. It has been further observed that simply compressing sentences individually and concatenating the results leads to suboptimal summaries (Daum´e III and Marcu, 2002). The use of sentence fusion in multidocument summarization has been extensively explored by Barzilay in her thesis (Barzilay, 2003; Barzilay et al., 1999), though in the multi-document setting, one has redundancy to fall back on. Additionally, the sentence fusion task is sufficiently constrained that it makes possible more complex and linguistically motivated manipulations than are reasonable for full document or multi-document summaries (and for which simple extraction techniques are unlikely to suffice). 3 Data Collection Our data comes from a collection of computer product reviews from the Ziff-Davis corporation. This corpus consists of roughly seven thousand documents paired with human written abstracts. The average document was 1080 words"
W04-1016,P00-1038,0,0.0268218,"urbing lack of agreement. 1 Introduction and Motivation The practices of automatic summarization vary widely across many dimensions, including source length, summary length, style, source, topic, language, and structure. Most typical are summaries of a single news document down to a headline or short summary, or of a collection of news documents down to a headline or short summary (Hahn and Harman, 2002). A few researchers have focused on other aspects of summarization, including single sentence (Knight and Marcu, 2002), paragraph or short document (Daum´e III and Marcu, 2002), query-focused (Berger and Mittal, 2000), or speech (Hori et al., 2003). The techniques relevant to, and the challenges faced in each of these tasks can be quite different. Nevertheless, they all rely on one critical assumption: there exists a notion of (relative) importance between pieces of information in a document (or utterance), regardless of whether we can detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive"
W04-1016,P02-1057,1,0.90435,"Missing"
W04-1016,W04-3216,1,0.838325,"Missing"
W04-1016,W03-0508,0,0.279232,"evant to, and the challenges faced in each of these tasks can be quite different. Nevertheless, they all rely on one critical assumption: there exists a notion of (relative) importance between pieces of information in a document (or utterance), regardless of whether we can detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly o"
W04-1016,J02-4006,0,0.0715519,"Missing"
W04-1016,N03-1020,0,0.0605769,"n detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend to show little (statistically significant) difference from one another. Moreover, a baseline system that simply takes the first sentences of a document performs just as well or better than int"
W04-1016,W03-1101,0,0.0363138,"Missing"
W04-1016,N04-1019,0,0.398312,"different. Nevertheless, they all rely on one critical assumption: there exists a notion of (relative) importance between pieces of information in a document (or utterance), regardless of whether we can detect this or not. Indeed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend"
W04-1016,P02-1040,0,0.0727142,"cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend to show little (statistically significant) difference from one another. Moreover, a baseline system that simply takes the first sentences of a document performs just as well or better than intelligently crafted systems when summarizing news stories. Additionally, studies of vast numbers of summar"
W04-1016,W03-2805,0,0.0152985,"deed, recent research has looked at this question in detail, and can be rather cleanly divided into two partitions. The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). The other half aims to develop automatic evaluation criteria to imitate the manual evaluation methods (or at least to complement them). Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al., 2002) in summarization. The results of these investigations have been mixed. In the DUC competitions (Hahn and Harman, 2002), when manual evaluation has been employed, it has been commonly observed that humanwritten summaries grossly outscore any machineproduced summary. All machine-produced summaries tend to show little (statistically significant) difference from one another. Moreover, a baseline system that simply takes the first sentences of a document performs just as well or better than intelligently crafted systems whe"
W04-1016,2003.mtsummit-papers.9,0,\N,Missing
W04-3216,P00-1041,0,0.0589207,"Missing"
W04-3216,J93-2003,0,0.0154546,"Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {hdaume,marcu}@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, researchers in text summarization have employed one of several techniques. Some research"
W04-3216,P02-1057,1,0.909237,"Missing"
W04-3216,J02-4006,0,0.308806,"Document/Abstract Alignment Hal Daum´e III and Daniel Marcu Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 {hdaume,marcu}@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts. Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs. Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al., 1993). 1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts. Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing. Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations. To get around directly tackling this problem, research"
W04-3216,J00-2011,0,0.0432923,"the alphabet K. πj is the probability of beginning in state j. The transition probability ai,j is the probability of transitioning from state i to state j. bi,j,k¯ is the probability of emitting (the non-empty) observation sequence k¯ while transitioning from state i to state j. Finally, x t denotes the state after emitting t symbols. The full derivation of the model is too lengthy to include; the interested reader is directed to (Daum´e III and Marcu, 2002b) for the derivations and proofs of the formulae. To assist the reader in understanding the mathematics, we follow the same notation as (Manning and Schutze, 2000). The formulae for the calculations are summarized in Table 2. 2.2.1 Forward algorithm The forward algorithm calculates the probability of an observation sequence. We define α j (t) as the probability of being in state j after emitting the first t − 1 symbols (in whatever grouping we want). 2.2.2 Backward algorithm Just as we can compute the probability of an observation sequence by moving forward, so can we calculate it by going backward. We define β i (t) as the probability of emitting the sequence o Tt given that we are starting out in state i. 2.2.3 Best path We define a path as a sequence"
W04-3216,J03-1002,0,0.0640924,"In Figure 1, we observe several phenomena: • Alignments can occur at the granularity of words and at the granularity of phrases. • The ordering of phrases in an abstract can be different from the ordering in the document. • Some abstract words do not have direct correspondents in the document, and some document words are never used. It is thus desirable to be able to automatically construct alignments between documents and their abstracts, so that the correspondences between the pairs are obvious. One might be initially tempted to use readily-available machine translation systems like GIZA++ (Och and Ney, 2003) to perform such Connecting Point has become the single largest Mac retailer after tripling it ’s Macintosh sales since January 1989 . Connecting Point Systems tripled it ’s sales of Apple Macintosh systems since last January . It is now the single largest seller of Macintosh . Figure 1: Example abstract/text alignment. alignments. However, as we will show, the alignments produced by such a system are inadequate for this task. The solution that we propose to this problem is an alignment model based on a novel mathematical structure we call the Phrase-Based HMM. 2 Designing a Model As observed"
W04-3216,C96-2141,0,0.0664892,"ak for Possible alignments (considering only the 40 independently annotated pairs). When considering only Sure alignments, the kappa statistic (over 7.2 million items, 2 annotators and 2 categories) for agreement was 0.63. When words from the ignore-list were thrown out, this rose to 0.68. Carletta (1995) suggests that kappa values over 0.80 reflect very strong agreement and that kappa values between 0.60 and 0.80 reflect good agreement. 3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al., 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003). We modified this slightly to allow longer inputs and higher fertilities. Such translation models require that input be in sentence-aligned form. In the summarization task, however, one abstract sentence often corresponds to multiple document sentences. In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999). In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human. Each pair w"
W04-3216,J96-2004,0,\N,Missing
W04-3233,J99-2004,0,0.0237495,"erforms comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient. Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is employed, which leads to globally poorer parses. In contrast, th"
W04-3233,J96-1002,0,0.0103478,"(CRFs). We adopt a slight variant of the MEMM framework. 2.1 Maximum Entropy Tagging Model In the formulation of the maximum entropy tagging model, we assume that the probability distribution of tags takes the form of an exponential distribution, parameterized by a sequence of feature weights, λm 1 , where there are m-many features. Thus, we where Zti−1 ,w¯ is a normalizing factor. Like other maximum entropy approaches, this distribution is unimodal and optimal values for the λs can be found through various algorithms; we use GIS. A good introduction to maximum entropy models can be found in (Berger et al., 1996). In our approach, we use a tag set of exactly five tags: {open, close, in, out, sing}. An open tag is assigned to all words that open a bracketing (regardless of the number of brackets opened) and do not also close a bracketing. A close tag is assigned to all words that close a bracketing and do not also open one. An in tag is assigned to all words enclosed in an NP, but which neither open nor close one. An out tag is assigned to all words which are not enclosed in an NP. A sing(leton) tag is assigned to all words that both open and close a bracketing (regardless of whether they open or close"
W04-3233,J95-4004,0,0.0482029,"Missing"
W04-3233,A00-2018,0,0.0919932,"ecoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient. Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is employed, which lead"
W04-3233,J03-4003,0,0.277131,"mined only at decoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), they can be significantly more computationally efficient. Statistical parsers tend to scale exponentially in sentence length, unless a narrow beam is emp"
W04-3233,P00-1007,0,0.0178366,"tition, NP Bracketing systems were trained on sections 15-18 of the Wall Street Journal corpus, while section 20 was used for testing. The bracketing information was extracted directly from the Penn Treebank, essentially disregarding all non-NP brackets. An example bracketed sentence is in Figure 1. There have been several successful approaches reported in the literature to solve this task. Tjong Kim Sang (1999) first used repeated chunking to attain an f-score of 82.98 during the CoNLL competition and subsequently (Sang, 2002) an f-score of 83.79 using a combination of two different systems. Krymolowski and Dagan (2000) have obtained similar results using more training data and lexicalization. Brandts (1999) has used cascaded HMMs to solve the NP Bracketing problem; however, he evaluated his system only on German NPs, so his results cannot be directly compared. Obviously, the difficulty that arises in NP Bracketing that differentiates it from NP Chunking is the issue of embedded NPs, thus requiring output in the 30 obtain a distribution for P rλm (ti ti−1 , w) ¯ of the 1 form:   m X 1 (1) λj fj (ti , ti−1 , w) ¯  exp  Zti−1 ,w¯ Charniak Collins Bracketer+SVM Bracketer Seconds to Parse (normalized) 25 20"
W04-3233,N01-1025,0,0.0610191,"Missing"
W04-3233,P02-1038,0,0.0920675,"Missing"
W04-3233,W95-0107,0,0.0539695,"amework based on support vector machines. We solve the problem of hierarchical structure in our tagging model by modeling underspecified tags, which are fully determined only at decoding time. The tagging model performs comparably to competing approaches and the subsequent reranking increases our system’s performance from an f-score of 81.7 to 86.1, surpassing the best reported results to date of 83.8. 1 Introduction and Prior Work Noun Phrase Bracketing (NP Bracketing) is the task of identifying any and all noun phrases in a sentence. It is a strictly more difficult problem than NP Chunking (Ramshaw and Marcus, 1995), in which only non-recursive (or “base”) noun phrases are identified. It is simultaneously strictly more simple than either full parsing (Collins, 2003; Charniak, 2000) or supertagging (Bangalore and Joshi, 1999). NP Bracketing is both a useful first step toward full parsing and also a meaningful task in its own right; for instance as an initial step toward co-reference resolution and noun-phrase translation. While existing NP Bracketers (including the one described in this paper) tend to achieve worse overall F-measures than a full statistical parser (eg., (Collins, 2003; Charniak, 2000)), t"
W04-3233,E99-1016,0,\N,Missing
W04-3233,N03-1028,0,\N,Missing
W10-0203,W06-1651,0,0.0134263,"es to propagate affect states onto the characters, and in some cases, to infer new affect states. 4.1 Step 1: Assigning Affect Tags to Words 4.1.1 Sentiment Analysis Resources AESOP incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech. • A list of 228 speech act verbs compiled from (Wierzbicka, 1987)5 , which are used for M states. 4.1.2 Patient Polarity Verbs As we discussed in Section 3.4, existing resources are not sufficient to identify affect states t"
W10-0203,W06-0301,0,0.016285,"fect tags from words and phrases to characters in the story via syntactic relations. During the course of our research, we came to appreciate that affect states, of the type required for plot units, can represent much more than just direct expressions of emotion. A common phenomena are affect states that result from a character being acted upon in a positive or negative way. For example, “the cat ate the mouse” produces a positive affect state for the cat and a negative affect 1 This is somewhat analogous to, but not exactly the same as, associating opinion words with their targets or topics (Kim and Hovy, 2006; Stoyanov and Cardie, 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17–25, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics The Father and His Sons (s1) A father had a family of sons who were perpetually quarreling among themselves. (s2) When he failed to heal their disputes by his exhortations, he determined to give them a practical illustration of the evils of disunion; and for this purpose he one day told them to bring him a bundle of sticks. (s3) When they had done so,"
W10-0203,C08-1103,0,0.0131807,"and phrases to characters in the story via syntactic relations. During the course of our research, we came to appreciate that affect states, of the type required for plot units, can represent much more than just direct expressions of emotion. A common phenomena are affect states that result from a character being acted upon in a positive or negative way. For example, “the cat ate the mouse” produces a positive affect state for the cat and a negative affect 1 This is somewhat analogous to, but not exactly the same as, associating opinion words with their targets or topics (Kim and Hovy, 2006; Stoyanov and Cardie, 2008). Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 17–25, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics The Father and His Sons (s1) A father had a family of sons who were perpetually quarreling among themselves. (s2) When he failed to heal their disputes by his exhortations, he determined to give them a practical illustration of the evils of disunion; and for this purpose he one day told them to bring him a bundle of sticks. (s3) When they had done so, he placed the faggot into th"
W10-0203,P05-1017,0,0.0912897,"ognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech. • A list of 228 speech act verbs compiled from (Wierzbicka, 1987)5 , which are used for M states. 4.1.2 Patient Polarity Verbs As we discussed in Section 3.4, existing resources are not sufficient to identify affect states that arise from a character being acted upon. Sentiment lexicons, for example, assign polarity to verbs irrespective of their agents or patients. To fill this gap, we tried to automatically acquire verbs that have a strong patient polarity (i.e., the p"
W10-0203,H05-2018,1,0.78312,"syntactic relations. AESOP produces affect states in a 3-step process. First, AESOP labels individual words and phrases with an M, +, or - affect tag. Second, it identifies all references to the two main characters of the story. Third, AESOP applies affect projection rules to propagate affect states onto the characters, and in some cases, to infer new affect states. 4.1 Step 1: Assigning Affect Tags to Words 4.1.1 Sentiment Analysis Resources AESOP incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when t"
W10-0203,H05-1044,0,0.0338825,"Second, it identifies all references to the two main characters of the story. Third, AESOP applies affect projection rules to propagate affect states onto the characters, and in some cases, to infer new affect states. 4.1 Step 1: Assigning Affect Tags to Words 4.1.1 Sentiment Analysis Resources AESOP incorporates several existing sentiment analysis resources to recognize affect states associated with emotions and speech acts. • OpinionFinder2 (Wilson et al., 2005) (Version 1.4) is used to identify all three types of states. We use the +/- labels assigned by its contextual polarity classifier (Wilson, 2005) to create +/- affect tags. The MPQASD tags produced by its Direct Subjective and Speech Event Identifier (Choi et al., 2006) are used as M affect tags. • Subjectivity Lexicon3 (Wilson, 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated partof-speech (POS). • Semantic Orientation Lexicon4 (Takamura et al., 2005): The positive/negative words in this list are assigned +/- affect tags, when they occur with the designated part-of-speech. • A list of 228 speech act verbs compiled from (Wierzbicka, 1987)5 , which are used for M states. 4"
W10-1503,N09-1003,0,0.261624,"Missing"
W10-1503,D07-1090,0,0.0694054,"r of its approximate counts by a factor of two. We show that it is possible to store all words and word pairs counts computed from 37 GB of web data in just 2 billion counters (8 GB RAM). The number of these counters is up to 30 times less than the stream size which is a big memory and space gain. In Semantic Orientation experiments, the PMI scores computed from 2 billion counters are as effective as exact PMI scores. 1 Items=word−pairs Items=words Log2 of # of unique Items 25 20 15 10 5 5 10 15 20 Log2 of # of words 25 Figure 1: Token Type Curve Introduction Approaches to solve NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) always benefited from having large amounts of data. In some cases (Turney and Littman, 2002; Patwardhan and Riloff, 2006), researchers attempted to use the evidence gathered from web via search engines to solve the problems. But the commercial search engines limit the number of automatic requests on a daily basis for various reasons such as to avoid fraud and computational overhead. Though we can crawl the data and save it on disk, most of the current approaches employ data structures that reside in main memory and thus do not scale well to huge corpo"
W10-1503,P89-1010,0,0.858152,"that are found in both rankings to the size of top ranked word pairs. 20M 50M 100M 200M 4 3 Accuracy = 2 1 0 0 2 4 6 8 10 12 Log2 of true frequency counts of words/word−pairs Figure 4: Comparing different size models with depth 3 4.3 Evaluating the CU PMI ranking In this experiment, we compare the word pairs association rankings obtained using PMI with CU and exact counts. We use two kinds of measures, namely accuracy and Spearman’s correlation, to measure the overlap in the rankings obtained by both these approaches. 4.3.1 PointWise Mutual Information The Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between two words w1 and w2 is defined as: P M I(w1 , w2 ) = log2 P (w1 , w2 ) P (w1 )P (w2 ) Here, P (w1 , w2 ) is the likelihood that w1 and w2 occur together, and P (w1 ) and P (w2 ) are their independent likelihoods respectively. The ratio between these probabilities measures the degree of statistical dependence between w1 and w2 . 22 |CP-WPs ∩ EP-WPs| |EP-WPs| Where CP-WPs represent the set of top ranked K word pairs under the counts stored using the CU sketch and EP-WPs represent the set of top ranked word pairs with the exact counts. Spearman’s rank correlation coefficient (ρ) computes"
W10-1503,N09-1058,1,0.791988,"Missing"
W10-1503,D09-1079,0,0.119186,"ed the NLP community to use streaming, randomized, approximate and sampling algorithms to handle large amounts of data. A randomized data structure called Bloom filter was used to construct space efficient language models (Talbot and Osborne, 2007) for Statistical Machine Translation (SMT). Recently, the streaming algorithm paradigm has been used to provide memory and space-efficient platform to deal with terabytes of data. For example, We (Goyal et al., 2009) pose language modeling as 18 a problem of finding frequent items in a stream of data and show its effectiveness in SMT. Subsequently, (Levenberg and Osborne, 2009) proposed a randomized language model to efficiently deal with unbounded text streams. In (Van Durme and Lall, 2009b), authors extend Talbot Osborne Morris Bloom (TOMB) (Van Durme and Lall, 2009a) Counter to find the highly ranked k PMI response words given a cue word. The idea of TOMB is similar to CM Sketch. TOMB can also be used to store word pairs and further compute PMI scores. However, we advocate CM Sketch as it is a very simple algorithm with strong guarantees and good properties (see Section 3). 2.2 Sketch Techniques A sketch is a summary data structure that is used to store streaming"
W10-1503,J07-3003,0,0.0211407,"eaming data onto a small-space sketch vector that can be easily updated and queried. It turns out that both updating and querying on this sketch vector requires only a constant time per operation. Streaming algorithms were first developed in the early 80s, but gained in popularity in the late 90s as researchers first realized the challenges of dealing with massive data sets. A good survey of the model and core challenges can be found in (Muthukrishnan, 2005). There has been considerable work on coming up with different sketch techniques (Charikar et al., 2002; Cormode and Muthukrishnan, 2004; Li and Church, 2007). A survey by (Rusu and Dobra, 2007; Cormode and Hadjieleftheriou, 2008) comprehensively reviews the literature. 3 Count-Min Sketch The Count-Min Sketch (Cormode and Muthukrishnan, 2004) is a compact summary data structure used to store the frequencies of all items in the input stream. The sketch allows fundamental queries on the data stream such as point, range and inner product queries to be approximately answered very quickly. It can also be applied to solve the finding frequent items problem (Manku and Motwani, 2002) in a data stream. In this paper, we are only interested in point queries."
W10-1503,W06-0208,0,0.025247,"t 2 billion counters (8 GB RAM). The number of these counters is up to 30 times less than the stream size which is a big memory and space gain. In Semantic Orientation experiments, the PMI scores computed from 2 billion counters are as effective as exact PMI scores. 1 Items=word−pairs Items=words Log2 of # of unique Items 25 20 15 10 5 5 10 15 20 Log2 of # of words 25 Figure 1: Token Type Curve Introduction Approaches to solve NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) always benefited from having large amounts of data. In some cases (Turney and Littman, 2002; Patwardhan and Riloff, 2006), researchers attempted to use the evidence gathered from web via search engines to solve the problems. But the commercial search engines limit the number of automatic requests on a daily basis for various reasons such as to avoid fraud and computational overhead. Though we can crawl the data and save it on disk, most of the current approaches employ data structures that reside in main memory and thus do not scale well to huge corpora. Fig. 1 helps us understand the seriousness of the situation. It plots the number of unique words/word pairs versus the total number of words in a corpus of size"
W10-1503,P05-1077,0,0.424798,"ctor of two. We show that it is possible to store all words and word pairs counts computed from 37 GB of web data in just 2 billion counters (8 GB RAM). The number of these counters is up to 30 times less than the stream size which is a big memory and space gain. In Semantic Orientation experiments, the PMI scores computed from 2 billion counters are as effective as exact PMI scores. 1 Items=word−pairs Items=words Log2 of # of unique Items 25 20 15 10 5 5 10 15 20 Log2 of # of words 25 Figure 1: Token Type Curve Introduction Approaches to solve NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) always benefited from having large amounts of data. In some cases (Turney and Littman, 2002; Patwardhan and Riloff, 2006), researchers attempted to use the evidence gathered from web via search engines to solve the problems. But the commercial search engines limit the number of automatic requests on a daily basis for various reasons such as to avoid fraud and computational overhead. Though we can crawl the data and save it on disk, most of the current approaches employ data structures that reside in main memory and thus do not scale well to huge corpora. Fig. 1 helps us understand the serious"
W10-1503,D07-1049,0,0.0309224,"1.6 Terawords was used by Agirre et al. (2009) to compute pairwise similarities of the words in the test sets using the MapReduce infrastructure on 2, 000 cores. Pantel et al. (2009) computed similarity between 500 million terms in the MapReduce framework over a 200 billion words in 50 hours using 200 quad-core nodes. The inaccessibility of clusters for every one has attracted the NLP community to use streaming, randomized, approximate and sampling algorithms to handle large amounts of data. A randomized data structure called Bloom filter was used to construct space efficient language models (Talbot and Osborne, 2007) for Statistical Machine Translation (SMT). Recently, the streaming algorithm paradigm has been used to provide memory and space-efficient platform to deal with terabytes of data. For example, We (Goyal et al., 2009) pose language modeling as 18 a problem of finding frequent items in a stream of data and show its effectiveness in SMT. Subsequently, (Levenberg and Osborne, 2009) proposed a randomized language model to efficiently deal with unbounded text streams. In (Van Durme and Lall, 2009b), authors extend Talbot Osborne Morris Bloom (TOMB) (Van Durme and Lall, 2009a) Counter to find the hig"
W10-1503,C08-1114,0,0.0738369,"counts by a factor of two. We show that it is possible to store all words and word pairs counts computed from 37 GB of web data in just 2 billion counters (8 GB RAM). The number of these counters is up to 30 times less than the stream size which is a big memory and space gain. In Semantic Orientation experiments, the PMI scores computed from 2 billion counters are as effective as exact PMI scores. 1 Items=word−pairs Items=words Log2 of # of unique Items 25 20 15 10 5 5 10 15 20 Log2 of # of words 25 Figure 1: Token Type Curve Introduction Approaches to solve NLP problems (Brants et al., 2007; Turney, 2008; Ravichandran et al., 2005) always benefited from having large amounts of data. In some cases (Turney and Littman, 2002; Patwardhan and Riloff, 2006), researchers attempted to use the evidence gathered from web via search engines to solve the problems. But the commercial search engines limit the number of automatic requests on a daily basis for various reasons such as to avoid fraud and computational overhead. Though we can crawl the data and save it on disk, most of the current approaches employ data structures that reside in main memory and thus do not scale well to huge corpora. Fig. 1 hel"
W10-1503,D09-1098,0,\N,Missing
W10-2608,W06-1615,0,0.910063,"il, 2004) for multi-task regularization where each task parameter was represented as sum of a mean parameter (that stays same for all tasks) and its deviation from this mean. SVM was used as the base classifier and the algorithm was formulated in the standard SVM dual optimization setting. Subsequently, this framework (Evgeniou and Pontil, 2004) was extended (Dredze et al., 2010) to online multidomain setting. Prior work on semi-supervised approaches to domain adaptation also exists in literature. Extraction of specific features from the available dataset was proposed (Arnold and Cohen, 2008; Blitzer et al., 2006) to facilitate the task of domain adaptation. Co-adaptation (Tur, 2009), a combination of co-training and domain adaptation, can also be considered as a semisupervised approach to domain adaptation. A semi-supervised EM algorithm for domain adaptation was proposed in (Dai et al., 2007). Similar to graph based semi-supervised approaches, a label propagation method was proposed (Xing et al., 2007) to facilitate domain adaptation. The recently proposed Domain Adaptation Machine (DAM) (Duan et al., 2009) is a semi-supervised extension of SVMs for domain adaptation and presents extensive empirical"
W10-2608,P07-1033,1,0.649343,"Missing"
W10-2808,P05-1077,0,0.766494,"al et al., 2010) developed techniques to make the computations feasible on a conventional machines by willing to accept some error in the counts. Similar to that work, this work exploits the idea of Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) to approximate the frequency of word pairs in the corpus without explicitly storing the word pairs themselves. In their, we stored Introduction In many NLP problems, researchers (Brants et al., 2007; Turney, 2008) have shown that having large amounts of data is beneficial. It has also been shown that (Agirre et al., 2009; Pantel et al., 2009; Ravichandran et al., 2005) having large amounts of data helps capturing the semantic similarity between pairs of words. However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task. In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale. The major difficulty in computing pairwise similarities stems from the rapid increase in the number of unique word-context pairs with the size of text corpus (number of tokens). Fig. 1 shows that 1 2 Note that the plot is in log-log sca"
W10-2808,N09-1003,0,0.178412,"Missing"
W10-2808,D07-1090,0,0.0738,"uce infrastructure (with 2, 000 cores) to compute pairwise similarities of words on a corpus of roughly 1.6 Terawords. In a different direction, our earlier work (Goyal et al., 2010) developed techniques to make the computations feasible on a conventional machines by willing to accept some error in the counts. Similar to that work, this work exploits the idea of Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) to approximate the frequency of word pairs in the corpus without explicitly storing the word pairs themselves. In their, we stored Introduction In many NLP problems, researchers (Brants et al., 2007; Turney, 2008) have shown that having large amounts of data is beneficial. It has also been shown that (Agirre et al., 2009; Pantel et al., 2009; Ravichandran et al., 2005) having large amounts of data helps capturing the semantic similarity between pairs of words. However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task. In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale. The major difficulty in computing pairwise similarities stems f"
W10-2808,C08-1114,0,0.0309422,"ith 2, 000 cores) to compute pairwise similarities of words on a corpus of roughly 1.6 Terawords. In a different direction, our earlier work (Goyal et al., 2010) developed techniques to make the computations feasible on a conventional machines by willing to accept some error in the counts. Similar to that work, this work exploits the idea of Count-Min (CM) sketch (Cormode and Muthukrishnan, 2004) to approximate the frequency of word pairs in the corpus without explicitly storing the word pairs themselves. In their, we stored Introduction In many NLP problems, researchers (Brants et al., 2007; Turney, 2008) have shown that having large amounts of data is beneficial. It has also been shown that (Agirre et al., 2009; Pantel et al., 2009; Ravichandran et al., 2005) having large amounts of data helps capturing the semantic similarity between pairs of words. However, computing distributional similarity (Sec. 2.1) between word pairs from large text collections is a computationally expensive task. In this work, we consider scaling distributional similarity methods for computing semantic similarity between words to Web-scale. The major difficulty in computing pairwise similarities stems from the rapid i"
W10-2808,N09-1058,1,0.916074,"Missing"
W10-2808,W10-1503,1,0.397157,"Missing"
W10-2808,D09-1079,0,0.108118,"eaming, and randomized algorithms to handle large amounts of data. Ravichandran et al. (2005) used locality sensitive hash functions for computing word-pair similarities from large text collections. Their approach stores a enormous matrix of all unique words and their contexts in main memory which makes it hard for larger data sets. In our work, we store all unique word-context pairs in CU sketch with a pre-defined size4 . Recently, the streaming algorithm paradigm has been used to provide memory and time-efficient platform to deal with terabytes of data. For example, we (Goyal et al., 2009); Levenberg and Osborne (2009) build approximate language models and show their effectiveness in SMT. In (Van Durme and Lall, 2009b), a TOMB Counter (Van Durme and Lall, 2009a) was used to find the top-K verbs “y” with the highest PMI for a given verb “x”. The idea of TOMB is similar to CU Sketch. However, we use CU Sketch because of its simplicity and attractive properties (see Sec. 3). In this work, we go one step further, and compute semantic similarity between word-pairs using approximate PMI scores from CU sketch. Background 2.1 Distributional Similarity 2.3 Sketch Techniques Distributional Similarity is based on the"
W10-2808,D09-1098,0,\N,Missing
W16-0319,P11-1026,0,\N,Missing
W16-0319,W15-1207,1,\N,Missing
W16-2210,P13-1141,1,0.857525,"Missing"
W16-2210,P07-1005,0,0.108569,"Missing"
W16-2210,N12-1047,0,0.0696872,"Missing"
W16-2210,N09-1025,0,0.0247122,"e X2 , practical X1 X2 i (r2 ) X → h X1 pratique X2 , X1 X2 practice i (r3 ) X → h X1 pratique X2 , X2 X1 process i F1 Une e´ tude de l’ (int´erˆet)X1 pratique (de notre approche)X2 . A study on the (interest)X1 practical (of our approach)X2 . E A study on the practical (interest)X1 (of our approach)X2 . The rule scoring heuristics defined by (Chiang, 2005) do not handle rule selection in a satisfactory way and many authors have come up with solutions. Models that use the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as w"
W16-2210,P10-1146,0,0.0275418,"2 i (r2 ) X → h X1 pratique X2 , X1 X2 practice i (r3 ) X → h X1 pratique X2 , X2 X1 process i F1 Une e´ tude de l’ (int´erˆet)X1 pratique (de notre approche)X2 . A study on the (interest)X1 practical (of our approach)X2 . E A study on the practical (interest)X1 (of our approach)X2 . The rule scoring heuristics defined by (Chiang, 2005) do not handle rule selection in a satisfactory way and many authors have come up with solutions. Models that use the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as well as context"
W16-2210,chrupala-etal-2008-learning,0,0.0702923,"Missing"
W16-2210,P10-2002,0,0.211377,"structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as well as contextual information about the source sentence. This task is modeled as a multiclass classification problem. Because of the very large size of hierarchical grammars, the training procedure for discriminative rule selection models is typically very expensive: multiclass classification is performed over Introduction Hierarchical phrase-based machine translation (Chiang, 2005) performs non-local reordering in a formally syntax-based way. It allows flexible rule extraction an"
W16-2210,C08-1041,0,0.223471,"ns. Models that use the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as well as contextual information about the source sentence. This task is modeled as a multiclass classification problem. Because of the very large size of hierarchical grammars, the training procedure for discriminative rule selection models is typically very expensive: multiclass classification is performed over Introduction Hierarchical phrase-based machine translation (Chiang, 2005) performs non-local reordering in a formally syntax-based way. I"
W16-2210,J96-1002,0,0.335164,"Missing"
W16-2210,D10-1054,0,0.235429,"se the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as well as contextual information about the source sentence. This task is modeled as a multiclass classification problem. Because of the very large size of hierarchical grammars, the training procedure for discriminative rule selection models is typically very expensive: multiclass classification is performed over Introduction Hierarchical phrase-based machine translation (Chiang, 2005) performs non-local reordering in a formally syntax-based way. It allows flexible"
W16-2210,P13-2121,0,0.0440768,"Missing"
W16-2210,2009.iwslt-papers.4,0,0.020779,"er a hierarchical baseline while our global model with exhaustive training yields significant improvements on scientific and medical texts (see Section 4). In a second contribution, we successfully scale rule selection models to large scale translation tasks but fail to produce significant improvements in BLEU over a hierarchical baseline on these tasks. Because our approach needs scaling to a large amount of training examples, we need a classifier that is fast and supports online streaming. We use the high-speed classifier Vowpal Wabbit2 (VW) which we fully integrate in the syntax component (Hoang et al., 2009) of the Moses machine translation toolkit (Koehn et al., 2007). To allow researchers to replicate our results and improve on our work, we make our implementation publicly available as part of Moses. 2 Global Rule Selection Model The goal of rule selection is to choose the correct target side of a hierarchical rule, given a source side as well as other sources of information such as the shape of the rule or its context of application in the source sentence. The latter includes lexical features (e.g. the words surrounding the source span of an applied rule) or syntactic features (e.g. the positi"
W16-2210,D07-1007,0,0.0402698,"udy on the practical (interest)X1 (of our approach)X2 . The rule scoring heuristics defined by (Chiang, 2005) do not handle rule selection in a satisfactory way and many authors have come up with solutions. Models that use the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as well as contextual information about the source sentence. This task is modeled as a multiclass classification problem. Because of the very large size of hierarchical grammars, the training procedure for discriminative rule selection models is typically"
W16-2210,D07-1103,0,0.0635156,"Missing"
W16-2210,P07-2045,0,0.00618631,"ive training yields significant improvements on scientific and medical texts (see Section 4). In a second contribution, we successfully scale rule selection models to large scale translation tasks but fail to produce significant improvements in BLEU over a hierarchical baseline on these tasks. Because our approach needs scaling to a large amount of training examples, we need a classifier that is fast and supports online streaming. We use the high-speed classifier Vowpal Wabbit2 (VW) which we fully integrate in the syntax component (Hoang et al., 2009) of the Moses machine translation toolkit (Koehn et al., 2007). To allow researchers to replicate our results and improve on our work, we make our implementation publicly available as part of Moses. 2 Global Rule Selection Model The goal of rule selection is to choose the correct target side of a hierarchical rule, given a source side as well as other sources of information such as the shape of the rule or its context of application in the source sentence. The latter includes lexical features (e.g. the words surrounding the source span of an applied rule) or syntactic features (e.g. the position of an applied rule in the source parse tree). The rule sele"
W16-2210,P13-1111,0,0.0299038,"Missing"
W16-2210,W04-3250,0,0.233709,"Missing"
W16-2210,D08-1010,0,0.0427009,"Missing"
W16-2210,2011.mtsummit-papers.28,0,0.0360245,"h X1 pratique X2 , X1 X2 practice i (r3 ) X → h X1 pratique X2 , X2 X1 process i F1 Une e´ tude de l’ (int´erˆet)X1 pratique (de notre approche)X2 . A study on the (interest)X1 practical (of our approach)X2 . E A study on the practical (interest)X1 (of our approach)X2 . The rule scoring heuristics defined by (Chiang, 2005) do not handle rule selection in a satisfactory way and many authors have come up with solutions. Models that use the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target side of a rule given its source side as well as contextual information abo"
W16-2210,P08-1114,0,0.035715,"BLEU on these tasks. 1 (r1 ) X → h X1 pratique X2 , practical X1 X2 i (r2 ) X → h X1 pratique X2 , X1 X2 practice i (r3 ) X → h X1 pratique X2 , X2 X1 process i F1 Une e´ tude de l’ (int´erˆet)X1 pratique (de notre approche)X2 . A study on the (interest)X1 practical (of our approach)X2 . E A study on the practical (interest)X1 (of our approach)X2 . The rule scoring heuristics defined by (Chiang, 2005) do not handle rule selection in a satisfactory way and many authors have come up with solutions. Models that use the syntactic structure of the source and target sentence have been proposed by (Marton and Resnik, 2008; Marton et al., 2012; Chiang et al., 2009; Chiang, 2010; Liu et al., 2011). These approaches exclusively take into account syntactic structure and do not model rule selection (see Section 6 for a detailed discussion). Following the work on phrase-sense disambiguation by (Carpuat and Wu, 2007), other authors improve rule selection by defining features on the structure of hierarchical rules and combining these with information about the source sentence (Chan et al., 2007; He et al., 2008; He et al., 2010; Cui et al., 2010). In these approaches, rule selection is the task of selecting the target"
W16-2210,P02-1040,0,0.0953472,"Missing"
W16-2210,P06-1055,0,0.0763415,"Missing"
W17-2315,W13-2004,0,0.0600022,"Missing"
W17-2315,W09-1404,0,0.0199083,"rly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs"
W17-2315,W13-2003,0,0.0848906,"Missing"
W17-2315,W11-1827,0,0.0615738,"Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs in the AMR graph. AMR has bee"
W17-2315,W13-2014,0,0.103813,"Missing"
W17-2315,W09-1401,0,0.669066,"ntifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community has been working towards the goal of creating a curated knowledge base of biomolecule entity interactions. The scientific literature in the biomedical domain runs to millions of articles and is an excellent source of such information. However, automatically extracting information from text is a challenge because natural language allows us to express the same information in several different ways. The series of Genia Event Extraction shared tasks (Kim et al., 2009, 2011, 2013, 2016) has resulted in various significant approaches to biomolecule event extraction spanning methods that use learnt patterns from annotated text (Bui et al., 2013) to machine learning methods (Bj¨orne and Salakoski, 2013) that use syntactic parses as features. In this work, we find that a semantic analysis of text that relies on Abstract Meaning Representations (Banarescu et al., 2013) is highly useful because it normalizes many lexical and syntactic variations in text. 1 E1 = phosphorylation of radixin; E2 = LPA induces E1. We hypothesize that an event structure is a subThis d"
W17-2315,W16-3003,0,0.0251471,"Missing"
W17-2315,W09-1407,0,0.0421696,"relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point an"
W17-2315,W11-1802,0,0.209454,"fying the relation between them. For e.g. in figure 1 the path {‘induce-01’, ‘arg0’, ‘LPA’} suggests that LPA is the cause of induce. We encode this path using word embeddings pre-trained on millions of biomedical text and develop two pipelined neural network models: (a) to identify the theme of an interaction; and (b) to identify the cause of the interaction, if there exists one. 2 2.1 AMR based event extraction model Task description The biomedical event extraction task in this work is adopted from the Genia Event Extraction subtask of the well-known BioNLP shared task ((Kim et al., 2009), (Kim et al., 2011), (Kim et al., 2013)). Table 2 shows a sample event annotation for the sentence in Figure 1. The protein annotations T1- T4 are given as starting points. The task is to identify the events E1-E4 with their interaction type and arguments. Table 1 describes the various event types and the arguments they accept. The first four event types require only unary theme argument. The binding event can take a variable number of theme arguments. The last four events take a theme argument and, when expressed, also a cause argument. Their theme or cause may in turn be another event, creating a nested event"
W17-2315,P14-1134,0,0.022913,"1 Figure 1: AMR with sample event annotations for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several"
W17-2315,W13-2010,0,0.0148371,"column corresponds to the results of EVEX (Hakala et al., 2013) model on the 2013 test set. Certain notable numbers are emphasized and discussed under results 5.4. limited annotated data. Distant supervision techniques have been successfully used before for relation extraction (Mintz et al., 2009) in general domain. Recent work by (Liu et al., 2014) uses minimal supervision strategy for extracting relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (B"
W17-2315,W13-2005,0,0.0151187,"to the results of EVEX (Hakala et al., 2013) model on the 2013 test set. Certain notable numbers are emphasized and discussed under results 5.4. limited annotated data. Distant supervision techniques have been successfully used before for relation extraction (Mintz et al., 2009) in general domain. Recent work by (Liu et al., 2014) uses minimal supervision strategy for extracting relations particularly in biomedical texts. Our work departs from theirs in that we introduce a novel AMR path based heuristic to selectively sample the sentences obtained from distant supervision. (Liu et al., 2013; MacKinlay et al., 2013) learn subgraph patterns from the event annotations in the training data and cast the event detection as subgraph matching problem. Non-feature based approaches like graph kernels compare syntactic structures directly (Airola et al., 2008; Bunescu et al., 2005). Rule based methods that either use manually crafted rules or generate rules from training data (Cohen et al., 2009; Kaljurand et al., 2009; Kilicoglu and Bergler, 2011; Bui et al., 2013) have obtained high precision on these tasks. In our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 201"
W17-2315,S16-1166,0,0.0218552,"This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community has been worki"
W17-2315,P12-1076,0,0.0200641,"e may in turn be another event, creating a nested event (For e.g. event E2 in Table 2). Experimental results show that our model, although achieves a reasonable precision, suffers from low recall. Our third contribution is a distant supervision (Mintz et al., 2009) based approach to collect additional annotated training data. Distant supervision works on the assumption that given a known relation between two entities, a sentence containing the two entities is likely to express this relation and hence can serve as training data for that relation. Data gathered using such a method can be noisy (Takamatsu et al., 2012). Roth et al. (2013) have discussed several prior work that address this issue. In our work, we introduce a method based on AMR path heuristic 2.2 Model description We cast this event extraction problem as a subgraph identification problem. Given a sentence we 127 Event Type Gene expression Transcription Localization Protein catabolism ==[SVT-TOTAL]== Binding Phosphorylation Regulation Positive regulation Negative regulation ==[REG-TOTAL]== ==[ALL-TOTAL]== first obtain its AMR graph automatically using an AMR parser (Pust et al., 2015). Next, we identify protein nodes and interaction nodes in"
W17-2315,N15-1040,0,0.0124831,"ample event annotations for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biom"
W17-2315,P09-1113,0,0.228838,"1-E4 with their interaction type and arguments. Table 1 describes the various event types and the arguments they accept. The first four event types require only unary theme argument. The binding event can take a variable number of theme arguments. The last four events take a theme argument and, when expressed, also a cause argument. Their theme or cause may in turn be another event, creating a nested event (For e.g. event E2 in Table 2). Experimental results show that our model, although achieves a reasonable precision, suffers from low recall. Our third contribution is a distant supervision (Mintz et al., 2009) based approach to collect additional annotated training data. Distant supervision works on the assumption that given a known relation between two entities, a sentence containing the two entities is likely to express this relation and hence can serve as training data for that relation. Data gathered using such a method can be noisy (Takamatsu et al., 2012). Roth et al. (2013) have discussed several prior work that address this issue. In our work, we introduce a method based on AMR path heuristic 2.2 Model description We cast this event extraction problem as a subgraph identification problem. G"
W17-2315,N15-1119,1,0.71907,"our work, we take inspiration from the Turk Event Extraction System (TEES) (Bj¨orne and Salakoski, 2013) (the event extraction system for EVEX) that has consistently been the top performer in these series of tasks. They represent events using a graph format and break the event extraction task into separate multi-class classification tasks using SVM as their classifier. In our work we take a step further by making use of a deeper semantic representation as a starting point and identifying subgraphs in the AMR graph. AMR has been successfully used for deeper semantic tasks like entity linking (Pan et al., 2015) and abstractive summarization (Mihalcea et al., 2015). Work by Garg et al. (2015) is the first one to make use of AMR representation for extracting interactions from biomedical text. They use graph kernel methods to answer the binary question of whether a given AMR subgraph expresses an interaction or not. Our work departs from theirs in that they concentrate only on binary interactions whereas we use AMR to identify complex nested events. Also, our approach additionally makes use of distant supervision to cope with the problem of 7 Conclusion In this work, we show the effectiveness of using"
W17-2315,D15-1136,1,0.798412,"ions for sentence “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho” AMR is a rooted, directed acyclic graph (DAG) that captures the notion of who did what to whom in text, in a way that sentences that have the same basic meaning often have the same AMR. The nodes in the graph (also called concepts) map to words in the sentence and the edges map to relations between the words. In the recent past, there have been several efforts towards parsing a sentence into its AMR (Flanigan et al., 2014; Wang et al., 2015; Pust et al., 2015; May, 2016). AMR naturally captures hierarchical relations between entities in text making it favorable for complex event detection. For example, consider the following sentence from the biomedical literature: “This LPA-induced rapid phosphorylation of radixin was significantly suppressed in the presence of C3 toxin, a potent inhibitor of Rho”. Figure 1 shows its Abstract Meaning Representation (AMR). The subgraph rooted at phosphorylate-01 identifies the event E1 and the subgraph rooted at induce-01 identifies the event E2 where Introduction For several years now, the biomedical community ha"
W17-2315,W11-1807,0,0.0121183,"th between pk and pi via tj ; and the label is 1 if pk is the cause of the event em , 0 otherwise. 5.3 Results and Discussion 6 Related work The biomedical event extraction task described in this work was first introduced in the BioNLP Shared Task in 2009 (Kim et al., 2009). This task helped shift the focus of relation extraction efforts from identifying simple binary interactions to identifying complex nested events that better represent the biological interactions stated frequently in text. Existing approaches to this task include SVM (Bj¨orne and Salakoski, 2013) other ML based approaches (Riedel and McCallum, 2011; Miwa et al., 2010, 2012). Methods like LSTM model setup We implement our LSTM model using the lasagne library. For the first LSTM model, we use softmax as our non-linear function and optimize the cat132 Event Type Gene expression Transcription Localization Protein catabolism ==[SVT-TOTAL]== Binding Phosphorylation Regulation Positive regulation Negative regulation ==[REG-TOTAL]== ==[ALL-TOTAL]== Recall 66.33 55.10 36.55 73.33 57.82 27.61 49.21 16.30 25.98 23.17 21.81 44.42 LSTM Precision 66.55 28.57 63.72 84.62 60.86 25.94 53.75 29.18 35.16 30.50 31.61 51.01 F1 66.44 37.63 46.45 78.57 57.27"
W17-2315,W13-2002,0,\N,Missing
W17-2315,W13-2322,1,\N,Missing
W17-4304,Q13-1033,0,0.0222748,"uation. Performance was measured by F-score over predicted noun phrases (for which one has to predict the entire noun phrase correctly to get any points). Dependency Parsing is a syntactic analysis task, in which each word in a sentence gets assigned a grammatical head (or “parent”). The experimental setup is similar to part-of-speech tagging. We train an arc-eager dependency parser (Nivre, 2003), which chooses among (at most) four actions at each state: Shift, Reduce, Left or Right. As in part of speech tagging, the reference policy is trained on the TweetNLP dataset (using an oracle due to (Goldberg and Nivre, 2013)), and evaluated on the Penn Treebank corpus (again, sections 02 − 21 and section 23). The loss is unlabeled attachment score (UAS), which measures the fraction of words that pick the correct parent. Thompson Sampling estimates the following elements: a set Θ of parameters µ; a prior distribution P (µ) on these parameters; past observations D consisting of observed contexts and rewards; a likelihood function P (r|b, µ), which gives the probability of reward given a context b and a parameter µ; In each round, Thompson Sampling selects an action according to its posterior probability of having t"
W17-4304,P04-1015,0,0.0747372,"on problems, which essentially train a policy to make sequence of decisions that are stitched together into a final structured output. Such algorithms decompose a joint prediction task into a sequence of action predictions, such as predicting the label of the next word in sequence labeling or predicting a shift/reduce action in dependency parsing3 ; these predictions are tied by features and/or internal state. Algorithms in this family have recently met with success in neural networks (Bengio et al., 2015; Wiseman and Rush, 2016), though date back to models typically based on linear policies (Collins and Roark, 2004; Daum´e III and Marcu, 2005; Xu et al., 2007; Daum´e III et al., 2009; Ross et al., 2010; Ross and Bagnell, 2014; Doppa et al., 2014; Chang et al., 2015). Most learning to search algorithms operate by considering a search space like that shown in Figure 2. The learning algorithm first rolls-in for a few steps using a roll-in policy π in to some state R, then considers all possible actions available at state R, and then rolls out using a roll-out policy π out until the end of the sequence. In the fully supervised case, the learning algorithm can then compute a loss for all possible outputs, an"
W17-4304,J93-2004,0,0.0596682,"sary exploration. 3.1 Tasks, Policy Classes and Data Sets We experiment with the following three tasks. For each, we briefly define the problem, describe the policy class that we use for solving that problem in a learning to search framework (we adopt a similar setting to that of (Chang et al., 2016), who describes the policies in more detail), and describe the data sets that we use. The regression problems are solved using squared error regression, and the classification problems (policy learning) is solved via cost-sensitive one-against-all. Part-Of-Speech Tagging over the 45 Penn Treebank (Marcus et al., 1993) tags. To simulate a domain adaptation setting, we train a reference policy on the TweetNLP dataset (Owoputi et al., 2013), which achieves good accuracy in do3.2 Main Results Here, we describe experimental results (Table 1) comparing several algorithms: (line B) The bandit variant of the LOLS algorithm, which uses importance sampling and -greedy exploration; (lines C-F) BLS, with bandit feedback and perword error correction, with variance reduction and four exploration strategies: -greedy, Boltzmann, Thompson, and “oracle” exploration in which case the oracle action is always chosen during e"
W17-4304,W03-3017,0,0.0782614,"ecognition. We used the CoNLL-2000 datasetfor training and testing. We used the smaller test split (2, 012 sentences) for training a reference policy, and used the training split (8, 500 sentences) for online evaluation. Performance was measured by F-score over predicted noun phrases (for which one has to predict the entire noun phrase correctly to get any points). Dependency Parsing is a syntactic analysis task, in which each word in a sentence gets assigned a grammatical head (or “parent”). The experimental setup is similar to part-of-speech tagging. We train an arc-eager dependency parser (Nivre, 2003), which chooses among (at most) four actions at each state: Shift, Reduce, Left or Right. As in part of speech tagging, the reference policy is trained on the TweetNLP dataset (using an oracle due to (Goldberg and Nivre, 2013)), and evaluated on the Penn Treebank corpus (again, sections 02 − 21 and section 23). The loss is unlabeled attachment score (UAS), which measures the fraction of words that pick the correct parent. Thompson Sampling estimates the following elements: a set Θ of parameters µ; a prior distribution P (µ) on these parameters; past observations D consisting of observed contex"
W17-4304,N13-1039,0,0.0316063,"Missing"
W17-4304,W09-1119,0,0.0469475,"of domain. We simulate bandit feedback over the entire Penn Treebank Wall Street Journal (sections 02–21 and 23), comprising 42k sentences and about one million words. (Adapting from tweets to WSJ is nonstandard; we do it here because we need a large dataset on which to simulate bandit feedback.) The measure of performance is average per-word accuracy (one minus Hamming loss). Noun Phrase Chunking is a sequence segmentation task, in which sentences are divided into base noun phrases.We solve this problem using a sequence span identification predictor based on Begin-In-Out encoding, following (Ratinov and Roth, 2009), though applied to chunking rather than named-entity recognition. We used the CoNLL-2000 datasetfor training and testing. We used the smaller test split (2, 012 sentences) for training a reference policy, and used the training split (8, 500 sentences) for online evaluation. Performance was measured by F-score over predicted noun phrases (for which one has to predict the entire noun phrase correctly to get any points). Dependency Parsing is a syntactic analysis task, in which each word in a sentence gets assigned a grammatical head (or “parent”). The experimental setup is similar to part-of-sp"
W17-4304,P16-1152,0,0.0280733,"ved performance comes at a high labeling cost: a human has to provide exact labels for each decision, not just binary “yes/no” labels. 4 Discussion & Conclusion The most similar algorithm to ours is the bandit version of LOLS (Chang et al., 2015) (which is analyzed theoretically but not empirically); the key differences between BLS and LOLS are: (a) BLS employs a doubly-robust estimator for “guessing” the costs of counterfactual actions; (b) BLS employs alternative exploration strategies; (c) BLS is effective in practice at improving the performance of an initial policy. In the NLP community, Sokolov et al. (2016a) and Sokolov et al. (2016b) have proposed a policy gradient-like method for optimizing log-linear models like conditional random fields (Lafferty et al., 2001) under bandit feedback. Their evaluation is most impressive on the problem of domain adaptation of a machine translation system, in which they show that their approach is able to learn solely from bandit-style feedback, though requiring a large number of samples. In the learning-to-search setting, the difference between structured prediction under bandit feedback and reinforcement learning gets blurry. A distinction in the problem defi"
W17-4304,D16-1137,0,0.0604145,"Missing"
W17-4778,W11-2123,0,0.0550158,"Missing"
W17-4778,P17-4012,0,0.0997538,"we focused on the task of bandit machine translation. This shared task was set up, consistent with (Kreutzer et al., 2017), simultaneously as a bandit learning problem and a domain adaptation problem. This raises the natural question: can we combine these potentially complementary information sources? To investigate this question, we started from a standard neural machine translation (NMT) setup §21 , and then we: 1. applied domain adaptation techniques by data selection (Moore and Lewis, 2010) to the outof-domain data, with the goals of filtering out 1 Our implementation is based on OpenNMT (Klein et al., 2017), an open-source toolkit for neural MT. 667 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 667–673 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Because this task is—at it’s core—a domain adaptation problem (for which a bandit learning signal is available to “help”), we also explored the use of standard domain adaptation techniques. We make a strong assumption that a sizable amount of monolingual, source language data is available before bandit feedback begins.2 We believe that in many realistic setti"
W17-4778,P17-1138,0,0.0293729,"ranslations, or even implicit feedback, for example, the conversation partner asking for clarifications. There has recently been a flurry of work specifically addressing the bandit structured prediction problem (Chang et al., 2015; Sokolov et al., 2016a,b), of which machine translation is a special case. Introduction We describe the University of Maryland systems for bandit machine translation. For the shared translation task of the EMNLP 2017’s second conference on machine translation (WMT17), we focused on the task of bandit machine translation. This shared task was set up, consistent with (Kreutzer et al., 2017), simultaneously as a bandit learning problem and a domain adaptation problem. This raises the natural question: can we combine these potentially complementary information sources? To investigate this question, we started from a standard neural machine translation (NMT) setup §21 , and then we: 1. applied domain adaptation techniques by data selection (Moore and Lewis, 2010) to the outof-domain data, with the goals of filtering out 1 Our implementation is based on OpenNMT (Klein et al., 2017), an open-source toolkit for neural MT. 667 Proceedings of the Conference on Machine Translation (WMT),"
W17-4778,D15-1166,0,0.105043,"Missing"
W17-4778,P10-2041,0,0.303416,"machine translation. For the shared translation task of the EMNLP 2017’s second conference on machine translation (WMT17), we focused on the task of bandit machine translation. This shared task was set up, consistent with (Kreutzer et al., 2017), simultaneously as a bandit learning problem and a domain adaptation problem. This raises the natural question: can we combine these potentially complementary information sources? To investigate this question, we started from a standard neural machine translation (NMT) setup §21 , and then we: 1. applied domain adaptation techniques by data selection (Moore and Lewis, 2010) to the outof-domain data, with the goals of filtering out 1 Our implementation is based on OpenNMT (Klein et al., 2017), an open-source toolkit for neural MT. 667 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 667–673 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Because this task is—at it’s core—a domain adaptation problem (for which a bandit learning signal is available to “help”), we also explored the use of standard domain adaptation techniques. We make a strong assumption that a sizable amount o"
W17-4778,P16-1152,0,0.0195178,"02; Langford and Zhang, 2008; Beygelzimer et al., 2010; Dudik et al., 2011). For example, a neural translation system trained on parliament proceedings often performs quite poorly at translating anything else. However, a translation system that is deployed to facilitate conversations between users might receive either explicit feedback (e.g. thumbs up/down) on its translations, or even implicit feedback, for example, the conversation partner asking for clarifications. There has recently been a flurry of work specifically addressing the bandit structured prediction problem (Chang et al., 2015; Sokolov et al., 2016a,b), of which machine translation is a special case. Introduction We describe the University of Maryland systems for bandit machine translation. For the shared translation task of the EMNLP 2017’s second conference on machine translation (WMT17), we focused on the task of bandit machine translation. This shared task was set up, consistent with (Kreutzer et al., 2017), simultaneously as a bandit learning problem and a domain adaptation problem. This raises the natural question: can we combine these potentially complementary information sources? To investigate this question, we started from a s"
W17-4778,P16-1162,0,\N,Missing
W17-5401,W17-5404,0,0.0118065,"tate University The breaker team from OSU also used a variety of strategies, classified as morphosyntactic, semantic, pragmatic, and world knowledge-based changes, to target hypothesized weaknesses in the sentiment analysis systems (Mahler et al., 2017). University of Melbourne, CNNs The builder team from University of Melbourne (which also participated as a breaker team), contributed two sentiment analysis systems consisting of convolutional neural networks. One CNN was trained on data labeled at the phrase level (PCNN), and the other was trained on data labeled at the sentence level (SCNN) (Li et al., 2017). University of Melbourne The breaker team from University of Melbourne opted to generate test minimal pairs automatically, borrowing from methods for generating adversarial examples in computer vision. They used reinforcement learning, optimizing on reversed labels, to identify tokens or phrases to be changed, and then applied a substitution method (Li et al., 2017). Some human supervision was used to ensure grammaticality and correct labeling of the sentences. Recursive Neural Tensor Network To supplement our submitted builder systems, we tested several additional sentiment analysis systems"
W17-5401,W17-5405,0,0.0607234,"Missing"
W17-5401,D11-1037,1,0.842048,"terested in the scientific questions addressed by testing a model’s ability to handle less frequent phenomena, it should be noted that any NLP system that is released is likely to be adversarially tested by users who want to break it for fun. This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to stimulate additional di"
W17-5401,marelli-etal-2014-sick,0,0.0196372,"tific questions addressed by testing a model’s ability to handle less frequent phenomena, it should be noted that any NLP system that is released is likely to be adversarially tested by users who want to break it for fun. This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to stimulate additional discussion. 3 Shared Task"
W17-5401,P05-1015,0,0.0378583,"h to draw items that could then be altered to create minimal pairs. Sentiment breakers were provided an additional set of 500 sentiment sentences, collected and annotated by the same method as that used for the 500 blind dev sentences for sentiment. QA-SRL breakers were provided an additional set of 814 items, collected and annotated by the same method as the blind dev items for QA-SRL. Shared Task Data 4.1 Blind Development Data Training Data For the sentiment training data, we used the Sentiment Treebank dataset from Socher et al. (2013), developed from the Rotten Tomatoes review dataset of Pang and Lee (2005).5 Each sentence in the dataset has a sentiment value between 0 and 1, as well as sentiment values for the phrases in its syntactic parse. In order to establish a binary labeling scheme at the sentence level, we mapped sentences in range (0, 0.4) to “negative” and sentences in range (0.6, 1.0) to “positive”. Neutral sentences—those with a sentiment value between 0.4 and 0.6—were removed. The sentiment training data had a total of 6921 sentences and 166738 phrases. Phrase-level sentiment labels were made available to participants as an optional resource. 4.4 Test Data The test data for evaluati"
W17-5401,de-marneffe-etal-2006-generating,0,0.0317309,"Missing"
W17-5401,D09-1085,0,0.0332705,". Even if one is uninterested in the scientific questions addressed by testing a model’s ability to handle less frequent phenomena, it should be noted that any NLP system that is released is likely to be adversarially tested by users who want to break it for fun. This state of affairs has not gone unnoticed. On the one hand, there is work on creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to st"
W17-5401,W16-2524,1,0.872675,"n creating targeted evaluation datasets that exhibit and are annotated for particular linguistic phenomena, in order to facilitate fine-grained analysis of the linguistic capacities of systems for tasks such as parsing, entailment, and semantic relatedness (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). Additionally, there is an increasing amount of work on developing methods of exposing exactly what linguistic knowledge NLP models develop (K´ad´ar et al., 2016; Li et al., 2015) and what linguistic information is encoded in models’ produced representations (Adi et al., 2016; Ettinger et al., 2016). Our aim in organizing this workshop was to build on this foundation, designing the shared task to generate data specifically created to identify the boundaries of systems’ linguistic capacities, and welcoming further related research contributions to stimulate additional discussion. 3 Shared Task: Build It Break It, The Language Edition To address the issues identified above, we developed a shared task inspired by the Build It Break It Fix It Contest2 and adapted for application to NLP. The shared task proceeded in three phases: a building phase, a breaking phase, and a scoring phase: 1. In"
W17-5401,D12-1110,0,0.00934307,"Tensor Network To supplement our submitted builder systems, we tested several additional sentiment analysis systems on the breaker test set. The first of these was the Stanford Recursive Neural Tensor Network (RNTN) (Socher et al., 2013). This model is a recursive neural network-based sentiment classifier, composing words and phrases of input sentences based on binary branching syntactic structure, and using the composed representations as input features to softmax classifiers at every syntactic node. This model, rather than parameterizing the composition function by the words being composed (Socher et al., 2012), uses a single more powerful tensor-based composition function for composing each node of the syntactic tree. Team 4 The fourth sentiment breaker team did not submit a description paper, but the results from this team’s test set are reported below. 5.3 Builder Team: QA-SRL The organizers provided a QA-SRL system, as there were no external builder submissions for this task. The provided system was a logistic regression classifier, trained with 1-through-5 skipgrams with a maximum skip of 4. Potential answers were neighbors and neighbors-of-neighbors in a dependency parse of the sentence (Stanf"
W17-5401,D15-1076,0,0.217943,"ilders, we had a number of considerations. The task should be one that requires strong linguistic capabilities, so that in identifying the boundaries of the systems, breakers are encouraged to target linguistic phenomena key to increasing the robustness of language understanding. Additionally, we want the task to be without significant barrier to entry, to encourage builder participation. In the interest of balancing these considerations and testing the effectiveness of different tasks, we ran two tasks in parallel: sentiment analysis and question-answer driven semantic role labeling (QA-SRL; He et al., 2015). The sentiment task consists of standard sentiment analysis performed on movie reviews. In the QA-SRL task, the input is a sentence and a question related to one of the predicates in the sentence, and the output is a span of the sentence that answers the question. See Figure 1 for an example item. The task allows for testing semantic role labeling without the need for a pre-defined set of roles, or for annotators with significant training or linguistic expertise. 3.2 Breaking Satisfaction of requirement 1 is what we will refer to as “breaking” a system (note that this also applies if the syst"
W17-5401,D13-1170,0,0.0532153,"ropriate, the answer), leaving the question unaltered. For instance, breakers could generate the following item to be paired with the example in Figure 1: (2) 4.2 Blind dev data was provided for builders to submit initial predictions on, as produced by their systems. These predictions were made available for breakers, to be used as a reference when creating test minimal pairs. For sentiment, we collected an additional 500 sentences from a pool of Rotten Tomatoes reviews for movies in the years 20032005. For annotations, we used the same method of annotation via crowd-sourcing that was used by Socher et al. (2013). For QA-SRL, we extracted a set of 814 sentences from Wikipedia and annotated these by crowd-sourcing, following the method of He et al. (2015). Sent0 UCD finished the 2006 championship as Dublin champions, when they beat St Vincents in the final. 0 Ans UCD (unchanged) We might anticipate that the system would now predict the pronoun they as the answer to the question, without resolving to UCD.4 The sets of minimal pairs created by the breakers then constituted the test set of the shared task, which was sent to builders to generate predictions on for scoring. 4 4.3 Starter Data for Breakers A"
W17-5401,P14-1062,0,0.0169686,"Missing"
W17-5401,W17-5410,0,0.0911016,"Missing"
W18-0603,W16-0311,0,0.1294,"Missing"
W18-0603,W17-1612,0,0.0615839,"ne is that we have so far limited ourselves to Reddit, which may have particular characteristics that fail to generalize; in particular, evidence suggests that users show different behavior when posting anonymously, with both positive and negative implications (Christopherson, 2007; De Choudhury and De, 2014). A second limitation is that, without health records, outcomes, or even self-report questionnaires from the users whose postings were asDataset Availability and Ethical Considerations The research we report was approved by the University of Maryland’s Institutional Review Board (IRB). As Benton et al. (2017) discuss, human subjects research using previously existing data falls into a category exempted from the requirement of full IRB review as long as the data are either from publicly available sources or they do not provide a way to recover the identity of the subjects. In our case, the data are publicly available and from a site where users are anonymous. As an extra precaution we replace Reddit usernames with numeric identifiers. Benton et al. (2017) point out that even exempt research needs to be reviewed by an IRB to make an exemption determination. In addition, they discuss the importance o"
W18-0603,S17-2093,0,0.0173585,"Portability and Accountability Act, or HIPAA. Resnik (2017) has argued that, owing to the fact that the law was written without anticipating the importance of large scale, community-wide research datasets, the state of the art in clinical natural language processing is significantly behind the state of the art in other domains. For example, the widely used Enron email corpus contains 1.2 million emails (Klimt and Yang, 2004); in contrast, the SemEval-2017 Clinical TempEval shared task used 400 manually de-identified clinical notes and pathology reports from cancer patients at the Mayo Clinic (Bethard et al., 2017). 3 https://www.reddit.com/r/datasets/ 26 3.1 In order to determine user-level risk, we consider a user to have the highest risk associated with any of their annotation units. We defined a four-way categorization of risk adapting Corbitt-Hall et al. (2016) (who provided lay definitions based on risk categories in Joiner et al. (1999)): (a) No Risk (or “None”): I don’t see evidence that this person is at risk for suicide; (b) Low Risk: There may be some factors here that could suggest risk, but I don’t really think this person is at much of a risk of suicide; (c) Moderate Risk: I see indication"
W18-0603,P16-1191,0,0.0146585,"tional Neural Networks Assessment Classifier We conduct screening experiments looking at evidence within t days before the “signal” (i.e. the first SuicideWatch post), where t could be 1, 2, 5, or 7 days. For control users, a random post is chosen as the point from which t is determined. A user’s relevant posts for screening, from which the In additional to our baseline classifier for the assessment task, we explored using a convolutional neural network (CNN), since CNNs are effective in many NLP tasks, especially text classification problems like sentence-level sentiment analysis (Kim, 2014; Flekova and Gurevych, 2016). We adopt a similar CNN architecture to the one introduced in Kim (2014) due to its popularity and ease of scalability to multiple tasks and strong results on many datasets. Figure 1 depicts the structure of our CNN architecture, where the input of the network is the concatenation of all user’s posts and 17 We also experimented with logistic regression and XGBoost, with substantially inferior results. 18 For these experiments logistic regression and XGBoost had performance very similar to SVM. 4.4 Screening 30 comprehensive discussion of crowdsourcing, using CrowdFlower, as a means for obtain"
W18-0603,W14-3207,0,0.55779,"(Reddit, 2018). Since users might have chosen to include potentially identifying information in their usernames, we go a step further and replace usernames with unique numeric identifiers.6 We discuss privacy and other issues further in Section 6. based on the same detailed instructions. We evaluated levels of inter-rater agreement within and across groups and also looked at differences between groups. In addition, we present initial automatic risk-level classification and screening results for SuicideWatch data using machine learning. 2 Dataset Our approach to data collection is inspired by Coppersmith et al. (2014), who introduced an innovative way to solve for the absence of clinical ground truth when studying mental health in social media. Their approach is to identify users who have produced an overt signal, in social media, indicating they might be a positive instance of the relevant condition, and then manually assessing the signal to filter out candidates for which the signal does not appear genuine. They applied this on Twitter by seeking variations of the statement I have been diagnosed with X, (where X is depression, PTSD, or other conditions), and then manually filtering tweets for which the s"
W18-0603,W14-3213,0,0.0395626,"ing that there is clear value in the crowdsourced annotations. Table 4 shows a confusion matrix measuring crowdsourcers’ consensus against the all-experts consensus, and it appears that most of the errors involve erring on the side of caution, misclassifying more than half of the low-risk users as having higher risk, and misclassifying a large number of All Experts Crowdflower None Low Moderate Severe None Low Moderate Severe 29 11 6 1 1 13 11 1 1 20 47 8 5 6 51 34 Table 4: All Experts vs. Crowdsourcers 12 Performance differences between experts and nonexperts require more study. For example, Homan et al. (2014) found that two novice annotators were more likely to assign their expert’s “low distress” tweets to the “no distress” category. Conversely, on a related but coarser-grained categorization task, Liu et al. (2017) find “some evidence that multiple crowdsourcing workers, when they reach high inter-annotator agreement, can provide reliable quality of annotations”. Short Experts 10 We conjecture that, with fewer jobs left available, annotators were less inclined to go through the detailed instructions and test because there was less for them to get paid for. 11 See Confidence Score https://success"
W18-0603,Q14-1025,0,0.0200369,"eliability cutoff for chance-corrected agreement (> 0.8, Krippendorff (2004)), which is to our knowledge the first result demonstrating inter-rater reliability by clinical experts for suicide risk based on social media postings. Inter-rater reliability for the pair receiving short instructions was substantially lower (0.768), demonstrating the value of our detailed rubric based on explicitly identified risk factors. We generated consensus user-level labels based on the expert annotations using a well known model for inferring true labels from multiple noisy annotations (Dawid and Skene, 1979; Passonneau and Carpenter, 2014), including consensus for the pairs receiving long instructions (Long Experts), short instructions (Short Experts), and consensus among all four experts. Table 2 summarizes the data, partitioning categories according to the allexperts consensus. Krippendorff α exp L1 exp L2 exp S1 exp S2 exp exp exp exp 1 - 0.837 1 - 0.804 0.808 1 - 0.823 0.831 0.768 1 L1 L2 S1 S2 Table 1: Krippendorff’s α pairwise among experts 9 Random selection was from the set of crowdsourceannotated users obtained in Section 3.2, ensuring that all expert annotations would be accompanied by crowdsourced annotations. Recall"
W18-0603,D14-1181,0,0.0058191,"Missing"
W18-0603,W16-0312,0,0.474076,"t, Cornell University, Ithaca, NY 4 Stanford Center for Population Health Sciences, Stanford University, Stanford, CA {shing,srnair,hal,resnik}@umd.edu ayah.zirikly@nih.gov, mdf224@cornell.edu Abstract who post material indicating imminent risk and the need for intervention. An emerging subset of the artificial intelligence and language technology communities has been making progress on automated methods that analyze online postings to flag mental health conditions, with the goal of being able to screen or monitor for suicide risk and other conditions (Calvo et al., 2017; Resnik et al., 2014; Milne et al., 2016; Milne, 2017). Some sites have been taking advantage of these methods to add automation to their moderation, in the form of a pipeline from algorithmic risk assessment to human moderator review to preventive action. With all of these technology-driven developments taking place so quickly, it is easy to forget that clinician assessment of suicidality from online writing is a new and largely unstudied problem. To what extent is level of suicide risk discernable from online postings? How are traditional training and experience in assessment brought to bear in the absence of interaction with the"
W18-0603,D08-1027,0,0.215817,"Missing"
W18-0603,N16-1174,0,0.0103924,"tap the potential of the dataset. For example, metadata associated with posts includes potentially valuable temporal information (Coppersmith et al., 2015), and we also have not yet explored the value of the annotators’ selecting the post that most strongly supports their judgment. In addition, the classification results here are just an initial exploration of the problem; for example, we plan to follow Vioul`es et al. (2018) in exploring hierarchical rather than four-way classification, which yielded substantial improvements, and we are exploring the role of hierarchical attention networks (Yang et al., 2016) as a way to cut through noise to identify the most relevant signals. We look forward to other researchers joining us in order to foster more rapid progress. sessed, we cannot validate clinician assessments; nor are we able to provide clinical evidence for improved validity using the detailed assessment instructions. Outcomes data would clearly be preferable if it were available; for example, Pokorny (1983) and Goldstein et al. (1991) attempt prediction of suicide using a wide range of variables and clinical measures for thousands of psychiatric inpatients. However, outcomes data are very diff"
W18-0603,D17-1322,0,0.190234,"Missing"
W18-0603,W16-0321,1,0.891999,"Missing"
