2006.iwslt-evaluation.11,P05-1033,0,0.0533135,"cal machine translation method. In the following sections, we ﬁrst explain our translation model and phrase reordering model. We then report the experiments’ results using our phrase reordering model based on predicate-argument structure. 1. Introduction 2. Baseline Translation Model Recently, phrase-based statistical machine translation model has become the mainstream in the machine translation community. Phrase-based approaches are capable of constructing better context-dependent word selection model than wordbased approaches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issu"
2006.iwslt-evaluation.11,P03-1039,0,0.0209387,"ranslation Model Recently, phrase-based statistical machine translation model has become the mainstream in the machine translation community. Phrase-based approaches are capable of constructing better context-dependent word selection model than wordbased approaches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heu"
2006.iwslt-evaluation.11,P01-1067,0,0.0677036,"aches. Though the unit of translation is still under active development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence"
2006.iwslt-evaluation.11,N03-1017,0,0.0124383,"e development [1], there is no approach more widely used than phrase-based one. Statistical machine translation, however, uses less linguistic knowledge such as syntax and semantics than conventional rule-based machine translation systems. For instance, the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target languag"
2006.iwslt-evaluation.11,P05-1066,0,0.122301,"the chunk-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target language. The translation model trained on a reWe followed the noisy channel approach to machine translation. In this approach, we search for the target (English) sentence by maximizing the probability of the target sentence eˆ given the source (foreign) senten"
2006.iwslt-evaluation.11,2001.mtsummit-papers.45,0,0.178026,"k-based approach in [2] does not rely on monolingual chunker and the hierarchical phrase-based approach in [1] does not use any kind of syntactic information except for a synchronous context-free grammar. Some SMT systems, however, try to incorporate syntactic knowledge, such as [3], yet it is hard to use it effectively as described in [4]. Another issue in statistical machine translation is reordering. Global reordering is essential to translation of languages with different word orders [5], and some aspects of global reordering in translation between German and English was stated in [6] and [7]. They used some heuristics to pre-process German corpus and reported successful results. In this paper, we present a novel phrase reordering model based on a predicate-argument structure analyzer. Given predicate-argument structure information from the analyzer, source sentence is reordered according to match that of the target language. The translation model trained on a reWe followed the noisy channel approach to machine translation. In this approach, we search for the target (English) sentence by maximizing the probability of the target sentence eˆ given the source (foreign) sentence fˆ. B"
2006.iwslt-evaluation.11,J03-1002,0,0.0120577,"of the source phrase phrase that was translated into the ith target phrase and bi−1 denotes the end position of the source phrase translated into the (i − 1)th target phrase. 77 I P (f 1 |eI1 ) = ΠIi=1 φ(f i |ei )d(ai a predicate-argument structure analyzer [12] on our corpus, which assigns these three cases to the arguments of predicates given a sentence. Figure 1 describes Japanese predicate-argument structure analysis of the following sentence: − bi−1 ) Translation probability is obtained from the relative frequency of the source phrase given the target phrase aligned by the GIZA++ toolkit [9]. count(f , e) φ(f |e) = ∑ f count(f , e) 住所/address を/WO-ACC ここ/here に/NILOC 書い/write て/PARTICLE 下さい/please where count(f , e) gives the source phrase f aligned to the target phrase e in the parallel corpus. The distortion model can be deﬁned as follows with an appropriate value for the parameter α: In this case, “書い/write て/PARTICLE 下さい/please” is identiﬁed as a predicate, “住所/address を/WO-ACC” is assigned WO case, and “ここ/here に/NI-LOC” is assigned NI case, respectively. Our predicate-argument structure analyzer does not only use dependency information and explicit case markers, but also us"
2006.iwslt-evaluation.11,P06-1079,1,0.810285,"f 1 . Each I source phrase f i in f 1 is translated into a a target phrase ei . The target phrases may be reordered. Phrase translation is then modeled by a probability distribution φ(f i |ei ) and reordering of target phrases is modeled by a relative distortion probability distribution d(ai − bi−1 ), where ai denotes the starting position of the source phrase phrase that was translated into the ith target phrase and bi−1 denotes the end position of the source phrase translated into the (i − 1)th target phrase. 77 I P (f 1 |eI1 ) = ΠIi=1 φ(f i |ei )d(ai a predicate-argument structure analyzer [12] on our corpus, which assigns these three cases to the arguments of predicates given a sentence. Figure 1 describes Japanese predicate-argument structure analysis of the following sentence: − bi−1 ) Translation probability is obtained from the relative frequency of the source phrase given the target phrase aligned by the GIZA++ toolkit [9]. count(f , e) φ(f |e) = ∑ f count(f , e) 住所/address を/WO-ACC ここ/here に/NILOC 書い/write て/PARTICLE 下さい/please where count(f , e) gives the source phrase f aligned to the target phrase e in the parallel corpus. The distortion model can be deﬁned as follows with"
2006.iwslt-evaluation.11,P06-1132,0,0.0301701,"Missing"
2006.iwslt-evaluation.11,J05-1004,0,0.0289442,"Missing"
2006.iwslt-evaluation.11,W03-1707,0,0.0504988,"Missing"
2006.iwslt-evaluation.11,W02-2016,1,0.823108,"error rate training (MERT) tool provided by CMU [20] with 500 normal order sentences to tune 4. Experiments and Discussions 4.1. Corpus and Tools We participated in Open Data Track in Japanese-English translation because we have built only Japanese predicateargument structure analyzer and thus source language is limited to Japanese in our phrase reordering model. We used ChaSen [16] for Word segmentation and POS tagging for Japanese. We did not use the original word segmentation information of Japanese because we used another POS tagger, ChaSen, instead. Dependency parsing was done by CaboCha [17]. We used tokenizer.sed from LDC to tokenize English sentences, and MXPOST [18] for POS tagging. Word translation probabilities were calculated by GIZA++ [9]. English words were lowercased for training and testing. We used a back-off word trigram model for the language model. It is trained on the lowercased English side of the parallel corpus by Palmkit [19]. We ﬁrst manually aligned English and Japanese sentences and obtained parallel corpus of 45,909 JapaneseEnglish sentences from 39,953 conversations. We then reordered Japanese sentences by using the predicate-argument structure analyzer. W"
2006.iwslt-evaluation.11,koen-2004-pharaoh,0,\N,Missing
2006.iwslt-evaluation.11,2005.iwslt-1.16,1,\N,Missing
2020.aacl-main.20,2020.findings-emnlp.58,0,0.26578,"at has the same structure and is fine-tuned on training data of ∗ † Currently at Nomura Research Institute, Ltd. Currently at Retrieva, Inc. Figure 1: Two methods for incorporating a pre-trained model into the GEC model. the downstream task. Using this two-stage method, the performance is expected to improve because downstream tasks are informed by the knowledge learned by the pre-trained model. Recent works (Kaneko et al., 2020; Kantor et al., 2019) show that BERT helps improve the performance on the English GEC task. As the Chinese pre-trained models are developed and released continuously (Cui et al., 2020; Zhang et al., 2019), the Chinese GEC task may also benefit from using those pre-trained models. In this study, as shown in Figure 1, we develop a Chinese GEC model based on Transformer with a pre-trained model using two methods: first, by initializing the encoder with the pre-trained model (BERT-encoder); second, by utilizing the technique proposed by Zhu et al. (2020), which uses the pre-trained model for additional features (BERTfused); on the Natural Language Processing and Chinese Computing (NLPCC) 2018 Grammatical Error Correction shared task test dataset (Zhao et al., 2018), our single"
2020.aacl-main.20,N19-1423,0,0.0530495,"task. GEC systems receive an erroneous sentence written by a language learner and output the corrected sentence. In previous studies that adopted neural models for Chinese GEC (Ren et al., 2018; Zhou et al., 2018), the performance was improved by initializing the models with a distributed word representation, such as Word2Vec (Mikolov et al., 2013). However, in these methods, only the embedding layer of a pretrained model was used to initialize the models. In recent years, pre-trained models based on Bidirectional Encoder Representations from Transformers (BERT) have been studied extensively (Devlin et al., 2019; Liu et al., 2019), and the performance of many downstream Natural Language Processing (NLP) tasks has been dramatically improved by utilizing these pre-trained models. To learn existing knowledge of a language, a BERTbased pre-trained model is trained on a large-scale corpus using the encoder of Transformer (Vaswani et al., 2017). Subsequently, for a downstream task, a neural network model is initialized with the weights learned by a pre-trained model that has the same structure and is fine-tuned on training data of ∗ † Currently at Nomura Research Institute, Ltd. Currently at Retrieva, Inc."
2020.aacl-main.20,N18-1055,0,0.0762515,"oBERTawwm-ext model developed by Cui et al. (2020). The main difference between Chinese-RoBERTawwm-ext and the original BERT is that the latter uses whole word masking (WWM) to train the model. In WWM, when a Chinese character is masked, other Chinese characters that belong to the same word should also be masked. 3.2 Grammatical Error Correction Model In this study, we use Transformer as the correction model. Transformer has shown excellent performance in sequence-to-sequence tasks, such as machine translation, and has been widely adopted in recent studies on English GEC (Kiyono et al., 2019; Junczys-Dowmunt et al., 2018). However, a BERT-based pre-trained model only uses the encoder of Transformer; therefore, it cannot be directly applied to sequence-to-sequence tasks that require both an encoder and a decoder, such as GEC. Hence, we incorporate the encoderdecoder model with the pre-trained model in two ways as described in the following subsections. BERT-encoder We initialize the encoder of Transformer with the parameters learned by Chinese-RoBERTa-wwm-ext; the decoder is initialized randomly. Finally, we fine-tune the initialized model on Chinese GEC data. Methods In the proposed method, we construct a corr"
2020.aacl-main.20,2020.acl-main.391,0,0.309196,"rpus using the encoder of Transformer (Vaswani et al., 2017). Subsequently, for a downstream task, a neural network model is initialized with the weights learned by a pre-trained model that has the same structure and is fine-tuned on training data of ∗ † Currently at Nomura Research Institute, Ltd. Currently at Retrieva, Inc. Figure 1: Two methods for incorporating a pre-trained model into the GEC model. the downstream task. Using this two-stage method, the performance is expected to improve because downstream tasks are informed by the knowledge learned by the pre-trained model. Recent works (Kaneko et al., 2020; Kantor et al., 2019) show that BERT helps improve the performance on the English GEC task. As the Chinese pre-trained models are developed and released continuously (Cui et al., 2020; Zhang et al., 2019), the Chinese GEC task may also benefit from using those pre-trained models. In this study, as shown in Figure 1, we develop a Chinese GEC model based on Transformer with a pre-trained model using two methods: first, by initializing the encoder with the pre-trained model (BERT-encoder); second, by utilizing the technique proposed by Zhu et al. (2020), which uses the pre-trained model for addi"
2020.aacl-main.20,D19-1119,0,0.0663708,"we use the Chinese-RoBERTawwm-ext model developed by Cui et al. (2020). The main difference between Chinese-RoBERTawwm-ext and the original BERT is that the latter uses whole word masking (WWM) to train the model. In WWM, when a Chinese character is masked, other Chinese characters that belong to the same word should also be masked. 3.2 Grammatical Error Correction Model In this study, we use Transformer as the correction model. Transformer has shown excellent performance in sequence-to-sequence tasks, such as machine translation, and has been widely adopted in recent studies on English GEC (Kiyono et al., 2019; Junczys-Dowmunt et al., 2018). However, a BERT-based pre-trained model only uses the encoder of Transformer; therefore, it cannot be directly applied to sequence-to-sequence tasks that require both an encoder and a decoder, such as GEC. Hence, we incorporate the encoderdecoder model with the pre-trained model in two ways as described in the following subsections. BERT-encoder We initialize the encoder of Transformer with the parameters learned by Chinese-RoBERTa-wwm-ext; the decoder is initialized randomly. Finally, we fine-tune the initialized model on Chinese GEC data. Methods In the propo"
2020.aacl-main.20,P16-1173,0,0.0155658,"entence is greater than 15; iii) the number of characters of the source sentence or the target sentence exceeds 64. Once the training data were filtered, we obtained 971,318 sentence pairs. Because the NLPCC 2018 Grammatical Error Correction shared task did not provide development data, we opted to randomly extract 5,000 sentences from the training data as the development data following Ren et al. (2018). The test data consist of 2,000 sentences extracted from the PKU Chinese Learner Corpus. According to Zhao et al. (2018), the annotation guidelines follow the minimum edit distance principle (Nagata and Sakaguchi, 2016), which selects the edit operation that minimizes the edit distance from the original sentence. Model We implement the Transformer model using fairseq 0.8.0.2 and load the pre-trained model using pytorch transformer 2.2.0.3 We then train the following models based on Transformer. Baseline: a plain Transformer model that is initialized randomly without using a pre-trained model. BERT-encoder: the correction model introduced in Section 3.2. BERT-fused: the correction model introduced in Section 3.2. We use the implementation provided by Zhu et al. (2020).4 Finally, we train a 4-ensemble BERT-enc"
2020.aacl-main.20,W14-1701,0,0.13606,"Missing"
2020.aacl-main.20,W13-3601,0,0.0197822,"uage Processing, pages 163–168 c December 4 - 7, 2020. 2020 Association for Computational Linguistics by the top team of the shared task. Moreover, using a 4-ensemble model, we obtain an F0.5 score of 35.51, which outperforms the results from the top team by a large margin. We annotate the error types of the development data; the results show that word-level errors dominate all error types and that sentence-level errors remain challenging and require a stronger approach. 2 Related Work Given the success of the shared tasks on English GEC at the Conference on Natural Language Learning (CoNLL) (Ng et al., 2013, 2014), a Chinese GEC shared task was performed at the NLPCC 2018. In this task, approximately one million sentences from the language learning website Lang81 were used as training data and two thousand sentences from the PKU Chinese Learner Corpus (Zhao et al., 2018) were used as test data. Here, we briefly describe the three methods with the highest performance. First, Fu et al. (2018) combined a 5-gram language model-based spell checker with subwordlevel and character-level encoder-decoder models using Transformer to obtain five types of outputs. Then, they re-ranked these outputs using th"
2020.aacl-main.20,P19-1139,0,0.0253172,"ructure and is fine-tuned on training data of ∗ † Currently at Nomura Research Institute, Ltd. Currently at Retrieva, Inc. Figure 1: Two methods for incorporating a pre-trained model into the GEC model. the downstream task. Using this two-stage method, the performance is expected to improve because downstream tasks are informed by the knowledge learned by the pre-trained model. Recent works (Kaneko et al., 2020; Kantor et al., 2019) show that BERT helps improve the performance on the English GEC task. As the Chinese pre-trained models are developed and released continuously (Cui et al., 2020; Zhang et al., 2019), the Chinese GEC task may also benefit from using those pre-trained models. In this study, as shown in Figure 1, we develop a Chinese GEC model based on Transformer with a pre-trained model using two methods: first, by initializing the encoder with the pre-trained model (BERT-encoder); second, by utilizing the technique proposed by Zhu et al. (2020), which uses the pre-trained model for additional features (BERTfused); on the Natural Language Processing and Chinese Computing (NLPCC) 2018 Grammatical Error Correction shared task test dataset (Zhao et al., 2018), our single models obtain F0.5 s"
2020.aacl-main.83,2020.tacl-1.5,0,0.0262759,"as a GEC input. Their study is similar to our research in that both studies use publicly available generic pretrained models to perform GEC. The difference between these studies is that Kaneko et al. (2020) used the architecture of the pretrained model as an encoder. Therefore, their method still requires pretraining with a large 3 Generic Pretrained Model BART (Lewis et al., 2020) is pretrained by predicting an original sequence, given a masked and shufﬂed sequence using a Transformer. They introduced masked tokens with various lengths based on the Poisson distribution, inspired by SpanBERT (Joshi et al., 2020), at multiple positions. BART is pretrained with large monolingual corpora (160 GB), including news, books, stories, and web-text domains. This model achieved strong results in several generation tasks; thus, it is regarded as a generic model. They released pretrained models using English monolingual corpora for several tasks, including summarization, which we used for English GEC. Liu et al. (2020) proposed multilingual BART (mBART) for a machine translation task, which we used for GEC of several languages. The latter model was trained using monolingual corpora for 25 languages simultaneously"
2020.aacl-main.83,2020.acl-main.391,0,0.450774,"and Mamoru Komachi Tokyo Metropolitan University satoru.katsumata@retrieva.jp, komachi@tmu.ac.jp Abstract achieved strong results for English GEC. N´aplava and Straka (2019) generated a pseudo corpus by introducing artiﬁcial errors into monolingual corpora and achieved the best scores for GEC in several languages by adopting the methods proposed by Grundkiewicz et al. (2019). These task-oriented pretraining approaches require extensive use of a pseudo-parallel corpus. Speciﬁcally, Grundkiewicz et al. (2019) used 100M ungrammatical and grammatical sentence pairs, while Kiyono et al. (2019) and Kaneko et al. (2020) used 70M sentence pairs, which required time-consuming pretraining of GEC models using the pseudo corpus. In this study, we determined the effectiveness of publicly available pretrained Enc–Dec models for GEC. Speciﬁcally, we investigated pretrained models without the need for pseudodata. We explored a pretrained model proposed by Lewis et al. (2020) called bidirectional and auto-regressive transformers (BART). Liu et al. (2020) also proposed multilingual BART. These models were pretrained by predicting the original sequence, given a masked and shufﬂed sentence. The motivation for using these"
2020.aacl-main.83,P07-2045,0,0.00570871,"niques to improve the accuracy of GEC. To compare these models, we experimented with an ensemble of ﬁve models. Our ensemble model was slightly better than our single model, but worse than the ensemble models by Kiyono et al. (2019) and Kaneko et al. (2020). The BART-based model along with the ensemble model achieved results comparable to current strong results despite only requiring ﬁne-tuning of the BART model. We believe that the reason for the ineffectiveness of the ensemble method is that the ﬁve models are not signiﬁcantly different as the 4 We used detokenizer.perl in the Moses script (Koehn et al., 2007). 5 https://spacy.io 6 We used the built-in de model. 7 https://github.com/aatimofeev/spacy russian tokenizer 8 https://github.com/ufal/morphodita German, Czech, and Russian. The dataset settings in this study were almost the same as those 3 BART, mBART: https://github.com/pytorch/fairseq 829 CoNLL-14 (M2 ) Kiyono et al. (2019) Kaneko et al. (2020) BART-based JFLEG BEA-test P R F0.5 GLEU P R F0.5 67.9/73.3 69.2/72.6 69.3/69.9 44.1/44.2 45.6/46.4 45.0/45.1 61.3/64.7 62.6/65.2 62.6/63.0 59.7/61.2 61.3/62.0 57.3/57.2 65.5/74.7 67.1/72.3 68.3/68.8 59.4/56.7 60.1/61.4 57.1/57.1 64.2/70.2 65.6/69.8"
2020.aacl-main.83,D18-2012,0,0.0275272,"orpus Train Dev Test En BEA JFLEG CoNLL-2014 1,157,370 - 4,384 - 4,477 747 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES-GEC 42,210 2,485 2,676 Ru RULEC-GEC 4,980 2,500 5,000 used by N´aplava and Straka (2019) for each language. We used ofﬁcial training data and decided the best model by using the development data. In addition, we trained the mBART-based models for German, Czech, and Russian GEC. We used mbart.cc25 for the mBART-based models. For the mBART-based model, we followed Liu et al. (2020); we detokenized4 the GEC training data for the mBART-based model and applied SentencePiece (Kudo and Richardson, 2018) with the SentencePiece model shared by Liu et al. (2020). Using this preprocessing, the input sentence may not represent grammatical information, compared with the sentence tokenized using a morphological analysis tool and subword tokenizer. However, what preprocessing is appropriate for GEC is beyond this paper’s scope and will be treated as future work. For evaluation, we tokenized the outputs after recovering the subwords. Then, we used a spaCy-based5 tokenizer for German6 and Russian7 , and the MorphoDiTa tokenizer8 for Czech. Moreover, the M2 scorer was used for each language. We compare"
2020.aacl-main.83,P17-1074,0,0.0295728,"We used BEA-dev to determine the best model. We trained the BART-based models by using bart.large. This model was proposed for the summarization task, which required some constraints in inference to ensure appropriate outputs; however, we did not impose any constraints because our task was different. We applied byte pair encoding (BPE) (Sennrich et al., 2016) to the training data for the BART-based model by using the BPE model of Lewis et al. (2020). We used the M2 scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) for CoNLL-14 and JFLEG, respectively, and used the ERRANT scorer (Bryant et al., 2017) for BEAtest. We compared these scores with strong results (Kiyono et al., 2019; Kaneko et al., 2020). 4.2 Results English. Table 2 presents the results of the English GEC task. When using a single model, the BART-based model is better than the model proposed by Kiyono et al. (2019), and the results are comparable to those reported by Kaneko et al. (2020) in terms of CoNLL-14 and BEA-test. Kiyono et al. (2019) and Kaneko et al. (2020) incorporated several techniques to improve the accuracy of GEC. To compare these models, we experimented with an ensemble of ﬁve models. Our ensemble model was s"
2020.aacl-main.83,2020.acl-main.703,0,0.553567,"ndkiewicz et al. (2019). These task-oriented pretraining approaches require extensive use of a pseudo-parallel corpus. Speciﬁcally, Grundkiewicz et al. (2019) used 100M ungrammatical and grammatical sentence pairs, while Kiyono et al. (2019) and Kaneko et al. (2020) used 70M sentence pairs, which required time-consuming pretraining of GEC models using the pseudo corpus. In this study, we determined the effectiveness of publicly available pretrained Enc–Dec models for GEC. Speciﬁcally, we investigated pretrained models without the need for pseudodata. We explored a pretrained model proposed by Lewis et al. (2020) called bidirectional and auto-regressive transformers (BART). Liu et al. (2020) also proposed multilingual BART. These models were pretrained by predicting the original sequence, given a masked and shufﬂed sentence. The motivation for using these models for GEC was that it achieved strong results for several text generation tasks, such as summarization; we refer to it as a generic pretrained model. We used generic pretrained BART models to compare with GEC models using a pseudo-corpus approach (Kiyono et al., 2019; Kaneko et al., 2020; N´aplava and Straka, 2019). We conducted GEC experiments"
2020.aacl-main.83,W19-4406,0,0.0452594,"(mBART) for a machine translation task, which we used for GEC of several languages. The latter model was trained using monolingual corpora for 25 languages simultaneously. They used a special token for representing the language of a sentence. For example, they added &lt;de_DE&gt; and &lt;ru_RU&gt; into the initial token of the encoder and decoder for De–Ru translation. To ﬁne-tune mBART for German, Czech, and Russian GEC, we set the target language for the special token referring to that language. 4 Experiment 4.1 Settings Common Settings. As presented in Table 1, we used learner corpora, including BEA2 (Bryant et al., 2019; Granger, 1998; Mizumoto et al., 2011; Tajiri et al., 2012; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), JFLEG (Napoles et al., 2017), and CoNLL-14 (Ng et al., 2014) data for 2 BEA corpus is made of several corpora. Details can be found in Bryant et al. (2019). 828 lang Corpus Train Dev Test En BEA JFLEG CoNLL-2014 1,157,370 - 4,384 - 4,477 747 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES-GEC 42,210 2,485 2,676 Ru RULEC-GEC 4,980 2,500 5,000 used by N´aplava and Straka (2019) for each language. We used ofﬁcial training data and decided the best model by using the development data"
2020.aacl-main.83,N12-1067,0,0.0350141,"d not use the unchanged sentences in the source and target sides; thus, the training data consisted of 561,525 sentences. We used BEA-dev to determine the best model. We trained the BART-based models by using bart.large. This model was proposed for the summarization task, which required some constraints in inference to ensure appropriate outputs; however, we did not impose any constraints because our task was different. We applied byte pair encoding (BPE) (Sennrich et al., 2016) to the training data for the BART-based model by using the BPE model of Lewis et al. (2020). We used the M2 scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) for CoNLL-14 and JFLEG, respectively, and used the ERRANT scorer (Bryant et al., 2017) for BEAtest. We compared these scores with strong results (Kiyono et al., 2019; Kaneko et al., 2020). 4.2 Results English. Table 2 presents the results of the English GEC task. When using a single model, the BART-based model is better than the model proposed by Kiyono et al. (2019), and the results are comparable to those reported by Kaneko et al. (2020) in terms of CoNLL-14 and BEA-test. Kiyono et al. (2019) and Kaneko et al. (2020) incorporated several techniques to improve"
2020.aacl-main.83,2020.tacl-1.47,0,0.282149,"ive use of a pseudo-parallel corpus. Speciﬁcally, Grundkiewicz et al. (2019) used 100M ungrammatical and grammatical sentence pairs, while Kiyono et al. (2019) and Kaneko et al. (2020) used 70M sentence pairs, which required time-consuming pretraining of GEC models using the pseudo corpus. In this study, we determined the effectiveness of publicly available pretrained Enc–Dec models for GEC. Speciﬁcally, we investigated pretrained models without the need for pseudodata. We explored a pretrained model proposed by Lewis et al. (2020) called bidirectional and auto-regressive transformers (BART). Liu et al. (2020) also proposed multilingual BART. These models were pretrained by predicting the original sequence, given a masked and shufﬂed sentence. The motivation for using these models for GEC was that it achieved strong results for several text generation tasks, such as summarization; we refer to it as a generic pretrained model. We used generic pretrained BART models to compare with GEC models using a pseudo-corpus approach (Kiyono et al., 2019; Kaneko et al., 2020; N´aplava and Straka, 2019). We conducted GEC experiments for four languages: English, German, Czech, and Russian. The Enc–Dec model based"
2020.aacl-main.83,D19-5545,0,0.435319,"Missing"
2020.aacl-main.83,P15-2097,0,0.0235825,"in the source and target sides; thus, the training data consisted of 561,525 sentences. We used BEA-dev to determine the best model. We trained the BART-based models by using bart.large. This model was proposed for the summarization task, which required some constraints in inference to ensure appropriate outputs; however, we did not impose any constraints because our task was different. We applied byte pair encoding (BPE) (Sennrich et al., 2016) to the training data for the BART-based model by using the BPE model of Lewis et al. (2020). We used the M2 scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) for CoNLL-14 and JFLEG, respectively, and used the ERRANT scorer (Bryant et al., 2017) for BEAtest. We compared these scores with strong results (Kiyono et al., 2019; Kaneko et al., 2020). 4.2 Results English. Table 2 presents the results of the English GEC task. When using a single model, the BART-based model is better than the model proposed by Kiyono et al. (2019), and the results are comparable to those reported by Kaneko et al. (2020) in terms of CoNLL-14 and BEA-test. Kiyono et al. (2019) and Kaneko et al. (2020) incorporated several techniques to improve the accuracy of GEC. To compare"
2020.aacl-main.83,E17-2037,0,0.013145,"or 25 languages simultaneously. They used a special token for representing the language of a sentence. For example, they added &lt;de_DE&gt; and &lt;ru_RU&gt; into the initial token of the encoder and decoder for De–Ru translation. To ﬁne-tune mBART for German, Czech, and Russian GEC, we set the target language for the special token referring to that language. 4 Experiment 4.1 Settings Common Settings. As presented in Table 1, we used learner corpora, including BEA2 (Bryant et al., 2019; Granger, 1998; Mizumoto et al., 2011; Tajiri et al., 2012; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), JFLEG (Napoles et al., 2017), and CoNLL-14 (Ng et al., 2014) data for 2 BEA corpus is made of several corpora. Details can be found in Bryant et al. (2019). 828 lang Corpus Train Dev Test En BEA JFLEG CoNLL-2014 1,157,370 - 4,384 - 4,477 747 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES-GEC 42,210 2,485 2,676 Ru RULEC-GEC 4,980 2,500 5,000 used by N´aplava and Straka (2019) for each language. We used ofﬁcial training data and decided the best model by using the development data. In addition, we trained the mBART-based models for German, Czech, and Russian GEC. We used mbart.cc25 for the mBART-based models. For the mB"
2020.aacl-main.83,W14-1701,0,0.0185555,"used a special token for representing the language of a sentence. For example, they added &lt;de_DE&gt; and &lt;ru_RU&gt; into the initial token of the encoder and decoder for De–Ru translation. To ﬁne-tune mBART for German, Czech, and Russian GEC, we set the target language for the special token referring to that language. 4 Experiment 4.1 Settings Common Settings. As presented in Table 1, we used learner corpora, including BEA2 (Bryant et al., 2019; Granger, 1998; Mizumoto et al., 2011; Tajiri et al., 2012; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), JFLEG (Napoles et al., 2017), and CoNLL-14 (Ng et al., 2014) data for 2 BEA corpus is made of several corpora. Details can be found in Bryant et al. (2019). 828 lang Corpus Train Dev Test En BEA JFLEG CoNLL-2014 1,157,370 - 4,384 - 4,477 747 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES-GEC 42,210 2,485 2,676 Ru RULEC-GEC 4,980 2,500 5,000 used by N´aplava and Straka (2019) for each language. We used ofﬁcial training data and decided the best model by using the development data. In addition, we trained the mBART-based models for German, Czech, and Russian GEC. We used mbart.cc25 for the mBART-based models. For the mBART-based model, we followed Liu"
2020.aacl-main.83,2020.bea-1.16,0,0.0906348,"; Kaneko et al., 2020). For example, Kiyono et al. (2019) generated a pseudo corpus using back-translation and ∗ Currently working at Retrieva, Inc. https://github.com/Katsumata420/generic-pretrainedGEC 1 827 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 827–832 c December 4 - 7, 2020. 2020 Association for Computational Linguistics for GEC. 2 amount of pseudodata. The current SOTA approach for English GEC uses the sequence tagging model proposed by Omelianchuk et al. (2020). They designed tokenlevel transformations to map input tokens to target corrections to produce training data. The sequence tagging model then predicts the transformation corresponding to the input token. We do not attempt to make a comparison with this approach, as the purpose of our study is to create a strong GEC model without using pseudodata or linguistic knowledge. Previous Work The Enc–Dec approach for GEC often uses the task-oriented pretraining strategy. For example, Zhao et al. (2019) and Grundkiewicz et al. (2019) reported that pretraining of the Enc–Dec model using a pseudo corpus"
2020.aacl-main.83,Q19-1001,0,0.208894,"subword tokenizer. However, what preprocessing is appropriate for GEC is beyond this paper’s scope and will be treated as future work. For evaluation, we tokenized the outputs after recovering the subwords. Then, we used a spaCy-based5 tokenizer for German6 and Russian7 , and the MorphoDiTa tokenizer8 for Czech. Moreover, the M2 scorer was used for each language. We compared these scores with the current SOTA results (N´aplava and Straka, 2019). Table 1: Data statistics. English; Falko+MERLIN data (Boyd et al., 2014) for German; AKCES-GEC (N´aplava and Straka, 2019) for Czech; and RULEC-GEC (Rozovskaya and Roth, 2019) for Russian. Our models were ﬁne-tuned using a single GPU (NVIDIA TITAN RTX), and our implementations were based on publicly available code3 . We used the hyperparameters provided in some previous works (Lewis et al., 2020; Liu et al., 2020), unless otherwise noted. The scores excluding the ensemble method were averaged in ﬁve ﬁne-tuned experiments with random seeds. English. Our setting for the English datasets was almost the same as that of Kiyono et al. (2019). We extracted the training data from BEA-train for English GEC. Similar to Kiyono et al. (2019), we did not use the unchanged sente"
2020.aacl-main.83,P16-1162,0,0.0155066,"same as that of Kiyono et al. (2019). We extracted the training data from BEA-train for English GEC. Similar to Kiyono et al. (2019), we did not use the unchanged sentences in the source and target sides; thus, the training data consisted of 561,525 sentences. We used BEA-dev to determine the best model. We trained the BART-based models by using bart.large. This model was proposed for the summarization task, which required some constraints in inference to ensure appropriate outputs; however, we did not impose any constraints because our task was different. We applied byte pair encoding (BPE) (Sennrich et al., 2016) to the training data for the BART-based model by using the BPE model of Lewis et al. (2020). We used the M2 scorer (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) for CoNLL-14 and JFLEG, respectively, and used the ERRANT scorer (Bryant et al., 2017) for BEAtest. We compared these scores with strong results (Kiyono et al., 2019; Kaneko et al., 2020). 4.2 Results English. Table 2 presents the results of the English GEC task. When using a single model, the BART-based model is better than the model proposed by Kiyono et al. (2019), and the results are comparable to those reported by Kane"
2020.aacl-main.83,P12-2039,1,0.760232,"EC of several languages. The latter model was trained using monolingual corpora for 25 languages simultaneously. They used a special token for representing the language of a sentence. For example, they added &lt;de_DE&gt; and &lt;ru_RU&gt; into the initial token of the encoder and decoder for De–Ru translation. To ﬁne-tune mBART for German, Czech, and Russian GEC, we set the target language for the special token referring to that language. 4 Experiment 4.1 Settings Common Settings. As presented in Table 1, we used learner corpora, including BEA2 (Bryant et al., 2019; Granger, 1998; Mizumoto et al., 2011; Tajiri et al., 2012; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), JFLEG (Napoles et al., 2017), and CoNLL-14 (Ng et al., 2014) data for 2 BEA corpus is made of several corpora. Details can be found in Bryant et al. (2019). 828 lang Corpus Train Dev Test En BEA JFLEG CoNLL-2014 1,157,370 - 4,384 - 4,477 747 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES-GEC 42,210 2,485 2,676 Ru RULEC-GEC 4,980 2,500 5,000 used by N´aplava and Straka (2019) for each language. We used ofﬁcial training data and decided the best model by using the development data. In addition, we trained the mBART-based models for German"
2020.aacl-main.83,P11-1019,0,0.0607402,"es. The latter model was trained using monolingual corpora for 25 languages simultaneously. They used a special token for representing the language of a sentence. For example, they added &lt;de_DE&gt; and &lt;ru_RU&gt; into the initial token of the encoder and decoder for De–Ru translation. To ﬁne-tune mBART for German, Czech, and Russian GEC, we set the target language for the special token referring to that language. 4 Experiment 4.1 Settings Common Settings. As presented in Table 1, we used learner corpora, including BEA2 (Bryant et al., 2019; Granger, 1998; Mizumoto et al., 2011; Tajiri et al., 2012; Yannakoudakis et al., 2011; Dahlmeier et al., 2013), JFLEG (Napoles et al., 2017), and CoNLL-14 (Ng et al., 2014) data for 2 BEA corpus is made of several corpora. Details can be found in Bryant et al. (2019). 828 lang Corpus Train Dev Test En BEA JFLEG CoNLL-2014 1,157,370 - 4,384 - 4,477 747 1,312 De Falko+MERLIN 19,237 2,503 2,337 Cz AKCES-GEC 42,210 2,485 2,676 Ru RULEC-GEC 4,980 2,500 5,000 used by N´aplava and Straka (2019) for each language. We used ofﬁcial training data and decided the best model by using the development data. In addition, we trained the mBART-based models for German, Czech, and Russian GEC. We"
2020.aacl-main.83,N19-1014,0,0.0370551,"dodata. The current SOTA approach for English GEC uses the sequence tagging model proposed by Omelianchuk et al. (2020). They designed tokenlevel transformations to map input tokens to target corrections to produce training data. The sequence tagging model then predicts the transformation corresponding to the input token. We do not attempt to make a comparison with this approach, as the purpose of our study is to create a strong GEC model without using pseudodata or linguistic knowledge. Previous Work The Enc–Dec approach for GEC often uses the task-oriented pretraining strategy. For example, Zhao et al. (2019) and Grundkiewicz et al. (2019) reported that pretraining of the Enc–Dec model using a pseudo corpus is effective for the GEC task. In particular, they introduced word- and character-level errors into a sentence in monolingual corpora. They developed a confusion set derived from a spellchecker and randomly replaced a word in a sentence. They also randomly deleted a word, inserted a random word, and swapped a word with an adjacent word. They performed these same operations, i.e., replacing, deleting, inserting, and swapping, for characters. The pseudo corpus made by the above methods consisted"
2020.aacl-main.83,I11-1017,1,\N,Missing
2020.aacl-main.83,W13-1703,0,\N,Missing
2020.aacl-main.83,boyd-etal-2014-merlin,0,\N,Missing
2020.aacl-main.83,N19-1423,0,\N,Missing
2020.aacl-main.83,W19-4427,0,\N,Missing
2020.aacl-srw.10,N19-1423,0,0.641974,"recent Indonesian NER work that used BiLSTM-CRF was conducted by Wintaka et al. (2019) using FastText as the word representation. It has been claimed that FastText offers advantages in handling misspelled words and out-of-vocabulary (OOV) problems (Bojanowski et al., 2017). Therefore, we used BiLSTM-CRF with FastText as our baseline model. We also experimented with Bidirectional Encoder Representations from Transformers (BERT), a transformer-based language model known to work best in various tasks in NLP as well as NER by acquiring contextual word meanings based on their usage in a sentence (Devlin et al., 2019). For the experiment, we compared three models: the multilingual transformer-based models; mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) and a monolingual BERT for the Indonesian language (IndoBERT) (Wilie et al., 2020). Regarding the limited vocabulary in our low-resourced data, we hypothesized that these embeddings could solve the OOV problem because of the vocabuIn recent years, named entity recognition (NER) tasks in the Indonesian language have undergone extensive development. There are only a few corpora for Indonesian NER; hence, recent Indonesian NER studies have used di"
2020.aacl-srw.10,C18-1139,0,0.084055,"onsistency and made the dataset publicly available.1 2. We analyzed the impact of the data consistency by comparing the performance of NER models trained on the previous and reannotated datasets. Our dataset significantly improved NER performance. 3. We compared the static and dynamic word embeddings for the Indonesian NER task and showed the impact of different embeddings on the NER model performance. 2 Related Works Several recent NER methods employed the bidirectional neural network and a conditional random field (CRF) as the encoder–decoder layer (Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Using contextual information as a representation input to the encoder model improved the score substantially as it helped the model learn the context of the entities. Akbik et al. (2018) introduced Flair embedding, a contextual string embedding approach, and showed that stacking word and character embeddings increased the model’s ability to understand contextual and word-level semantic representations. The use of a Transformer (such as in BERT) also demonstrated significant results for numerous NLP downstream tasks, for example, by BERT, which was proven to work on certain tasks, as well as"
2020.aacl-srw.10,L18-1550,0,0.0419523,"Missing"
2020.aacl-srw.10,2020.osact-1.2,0,0.0128038,"as for NER (Devlin et al., 2019; Conneau et al., 2020). The current state-of-the-art NER model to date was created by fine-tuning a clozedriven pre-trained bidirectional transformer model (Baevski et al., 2019). Nevertheless, we focus on low-resourced language NER. The majority of previous studies on low-resourced NER also implemented BiLSTMCRF as the sequence labeling method and experimented with input representation (Pham and LeHong, 2018; Poostchi et al., 2018; Singh et al., 2019). BERT has also been employed in several low-resource languages, including Bulgarian (Marinova, 2019), Arabic (Antoun et al., 2020), and Basque (Agerri et al., 2020). 3 Methodology 3.1 Inconsistency of Existing Dataset We used an open dataset released by S&N (2016), which is available on GitHub5 . However, we found that several tokens in the dataset were not tagged correctly. For example, tokens of certain organizations and persons were not tagged or were tagged incorrectly. Table 1 shows an examples of inconsistency in the annotation. The three sentences 2 https://www.kompas.com/ https://www.tempo.co/ 4 https://www.tribunnews.com/ 5 https://github.com/yusufsyaifudin/ Indonesia-ner 3 1 https://github.com/khairunnisaor/ id"
2020.aacl-srw.10,J08-4004,0,0.0571192,"nd person. We omitted the other two, time and quantity, because we wanted to build a model with a strong foundation for recognizing ambiguous nouns. As most time and quantity entities are written in numeric form, they are easy to be recognized by a well-developed NER model. In the experiment, we considered just the three entities in both datasets so that the results would be reasonably comparable. We calculated the inter-annotator agreement of the three annotators and obtained an agreement score of 0.92 using Fleiss’ kappa (Fleiss, 1971), which indicates a high agreement and good reliability (Artstein and Poesio, 2008). In this study, we used the same split as that used in S&N (2016). However, owing to the absence of the development set as in theirs, we randomly sampled data from the training set to constitute the development set, as indicated in Table 2. Table 2: Data statistics. S&N Tags LOC ORG PER O # of tags LOC Our Tags ORG PER O # of tags 1,153 5 4 91 26 1,562 3 701 4 2 2,317 127 344 78 39 42,241 1,527 1,647 2,363 43,160 1,253 2,292 2,450 42,702 Dataset Re-annotation Table 3: Confusion matrix of our re-annotation from S&N (2016). The number of tags is represented at the token level. The first column"
2020.aacl-srw.10,D19-1539,0,0.0126004,"8) introduced Flair embedding, a contextual string embedding approach, and showed that stacking word and character embeddings increased the model’s ability to understand contextual and word-level semantic representations. The use of a Transformer (such as in BERT) also demonstrated significant results for numerous NLP downstream tasks, for example, by BERT, which was proven to work on certain tasks, as well as for NER (Devlin et al., 2019; Conneau et al., 2020). The current state-of-the-art NER model to date was created by fine-tuning a clozedriven pre-trained bidirectional transformer model (Baevski et al., 2019). Nevertheless, we focus on low-resourced language NER. The majority of previous studies on low-resourced NER also implemented BiLSTMCRF as the sequence labeling method and experimented with input representation (Pham and LeHong, 2018; Poostchi et al., 2018; Singh et al., 2019). BERT has also been employed in several low-resource languages, including Bulgarian (Marinova, 2019), Arabic (Antoun et al., 2020), and Basque (Agerri et al., 2020). 3 Methodology 3.1 Inconsistency of Existing Dataset We used an open dataset released by S&N (2016), which is available on GitHub5 . However, we found that"
2020.aacl-srw.10,P18-1007,0,0.012563,"-PER Manusia human O I-ORG I-ORG I-ORG Table 6: Examples of errors in prediction comparing S&N (2016) and our annotation when trained using the baseline BiLSTM-CRF model. Red indicates incorrect tokens and blue indicates the correct ones. (2020) stated that the XLM-R model might achieve a better result on the NER task as entity names usually come from English or other languages. However, most of our dataset’s entity names are in Indonesian, and hence, the IndoBERT pre-trained model supports this condition better. Additionally, mBERT and XLM-R use WordPiece (Wu et al., 2016) and SentencePiece (Kudo, 2018) tokenization respectively, whereby longer tokens are split into more common tokens. Multilingual models contain many languages in their vocabularies. When the pre-trained model is frozen for feature representation use, it is possible that some Indonesian sub-words are biased because they share representations with other language’s sub-words. The IndoBERT was trained on the Indo4B dataset, which also contains Indonesian news corpora (Wilie et al., 2020). We hypothesize that the BiLSTM-CRF architecture fits better with our sequence classification task, supported by the rich Indonesian vocabular"
2020.aacl-srw.10,Q17-1010,0,0.235771,"resource to improve the NLP foundation for the Indonesian language. The most problematic entity is organization, followed by location and person. Certain tokens had been tagged as entities that they were not; for example, the term “DPP” (which means “party’s representative council”) is not an organization name but had been tagged as such. The most recent Indonesian NER work that used BiLSTM-CRF was conducted by Wintaka et al. (2019) using FastText as the word representation. It has been claimed that FastText offers advantages in handling misspelled words and out-of-vocabulary (OOV) problems (Bojanowski et al., 2017). Therefore, we used BiLSTM-CRF with FastText as our baseline model. We also experimented with Bidirectional Encoder Representations from Transformers (BERT), a transformer-based language model known to work best in various tasks in NLP as well as NER by acquiring contextual word meanings based on their usage in a sentence (Devlin et al., 2019). For the experiment, we compared three models: the multilingual transformer-based models; mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) and a monolingual BERT for the Indonesian language (IndoBERT) (Wilie et al., 2020). Regarding the limi"
2020.aacl-srw.10,W18-6112,0,0.0901921,"essing (NLP). However, NER still suffers from data sparseness for the majority of languages, including Indonesian. Various Indonesian NER approaches have been proposed, ranging from rule-based methods (Budi et al., 2005) to machine learning-based techniques (Leonandya et al., 2015; Aryoyudanta et al., 2016). The DBpedia and Wikipedia datasets are mainly used for supervised approaches (Alfina et al., 2016; Leonandya et al., 2015; Aryoyudanta et al., 2016; Gunawan et al., 2018). Other datasets include Twitter (Taufik et al., 2016; Wintaka et al., 2019) and conversational datasets from chatbots (Kurniawan and Louvan, 2018), but the sizes of these datasets 64 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 64–71 c December 4 - 7, 2020. 2020 Association for Computational Linguistics lary coverage in the large-scale data used for pretraining those embeddings. Our contributions can be summarized as follows: Deep learning has recently been used in Indonesian NER research. The most widely used method is the BiLSTM algorithm (Huang et al., 2015; Lam"
2020.aacl-srw.10,N16-1030,0,0.473151,"18), but the sizes of these datasets 64 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 64–71 c December 4 - 7, 2020. 2020 Association for Computational Linguistics lary coverage in the large-scale data used for pretraining those embeddings. Our contributions can be summarized as follows: Deep learning has recently been used in Indonesian NER research. The most widely used method is the BiLSTM algorithm (Huang et al., 2015; Lample et al., 2016). Various input representation methods have been applied, such as convolutional neural networks (CNNs) for word n-gram representation (Gunawan et al., 2018) and pre-trained word embeddings with part-of-speech (PoS) tags (Hoesen and Purwarianti, 2018). In exploring the OOV problem in conversational text, Kurniawan and Louvan (2018) also employed BiLSTM-CRF without including any pre-trained word representation. In recent work by Wintaka et al. (2019), the same neural sequence labeling model was implemented, and pre-trained FastText Indonesian word embedding was applied as the input. In work simi"
2020.aacl-srw.10,2020.acl-main.747,0,0.0312497,"Missing"
2020.aacl-srw.10,P16-1101,0,0.0435722,"to its building or location. In this case, we annotate the entity as a location name. location tags were incorrect, and almost 500 tokens were not tagged. Comparing our annotation, we calculated the percent difference by dividing the difference in the number of tags by the total number of tokens. 4 Experiment 4.1 NER Methods BiLSTM-CRF is a deep learning algorithm introduced by Huang et al. (2015) and has mainly been used for the NER task owing to its ability to solve sequence tagging problems. Following its successes in dealing with English NER tasks (Lample et al., 2016; Akbik et al., 2018; Ma and Hovy, 2016), BiLSTM-CRF was also implemented in recent Indonesian NER; relatively good results were obtained compared to those of rule-based and former machine learning approaches (Hoesen and Purwarianti, 2018; Kurniawan and Louvan, 2018; Wintaka et al., 2019). We employed a method used by Wintaka et al. (2019) as our baseline. They used FastText (Bojanowski et al., 2017), a pre-trained word embedding with sub-word features, as the input representation for the BiLSTM-CRF. In addition to FastText, we also used some pretrained multilingual and monolingual models as the input representation for BiLSTM-CRF."
2020.aacl-srw.10,W00-0733,0,0.122254,"Missing"
2020.aacl-srw.10,R19-2008,0,0.0163254,"on certain tasks, as well as for NER (Devlin et al., 2019; Conneau et al., 2020). The current state-of-the-art NER model to date was created by fine-tuning a clozedriven pre-trained bidirectional transformer model (Baevski et al., 2019). Nevertheless, we focus on low-resourced language NER. The majority of previous studies on low-resourced NER also implemented BiLSTMCRF as the sequence labeling method and experimented with input representation (Pham and LeHong, 2018; Poostchi et al., 2018; Singh et al., 2019). BERT has also been employed in several low-resource languages, including Bulgarian (Marinova, 2019), Arabic (Antoun et al., 2020), and Basque (Agerri et al., 2020). 3 Methodology 3.1 Inconsistency of Existing Dataset We used an open dataset released by S&N (2016), which is available on GitHub5 . However, we found that several tokens in the dataset were not tagged correctly. For example, tokens of certain organizations and persons were not tagged or were tagged incorrectly. Table 1 shows an examples of inconsistency in the annotation. The three sentences 2 https://www.kompas.com/ https://www.tempo.co/ 4 https://www.tribunnews.com/ 5 https://github.com/yusufsyaifudin/ Indonesia-ner 3 1 https:"
2020.aacl-srw.10,2020.aacl-main.85,0,0.379591,"OOV) problems (Bojanowski et al., 2017). Therefore, we used BiLSTM-CRF with FastText as our baseline model. We also experimented with Bidirectional Encoder Representations from Transformers (BERT), a transformer-based language model known to work best in various tasks in NLP as well as NER by acquiring contextual word meanings based on their usage in a sentence (Devlin et al., 2019). For the experiment, we compared three models: the multilingual transformer-based models; mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) and a monolingual BERT for the Indonesian language (IndoBERT) (Wilie et al., 2020). Regarding the limited vocabulary in our low-resourced data, we hypothesized that these embeddings could solve the OOV problem because of the vocabuIn recent years, named entity recognition (NER) tasks in the Indonesian language have undergone extensive development. There are only a few corpora for Indonesian NER; hence, recent Indonesian NER studies have used diverse datasets. Although an open dataset is available, it includes only approximately 2,000 sentences and contains inconsistent annotations, thereby preventing accurate training of NER models without reliance on pre-trained models. Th"
2020.aacl-srw.10,D19-6125,0,0.0119954,"of pre-trained transformer-based language models, both the multilingual and the monolingual models, yielded better prediction results. Although the performance of Indonesian NER using either BiLSTM-CRF or fine-tuning depends on the pretrained language model, we found that IndoBERT works best when using BiLSTM-CRF architecture, compared to the fine-tuning approach. In the future, we plan to address word ambiguity in Indonesian by creating a gazetteer to add more supervision and perform distant supervised learning to aid the model in differentiating a word to be classified as each entity as in Nooralahzadeh et al. (2019). Also, we would like to work on other techniques such as transferring knowledge using a teacher-student learning from a high resource language such as English to a low-resource, such as Indonesian (Wu et al., 2020; Sun et al., 2019). Discussion Table 6 presents some examples of errors by the model trained on S&N (2016) and on our annotation. In Sentence 1, the words “Ketua Umum Gerindra” were tagged as part of a person’s name, although they are not. The S&N (2016) model identified “Prabowo Subianto” correctly as a person’s name but did not tag “Gerindra” as an organization name. Meanwhile, ou"
2020.aacl-srw.10,N18-1202,0,0.0244741,"aset to improve its consistency and made the dataset publicly available.1 2. We analyzed the impact of the data consistency by comparing the performance of NER models trained on the previous and reannotated datasets. Our dataset significantly improved NER performance. 3. We compared the static and dynamic word embeddings for the Indonesian NER task and showed the impact of different embeddings on the NER model performance. 2 Related Works Several recent NER methods employed the bidirectional neural network and a conditional random field (CRF) as the encoder–decoder layer (Lample et al., 2016; Peters et al., 2018; Akbik et al., 2018). Using contextual information as a representation input to the encoder model improved the score substantially as it helped the model learn the context of the entities. Akbik et al. (2018) introduced Flair embedding, a contextual string embedding approach, and showed that stacking word and character embeddings increased the model’s ability to understand contextual and word-level semantic representations. The use of a Transformer (such as in BERT) also demonstrated significant results for numerous NLP downstream tasks, for example, by BERT, which was proven to work on certa"
2020.aacl-srw.10,2020.acl-main.581,0,0.0265652,"Missing"
2020.aacl-srw.10,L18-1701,0,0.0175304,"n BERT) also demonstrated significant results for numerous NLP downstream tasks, for example, by BERT, which was proven to work on certain tasks, as well as for NER (Devlin et al., 2019; Conneau et al., 2020). The current state-of-the-art NER model to date was created by fine-tuning a clozedriven pre-trained bidirectional transformer model (Baevski et al., 2019). Nevertheless, we focus on low-resourced language NER. The majority of previous studies on low-resourced NER also implemented BiLSTMCRF as the sequence labeling method and experimented with input representation (Pham and LeHong, 2018; Poostchi et al., 2018; Singh et al., 2019). BERT has also been employed in several low-resource languages, including Bulgarian (Marinova, 2019), Arabic (Antoun et al., 2020), and Basque (Agerri et al., 2020). 3 Methodology 3.1 Inconsistency of Existing Dataset We used an open dataset released by S&N (2016), which is available on GitHub5 . However, we found that several tokens in the dataset were not tagged correctly. For example, tokens of certain organizations and persons were not tagged or were tagged incorrectly. Table 1 shows an examples of inconsistency in the annotation. The three sentences 2 https://www.kom"
2020.acl-srw.11,P17-1176,0,0.0192,"atical differences in South Korean evaluation data. 2 Related Work differences between South Korean and North Korean words or phrases that have the same meaning. We only consider the differences in the WS and ISR in our study, as differences in compound words in the evaluation data rarely appear. The pivot language approach increases the translation error between the source language and the target language, because the translation model of each language is independently trained. Cheng et al. (2017) addressed this problem by allowing interaction during the translation model training. Moreover, Chen et al. (2017) proposed a method to train a source-to-target model using a pretrained teacher model as its guide. Marujo et al. (2011) proposed a rule-based method to convert similar languages into a target language to extend the language resources of the target side. Wang et al. (2016) presented a method to extract the conversion rules between similar languages. Firat et al. (2016) proposed a many-to-many translation model with several encoders and decoders. However, the accuracy of a many-to-many translation model with a single shared encoder and decoder was found to be higher (Johnson et al., 2017). Fina"
2020.acl-srw.11,N19-1423,0,0.0311998,"Missing"
2020.acl-srw.11,D16-1026,0,0.056759,"Missing"
2020.acl-srw.11,W17-3204,0,0.0262372,"Introduction Neural machine translation (NMT) has been adapted to many languages; however, machine translation of the North Korean language1 has seldom been performed. One of the reasons is the lack of large-scale bilingual data for training North Korean neural models. It is known that large-scale bilingual data are required to improve the translation accuracy of an NMT model. For example, one of the previous works suggests that an NMT system is less accurate than a phrase-based statistical machine translation system if there are no more than 100 million words in the bilingual training data (Koehn and Knowles, 2017). There are three approaches to solve low language resource bottleneck. First, Wang et al. (2006) proposed a method to train a translation model using a pivot language as an intermediate language. This approach translates from the source language to • Because there is no evaluation dataset between North Korean and English, we create a North Korean-English evaluation dataset by manually translating the South Korean-English bilingual evaluation dataset into a North Korean one. 1 Korean is a language mainly used in the Korean peninsula; however, there are some grammatical differences between the"
2020.acl-srw.11,2011.eamt-1.19,0,0.0854096,"o, Tokyo 191-0065, Japan {kim-hwichan, hirasawa-tosho}@ed.tmu.ac.jp, komachi@tmu.ac.jp Abstract the pivot language and from the pivot language to the target language. However, there is no good pivot language between North Korean and English. Second, Johnson et al. (2017) proposed a many-tomany translation model, where multiple languages are translated into other languages using a single shared encoder and decoder. They demonstrated that this model can translate a language pair that is unseen in training data. However, North Korean does not have any bilingual data between any languages. Third, Marujo et al. (2011) proposed a rulebased method to convert similar languages into a target language, such as Brazilian Portuguese to European Portuguese, and extended the target language resources. North Korean is a language remarkably similar to South Korean, but conversion from South Korean to North Korean needs to be determined considering the context, which makes rule-based conversion difficult. Therefore, in this study, we propose a method to tokenize South Korean input sentences at the character level and decompose them into phonemes to mitigate the grammatical differences between South Korean and North Ko"
2020.acl-srw.11,W18-6319,0,0.0118353,"n Sennrich and Zhang (2019) (Table 2). We use a News Korean-English parallel corpus for training the model and convert it into North Korean grammar (3.2) for evaluating the model. We perform tokenization and truecasing using Moses scripts for all the input sentence pairs. We delete sentences with more than 200 words from the training data. Table 3 presents the training, development, and test data statistics. In the evaluation, we perform detruecasing and detokenization for the translation outputs using Moses script and evaluate the bilingual evaluation understudy (BLEU) score using sacreBLEU (Post, 2018). We select the model using South Korean and North Korean development data. In this study, in addition to the word level data of South Korean and North Korean as input languages, we use the four preprocessing methods, which are described in the following paragraphs and presented in Table 5. 3 Word (phoneme BPE) model. We decompose the words into phonemes and apply BPE. We set the merge operation to 30k and the frequency threshold to 10. We use hgtk (Hangul toolkit)4 for the decomposition into phonemes. Character (phoneme BPE) model. We perform the character level tokenization, decomposition in"
2020.acl-srw.11,P16-1162,0,0.0552991,"th Korean-English evaluation dataset will be published at the same address 2. Table 1 presents the percentage of sentences with grammatical differences between North Korean and South Korean evaluation data. From this table, we can see that the WS and ISR are the main grammatical differences between South Korean and North Korean. 4 Sent. 93,975 1,000 2,000 1,733 350 In addition, we retain the word or phrase boundary in the input sentence in this model. For example, when decomposing the sentence “롱구 는⬚운동” into phonemes, it is decomposed as “ㄹㅗㅇㄱㅜㄴㅡㄴ⬚ㅇㅜㄴㄷㅗㅇ.” By applying byte-pair encoding (BPE, Sennrich et al., 2016) to the sentence that has been decomposed into phonemes, it is possible to segment the sentence at the phoneme level while considering word or phrase boundaries. Korean Neural Machine Translation using Character Tokenization and Phoneme Decomposition We propose a method to tokenize input sentences into characters or decompose them into phonemes. Using this method, it is possible to reduce the influence of grammatical differences between South Korean and North Korean to train a machine translation model in North Korean using bilingual South Korean data. In the following South Korean or North Ko"
2020.acl-srw.11,P19-1021,0,0.0166202,"19 ISR 5.53±.05 9.28±.30 10.32±.31 9.62±.37 10.60±.16 Table 4: Evaluation of each model in South Korean / North Korean to English translation. These are BLEU scores of evaluation data set and WS and ISR subsets. These BLEU scores are the average of three models. The char (phonBPE) model achieved the highest scores in dev, test and two subsets. SK EN word word (charBPE) char word (phonBPE) char (phonBPE) word word (charBPE) Types 213,552 32,083 15,372 29,442 1,736 53,222 16,024 Tokens 1,567,469 2,057,155 4,231,099 2,091,575 4,316,529 2,297,744 2,494,763 Word (character BPE) model. According to Sennrich and Zhang (2019), we apply character level BPE to each of the South Korean, North Korean, and English sides that had been split with words. We set the merge operation to 30k and the frequency threshold to 10. For the following South Korean and North Korean preprocessing steps, the English side used only the word (character BPE) model. In addition to our re-implementation of Sennrich and Zhang (2019), we cite the BLEU score reported in their paper. Table 5: Data statistics after each preprocessing. 5 Character model. We perform character level tokenization. As for English and Hanja included in the South Korean"
2020.acl-srw.11,D17-1075,0,0.0254305,"ang et al. (2016) presented a method to extract the conversion rules between similar languages. Firat et al. (2016) proposed a many-to-many translation model with several encoders and decoders. However, the accuracy of a many-to-many translation model with a single shared encoder and decoder was found to be higher (Johnson et al., 2017). Finally, the translation accuracy was improved by preprocessing of the bilingual data. Zhang and Komachi (2018) demonstrated that higher translation accuracy can be obtained by decomposing Kanji into ideographic characters and strokes in Japanese-Chinese NMT. Stratos (2017) proposed a speech-parsing model for South Korean with character-level tokenization and decomposition into phonemes, demonstrating an improvement in the speech-parsing accuracy. 3 3.1 Word segmentation. South Korean and North Korean differ in the way to tokenize words containing formal and proper nouns and in quantitative expressions. For example, words are separated in both South Korean and North Korean when particles appear; however, they are not separated in North Korean if the next word after a particle is a formal noun. In Table 1, the word meaning “many things” is written as “많은 것” in So"
2020.acl-srw.11,P06-2112,0,0.0365989,"nslation of the North Korean language1 has seldom been performed. One of the reasons is the lack of large-scale bilingual data for training North Korean neural models. It is known that large-scale bilingual data are required to improve the translation accuracy of an NMT model. For example, one of the previous works suggests that an NMT system is less accurate than a phrase-based statistical machine translation system if there are no more than 100 million words in the bilingual training data (Koehn and Knowles, 2017). There are three approaches to solve low language resource bottleneck. First, Wang et al. (2006) proposed a method to train a translation model using a pivot language as an intermediate language. This approach translates from the source language to • Because there is no evaluation dataset between North Korean and English, we create a North Korean-English evaluation dataset by manually translating the South Korean-English bilingual evaluation dataset into a North Korean one. 1 Korean is a language mainly used in the Korean peninsula; however, there are some grammatical differences between the Republic of Korea and the Democratic People’s Republic of Korea. In this study, we refer to the K"
2020.acl-srw.11,J16-2004,0,0.0231309,"tion data rarely appear. The pivot language approach increases the translation error between the source language and the target language, because the translation model of each language is independently trained. Cheng et al. (2017) addressed this problem by allowing interaction during the translation model training. Moreover, Chen et al. (2017) proposed a method to train a source-to-target model using a pretrained teacher model as its guide. Marujo et al. (2011) proposed a rule-based method to convert similar languages into a target language to extend the language resources of the target side. Wang et al. (2016) presented a method to extract the conversion rules between similar languages. Firat et al. (2016) proposed a many-to-many translation model with several encoders and decoders. However, the accuracy of a many-to-many translation model with a single shared encoder and decoder was found to be higher (Johnson et al., 2017). Finally, the translation accuracy was improved by preprocessing of the bilingual data. Zhang and Komachi (2018) demonstrated that higher translation accuracy can be obtained by decomposing Kanji into ideographic characters and strokes in Japanese-Chinese NMT. Stratos (2017) pr"
2020.acl-srw.11,W18-6303,1,0.83119,"el as its guide. Marujo et al. (2011) proposed a rule-based method to convert similar languages into a target language to extend the language resources of the target side. Wang et al. (2016) presented a method to extract the conversion rules between similar languages. Firat et al. (2016) proposed a many-to-many translation model with several encoders and decoders. However, the accuracy of a many-to-many translation model with a single shared encoder and decoder was found to be higher (Johnson et al., 2017). Finally, the translation accuracy was improved by preprocessing of the bilingual data. Zhang and Komachi (2018) demonstrated that higher translation accuracy can be obtained by decomposing Kanji into ideographic characters and strokes in Japanese-Chinese NMT. Stratos (2017) proposed a speech-parsing model for South Korean with character-level tokenization and decomposition into phonemes, demonstrating an improvement in the speech-parsing accuracy. 3 3.1 Word segmentation. South Korean and North Korean differ in the way to tokenize words containing formal and proper nouns and in quantitative expressions. For example, words are separated in both South Korean and North Korean when particles appear; howeve"
2020.acl-srw.5,D19-5546,0,0.399169,"several studies have proposed models to solve grammatical error correction (GEC) task as an application of writing support for language learners of various languages, such as English or Russian. A standard approach to improve GEC models is to incorporate pseudo errors into large monolingual datasets for pretraining. In particular, previous works achieved state-of-the-art performance by pre-training the model using pseudo data with a subsequent ﬁnetuning of the pre-trained model using a learner corpus (Zhao et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019; Náplava and Straka, 2019; Grundkiewicz and Junczys-Dowmunt, 2019). Considering the aforementioned approach, several methods have been proposed for the generation of pseudo data for pre-training a GEC model. ∗ • We show the effect of realistic pseudo errors by considering the types of errors typically made by language learners for the Russian GEC task. 2 Related Works Pseudo data have been generated for GEC tasks in several previous works. Zhao et al. (2019) generated pseudo data by adding randomly generated pseudo errors, in an error-free sentence. In particular, in this approach, randomly selected words were replaced or deleted from a large monolingual dat"
2020.acl-srw.5,W19-4427,0,0.481424,"improves the performance of the GEC model. Recently, several studies have proposed models to solve grammatical error correction (GEC) task as an application of writing support for language learners of various languages, such as English or Russian. A standard approach to improve GEC models is to incorporate pseudo errors into large monolingual datasets for pretraining. In particular, previous works achieved state-of-the-art performance by pre-training the model using pseudo data with a subsequent ﬁnetuning of the pre-trained model using a learner corpus (Zhao et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019; Náplava and Straka, 2019; Grundkiewicz and Junczys-Dowmunt, 2019). Considering the aforementioned approach, several methods have been proposed for the generation of pseudo data for pre-training a GEC model. ∗ • We show the effect of realistic pseudo errors by considering the types of errors typically made by language learners for the Russian GEC task. 2 Related Works Pseudo data have been generated for GEC tasks in several previous works. Zhao et al. (2019) generated pseudo data by adding randomly generated pseudo errors, in an error-free sentence. In particular, in this approach, randomly s"
2020.acl-srw.5,D18-1541,0,0.095379,"o restrict word replacements made by learners in the resulting dataset. They used the conditional probability P (cor|err) based on the spellchecker distribution; however, it is not the same as P (err|cor), nor does it include error types other than spelling errors. Conversely, in our work, we approximate P (err|cor) using a uniform distribution for the set of candidates for a correct word. This uniform distribution is developed using prior knowledge of error types instead of that obtained from a spellchecker. Thus, our generated pseudo data contains comparatively more realistic pseudo errors. Kasewa et al. (2018) determined the distribution of the pseudo error generation model P (err|cor) from parallel data obtained using a grammatical error detection task. Moreover, Grundkiewicz and Junczys-Dowmunt (2019) developed a confusion set that retained out-of-vocabulary words and preserved consistent letter casing. However, using this approach, unrealistic errors might be included in the pseudo data because it primarily considers the surface of words. Further, Náplava and Straka (2019) conducted a GEC experiment in multiple lan28 guages, such as English, Russian, German, and Czech, and proposed a pseudo erro"
2020.acl-srw.5,D19-1119,0,0.0677594,"ted monolingual data improves the performance of the GEC model. Recently, several studies have proposed models to solve grammatical error correction (GEC) task as an application of writing support for language learners of various languages, such as English or Russian. A standard approach to improve GEC models is to incorporate pseudo errors into large monolingual datasets for pretraining. In particular, previous works achieved state-of-the-art performance by pre-training the model using pseudo data with a subsequent ﬁnetuning of the pre-trained model using a learner corpus (Zhao et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019; Náplava and Straka, 2019; Grundkiewicz and Junczys-Dowmunt, 2019). Considering the aforementioned approach, several methods have been proposed for the generation of pseudo data for pre-training a GEC model. ∗ • We show the effect of realistic pseudo errors by considering the types of errors typically made by language learners for the Russian GEC task. 2 Related Works Pseudo data have been generated for GEC tasks in several previous works. Zhao et al. (2019) generated pseudo data by adding randomly generated pseudo errors, in an error-free sentence. In particular, i"
2020.acl-srw.5,C12-2084,1,0.595238,"d in the dictionary. 3.2 Error Types Figure 1 shows an example of pseudo error generation according to the most common error types in learners’ corpora. As an example of preposition errors, we limit the confusion set by deﬁning the pseudo error generation model as P (err|cor = “to”) where err ∈ {about, by, for, from, in, of, with, on, at}. The pseudo error is generated using a uniform distribution for the pseudo error generation model P (err|cor). 4 Experiments 4.1 Data Table 2 lists the details of monolingual and parallel data used for training in our study. As training data, we used Lang-8 (Mizumoto et al., 2012) and NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) for English, while we used Lang-8 and Russian Learner Corpus of Academic Writing-GEC (RULEC-GEC) (Rozovskaya and Roth, 2019) for Russian. As pre-training data (i.e., pseudo data), we used One Billion Corpus 2 for English and Russian News Crawl 3 for Russian. English. As listed in Table 1, the common error types in English are those related to article/determiner, collocation/idiom, noun number, preposition, and word form. Thus, for English, we consider each error type as follows: • For article/determiner errors, the set of repl"
2020.acl-srw.5,P10-2041,0,0.0486904,"pretraining a GEC model. In this study, we combine the proposed method of pseudo data generation with previous methods. In particular, we incorporate the basic random approach (deletion, insertion, swapping) in our approach, as well as the more recent sophisticated approach proposed by Grundkiewicz et al. (2019) (character level perturb, confusion set based on an unsupervised spellchecker). 3.1 Data Selection We assume that the sentences, where errors of the learners’ error types are added, should be similar to that of the learners’ sentences themselves. Thus, we used a data selection method (Moore and Lewis, 2010), where an N-gram language model (LM) is used to score input sentences. This method creates a generic LM N and targets LM I sets for the generic and target domains, respectively. Subsequently, the entropy H is calculated for the sentence s in monolingual data from these LM sets (LMmodel ∈ {I, N }). Finally, the entropy difference (Equation 1) for the sentence is calculated. Data selection is then performed based on the similarity to the target domain in descending order of the assigned score. Lang. score(s) = H(s; N ) − H(s; I) (1) 1 H(s; LMmodel ) = − log PLMmodel (s) |s| where |s |indicates"
2020.acl-srw.5,D19-5545,0,0.0673,"the GEC model. Recently, several studies have proposed models to solve grammatical error correction (GEC) task as an application of writing support for language learners of various languages, such as English or Russian. A standard approach to improve GEC models is to incorporate pseudo errors into large monolingual datasets for pretraining. In particular, previous works achieved state-of-the-art performance by pre-training the model using pseudo data with a subsequent ﬁnetuning of the pre-trained model using a learner corpus (Zhao et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019; Náplava and Straka, 2019; Grundkiewicz and Junczys-Dowmunt, 2019). Considering the aforementioned approach, several methods have been proposed for the generation of pseudo data for pre-training a GEC model. ∗ • We show the effect of realistic pseudo errors by considering the types of errors typically made by language learners for the Russian GEC task. 2 Related Works Pseudo data have been generated for GEC tasks in several previous works. Zhao et al. (2019) generated pseudo data by adding randomly generated pseudo errors, in an error-free sentence. In particular, in this approach, randomly selected words were replace"
2020.acl-srw.5,W13-3601,0,0.117844,"an entry of “no article” as well (i.e., deletion). 4.2 Experimental Setting • For noun number errors, the error can be generated by swapping the singular or plural form of a noun with the plural or singular form, respectively. We used the transformer model with copyaugmented architecture (Zhao et al., 2019) as the GEC model with almost the same hyperparameters. In particular, we set max-epoch = 3 for pretraining, and 15 for training. As an evaluation metric, we computed the precision, recall, and F0.5 score for the CoNLL-2014 dataset and RULECGEC test set. Furthermore, we used the CoNLL2013 (Ng et al., 2013) data and the RULEC-GEC dev data for development. • For preposition errors, we deﬁne a candidate set as the top 10 most frequently used prepositions (Bryant and Briscoe, 2018). We only replace the preposition with one from the candidate sets. • For word form errors, we deﬁne a candidate set for replacement using word_forms 1 . 2 https://www.statmt.org/lm-benchmark/ http://www.statmt.org/wmt18/ translation-task.html 1 3 https://github.com/gutfeeling/ word_forms 29 CoNLL-2014 (En) RULEC-GEC test (Ru) Pseudo data Prec. Rec. F0.5 Prec. Rec. F0.5 Random errors w/o Data selection (baseline) 10M 67.5"
2020.acl-srw.5,Q19-1001,0,0.0248694,", we limit the confusion set by deﬁning the pseudo error generation model as P (err|cor = “to”) where err ∈ {about, by, for, from, in, of, with, on, at}. The pseudo error is generated using a uniform distribution for the pseudo error generation model P (err|cor). 4 Experiments 4.1 Data Table 2 lists the details of monolingual and parallel data used for training in our study. As training data, we used Lang-8 (Mizumoto et al., 2012) and NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) for English, while we used Lang-8 and Russian Learner Corpus of Academic Writing-GEC (RULEC-GEC) (Rozovskaya and Roth, 2019) for Russian. As pre-training data (i.e., pseudo data), we used One Billion Corpus 2 for English and Russian News Crawl 3 for Russian. English. As listed in Table 1, the common error types in English are those related to article/determiner, collocation/idiom, noun number, preposition, and word form. Thus, for English, we consider each error type as follows: • For article/determiner errors, the set of replacement candidates is the entire vocabulary in the random baseline. However, we limit the set of replacement candidates to other articles and determiners only. This set contains an entry of “n"
2020.acl-srw.5,N19-1014,0,0.365621,"sing randomly selected monolingual data improves the performance of the GEC model. Recently, several studies have proposed models to solve grammatical error correction (GEC) task as an application of writing support for language learners of various languages, such as English or Russian. A standard approach to improve GEC models is to incorporate pseudo errors into large monolingual datasets for pretraining. In particular, previous works achieved state-of-the-art performance by pre-training the model using pseudo data with a subsequent ﬁnetuning of the pre-trained model using a learner corpus (Zhao et al., 2019; Kiyono et al., 2019; Grundkiewicz et al., 2019; Náplava and Straka, 2019; Grundkiewicz and Junczys-Dowmunt, 2019). Considering the aforementioned approach, several methods have been proposed for the generation of pseudo data for pre-training a GEC model. ∗ • We show the effect of realistic pseudo errors by considering the types of errors typically made by language learners for the Russian GEC task. 2 Related Works Pseudo data have been generated for GEC tasks in several previous works. Zhao et al. (2019) generated pseudo data by adding randomly generated pseudo errors, in an error-free sente"
2020.acl-srw.5,W18-0529,0,0.0237034,"orm of a noun with the plural or singular form, respectively. We used the transformer model with copyaugmented architecture (Zhao et al., 2019) as the GEC model with almost the same hyperparameters. In particular, we set max-epoch = 3 for pretraining, and 15 for training. As an evaluation metric, we computed the precision, recall, and F0.5 score for the CoNLL-2014 dataset and RULECGEC test set. Furthermore, we used the CoNLL2013 (Ng et al., 2013) data and the RULEC-GEC dev data for development. • For preposition errors, we deﬁne a candidate set as the top 10 most frequently used prepositions (Bryant and Briscoe, 2018). We only replace the preposition with one from the candidate sets. • For word form errors, we deﬁne a candidate set for replacement using word_forms 1 . 2 https://www.statmt.org/lm-benchmark/ http://www.statmt.org/wmt18/ translation-task.html 1 3 https://github.com/gutfeeling/ word_forms 29 CoNLL-2014 (En) RULEC-GEC test (Ru) Pseudo data Prec. Rec. F0.5 Prec. Rec. F0.5 Random errors w/o Data selection (baseline) 10M 67.5 34.1 56.5 22.7 3.6 11.1 Random errors w/ Data selection 2M 4M 6M 8M 10M 67.9 68.0 67.4 68.9 68.2 31.1 32.5 33.7 34.3 34.9 54.9 55.8 56.2 57.3 57.3 18.7 19.2 20.5 25.3 27.7 0."
2020.acl-srw.5,W13-1703,0,0.119261,"pseudo error generation according to the most common error types in learners’ corpora. As an example of preposition errors, we limit the confusion set by deﬁning the pseudo error generation model as P (err|cor = “to”) where err ∈ {about, by, for, from, in, of, with, on, at}. The pseudo error is generated using a uniform distribution for the pseudo error generation model P (err|cor). 4 Experiments 4.1 Data Table 2 lists the details of monolingual and parallel data used for training in our study. As training data, we used Lang-8 (Mizumoto et al., 2012) and NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) for English, while we used Lang-8 and Russian Learner Corpus of Academic Writing-GEC (RULEC-GEC) (Rozovskaya and Roth, 2019) for Russian. As pre-training data (i.e., pseudo data), we used One Billion Corpus 2 for English and Russian News Crawl 3 for Russian. English. As listed in Table 1, the common error types in English are those related to article/determiner, collocation/idiom, noun number, preposition, and word form. Thus, for English, we consider each error type as follows: • For article/determiner errors, the set of replacement candidates is the entire vocabulary in the random baseline."
2020.coling-main.193,P15-1068,0,0.105886,"Our beam search method adjusts the search token in the beam according to the probability that the prediction is copied from the source sentence. The experimental results show that our proposed method generates more diverse corrections than existing methods without losing accuracy in the GEC task. 1 Introduction Grammatical error correction (GEC) is a task that corrects grammatical errors in an input text. Depending on the input, there are multiple ways to correct such text. For example, 10 annotators can produce 10 different valid correction results for the same grammatically incorrect text (Bryant and Ng, 2015). If a GEC model presents multiple candidates for correction, it helps the user decide whether to utilize the correction results such that the user can select a favorite correct expression from among the candidates. However, currently existing GEC models do not consider the generation of multiple correction candidates. Generally, in GEC, the method for obtaining multiple corrections involves the use of a plain beam search to generate the n-best candidates (Grundkiewicz et al., 2019; Kaneko et al., 2020). However, it has been shown that a plain beam search does not provide a great enough variet"
2020.coling-main.193,N12-1067,0,0.271375,"Missing"
2020.coling-main.193,W13-1703,0,0.123264,"al., 2018) approach, wherein the number of groups is defined from the number of desired diverse sentences, and a diversity strength for the beam search is selected such that the output tokens at each time step in each group differs. We set the number of groups to n of n-best and the diversity strength to 0.7 for diverse global beam search. For diverse local beam search, we used β = 1.0 and λ = 4.0 for diverse local beam search1 . 4.2 Datasets For models that have been pre-trained with publicly available pseudo-data, we fine-tuned them using published training data2 . We used the public NUCLE (Dahlmeier et al., 2013), Lang-8 (Mizumoto et al., 2011), and FCE (Yannakoudakis et al., 2011) corpora as our training data. We used the JFLEG test set and dev set corrected by four different annotators (Napoles et al., 2017) as the development set. We also used the CoNLL-2014 dataset as the test set. The original CoNLL-2014 dataset was corrected by two different annotators (Ng et al., 2014). However, in this work, eight corrections made by (Bryant and Ng, 2015) and four corrections with minimal corrections made by (Sakaguchi et al., 2016) were also used as references. 4.3 Evaluation Performance of GEC (G-score). We"
2020.coling-main.193,W19-4427,0,0.0857128,"or example, 10 annotators can produce 10 different valid correction results for the same grammatically incorrect text (Bryant and Ng, 2015). If a GEC model presents multiple candidates for correction, it helps the user decide whether to utilize the correction results such that the user can select a favorite correct expression from among the candidates. However, currently existing GEC models do not consider the generation of multiple correction candidates. Generally, in GEC, the method for obtaining multiple corrections involves the use of a plain beam search to generate the n-best candidates (Grundkiewicz et al., 2019; Kaneko et al., 2020). However, it has been shown that a plain beam search does not provide a great enough variety of candidates and produces lists of nearly identical sequences (Vijayakumar et al., 2018). Therefore, the n-best candidates generated by a beam search without diversity control are not expected to provide useful additional information. Considering this problem, several beam search methods have been proposed to generate diverse candidates (Li et al., 2016; Vijayakumar et al., 2018; Kulikov et al., 2019). These diverse beam search methods encourage diversity by globally rewriting a"
2020.coling-main.193,P19-2020,1,0.844366,"t al., 2018; Kulikov et al., 2019). These diverse beam search methods encourage diversity by globally rewriting all tokens in a sentence. We will refer to such methods as diverse global beam search methods. Conversely, considering a local sequence transduction task in GEC, wherein most of the tokens in the source and target sentences overlap, excessive correction of the input sentence is not preferred because unnecessary rewriting damages the grammatically correct parts of the input sentence. Furthermore, encouraging more corrections than necessary decreases the performance of the GEC itself (Hotate et al., 2019). We hypothesize that both plain beam search and diverse global beam search methods may not be suitable for GEC tasks, and a GEC model must correct the grammatical errors of the input sentence in diverse ways while preserving the correct portions of the sentence. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2132 Proceedings of the 28th International Conference on Computational Linguistics, pages 2132–2137 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: Illustration of error correcti"
2020.coling-main.193,2020.acl-main.391,1,0.832052,"an produce 10 different valid correction results for the same grammatically incorrect text (Bryant and Ng, 2015). If a GEC model presents multiple candidates for correction, it helps the user decide whether to utilize the correction results such that the user can select a favorite correct expression from among the candidates. However, currently existing GEC models do not consider the generation of multiple correction candidates. Generally, in GEC, the method for obtaining multiple corrections involves the use of a plain beam search to generate the n-best candidates (Grundkiewicz et al., 2019; Kaneko et al., 2020). However, it has been shown that a plain beam search does not provide a great enough variety of candidates and produces lists of nearly identical sequences (Vijayakumar et al., 2018). Therefore, the n-best candidates generated by a beam search without diversity control are not expected to provide useful additional information. Considering this problem, several beam search methods have been proposed to generate diverse candidates (Li et al., 2016; Vijayakumar et al., 2018; Kulikov et al., 2019). These diverse beam search methods encourage diversity by globally rewriting all tokens in a sentenc"
2020.coling-main.193,W19-8609,0,0.118962,"involves the use of a plain beam search to generate the n-best candidates (Grundkiewicz et al., 2019; Kaneko et al., 2020). However, it has been shown that a plain beam search does not provide a great enough variety of candidates and produces lists of nearly identical sequences (Vijayakumar et al., 2018). Therefore, the n-best candidates generated by a beam search without diversity control are not expected to provide useful additional information. Considering this problem, several beam search methods have been proposed to generate diverse candidates (Li et al., 2016; Vijayakumar et al., 2018; Kulikov et al., 2019). These diverse beam search methods encourage diversity by globally rewriting all tokens in a sentence. We will refer to such methods as diverse global beam search methods. Conversely, considering a local sequence transduction task in GEC, wherein most of the tokens in the source and target sentences overlap, excessive correction of the input sentence is not preferred because unnecessary rewriting damages the grammatically correct parts of the input sentence. Furthermore, encouraging more corrections than necessary decreases the performance of the GEC itself (Hotate et al., 2019). We hypothesi"
2020.coling-main.193,I11-1017,1,0.76168,"number of groups is defined from the number of desired diverse sentences, and a diversity strength for the beam search is selected such that the output tokens at each time step in each group differs. We set the number of groups to n of n-best and the diversity strength to 0.7 for diverse global beam search. For diverse local beam search, we used β = 1.0 and λ = 4.0 for diverse local beam search1 . 4.2 Datasets For models that have been pre-trained with publicly available pseudo-data, we fine-tuned them using published training data2 . We used the public NUCLE (Dahlmeier et al., 2013), Lang-8 (Mizumoto et al., 2011), and FCE (Yannakoudakis et al., 2011) corpora as our training data. We used the JFLEG test set and dev set corrected by four different annotators (Napoles et al., 2017) as the development set. We also used the CoNLL-2014 dataset as the test set. The original CoNLL-2014 dataset was corrected by two different annotators (Ng et al., 2014). However, in this work, eight corrections made by (Bryant and Ng, 2015) and four corrections with minimal corrections made by (Sakaguchi et al., 2016) were also used as references. 4.3 Evaluation Performance of GEC (G-score). We evaluated each decoding method u"
2020.coling-main.193,E17-2037,0,0.134968,"e step in each group differs. We set the number of groups to n of n-best and the diversity strength to 0.7 for diverse global beam search. For diverse local beam search, we used β = 1.0 and λ = 4.0 for diverse local beam search1 . 4.2 Datasets For models that have been pre-trained with publicly available pseudo-data, we fine-tuned them using published training data2 . We used the public NUCLE (Dahlmeier et al., 2013), Lang-8 (Mizumoto et al., 2011), and FCE (Yannakoudakis et al., 2011) corpora as our training data. We used the JFLEG test set and dev set corrected by four different annotators (Napoles et al., 2017) as the development set. We also used the CoNLL-2014 dataset as the test set. The original CoNLL-2014 dataset was corrected by two different annotators (Ng et al., 2014). However, in this work, eight corrections made by (Bryant and Ng, 2015) and four corrections with minimal corrections made by (Sakaguchi et al., 2016) were also used as references. 4.3 Evaluation Performance of GEC (G-score). We evaluated each decoding method using the GLEU score (Napoles et al., 2017) for the JFLEG and the F0.5 score by using the MaxMatch scorer (Dahlmeier and Ng, 2012) for the CoNLL-2014 test set as general"
2020.coling-main.193,W14-1701,0,0.0729648,"sed β = 1.0 and λ = 4.0 for diverse local beam search1 . 4.2 Datasets For models that have been pre-trained with publicly available pseudo-data, we fine-tuned them using published training data2 . We used the public NUCLE (Dahlmeier et al., 2013), Lang-8 (Mizumoto et al., 2011), and FCE (Yannakoudakis et al., 2011) corpora as our training data. We used the JFLEG test set and dev set corrected by four different annotators (Napoles et al., 2017) as the development set. We also used the CoNLL-2014 dataset as the test set. The original CoNLL-2014 dataset was corrected by two different annotators (Ng et al., 2014). However, in this work, eight corrections made by (Bryant and Ng, 2015) and four corrections with minimal corrections made by (Sakaguchi et al., 2016) were also used as references. 4.3 Evaluation Performance of GEC (G-score). We evaluated each decoding method using the GLEU score (Napoles et al., 2017) for the JFLEG and the F0.5 score by using the MaxMatch scorer (Dahlmeier and Ng, 2012) for the CoNLL-2014 test set as general evaluation metrics for the GEC. Diversity of corrections (C-score). To evaluate the diversity of corrections, we calculated the coverage score between the n-best candida"
2020.coling-main.193,Q16-1013,0,0.0156405,"we fine-tuned them using published training data2 . We used the public NUCLE (Dahlmeier et al., 2013), Lang-8 (Mizumoto et al., 2011), and FCE (Yannakoudakis et al., 2011) corpora as our training data. We used the JFLEG test set and dev set corrected by four different annotators (Napoles et al., 2017) as the development set. We also used the CoNLL-2014 dataset as the test set. The original CoNLL-2014 dataset was corrected by two different annotators (Ng et al., 2014). However, in this work, eight corrections made by (Bryant and Ng, 2015) and four corrections with minimal corrections made by (Sakaguchi et al., 2016) were also used as references. 4.3 Evaluation Performance of GEC (G-score). We evaluated each decoding method using the GLEU score (Napoles et al., 2017) for the JFLEG and the F0.5 score by using the MaxMatch scorer (Dahlmeier and Ng, 2012) for the CoNLL-2014 test set as general evaluation metrics for the GEC. Diversity of corrections (C-score). To evaluate the diversity of corrections, we calculated the coverage score between the n-best candidates and references. We used the weighted recall, which was used as the evaluation metric in the 2020 Duolingo Shared Task3 as a coverage score. In this"
2020.coling-main.193,P11-1019,0,0.0460196,"the number of desired diverse sentences, and a diversity strength for the beam search is selected such that the output tokens at each time step in each group differs. We set the number of groups to n of n-best and the diversity strength to 0.7 for diverse global beam search. For diverse local beam search, we used β = 1.0 and λ = 4.0 for diverse local beam search1 . 4.2 Datasets For models that have been pre-trained with publicly available pseudo-data, we fine-tuned them using published training data2 . We used the public NUCLE (Dahlmeier et al., 2013), Lang-8 (Mizumoto et al., 2011), and FCE (Yannakoudakis et al., 2011) corpora as our training data. We used the JFLEG test set and dev set corrected by four different annotators (Napoles et al., 2017) as the development set. We also used the CoNLL-2014 dataset as the test set. The original CoNLL-2014 dataset was corrected by two different annotators (Ng et al., 2014). However, in this work, eight corrections made by (Bryant and Ng, 2015) and four corrections with minimal corrections made by (Sakaguchi et al., 2016) were also used as references. 4.3 Evaluation Performance of GEC (G-score). We evaluated each decoding method using the GLEU score (Napoles et al., 2"
2020.coling-main.193,N19-1014,0,0.0546925,"ctions. 3 Diverse local beam search Diverse local beam search encourages candidates of diverse corrections for parts of the input sentence that must be corrected and discourages candidates for already correct parts of the input sentence. Consequently, it runs the computation for fewer parts to generate diverse candidates. For this purpose, a penalty score sb,t is assigned to each beam b at each time step t, indicating whether a correction should be made. Although different methods can be used to calculate a penalty score, in this study, we use a copy probability from the copy-augmented model (Zhao et al., 2019) as a penalty score sb,t . We explain the copy-augmented model in greater detail in Section 4.1. Using the penalty score, we penalize the beam search score k as follows: kb,t = (λsb,t + β) log pb,t (1) 2133 where p is the output distribution of the GEC model. β and λ are hyperparameters, where β prevents the penalty from falling to zero and λ determines the strength of the penalty. 4 Experiments 4.1 Model We used the copy-augmented model (Zhao et al., 2019) as the GEC model. This model controls the balance between the copy distribution pcopy and generation distribution pgen via the balancing f"
2020.coling-main.415,K19-1035,0,0.0195775,"dialog responses. For question answering, Lee and Lee (2019) proposed a cross-lingual transfer learning method that uses generative adversarial networks. These studies focused on the tasks for which semantic information is more important, as opposed to GEC, for which grammatical information is the key factor. Various studies have analyzed the transfer of syntactic knowledge between languages. Kim et al. (2017) proposed a part-of-speech tagging method for learning language-independent and languagedependent expressions between languages by combining two models corresponding to the expressions. Ahmad et al. (2019) utilized adversarial training to train contextual encoders that produce invariant representations across languages, thereby facilitating cross-lingual transfers for dependency parsing. Wu 4705 Figure 1: Overall training steps. and Dredze (2019) used multilingual BERT for ﬁve tasks, such as POS tagging and dependency parsing, and demonstrated that its performance can be improved by using multilingual knowledge. As in our study, these studies perform transfer learning between languages in tasks for which syntactic information is important. However, it is not clear whether linguistic knowledge a"
2020.coling-main.415,W18-6111,0,0.0169718,"e 3. In this study, we use WMT-2019’s News Crawl 2 and Japanese Wikipedia data 3 as monolingual data for training the MLM, and we used TED talks (Cettolo et al., 2012) 4 as parallel data for training the TLM. The development and test data for MLM are extracted from each monolingual dataset, excluding the training data. The development and test data for TLM include data from TED Talks, in addition to the data for MLM. We use RULEC-GEC (Ru) (Rozovskaya and Roth, 2019) 5 , Lang-8 6 , NUCLE (En) (Dahlmeier et al., 2013) 7 , AKCES-GEC (Cs) (N´aplava and Straka, 2019) 8 , and Falko-MERLIN-GEC (De) (Boyd, 2018) 9 as the learner corpora for training the GEC model 10 . For the development and test data for GEC, we use the Russian, Czech, and German data attached to each corpus. We use English data from CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 and Japanese data from the NAIST Goyo Corpus (Oyama et al., 2013) for the development and test data 11 . We also use Russian News Crawl (2015–2018), Czech News Crawl (2014–2018), and English News Crawl (2015–2018) to train the language model for re-ranking the GEC model. The TED Talks data are reconstructed from the original English translation data by extract"
2020.coling-main.415,P17-1074,0,0.0288703,"the numbers of errors in the Lang-8-En corpus, excluding error types whose number of errors in the test data is less than or equal to 50. are marked by case systems. Only the TLM model transferred from Czech generates the correct output. We hypothesize that the reason that Adj:case and Noun:case errors are corrected into prepositional cases is because our model captures the grammatical information of Czech, which is also useful in Russian. 5.3 Size of Target Language Data In this subsection, we analyze the effect of the size of the target language data on transfer learning. We use ERRANT 17 (Bryant et al., 2017) to annotate the training and evaluation data with error types and analyze the knowledge that is effective (i.e., transferable) in transfer learning to a high-resource target. Table 9 shows the recall results of the English GEC models on the top-ﬁve error types (determiner, preposition, punctuation, verb, and verb tense) and the bottom-ﬁve error types (spelling, pronoun, verb form, morphology, and subject-verb agreement) in terms of the numbers of errors in the Lang-8-En corpus. The results of “NUCLE only,” wherein the target is at a low-resource setting, demonstrate that the model that uses t"
2020.coling-main.415,2012.eamt-1.60,0,0.0109921,"nolingual News Crawl (to train a language model for re-ranking) News Crawl (18) Wikipedia 33M 1.2M 1.2M 2.5K 2.5K - Learner corpora RULEC-GEC (Ru) Lang-8-Ru NUCLE (En) CoNLL 2013 (En) CoNLL 2014 (En) Lang-8-En AKCES-GEC (Cs) Falko-MERLIN-GEC (De) Lang-8-De Lang-8-Ja NAIST Goyo Corpus (Ja) 5K 49K 57K 1.3M 40K 15K 39K 54K - 2.5K 1.4K 2.5K 2.5K 3.3K 5K 1.3K 2.4K - Table 3: GEC data overview. 4.2 Data The data used in the experiment are presented in Table 3. In this study, we use WMT-2019’s News Crawl 2 and Japanese Wikipedia data 3 as monolingual data for training the MLM, and we used TED talks (Cettolo et al., 2012) 4 as parallel data for training the TLM. The development and test data for MLM are extracted from each monolingual dataset, excluding the training data. The development and test data for TLM include data from TED Talks, in addition to the data for MLM. We use RULEC-GEC (Ru) (Rozovskaya and Roth, 2019) 5 , Lang-8 6 , NUCLE (En) (Dahlmeier et al., 2013) 7 , AKCES-GEC (Cs) (N´aplava and Straka, 2019) 8 , and Falko-MERLIN-GEC (De) (Boyd, 2018) 9 as the learner corpora for training the GEC model 10 . For the development and test data for GEC, we use the Russian, Czech, and German data attached to"
2020.coling-main.415,D16-1195,0,0.0160859,"for which syntactic information is important. However, it is not clear whether linguistic knowledge about grammatical errors can be transferred across languages. Several GEC studies using L1 information have been conducted. Rozovskaya and Roth (2011) adopted information from ﬁve L1s with different priorities to preposition correction using the na¨ıve Bayes classiﬁer. Rozovskaya et al. (2017) extended this method to eleven L1s and three error types. Mizumoto et al. (2011) demonstrated that using the same L1 for training and test data in an SMT-based GEC system improved the system performance. Chollampatt et al. (2016) extended this method by incorporating three different L1 neural language models into an SMT-based GEC model as features to adapt to each L1. In these studies, GEC was performed considering the L1 information; however, unlike our study, the objective of these studies did not include the transfer of grammatical knowledge between languages. 3 GEC using Cross-lingual Transfer Learning 3.1 Overall Training Steps We train a GEC model that employs the Masked Language Modeling (MLM) / Translation Language Modeling (TLM) (Conneau and Lample, 2019) shown in Subsection 3.2, and the transfer learning met"
2020.coling-main.415,D17-1078,0,0.0206015,"in the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such setting involves cross-lingual transfer learning (Zoph et al., 2016), which aims to improve the accuracy of low-resource target models using knowledge from high-resource source models. The similarities between these languages is a key factor for successfully transferring grammatical knowledge (Cotterell and Heigold, 2017; Johnson et al., 2017). For example, languages within the same language family share several rules of grammar and nuances of vocabulary, which aid the learning process of the target models. However, thus far, no study has investigated the use of cross-lingual transfer learning for GEC from other languages; therefore, it is unclear if useful grammatical knowledge (e.g., case inﬂection or conjugation) can be transferred. Table 1 shows example case inﬂections of words that mean “sister” in English, Russian, and Czech. In English, the difference between nominative and genitive is marked by the su"
2020.coling-main.415,D19-1146,0,0.0180064,"ng Method for GEC In this study, we investigate whether grammatical knowledge can be transferred in GEC using crosslingual transfer learning. Various methods are utilized for cross-lingual transfer learning, as discussed in Section 2. In this study, we focused on sharing both lexical and grammatical knowledge between languages. Thus, we use the EncDec model to facilitate transfer learning. Several studies on NMT have demonstrated that training a source model on a combination of different languages is effective when performing ﬁne-tuning on low-resource language pairs (Imankulova et al., 2019; Dabre et al., 2019). Therefore, we train the GEC models by concatenating the learner data from both the source and the target languages, wherein each batch may consist of tokens from the two languages; subsequently, we ﬁne-tune the models using the learner data of the target language. Finally, the outputs of all models are re-ranked, as proposed by Chollampatt et al. (2018). 4 Experiments 4.1 Languages In this study, we perform experiments with GEC on three target languages: Russian, Czech, and English. For each target language, we use three source languages: one with high similarity, one with moderate similarit"
2020.coling-main.415,N12-1067,0,0.038311,"r encoding (Sennrich et al., 2016) using fastBPE 15 . 4.3 Settings We use the same architecture as Conneau and Lample (2019) for the MLM/TLM and transformer encoder and decoder for GEC models. Both the encoder and the decoder of the GEC model are initialized with the parameters of MLM/TLM. The number of layers in the model is six, the dimension of the hidden and embedding layers is 1,024, the batch size is 32, and a dropout with a probability of 0.1 is applied. The best model is selected using perplexity on the development data. We report the precision, recall, and F0.5 scores using m2scorer (Dahlmeier and Ng, 2012) for the test data. 4.4 Baseline In this study, we use two baselines to compare the effects of transfer learning and the MLM/TLM. PLAIN In this setting, we do not use the MLM/TLM. Therefore, the GEC model learns grammatical knowledge from the learner corpus only. MLM {Ru,Cs,En}-only In this setting, we pre-train the MLM with the target language monolingual corpus only and train the GEC model with the target language learner corpus only. This model learns grammatical knowledge from the large-scale target language monolingual corpus and learner corpus; it does not use knowledge of other language"
2020.coling-main.415,W13-1703,0,0.0276217,"1.3K 2.4K - Table 3: GEC data overview. 4.2 Data The data used in the experiment are presented in Table 3. In this study, we use WMT-2019’s News Crawl 2 and Japanese Wikipedia data 3 as monolingual data for training the MLM, and we used TED talks (Cettolo et al., 2012) 4 as parallel data for training the TLM. The development and test data for MLM are extracted from each monolingual dataset, excluding the training data. The development and test data for TLM include data from TED Talks, in addition to the data for MLM. We use RULEC-GEC (Ru) (Rozovskaya and Roth, 2019) 5 , Lang-8 6 , NUCLE (En) (Dahlmeier et al., 2013) 7 , AKCES-GEC (Cs) (N´aplava and Straka, 2019) 8 , and Falko-MERLIN-GEC (De) (Boyd, 2018) 9 as the learner corpora for training the GEC model 10 . For the development and test data for GEC, we use the Russian, Czech, and German data attached to each corpus. We use English data from CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 and Japanese data from the NAIST Goyo Corpus (Oyama et al., 2013) for the development and test data 11 . We also use Russian News Crawl (2015–2018), Czech News Crawl (2014–2018), and English News Crawl (2015–2018) to train the language model for re-ranking the GEC model."
2020.coling-main.415,W19-4427,0,0.0811864,"ges has a signiﬁcant impact on the accuracy of correcting certain types of errors. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatically incorrect sentences. The demand for GEC has grown signiﬁcantly in recent decades because of the increasing opportunities for cross-cultural collaboration. Previous studies in the literature primarily focused on improving automated GEC for the English language. Thus, because of the large amount of data available for training, several machine-learning-based methods have achieved high scores in English GEC (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019; Kaneko et al., 2020). In recent years, researchers have started working on other languages, including Russian and Czech (Rozovskaya and Roth, 2019; N´aplava and Straka, 2019). However, for these languages, the language resources required to train the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such setting involves cross-lingu"
2020.coling-main.415,W19-6613,1,0.812878,"019). 3.3 Transfer-learning Method for GEC In this study, we investigate whether grammatical knowledge can be transferred in GEC using crosslingual transfer learning. Various methods are utilized for cross-lingual transfer learning, as discussed in Section 2. In this study, we focused on sharing both lexical and grammatical knowledge between languages. Thus, we use the EncDec model to facilitate transfer learning. Several studies on NMT have demonstrated that training a source model on a combination of different languages is effective when performing ﬁne-tuning on low-resource language pairs (Imankulova et al., 2019; Dabre et al., 2019). Therefore, we train the GEC models by concatenating the learner data from both the source and the target languages, wherein each batch may consist of tokens from the two languages; subsequently, we ﬁne-tune the models using the learner data of the target language. Finally, the outputs of all models are re-ranked, as proposed by Chollampatt et al. (2018). 4 Experiments 4.1 Languages In this study, we perform experiments with GEC on three target languages: Russian, Czech, and English. For each target language, we use three source languages: one with high similarity, one wi"
2020.coling-main.415,2020.acl-main.391,1,0.902496,"orrecting certain types of errors. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatically incorrect sentences. The demand for GEC has grown signiﬁcantly in recent decades because of the increasing opportunities for cross-cultural collaboration. Previous studies in the literature primarily focused on improving automated GEC for the English language. Thus, because of the large amount of data available for training, several machine-learning-based methods have achieved high scores in English GEC (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019; Kaneko et al., 2020). In recent years, researchers have started working on other languages, including Russian and Czech (Rozovskaya and Roth, 2019; N´aplava and Straka, 2019). However, for these languages, the language resources required to train the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such setting involves cross-lingual transfer learning (Zoph et al., 2016), w"
2020.coling-main.415,D17-1302,0,0.0194423,"by concatenating the training data from multiple languages. Schuster et al. (2018) presented a method that uses a bidirectional NMT encoder for cross-lingual contextual word representations, to generate dialog responses. For question answering, Lee and Lee (2019) proposed a cross-lingual transfer learning method that uses generative adversarial networks. These studies focused on the tasks for which semantic information is more important, as opposed to GEC, for which grammatical information is the key factor. Various studies have analyzed the transfer of syntactic knowledge between languages. Kim et al. (2017) proposed a part-of-speech tagging method for learning language-independent and languagedependent expressions between languages by combining two models corresponding to the expressions. Ahmad et al. (2019) utilized adversarial training to train contextual encoders that produce invariant representations across languages, thereby facilitating cross-lingual transfers for dependency parsing. Wu 4705 Figure 1: Overall training steps. and Dredze (2019) used multilingual BERT for ﬁve tasks, such as POS tagging and dependency parsing, and demonstrated that its performance can be improved by using mult"
2020.coling-main.415,D19-1119,0,0.0494118,"on the accuracy of correcting certain types of errors. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatically incorrect sentences. The demand for GEC has grown signiﬁcantly in recent decades because of the increasing opportunities for cross-cultural collaboration. Previous studies in the literature primarily focused on improving automated GEC for the English language. Thus, because of the large amount of data available for training, several machine-learning-based methods have achieved high scores in English GEC (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019; Kaneko et al., 2020). In recent years, researchers have started working on other languages, including Russian and Czech (Rozovskaya and Roth, 2019; N´aplava and Straka, 2019). However, for these languages, the language resources required to train the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such setting involves cross-lingual transfer learning"
2020.coling-main.415,I11-1017,1,0.85341,"Missing"
2020.coling-main.415,D19-5545,0,0.0414155,"Missing"
2020.coling-main.415,W13-3601,0,0.0280852,"lopment and test data for MLM are extracted from each monolingual dataset, excluding the training data. The development and test data for TLM include data from TED Talks, in addition to the data for MLM. We use RULEC-GEC (Ru) (Rozovskaya and Roth, 2019) 5 , Lang-8 6 , NUCLE (En) (Dahlmeier et al., 2013) 7 , AKCES-GEC (Cs) (N´aplava and Straka, 2019) 8 , and Falko-MERLIN-GEC (De) (Boyd, 2018) 9 as the learner corpora for training the GEC model 10 . For the development and test data for GEC, we use the Russian, Czech, and German data attached to each corpus. We use English data from CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 and Japanese data from the NAIST Goyo Corpus (Oyama et al., 2013) for the development and test data 11 . We also use Russian News Crawl (2015–2018), Czech News Crawl (2014–2018), and English News Crawl (2015–2018) to train the language model for re-ranking the GEC model. The TED Talks data are reconstructed from the original English translation data by extracting the corresponding sentence pairs in each language. News Crawl, Wikipedia, Lang-8, and NUCLE data are obtained by extracting the number of sentences depicted in Table 3 from the original data. To maintain a consistent e"
2020.coling-main.415,Y13-1014,1,0.81967,"luding the training data. The development and test data for TLM include data from TED Talks, in addition to the data for MLM. We use RULEC-GEC (Ru) (Rozovskaya and Roth, 2019) 5 , Lang-8 6 , NUCLE (En) (Dahlmeier et al., 2013) 7 , AKCES-GEC (Cs) (N´aplava and Straka, 2019) 8 , and Falko-MERLIN-GEC (De) (Boyd, 2018) 9 as the learner corpora for training the GEC model 10 . For the development and test data for GEC, we use the Russian, Czech, and German data attached to each corpus. We use English data from CoNLL 2013 (Ng et al., 2013) and CoNLL 2014 and Japanese data from the NAIST Goyo Corpus (Oyama et al., 2013) for the development and test data 11 . We also use Russian News Crawl (2015–2018), Czech News Crawl (2014–2018), and English News Crawl (2015–2018) to train the language model for re-ranking the GEC model. The TED Talks data are reconstructed from the original English translation data by extracting the corresponding sentence pairs in each language. News Crawl, Wikipedia, Lang-8, and NUCLE data are obtained by extracting the number of sentences depicted in Table 3 from the original data. To maintain a consistent experimental setting, the size of each source language data is adjusted to be the"
2020.coling-main.415,P11-1093,0,0.0181664,"y facilitating cross-lingual transfers for dependency parsing. Wu 4705 Figure 1: Overall training steps. and Dredze (2019) used multilingual BERT for ﬁve tasks, such as POS tagging and dependency parsing, and demonstrated that its performance can be improved by using multilingual knowledge. As in our study, these studies perform transfer learning between languages in tasks for which syntactic information is important. However, it is not clear whether linguistic knowledge about grammatical errors can be transferred across languages. Several GEC studies using L1 information have been conducted. Rozovskaya and Roth (2011) adopted information from ﬁve L1s with different priorities to preposition correction using the na¨ıve Bayes classiﬁer. Rozovskaya et al. (2017) extended this method to eleven L1s and three error types. Mizumoto et al. (2011) demonstrated that using the same L1 for training and test data in an SMT-based GEC system improved the system performance. Chollampatt et al. (2016) extended this method by incorporating three different L1 neural language models into an SMT-based GEC model as features to adapt to each L1. In these studies, GEC was performed considering the L1 information; however, unlike"
2020.coling-main.415,Q19-1001,0,0.250274,"lly incorrect sentences. The demand for GEC has grown signiﬁcantly in recent decades because of the increasing opportunities for cross-cultural collaboration. Previous studies in the literature primarily focused on improving automated GEC for the English language. Thus, because of the large amount of data available for training, several machine-learning-based methods have achieved high scores in English GEC (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019; Kaneko et al., 2020). In recent years, researchers have started working on other languages, including Russian and Czech (Rozovskaya and Roth, 2019; N´aplava and Straka, 2019). However, for these languages, the language resources required to train the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such setting involves cross-lingual transfer learning (Zoph et al., 2016), which aims to improve the accuracy of low-resource target models using knowledge from high-resource source models. The similari"
2020.coling-main.415,J17-4002,0,0.0212782,"for ﬁve tasks, such as POS tagging and dependency parsing, and demonstrated that its performance can be improved by using multilingual knowledge. As in our study, these studies perform transfer learning between languages in tasks for which syntactic information is important. However, it is not clear whether linguistic knowledge about grammatical errors can be transferred across languages. Several GEC studies using L1 information have been conducted. Rozovskaya and Roth (2011) adopted information from ﬁve L1s with different priorities to preposition correction using the na¨ıve Bayes classiﬁer. Rozovskaya et al. (2017) extended this method to eleven L1s and three error types. Mizumoto et al. (2011) demonstrated that using the same L1 for training and test data in an SMT-based GEC system improved the system performance. Chollampatt et al. (2016) extended this method by incorporating three different L1 neural language models into an SMT-based GEC model as features to adapt to each L1. In these studies, GEC was performed considering the L1 information; however, unlike our study, the objective of these studies did not include the transfer of grammatical knowledge between languages. 3 GEC using Cross-lingual Tra"
2020.coling-main.415,P16-1162,0,0.0101305,"ata. Thus, in total, 57K sentences are included in the English training data. We call this setting, “NUCLE only.” In the second setting, NUCLE and Lang-8-En are used as the training data for English. Thus, in total, 1.3M + 57K sentences are included in the English training data. We call this setting, “NUCLE + Lang-8-En.” To tokenize the Japanese sentences, we use MeCab 12 with the UniDic (v.2.1.1) dictionary. Other languages are tokenized using NLTK 13 . For the target language for GEC, we use pyspellchecker 14 to preprocess all data. Then we convert them into subwords via byte pair encoding (Sennrich et al., 2016) using fastBPE 15 . 4.3 Settings We use the same architecture as Conneau and Lample (2019) for the MLM/TLM and transformer encoder and decoder for GEC models. Both the encoder and the decoder of the GEC model are initialized with the parameters of MLM/TLM. The number of layers in the model is six, the dimension of the hidden and embedding layers is 1,024, the batch size is 32, and a dropout with a probability of 0.1 is applied. The best model is selected using perplexity on the development data. We report the precision, recall, and F0.5 scores using m2scorer (Dahlmeier and Ng, 2012) for the te"
2020.coling-main.415,D19-1077,0,0.0537374,"Missing"
2020.coling-main.415,N18-1057,0,0.0200388,"e types of errors than others, depending on the size of the target data. 2 Related Work Most recent GEC methods use the encoder-decoder (EncDec) model, which requires large-scale training data (Zhao et al., 2019; Grundkiewicz et al., 2019). Therefore, several studies created additional pseudodata in low-resource scenarios (N´aplava and Straka, 2019; Rozovskaya and Roth, 2019). For example, because it is easy to generate a grammatically incorrect sentence from a grammatically correct sentence, extensive research has been conducted on generating pseudo-data from large-scale monolingual corpora (Xie et al., 2018; Kiyono et al., 2019). In addition, the use of EncDec models pretrained with large-scale unlabeled data is known to be effective for GEC (Kaneko et al., 2020). These studies aimed to improve the performance of GEC using large-scale training data. Furthermore, research has also been conducted on the use of linguistic knowledge from other languages in neural machine translation (NMT). Zoph et al. (2016) proposed a method to ﬁne-tune NMT models trained from high-resource language pairs on low-resource language pairs. Johnson et al. (2017) demonstrated that a language can be translated with no tr"
2020.coling-main.415,N19-1014,0,0.108927,"ty to source languages has a signiﬁcant impact on the accuracy of correcting certain types of errors. 1 Introduction Grammatical error correction (GEC) is the task of correcting grammatically incorrect sentences. The demand for GEC has grown signiﬁcantly in recent decades because of the increasing opportunities for cross-cultural collaboration. Previous studies in the literature primarily focused on improving automated GEC for the English language. Thus, because of the large amount of data available for training, several machine-learning-based methods have achieved high scores in English GEC (Zhao et al., 2019; Grundkiewicz et al., 2019; Kiyono et al., 2019; Kaneko et al., 2020). In recent years, researchers have started working on other languages, including Russian and Czech (Rozovskaya and Roth, 2019; N´aplava and Straka, 2019). However, for these languages, the language resources required to train the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such s"
2020.coling-main.415,D16-1163,0,0.072854,"Kaneko et al., 2020). In recent years, researchers have started working on other languages, including Russian and Czech (Rozovskaya and Roth, 2019; N´aplava and Straka, 2019). However, for these languages, the language resources required to train the GEC models accurately are not sufﬁciently available. It is known that using high-resource languages as the source languages can improve the accuracy of deep neural models for low-resource target languages in various settings (Johnson et al., 2017; Ruder et al., 2019; Dabre et al., 2020). One such setting involves cross-lingual transfer learning (Zoph et al., 2016), which aims to improve the accuracy of low-resource target models using knowledge from high-resource source models. The similarities between these languages is a key factor for successfully transferring grammatical knowledge (Cotterell and Heigold, 2017; Johnson et al., 2017). For example, languages within the same language family share several rules of grammar and nuances of vocabulary, which aid the learning process of the target models. However, thus far, no study has investigated the use of cross-lingual transfer learning for GEC from other languages; therefore, it is unclear if useful gr"
2020.coling-main.573,I17-2058,0,0.0844909,"Missing"
2020.coling-main.573,W19-4406,0,0.0257381,"Missing"
2020.coling-main.573,S17-2001,0,0.0233278,"Furthermore, to verify the effectiveness of our dataset based on the GEC systems, we compared our metric with a BERT-based metric fine-tuned on datasets not based on the GEC systems. 5.1 Fine-tuning BERT SOME (BERT w/ existing data) The existing datasets described in Section 2 were used for grammaticality2 and fluency3 sub-metrics for fine-tuning BERT in the baseline method. For fluency, the dataset from the BNC was used for training the fluency, whereas the dataset from Wikipedia was used for development. For meaning preservation, we used the dataset7 of the Semantic Textual Similarity task (Cer et al., 2017), which evaluates the semantic similarity between two sentences using continuous values in [0.0, 5.0]. SOME (BERT w/ our data) Our dataset, introduced in Section 3, was divided into train/dev/test with 3,376/422/423 sentences and used for fine-tuning BERT8 , hyperparameter tuning, and intrinsic evaluation of each sub-metric, respectively. Refer to Appendix C for the hyperparameter settings. 5.2 Meta-Evaluation System-level meta-evaluation In the system level meta-evaluation, the average of the sentence scores was used as the system score , and the correlation coefficients with the manual evalu"
2020.coling-main.573,P18-1059,0,0.0117377,"e task of automatically correcting grammatically incorrect sentences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives of (i) grammaticality, (ii) fluency, and (iii) meaning preservation. However, the correlation with the manual evaluation of system output can be further improved because they are not considered for optimizing each sub-metric. To achieve a better correlation with manual evaluation, we create a dataset to optimize"
2020.coling-main.573,N12-1067,0,0.0229837,"error correction systems to optimize the metrics. Experimental results show that the proposed metric improves the correlation with manual evaluation in both systemand sentence-level meta-evaluation. Our dataset and metric will be made publicly available.1 1 Introduction Grammatical error correction (GEC) is the task of automatically correcting grammatically incorrect sentences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives o"
2020.coling-main.573,W14-3348,0,0.0189197,"mmaticaly: 3.8 Fluency: 3.8 Meaning: 1.6 Source text: The increasing longevity is due to fast development of the society so as the living pressure. System output: The increase in longevity is due to the fast development of society so as the living pressure. Grammaticaly: 2.6 Fluency: 2.4 Meaning: 3.8 Figure 1: Histogram of each manual evaluation and examples of annotation. of grammaticality. Although the GUG is a dataset annotated for grammaticality to sentences written by language learners, our target is the learner sentence corrected by the GEC system. They used a language model and METEOR (Denkowski and Lavie, 2014) as sub-metric for fluency and meaning preservation, respectively; yet these sub-metrics are not optimized for manual evaluation. The weighted linear sum of each evaluation score was used as the final score. Although our metric follows Asano et al. (2017), each sub-metric is trained on our dataset, thus achieving a higher correlation with manual evaluation. Apart from the GUG dataset, a fluency annotated dataset3 (Lau et al., 2015) exists with manual evaluations of acceptability for pseudo-error sentences generated by round-trip translation of English sentences from the British National Corpus"
2020.coling-main.573,N19-1423,0,0.0097229,"ram of manual evaluations and examples of annotation. Ratings of 2 or lower generally exhibited a low frequency; the majority of the meaning preservation ratings were 3 or higher. 4 Automatic Evaluation of GEC using BERT Using our dataset introduced in the previous section, we trained regression models corresponding to each sub-metric of (Asano et al., 2017). For grammaticality and fluency, the manual evaluations were estimated only from the system outputs, whereas, for meaning preservation, the manual evaluations were estimated from pairs of source sentences and system outputs. We used BERT (Devlin et al., 2019) for the regression models. BERT is a sentence encoder pre-trained with large-scale corpora, such as Wikipedia, based on both masked language modeling and next sentence prediction, which can achieve high performance in various natural language processing tasks by fine-tuning on a small dataset of the target task. We fine-tuned three BERT models for each perspective of grammaticality, fluency, and meaning preservation, and constructed sub-metrics that were optimized for manual evaluations of each perspective. The final evaluation score is calculated by the weighted linear sum of each evaluation"
2020.coling-main.573,D14-1020,0,0.061153,"Missing"
2020.coling-main.573,W14-3333,0,0.0730499,"Missing"
2020.coling-main.573,N18-2046,0,0.0144674,"stems, and thus we collected manual evaluations of the output of the GEC systems to train the metrics. In this study, these datasets are referred to as existing data. 3 Manual Evaluation of GEC System Outputs Data and GEC systems We collected manual evaluations for the grammaticality, fluency, and meaning preservation of the system outputs of 1,381 sentences from CoNLL 2013,4 which are often used to evaluate GEC systems. To collect the manual evaluations for various system outputs, each source sentence was corrected by the following five typical systems: statistical machine translation (SMT) (Grundkiewicz and Junczys-Dowmunt, 2018), recurrent neural network (RNN) (Luong et al., 2015), convolutional neural network (CNN) (Chollampatt and Ng, 2018), self-attention network (SAN) (Vaswani et al., 2017), and SAN with copy mechanism (SAN+Copy) (Zhao et al., 2019). More details can be found in Appendix A. Annotation By excluding duplicate corrected sentences, manual evaluation for the grammaticality, fluency, and meaning preservation were assigned to a total of 4,223 sentences, as follows: Grammaticality: Annotators evaluated the grammatical correctness of the system output. We followed the five-point scale evaluation criteria"
2020.coling-main.573,D15-1052,0,0.303965,"for fine-tuning BERT8 , hyperparameter tuning, and intrinsic evaluation of each sub-metric, respectively. Refer to Appendix C for the hyperparameter settings. 5.2 Meta-Evaluation System-level meta-evaluation In the system level meta-evaluation, the average of the sentence scores was used as the system score , and the correlation coefficients with the manual evaluations were calculated. Following Asano et al. (2017), system-level meta-evaluation was performed using Pearson’s correlation coefficient and Spearman’s rank correlation coefficient with the manual ranking of 12 systems described in (Grundkiewicz et al., 2015). The weights of the evaluation score (↵, , and ) were tuned on the JFLEG dataset (Napoles et al., 2017), following Asano et al. (2017). To perform a comprehensive evaluation considering all perspectives, we performed a grid search in increments of 0.01 in the range of 0.01 to 0.98 for each weight and maximized Pearson’s correlation coefficient. Following the recommendation of Graham and Baldwin (2014), we used Williams significance test to identify differences in correlation that are statistically significant. 6 Incomplete or unclear sentences. http://ixa2.si.ehu.es/stswiki/index.php/STSbench"
2020.coling-main.573,P14-2029,0,0.54856,"lysis reveals that optimization for both the manual evaluation and the output of GEC systems contribute to improvement. 2 Related Work Napoles et al. (2016) pioneered the reference-less GEC metric. They presented a metric based on grammatical error detection tools and linguistic features such as language models, and demonstrated that its performance was close to that of reference-based metrics. Asano et al. (2017) combined three submetrics: grammaticality, fluency, and meaning preservation, and outperformed reference-based metrics. They trained a logistic regression model on the GUG dataset2 (Heilman et al., 2014) for the sub-metric This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://github.com/kokeman/SOME 2 https://github.com/EducationalTestingService/gug-data License details: http:// 6516 Proceedings of the 28th International Conference on Computational Linguistics, pages 6516–6522 Barcelona, Spain (Online), December 8-13, 2020 Source text: This will inversely improve the sale of the shop. System output: This will deﬁnitely improve the sales of the shop. Grammaticaly: 3.8 Fluency: 3.8 Meaning: 1.6 Source text: The incr"
2020.coling-main.573,P15-1156,0,0.0358276,"aticality to sentences written by language learners, our target is the learner sentence corrected by the GEC system. They used a language model and METEOR (Denkowski and Lavie, 2014) as sub-metric for fluency and meaning preservation, respectively; yet these sub-metrics are not optimized for manual evaluation. The weighted linear sum of each evaluation score was used as the final score. Although our metric follows Asano et al. (2017), each sub-metric is trained on our dataset, thus achieving a higher correlation with manual evaluation. Apart from the GUG dataset, a fluency annotated dataset3 (Lau et al., 2015) exists with manual evaluations of acceptability for pseudo-error sentences generated by round-trip translation of English sentences from the British National Corpus (BNC) and Wikipedia using Google Translate. We assume that sentences written by learners or translated by systems have different properties from those generated by GEC systems, and thus we collected manual evaluations of the output of the GEC systems to train the metrics. In this study, these datasets are referred to as existing data. 3 Manual Evaluation of GEC System Outputs Data and GEC systems We collected manual evaluations fo"
2020.coling-main.573,D15-1166,0,0.0222827,"systems to train the metrics. In this study, these datasets are referred to as existing data. 3 Manual Evaluation of GEC System Outputs Data and GEC systems We collected manual evaluations for the grammaticality, fluency, and meaning preservation of the system outputs of 1,381 sentences from CoNLL 2013,4 which are often used to evaluate GEC systems. To collect the manual evaluations for various system outputs, each source sentence was corrected by the following five typical systems: statistical machine translation (SMT) (Grundkiewicz and Junczys-Dowmunt, 2018), recurrent neural network (RNN) (Luong et al., 2015), convolutional neural network (CNN) (Chollampatt and Ng, 2018), self-attention network (SAN) (Vaswani et al., 2017), and SAN with copy mechanism (SAN+Copy) (Zhao et al., 2019). More details can be found in Appendix A. Annotation By excluding duplicate corrected sentences, manual evaluation for the grammaticality, fluency, and meaning preservation were assigned to a total of 4,223 sentences, as follows: Grammaticality: Annotators evaluated the grammatical correctness of the system output. We followed the five-point scale evaluation criteria (4: Perfect, 3: Comprehensible, 2: Somewhat comprehen"
2020.coling-main.573,N19-1132,1,0.887866,"Missing"
2020.coling-main.573,P15-2097,0,0.0331299,"ze the metrics. Experimental results show that the proposed metric improves the correlation with manual evaluation in both systemand sentence-level meta-evaluation. Our dataset and metric will be made publicly available.1 1 Introduction Grammatical error correction (GEC) is the task of automatically correcting grammatically incorrect sentences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives of (i) grammaticality, (ii) fluen"
2020.coling-main.573,D16-1228,0,0.10179,"ences, especially those written by language learners. To develop GEC systems efficiently, we construct an evaluation metric that has a high correlation with manual evaluations. Reference-based metrics such as Max Match (M2 ) (Dahlmeier and Ng, 2012) and GLEU (Napoles et al., 2015) are commonly used for automatic evaluation in the GEC task. However, these metrics penalize sentences whose words or phrases are not included in the reference, even if they are correct expressions because it is difficult to cover all possible references (Choshen and Abend, 2018). In contrast, reference-less metrics (Napoles et al., 2016; Asano et al., 2017) do not suffer from this limitation. Among them, Asano et al. (2017) achieved a higher correlation with manual evaluations than reference-based metrics by integrating sub-metrics from the three perspectives of (i) grammaticality, (ii) fluency, and (iii) meaning preservation. However, the correlation with the manual evaluation of system output can be further improved because they are not considered for optimizing each sub-metric. To achieve a better correlation with manual evaluation, we create a dataset to optimize each submetric to the manual evaluation of GEC systems. Ou"
2020.coling-main.573,E17-2037,0,0.186197,"Missing"
2020.coling-main.573,Q16-1029,0,0.0258589,"2: Somewhat comprehensible, 1: Incomprehensible, and 0: Other) proposed by Heilman et al. (2014). Fluency: Annotators evaluated how natural the sentence sounds for native speakers. We followed the criteria (4: Extremely natural, 3: Somewhat natural, 2: Somewhat unnatural, and 1: Extremely unnatural) proposed by Lau et al. (2015). Meaning preservation: Annotators evaluated the extent to which the meaning of source sentences is preserved in system output. We followed the criteria (4: Identical, 3: Minor differences, 2: Moderate differences, 1: Substantially different, and 0: Other) proposed by Xu et al. (2016). We used Amazon Mechanical Turk5 and recruited five native English annotators. The average of the ratings excluding “0: Other” was used as the final sentence score. For more details, refer to Appendix B. Finally, we created a dataset with manual evaluations for a total of 4,221 sentences, excluding sentences in which three or more annotators 3 https://clasp.gu.se/about/people/shalom-lappin/smog/experiments-and-datasets https://www.comp.nus.edu.sg/˜nlp/conll13st.html 5 https://www.mturk.com/ 4 6517 answered “0: Other.”6 Figure 1 presents a histogram of manual evaluations and examples of annota"
2020.coling-main.573,N19-1014,0,0.0187269,"evaluations for the grammaticality, fluency, and meaning preservation of the system outputs of 1,381 sentences from CoNLL 2013,4 which are often used to evaluate GEC systems. To collect the manual evaluations for various system outputs, each source sentence was corrected by the following five typical systems: statistical machine translation (SMT) (Grundkiewicz and Junczys-Dowmunt, 2018), recurrent neural network (RNN) (Luong et al., 2015), convolutional neural network (CNN) (Chollampatt and Ng, 2018), self-attention network (SAN) (Vaswani et al., 2017), and SAN with copy mechanism (SAN+Copy) (Zhao et al., 2019). More details can be found in Appendix A. Annotation By excluding duplicate corrected sentences, manual evaluation for the grammaticality, fluency, and meaning preservation were assigned to a total of 4,223 sentences, as follows: Grammaticality: Annotators evaluated the grammatical correctness of the system output. We followed the five-point scale evaluation criteria (4: Perfect, 3: Comprehensible, 2: Somewhat comprehensible, 1: Incomprehensible, and 0: Other) proposed by Heilman et al. (2014). Fluency: Annotators evaluated how natural the sentence sounds for native speakers. We followed the"
2020.eamt-1.12,W18-6402,0,0.305768,"lication of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English→German and English→French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions. Object detection Abstract attention Figure 1: Overview of our MNMT model. many studies (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; H"
2020.eamt-1.12,W16-2358,0,0.0423569,"Missing"
2020.eamt-1.12,J82-2005,0,0.576686,"Missing"
2020.eamt-1.12,W17-4746,0,0.0849293,"Missing"
2020.eamt-1.12,W18-6438,0,0.0342887,"Missing"
2020.eamt-1.12,D17-1105,0,0.0955703,"ngly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; Huang et al., 2016; Calixto and Liu, 2017) incorporated global visual features extracted from the entire image by convolutional neural networks (CNNs) into a source word sequence or hidden states of a recurrent neural network (RNN). Furthermore, other studies started using local visual features in the context of an attention-based NMT. These features were extracted from equally-sized grids in an image by a CNN. For instance, multimodal attention (Caglayan et al., 2016b) has been designed for a mix of text and local visual features. Additionally, double attention mechanisms (Calixto et al., 2017) have been proposed for text homme en De"
2020.eamt-1.12,W16-2359,0,0.018129,"r hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms into decoder using grid visual features by VGG-19 CNN. Because the grid regions do not contain semantic visual features, the multimodal attention mechanism can not capture useful information with grid visual features. Therefore, instead of multimodal attention, Calixto, Liu, and Campbell (2017) proposed two individual attention mechanisms focusing on two modalities. Similarly, Libovick´y and Helcl (2017) proposed two attention strategies that can be applied to all hidden layers or context vectors of each modality. But they still used grid"
2020.eamt-1.12,P17-1175,0,0.0528235,"Missing"
2020.eamt-1.12,P19-1642,0,0.0192037,"features into source sentence vectors and encoder/decoder hidden states. Elliott and K´ad´ar (2017) utilized global visual features to learn both machine translation and visually grounding task simultaneously. As for the best system in WMT 2017,7 Caglayan et al. (2017) proposed different methods to incorporate global visual features based on attention-based NMT model such as initial encoder/decoder hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms into decoder using grid visual features by VGG-19 CNN. Because the grid regions do not contain semantic visual features, the multimodal attention"
2020.eamt-1.12,D17-1095,0,0.0549949,"s. Additionally, double attention mechanisms (Calixto et al., 2017) have been proposed for text homme en Decoder yt-1 yt chemise rouge st-1 st &lt;eos&gt; zt ct Att_text wp (max) rp Bi-directional GRU encoder 100 semantic image region feature vectores Att_img hi Faster R-CNN + ResNet-101 Man Source image in a red xi shirt &lt;eos&gt; Source sentence Figure 2: Our model of double attention-based MNMT with semantic image regions. and local visual features, respectively. Although previous studies improved the use of local visual features and the text modality, these improvements were minor. As discussed in (Delbrouck and Dupont, 2017), these local visual features may not be suitable to attention-based NMT, because the attention mechanism cannot understand complex relationships between textual objects and visual concepts. Other studies utilized richer local visual features to MNMT such as dense captioning features (Delbrouck et al., 2017). However, their efforts have not convincingly demonstrated that visual features can improve the translation quality. Caglayan et al. (2019) demonstrated that, when the textual context is limited, visual features can assist in generating better translations. MMT models disregard visual feat"
2020.eamt-1.12,W18-6439,0,0.113891,"studies have fused either global or local visual image features into MMT. 6.1 Global visual feature Calixto and Liu (2017) incorporated global visual features into source sentence vectors and encoder/decoder hidden states. Elliott and K´ad´ar (2017) utilized global visual features to learn both machine translation and visually grounding task simultaneously. As for the best system in WMT 2017,7 Caglayan et al. (2017) proposed different methods to incorporate global visual features based on attention-based NMT model such as initial encoder/decoder hidden states using elementwise multiplication. Delbrouck and Dupont (2018) proposed a variation of the conditional gated recurrent unit decoder, which receives the global visual features as input. Calixto et al. (2019) incorporated global visual features through latent variables. Although their results surpassed the performance of the NMT baseline, the visual features of an entire image are complex and non-specific, so that the effect of the image is not fully exerted. computing the multimodal context vector, wherein the local visual features were extracted by the ResNet-50 CNN. Similarly, Calixto et al. (2016) incorporated multiple multimodal attention mechanisms i"
2020.eamt-1.12,W14-3348,0,0.0161004,"testset of Multi30k. All scores are averages of three runs. We present the results using the mean and the standard deviation. † and ‡ indicate that the result is significantly better than OpenNMT and double-attentive MNMT at p-value &lt; 0.01, respectively. Additionally, we report the best results of using grid and global visual features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, doubly-attentive MNMT and our model on M"
2020.eamt-1.12,P17-2031,0,0.0369791,"Missing"
2020.eamt-1.12,I17-1014,0,0.0393147,"Missing"
2020.eamt-1.12,P02-1040,0,0.106569,"models on the En→De and En→Fr 2016 testset of Multi30k. All scores are averages of three runs. We present the results using the mean and the standard deviation. † and ‡ indicate that the result is significantly better than OpenNMT and double-attentive MNMT at p-value &lt; 0.01, respectively. Additionally, we report the best results of using grid and global visual features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, d"
2020.eamt-1.12,W16-3210,0,0.111543,"Missing"
2020.eamt-1.12,W17-4718,0,0.0640345,"st, we propose the application of semantic image regions for MNMT by integrating visual and textual features using two individual attention mechanisms (double attention). We conducted experiments on the Multi30k dataset and achieved an improvement of 0.5 and 0.9 BLEU points for English→German and English→French translation tasks, compared with the MNMT with grid visual features. We also demonstrated concrete improvements on translation performance benefited from semantic image regions. Object detection Abstract attention Figure 1: Overview of our MNMT model. many studies (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018) have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent stud"
2020.eamt-1.12,W16-2346,0,0.39395,"Missing"
2020.eamt-1.12,D16-1044,0,0.0377199,"eline NMT for the English–German task. Helcl, Libovick´y, and Variˇs (2018) set an additional attention sub-layer after the self-attention based on the Transformer architecture, and integrated grid visual features extracted by a pretrained CNN. Caglayan et al. (2018) enhanced the multimodal attention into the filtered attention, which filters out grid regions irrelevant to translation and focuses on the most important part of the grid visual features. They made efforts to integrate a stronger attention function, but the considered regions were still grid visual features. Grid visual features. Fukui et al. (2016) applied multimodal compact bilinear pooling to combine the grid visual features and text vectors, but their model does not convincingly surpass an attentionbased NMT baseline. Caglayan et al. (2016a) integrated local visual features extracted by ResNet-50 and source text vectors into an NMT decoder using shared transformation. They reported that the results obtained by their method did not surpass the results obtained by NMT systems. Caglayan, Barrault, and Bougares (2016b) proposed a multimodal attention mechanism based on (Caglayan et al., 2016a). They integrated two modalities by Image reg"
2020.eamt-1.12,W18-6441,0,0.0269252,"Missing"
2020.eamt-1.12,W16-2360,0,0.0519306,") have been increasingly focusing on incorporating multimodal contents, particularly images, to improve translations. Hence, researchers in this field have established a shared task called multimodal machine translation (MMT), which consists of translating a target sentence from a source language description into another language using information from the image described by the source sentence. The first MMT study by (Elliott et al., 2015) demonstrated the potential of improving the translation quality by using image. To effectively use an image, several subsequent studies (Gao et al., 2015; Huang et al., 2016; Calixto and Liu, 2017) incorporated global visual features extracted from the entire image by convolutional neural networks (CNNs) into a source word sequence or hidden states of a recurrent neural network (RNN). Furthermore, other studies started using local visual features in the context of an attention-based NMT. These features were extracted from equally-sized grids in an image by a CNN. For instance, multimodal attention (Caglayan et al., 2016b) has been designed for a mix of text and local visual features. Additionally, double attention mechanisms (Calixto et al., 2017) have been propo"
2020.eamt-1.12,W04-3250,0,0.0908902,"features on Multi30k dataset according to (Caglayan et al., 2017), which is the state-of-the-art system for En→De translation on this dataset. 3.3 Evaluation We evaluated the quality of the translation according to the token level BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. We trained all models (baselines and proposed model) three times and calculated the BLEU and METEOR scores, respectively. Based on the calculation results, we report the mean and standard deviation over three runs. Moreover, we report the statistical significance with bootstrap resampling (Koehn, 2004) using the merger of three test translation results. We defined the threshold for the statistical significance test as 0.01, and report only if the p-value was less than the threshold. 4 Results In Table 1, we present the results for the OpenNMT, doubly-attentive MNMT and our model on Multi30k dataset. Additionally, we also compared with Caglayan et al. (2017), which achieved the best performance under the same condition with our experiments. Comparing the baselines, the doubly-attentive MNMT outperformed OpenNMT. Because there did not exist a big difference amongst the three image feature ext"
2020.lrec-1.157,P18-2080,0,0.0221328,"uared error (RMSE). In this study, every model predicts a real number. Thus, we used the RMSE score to measure the difference between the gold scores and the predicted scores. We also used the RMSE score to train the models. RMSE is calculated using Equation (1): v u N u1 ∑ 2 RM SE = t (1) (yi − yˆi ) N i=1 where N is the total number of essays, yi is the actual score of i-th essay, and yˆi is the predicted score of i- th essay. The second metric is a quadratic weighted kappa (QWK). The QWK was used as an evaluation metric in the ASAP competition and in recent studies (Taghipour and Ng, 2016; Cozma et al., 2018). This metric gives a square penalty depending on the distance between integer values: gold scores and predicted scores. QWK is calculated using Equation (2): ∑ i,j wi,j Oi,j κ=1− ∑ (2) i,j wi,j Ei,j where matrix O is calculated such that Oi,j is the number of essays that was scored i by the human annotator and scored j by the AES system. Matrix E is calculated as the outer product between the histogram vectors of the actual and predicted scores. The matrices E and O are normalized such that E and O have the same sum. Weight wi,j is calculated using Equation (3): wi,j = (3) where R is the maxi"
2020.lrec-1.157,N19-1423,0,0.49119,"an essay dataset with annotations for a holistic score and multiple trait scores, including content, organization, and language scores. In particular, we developed AES systems using two different approaches: a feature-based approach and a neural-network-based approach. In the former approach, we used Japanese-specific linguistic features, including character-type features such as “kanji” and “hiragana.” In the latter approach, we used two models: a long short-term memory (LSTM) model (Hochreiter and Schmidhuber, 1997) and a bidirectional encoder representations from transformers (BERT) model (Devlin et al., 2019), which achieved the highest accuracy in various natural language processing tasks in 2018. Overall, the BERT model achieved the best root mean squared error and quadratic weighted kappa scores. In addition, we analyzed the robustness of the outputs of the BERT model. We have released and shared this system to facilitate further research on AES for Japanese as a second language learners. Keywords: Learner Written Essay, Japanese, Automated Essay Scoring 1. Introduction Automated essay scoring (AES) is a task in which computer technology is used to evaluate written text. Humans find it difficul"
2020.lrec-1.157,P06-1030,0,0.0789402,". 2. Related Work In recent years, many AES engines have been developed (Page, 1966; Shermis and Burstein, 2013). Some of them provide not only a holistic score but also other scores of essay quality. In 2012, Kaggle organized an AES competition called Automated Student Assessment Prize (ASAP)2 . The dataset of this competition contains more than 10,000 essays with holistic scores, and it is now used in English AES research. However, few such Japanese datasets are available because of insufficient resources and the difficulty of obtaining essay data. The Japanese essay scoring system (Jess) (Ishioka and Kameda, 2006) has been created for scoring essays in college-entrance exams. Jess can provide not only a holistic score but also multiple trait scores— rhetoric, organization, and content scores—using statistical methods. Jess uses Mainichi Daily News data to measure the difference between an input essay and expert-written essays using linguistic features such as sentence length and 1 The systems are available at https://github.com/reo11/aes-forjapanese-learner 2 https://www.kaggle.com/c/asap-aes 1250 Data Name No. of Essays with Holistic Score No. of Essays with Multiple-trait Scores No. of Learner Mother"
2020.lrec-1.157,L18-1187,0,0.0119922,"score that summarizes the quality of an essay. Such a score provides little feedback, especially for a language learner. For example, when a system only returns a low holistic score, the learner cannot understand which aspect of the essay is inadequate without language teachers. To address this problem, some studies scored various dimensions of essay quality, such as prompt adherence (Persing and Ng, 2014), organization (Persing et al., 2010), and coherence (Miltsakaki and Kukich, 2004). Furthermore, some datasets are available for evaluating additional dimensions of essay quality in English (Mathias and Bhattacharyya, 2018). However, only a few evaluation datasets are available for Japanese writings, and even fewer Japanese learner essay datasets are. Nevertheless, Tanaka and Kubota overcame this lack of availability of Japanese learner datasets by creating a new dataset for Japanese learners (Tanaka and Kubota, 2016). In this dataset, they annotated a holistic score and three trait scores—content, organization, and language scores—of essay quality on a learner’s ability. With the Japanese learner corpora annotated for multiple traits, we created essay scoring systems for Japanese learners using two different ma"
2020.lrec-1.157,W19-4450,0,0.014986,"rating words in Japanese. Second, each morpheme is converted into a vector. Third, each vector is inputted into a recurrent neural network (unidirectional LSTM) across sentences. The whole vectors in the essay are taken as a series in this model. Fourth, the hidden layer vectors are aggregated using an attention mechanism. Finally, the vector is converted to a single scalar in the linear layer. BERT is a fine-tuning-based language representation model that uses a transformer architecture (Vaswani et al., 2017) trained on two tasks: masked language model and next sentence prediction. Recently, Nadeem et al. (2019) applied BERT to AES systems. Figure 2-(b) shows the architecture of our BERT model. The BERT model takes a tokenized prompt and a whole essay as an input. When tokenizing the input, a [CLS] token is added at the beginning. The prompt and the essay are distinguished by a [SEP] token added at the end. If the essay length exceeds the upper limit of the sequence, the rest of the essay will be excluded so that it will fit in the sequence including three tokens: a [CLS] token and two [SEP] tokens. The prompt and the essay sequences are inputted into the transformer encoder and become hidden layer s"
2020.lrec-1.157,P14-1144,0,0.0224009,"ys. In this light, AES has emerged as one of the most important educational applications of natural language processing. The major weakness of existing scoring systems is that they only provide a single holistic score that summarizes the quality of an essay. Such a score provides little feedback, especially for a language learner. For example, when a system only returns a low holistic score, the learner cannot understand which aspect of the essay is inadequate without language teachers. To address this problem, some studies scored various dimensions of essay quality, such as prompt adherence (Persing and Ng, 2014), organization (Persing et al., 2010), and coherence (Miltsakaki and Kukich, 2004). Furthermore, some datasets are available for evaluating additional dimensions of essay quality in English (Mathias and Bhattacharyya, 2018). However, only a few evaluation datasets are available for Japanese writings, and even fewer Japanese learner essay datasets are. Nevertheless, Tanaka and Kubota overcame this lack of availability of Japanese learner datasets by creating a new dataset for Japanese learners (Tanaka and Kubota, 2016). In this dataset, they annotated a holistic score and three trait scores—con"
2020.lrec-1.157,D10-1023,0,0.037159,"one of the most important educational applications of natural language processing. The major weakness of existing scoring systems is that they only provide a single holistic score that summarizes the quality of an essay. Such a score provides little feedback, especially for a language learner. For example, when a system only returns a low holistic score, the learner cannot understand which aspect of the essay is inadequate without language teachers. To address this problem, some studies scored various dimensions of essay quality, such as prompt adherence (Persing and Ng, 2014), organization (Persing et al., 2010), and coherence (Miltsakaki and Kukich, 2004). Furthermore, some datasets are available for evaluating additional dimensions of essay quality in English (Mathias and Bhattacharyya, 2018). However, only a few evaluation datasets are available for Japanese writings, and even fewer Japanese learner essay datasets are. Nevertheless, Tanaka and Kubota overcame this lack of availability of Japanese learner datasets by creating a new dataset for Japanese learners (Tanaka and Kubota, 2016). In this dataset, they annotated a holistic score and three trait scores—content, organization, and language scor"
2020.lrec-1.157,D16-1193,0,0.110302,"(POS) tags, and total number of characters. In addition, it can predict a holistic score robustly with just a small training dataset; however, because it does not use surface information, it cannot predict multiple traits that require surface text information. Moreover, if the regression equation is known to the learners, they could cheat the system easily. By contrast, neural-network-based methods do not need to create features and have produced state-of-the-art results in various datasets (Ke and Ng, 2019). As a result, the neural approach for AES has been actively studied in recent years (Taghipour and Ng, 2016). However, no neural-network-based AES system is available for the Japanese language; furthermore, the BERT model has not been applied for an AES task with multiple dimensions thus far. Therefore, we create such a system and report the obtained results. Further, because the features in the neural approach are not explicit to the learners, we analyze how robust neural-network-based models are to cheating. 3. Dataset We used the GoodWriting dataset3 . This dataset contains more than 800 essays written by Japanese learners overseas. Each essay was annotated by three annotators, and the final scor"
2020.lrec-1.157,W15-0626,0,0.0285171,"of the classes corresponding to the scores. We created a Japanese AES system using regression models for two reasons. First, state-of-the-art methods for various AES datasets use a regression approach (Ke and Ng, 2019). Second, even if the amount of data for each score is biased, it does not overfit as much as the classification models do. We propose five models using the GoodWriting dataset: three models with a feature-based method and two models with a neural method. Each model is described in the following subsections. 1252 4.1. Feature-based Methods We follow the feature set described by Zesch et al. (2015). Additionally, we use linguistic features unique to Japanese, including character-type features such as “kanji” and “hiragana,” that are used in jWriter (Lee and Hasebe, 2017) and GoodWriting Rater7 . Table 3 shows the features used in our systems. Each feature is designed for either holistic, content, organization, or language traits. All the features listed in Table 3 are used when predicting individual scores. We apply linear regression and linear support vector regression (SVR), which have been used in recent studies (Ke and Ng, 2019). We also used random forest regression to examine a de"
2020.lrec-1.157,den-etal-2008-proper,0,0.119708,"Missing"
2020.lrec-1.157,D18-2012,0,0.0151087,"1 ± .0121 0.540 ± .0131 0.597 ± .0130 0.571 ± .0181 0.462 ± .0138 0.332 ± .0138 0.621 ± .0094 0.523 ± .0181 0.503 ± .0213 0.477 ± .0140 0.346 ± .0204 0.569 ± .0135 Table 4: RMSE and QWK scores of AES models. The best score in each column is indicated in bold. The average score is the mean value of the holistic, content, organization, and language scores. and we set the dropout probability to 0.5. We use the BERTbase architecture (Devlin et al., 2019) in our experiments. We use a pre-trained BERT model with SentencePiece for Japanese text11 of which the vocabulary is obtained by SentencePiece (Kudo and Richardson, 2018). We selected this model because it can input the longest sequence (512 tokens) among the pre-trained Japanese models that are publicly available. The vocabulary size of this model is 32,000. We fine-tuned the pretrained BERT model with learning rate of 5e-5, batch size of 4, and maximum sequence length of 512. We set the dropout probability for all fully connected layers in the embeddings, encoder, and pooling layers to 0.1 and the dropout ratio for the attention probabilities to 0.1. All scores are evaluated as five-fold cross-validation. Additionally, because the evaluation results vary dep"
2020.lrec-1.26,W19-4431,1,0.840369,"ce, Mizumoto et al. (2011) used the Lang-8 corpus and applied the SMT technique to correct the grammar of JSL learners’ sentences. Liu et al. (2018) applied the NMT technique to correct the grammar of Japanese sentences, but they limited the types of errors because they focused only on Japanese functional expressions. Ogawa and Yamamoto (2019) proposed a grammatical error correction system for Japanese particles based on a shallow-and-wide convolutional neural network (CNN) classiﬁcation model built by training corrected sentences in the Lang-8 corpus. In Japanese grammatical error detection, Arai et al. (2019) applied the NMT technique. Moreover, they used the original annotation in the Lang-8 corpus, but they did not perform grammatical error correction because their purpose was to build an example sentence retrieval system. In this study, we apply an NMT technique to correct the grammar of JSL learners’ sentences without limiting error types and use a manually annotated corpus to evaluate the system. 205 Same corrections Different corrections L 夜、ラベンダーが室内に置きます。 (Lavender will put in the room at night.) このストーリーは、結果がない恋についてのちょっと悲しいストーリーです。 (This story is a little bit sad story about love without re"
2020.lrec-1.26,P06-1032,0,0.0978422,"prepositions, and to solve the task with a classiﬁer (De Felice and Pulman, 2008; Dahlmeier and Ng, 2011; Tajiri et al., 2012). This is because there were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi et al., 2017; Chollampatt and Ng, 2018; Kiyono et al., 2019) techniques extensively. The Japanese portion of the Lang-8 corpus has a wide coverage. Thus, these techniques can also be applied to JSL texts. For instance, Mizumoto et al. (2011) used the Lang-8 corpus and applied the SMT technique to correct the grammar of JSL learners’ sentences. Liu et al. (2018) applied the NMT technique to correct the grammar of Japanese sentences, but they limited the types of errors because they focused only on Japanese func"
2020.lrec-1.26,P11-1092,0,0.0360772,". Similarly, we also created a multi-reference corpus of JSL learners’ sentences corrected by two or three individuals. Instead of using crowdsourcing, we created rules among the annotators to perform a consistent annotation based on minimal edits because the writings in Lang-8 are already colloquial. 2.2. Grammatical Error Correction In previous grammatical error correction for learners of English as a second language, it is typical to limit the error types according to the parts of speech, such as articles and prepositions, and to solve the task with a classiﬁer (De Felice and Pulman, 2008; Dahlmeier and Ng, 2011; Tajiri et al., 2012). This is because there were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi"
2020.lrec-1.26,N12-1067,0,0.0332625,"and statistical machine translation (SMT) techniques to correct the grammar of the JSL learners’ sentences and evaluated their results using our corpus. We also compared the performance of the NMT system with that of the SMT system. Keywords: corpus construction, second-language learner corpus, Japanese grammatical error correction 1. Introduction Grammatical error correction is the task of receiving a second-language learner sentence and outputting a sentence in which the errors were corrected. Many automatic evaluation methods in grammatical error correction systems use corrected sentences (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). This requires a highly reliable evaluation corpus for an accurate evaluation (Napoles et al., 2016). The Lang-8 corpus (Mizumoto et al., 2011) is one of the largest corpora used as a training dataset for machine translation-based grammatical error correction systems. Further, it is a corpus constructed from the revision log of Lang-81 and contains learner sentences and corrected sentences in nearly 80 languages. Japanese is the second largest language after English, which amounts to approximately 1.3 million sentence pairs. However, the Lang-8"
2020.lrec-1.26,C08-1022,0,0.0314896,"Missing"
2020.lrec-1.26,N15-1060,0,0.0141635,"translation (SMT) techniques to correct the grammar of the JSL learners’ sentences and evaluated their results using our corpus. We also compared the performance of the NMT system with that of the SMT system. Keywords: corpus construction, second-language learner corpus, Japanese grammatical error correction 1. Introduction Grammatical error correction is the task of receiving a second-language learner sentence and outputting a sentence in which the errors were corrected. Many automatic evaluation methods in grammatical error correction systems use corrected sentences (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). This requires a highly reliable evaluation corpus for an accurate evaluation (Napoles et al., 2016). The Lang-8 corpus (Mizumoto et al., 2011) is one of the largest corpora used as a training dataset for machine translation-based grammatical error correction systems. Further, it is a corpus constructed from the revision log of Lang-81 and contains learner sentences and corrected sentences in nearly 80 languages. Japanese is the second largest language after English, which amounts to approximately 1.3 million sentence pairs. However, the Lang-8 corpus is not suitable as"
2020.lrec-1.26,W11-2123,0,0.0378401,"ps://github.com/nusnlp/mlconvgec2018 https://taku910.github.io/mecab 17 https://unidic.ninjal.ac.jp 18 https://pypi.org/project/mojimoji 19 http://statmt.org/moses 20 https://github.com/moses-smt/giza-pp 16 3. Char-Word Model 14 Parentheses and characters in the parentheses were removed using a regular expression. 208 NMT system SMT system TP FP FN Precision Recall F0.5 Insertion Deletion Substitution 39 14 187 24 171 196 17.3 36.8 18.6 6.67 17.5 19.3 135 12 15 7 76 19 Table 8: Analysis of the NMT system using the Char-Word Model and the SMT system using the Char-Char Model in the core data. (Heaﬁeld, 2011). We used the word 3-gram language model in the Word-Word Model and Char-Word Model. We also used the character 5-gram language model in the CharChar Model. In addition, in the SMT system, we added sentence pairs that copied the corrected sentences in the training dataset to the learner sentence as new training data to reduce unknown words. We performed word segmentation and converted full-width to half-width characters in the same way as for the NMT system. However, BPE was not used in the SMT system. Metric. We used the generalized language evaluation understanding metric (GLEU) (Napoles et"
2020.lrec-1.26,P12-2076,0,0.0259721,"ammatical Error Correction In previous grammatical error correction for learners of English as a second language, it is typical to limit the error types according to the parts of speech, such as articles and prepositions, and to solve the task with a classiﬁer (De Felice and Pulman, 2008; Dahlmeier and Ng, 2011; Tajiri et al., 2012). This is because there were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi et al., 2017; Chollampatt and Ng, 2018; Kiyono et al., 2019) techniques extensively. The Japanese portion of the Lang-8 corpus has a wide coverage. Thus, these techniques can also be applied to JSL texts. For instance, Mizumoto et al. (2011) used the Lang-8 corpus and applied the SMT technique to correct th"
2020.lrec-1.26,W14-1703,0,0.0177418,"g the Char-Char Model in the core data. The number of true positives (TP) in the NMT system was larger than that in the SMT system. On the other hand, the number of false positives (FP) in the NMT system was considerably larger than that in the SMT system. In other words, the NMT system changed many points that did not need to be changed. In addition, it turns out that the number of corrections in the SMT system was smaller than that in the NMT system. This is because we tuned the BLEU score using MERT, and the SMT system learned parameter weights that disabled nearly all correction attempts (Junczys-Dowmunt and Grundkiewicz, 2014). As a result, the precision and F0.5 of the NMT system are lower than thoes of the SMT system. 5. Conclusions We created and released a highly reliable evaluation corpus for a grammatical error correction system of JSL learners’ sentences. Unlike the Lang-8 corpus, our corpus is suitable as an evaluation dataset for grammatical error correction of JSL learners’ sentences. Lang-8’s original annotation contains annotator’s comments that are noise for evaluation. In contrast, our evaluation corpus does not contain such comments. In addition, in many cases, only one corrected sentence is provided"
2020.lrec-1.26,D16-1161,0,0.0253462,"Missing"
2020.lrec-1.26,D19-1119,0,0.126819,"here were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi et al., 2017; Chollampatt and Ng, 2018; Kiyono et al., 2019) techniques extensively. The Japanese portion of the Lang-8 corpus has a wide coverage. Thus, these techniques can also be applied to JSL texts. For instance, Mizumoto et al. (2011) used the Lang-8 corpus and applied the SMT technique to correct the grammar of JSL learners’ sentences. Liu et al. (2018) applied the NMT technique to correct the grammar of Japanese sentences, but they limited the types of errors because they focused only on Japanese functional expressions. Ogawa and Yamamoto (2019) proposed a grammatical error correction system for Japanese particles based on a shallow-and-wide c"
2020.lrec-1.26,P07-2045,0,0.01682,"nvolution window width of three. The output of each encoder and decoder layer was of 1,024 dimensions. We used MeCab16 (ver.0.996) using UniDic17 (ver.2.2.0) as a dictionary for word segmentation. Furthermore, we used Byte Pair Encoding (BPE) (Sennrich et al., 2016) for subword processing of rare words, and the vocabulary size was 30,000 words. In addition, we converted full-width to half-width characters using mojimoji18 (ver.0.0.9). SMT system. As a comparative experiment, we evaluated a grammatical error correction system of JSL learners’ sentences using the SMT technique. We used Moses19 (Koehn et al., 2007) as a method for the SMT toolkit and set distortion-limit to the value -1. We also used GIZA++20 (Och and Ney, 2003) as the word alignment tool. Following Mizumoto et al. (2011), we created a word 3-gram language model and character 5-gram language model from the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) using KenLM 15 https://github.com/nusnlp/mlconvgec2018 https://taku910.github.io/mecab 17 https://unidic.ninjal.ac.jp 18 https://pypi.org/project/mojimoji 19 http://statmt.org/moses 20 https://github.com/moses-smt/giza-pp 16 3. Char-Word Model 14 Parenthes"
2020.lrec-1.26,Y18-1046,0,0.0749718,"s’ ﬁrst language” (JPDB) (Inoue et al., 2006) is a Japanese learner corpus consisting of handwritten compositions. Each composition has the corrected sentence annotated by Japanese teachers. The NAIST Misuse Corpus (Oyama et al., 2013) is a corpus that assigns error tags to the corrected sentences in JPDB. The types of errors differ between handwritten sentences and typewritten sentences. Thus, the JPDB and NAIST Misuse Corpus are not suitable as evaluation datasets for correcting grammatical errors in typewritten sentences. Conversely, we corrected typewritten sentences to create our corpus. Liu et al. (2018) manually performed grammatical error correction limiting error types and adding error tags in the Lang-8 corpus to study a grammatical error correction system on Japanese functional expressions. However, the authors have not released that data. We performed grammatical error correction without limiting error types and released this as a corpus. NUCLE (Dahlmeier et al., 2013) is an annotated English learner corpus consisting of approximately 1,400 compositions written by university students in Singapore. It has sentences corrected by native English teachers corresponding to the learners’ sente"
2020.lrec-1.26,I11-1017,1,0.688239,"NMT system with that of the SMT system. Keywords: corpus construction, second-language learner corpus, Japanese grammatical error correction 1. Introduction Grammatical error correction is the task of receiving a second-language learner sentence and outputting a sentence in which the errors were corrected. Many automatic evaluation methods in grammatical error correction systems use corrected sentences (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). This requires a highly reliable evaluation corpus for an accurate evaluation (Napoles et al., 2016). The Lang-8 corpus (Mizumoto et al., 2011) is one of the largest corpora used as a training dataset for machine translation-based grammatical error correction systems. Further, it is a corpus constructed from the revision log of Lang-81 and contains learner sentences and corrected sentences in nearly 80 languages. Japanese is the second largest language after English, which amounts to approximately 1.3 million sentence pairs. However, the Lang-8 corpus is not suitable as an evaluation dataset because annotators not only correct a learner’s sentence, but they also sometimes write comments to the learner. Table 1 shows an example of a c"
2020.lrec-1.26,P15-2097,0,0.12854,"es to correct the grammar of the JSL learners’ sentences and evaluated their results using our corpus. We also compared the performance of the NMT system with that of the SMT system. Keywords: corpus construction, second-language learner corpus, Japanese grammatical error correction 1. Introduction Grammatical error correction is the task of receiving a second-language learner sentence and outputting a sentence in which the errors were corrected. Many automatic evaluation methods in grammatical error correction systems use corrected sentences (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). This requires a highly reliable evaluation corpus for an accurate evaluation (Napoles et al., 2016). The Lang-8 corpus (Mizumoto et al., 2011) is one of the largest corpora used as a training dataset for machine translation-based grammatical error correction systems. Further, it is a corpus constructed from the revision log of Lang-81 and contains learner sentences and corrected sentences in nearly 80 languages. Japanese is the second largest language after English, which amounts to approximately 1.3 million sentence pairs. However, the Lang-8 corpus is not suitable as an evaluation dataset"
2020.lrec-1.26,D16-1228,0,0.0363051,"Missing"
2020.lrec-1.26,J03-1002,0,0.0147508,"6 (ver.0.996) using UniDic17 (ver.2.2.0) as a dictionary for word segmentation. Furthermore, we used Byte Pair Encoding (BPE) (Sennrich et al., 2016) for subword processing of rare words, and the vocabulary size was 30,000 words. In addition, we converted full-width to half-width characters using mojimoji18 (ver.0.0.9). SMT system. As a comparative experiment, we evaluated a grammatical error correction system of JSL learners’ sentences using the SMT technique. We used Moses19 (Koehn et al., 2007) as a method for the SMT toolkit and set distortion-limit to the value -1. We also used GIZA++20 (Och and Ney, 2003) as the word alignment tool. Following Mizumoto et al. (2011), we created a word 3-gram language model and character 5-gram language model from the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) using KenLM 15 https://github.com/nusnlp/mlconvgec2018 https://taku910.github.io/mecab 17 https://unidic.ninjal.ac.jp 18 https://pypi.org/project/mojimoji 19 http://statmt.org/moses 20 https://github.com/moses-smt/giza-pp 16 3. Char-Word Model 14 Parentheses and characters in the parentheses were removed using a regular expression. 208 NMT system SMT system TP FP FN Pre"
2020.lrec-1.26,P03-1021,0,0.0119777,"to half-width characters in the same way as for the NMT system. However, BPE was not used in the SMT system. Metric. We used the generalized language evaluation understanding metric (GLEU) (Napoles et al., 2015) to evaluate the performance of each grammatical error correction system. We used 4-grams when calculating the GLEU score. In the NMT system, training was terminated at the epoch in which the best GLEU score was achieved in the development dataset. The maximum number of epochs was 100. The parameters for the SMT system were adjusted to maximize BLEU (Papineni et al., 2002) using MERT (Och, 2003) for the development dataset. 4.2. Results Table 6 shows the GLEU scores of the NMT and SMT systems using each model. We also calculated the GLEU score of the learners’ sentences. In the NMT system, the GLEU score of the NMT system using the Char-Word Model was the highest. On the other hand, in the SMT system, the GLEU score of the SMT system using the Char-Char Model was the highest. For any output, changing the evaluation corpus from our corpus to Lang-8 corpus reduced the GLEU score by 10 or more points because of remaining comments that could not be removed using a regular expression. The"
2020.lrec-1.26,P02-1040,0,0.1079,"gmentation and converted full-width to half-width characters in the same way as for the NMT system. However, BPE was not used in the SMT system. Metric. We used the generalized language evaluation understanding metric (GLEU) (Napoles et al., 2015) to evaluate the performance of each grammatical error correction system. We used 4-grams when calculating the GLEU score. In the NMT system, training was terminated at the epoch in which the best GLEU score was achieved in the development dataset. The maximum number of epochs was 100. The parameters for the SMT system were adjusted to maximize BLEU (Papineni et al., 2002) using MERT (Och, 2003) for the development dataset. 4.2. Results Table 6 shows the GLEU scores of the NMT and SMT systems using each model. We also calculated the GLEU score of the learners’ sentences. In the NMT system, the GLEU score of the NMT system using the Char-Word Model was the highest. On the other hand, in the SMT system, the GLEU score of the SMT system using the Char-Char Model was the highest. For any output, changing the evaluation corpus from our corpus to Lang-8 corpus reduced the GLEU score by 10 or more points because of remaining comments that could not be removed using a"
2020.lrec-1.26,I17-2062,0,0.0258316,"Missing"
2020.lrec-1.26,P16-1162,0,0.0190006,"tion errors. NMT system. We used a CNN based method (Chollampatt and Ng, 2018) for the grammatical error correction system using the NMT technique. We used the implementation15 published by Chollampatt and Ng (2018). Both the source and target embeddings were of 500 dimensions. Each encoder and decoder was made up of seven convolutional layers, with a convolution window width of three. The output of each encoder and decoder layer was of 1,024 dimensions. We used MeCab16 (ver.0.996) using UniDic17 (ver.2.2.0) as a dictionary for word segmentation. Furthermore, we used Byte Pair Encoding (BPE) (Sennrich et al., 2016) for subword processing of rare words, and the vocabulary size was 30,000 words. In addition, we converted full-width to half-width characters using mojimoji18 (ver.0.0.9). SMT system. As a comparative experiment, we evaluated a grammatical error correction system of JSL learners’ sentences using the SMT technique. We used Moses19 (Koehn et al., 2007) as a method for the SMT toolkit and set distortion-limit to the value -1. We also used GIZA++20 (Och and Ney, 2003) as the word alignment tool. Following Mizumoto et al. (2011), we created a word 3-gram language model and character 5-gram languag"
2020.lrec-1.26,P06-1132,0,0.0260708,"already colloquial. 2.2. Grammatical Error Correction In previous grammatical error correction for learners of English as a second language, it is typical to limit the error types according to the parts of speech, such as articles and prepositions, and to solve the task with a classiﬁer (De Felice and Pulman, 2008; Dahlmeier and Ng, 2011; Tajiri et al., 2012). This is because there were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi et al., 2017; Chollampatt and Ng, 2018; Kiyono et al., 2019) techniques extensively. The Japanese portion of the Lang-8 corpus has a wide coverage. Thus, these techniques can also be applied to JSL texts. For instance, Mizumoto et al. (2011) used the Lang-8 corpus and applied the SMT"
2020.lrec-1.26,P12-2039,1,0.909102,"ated a multi-reference corpus of JSL learners’ sentences corrected by two or three individuals. Instead of using crowdsourcing, we created rules among the annotators to perform a consistent annotation based on minimal edits because the writings in Lang-8 are already colloquial. 2.2. Grammatical Error Correction In previous grammatical error correction for learners of English as a second language, it is typical to limit the error types according to the parts of speech, such as articles and prepositions, and to solve the task with a classiﬁer (De Felice and Pulman, 2008; Dahlmeier and Ng, 2011; Tajiri et al., 2012). This is because there were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi et al., 2017; Chollam"
2020.lrec-1.26,N16-1042,0,0.0162742,"man, 2008; Dahlmeier and Ng, 2011; Tajiri et al., 2012). This is because there were no publicly available large English learner corpora. Similarly, to correct grammatical errors for learners of JSL, most studies limit the target learner’s error types to mainly particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012). After the emergence of a large-scale learner corpus, which is the Lang-8 corpus, it became possible to not limit the error types. Current grammatical error correction methods use SMT (Brockett et al., 2006; Junczys-Dowmunt and Grundkiewicz, 2016) and NMT (Yuan and Briscoe, 2016; Sakaguchi et al., 2017; Chollampatt and Ng, 2018; Kiyono et al., 2019) techniques extensively. The Japanese portion of the Lang-8 corpus has a wide coverage. Thus, these techniques can also be applied to JSL texts. For instance, Mizumoto et al. (2011) used the Lang-8 corpus and applied the SMT technique to correct the grammar of JSL learners’ sentences. Liu et al. (2018) applied the NMT technique to correct the grammar of Japanese sentences, but they limited the types of errors because they focused only on Japanese functional expressions. Ogawa and Yamamoto (2019) proposed a grammatical erro"
2020.lrec-1.26,E17-2037,0,0.0163684,", the authors have not released that data. We performed grammatical error correction without limiting error types and released this as a corpus. NUCLE (Dahlmeier et al., 2013) is an annotated English learner corpus consisting of approximately 1,400 compositions written by university students in Singapore. It has sentences corrected by native English teachers corresponding to the learners’ sentences. These teachers used minimal edits to make the learners’ sentences grammatically correct. Likewise, we also used minimal edits to create the corpus for Japanese grammatical error correction. JFLEG (Napoles et al., 2017) is an English learner corpus consisting of 747 sentences written by learners with different native languages or proﬁciency levels. Unlike NUCLE, the learners’ sentences are corrected by ﬂuency edits. In addition, JFLEG is a multi-reference corpus that is corrected by four crowdsourced annotators. Similarly, we also created a multi-reference corpus of JSL learners’ sentences corrected by two or three individuals. Instead of using crowdsourcing, we created rules among the annotators to perform a consistent annotation based on minimal edits because the writings in Lang-8 are already colloquial."
2020.lrec-1.26,Y13-1014,1,0.811182,"mall apartment.) G2 昨日の成績みたら、失敗しました。 昨日の成績をみたら、失敗していました。 (I failed because I saw yesterday’s results.) G3 ただちょっと心配してるね。。。 ただちょっと心配してるの。。。 (I’m just a little worried.) Table 2: Examples of the corrected sentences based on the annotation rules in Section 3.2. 2. Related Work 2.1. Learner Corpora “The JSL learners Parallel DataBase of Japanese writings and their translation of learners’ ﬁrst language” (JPDB) (Inoue et al., 2006) is a Japanese learner corpus consisting of handwritten compositions. Each composition has the corrected sentence annotated by Japanese teachers. The NAIST Misuse Corpus (Oyama et al., 2013) is a corpus that assigns error tags to the corrected sentences in JPDB. The types of errors differ between handwritten sentences and typewritten sentences. Thus, the JPDB and NAIST Misuse Corpus are not suitable as evaluation datasets for correcting grammatical errors in typewritten sentences. Conversely, we corrected typewritten sentences to create our corpus. Liu et al. (2018) manually performed grammatical error correction limiting error types and adding error tags in the Lang-8 corpus to study a grammatical error correction system on Japanese functional expressions. However, the authors h"
2020.ngt-1.15,2012.eamt-1.60,0,0.0100257,"to produce more diverse translation during fine-tuning. Data Table 2 summarizes the size of data used in our experiments for En→Ja track. The official dataset of STAPLE contains multiple translations for a single prompt. We did not use the official development and test data in our experiments because the correct data with answers were not available to the public. Therefore, we randomly divided the official training data into training data and development data in prompt units as shown in Table 2. We use OpenSubtitles4 (Lison and Tiedemann, 2016), Tatoeba5 (Tiedemann, 2012), TED6 train and dev (Cettolo et al., 2012) corpora as additional dataset which are similar to the STAPLE data in 4 F1 Single seed 1 Single seed 2 Multi seed 23.7 23.4 23.9 L2R R2L L2R & R2L 23.7 23.2 24.7 Hyperparameters Table 1 lists some specific hyperparameters used in our experiments. For fine-tuning, we used the same values as we used for pre-training regarding the values that are not listed in the table. We trained four L2R models and four R2L models with different seeds on the same data, then ensembled all of them by taking the union of their outputs. We adjusted the hyperparameters using the development set, described in the n"
2020.ngt-1.15,P19-1365,0,0.0898315,"tion (MT), MT evaluation, multilingual paraphrase, and language education technology fields. In Duolingo (the world’s largest language learning platform), some learning takes place via translation-based exercises and assessment is done by comparing the learners’ responses to a large set of acceptable human-generated translations. Therefore, retaining richer paraphrases of the translation results would help to generate more accurate feedback to the learners. Several studies have been conducted on the diversity of translation results (Vijayakumar et al., 2018; Xu et al., 2018; Shu et al., 2019; Ippolito et al., 2019). On the other hand, these methods rely on complex approaches. For example, modifying beam-search (Vijayakumar et al., 2018), introducing rewriting patterns or sentence codes (Xu et al., 2018; Shu et al., 2019) or using post-decoding clustering (Ippolito et al., 2019). However, we were curious if we can produce diverse outputs only using a simple approach. Therefore, we aim to generate a variety of translations simply using generally adopted neural MT (NMT) methods. For that purpose, we use the models trained on the left-to-right (L2R) and right-toleft (R2L) directions, where L2R produces targ"
2020.ngt-1.15,P07-2045,0,0.0263491,"ta, the left side indicates the number of prompts and the right side indicates the total number of sentences contained in each prompt. 3.2 F1 Table 4: The result for each model in terms of weighted F1 on the development set. terms of sentence length and data domain. We used STAPLE-train, OpenSubtitles, Tatoeba and TED-train as training data and STAPLE-dev, TEDdev and TED-test as development data for the pretraining. In fine-tuning, we used STAPLE-train as training data and STAPLE-dev as development data. We lowercased all the English data. English was tokenized using tokenizer.perl of Moses7 (Koehn et al., 2007) and Japanese was tokenized using MeCab8 with the IPA dictionary. After tokenization, we adopted sub-word segmentation mechanism (Sennrich et al., 2016)9 . Note that, for the training of R2L, we first applied tokenization for the target sentences, then applied sub-word segmentation and then performed the reversing. The size of the sub-word vocabularies was set to 8,000. The sub-word vocabularies were constructed using pre-train training data. http://opus.nlpl.eu/OpenSubtitles-v2018. php 7 5 8 https://github.com/moses-smt/mosesdecoder http://taku910.github.io/mecab 9 https://github.com/rsennric"
2020.ngt-1.15,L16-1147,0,0.0205921,"ne Model general translation ability during pre-training and further learns to produce more diverse translation during fine-tuning. Data Table 2 summarizes the size of data used in our experiments for En→Ja track. The official dataset of STAPLE contains multiple translations for a single prompt. We did not use the official development and test data in our experiments because the correct data with answers were not available to the public. Therefore, we randomly divided the official training data into training data and development data in prompt units as shown in Table 2. We use OpenSubtitles4 (Lison and Tiedemann, 2016), Tatoeba5 (Tiedemann, 2012), TED6 train and dev (Cettolo et al., 2012) corpora as additional dataset which are similar to the STAPLE data in 4 F1 Single seed 1 Single seed 2 Multi seed 23.7 23.4 23.9 L2R R2L L2R & R2L 23.7 23.2 24.7 Hyperparameters Table 1 lists some specific hyperparameters used in our experiments. For fine-tuning, we used the same values as we used for pre-training regarding the values that are not listed in the table. We trained four L2R models and four R2L models with different seeds on the same data, then ensembled all of them by taking the union of their outputs. We adj"
2020.ngt-1.15,2020.ngt-1.28,0,0.139564,"slation system that is able to produce diverse translations of each input sentence. However, creating such systems would require complex modifications in a model to ensure the diversity of outputs. In this paper, we investigated if it is possible to create such systems in a simple way and whether it can produce desired diverse outputs. In particular, we combined the outputs from forward and backward neural translation models (NMT). Our system achieved third place in En→Ja track, despite adopting only a simple approach. 1 Figure 1: Architecture of TMU system. Introduction WNGT20201 on STAPLE2 (Mayhew et al., 2020) addresses generating high-coverage sets of plausible translations which can be useful in machine translation (MT), MT evaluation, multilingual paraphrase, and language education technology fields. In Duolingo (the world’s largest language learning platform), some learning takes place via translation-based exercises and assessment is done by comparing the learners’ responses to a large set of acceptable human-generated translations. Therefore, retaining richer paraphrases of the translation results would help to generate more accurate feedback to the learners. Several studies have been conduct"
2020.ngt-1.15,N19-4009,0,0.0324515,"l techniques on an openended dialog task and image captioning task. These works introduce different complex modifications to the model in order to achieve diversity while generating the output. However, in this paper, we show how to simply generate diverse outputs. Transformer-big 20 4,096 Adam (β1 = 0.9, β2 = 0.98, ϵ = 1 × 10−8 ) 5 × 10−4 inverse sqrt 4,000 1 × 10−9 label smoothed cross-entropy (ϵls = 0.1) (Szegedy et al., 2016) 0.3 0.1 10 3 × 10−5 fixed Translation Beam size Ensemble 64 4 Table 1: Hyperparameter values of NMT model. 3 Experiments 3.1 System We used the open-source fairseq3 (Ott et al., 2019) for training NMT models. We adopt the Transformer (Vaswani et al., 2017) as our translation model. We train two types of models, L2R and R2L for decoding. For L2R, we train a forward model in a traditional way. For R2L model, we first reverse the target sentences and train a model so it will produce the output from backward. Then the output of R2L is reversed again to forward direction. We exclude sentences from the translation results by normalizing the log probabilities of the hypothesis sentences by sentence length with less than -1.55 score. Then, the n-best translation results of each L2"
2020.ngt-1.15,P16-1162,0,0.0311535,": The result for each model in terms of weighted F1 on the development set. terms of sentence length and data domain. We used STAPLE-train, OpenSubtitles, Tatoeba and TED-train as training data and STAPLE-dev, TEDdev and TED-test as development data for the pretraining. In fine-tuning, we used STAPLE-train as training data and STAPLE-dev as development data. We lowercased all the English data. English was tokenized using tokenizer.perl of Moses7 (Koehn et al., 2007) and Japanese was tokenized using MeCab8 with the IPA dictionary. After tokenization, we adopted sub-word segmentation mechanism (Sennrich et al., 2016)9 . Note that, for the training of R2L, we first applied tokenization for the target sentences, then applied sub-word segmentation and then performed the reversing. The size of the sub-word vocabularies was set to 8,000. The sub-word vocabularies were constructed using pre-train training data. http://opus.nlpl.eu/OpenSubtitles-v2018. php 7 5 8 https://github.com/moses-smt/mosesdecoder http://taku910.github.io/mecab 9 https://github.com/rsennrich/subword-nmt http://opus.nlpl.eu/Tatoeba.php 6 https://wit3.fbk.eu 136 Source Output 1 Output 2 your skirt is out of fashion. あなたのスカートは時代遅れである。 (Your s"
2020.ngt-1.15,P19-1177,0,0.0543544,"in machine translation (MT), MT evaluation, multilingual paraphrase, and language education technology fields. In Duolingo (the world’s largest language learning platform), some learning takes place via translation-based exercises and assessment is done by comparing the learners’ responses to a large set of acceptable human-generated translations. Therefore, retaining richer paraphrases of the translation results would help to generate more accurate feedback to the learners. Several studies have been conducted on the diversity of translation results (Vijayakumar et al., 2018; Xu et al., 2018; Shu et al., 2019; Ippolito et al., 2019). On the other hand, these methods rely on complex approaches. For example, modifying beam-search (Vijayakumar et al., 2018), introducing rewriting patterns or sentence codes (Xu et al., 2018; Shu et al., 2019) or using post-decoding clustering (Ippolito et al., 2019). However, we were curious if we can produce diverse outputs only using a simple approach. Therefore, we aim to generate a variety of translations simply using generally adopted neural MT (NMT) methods. For that purpose, we use the models trained on the left-to-right (L2R) and right-toleft (R2L) directions,"
2020.ngt-1.15,tiedemann-2012-parallel,0,0.0623828,"uring pre-training and further learns to produce more diverse translation during fine-tuning. Data Table 2 summarizes the size of data used in our experiments for En→Ja track. The official dataset of STAPLE contains multiple translations for a single prompt. We did not use the official development and test data in our experiments because the correct data with answers were not available to the public. Therefore, we randomly divided the official training data into training data and development data in prompt units as shown in Table 2. We use OpenSubtitles4 (Lison and Tiedemann, 2016), Tatoeba5 (Tiedemann, 2012), TED6 train and dev (Cettolo et al., 2012) corpora as additional dataset which are similar to the STAPLE data in 4 F1 Single seed 1 Single seed 2 Multi seed 23.7 23.4 23.9 L2R R2L L2R & R2L 23.7 23.2 24.7 Hyperparameters Table 1 lists some specific hyperparameters used in our experiments. For fine-tuning, we used the same values as we used for pre-training regarding the values that are not listed in the table. We trained four L2R models and four R2L models with different seeds on the same data, then ensembled all of them by taking the union of their outputs. We adjusted the hyperparameters us"
2020.nlptea-1.1,D13-1176,0,0.0203046,"ared with an AR decoder. To solve this problem, in recent studies, iteratively reﬁning the output (Lee et al., 2018; Gu et al., 2019) and partially autoregressively outputting the sentence divided into segments (Ran et al., 2020) have been proposed. Knowledge distillation (KD) (Kim and Rush, 2016) is also used to address this problem (Zhou et al., 2020). The output of the AR model is known to mitigate multimodality problems because diversity is suppressed such that the model can be easily learned (Ren et al., 2020). 2.1 AR NMT AR NMT is a standard decoding method in the encoder–decoder model (Kalchbrenner and Blunsom, 2013) for sequence-to-sequence learning. This method uses a recurrent language model (Mikolov et al., 2010) during inference. Given an original sentence, X = {x1 ,. . ., xT ′ }, and an objective sentence, Y = {y1 , . . . , yT }, an AR NMT model calculates the target sentence as p(yt |y0:t−1 , x1:T ′ ; θ), pF (ft′ |x1:T ′ ; θ)· t′ =1 where F is the set of all fertility sequences that sum into the length of Y , and x{f } represents token x repeated f times. As described earlier, it is necessary to predict the target sentence length in the NAR decoding method. Related Work p(Y |X; θ) = ′ T ∏ (2) • Usi"
2020.nlptea-1.1,2020.acl-main.391,0,0.0114172,"performs GEC; (5) it sends the correction to the front-end. (6) The front-end checks for changes; (a) it suggests changes to the user if there are changes; (b) otherwise, it does nothing. 2.4 GEC GEC is a task to correct errors, such as punctuation, grammar, and word selection errors. Various methods have been studied for this task. In recent years, owing to the development of NMT, GEC is often interpreted as a machine translation task. Almost all studies using the BEA Shared Task-2019 datasets (Bryant et al., 2019) used Transformerbased models (Omelianchuk et al., 2020; Kiyono et al., 2019; Kaneko et al., 2020; Grundkiewicz et al., 2019; Choe et al., 2019; Li et al., 2019). For example, in Li et al. (2019), a system that combined a CNN-based model with a Transformerbased model was used, and the method in Chollampatt and Ng (2018) was adopted as the CNN architecture. The following are previous studies on highspeed GEC using an NAR model. In Awasthi et al. (2019), GEC was regarded as a local sequence 3.2 Challenges for the System In this section, we consider the system input ((1) and (2)) and response time ((2) to (6a)). System Input We consider two problems with input from users of the system. The ﬁ"
2020.nlptea-1.1,D19-1435,0,0.104823,"of GEC in a writing support system, a slower inference speed would restrict its utility or lower the usability of the model. In Gu et al. (2018), a non-autoregressive (NAR) decoder that speeds up inference time by outputting all tokens simultaneously was proposed. Following the success of NAR models, in Gu et al. (2019), Levenshtein Transformer, an NAR NMT model that iteratively deletes and inserts inputs, was proposed. Its usefulness was veriﬁed in machine translation and document summarization tasks. Moreover, fast GEC methods with sequence tagging using an NAR model have been proposed. In Awasthi et al. (2019), GEC was regarded as a local sequence conversion task, and high-speed GEC was achieved by using an NAR model that iteratively adapted editing tags in parallel. In Omelianchuk et al. (2020), NAR GEC was performed by repetitive tagging of editing operations on each token of an input sentence, and higher correction accuracy and faster correction speed than in previous studies were achieved. However, these methods exhibited good performance by narrowing down the target language to English and preparing the editing operations as tags using language knowledge in advance. In this study, we focus on"
2020.nlptea-1.1,D16-1139,0,0.0242444,"d speed, aiming for the construction of a writing support system. ) p(yt |x1 {f1 }, . . . , xT ′ {fT ′ }; θ) , Furthermore, NAR NMT involves a problem named the multimodality problem (Gu et al., 2018). This problem causes errors (such as token repetitions and a lack of tokens) and signiﬁcantly deteriorates accuracy compared with an AR decoder. To solve this problem, in recent studies, iteratively reﬁning the output (Lee et al., 2018; Gu et al., 2019) and partially autoregressively outputting the sentence divided into segments (Ran et al., 2020) have been proposed. Knowledge distillation (KD) (Kim and Rush, 2016) is also used to address this problem (Zhou et al., 2020). The output of the AR model is known to mitigate multimodality problems because diversity is suppressed such that the model can be easily learned (Ren et al., 2020). 2.1 AR NMT AR NMT is a standard decoding method in the encoder–decoder model (Kalchbrenner and Blunsom, 2013) for sequence-to-sequence learning. This method uses a recurrent language model (Mikolov et al., 2010) during inference. Given an original sentence, X = {x1 ,. . ., xT ′ }, and an objective sentence, Y = {y1 , . . . , yT }, an AR NMT model calculates the target sente"
2020.nlptea-1.1,W19-4406,0,0.0202026,"front-end detects the change; (3) it sends the changed sentence to the back-end. (4) The back-end performs GEC; (5) it sends the correction to the front-end. (6) The front-end checks for changes; (a) it suggests changes to the user if there are changes; (b) otherwise, it does nothing. 2.4 GEC GEC is a task to correct errors, such as punctuation, grammar, and word selection errors. Various methods have been studied for this task. In recent years, owing to the development of NMT, GEC is often interpreted as a machine translation task. Almost all studies using the BEA Shared Task-2019 datasets (Bryant et al., 2019) used Transformerbased models (Omelianchuk et al., 2020; Kiyono et al., 2019; Kaneko et al., 2020; Grundkiewicz et al., 2019; Choe et al., 2019; Li et al., 2019). For example, in Li et al. (2019), a system that combined a CNN-based model with a Transformerbased model was used, and the method in Chollampatt and Ng (2018) was adopted as the CNN architecture. The following are previous studies on highspeed GEC using an NAR model. In Awasthi et al. (2019), GEC was regarded as a local sequence 3.2 Challenges for the System In this section, we consider the system input ((1) and (2)) and response tim"
2020.nlptea-1.1,D19-1119,0,0.0549192,"r problem is the correction speed. When the speed is slow, the usability of the system is limited, and the user experience is degraded. Therefore, in this study, we also focus on the non-autoregressive (NAR) model, which is a widely studied fast decoding method. We perform GEC in Japanese with traditional autoregressive and recent NAR models and analyze their accuracy and speed. 1 Introduction Grammatical error correction (GEC) is a writing support method for language learners. In recent years, neural GEC has been actively researched owing to its ability to produce ﬂuent text. For example, in Kiyono et al. (2019), state-of-the-art correction accuracy was achieved by using a Transformer (Vaswani et al., 2017), which is a powerful neural machine translation (NMT) model. Because the neural model can see the entire sequence, it can correct errors with long-range dependencies; these errors cannot be corrected by a statistical method that uses n-grams. However, considering the application of GEC in a writing support system, we must consider how to handle incomplete sentences. It is easy to present the GEC result when the user ﬁnishes writing a sentence. However, in case of an incomplete sentence, the user w"
2020.nlptea-1.1,W19-4423,0,0.0143194,"e front-end. (6) The front-end checks for changes; (a) it suggests changes to the user if there are changes; (b) otherwise, it does nothing. 2.4 GEC GEC is a task to correct errors, such as punctuation, grammar, and word selection errors. Various methods have been studied for this task. In recent years, owing to the development of NMT, GEC is often interpreted as a machine translation task. Almost all studies using the BEA Shared Task-2019 datasets (Bryant et al., 2019) used Transformerbased models (Omelianchuk et al., 2020; Kiyono et al., 2019; Kaneko et al., 2020; Grundkiewicz et al., 2019; Choe et al., 2019; Li et al., 2019). For example, in Li et al. (2019), a system that combined a CNN-based model with a Transformerbased model was used, and the method in Chollampatt and Ng (2018) was adopted as the CNN architecture. The following are previous studies on highspeed GEC using an NAR model. In Awasthi et al. (2019), GEC was regarded as a local sequence 3.2 Challenges for the System In this section, we consider the system input ((1) and (2)) and response time ((2) to (6a)). System Input We consider two problems with input from users of the system. The ﬁrst is how to process a sentence in the middle"
2020.nlptea-1.1,2020.lrec-1.26,1,0.765155,"r-based NAR neural model, to GEC. We update the model 300,000 times with a batch size of 64,000 tokens and select the model with the highest GLEU score (Napoles et al., 2016) for the validation set. Other hyperparameters are the same as in Gu et al. (2019). We use publicly available PyTorch-based code10 for implementation. In this paper, this model is called the LevT model. The maximum number of iterative reﬁnements was set to nine in a previous study (Gu et al., Dataset We use data from the Lang-8 learner corpus (Mizumoto et al., 2011). We use the TMU Evaluation Corpus for Japanese Learners (Koyama et al., 2020) for the validation and test sets5 . All data, including the training set, are preprocessed as in Koyama et al. (2020). Table 1 presents the number of sentences in the data. We use the same training set in our experiments with both complete and incomplete sentences. To evaluate the performance of GEC for incomplete sentences, we segment the test data to the word level and then create incomplete sentences 6 https://taku910.github.io/mecab/ https://unidic.ninjal.ac.jp/ 8 As a preliminary experiment, the source side was set to the character unit, and the target side was set to the subword unit; h"
2020.nlptea-1.1,D18-1149,0,0.0113952,"ons of this study can be summarized as follows. p(Y |X; θ) = f1 ,...,fT ′ ∈F T ∏ • We evaluate the performance of NAR and AR models for incomplete sentences in terms of accuracy and speed, aiming for the construction of a writing support system. ) p(yt |x1 {f1 }, . . . , xT ′ {fT ′ }; θ) , Furthermore, NAR NMT involves a problem named the multimodality problem (Gu et al., 2018). This problem causes errors (such as token repetitions and a lack of tokens) and signiﬁcantly deteriorates accuracy compared with an AR decoder. To solve this problem, in recent studies, iteratively reﬁning the output (Lee et al., 2018; Gu et al., 2019) and partially autoregressively outputting the sentence divided into segments (Ran et al., 2020) have been proposed. Knowledge distillation (KD) (Kim and Rush, 2016) is also used to address this problem (Zhou et al., 2020). The output of the AR model is known to mitigate multimodality problems because diversity is suppressed such that the model can be easily learned (Ren et al., 2020). 2.1 AR NMT AR NMT is a standard decoding method in the encoder–decoder model (Kalchbrenner and Blunsom, 2013) for sequence-to-sequence learning. This method uses a recurrent language model (Mik"
2020.nlptea-1.1,W19-4427,0,0.0172422,"sends the correction to the front-end. (6) The front-end checks for changes; (a) it suggests changes to the user if there are changes; (b) otherwise, it does nothing. 2.4 GEC GEC is a task to correct errors, such as punctuation, grammar, and word selection errors. Various methods have been studied for this task. In recent years, owing to the development of NMT, GEC is often interpreted as a machine translation task. Almost all studies using the BEA Shared Task-2019 datasets (Bryant et al., 2019) used Transformerbased models (Omelianchuk et al., 2020; Kiyono et al., 2019; Kaneko et al., 2020; Grundkiewicz et al., 2019; Choe et al., 2019; Li et al., 2019). For example, in Li et al. (2019), a system that combined a CNN-based model with a Transformerbased model was used, and the method in Chollampatt and Ng (2018) was adopted as the CNN architecture. The following are previous studies on highspeed GEC using an NAR model. In Awasthi et al. (2019), GEC was regarded as a local sequence 3.2 Challenges for the System In this section, we consider the system input ((1) and (2)) and response time ((2) to (6a)). System Input We consider two problems with input from users of the system. The ﬁrst is how to process a sen"
2020.nlptea-1.1,I11-1017,1,0.697218,"y, we apply the Levenshtein Transformer (Gu et al., 2019), which is a Transformer-based NAR neural model, to GEC. We update the model 300,000 times with a batch size of 64,000 tokens and select the model with the highest GLEU score (Napoles et al., 2016) for the validation set. Other hyperparameters are the same as in Gu et al. (2019). We use publicly available PyTorch-based code10 for implementation. In this paper, this model is called the LevT model. The maximum number of iterative reﬁnements was set to nine in a previous study (Gu et al., Dataset We use data from the Lang-8 learner corpus (Mizumoto et al., 2011). We use the TMU Evaluation Corpus for Japanese Learners (Koyama et al., 2020) for the validation and test sets5 . All data, including the training set, are preprocessed as in Koyama et al. (2020). Table 1 presents the number of sentences in the data. We use the same training set in our experiments with both complete and incomplete sentences. To evaluate the performance of GEC for incomplete sentences, we segment the test data to the word level and then create incomplete sentences 6 https://taku910.github.io/mecab/ https://unidic.ninjal.ac.jp/ 8 As a preliminary experiment, the source side was"
2020.nlptea-1.1,2020.bea-1.16,0,0.485383,"t speeds up inference time by outputting all tokens simultaneously was proposed. Following the success of NAR models, in Gu et al. (2019), Levenshtein Transformer, an NAR NMT model that iteratively deletes and inserts inputs, was proposed. Its usefulness was veriﬁed in machine translation and document summarization tasks. Moreover, fast GEC methods with sequence tagging using an NAR model have been proposed. In Awasthi et al. (2019), GEC was regarded as a local sequence conversion task, and high-speed GEC was achieved by using an NAR model that iteratively adapted editing tags in parallel. In Omelianchuk et al. (2020), NAR GEC was performed by repetitive tagging of editing operations on each token of an input sentence, and higher correction accuracy and faster correction speed than in previous studies were achieved. However, these methods exhibited good performance by narrowing down the target language to English and preparing the editing operations as tags using language knowledge in advance. In this study, we focus on the NAR model as a method for high-speed GEC. We perform GEC in Japanese using the NAR model that does not need to prepare editing operations in advance. We analyze the proposed method cons"
2020.nlptea-1.1,2020.acl-main.277,0,0.0212788,"NAR and AR models for incomplete sentences in terms of accuracy and speed, aiming for the construction of a writing support system. ) p(yt |x1 {f1 }, . . . , xT ′ {fT ′ }; θ) , Furthermore, NAR NMT involves a problem named the multimodality problem (Gu et al., 2018). This problem causes errors (such as token repetitions and a lack of tokens) and signiﬁcantly deteriorates accuracy compared with an AR decoder. To solve this problem, in recent studies, iteratively reﬁning the output (Lee et al., 2018; Gu et al., 2019) and partially autoregressively outputting the sentence divided into segments (Ran et al., 2020) have been proposed. Knowledge distillation (KD) (Kim and Rush, 2016) is also used to address this problem (Zhou et al., 2020). The output of the AR model is known to mitigate multimodality problems because diversity is suppressed such that the model can be easily learned (Ren et al., 2020). 2.1 AR NMT AR NMT is a standard decoding method in the encoder–decoder model (Kalchbrenner and Blunsom, 2013) for sequence-to-sequence learning. This method uses a recurrent language model (Mikolov et al., 2010) during inference. Given an original sentence, X = {x1 ,. . ., xT ′ }, and an objective sentence"
2020.nlptea-1.1,2020.acl-main.15,0,0.0124857,"rors (such as token repetitions and a lack of tokens) and signiﬁcantly deteriorates accuracy compared with an AR decoder. To solve this problem, in recent studies, iteratively reﬁning the output (Lee et al., 2018; Gu et al., 2019) and partially autoregressively outputting the sentence divided into segments (Ran et al., 2020) have been proposed. Knowledge distillation (KD) (Kim and Rush, 2016) is also used to address this problem (Zhou et al., 2020). The output of the AR model is known to mitigate multimodality problems because diversity is suppressed such that the model can be easily learned (Ren et al., 2020). 2.1 AR NMT AR NMT is a standard decoding method in the encoder–decoder model (Kalchbrenner and Blunsom, 2013) for sequence-to-sequence learning. This method uses a recurrent language model (Mikolov et al., 2010) during inference. Given an original sentence, X = {x1 ,. . ., xT ′ }, and an objective sentence, Y = {y1 , . . . , yT }, an AR NMT model calculates the target sentence as p(yt |y0:t−1 , x1:T ′ ; θ), pF (ft′ |x1:T ′ ; θ)· t′ =1 where F is the set of all fertility sequences that sum into the length of Y , and x{f } represents token x repeated f times. As described earlier, it is necess"
2020.nlptea-1.1,P16-1162,0,0.0208814,"m the beginning. For example, 10 sentences are created from a 10-word sentence. Next, based on the word alignment between the source and target sentences, we create parallel sentences for incomplete sentences. Consequently, 9,710 sentence pairs are created. We use these data to evaluate the performance of GEC for incomplete sentences. Tokenization We tokenize data in all models as follows. First, we segment data into morpheme units using MeCab6 (Ver. 0.996) using the UniDic7 (Ver. 2.2.0) as a dictionary. Next, we divide the morpheme units into subword units by applying the byte pair encoding (Sennrich et al., 2016) model for dealing with rare words. We apply character normalization (compatibility decomposition, followed by canonical composition) and share vocabulary between source and target sides.8 The vocabulary size was set to 30,000 words. We use sentencepiece9 for implementation. Response Time The processing speed from (2) to (6a) in the system ﬂow dominates the response time of the system, which affects the user experience. It is known that not only is responsiveness required, but also users prefer a system with constant response speed over a system with variable response speed (Shneiderman, 1979)"
2020.nlptea-1.11,W19-4406,0,0.0143447,"m achieves F1 scores of 82.00, 52.66, 22.24, 14.17 on detection level, iden87 Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications, pages 87–90 c Suzhou, China, December 4, 2020. 2020 Association for Computational Linguistics [Original Sentence] 然后 准备 别 的 材料 。 [Original BERT] 然 后 准 [MASK] 别 的 [MASK] 料 。 [Whole Word Masking] 然 后 [MASK] [MASK] 别 的 [MASK] [MASK] 。 [English Translation] Then prepare for other materials. tification level, position level and correction level evaluation. 2 Related Work In Building Educational Applications (BEA) 2019 (Bryant et al., 2019), several teams attempted to incorporate BERT into their correction models. Kaneko et al. (2019) first fine-tuned BERT on a learner corpus and then incorporated the word probability provided by BERT into re-ranking features. Using BERT for re-ranking features, they obtained an approximately 0.7 point improvement of the F0.5 score. Kantor et al. (2019) used BERT to solve the GEC task by iteratively querying BERT as a black box language model. They added a [MASK] token into source sentences and predicted the word represented by the [MASK] token. If the word probability predicted by BERT exceeded"
2020.nlptea-1.11,2020.aacl-main.20,1,0.835863,"(Wu et al., 2016) to preprocess Chinese sentences, and Chinese sentences are segmented into characters (not subwords) by WordPiece. Therefore, in WWM, when a Chinese character is masked, other Chinese characters that belong to the same word should also be masked. Table 1 shows an example of WWM. Training Strategy Cui et al. (2019) followed the training strategy studied by Liu et al. (2019). Although Cui et al. (2019) referred to the training strategy from Liu et al. (2019), there are still some differences between them (e.g., they did not use dynamic masking). Method We adopt the method from Wang et al. (2020) to construct our correction model. Additional details are introduced in the following sections. 3.1 Training Data In addition to Chinese Wikipedia (0.4B tokens) that was originally used to train BERT, an extended corpus (5.0B tokens), which consists of Baidu Baike (a Chinese encyclopedia) and QA data, was also used. The extended corpus has not been released due to a license issue. Chinese Pre-trained Model We use a BERT-based model as our pre-trained model. BERT is mainly trained with a task called Masked Language Model. In the Masked Language Model task, some tokens in a sentence are replace"
2020.nlptea-1.11,N19-1423,0,0.164128,"n extensively studied, and several downstream tasks have benefited from their utilization. In this study, we treat the grammar error diagnosis (GED) task as a grammatical error correction (GEC) problem and use a method that incorporates a pre-trained model into an encoderdecoder model to solve this problem. 1 Figure 1: The structure of our system. Introduction necessary to utilize a pre-trained model, which captures additional information about a language. In recent years, pre-trained models based on Bidirectional Encoder Representations from Transformers (BERT) have been studied extensively (Devlin et al., 2019; Liu et al., 2019), and the performance of many downstream Natural Language Processing (NLP) tasks has been dramatically improved by utilizing these pre-trained models. In order to learn existing knowledge of a language, a BERT-based pre-trained model is trained on a large-scale corpus using the encoder of Transformer (Vaswani et al., 2017). Subsequently, for a downstream task, a neural network model is initialized with the weights learned by a pre-trained model that has the same structure and is fine-tuned on training data of the downstream task. The performance is expected to improve by usi"
2020.nlptea-1.11,N18-1055,0,0.0276483,"Masked Language Model task, some tokens in a sentence are replaced with masked tokens ([MASK]), and the model needs to predict the replaced tokens. In this study, we use the Chinese-RoBERTawwm-ext model provided by Cui et al. (2019). The main differences between Chinese-RoBERTawwm-ext and original BERT are as follows: 3.2 Grammatical Error Correction Model In this study, we use Transformer as our correction model. Transformer has shown excellent performance in sequence-to-sequence tasks such as machine translation and has been widely adopted in recent English GEC studies (Kiyono et al., 2019; Junczys-Dowmunt et al., 2018). However, a BERT-based pre-trained model only uses the encoder of Transformer; therefore, it can not be directly applied to sequence-to-sequence tasks that require both an encoder and a decoder, such as GEC. Hence, we initialize the encoder of Transformer with the parameters learned by Chinese-RoBERTa-wwm-ext, and the decoder is initialized randomly. Finally, we fine-tune this initialized model on Chinese GEC data and use it as our correction model. Whole Word Masking (WWM) Devlin et al. (2019) proposed a new masking method called Whole Word Masking (WWM) after proposing their original BERT,"
2020.nlptea-1.11,D19-1119,0,0.0219376,"nguage Model. In the Masked Language Model task, some tokens in a sentence are replaced with masked tokens ([MASK]), and the model needs to predict the replaced tokens. In this study, we use the Chinese-RoBERTawwm-ext model provided by Cui et al. (2019). The main differences between Chinese-RoBERTawwm-ext and original BERT are as follows: 3.2 Grammatical Error Correction Model In this study, we use Transformer as our correction model. Transformer has shown excellent performance in sequence-to-sequence tasks such as machine translation and has been widely adopted in recent English GEC studies (Kiyono et al., 2019; Junczys-Dowmunt et al., 2018). However, a BERT-based pre-trained model only uses the encoder of Transformer; therefore, it can not be directly applied to sequence-to-sequence tasks that require both an encoder and a decoder, such as GEC. Hence, we initialize the encoder of Transformer with the parameters learned by Chinese-RoBERTa-wwm-ext, and the decoder is initialized randomly. Finally, we fine-tune this initialized model on Chinese GEC data and use it as our correction model. Whole Word Masking (WWM) Devlin et al. (2019) proposed a new masking method called Whole Word Masking (WWM) after"
2020.paclic-1.61,N15-1184,0,0.0683515,"Missing"
2020.paclic-1.61,W14-2517,0,0.0194481,"ranslation model is trained with only a small parallel corpus. Qi et al. (2018) demonstrated that the use of pre-trained word embeddings for the training of NMT improved the translation performance for language pairs that had only a small parallel corpus. For the domain adaptation method of word embeddings, we employed ﬁne-tuning. Faruqui et al. (2015) proposed the retroﬁtting method, demonstrating that ﬁne-tuning with another corpus improved the quality of the word embeddings. Yaginuma et al. (2018) performed word sense disambiguation in Japanese using ﬁne-tuned word embeddings. In addition, Kim et al. (2014) performed diachronic ﬁne-tuning. They automatically detected changes in language over time through a chronologically trained neural language model. They obtained word embeddings speciﬁc to each year and demonstrated that some words had changed their meanings. Based on their research, we believe that diachronically domain-adapted word embeddings can capture changes in language meanings over time. 3 Diachronic Domain Adaptation of Word Embeddings Using Historical Corpus In this study, we propose the initialization of inputs to the NMT model with diachronically domainadapted word embeddings. Fol"
2020.paclic-1.61,W17-3204,0,0.0237751,"udied. Although there is a study of statistical machine translation (SMT) from historical Japanese to contemporary Japanese (Hoshino et al., 2014), to the best of our knowledge, there have been no studies on an NMT system that translates historical Japanese to contemporary Japanese. NMT systems generally output ﬂuent translations. Therefore, NMT is expected to improve the ﬂuency of contemporary Japanese translation. However, it is difﬁcult to obtain a model with high performance when only small parallel corpora are available, as NMT systems usually require large parallel corpora for training (Koehn and Knowles, 2017). Because the available parallel corpus of historical and contemporary Japanese is small, NMT is not appropriate for translation from historical Japanese to contemporary Japanese. To improve translation performance, translation models are sometimes initialized with pre-trained word embeddings trained with a large corpus for language pairs that do not have sufﬁcient parallel corpora (Qi et al., 2018). We believe that this method can also be effective for the translation model from historical Japanese to contemporary Japanese. To obtain high-quality word embeddings, it is desirable to train them"
2020.paclic-1.61,D15-1166,0,0.0494313,". (1) Fine-tune word embeddings pre-trained with contemporary Japanese using the modern corpus, (2) Fine-tune the corpus obtained in step (1) using the Muromachi corpus, (3) Fine-tune the corpus obtained in step (2) using the Kamakura corpus, (4) Fine-tune the corpus obtained in step (3) using the Heian corpus. 4 Experiments 4.1 Model In our experiments, we employed an encoderdecoder model2 based on long short-term memory (LSTM) with attention. OpenNMT3 , an opensource NMT tool, was used for implementation. We utilized two unidirectional LSTM layers for the hidden layers and global attention (Luong et al., 2015) for attention. The vector sizes of the word embeddings and the hidden layers were set to 200 and 512, respectively, for both the encoder and decoder. Adam was used as the optimization algorithm, and the learning rate was set to 0.001. The vocabulary size treated by the model was limited to 20,000 for each of the source and target data, and unknown words were processed as <unk> tokens. The hyper parameters were determined according to preliminary experiments. We initialized the weights of the word embedding layer of the translation model. In this study, the word embeddings pre-trained with the"
2020.paclic-1.61,N13-1090,0,0.0439861,"Missing"
2020.paclic-1.61,ogiso-etal-2012-unidic,1,0.666485,"o of the number of sentences of the entire corpus for each period in which the sentences were written is identical to that of the text examples. The number of sentences in the modern, Kamakura, and Heian corpora was 4577, 30,075, and 52,032, respectively, totaling to 86,684 sentences. Therefore, the number of sentences in the modern, Kamakura, and Heian corpora was 123, 739, and 1231, respectively. (123:739:1,231) = (4,577:30,075:52,032) The number of examples in the test set is presented in Table 2. We used MeCab v0.9967 as a morphological analyzer and UniDic for Early Middle Japanese v1.38 (Ogiso et al., 2012) and UniDic v2.3.04 (Maekawa et al., 2010) as dictionaries for historical and contemporary Japanese, respectively. We limited the length of an input or output sentence to 100 words. Historical Japanese Total Number of Sentences 86,684 Vocabulary Size 49,200 Number of Tokens 2,774,745 Contemporary Japanese Total Number of Sentences 86,684 Vocabulary Size 45,690 Number of Tokens 3,611,783 Table 1: Parallel corpus of historical and contemporary Japanese.The data for translation are extracted from parallel corpus of Complete Collection of Japanese Classical Literature (see Table 3). 4.3 Word Embed"
2020.paclic-1.61,P02-1040,0,0.107385,"rce and target data, and unknown words were processed as <unk> tokens. The hyper parameters were determined according to preliminary experiments. We initialized the weights of the word embedding layer of the translation model. In this study, the word embeddings pre-trained with the contemporary Japanese corpus were diachronically domainadapted using the historical Japanese corpus, and used to initialize the weights of the word embedding layer of the encoder of the translation model. These word embeddings were also directly used for the decoder of the translation model. We used the BLEU score (Papineni et al., 2002) to evaluate the translation model. Each method was given different seeds validated on each 5,000th step and was tested with the translation model with the highest BLEU score. The average BLEU scores over 2 We also tried a transformer model but the performance greatly varied depending on each trial. Also, the averaged performance did not surpass the encoder-decoder model. Therefore we decided to use an encode-decoder model. 3 https://github.com/OpenNMT/OpenNMT three trials using different seeds were evaluated as the scores of the translation models. For comparison, we conducted experiments in"
2020.paclic-1.61,N18-2084,0,0.0724282,"nslation. However, it is difﬁcult to obtain a model with high performance when only small parallel corpora are available, as NMT systems usually require large parallel corpora for training (Koehn and Knowles, 2017). Because the available parallel corpus of historical and contemporary Japanese is small, NMT is not appropriate for translation from historical Japanese to contemporary Japanese. To improve translation performance, translation models are sometimes initialized with pre-trained word embeddings trained with a large corpus for language pairs that do not have sufﬁcient parallel corpora (Qi et al., 2018). We believe that this method can also be effective for the translation model from historical Japanese to contemporary Japanese. To obtain high-quality word embeddings, it is desirable to train them using a large training corpus. Therefore, the use of word embeddings trained with a contemporary Japanese corpus that is much larger than the historical Japanese corpus is expected. However, when the word embeddings trained with contemporary Japanese are directly used for the translation model, the model is expected to have poorer performance because the domains of the word embeddings and inputs di"
2020.wat-1.15,D19-1165,0,0.0262257,"nto Hanja helped translate the homophone of SK words. Yoo et al. (2019) proposed an approach of training Korean word representations using the data in which SK words were converted into Hanja words. They demonstrated the effectiveness of the representation learning method on several downstream tasks, such as a news headline generation and sentiment analysis. To train the Korean-to-Japanese translation model, we combine these two approaches using Hanja information for training the translation model and word embeddings to improve the translation quality. In domain adaptation approaches for NMT, Bapna and Firat (2019); Gu et al. (2019) trained an NMT model pre-trained with massive parallel data and retrained it with small parallel data within the target domain. In addition, Hu et al. (2019) 128 Model Baseline Hanja-conversion Partition train dev test-n test-n1 test-n2 test-n3 train dev test-n test-n1 test-n2 test-n3 Korean Tokens Types 31,151,846 21,936 106,433 6,475 272,975 9,530 108,327 6,425 153,195 7,522 11,453 1,666 32,066,032 26,046 109,460 6,944 298,404 9,832 111,543 6,941 175,131 6,410 11,730 1,759 Sent. 999,758 2,000 5,230 2,000 3,000 230 999,755 2,000 5,230 2,000 3,000 230 Japanese Tokens Types 3"
2020.wat-1.15,N19-1423,0,0.0112716,"rained model trained by all training data. We translate the test data for each domain using a domainspecific translation model. For training and validation, we split the training and development data into four domains: chemistry, electricity, mechanical engineering, and physics using domain information. We use these data to build domain-specific translation models. For testing, we use the domain information annotated with the test-n1 data. However, the test-n2 and test-n3 data do not have domain information. Therefore, we train a domain prediction model by fine-tuning Korean or Japanese BERT (Devlin et al., 2019) using the labeled training data of the Japan Patent Office Patent Corpus 2.0 to predict the domain information of test-n2 and test-n3 data. 5 Experimental Settings 5.1 Implementation We use the fairseq4 implementation of the transformer architecture for the baseline model and the Hanja-conversion model and extend the implementation for the Hanja-loss model. Table 7 presents some specific hyperparameters that are used in all models. To train the domain prediction model for domain adaptation (Section 4), we used the BidirectionalWordPiece tokenizer, character model of KR-BERT5 (Lee et al., 2020"
2020.wat-1.15,N19-1312,0,0.0203225,"te the homophone of SK words. Yoo et al. (2019) proposed an approach of training Korean word representations using the data in which SK words were converted into Hanja words. They demonstrated the effectiveness of the representation learning method on several downstream tasks, such as a news headline generation and sentiment analysis. To train the Korean-to-Japanese translation model, we combine these two approaches using Hanja information for training the translation model and word embeddings to improve the translation quality. In domain adaptation approaches for NMT, Bapna and Firat (2019); Gu et al. (2019) trained an NMT model pre-trained with massive parallel data and retrained it with small parallel data within the target domain. In addition, Hu et al. (2019) 128 Model Baseline Hanja-conversion Partition train dev test-n test-n1 test-n2 test-n3 train dev test-n test-n1 test-n2 test-n3 Korean Tokens Types 31,151,846 21,936 106,433 6,475 272,975 9,530 108,327 6,425 153,195 7,522 11,453 1,666 32,066,032 26,046 109,460 6,944 298,404 9,832 111,543 6,941 175,131 6,410 11,730 1,759 Sent. 999,758 2,000 5,230 2,000 3,000 230 999,755 2,000 5,230 2,000 3,000 230 Japanese Tokens Types 31,065,360 24,178 1"
2020.wat-1.15,P19-1286,0,0.0220505,"nto Hanja words. They demonstrated the effectiveness of the representation learning method on several downstream tasks, such as a news headline generation and sentiment analysis. To train the Korean-to-Japanese translation model, we combine these two approaches using Hanja information for training the translation model and word embeddings to improve the translation quality. In domain adaptation approaches for NMT, Bapna and Firat (2019); Gu et al. (2019) trained an NMT model pre-trained with massive parallel data and retrained it with small parallel data within the target domain. In addition, Hu et al. (2019) 128 Model Baseline Hanja-conversion Partition train dev test-n test-n1 test-n2 test-n3 train dev test-n test-n1 test-n2 test-n3 Korean Tokens Types 31,151,846 21,936 106,433 6,475 272,975 9,530 108,327 6,425 153,195 7,522 11,453 1,666 32,066,032 26,046 109,460 6,944 298,404 9,832 111,543 6,941 175,131 6,410 11,730 1,759 Sent. 999,758 2,000 5,230 2,000 3,000 230 999,755 2,000 5,230 2,000 3,000 230 Japanese Tokens Types 31,065,360 24,178 104,307 6,098 269,876 9,018 106,947 6,137 151,253 6,656 11,676 1,644 30,474,136 27,541 103,994 6,119 269,146 9,166 106,653 6,162 150,844 6,717 11,649 1,634 Tab"
2020.wat-1.15,W15-5008,0,0.0635441,"Missing"
2020.wat-1.15,D19-5208,0,0.0262425,"ptation by fine-tuning the model pre-trained by all training data using domain-specific data. In this study, we examine the effect of Hanja information and domain adaptation in a Koreanto-Japanese translation. The main contributions of this study are as follows: • We demonstrate that Hanja information is effective for Korean to Japanese translations within a specific domain. • In addition, our experiment shows that the translation model using Hanja information tends to translate literally. 2 Related Work Several studies have been conducted on Korean– Japanese neural machine translation (NMT). Park et al. (2019) trained a Korean-to-Japanese translation model using a transformer-based NMT system with relative positioning, back-translation, and multi-source methods. There have been other attempts that combine statistical machine translation (SMT) and NMT (Ehara, 2018; HyoungGyu Lee and Lee, 2015). Previous studies on Korean–Japanese NMT did not use Hanja information, whereas we train a Korean-to-Japanese translation model using data in which SK words were converted into Hanja words. Sennrich et al. (2016) proposed byte-pair encoding (BPE), i.e., a sub-word segmentation method, and suggested that overla"
2020.wat-1.15,P16-1162,0,0.508794,"l reasons (Lee and Ramsey, 2011). Many words in Japanese are composed of Chinese characters called Kanji. By contrast, Korean uses the Korean alphabet called Hangul to write sentences in almost all cases. However, Sino-Korean1 (SK) words, which can be converted into Hanja words, account for 65 percent of the Korean lexicon (Sohn, 2006). Table 1 presents an example of conversions of SK words into Hanja, which are compatible with Japanese Kanji words. In addition, several studies have suggested that overlapping tokens between the source and target languages can improve the translation accuracy (Sennrich et al., 2016; Zhang and Komachi, 2019). Park and Zhao (2019) trained a Korean-to-Chinese translation model by converting Korean SK words from Hangul into Hanja to increase the vocabulary overlap. In other words, the meaning of a vocabulary overlap on NMT is that each corresponding word’s embeddings are the same. Conneau et al. (2018) and Lample and Conneau (2019) improved translation accuracy by making embeddings between source words and their corresponding target words closer. From this fact, we hypothesize that if the embeddings of each corresponding word are closer, the translation accuracy will improv"
2020.wat-1.15,D19-1358,0,0.0441431,"Missing"
2020.wat-1.4,I11-1016,0,0.0755489,"Missing"
2020.wat-1.4,W18-6408,0,0.0255747,"Missing"
2020.wat-1.4,Q17-1026,0,0.0172456,"Baseline Model In the following section, we describe a baseline model for character-based NE translation. Furthermore, we propose filters to remove noisy samples from the Wikititles dataset and demonstrate that the sanitized data could improve the performance of the model. 4.1 Model The attention-based encoder–decoder model is a well-known architecture for MT (Bahdanau et al., 2015; Vaswani et al., 2017). The model tackles MT as a sequence-to-sequence problem. Although the model was first proposed to operate at the level of words, recent papers have proposed character-level neural MT models (Lee et al., 2017). In the present study, we employed Bahdanau et al. (2015) as our baseline model for English–Chinese NE translation. 4.2 Experimental Setup Model The encoder of our model has two layers with 256 hidden dimensions; therefore, the bidirectional GRU has a dimension of 512 and the decoder GRU state has a dimension of 256. The input word embedding and output vector sizes are 256. For training, we used the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 0.001, clipping gradient norm of 1.0, dropout rate of 0.5, batch size of 512, and early stopping patience of 10. In the evaluation phas"
2020.wat-1.4,P07-1016,0,0.0662953,"glish letters, must be converted into Hanzi, which represent ideas and meanings. In terms of literal translation, because Hanzi usually express certain connotations, choosing the appropriate Hanzi should also be considered. In addition, owing to the lack of apparent semantic content on location names and people’s names, these words cannot be expressed in Chinese through words equivalent in meaning. Further, it is very likely that the standard translation of these words cannot be found in existing lexical resources, which increases the complexity of the task. A semantic transliteration method (Li et al., 2007) is proposed for the translation of personal names from English to Chinese, which considers the language of origin, gender, and the given or surname information of the source names. The approach aims at maintaining the phonetic equivalence as well as optimizing the semantic transfer. However, as highlighted in the paper, the research is a case study, and the proposed mathematical framework does not extend to the machine transliteration of NEs. 3.1 Annotation Tables 1 and 2 list the criteria used for human evaluation as well as some examples of our evaluation data. As a global criterion, we ign"
2020.wat-1.4,D09-1069,0,0.0705955,"Missing"
2020.wat-1.4,P02-1040,0,0.108036,"g training and validation data. All entities in both English and Chinese are split into characters, and the space is replaced with a special token (in our case, &lt;s&gt;). The vocabularies are built with all words from the original/sanitized training data, yielding 1,063/353 characters in English and 9,598/8,852 in Chinese. 4.3 Human evaluation Further, we manually annotated 338 outputs of the model trained with sanitized data using the criteria introduced in Sec. 3.1. Figure 1 depicts the correlation between the BLEU score and human evaluation scores, where we use the sentence-level BLEU-4 score (Papineni et al., 2002) as the BLEU score for each translation item. We set F + max(P, M ) as the human evaluation scores, where F, P, and M represent fluency, pronunciation, and meaning, respectively. Here, considering that certain NEs are translated using hybrid methods, max(P, M ) is used to balance the weights from transliteration and literal translation. The Pearson correlation coefficient is also calculated, as approximately 0.12. Both Figure 1 and the Pearson correlation coefficient indicate that there is nearly no correlation between these two scores. Results and Analysis General performance Table 4 presents"
2020.wat-1.4,2020.lrec-1.399,0,0.0705555,"Missing"
2020.wat-1.4,W15-3049,0,0.0449091,"Missing"
2020.wat-1.4,P98-1036,0,0.0880346,"evaluation. For example, the NE pair of (Curtiss-Wright, 柯蒂斯-莱特) is evaluated high in terms of pronunciation. However, its fluency in Chinese is not good because it is not an original Chinese word. • We propose the evaluation criteria for new EnZh NE (company name) translations—fluency, pronunciation, and meaning. • We create a baseline model for NE translations and analyze the results. • We provide and release a novel method of evaluation dataset 1 for En-Zh, focusing on company names, which includes both real NE translations and our system output. 2 Related Work In terms of NE translation (Chen et al., 1998; Wan and Verspoor, 1998; Oh et al., 2009), because the two languages use completely different symbolic representations in terms of graphemes and 1 https://github.com/toshohirasawa/ enzh-named-entity-translation 58 Proceedings of the 7th Workshop on Asian Translation, pages 58–63 c December 4, 2020. 2020 Association for Computational Linguistics Score Fluency Pronunciation Meaning 5 4 3 2 1 Original AND #splitting= 0 Original AND #splitting= 1 Original AND #splitting= 2 Original AND #splitting≥ 3 Others Similar AND #syllables are close Similar Others Translated AND Shortly Translated Others Ta"
2020.wat-1.4,Q17-1007,0,0.0119543,"ted in the paper, the research is a case study, and the proposed mathematical framework does not extend to the machine transliteration of NEs. 3.1 Annotation Tables 1 and 2 list the criteria used for human evaluation as well as some examples of our evaluation data. As a global criterion, we ignore certain common words that do not contribute to the translation of business names, such as Inc., corporation, and group. When evaluating the performance of MT, different types of human judgment including fluency and adequacy are employed. A quantitative and qualitative investigation was conducted by (Tu et al., 2017). They confirmed that the source and target contexts in neural MT are highly correlated with translation adequacy and fluency, respectively. For our study, this finding may indicate that the more common the translation using existing Chinese expressions, the better is its fluency. As for 2 https://en.wikipedia.org/wiki/ Category:Companies_listed_on_the_New_ York_Stock_Exchange 59 adequacy, the consistency between the source and target contexts should be prioritized in terms of both pronunciation and meaning. Fluency measures whether a translation is fluent, regardless of the correct meaning, b"
2020.wat-1.4,P98-2220,0,0.504359,"ample, the NE pair of (Curtiss-Wright, 柯蒂斯-莱特) is evaluated high in terms of pronunciation. However, its fluency in Chinese is not good because it is not an original Chinese word. • We propose the evaluation criteria for new EnZh NE (company name) translations—fluency, pronunciation, and meaning. • We create a baseline model for NE translations and analyze the results. • We provide and release a novel method of evaluation dataset 1 for En-Zh, focusing on company names, which includes both real NE translations and our system output. 2 Related Work In terms of NE translation (Chen et al., 1998; Wan and Verspoor, 1998; Oh et al., 2009), because the two languages use completely different symbolic representations in terms of graphemes and 1 https://github.com/toshohirasawa/ enzh-named-entity-translation 58 Proceedings of the 7th Workshop on Asian Translation, pages 58–63 c December 4, 2020. 2020 Association for Computational Linguistics Score Fluency Pronunciation Meaning 5 4 3 2 1 Original AND #splitting= 0 Original AND #splitting= 1 Original AND #splitting= 2 Original AND #splitting≥ 3 Others Similar AND #syllables are close Similar Others Translated AND Shortly Translated Others Table 1: Criteria for huma"
2020.wat-1.7,W17-4746,0,0.0304808,"Missing"
2020.wat-1.7,D15-1166,0,0.0529704,"indicate that translation performance can be improved using our method of textual data augmentation with noising on the target side and probabilistic dropping of either context vector. 1 • Textual data augmentation are better than visual data augmentation for MMT. Introduction • Placing noise on the target side of the augmented data is effective in improving translation performance in the English→Japanese direction. In recent years, neural machine translation (NMT) has become the standard machine translation system owing to its high performance (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). However, NMT requires considerable parallel corpora for training; thus, it does not perform well in situations where low-resource data are present. To address this issue, Sennrich et al. (2016a) proposed back-translation that generates pseudo-parallel data by translating monolingual data in the target language. Multimodal machine translation (MMT) is a task whose purpose is to generate better translations with information from other modalities (such as images) related to the source sentences (Specia et al., 2016). Owing to the nature of MMT, which requires image information paired with sente"
2020.wat-1.7,D17-1105,0,0.0192745,"MT have been proposed in the recent studies. Caglayan et al. (2016) and Calixto et al. (2017) proposed the doubly-attentive model wherein the encoder is a bi-directional gated recurrent unit (BiGRU) (Cho et al., 2014) that processes only the source sequence, and the decoder is a conditional GRU (CGRU)1 that simultaneously 1 https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 80 Proceedings of the 7th Workshop on Asian Translation, pages 80–91 c December 4, 2020. 2020 Association for Computational Linguistics pays attention to the source sequence and the spatial visual feature. Calixto and Liu (2017) also used global visual features to initialize either the encoder or the decoder of the attention-based NMT. In the WMT 17 Shared Task on MMT, the models using global features were shown to be better than those using spatial features (Caglayan et al., 2017a). Additionally, Gr¨onroos et al. (2018) adapted the Transformer (Vaswani et al., 2017) model to a multimodal setting and proposed concatenating the regional visual features encoded as a pseudo-word embedding to the word embeddings of the source sentence. According to them, however, the improvement achieved by incorporating visual informati"
2020.wat-1.7,P16-1009,0,0.181511,"xtual data augmentation are better than visual data augmentation for MMT. Introduction • Placing noise on the target side of the augmented data is effective in improving translation performance in the English→Japanese direction. In recent years, neural machine translation (NMT) has become the standard machine translation system owing to its high performance (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). However, NMT requires considerable parallel corpora for training; thus, it does not perform well in situations where low-resource data are present. To address this issue, Sennrich et al. (2016a) proposed back-translation that generates pseudo-parallel data by translating monolingual data in the target language. Multimodal machine translation (MMT) is a task whose purpose is to generate better translations with information from other modalities (such as images) related to the source sentences (Specia et al., 2016). Owing to the nature of MMT, which requires image information paired with sentences, • The use of the dropnet method leads to improvements in translation performance. 2 Related Work Several approaches to MMT have been proposed in the recent studies. Caglayan et al. (2016)"
2020.wat-1.7,P17-1175,0,0.0322213,"Missing"
2020.wat-1.7,W14-4012,0,0.0676188,"Missing"
2020.wat-1.7,N19-1423,0,0.0181909,"ented data2 . We incorporate the dropnet method (see Subsection 3.3) into only this model for regularization owing to its architecture that combines each context vector. Model NMT Model Our baseline NMT (Caglayan et al., 2017a) is an attentive encoder-decoder model, wherein the encoder is BiGRU, and the decoder is CGRU. Thus, our MMT models are based on RNNs. To generate synthetic data via textual data augmentation methods (see Subsection 4.1), we used the Transformer (Vaswani et al., 2017) model. 3.2 MMT Model with Double Attention Dropnet method Zhu et al. (2020) studied incorporating BERT (Devlin et al., 2019) into the 2 We conducted preliminary experiments for both MMTdecinit and MMTdatt models using augmented data, although our dataset used in those experiments had the different splits of the dataset given by WAT 2020. As a result, the baseline MMTdatt model is inferior to the MMTdecinit model; moreover, the baseline MMTdatt model outperformed the ones pretrained on the augmented data. We therefore do not train MMTdatt models using augmented data. MMT Model with Decoder Initialization This MMT model initializes the hidden state of the decoder of our baseline NMT with global visual 81 pnet pnet (1"
2020.wat-1.7,W16-2346,0,0.0898108,"Missing"
2020.wat-1.7,D18-1045,0,0.127243,"xternal parallel data can significantly improve the performance. In contrast, we augmented training data without external data. With respect to research on data augmentation in NMT, in addition to the method mentioned in Section 1, Fadaee et al. (2017) generated synthetic sentence pairs containing low-frequency words by leveraging the language models trained on large monolingual corpora. Under simulated lowresource settings, their results showed that translations using this augmentation approach have more low-frequency words than those not using this approach, leading to improved performance. Edunov et al. (2018) investigated back-translation at a large scale for generating useful synthetic source sentences using several approaches. They obtained back-translated data via sampling and noisy beam outputs and added them to parallel corpora. They found that the above methods outperform the ones that generate synthetic sentences based on argmax inference (e.g., beam or greedy search), except in low-resource settings. 3 3.1 features (Caglayan et al., 2017a). This model’s architecture is used for our baseline MMT as well as MMT models using augmented data. We denote this model as MMTdecinit . 3.3 In our MMT"
2020.wat-1.7,P17-2090,0,0.0176128,"lly, Gr¨onroos et al. (2018) adapted the Transformer (Vaswani et al., 2017) model to a multimodal setting and proposed concatenating the regional visual features encoded as a pseudo-word embedding to the word embeddings of the source sentence. According to them, however, the improvement achieved by incorporating visual information is modest, and they observed that external parallel data can significantly improve the performance. In contrast, we augmented training data without external data. With respect to research on data augmentation in NMT, in addition to the method mentioned in Section 1, Fadaee et al. (2017) generated synthetic sentence pairs containing low-frequency words by leveraging the language models trained on large monolingual corpora. Under simulated lowresource settings, their results showed that translations using this augmentation approach have more low-frequency words than those not using this approach, leading to improved performance. Edunov et al. (2018) investigated back-translation at a large scale for generating useful synthetic source sentences using several approaches. They obtained back-translated data via sampling and noisy beam outputs and added them to parallel corpora. Th"
2020.wat-1.7,W18-6439,0,0.105459,"Missing"
2020.wat-1.7,N18-1057,0,0.0216985,"lickr30k dataset. The Japanese training data size is originally 59,566 sentences, but four sentences are missing; thus, we use 59,562 sentences (both Japanese and English) for training. We use Moses (Koehn et al., 2007) scripts to lowercase, normalize, and tokenize English sentences, Textual Data Augmentation Sampling This method samples the hypothesis from the output distribution at each decoding step to generate synthetic parallel data (Edunov et al., 2018). Random noising This method was originally used to generate synthetic ungrammatical sentences in the grammatical error correction task (Xie et al., 2018). They penalized every hypothesis on the beam by adding noise rβ to its hypothesis’ score, where r is drawn from the uniform distribution on the interval [0, 1] during the beam search procedure, and β controls the noise intensity. If β is sufficiently large, this method is similar to the method that randomly shuffles the ranks of the hypotheses 3 https://github.com/nlab-mpg/ Flickr30kEnt-JP 4 https://github.com/BryanPlummer/ flickr30k_entities 5 http://shannon.cs.illinois.edu/ DenotationGraph/ 82 and tokenized Japanese sentences using MeCab6 with the IPA dictionary. The evaluation metric used"
2020.wat-1.7,P07-2045,0,0.00531759,"rten and Khoshgoftaar, 2019). 5 Experimental Setup 5.1 Data Augmentation Method Data For training and validation, we use the Flickr30k Entities Japanese dataset3 for Japanese sentences, the Flickr30k Entities dataset4 for English sentences, and the Flickr30k dataset5 for images. For test data, we use both Japanese and English sentences provided by WAT 2020, and their associated images are in the Flickr30k dataset. The Japanese training data size is originally 59,566 sentences, but four sentences are missing; thus, we use 59,562 sentences (both Japanese and English) for training. We use Moses (Koehn et al., 2007) scripts to lowercase, normalize, and tokenize English sentences, Textual Data Augmentation Sampling This method samples the hypothesis from the output distribution at each decoding step to generate synthetic parallel data (Edunov et al., 2018). Random noising This method was originally used to generate synthetic ungrammatical sentences in the grammatical error correction task (Xie et al., 2018). They penalized every hypothesis on the beam by adding noise rβ to its hypothesis’ score, where r is drawn from the uniform distribution on the interval [0, 1] during the beam search procedure, and β c"
2020.wat-1.7,P17-2031,0,0.0377484,"Missing"
2020.wmt-1.120,C04-1046,0,0.134588,"ac.jp shimanaka-hiroki@ed.tmu.ac.jp Tomoyuki Kajiwara Osaka University Mamoru Komachi Tokyo Metropolitan University kajiwara@ids.osaka-u.ac.jp komachi@tmu.ac.jp Abstract We introduce the TMUOU1 submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track. 1 Introduction Quality Estimation (QE) is a task of estimating translation quality without reference sentences (Gandrabur and Foster, 2003; Blatz et al., 2004; Specia et al., 2018). Automatic evaluation metrics based on reference sentences, such as BLEU (Papineni et al., 2002), have contributed to improving translation quality on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Confere"
2020.wmt-1.120,W17-4755,0,0.0851111,"anslation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply simi"
2020.wmt-1.120,D18-2029,0,0.0765928,"Missing"
2020.wmt-1.120,2020.acl-main.747,0,0.114035,"Missing"
2020.wmt-1.120,D17-1070,0,0.0257736,"s the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al., 2020), a pre-trained multilingual sentence encoder. 2 WMT20 QE Shared Task"
2020.wmt-1.120,N19-1423,0,0.205736,"quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al., 2020), a pre-trained multilingual sentence encoder. 2 WMT20 QE Shared Task 1 In the WMT20 QE task 1 (Sentence-Level Direct Assessment), participants predict translation quality at the sentence level from pairs of source and MT output sentences. This task provides datasets for seven language pairs and sets up a multilingual track for a language-independent approach. 2.1"
2020.wmt-1.120,W03-0413,0,0.168434,"kamachi.akifumi@ist.osaka-u.ac.jp shimanaka-hiroki@ed.tmu.ac.jp Tomoyuki Kajiwara Osaka University Mamoru Komachi Tokyo Metropolitan University kajiwara@ids.osaka-u.ac.jp komachi@tmu.ac.jp Abstract We introduce the TMUOU1 submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track. 1 Introduction Quality Estimation (QE) is a task of estimating translation quality without reference sentences (Gandrabur and Foster, 2003; Blatz et al., 2004; Specia et al., 2018). Automatic evaluation metrics based on reference sentences, such as BLEU (Papineni et al., 2002), have contributed to improving translation quality on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translat"
2020.wmt-1.120,D19-1632,0,0.0757872,"Missing"
2020.wmt-1.120,P19-3020,0,0.0904359,"Missing"
2020.wmt-1.120,W17-4763,0,0.0600966,"of English-German dataset. Three or more professional translators annotated DA scores in the range of 0-100 points for each pair of source and MT output sentences. These annotations are following the FLORES setup (Guzm´an et al., 2019). The dataset consists of pairs of source and MT output sentences, z-standardized DA scores, and MT model score (log probabilities for words). Table 1 shows examples of the dataset. For each language pair, 7,000 training sets, 1,000 development sets, and 1,000 test sets are provided. 2.2 Baseline and Evaluation The baseline system is a Predictor-Estimator model (Kim et al., 2017) implemented in OpenKiwi3 (Kepler et al., 2019). The predictor is trained on a parallel corpus used to train the MT model, and predicts each target token from source and target contexts. And the estimator predicts the QE score from features produced by the predictor. Participants are evaluated by Pearson’s correlation metric (Pearson), mean absolute error (MAE), and root mean squared error (RMSE). A z-standardized DA score is used as a gold label. 3 TMUOU System Our system is an ensemble model of four regression models based on XLM-RoBERTa (Conneau et al., 2020) with language tags. We first ex"
2020.wmt-1.120,W18-6450,0,0.0117523,"ompetition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al., 2020), a pre-trained multilingual sentence encoder. 2 WMT20 QE Shared Task 1 In the WMT20 QE task 1 (Sentence-Level Direct Assessment), participants predict translation quality at the sentence level from pairs of source and MT output sentences. This task provides datasets for seven language pairs"
2020.wmt-1.120,W19-5302,0,0.0137512,"re machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieve"
2020.wmt-1.120,N19-4009,0,0.09205,"Missing"
2020.wmt-1.120,P02-1040,0,0.109431,"kajiwara@ids.osaka-u.ac.jp komachi@tmu.ac.jp Abstract We introduce the TMUOU1 submission for the WMT20 Quality Estimation Shared Task 1: Sentence-Level Direct Assessment. Our system is an ensemble model of four regression models based on XLM-RoBERTa with language tags. We ranked 4th in Pearson and 2nd in MAE and RMSE on a multilingual track. 1 Introduction Quality Estimation (QE) is a task of estimating translation quality without reference sentences (Gandrabur and Foster, 2003; Blatz et al., 2004; Specia et al., 2018). Automatic evaluation metrics based on reference sentences, such as BLEU (Papineni et al., 2002), have contributed to improving translation quality on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca"
2020.wmt-1.120,D19-1410,0,0.0342386,"sentence to estimate the QE score: &lt;s&gt; source &lt;/s&gt; &lt;s&gt; MT output &lt;/s&gt;. E0+LangTag Model To make it clear to the XLM-RoBERTa which language each sentence is in, we add a special token (LangTag) for language identification, such as &lt;en&gt;, at the beginning of each sentence. We have expanded the tokenizer and vocabulary and added the following eight LangTags: &lt;en&gt; &lt;et&gt; &lt;de&gt; &lt;ne&gt; &lt;ro&gt; &lt;ru&gt; &lt;si&gt; &lt;zh&gt;. An example of input to the model is as follows: &lt;s&gt; &lt;en&gt; source &lt;/s&gt; &lt;s&gt; &lt;de&gt; MT output &lt;/s&gt;. E0+AVG Model Averaged token vector is as fruitful as the &lt;s&gt; vector at the beginning of the first sentence (Reimers and Gurevych, 2019). We concatenate the averaged token vector and the &lt;s&gt; vector to get richer information from sentence pairs. E0+AVG+LangTag Model This model is a combination of the above models. As shown in Figure 1, we add LangTag at the beginning of each sentence and concatenate the &lt;s&gt; vector with the averaged token vector to estimate the QE score. 3.2 Ensemble Model We ensemble four models described above to make prediction stable. A Gradient Boosting Tree (Fried1038 QE Score QE Regression Layer f(・) E0 Encoded Token Average E0 E1 E2 E3 E4 E5 E6 E7 E8 E9 E10 E11 E12 E13 E14 E15 XLM-RoBERTa Position Embedd"
2020.wmt-1.120,W18-6456,1,0.842135,"tion (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encoder (Cer et al., 2018) as encoders, and achieved the highest performance in all to-English language pairs of WMT18 metrics shared task (Ma et al., 2018). Subsequently, Shimanaka et al. (2019) employed BERT (Devlin et al., 2019) as an encoder to further improve the correlation with manual evaluations. In this study, we apply similar approaches to the QE task. However, to support both source and target languages, we employ XLM-RoBERTa2 (Conneau et al.,"
2020.wmt-1.120,2006.amta-papers.25,0,0.0434531,"on benchmark datasets. However, in situations where machine translation (MT) is actually used, these metrics are sometimes unable to assess the translation quality owing to the lack of reference sentences. The development of QE methods that are well correlated with manual evaluations enable users to decide whether to use the translation results as is, post-edit the results, or employ other machine translations. At the Conference on Machine Translation (WMT), there have been conducted several QErelated competitions such as the QE task (Fonseca et al., 2019) for estimating post-edit rate HTER (Snover et al., 2006) and the QE as a Metric task (Ma et al., 2019) for relative evaluations of translation quality. This year, the WMT QE task held a new competition (Specia et al., 2020) on absolute evaluations of translation quality. In task 1, sentences are annotated with direct assessment (DA) scores as in the metrics task (Bojar et al., 2017). 1 We have been working on the metrics task with an approach that uses pre-trained sentence encoders (Shimanaka et al., 2018, 2019). Shimanaka et al. (2018) employed InferSent (Conneau et al., 2017), Quick-Thought (Logeswaran and Lee, 2018), and Universal Sentence Encod"
2020.wmt-1.70,P19-1126,0,0.0649443,"Missing"
2020.wmt-1.70,W17-4746,0,0.117841,"Missing"
2020.wmt-1.70,N19-1422,0,0.223837,"ayed by k tokens. However, their model for simultaneous translation relies only on the source sentence. In this research, we concentrate on the wait-k approach with fixed policy, so that the amount of input textual context can be controlled to analyze better whether multimodality is effective in SNMT. Multimodal NMT (MNMT) for full-sentence machine translation has been developed to enrich text modality by using visual information (Hitschler et al., 2016; Specia et al., 2016; Elliott and K´ad´ar, 2017). While the improvement brought by visual features is moderate, their usefulness is proven by Caglayan et al. (2019). They showed that MNMT models are able to capture visual clues under limited textual context, where source sentences are synthetically degraded by color deprivation, entity masking, and progressive masking. However, they use an artificial setting where they deliberately deprive the models of source-side textual context by masking. However, our research has discovered an actual end-task and has shown the effectiveness of using multimodal data for it. Compared with the entity masking experiments (Caglayan et al., 2019), where they use a model exposed to only k words, our model starts by waiting"
2020.wmt-1.70,P17-1175,0,0.0642675,"Missing"
2020.wmt-1.70,N18-2079,0,0.142382,"not appeared yet. For example, it is important to distinguish nouns in SVO-SOV translation and verbs in SOV-SVO translation (Ma et al., 2019). SNMT can be realized with two types of policy: fixed and adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to wait for another source word or emit a target word in one model. Previous models with adaptive policies include explicit prediction of the sentencefinal verb (Grissom II et al., 2014; Matsubara et al., 2000) and unseen syntactic constituents (Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a,c, 2020) have the advantage of exploiting input text information as effectively as possible due to the lack of such information in the first place. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with fixed policy, which generates the target sentence only from the source sentence that is delayed by k tokens. However, their model for simultaneous translation relies only on the source sentence. In this research, we concentrate on the wait-k approach with fixed policy, so that the amount of input textual context can be controlled to anal"
2020.wmt-1.70,D18-1329,0,0.0282053,"tual information is limited. Note that the difference of BLEU scores between MSNMT and SNMT becomes larger as the k gets smaller, especially when the target language is distant from English in terms of word order (e.g. Cs and Ja). On the other hand, the availability of more tokens during the decoding process (k ≥ 5) leads to the text information becoming sufficient in some cases. 8 Due to space constraints, we show results only for test sets. 9 https://github.com/SimulTrans-demo/ STACL Analysis Adversarial Evaluation In order to determine whether MSNMT systems are aware of the visual context (Elliott, 2018), we perform the adversarial evaluation on the test set. We present our system with correct visual data with its source sentence (Congruent) as opposed to random visual data as an input (Incongruent) (Elliott, 2018).Therefore, we reversed the order of 1,000 images of the test set, so there will be no overlapping congruent visual data. Then we reconstruct image features for those images to use as an input. Results of image awareness experiments are shown in Table 2. We can see the large difference in BLEU scores between MSNMT congruent (C columns) and incongruent (I columns) settings when k are"
2020.wmt-1.70,W17-4718,0,0.211319,"iving every new source token, eventually seeing all input text. In MNMT, visual features are incorporated into standard machine translation in many ways. Doubly-attentive models are used to capture the textual and visual context vectors independently and then combine these context vectors in a concatenation manner (Calixto et al., 2017) or hierarchical manner (Libovick´y and Helcl, 2017). Some studies use visual features in a multitask learning scenario (Elliott and K´ad´ar, 2017; Zhou et al., 2018). Also, recent work on MNMT has partly addressed lexical ambiguity by using visual information (Elliott et al., 2017; Lala and Specia, 2018; Gella et al., 2019) showing that using textual context with visual features outperform unimodal models. In our study, visual features are extracted using image processing techniques and then integrated into an SNMT model as additional information, which is supposed to be useful to predict missing words in a simultaneous translation scenario. To the best of our knowledge, this is the first work that incorporates external knowledge into an SNMT model. 3 Multimodal Simultaneous Neural Machine Translation Architecture Our main goal is to investigate if image information wo"
2020.wmt-1.70,W16-3210,0,0.269082,"Missing"
2020.wmt-1.70,I17-1014,0,0.258575,"Missing"
2020.wmt-1.70,N19-1200,0,0.0190177,"ng all input text. In MNMT, visual features are incorporated into standard machine translation in many ways. Doubly-attentive models are used to capture the textual and visual context vectors independently and then combine these context vectors in a concatenation manner (Calixto et al., 2017) or hierarchical manner (Libovick´y and Helcl, 2017). Some studies use visual features in a multitask learning scenario (Elliott and K´ad´ar, 2017; Zhou et al., 2018). Also, recent work on MNMT has partly addressed lexical ambiguity by using visual information (Elliott et al., 2017; Lala and Specia, 2018; Gella et al., 2019) showing that using textual context with visual features outperform unimodal models. In our study, visual features are extracted using image processing techniques and then integrated into an SNMT model as additional information, which is supposed to be useful to predict missing words in a simultaneous translation scenario. To the best of our knowledge, this is the first work that incorporates external knowledge into an SNMT model. 3 Multimodal Simultaneous Neural Machine Translation Architecture Our main goal is to investigate if image information would bring improvement on SNMT. As a result,"
2020.wmt-1.70,D14-1140,0,0.0873197,"Missing"
2020.wmt-1.70,E17-1099,0,0.084465,"e words that have not appeared yet. For example, it is important to distinguish nouns in SVO-SOV translation and verbs in SOV-SVO translation (Ma et al., 2019). SNMT can be realized with two types of policy: fixed and adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to wait for another source word or emit a target word in one model. Previous models with adaptive policies include explicit prediction of the sentencefinal verb (Grissom II et al., 2014; Matsubara et al., 2000) and unseen syntactic constituents (Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a,c, 2020) have the advantage of exploiting input text information as effectively as possible due to the lack of such information in the first place. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with fixed policy, which generates the target sentence only from the source sentence that is delayed by k tokens. However, their model for simultaneous translation relies only on the source sentence. In this research, we concentrate on the wait-k approach with fixed policy, so that the amount of input textual context can b"
2020.wmt-1.70,N19-3012,1,0.915649,"ge into an SNMT model. 3 Multimodal Simultaneous Neural Machine Translation Architecture Our main goal is to investigate if image information would bring improvement on SNMT. As a result, two tasks could benefit from each other by combining them. In this section, we describe our MSNMT model, which is composed by combining an SNMT framework wait-k (Ma et al., 2019) and a multimodal model (Libovick´y and Helcl, 2017). We base our model on the RNN architecture, which is widely used in MNMT research (Libovick´y and Helcl, 2017; Caglayan et al., 2017a; Elliott and K´ad´ar, 2017; Zhou et al., 2018; Hirasawa et al., 2019). The model takes a sentence and its corresponding image as inputs. The decoder of the MSNMT model outputs the target language sentence in a simultaneous and multimodal manner by attaching 595 3.2 attention not only to the source sentence but also to the image related to the source sentence.3 3.1 Simultaneous Translation We first briefly review standard NMT to set up the notations. The encoder of standard NMT model always takes the whole input sequence X = (x1 , ..., xn ) of length n where each xi is a word embedding and produces source hidden states H = (h1 , ..., hn ). The decoder predicts t"
2020.wmt-1.70,P16-1227,0,0.0519207,"t place. Meanwhile, Ma et al. (2019) proposed a simple wait-k method with fixed policy, which generates the target sentence only from the source sentence that is delayed by k tokens. However, their model for simultaneous translation relies only on the source sentence. In this research, we concentrate on the wait-k approach with fixed policy, so that the amount of input textual context can be controlled to analyze better whether multimodality is effective in SNMT. Multimodal NMT (MNMT) for full-sentence machine translation has been developed to enrich text modality by using visual information (Hitschler et al., 2016; Specia et al., 2016; Elliott and K´ad´ar, 2017). While the improvement brought by visual features is moderate, their usefulness is proven by Caglayan et al. (2019). They showed that MNMT models are able to capture visual clues under limited textual context, where source sentences are synthetically degraded by color deprivation, entity masking, and progressive masking. However, they use an artificial setting where they deliberately deprive the models of source-side textual context by masking. However, our research has discovered an actual end-task and has shown the effectiveness of using mult"
2020.wmt-1.70,L18-1602,0,0.0192768,"token, eventually seeing all input text. In MNMT, visual features are incorporated into standard machine translation in many ways. Doubly-attentive models are used to capture the textual and visual context vectors independently and then combine these context vectors in a concatenation manner (Calixto et al., 2017) or hierarchical manner (Libovick´y and Helcl, 2017). Some studies use visual features in a multitask learning scenario (Elliott and K´ad´ar, 2017; Zhou et al., 2018). Also, recent work on MNMT has partly addressed lexical ambiguity by using visual information (Elliott et al., 2017; Lala and Specia, 2018; Gella et al., 2019) showing that using textual context with visual features outperform unimodal models. In our study, visual features are extracted using image processing techniques and then integrated into an SNMT model as additional information, which is supposed to be useful to predict missing words in a simultaneous translation scenario. To the best of our knowledge, this is the first work that incorporates external knowledge into an SNMT model. 3 Multimodal Simultaneous Neural Machine Translation Architecture Our main goal is to investigate if image information would bring improvement o"
2020.wmt-1.70,P17-2031,0,0.178153,"Missing"
2020.wmt-1.70,2020.lrec-1.518,0,0.015806,"Z, y<t ) t=1 (12) 4 4.1 Experimental Setup Dataset We experiment with our model in four translation directions consisting of 5 languages: English (En), German (De), French (Fr), Czech (Cs), and Japanese (Ja). All language pairs include En on the source side. We used the train, development, and test sets from the Multi30k (Elliott et al., 2016) dataset published in the WMT16 Shared Task, which is a benchmark dataset generally used in MNMT research (Libovick´y and Helcl, 2017; Caglayan et al., 2019; Elliott and K´ad´ar, 2017; Zhou et al., 2018; Hirasawa et al., 2019) for En→De, En→Fr and En→Cs. Nakayama et al. (2020) released F30kEnt-JP dataset4 which contains Japanese translations of first two original English captions for each image of the Flickr30k Entities dataset (Plummer et al., 2017). They follow the same annotation rules as the Flickr30k Entities dataset using exactly the same tags with entity types and IDs. We preprocessed this data as follows: 1) The parallel En→Ja data was created by taking alignment using corresponding IDs assigned to each Japanese translation entity with the IDs of Flickr30k entities.5 2) The created parallel data was aligned with its corresponding images using text files nam"
2020.wmt-1.70,P15-1020,0,0.411361,"y used in international summits and conferences where real-time comprehension is one of the essential aspects. Simultaneous translation is already a difficult task for human interpreters because the message must be understood and translated while the input sentence is still incomplete, especially for language pairs with different word orders (e.g. SVO-SOV) (Seeber, 2015). Consequently, simultaneous translation is more challenging for machines. Previous works attempt to solve this task by predicting the sentence-final verb (Grissom II et al., 2014), or predicting unseen syntactic constituents (Oda et al., 2015). Given the difficulty ∗ of predicting future inputs based on existing limited inputs, Ma et al. (2019) proposed a simple simultaneous neural machine translation (SNMT) approach wait-k which generates the target sentence concurrently with the source sentence, but always k tokens behind, satisfying low latency requirements. However, previous approaches solve the given task by solely using the text modality, which may be insufficient to produce a reliable translation. Simultaneous interpreters often consider various additional information sources such as visual clues or acoustic data while trans"
2020.wmt-1.70,P16-1162,0,0.0403673,"ote that the English side of En→Ja parallel data extracted from F30kEnt-JP and English side of Multi30k data are thought to be somewhat comparable but not strictly the same while their corresponding images are the same. Data split for all language pairs were as follows: training set, 29,000 sentence pairs, development set, 1,014 sentence pairs, and 1,000 sentence pairs for the test set. This dataset’s average sentence length is 12-13 tokens for En, De, Fr, Cs and 20 tokens for Ja. We limit the vocabulary size of the source and the target languages after concatenating them to 10,000 sub-words (Sennrich et al., 2016). All sentences are preprocessed with lower-casing, tokenizing, and normalizing the punctuation using the Moses script6 . To tokenize Japanese sentences, we used MeCab7 with the IPA dictionary. Visual features are extracted using pre-trained ResNet (He et al., 2016). Technically, we encode all images in Multi30k with ResNet-50 and pick out the hidden state in the pool5 layer as a 2,048dimension visual feature. 4.2 Systems We compare the following models: 1. SNMT: We use only text modality for training data as a baseline for each wait-k model. 2. MSNMT: We use image modality along with text mod"
2020.wmt-1.70,W16-2346,0,0.442511,"Missing"
2020.wmt-1.70,D19-1137,0,0.684534,"rough analysis, we conclude that the proposed method is able to predict tokens that have not appeared yet for source-target language pairs with different word order (e.g. English→Japanese). By providing an adversarial evaluation, we showed that the models indeed utilize visual information. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. For example, it is important to distinguish nouns in SVO-SOV translation and verbs in SOV-SVO translation (Ma et al., 2019). SNMT can be realized with two types of policy: fixed and adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to wait for another source word or emit a target word in one model. Previous models with adaptive policies include explicit prediction of the sentencefinal verb (Grissom II et al., 2014; Matsubara et al., 2000) and unseen syntactic constituents (Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a,c, 2020) have the advantage of exploiting input text information as effectively as possible due to the lack of such information in the first place. Meanwhile, Ma et al. (20"
2020.wmt-1.70,P19-1582,0,0.17758,"rough analysis, we conclude that the proposed method is able to predict tokens that have not appeared yet for source-target language pairs with different word order (e.g. English→Japanese). By providing an adversarial evaluation, we showed that the models indeed utilize visual information. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. For example, it is important to distinguish nouns in SVO-SOV translation and verbs in SOV-SVO translation (Ma et al., 2019). SNMT can be realized with two types of policy: fixed and adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to wait for another source word or emit a target word in one model. Previous models with adaptive policies include explicit prediction of the sentencefinal verb (Grissom II et al., 2014; Matsubara et al., 2000) and unseen syntactic constituents (Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a,c, 2020) have the advantage of exploiting input text information as effectively as possible due to the lack of such information in the first place. Meanwhile, Ma et al. (20"
2020.wmt-1.70,D19-1144,0,0.275037,"rough analysis, we conclude that the proposed method is able to predict tokens that have not appeared yet for source-target language pairs with different word order (e.g. English→Japanese). By providing an adversarial evaluation, we showed that the models indeed utilize visual information. 2 Related Work For simultaneous translation, it is crucial to predict the words that have not appeared yet. For example, it is important to distinguish nouns in SVO-SOV translation and verbs in SOV-SVO translation (Ma et al., 2019). SNMT can be realized with two types of policy: fixed and adaptive policies (Zheng et al., 2019b). Adaptive policy decides whether to wait for another source word or emit a target word in one model. Previous models with adaptive policies include explicit prediction of the sentencefinal verb (Grissom II et al., 2014; Matsubara et al., 2000) and unseen syntactic constituents (Oda et al., 2015). Most dynamic models with adaptive policies (Gu et al., 2017; Dalvi et al., 2018; Arivazhagan et al., 2019; Zheng et al., 2019a,c, 2020) have the advantage of exploiting input text information as effectively as possible due to the lack of such information in the first place. Meanwhile, Ma et al. (20"
2020.wmt-1.70,2020.acl-main.42,0,0.0206517,"Missing"
2020.wmt-1.70,D18-1400,0,0.0636221,"y k words, our model starts by waiting for the first k source words and then generates each target word after receiving every new source token, eventually seeing all input text. In MNMT, visual features are incorporated into standard machine translation in many ways. Doubly-attentive models are used to capture the textual and visual context vectors independently and then combine these context vectors in a concatenation manner (Calixto et al., 2017) or hierarchical manner (Libovick´y and Helcl, 2017). Some studies use visual features in a multitask learning scenario (Elliott and K´ad´ar, 2017; Zhou et al., 2018). Also, recent work on MNMT has partly addressed lexical ambiguity by using visual information (Elliott et al., 2017; Lala and Specia, 2018; Gella et al., 2019) showing that using textual context with visual features outperform unimodal models. In our study, visual features are extracted using image processing techniques and then integrated into an SNMT model as additional information, which is supposed to be useful to predict missing words in a simultaneous translation scenario. To the best of our knowledge, this is the first work that incorporates external knowledge into an SNMT model. 3 Mul"
2020.wmt-1.70,W11-2107,0,\N,Missing
2021.acl-srw.15,P14-2131,0,0.0381038,"al hypothesis (Harris, 1954). The continuous bag-of-words (CBOW) model, which is one of the learning methods of word2vec, obtains word embeddings by maximizing the predicted probability of the target word, wt : T p(wt |Cwt ) ∝ exp(η(wt ) η˜(Cwt )), (4) where Cwt = {wt±i |1 ≤ i ≤ δ} represents the set of nearby context words, δ is thePcontext window width, and η˜(Cwt ) := |Cwt |−1 w∈Cw η(w) t denotes the average vector of all context word vectors. We use the CBOW model to learn word embeddings. In this study, we used a relatively large context window of δ = 10 to learn the topical information (Bansal et al., 2014). In general, it has been shown that the quality of word embeddings improves by centering (Hara et al., 2015; Mu and Viswanath, 2018). Accordingly, acquired distributed representations of the word, η(w1 ), η(w2 ), . . . , η(wV ), are centered and normalized as follows: ( ) X 1 ψ(wv ) = τ S − 2 η(wv ) − V −1 η(wi ) , (5) u ∼ N (0, Id ). f |Ψ ∼ GP(0, ΨT Ψ) = GP(0, Kψ ). η(wi )T η(wi ). (6) i (9) f follows the same Gaussian process as expressed in Eq. (7). Therefore, in the proposed method, we define the Gaussian process representing the meaning of the document using the document vector, u, which"
2021.acl-srw.15,P15-1077,0,0.132443,"inates) are free parameters. Therefore, the estimation of the model is time-consuming because of the large number of parameters. In addition, the only information used for the estimation of the word embeddings is the frequency of words, which makes it difficult to capture the semantics of words. In this study, we propose a new method in which the latent coordinates of words, which are one of the free parameters of the CSTM, are learned in advance using word2vec (Mikolov et al., 2013), and the learned distributed representation of the words are introduced into the CSTM. As in the Gaussian LDA (Das et al., 2015), when we use the word embeddings that capture the semantics of words and provide them as prior information to the model, we can expect improved performance and faster convergence. In the experiments, we use English and Japanese corpora to compare the proposed method with the baseline CSTM in terms of perplexity and convergence speed. We also perform a document classification task to evaluate the quality of the document representations that are 138 Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Co"
2021.acl-srw.15,N19-1423,0,0.0590487,"Missing"
2021.acl-srw.15,2020.tacl-1.29,0,0.0319985,"ed by the proposed model are useful for document classification. 2 2.1 Related Work Word Embeddings and Topic Models There are several studies that aimed to improve the performance of topic models by using a distributed representation of words. Das et al. (2015) proposed the Gaussian LDA (G-LDA), which uses a multivariate Gaussian distribution in the same space of word embeddings to estimate topics in the embedding space. Compared with the LDA, it has high coherence (Chang et al., 2009) because it introduces prior knowledge of semantics of words by using pre-trained word embeddings. Recently, Dieng et al. (2020) proposed the embedded topic model (ETM). The ETM models each word with a categorical distribution whose natural parameter is the inner product between the embedding of word and an embedding of its assigned topic. It outperformed traditional topic models including the LDA. However, both topic models use latent topics to model the documents. The G-LDA defines latent topics as multivariate Gaussian distribution, and the ETM uses topic embeddings for formulating the word probability. Therefore, those topic models hardly control word probability directly depending on a document. In Section 2.2, we"
2021.acl-srw.15,N18-1202,0,0.141577,"Missing"
2021.findings-acl.194,Q17-1010,0,0.0163319,"Missing"
2021.findings-acl.194,A00-2018,0,0.126656,"omplexity, the locality makes them less accurate and necessitates additional grammars or lookahead features for improvement (Kuhlmann et al., 2011; Zhu et al., 2013; Liu and Zhang, 2017c). By contrast, chart-based parsers are conceptually simple and accurate when used with a CYK-style algorithm (Kitaev and Klein, 2018; Zhou and Zhao, 2019) for finding the global optima. However, their complexity is O(n3 ). To achieve both accuracy and simplicity (without high complexity) is a critical problem in parsing. Recent efforts were made using neural models. In contrast to earlier symbolic approaches (Charniak, 2000; Klein and Manning, 2003), neural models are simplified by utilizing their adaptive distributed representation, thereby eliminating complicated symbolic engineering. The seq2seq model for parsing (Vinyals et al., 2015) leverages such representation to interpret the structural task as a general sequential task. With augmented data and ensemble, it outperforms the symbolic models mentioned in Petrov et al. (2006) and provides a complexity of O(n2 ) with the attention mechanism (Bahdanau et al., 2015). However, its performance is inferior to those of specialized neural parsers (Liu and Zhang, 20"
2021.findings-acl.194,P05-1022,0,0.0192591,"s as a training strategy 1 Our code, visualization tool, and pre-trained models are available at https://github.com/tmu-nlp/nccp Previous Work Transition-based parsers. A transducer takes sequential lexical inputs and produces sequential tree-constructing actions in O(n) time. Although it can perfectly parse formal languages, complex semantics and long dependencies make it difficult to parse natural languages. Informative features (Liu and Zhang, 2017c; Kitaev and Klein, 2020; Yang and Deng, 2020), or training and decoding strategies such as dynamic oracles (Cross and Huang, 2016), reranking (Charniak and Johnson, 2005), beam search, and ensemble, can increase the accuracy. However, these make the models complex, and the paradigm fails to naturally parallelize actions. Chart-based parsers. An exhaustive search algorithm checks every possibility in a triangular chart and finds the optimal tree globally. Recent neural chart parsers have achieved state-of-the-art accuracy (Kitaev and Klein, 2018; Zhou and Zhao, 2019; Mrini et al., 2020; Zhang et al., 2020). Despite their high accuracy, they are comparatively inefficient. Only 2n − 1 of O(n2 ) scoring nodes in the chart contain true constituents; many are filler"
2021.findings-acl.194,J93-2004,0,0.0743832,"labels (e.g., SBAR+S), and all trace branches were removed. The CNF with either a left or a 2 Multi-branching trees do not require binarization. The ‘ Sub’ group disappears, but the ‘#POS’ group persists. right factor is commonly used. However, it is heuristically biased, and trees can be binarized using other balanced splits such as always splitting from the center to create a complete binary tree (mid-out) and iteratively performing left and right to create another balanced tree (mid-in). Finally, the orientation is extracted from the paths of these binary trees. We binarized Penn Treebank (Marcus et al., 1993, PTB) for English, Chinese Treebank (Xue et al., 2005, CTB) for Chinese, and Keyaki Treebank3 (Butler et al., 2012, KTB) for Japanese to present the syntactic branching tendencies in Table 2. As English is a right-branching language, its majority orientation is to the right. Even leftfactoring cannot reverse the trend, but it should create a greater balance. Figure 4 shows that it is less effective to stratify PTB with a right factor because it enhances the tendency. The reverse tendency emerges in the KTB corpus as Japanese is a left-branching language. For Chinese, CTB does 3 https://github"
2021.findings-acl.194,2020.findings-emnlp.65,0,0.149533,"ures (Liu and Zhang, 2017c; Kitaev and Klein, 2020; Yang and Deng, 2020), or training and decoding strategies such as dynamic oracles (Cross and Huang, 2016), reranking (Charniak and Johnson, 2005), beam search, and ensemble, can increase the accuracy. However, these make the models complex, and the paradigm fails to naturally parallelize actions. Chart-based parsers. An exhaustive search algorithm checks every possibility in a triangular chart and finds the optimal tree globally. Recent neural chart parsers have achieved state-of-the-art accuracy (Kitaev and Klein, 2018; Zhou and Zhao, 2019; Mrini et al., 2020; Zhang et al., 2020). Despite their high accuracy, they are comparatively inefficient. Only 2n − 1 of O(n2 ) scoring nodes in the chart contain true constituents; many are filler nodes. Chart parsers are often specially engineered for high-speed decoding. (e.g., using Cython) Other parsers. Shen et al. (2018) and Nguyen et al. (2020) proposed local-and-greedy parsers in the top-down splitting style. Their models facilitate divide-and-conquer algorithms that construct the tree based on the magnitude of the splitting scores. A similar way of leveraging concurrent and greedy operations appears i"
2021.findings-acl.194,2020.acl-main.301,0,0.025308,"Missing"
2021.findings-acl.194,2020.acl-main.299,0,0.0148151,"scoring nodes in the chart contain true constituents; many are filler nodes. Chart parsers are often specially engineered for high-speed decoding. (e.g., using Cython) Other parsers. Shen et al. (2018) and Nguyen et al. (2020) proposed local-and-greedy parsers in the top-down splitting style. Their models facilitate divide-and-conquer algorithms that construct the tree based on the magnitude of the splitting scores. A similar way of leveraging concurrent and greedy operations appears in an easy-first parser (Goldberg and Elhadad, 2010). Sequential labeling (G´omezRodr´ıguez and Vilares, 2018; Wei et al., 2020) is a new active thread that also enables parallelism and fast decoding. Collobert (2011) designed an iterative chunking process for parsing. His work stratifies trees into levels of IOBES prefixed constituent chunking nodes. Similar to ours, his parser works from the bottom levels to higher levels. However, the complexity is fixed at O(n2 ) without any node combinations. All models introduced in this section do not exploit vector compositionality. 2200 S VP _S NP _S NP CC PRP VBZ NNP VP R #CC NP #VBZ NP CC PRP VBZ NNP L R L The bottom layer Yet I want Coke Yet I want Coke in binary model (Bin"
2021.findings-acl.194,D14-1162,0,0.0815184,"Missing"
2021.findings-acl.194,N18-1202,0,0.00951596,"an easy-first dependency parser (Goldberg and Elhadad, 2010). Input component. In terms of encoder, Tables 4–6 examine the impact of BiLSTMcxt with fastText or XLNet, and the following conclusions can be drawn. 1) The top rows of Table 4 suggest that frozen fastText embeddings contain sub-word information, whereas tuning them disturbs the frozen information because the n-gram model is not part of our model. 2) Table 5 shows that the deeper the contextualization BiLSTMcxt (or XLNet), the better the results. 3) Tables 5 and 6 indicate that the tuning process for the pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019) achieves a significant improvement. Speed and size. One of our research goals was to achieve simplicity and efficiency. In terms of speed, our models parallelize more actions than transition-based parsers and have fewer computing nodes than chart parsers. In terms of size, our models contain approximately 4M parameters in addition to the 13M fastText (or 114M XLNet) pre-trained embeddings, which is fewer than those 725 795 465 ● ● ● ● ● ● ● binary ● 0−9 XLNet 10−19 162 ● ● 35 9 ● ● ● ● multi−branching fastText+BiLSTM 20−29 30−39 40−49 50−59 60−69 Sente"
2021.findings-acl.194,P06-1055,0,0.0539026,"To achieve both accuracy and simplicity (without high complexity) is a critical problem in parsing. Recent efforts were made using neural models. In contrast to earlier symbolic approaches (Charniak, 2000; Klein and Manning, 2003), neural models are simplified by utilizing their adaptive distributed representation, thereby eliminating complicated symbolic engineering. The seq2seq model for parsing (Vinyals et al., 2015) leverages such representation to interpret the structural task as a general sequential task. With augmented data and ensemble, it outperforms the symbolic models mentioned in Petrov et al. (2006) and provides a complexity of O(n2 ) with the attention mechanism (Bahdanau et al., 2015). However, its performance is inferior to those of specialized neural parsers (Liu and Zhang, 2017a,b,c). Socher et al. (2013) proposed a parsing strategy for a symbolic constituent parser augmented with neural vector compositionality. It did not outperform the two paradigms in neural style probably because the neural techniques, such as contextualization, are not fully exploited. Kitaev and Klein (2020) showed that a simple transitionbased model with a dynamic distributed representation, BERT (Devlin et a"
2021.findings-acl.194,D18-1036,0,0.0283033,"Missing"
2021.findings-acl.194,P18-1108,0,0.176091,"ails to naturally parallelize actions. Chart-based parsers. An exhaustive search algorithm checks every possibility in a triangular chart and finds the optimal tree globally. Recent neural chart parsers have achieved state-of-the-art accuracy (Kitaev and Klein, 2018; Zhou and Zhao, 2019; Mrini et al., 2020; Zhang et al., 2020). Despite their high accuracy, they are comparatively inefficient. Only 2n − 1 of O(n2 ) scoring nodes in the chart contain true constituents; many are filler nodes. Chart parsers are often specially engineered for high-speed decoding. (e.g., using Cython) Other parsers. Shen et al. (2018) and Nguyen et al. (2020) proposed local-and-greedy parsers in the top-down splitting style. Their models facilitate divide-and-conquer algorithms that construct the tree based on the magnitude of the splitting scores. A similar way of leveraging concurrent and greedy operations appears in an easy-first parser (Goldberg and Elhadad, 2010). Sequential labeling (G´omezRodr´ıguez and Vilares, 2018; Wei et al., 2020) is a new active thread that also enables parallelism and fast decoding. Collobert (2011) designed an iterative chunking process for parsing. His work stratifies trees into levels of I"
2021.findings-acl.194,P19-1230,0,0.0930051,"ation and stratification. Introduction Transition-based and chart-based methods are two main paradigms for constituency parsing. Transition-based parsers (Dyer et al., 2016; Kitaev and Klein, 2020) build a tree with a sequence of local actions. Despite their O(n) computational complexity, the locality makes them less accurate and necessitates additional grammars or lookahead features for improvement (Kuhlmann et al., 2011; Zhu et al., 2013; Liu and Zhang, 2017c). By contrast, chart-based parsers are conceptually simple and accurate when used with a CYK-style algorithm (Kitaev and Klein, 2018; Zhou and Zhao, 2019) for finding the global optima. However, their complexity is O(n3 ). To achieve both accuracy and simplicity (without high complexity) is a critical problem in parsing. Recent efforts were made using neural models. In contrast to earlier symbolic approaches (Charniak, 2000; Klein and Manning, 2003), neural models are simplified by utilizing their adaptive distributed representation, thereby eliminating complicated symbolic engineering. The seq2seq model for parsing (Vinyals et al., 2015) leverages such representation to interpret the structural task as a general sequential task. With augmented"
2021.findings-acl.194,P13-1045,0,0.0131116,"d Manning, 2003), neural models are simplified by utilizing their adaptive distributed representation, thereby eliminating complicated symbolic engineering. The seq2seq model for parsing (Vinyals et al., 2015) leverages such representation to interpret the structural task as a general sequential task. With augmented data and ensemble, it outperforms the symbolic models mentioned in Petrov et al. (2006) and provides a complexity of O(n2 ) with the attention mechanism (Bahdanau et al., 2015). However, its performance is inferior to those of specialized neural parsers (Liu and Zhang, 2017a,b,c). Socher et al. (2013) proposed a parsing strategy for a symbolic constituent parser augmented with neural vector compositionality. It did not outperform the two paradigms in neural style probably because the neural techniques, such as contextualization, are not fully exploited. Kitaev and Klein (2020) showed that a simple transitionbased model with a dynamic distributed representation, BERT (Devlin et al., 2019), nearly delivers a state-of-the-art performance. We propose a pair of greedy combinatory parsers (i.e., neural combinators) that efficiently utilize vector compositionality with recurrent components to 219"
2021.findings-acl.194,P17-1076,0,0.0524105,"Missing"
2021.findings-acl.194,P13-1043,0,0.0145589,"n flows by dotted red arrows. Binary parsing explores the internal constituents of S. Special labels prefixed with “#” or “ ” are sub category placeholders caused by binarization and stratification. Introduction Transition-based and chart-based methods are two main paradigms for constituency parsing. Transition-based parsers (Dyer et al., 2016; Kitaev and Klein, 2020) build a tree with a sequence of local actions. Despite their O(n) computational complexity, the locality makes them less accurate and necessitates additional grammars or lookahead features for improvement (Kuhlmann et al., 2011; Zhu et al., 2013; Liu and Zhang, 2017c). By contrast, chart-based parsers are conceptually simple and accurate when used with a CYK-style algorithm (Kitaev and Klein, 2018; Zhou and Zhao, 2019) for finding the global optima. However, their complexity is O(n3 ). To achieve both accuracy and simplicity (without high complexity) is a critical problem in parsing. Recent efforts were made using neural models. In contrast to earlier symbolic approaches (Charniak, 2000; Klein and Manning, 2003), neural models are simplified by utilizing their adaptive distributed representation, thereby eliminating complicated symbo"
2021.findings-acl.194,P15-1113,0,0.0364601,"Missing"
2021.naacl-main.197,D14-1179,0,0.0477248,"Missing"
2021.naacl-main.197,D19-5611,0,0.0206095,"er linguistic knowledge from English to the target language together with the target task, we implement a NMT decoder based on the shared encoder. We use a sequenceto-sequence model (Sutskever et al., 2014) with a recurrent neural network decoder, which suits the auto-regressive nature of the machine translation tasks (Cho et al., 2014), and an attention mechanism to avoid compressing the whole source sentence into a fixed-length vector (Bahdanau et al., 2015). We found that fine-tuning the shared encoder achieves good performance on our machine translation datasets (Conneau and Lample, 2019; Clinchant et al., 2019), alleviating the need for freezing its parameters during training in order to avoid catastrophic forgetting (Imamura and Sumita, 2019; Goodfellow et al., 2014). Similar to MLM, we use 100,000 sentences, and a weight of 0.01. Data For this auxiliary task, we use the same data as for NMT- TRANSFER, described in detail above. 3.5 Universal Dependencies (aux-ud) Using syntax in hierarchical multi-task learning has previously shown to be beneficial (Hashimoto et al., 2017; Godwin et al., 2016). We here use full Universal Dependency (UD) parsing, i.e., partof-speech (POS) tagging, lemmatization, mo"
2021.naacl-main.197,2020.acl-main.747,0,0.0993936,"Missing"
2021.naacl-main.197,D17-1206,0,0.0273981,"d that fine-tuning the shared encoder achieves good performance on our machine translation datasets (Conneau and Lample, 2019; Clinchant et al., 2019), alleviating the need for freezing its parameters during training in order to avoid catastrophic forgetting (Imamura and Sumita, 2019; Goodfellow et al., 2014). Similar to MLM, we use 100,000 sentences, and a weight of 0.01. Data For this auxiliary task, we use the same data as for NMT- TRANSFER, described in detail above. 3.5 Universal Dependencies (aux-ud) Using syntax in hierarchical multi-task learning has previously shown to be beneficial (Hashimoto et al., 2017; Godwin et al., 2016). We here use full Universal Dependency (UD) parsing, i.e., partof-speech (POS) tagging, lemmatization, morphological tagging and dependency parsing as joint 2483 mBERT lang2vec en — de-st — de 0.18 da 0.18 nl 0.19 it 0.22 sr 0.23 id 0.24 ar 0.30 zh 0.33 kk 0.37 tr 0.38 ja∗ 0.41 Avg. 97.6 0.0 97.3 0.0 97.5 48.5 50.9 53.0 44.5 47.6 33.0 34.5 34.6 33.3 29.1 73.9 60.8 75.9 71.4 73.7 80.4 63.7 82.2 76.9 73.3 75.0 51.0 78.0 71.9 61.8 67.4 41.3 63.8 58.5 56.8 71.1 54.2 69.5 62.9 61.1 45.8 48.2 48.1 38.7 42.6 72.9 27.9 69.4 70.3 64.9 48.5 0.2 51.3 38.2 45.2 55.7 52.0 58.4 50.2 5"
2021.naacl-main.197,2021.eacl-main.87,0,0.0771479,"Missing"
2021.naacl-main.197,W18-2501,0,0.0134155,"Auxiliary tasks are sorted by dataset availability (MLM  NMT  UD), where the first type can be used with any raw text, the second one needs parallel data – which is readily available for many languages as a byproduct of multilingual data sources – and the last one requires explicit human annotation. For South Tyrolean, a German dialect, no labeled target data of any sort is available; we use the German task data instead. We provide more details of data sources and sizes in Appendix B. 3.1 Baseline All our models are implemented in MaChAmp v0.2 (van der Goot et al., 2021), an AllenNLPbased (Gardner et al., 2018) multi-task learning toolkit. It uses contextual embeddings, and finetunes them during training. In the multi-task setup, the encoding is shared, and each task has its own decoder. For slot prediction, a greedy decoding with a softmax layer is used, for intents it uses a linear classification layer over the [CLS] token (see Figure 2).8 The data for each task is split in batches, and the batches are then shuffled. We use the default hyperparameters of MaChAmp for all experiments which were optimized on a wide variety of tasks (van der Goot et al., 2021).9 The following models are extensions of"
2021.naacl-main.197,N18-3017,0,0.0173969,"esentations. The first stream of research iliary tasks in two settings. Our results showed that focuses on generating training data in the target lan- masked language modeling led to the most stable guage with machine translation and mapping the performance improvements; however, when a lanslot labels through attention or an external word guage is not seen during pre-training, UD parsing aligner. The translation-based approach can be fur- led to an even larger performance increase. On ther improved by filtering the resulting training the intents, generating target language training data data (Gaspers et al., 2018; Do and Gaspers, 2019), using machine translation was outperforming all post-fixing the annotation by humans (Castellucci our proposed models, at a much higher computaet al., 2019), or by using a soft-alignment based on tional cost however. Our analysis further shows that attention, which alleviates error propagation and nmt-transfer struggles with span detection. Given outperforms annotation projection using external training time and availability trade-off, MLM multiword aligners (Xu et al., 2020). tasking is a viable approach for SLU. 2487 Acknowledgements International Workshop on Spoken"
2021.naacl-main.197,2020.acl-main.740,0,0.0555848,"Missing"
2021.naacl-main.197,H90-1021,0,0.730752,"Missing"
2021.naacl-main.197,D19-5603,0,0.018541,"ared encoder. We use a sequenceto-sequence model (Sutskever et al., 2014) with a recurrent neural network decoder, which suits the auto-regressive nature of the machine translation tasks (Cho et al., 2014), and an attention mechanism to avoid compressing the whole source sentence into a fixed-length vector (Bahdanau et al., 2015). We found that fine-tuning the shared encoder achieves good performance on our machine translation datasets (Conneau and Lample, 2019; Clinchant et al., 2019), alleviating the need for freezing its parameters during training in order to avoid catastrophic forgetting (Imamura and Sumita, 2019; Goodfellow et al., 2014). Similar to MLM, we use 100,000 sentences, and a weight of 0.01. Data For this auxiliary task, we use the same data as for NMT- TRANSFER, described in detail above. 3.5 Universal Dependencies (aux-ud) Using syntax in hierarchical multi-task learning has previously shown to be beneficial (Hashimoto et al., 2017; Godwin et al., 2016). We here use full Universal Dependency (UD) parsing, i.e., partof-speech (POS) tagging, lemmatization, morphological tagging and dependency parsing as joint 2483 mBERT lang2vec en — de-st — de 0.18 da 0.18 nl 0.19 it 0.22 sr 0.23 id 0.24 a"
2021.naacl-main.197,2020.emnlp-main.40,0,0.0240877,"ge, we manually picked a matching UD treebank from version 2.6 (Nivre et al., 2020) (details in the Appendix). Whenever available, we picked an in-language treebank, otherwise we choose a related language. We used size, annotation quality, and domain as criteria. 4 4.1 Results Experimental Setup as is standard for these tasks.11 All reported results (including analysis and test data) are the average over 5 runs with different random seeds. To choose the final model, we use the scores on the English development data. We are aware that this was recently shown to be sub-optimal in some settings (Keung et al., 2020), however there is no clear solution on how to circumvent this in a pure zero-shot cross-lingual setup (i.e. without assuming any target language target task annotation data). We use multilingual BERT (mBERT) as contextual encoder for our experiments. We are also interested in low-resource setups. As all of our languages are included in pre-training of mBERT (except the de-st dialect), we also study XLM15 (XLM MLM - TLM - XNLI 15-1024), which in pre-training covers only 5 of the 13 X SID languages, to simulate further a real low-resource setup. Table 3 reports the scores on 13 X SID languages,"
2021.naacl-main.197,L16-1147,0,0.0292956,"target language outputs; we map the label of each token to the highest scoring alignment target token. We convert the output to valid BIO tags: we use the label of the B for the whole span, and an I following an O is converted to a B. Data To ensure that our machine translation data is suitable for the target domain, we choose to use a combination of transcribed spoken parallel data. For languages included in the IWSLT 2016 Ted talks dataset (Cettolo et al., 2016), we use the train and development data included, and enlarge the training data with the training split from Opensubtitles10 2018 (Lison and Tiedemann, 2016), and Tatoeba (Tiedemann, 2012). For languages absent in IWSLT2016, we used the Opensubtitles data for training and Tatoeba as development set. For Kazakh, the Opensubtitles data only contains 2,000 sentences, so we concatenated out-ofdomain data from the WMT2019 data (Barrault et al., 2019), consisting of English-Kazakh crawled corpora. We adapt the BertBasic tokenizer (which splits punctuation, it does not perform subword tokenization) to match the Facebook and Snips dataset tokenization and use this to pre-tokenize the data. 10 http://www.opensubtitles.org/ 3.3 Masked Language Modeling (aux"
2021.naacl-main.197,E17-2002,0,0.0605313,"Missing"
2021.naacl-main.197,N19-1380,0,0.142156,"(Chen et al., 2019; Qin et al., 2020). Despite advances in neural modeling for slot and intent detection (§ 6), datasets for SLU remain limited, hampering progress toward providing SLU for many language varieties. Most availAdd reminder to swim at 11am tomorrow intent: add reminder Figure 1: English example from X SID annotated with intents (add reminder) and slots ( todo , datetime ). The full set of languages is shown in Table 2. able datasets either support only a specific domain (like air traffic systems) (Xu et al., 2020), or are broader but limited to English and a few other languages (Schuster et al., 2019; Coucke et al., 2018). We release X SID, a new benchmark intended for SLU evaluation in low-resource scenarios. X SID contains evaluation data for 13 languages from six language families, including a very low-resource dialect. It homogenizes annotation styles of two recent datasets (Schuster et al., 2019; Coucke et al., 2018) and provides the broadest public multilingual evaluation data for modern digital assistants. Most previous efforts to multilingual SLU typically focus on translation or multilingual embeddings transfer. In this work, we propose an orthogonal approach, and study non-Engli"
2021.naacl-main.197,P16-1162,0,0.00928267,"anslation with Attention (nmt-transfer) For comparison, we trained a NMT model to translate the NLU training data into the target language, and map the annotations using attention. As opposed to most previous work using this method (Xu et al., 2020; He et al., 2013; Schuster et al., 2019), we opt for an open-source implementation and provide the scripts to rerun the experiments. More specifically, we use the Fairseq toolkit (Ott et al., 2019) implementation of the Transformer-based model (Vaswani et al., 2017) with default hyperparameters. Sentences were encoded using bytepair encoding (BPE) (Sennrich et al., 2016), with a shared vocabulary of 32,000 tokens. At inference time, we set the beam size to 4, and extracted alignment scores to target tokens calculated from the attention weights matrix. These scores are used to align annotation labels to target language outputs; we map the label of each token to the highest scoring alignment target token. We convert the output to valid BIO tags: we use the label of the B for the whole span, and an I following an O is converted to a B. Data To ensure that our machine translation data is suitable for the target domain, we choose to use a combination of transcribe"
2021.naacl-main.197,E12-2021,0,0.0163083,"Missing"
2021.naacl-main.197,2021.eacl-demos.22,1,0.807471,"Missing"
2021.naacl-srw.16,P17-1074,0,0.0232318,"odel when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. Table 1: Dataset used in the experiments. BT models with different architectures. Some studies have used a combination of pseudo data generated by different methods for training the GEC model (Lichtarge et al., 2019; Zhou et al., 2020a,b; Wan et al., 2020). For example, Zhou et al. (2020a) proposed a pseudo data generation method that pairs"
2021.naacl-srw.16,W19-5206,0,0.132362,"this study are as follows: • We confirmed that correction tendencies of the GEC model are different for each BT model. • We found that the combination of different BT models improves or interpolates the F0.5 scores compared with that of single BT models with different seeds. using a combination of pseudo data and genuine parallel data. This is because the amount of pseudo data is much larger than that of genuine parallel data. This usage of pseudo data in GEC contrasts with the usage of a combination of pseudo data and genuine parallel data in MT (Sennrich et al., 2016a; Edunov et al., 2018; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019; Zhou et al., 2020a; Hotate et al., 2020), the"
2021.naacl-srw.16,W19-4423,0,0.0586664,"of parallel data for studies using BT have employed the same artraining (Koehn and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the G"
2021.naacl-srw.16,N12-1067,0,0.0197931,"un errors. Simiformance and (2) using pseudo data for pre-training larly, we reported correction tendencies of the GEC improves the GEC model more effectively than model when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. Table 1: Dataset used in the experiments. BT models with different architectures. Some studies have used a combination of pseudo data generated by different methods for training the"
2021.naacl-srw.16,W13-1703,0,0.0355991,"ated by BT to pre-train the GEC model. However, they did not report the correction tendencies of the GEC model when using combined pseudo data. Conversely, we reported correction tendencies when using a combination of pseudo data generated by different BT models. 3 3.1 Experimental Setup Dataset Table 1 shows the details of the dataset used in the experiments. We used the BEA-2019 workshop official shared task dataset (Bryant et al., 2019) as the training and validation data. This dataset consists of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011; Tajiri et al., 2012), NUCLE (Dahlmeier et al., 2013), and W&I+LOCNESS (Granger, 1998; Yannakoudakis et al., 2018). Following Chollampatt and Ng (2018), we removed sentence pairs with identical source and target sentences from the training data. Next, we applied byte pair encoding (Sennrich et al., 2016b) to both source and target sentences. Here, we acquired subwords from the target sentences in the training data and set the vocabulary size to 8,000. Hereinafter, we refer to the training and validation data as BEA-train and BEA-valid, respectively. We used Wikipedia1 as a seed corpus to generate pseudo data and removed possibly inappropriate se"
2021.naacl-srw.16,2020.emnlp-main.475,0,0.0482871,"Missing"
2021.naacl-srw.16,D18-1045,0,0.0155411,"main contributions of this study are as follows: • We confirmed that correction tendencies of the GEC model are different for each BT model. • We found that the combination of different BT models improves or interpolates the F0.5 scores compared with that of single BT models with different seeds. using a combination of pseudo data and genuine parallel data. This is because the amount of pseudo data is much larger than that of genuine parallel data. This usage of pseudo data in GEC contrasts with the usage of a combination of pseudo data and genuine parallel data in MT (Sennrich et al., 2016a; Edunov et al., 2018; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019; Zhou et al., 2020a; Ho"
2021.naacl-srw.16,2020.acl-main.253,0,0.0263329,"et al., 2019) generates pseudo data using a Correction confusion set based on a spell checker. The second Sennrich et al. (2016a) showed that BT can effec- method (Choe et al., 2019) generates pseudo data tively improve neural machine translation. There- using human edits extracted from annotated GEC fore, many MT studies focused on BT (Poncelas corpora or replacing prepositions/nouns/verbs with et al., 2018; Fadaee and Monz, 2018; Edunov et al., predefined rules. Based on the comparison results 2018; Graça et al., 2019; Caswell et al., 2019; of these methods, it was reported that the former Edunov et al., 2020; Soto et al., 2020; Dou et al., has better performance in correcting spelling er2020). Subsequently, BT was applied to GEC. For rors, whereas the latter has better performance in example, Xie et al. (2018) proposed noising beam correcting noun number and tense errors. In adsearch methods, and Ge et al. (2018a) proposed dition, Lichtarge et al. (2019) compared pseudo back-boost learning. Moreover, Rei et al. (2017) data extracted from Wikipedia edit histories with and Kasewa et al. (2018) applied BT to a grammat- that generated by round-trip translation. They reical error detection task. porte"
2021.naacl-srw.16,D18-1040,0,0.0230039,"2020) conducted a comparative study of two rule/probability-based pseudo 2 Related Works data generation methods. The first method (Grund2.1 Back-Translation in Grammatical Error kiewicz et al., 2019) generates pseudo data using a Correction confusion set based on a spell checker. The second Sennrich et al. (2016a) showed that BT can effec- method (Choe et al., 2019) generates pseudo data tively improve neural machine translation. There- using human edits extracted from annotated GEC fore, many MT studies focused on BT (Poncelas corpora or replacing prepositions/nouns/verbs with et al., 2018; Fadaee and Monz, 2018; Edunov et al., predefined rules. Based on the comparison results 2018; Graça et al., 2019; Caswell et al., 2019; of these methods, it was reported that the former Edunov et al., 2020; Soto et al., 2020; Dou et al., has better performance in correcting spelling er2020). Subsequently, BT was applied to GEC. For rors, whereas the latter has better performance in example, Xie et al. (2018) proposed noising beam correcting noun number and tense errors. In adsearch methods, and Ge et al. (2018a) proposed dition, Lichtarge et al. (2019) compared pseudo back-boost learning. Moreover, Rei et al. (201"
2021.naacl-srw.16,C16-1079,0,0.0190426,"re effectively than model when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. Table 1: Dataset used in the experiments. BT models with different architectures. Some studies have used a combination of pseudo data generated by different methods for training the GEC model (Lichtarge et al., 2019; Zhou et al., 2020a,b; Wan et al., 2020). For example, Zhou et al. (2020a) proposed a pseudo data generat"
2021.naacl-srw.16,P18-1097,0,0.154847,"do data Despite their success, EncDec-based models reis back-translation (BT). Most previous GEC quire considerable amounts of parallel data for studies using BT have employed the same artraining (Koehn and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT mode"
2021.naacl-srw.16,W19-5205,0,0.0206527,"generation methods. The first method (Grund2.1 Back-Translation in Grammatical Error kiewicz et al., 2019) generates pseudo data using a Correction confusion set based on a spell checker. The second Sennrich et al. (2016a) showed that BT can effec- method (Choe et al., 2019) generates pseudo data tively improve neural machine translation. There- using human edits extracted from annotated GEC fore, many MT studies focused on BT (Poncelas corpora or replacing prepositions/nouns/verbs with et al., 2018; Fadaee and Monz, 2018; Edunov et al., predefined rules. Based on the comparison results 2018; Graça et al., 2019; Caswell et al., 2019; of these methods, it was reported that the former Edunov et al., 2020; Soto et al., 2020; Dou et al., has better performance in correcting spelling er2020). Subsequently, BT was applied to GEC. For rors, whereas the latter has better performance in example, Xie et al. (2018) proposed noising beam correcting noun number and tense errors. In adsearch methods, and Ge et al. (2018a) proposed dition, Lichtarge et al. (2019) compared pseudo back-boost learning. Moreover, Rei et al. (2017) data extracted from Wikipedia edit histories with and Kasewa et al. (2018) applied BT to"
2021.naacl-srw.16,N18-2046,0,0.0241864,"Missing"
2021.naacl-srw.16,2020.acl-main.391,1,0.833595,"odels as GEC models. After Yuan et al., 2016a). In BT, we train a BT model (i.e., the and Briscoe (2016) applied an encoder–decoder reverse model of the GEC model), which generates (EncDec) model (Sutskever et al., 2014; Bahdanau an ungrammatical sentence from a given grammatiet al., 2015) to GEC, various EncDec-based GEC models have been proposed (Ji et al., 2017; Chol- cal sentence. Subsequently, a grammatical sentence lampatt and Ng, 2018; Junczys-Dowmunt et al., is provided as an input to the BT model, generating a sentence containing pseudo errors. Finally, pairs 2018; Zhao et al., 2019; Kaneko et al., 2020). GEC models have different correction tenden- of erroneous sentences and their input sentences are used as pseudo data to train a GEC model. cies in each architecture. For example, a GEC ∗ Kiyono et al. (2019) reported that a GEC model Current affiliation: Recruit Co., Ltd. † Current affiliation: Tokyo Institute of Technology using BT achieved the best performance among 126 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 126–135 June 6–11, 2021. ©2021 Association for Computational Linguistics other pseudo data generation methods. However, most previous GEC studies using BT hav"
2021.naacl-srw.16,D19-5546,0,0.0116453,"different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the GEC model are different ferent BT models improves or interpolates the when using (1) a pseudo data generation method F0.5 scores of each error type compared with by adding noise to latent repres"
2021.naacl-srw.16,W19-4414,0,0.0167699,"jp Aomi Koyama Abstract model based on CNN (Gehring et al., 2017) tends to correct errors effectively using the local conGrammatical error correction (GEC) suffers text (Chollampatt and Ng, 2018). Furthermore, from a lack of sufficient parallel data. Theresome studies have combined multiple GEC models fore, GEC studies have developed various methods to generate pseudo data, which comto exploit the difference in correction tendencies, prise pairs of grammatical and artificially prothereby improving performance (Grundkiewicz and duced ungrammatical sentences. Currently, a Junczys-Dowmunt, 2018; Kantor et al., 2019). mainstream approach to generate pseudo data Despite their success, EncDec-based models reis back-translation (BT). Most previous GEC quire considerable amounts of parallel data for studies using BT have employed the same artraining (Koehn and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion te"
2021.naacl-srw.16,W19-4427,0,0.0538562,"n and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the GEC model are different ferent BT models improves or interpolates the when using (1) a"
2021.naacl-srw.16,P14-2029,0,0.0282162,"eas the latter enables better performance in (1) the GEC model using BT achieved the best per- correcting preposition and pronoun errors. Simiformance and (2) using pseudo data for pre-training larly, we reported correction tendencies of the GEC improves the GEC model more effectively than model when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. Table 1: Dataset used in the experiments. BT models"
2021.naacl-srw.16,2020.coling-main.193,1,0.771593,"18; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019; Zhou et al., 2020a; Hotate et al., 2020), they used a less common method of utilizing pseudo data for re-training after training with genuine parallel data. Therefore, we used Transformer as the GEC model and investigated correction tendencies when using Transformer, CNN, and LSTM as BT models. Further, we used pseudo data to pre-train the GEC model. 2.2 Correction Tendencies When Using Each Pseudo Data Generation Method White and Rozovskaya (2020) conducted a comparative study of two rule/probability-based pseudo 2 Related Works data generation methods. The first method (Grund2.1 Back-Translation in Grammatical Error kiewicz et al."
2021.naacl-srw.16,W19-4449,0,0.015333,"ows: • We confirmed that correction tendencies of the GEC model are different for each BT model. • We found that the combination of different BT models improves or interpolates the F0.5 scores compared with that of single BT models with different seeds. using a combination of pseudo data and genuine parallel data. This is because the amount of pseudo data is much larger than that of genuine parallel data. This usage of pseudo data in GEC contrasts with the usage of a combination of pseudo data and genuine parallel data in MT (Sennrich et al., 2016a; Edunov et al., 2018; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019; Zhou et al., 2020a; Hotate et al., 2020), they used a less common method"
2021.naacl-srw.16,P17-1070,0,0.0181128,"ndencies allows us to improve the GEC model further. tion from ungrammatical sentences to grammatical One of the most common methods to genersentences, and GEC studies use machine translaate pseudo data is back-translation (BT) (Sennrich tion (MT) models as GEC models. After Yuan et al., 2016a). In BT, we train a BT model (i.e., the and Briscoe (2016) applied an encoder–decoder reverse model of the GEC model), which generates (EncDec) model (Sutskever et al., 2014; Bahdanau an ungrammatical sentence from a given grammatiet al., 2015) to GEC, various EncDec-based GEC models have been proposed (Ji et al., 2017; Chol- cal sentence. Subsequently, a grammatical sentence lampatt and Ng, 2018; Junczys-Dowmunt et al., is provided as an input to the BT model, generating a sentence containing pseudo errors. Finally, pairs 2018; Zhao et al., 2019; Kaneko et al., 2020). GEC models have different correction tenden- of erroneous sentences and their input sentences are used as pseudo data to train a GEC model. cies in each architecture. For example, a GEC ∗ Kiyono et al. (2019) reported that a GEC model Current affiliation: Recruit Co., Ltd. † Current affiliation: Tokyo Institute of Technology using BT achieved"
2021.naacl-srw.16,N18-1055,0,0.0379022,"Missing"
2021.naacl-srw.16,D18-1541,0,0.0168824,"n results 2018; Graça et al., 2019; Caswell et al., 2019; of these methods, it was reported that the former Edunov et al., 2020; Soto et al., 2020; Dou et al., has better performance in correcting spelling er2020). Subsequently, BT was applied to GEC. For rors, whereas the latter has better performance in example, Xie et al. (2018) proposed noising beam correcting noun number and tense errors. In adsearch methods, and Ge et al. (2018a) proposed dition, Lichtarge et al. (2019) compared pseudo back-boost learning. Moreover, Rei et al. (2017) data extracted from Wikipedia edit histories with and Kasewa et al. (2018) applied BT to a grammat- that generated by round-trip translation. They reical error detection task. ported that the former enables better performance Kiyono et al. (2019) compared pseudo data gen- in correcting morphology and orthography errors, eration methods, including BT. They reported that whereas the latter enables better performance in (1) the GEC model using BT achieved the best per- correcting preposition and pronoun errors. Simiformance and (2) using pseudo data for pre-training larly, we reported correction tendencies of the GEC improves the GEC model more effectively than model w"
2021.naacl-srw.16,D19-1119,0,0.262968,"al., 2014; Bahdanau an ungrammatical sentence from a given grammatiet al., 2015) to GEC, various EncDec-based GEC models have been proposed (Ji et al., 2017; Chol- cal sentence. Subsequently, a grammatical sentence lampatt and Ng, 2018; Junczys-Dowmunt et al., is provided as an input to the BT model, generating a sentence containing pseudo errors. Finally, pairs 2018; Zhao et al., 2019; Kaneko et al., 2020). GEC models have different correction tenden- of erroneous sentences and their input sentences are used as pseudo data to train a GEC model. cies in each architecture. For example, a GEC ∗ Kiyono et al. (2019) reported that a GEC model Current affiliation: Recruit Co., Ltd. † Current affiliation: Tokyo Institute of Technology using BT achieved the best performance among 126 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 126–135 June 6–11, 2021. ©2021 Association for Computational Linguistics other pseudo data generation methods. However, most previous GEC studies using BT have used the BT model with the same architecture as the GEC model (Xie et al., 2018; Ge et al., 2018a,b; Zhang et al., 2019; Kiyono et al., 2019, 2020). Thus, it is unclear whether the correction tendencies diffe"
2021.naacl-srw.16,W17-3204,0,0.0182518,"esome studies have combined multiple GEC models fore, GEC studies have developed various methods to generate pseudo data, which comto exploit the difference in correction tendencies, prise pairs of grammatical and artificially prothereby improving performance (Grundkiewicz and duced ungrammatical sentences. Currently, a Junczys-Dowmunt, 2018; Kantor et al., 2019). mainstream approach to generate pseudo data Despite their success, EncDec-based models reis back-translation (BT). Most previous GEC quire considerable amounts of parallel data for studies using BT have employed the same artraining (Koehn and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al.,"
2021.naacl-srw.16,N19-1333,0,0.173857,"genuine parallel data in MT (Sennrich et al., 2016a; Edunov et al., 2018; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019; Zhou et al., 2020a; Hotate et al., 2020), they used a less common method of utilizing pseudo data for re-training after training with genuine parallel data. Therefore, we used Transformer as the GEC model and investigated correction tendencies when using Transformer, CNN, and LSTM as BT models. Further, we used pseudo data to pre-train the GEC model. 2.2 Correction Tendencies When Using Each Pseudo Data Generation Method White and Rozovskaya (2020) conducted a comparative study of two rule/probability-based pseudo 2 Related Works data generation methods. The first"
2021.naacl-srw.16,I11-1017,1,0.737082,"d pseudo data generated by it with pseudo data generated by BT to pre-train the GEC model. However, they did not report the correction tendencies of the GEC model when using combined pseudo data. Conversely, we reported correction tendencies when using a combination of pseudo data generated by different BT models. 3 3.1 Experimental Setup Dataset Table 1 shows the details of the dataset used in the experiments. We used the BEA-2019 workshop official shared task dataset (Bryant et al., 2019) as the training and validation data. This dataset consists of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011; Tajiri et al., 2012), NUCLE (Dahlmeier et al., 2013), and W&I+LOCNESS (Granger, 1998; Yannakoudakis et al., 2018). Following Chollampatt and Ng (2018), we removed sentence pairs with identical source and target sentences from the training data. Next, we applied byte pair encoding (Sennrich et al., 2016b) to both source and target sentences. Here, we acquired subwords from the target sentences in the training data and set the vocabulary size to 8,000. Hereinafter, we refer to the training and validation data as BEA-train and BEA-valid, respectively. We used Wikipedia1 as a seed corpus to gene"
2021.naacl-srw.16,P15-2097,0,0.0217727,"pre-training larly, we reported correction tendencies of the GEC improves the GEC model more effectively than model when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. Table 1: Dataset used in the experiments. BT models with different architectures. Some studies have used a combination of pseudo data generated by different methods for training the GEC model (Lichtarge et al., 2019; Zhou et al., 2"
2021.naacl-srw.16,E17-2037,0,0.0134186,"better performance in (1) the GEC model using BT achieved the best per- correcting preposition and pronoun errors. Simiformance and (2) using pseudo data for pre-training larly, we reported correction tendencies of the GEC improves the GEC model more effectively than model when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. Table 1: Dataset used in the experiments. BT models with different archite"
2021.naacl-srw.16,W14-1701,0,0.0207865,"methods, including BT. They reported that whereas the latter enables better performance in (1) the GEC model using BT achieved the best per- correcting preposition and pronoun errors. Simiformance and (2) using pseudo data for pre-training larly, we reported correction tendencies of the GEC improves the GEC model more effectively than model when using pseudo data generated by three 127 Dataset BEA-train BEA-valid CoNLL-2014 JFLEG BEA-test Wikipedia Refs. Split 3.2 564,684 4,384 1 1 train valid 1,312 747 4,477 2 4 5 test test test 9,000,000 - - We evaluated the CoNLL-2014 test set (CoNLL2014) (Ng et al., 2014), the JFLEG test set (JFLEG) (Heilman et al., 2014; Napoles et al., 2017), and the official test set of the BEA-2019 shared task (BEA-test). We reported M2 (Dahlmeier and Ng, 2012) for the CoNLL-2014 and GLEU (Napoles et al., 2015, 2016) for the JFLEG. We also reported the scores measured by ERRANT (Felice et al., 2016; Bryant et al., 2017) for the BEA-valid and BEA-test. All the reported results, except for the ensemble model, are the average of three distinct trials using three different random seeds2 . In the ensemble model, we reported the ensemble results of the three GEC models. Sents. T"
2021.naacl-srw.16,N19-4009,0,0.0184054,"ntences in the training data and set the vocabulary size to 8,000. Hereinafter, we refer to the training and validation data as BEA-train and BEA-valid, respectively. We used Wikipedia1 as a seed corpus to generate pseudo data and removed possibly inappropriate sentences, such as URLs. In total, we extracted 9M sentences randomly. 1 We used the 2020-07-06 dump file at https://dumps. wikimedia.org/other/cirrussearch/. 3.3 Evaluation Grammatical Error Correction Model Following Kiyono et al. (2019), we adopted Transformer, which is a representative EncDec-based model, using the fairseq toolkit (Ott et al., 2019). We used the “Transformer (base)” settings of Vaswani et al. (2017)3 , which has a 6-layer encoder and decoder with a dimensionality of 512 for both input and output and 2,048 for inner-layers, and 8 self-attention heads. We pre-trained GEC models on each 9M pseudo data generated by each BT model4 and then fine-tuned them on BEA-train. We optimized the model by using Adam (Kingma and Ba, 2015) in pre-training and with Adafactor (Shazeer and Stern, 2018) in fine-tuning. Most of the hyperparameter settings were the same as those described in Kiyono et al. (2019). Additionally, we trained a GEC"
2021.naacl-srw.16,W17-5032,0,0.0228495,"and Monz, 2018; Edunov et al., predefined rules. Based on the comparison results 2018; Graça et al., 2019; Caswell et al., 2019; of these methods, it was reported that the former Edunov et al., 2020; Soto et al., 2020; Dou et al., has better performance in correcting spelling er2020). Subsequently, BT was applied to GEC. For rors, whereas the latter has better performance in example, Xie et al. (2018) proposed noising beam correcting noun number and tense errors. In adsearch methods, and Ge et al. (2018a) proposed dition, Lichtarge et al. (2019) compared pseudo back-boost learning. Moreover, Rei et al. (2017) data extracted from Wikipedia edit histories with and Kasewa et al. (2018) applied BT to a grammat- that generated by round-trip translation. They reical error detection task. ported that the former enables better performance Kiyono et al. (2019) compared pseudo data gen- in correcting morphology and orthography errors, eration methods, including BT. They reported that whereas the latter enables better performance in (1) the GEC model using BT achieved the best per- correcting preposition and pronoun errors. Simiformance and (2) using pseudo data for pre-training larly, we reported correction"
2021.naacl-srw.16,P16-1009,0,0.589817,"th different seeds. The main contributions of this study are as follows: • We confirmed that correction tendencies of the GEC model are different for each BT model. • We found that the combination of different BT models improves or interpolates the F0.5 scores compared with that of single BT models with different seeds. using a combination of pseudo data and genuine parallel data. This is because the amount of pseudo data is much larger than that of genuine parallel data. This usage of pseudo data in GEC contrasts with the usage of a combination of pseudo data and genuine parallel data in MT (Sennrich et al., 2016a; Edunov et al., 2018; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019;"
2021.naacl-srw.16,P16-1162,0,0.581485,"th different seeds. The main contributions of this study are as follows: • We confirmed that correction tendencies of the GEC model are different for each BT model. • We found that the combination of different BT models improves or interpolates the F0.5 scores compared with that of single BT models with different seeds. using a combination of pseudo data and genuine parallel data. This is because the amount of pseudo data is much larger than that of genuine parallel data. This usage of pseudo data in GEC contrasts with the usage of a combination of pseudo data and genuine parallel data in MT (Sennrich et al., 2016a; Edunov et al., 2018; Caswell et al., 2019). Htut and Tetreault (2019) compared four GEC models—Transformer, CNN, PRPN (Shen et al., 2018), and ON-LSTM (Shen et al., 2019)—using pseudo data generated by different BT models. Specifically, they used Transformer and CNN as BT models. It was reported that the Transformer using pseudo data generated by CNN achieved the best F0.5 score. However, the correction tendencies for each BT model were not reported. Moreover, although using pseudo data for pre-training is common in GEC (Zhao et al., 2019; Lichtarge et al., 2019; Grundkiewicz et al., 2019;"
2021.naacl-srw.16,2020.acl-main.359,0,0.0197232,"tes pseudo data using a Correction confusion set based on a spell checker. The second Sennrich et al. (2016a) showed that BT can effec- method (Choe et al., 2019) generates pseudo data tively improve neural machine translation. There- using human edits extracted from annotated GEC fore, many MT studies focused on BT (Poncelas corpora or replacing prepositions/nouns/verbs with et al., 2018; Fadaee and Monz, 2018; Edunov et al., predefined rules. Based on the comparison results 2018; Graça et al., 2019; Caswell et al., 2019; of these methods, it was reported that the former Edunov et al., 2020; Soto et al., 2020; Dou et al., has better performance in correcting spelling er2020). Subsequently, BT was applied to GEC. For rors, whereas the latter has better performance in example, Xie et al. (2018) proposed noising beam correcting noun number and tense errors. In adsearch methods, and Ge et al. (2018a) proposed dition, Lichtarge et al. (2019) compared pseudo back-boost learning. Moreover, Rei et al. (2017) data extracted from Wikipedia edit histories with and Kasewa et al. (2018) applied BT to a grammat- that generated by round-trip translation. They reical error detection task. ported that the former e"
2021.naacl-srw.16,P12-2039,1,0.730372,"by it with pseudo data generated by BT to pre-train the GEC model. However, they did not report the correction tendencies of the GEC model when using combined pseudo data. Conversely, we reported correction tendencies when using a combination of pseudo data generated by different BT models. 3 3.1 Experimental Setup Dataset Table 1 shows the details of the dataset used in the experiments. We used the BEA-2019 workshop official shared task dataset (Bryant et al., 2019) as the training and validation data. This dataset consists of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011; Tajiri et al., 2012), NUCLE (Dahlmeier et al., 2013), and W&I+LOCNESS (Granger, 1998; Yannakoudakis et al., 2018). Following Chollampatt and Ng (2018), we removed sentence pairs with identical source and target sentences from the training data. Next, we applied byte pair encoding (Sennrich et al., 2016b) to both source and target sentences. Here, we acquired subwords from the target sentences in the training data and set the vocabulary size to 8,000. Hereinafter, we refer to the training and validation data as BEA-train and BEA-valid, respectively. We used Wikipedia1 as a seed corpus to generate pseudo data and r"
2021.naacl-srw.16,2020.acl-srw.5,1,0.784934,"chitectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the GEC model are different ferent BT models improves or interpolates the when using (1) a pseudo data generation method F0.5 scores of each error type compared with by adding noise to latent representations and (2) a that of single BT models with different seeds. rule-based pseudo data"
2021.naacl-srw.16,2020.coling-main.200,0,0.0775782,"tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the GEC model are different ferent BT models improves or interpolates the when using (1) a pseudo data generation method F0.5 scores of each error type compared with by adding noise to latent representations and (2) a that of single BT models with different seeds. rule-based pseudo data generation method. Further1 Introduction more, they improved the GEC model by combining Grammatical error correction (GEC) aims to auto- pseudo dat"
2021.naacl-srw.16,2020.emnlp-main.228,0,0.0365176,"Missing"
2021.naacl-srw.16,2020.bea-1.21,0,0.0584126,"Missing"
2021.naacl-srw.16,N18-1057,0,0.340447,"018; Kantor et al., 2019). mainstream approach to generate pseudo data Despite their success, EncDec-based models reis back-translation (BT). Most previous GEC quire considerable amounts of parallel data for studies using BT have employed the same artraining (Koehn and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a;"
2021.naacl-srw.16,P11-1019,0,0.0137434,"chine translation. Then, they combined pseudo data generated by it with pseudo data generated by BT to pre-train the GEC model. However, they did not report the correction tendencies of the GEC model when using combined pseudo data. Conversely, we reported correction tendencies when using a combination of pseudo data generated by different BT models. 3 3.1 Experimental Setup Dataset Table 1 shows the details of the dataset used in the experiments. We used the BEA-2019 workshop official shared task dataset (Bryant et al., 2019) as the training and validation data. This dataset consists of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011; Tajiri et al., 2012), NUCLE (Dahlmeier et al., 2013), and W&I+LOCNESS (Granger, 1998; Yannakoudakis et al., 2018). Following Chollampatt and Ng (2018), we removed sentence pairs with identical source and target sentences from the training data. Next, we applied byte pair encoding (Sennrich et al., 2016b) to both source and target sentences. Here, we acquired subwords from the target sentences in the training data and set the vocabulary size to 8,000. Hereinafter, we refer to the training and validation data as BEA-train and BEA-valid, respectively. We used Wiki"
2021.naacl-srw.16,N16-1042,0,0.0499913,"Missing"
2021.naacl-srw.16,N19-1014,0,0.195219,"eir success, EncDec-based models reis back-translation (BT). Most previous GEC quire considerable amounts of parallel data for studies using BT have employed the same artraining (Koehn and Knowles, 2017). However, chitecture for both GEC and BT models. However, GEC models have different correction GEC suffers from a lack of sufficient parallel data. tendencies depending on their architectures. Accordingly, GEC studies have developed various Thus, in this study, we compare the correcpseudo data generation methods (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et"
2021.naacl-srw.16,2020.findings-emnlp.30,0,0.476433,"s (Xie et al., 2018; tion tendencies of the GEC models trained on Ge et al., 2018a; Zhao et al., 2019; Lichtarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the GEC model are different ferent BT models improves or interpolates the when using (1) a pseudo data generation method F0.5 scores of each error type compared with by adding noise to latent representations and (2) a that of single BT models with different seeds. rule-based pseudo data generation method. Further1 Introduction more, they improved the GEC model by combining Grammatical error correction (GEC) aims"
2021.naacl-srw.16,2020.findings-emnlp.136,0,0.0670254,"htarge et al., pseudo data generated by different BT models, 2019; Xu et al., 2019; Choe et al., 2019; Qiu et al., namely, Transformer, CNN, and LSTM. The 2019; Grundkiewicz et al., 2019; Kiyono et al., results confirm that the correction tendencies 2019; Grundkiewicz and Junczys-Dowmunt, 2019; for each error type are different for every BT Wang et al., 2020; Takahashi et al., 2020; Wang and model. Additionally, we examine the correction tendencies when using a combination of Zheng, 2020; Zhou et al., 2020a; Wan et al., 2020). pseudo data generated by different BT models. Moreover, Wan et al. (2020) showed that the corAs a result, we find that the combination of difrection tendencies of the GEC model are different ferent BT models improves or interpolates the when using (1) a pseudo data generation method F0.5 scores of each error type compared with by adding noise to latent representations and (2) a that of single BT models with different seeds. rule-based pseudo data generation method. Further1 Introduction more, they improved the GEC model by combining Grammatical error correction (GEC) aims to auto- pseudo data generated by these methods. Therefore, the combination of pseudo data gen"
2021.naacl-srw.18,2020.acl-main.194,0,0.0603296,"Missing"
2021.naacl-srw.18,D19-1570,0,0.171231,"he translation quality (Koehn and Knowles, 2017). In other words, the less training data we have, the lower will be the accuracy of the translation. This issue is prevalent in low-resource languages. There∗ † Current affiliation: Recruit Co., Ltd. Current affiliation: Tokyo Institute of Technology fore, various data augmentation methods for lowresource parallel corpora have been studied. For instance, the generation of pseudo data was proposed by back-translating the monolingual corpora or paraphrasing the parallel corpora as additional training data (Wang et al., 2018; Sennrich et al., 2016; Li et al., 2019). Hence, this study proposes a data augmentation method that can be effective in long sentence translations. The proposed method is illustrated in Figure 1. Long sentences were obtained by concatenating two sentences at random and adding them to the original data. The translation quality is expected to be improved by this method because the low quality of translation of long sentences was caused by insufficient number of long sentences in the training data, which reduces this concern in the proposed method. This study presents an improved BLEU score and higher quality in long sentence translat"
2021.naacl-srw.18,D18-1100,0,0.104141,"etween the size of the training data and the translation quality (Koehn and Knowles, 2017). In other words, the less training data we have, the lower will be the accuracy of the translation. This issue is prevalent in low-resource languages. There∗ † Current affiliation: Recruit Co., Ltd. Current affiliation: Tokyo Institute of Technology fore, various data augmentation methods for lowresource parallel corpora have been studied. For instance, the generation of pseudo data was proposed by back-translating the monolingual corpora or paraphrasing the parallel corpora as additional training data (Wang et al., 2018; Sennrich et al., 2016; Li et al., 2019). Hence, this study proposes a data augmentation method that can be effective in long sentence translations. The proposed method is illustrated in Figure 1. Long sentences were obtained by concatenating two sentences at random and adding them to the original data. The translation quality is expected to be improved by this method because the low quality of translation of long sentences was caused by insufficient number of long sentences in the training data, which reduces this concern in the proposed method. This study presents an improved BLEU score and"
2021.naacl-srw.18,W17-5701,0,0.0206229,"two sentences as input during the test, but the BLEU score was worse than the proposed method. vanilla + concat. Original data and augmented data by sentence concatenation. Sentences with length of less than 25 words after concatenation were removed to improve the translation quality of long sentences. vanilla + ST. Original data and augmented data by self-training. vanilla + BT. Original data and augmented data by back-translation. vanilla + BT + concat. The composite data of the original data, the back-translated data, and their sentence concatenation.2 4.2 Setup We used ASPEC3 from WAT17 (Nakazawa et al., 2017) to perform English-to-Japanese translation. This dataset contains 2M sentences as training data, 1,790 as valid data and 1,812 as test data. We also followed the official segmentation using SentencePiece (Kudo and Richardson, 2018) with a vocabulary size of 16,384. A total of 400K sentences were randomly extracted from the original training 2 The results of the experiment showed that the score of “vanilla + BT” was higher than that of “vanilla + ST.” Therefore, in this study, the proposed method was combined only with “vanilla + BT.” 3 http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2017/snmt/ 145 ade"
2021.naacl-srw.18,K19-1031,0,0.0198984,"roceedings of NAACL-HLT 2021: Student Research Workshop, pages 143–149 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: Proposed method: Augmentation of data by combining the back-translation and the concatenation of two sentences. During concatenation, each sentence is randomly sampled, so that they do not have context overlap with each other. overall quality of NMT is better than that of SMT but that SMT outperforms NMT on sentences of 60 words and longer. They stated that this degradation in quality was caused by the short length of the translations. Additionally, Neishi and Yoshinaga (2019) propose to use the relative position information instead of the absolute position information to mitigate the performance drop of NMT models for long sentences. They conducted an analysis of the translation quality and sentence length on lengthcontrolled English–to–Japanese parallel data and showed that the absolute positional information sharply drops the BLEU score of the transformer model (Vaswani et al., 2017) in translating sentences that are longer than those in the training data. and used a backward translation model to generate the source-side. It was observed that these methods can e"
2021.naacl-srw.18,N19-4009,0,0.0341485,"in” denotes the sentence generated by our proposed method, “vanilla + BT + concat,” is superior to that of “vanilla + BT,” and “lose” denotes the opposite of “win.” Figure 3: Effectiveness of the proposed method for each data size by sentence length: Vertical axis represents BLEU score of “vanilla + concat + BT” minus BLEU score of “vanilla + BT.” data and selected as the training data to be used in this experiment. Regarding self-training and back-translation models, we used only the training corpus, following Li et al. (2019). The transformer models from Fairseq were used in the experiment (Ott et al., 2019)4 . Adam was set as the optimizer with a dropout of 0.3, a maximum of 300,000 steps in the training process, and a total batch size of approximately 65,536 tokens per step. The same architecture was also used to train the self-training and the back-translation models. The BLEU score (Papineni et al., 2002) was used for automatic evaluation. We computed the average of the BLEU scores of three runs with different seeds. Human evaluation was also conducted. For three native Japanese evaluators, 100 sentences were randomly selected from the test set per evaluator. They performed pairwise evaluatio"
2021.naacl-srw.18,P02-1040,0,0.109987,"oncat + BT” minus BLEU score of “vanilla + BT.” data and selected as the training data to be used in this experiment. Regarding self-training and back-translation models, we used only the training corpus, following Li et al. (2019). The transformer models from Fairseq were used in the experiment (Ott et al., 2019)4 . Adam was set as the optimizer with a dropout of 0.3, a maximum of 300,000 steps in the training process, and a total batch size of approximately 65,536 tokens per step. The same architecture was also used to train the self-training and the back-translation models. The BLEU score (Papineni et al., 2002) was used for automatic evaluation. We computed the average of the BLEU scores of three runs with different seeds. Human evaluation was also conducted. For three native Japanese evaluators, 100 sentences were randomly selected from the test set per evaluator. They performed pairwise evaluation between “vanilla + BT” and “vanilla + BT + concat” from two perspectives: adequacy and fluency. 4 https://github.com/pytorch/fairseq length sentences vanilla + BT vanilla + BT + concat all 999,998 22.1 22.2 1 – 10 11 – 20 21 – 30 31 – 40 41 – 50 51 – 60 61 – 70 71 – 100 101 – 200 22,725 232,829 329,597 2"
2021.naacl-srw.18,P16-1009,0,0.0208744,"the training data and the translation quality (Koehn and Knowles, 2017). In other words, the less training data we have, the lower will be the accuracy of the translation. This issue is prevalent in low-resource languages. There∗ † Current affiliation: Recruit Co., Ltd. Current affiliation: Tokyo Institute of Technology fore, various data augmentation methods for lowresource parallel corpora have been studied. For instance, the generation of pseudo data was proposed by back-translating the monolingual corpora or paraphrasing the parallel corpora as additional training data (Wang et al., 2018; Sennrich et al., 2016; Li et al., 2019). Hence, this study proposes a data augmentation method that can be effective in long sentence translations. The proposed method is illustrated in Figure 1. Long sentences were obtained by concatenating two sentences at random and adding them to the original data. The translation quality is expected to be improved by this method because the low quality of translation of long sentences was caused by insufficient number of long sentences in the training data, which reduces this concern in the proposed method. This study presents an improved BLEU score and higher quality in long"
2021.wat-1.13,Y17-1038,0,0.0129224,"g pretraining models such as BART in translation task is similar to transfer learning (Zoph et al., 2016). Transfer learning in NMT is a method that trains the network of the parent language pair (the parent model) as the initial network and then fine-tunes it for the child language pair (the child model). In the terminology of transfer learning, the pretrained BART and fine-tuned model are the parent model and child model, respectively. Previous studies have shown that transfer learning works most efficiently when the source languages of the parent and child models are syntactically similar (Dabre et al., 2017; Nguyen and Chiang, 2017). Therefore, we hypothesize that BART is more effective when the language pair for fine-tuning is syntactically similar to the pre-training language. In this study, we examine the effects of Japanese BART on the translation task. We use Korean/Japanese and English/Japanese bilingual data of Japan Patent Office Patent Corpus 2.0 (JPO corpus) for fine-tuning. We also experiment in both translation directions of KoJa and EnJa. 133 Proceedings of the 8th Workshop on Asian Translation, pages 133–137 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Comp"
2021.wat-1.13,N19-1423,0,0.152068,"nese BART can also improve translation accuracy in both KoreanJapanese and EnglishJapanese translations. 1 Introduction Neural Machine Translation (NMT) has achieved high translation accuracy in large-scale data conditions. However, translation accuracy of NMT drops in the lack of bilingual data (Koehn and Knowles, 2017). There are several approaches such as backtranslation (Sennrich et al., 2016) and transfer learning (Zoph et al., 2016) to address this problem. Furthermore, in addition to these methods, there are some approaches to use pre-trained models using only monolingual data. BERT (Devlin et al., 2019), which is the most typical pre-trained model, can boost the accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide significant improvement (Imamura and Sumita, 2019). Recently, several studies proposed pre-trained encoder-decoder models using a monolingual data. Lewis et al. (2020) proposed BART, which is one of the pre-trained encoder-decoder models. They demonstrated that BART works well for not only comp"
2021.wat-1.13,D19-5603,0,0.0744274,"ches such as backtranslation (Sennrich et al., 2016) and transfer learning (Zoph et al., 2016) to address this problem. Furthermore, in addition to these methods, there are some approaches to use pre-trained models using only monolingual data. BERT (Devlin et al., 2019), which is the most typical pre-trained model, can boost the accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide significant improvement (Imamura and Sumita, 2019). Recently, several studies proposed pre-trained encoder-decoder models using a monolingual data. Lewis et al. (2020) proposed BART, which is one of the pre-trained encoder-decoder models. They demonstrated that BART works well for not only comprehension tasks such as GLEU (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) but also text generation tasks such as text summarization and translation. However, they reported only the effect of English BART, so they did not investigate BART trained by monolingual data of another language. Furthermore, in the translation task, they experimented wi"
2021.wat-1.13,W17-3204,0,0.0269447,"2020), was shown to improve translation accuracy via fine-tuning with bilingual data. However, they experimented only Romanian→English translation using English BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both KoreanJapanese and EnglishJapanese translations. 1 Introduction Neural Machine Translation (NMT) has achieved high translation accuracy in large-scale data conditions. However, translation accuracy of NMT drops in the lack of bilingual data (Koehn and Knowles, 2017). There are several approaches such as backtranslation (Sennrich et al., 2016) and transfer learning (Zoph et al., 2016) to address this problem. Furthermore, in addition to these methods, there are some approaches to use pre-trained models using only monolingual data. BERT (Devlin et al., 2019), which is the most typical pre-trained model, can boost the accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide"
2021.wat-1.13,D18-2012,0,0.0255865,"indicated in Table 2 for both finetuning JaBART and training the baseline model. We fine-tune and train the models using the fairseq implementation2 . 3.2 Data To train and fin-tune the models, we use Ko–Ja and En–Ja datasets of JPO corpus. Korean and English have almost no subword overlaps with Japanese, because these languages use Hangul, Latin alphabets, and Hiragana/Katakana/Kanji characters, respectively. For Japanese pre-processing, we use JaBART tokenizer. For Korean and English, we tokenize sentences using MeCab-ko3 and Moses scripts4 , respectively. Then, we apply the SentencePiece (Kudo and Richardson, 2018) with a 32k vocabulary size. Table 1 presents the training, de1 https://github.com/utanaka2000/fairseq/blob/ japanese bart pretrained model 134 2 https://github.com/utanaka2000/fairseq https://bitbucket.org/eunjeon/mecab-ko 4 https://github.com/moses-smt/mosesdecodertree/ RELEASE-4.0 3 Ko→Ja Single Ensemble Single Ensemble Baseline JaBART ∆ Baseline JaBART ∆ Baseline JaBART ∆ Baseline JaBART ∆ Ja→Ko dev test dev test 67.400±.080 / 68.750±.104 / +1.350 / 68.770 / 69.570 / +0.800 / - 67.816±.028 / 68.563±.065 / +0.746 / 68.590 / 69.440 / +0.850 / - dev 71.510±.166 / 0.947±.001 72.760±.140 / 0.94"
2021.wat-1.13,2020.acl-main.703,0,0.169201,"thermore, in addition to these methods, there are some approaches to use pre-trained models using only monolingual data. BERT (Devlin et al., 2019), which is the most typical pre-trained model, can boost the accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide significant improvement (Imamura and Sumita, 2019). Recently, several studies proposed pre-trained encoder-decoder models using a monolingual data. Lewis et al. (2020) proposed BART, which is one of the pre-trained encoder-decoder models. They demonstrated that BART works well for not only comprehension tasks such as GLEU (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) but also text generation tasks such as text summarization and translation. However, they reported only the effect of English BART, so they did not investigate BART trained by monolingual data of another language. Furthermore, in the translation task, they experimented with only Romanian→English translation, which have subword overlap. Therefore, the effect in translations between langu"
2021.wat-1.13,I17-2050,0,0.0155926,"such as BART in translation task is similar to transfer learning (Zoph et al., 2016). Transfer learning in NMT is a method that trains the network of the parent language pair (the parent model) as the initial network and then fine-tunes it for the child language pair (the child model). In the terminology of transfer learning, the pretrained BART and fine-tuned model are the parent model and child model, respectively. Previous studies have shown that transfer learning works most efficiently when the source languages of the parent and child models are syntactically similar (Dabre et al., 2017; Nguyen and Chiang, 2017). Therefore, we hypothesize that BART is more effective when the language pair for fine-tuning is syntactically similar to the pre-training language. In this study, we examine the effects of Japanese BART on the translation task. We use Korean/Japanese and English/Japanese bilingual data of Japan Patent Office Patent Corpus 2.0 (JPO corpus) for fine-tuning. We also experiment in both translation directions of KoJa and EnJa. 133 Proceedings of the 8th Workshop on Asian Translation, pages 133–137 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics Lang"
2021.wat-1.13,D16-1264,0,0.0150565,"accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide significant improvement (Imamura and Sumita, 2019). Recently, several studies proposed pre-trained encoder-decoder models using a monolingual data. Lewis et al. (2020) proposed BART, which is one of the pre-trained encoder-decoder models. They demonstrated that BART works well for not only comprehension tasks such as GLEU (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016) but also text generation tasks such as text summarization and translation. However, they reported only the effect of English BART, so they did not investigate BART trained by monolingual data of another language. Furthermore, in the translation task, they experimented with only Romanian→English translation, which have subword overlap. Therefore, the effect in translations between language pairs without subword overlapping is not clear. Furthermore, they did not experiment in translation direction where the source language matches the language of the pre-trained model. Additionally, we conside"
2021.wat-1.13,P16-1009,0,0.0373624,"data. However, they experimented only Romanian→English translation using English BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both KoreanJapanese and EnglishJapanese translations. 1 Introduction Neural Machine Translation (NMT) has achieved high translation accuracy in large-scale data conditions. However, translation accuracy of NMT drops in the lack of bilingual data (Koehn and Knowles, 2017). There are several approaches such as backtranslation (Sennrich et al., 2016) and transfer learning (Zoph et al., 2016) to address this problem. Furthermore, in addition to these methods, there are some approaches to use pre-trained models using only monolingual data. BERT (Devlin et al., 2019), which is the most typical pre-trained model, can boost the accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide significant improvement (Imamura and Sumita, 2019). Recently, several studies"
2021.wat-1.13,W18-5446,0,0.0367177,"Missing"
2021.wat-1.13,2020.wat-1.16,0,0.0647822,"er-decoder models such as MASS (Song et al., 2019) and BART (Lewis et al., 2020), and these models can improve the translation accuracy via fine-tuning with bilingual data. MASS (Song et al., 2019) uses monolingual data from both the source and target languages for pre-training when applying to the NMT. On the contrary, BART (Lewis et al., 2020) uses only monolingual data of target language, unlike MASS. Liu et al. (2020) trained multilingual BART (mBART) using monolingual data of 25 languages. They indicated that mBART initialization leads significant gains in low resource settings. However, Wang and Htun (2020) showed that mBART cannot obtain improvements in the Patent task. 3 3.1 Experimental Settings Implementation In this study, we use Japanese BART1 base v1.1 (JaBART) trained using Japanese Wikipedia sentences (18M sentences). For fine-tuning, we do not use an additional encoder like in Lewis et al. (2020)’s method. Instead, we add randomly initialized embeddings for each unknown subword in JaBART to both encoder and decoder. We share the embeddings of characters that match across Embedding dimension Attention heads Layers Feed forward dimension Optimizer Adam betas Learning rate Dropout Label s"
2021.wat-1.13,D16-1163,0,0.201103,"→English translation using English BART. In this paper, we examine the effectiveness of Japanese BART using Japan Patent Office Corpus 2.0. Our experiments indicate that Japanese BART can also improve translation accuracy in both KoreanJapanese and EnglishJapanese translations. 1 Introduction Neural Machine Translation (NMT) has achieved high translation accuracy in large-scale data conditions. However, translation accuracy of NMT drops in the lack of bilingual data (Koehn and Knowles, 2017). There are several approaches such as backtranslation (Sennrich et al., 2016) and transfer learning (Zoph et al., 2016) to address this problem. Furthermore, in addition to these methods, there are some approaches to use pre-trained models using only monolingual data. BERT (Devlin et al., 2019), which is the most typical pre-trained model, can boost the accuracy of many downstream tasks compared to models without BERT via fine-tuning with the task-specific training data. However, applying BERT to NMT in fine-tuning form like the other tasks requires two-stage optimization and does not provide significant improvement (Imamura and Sumita, 2019). Recently, several studies proposed pre-trained encoder-decoder mode"
2021.wat-1.20,W18-6402,0,0.0223804,"isual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has achieved state-of-the-art translation performance. However, there remain numerous situations where textual context alone is insufficient for correct translation, such as in the presence of ambiguous words and grammatical gender. Therefore, researchers in this field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the translation performance"
2021.wat-1.20,Q17-1010,0,0.0161645,"ject class instead of only the object class. We take these visual concepts to represent the image regions. We set each image labeled with 36 visual concepts of image regions, which are space-separated phrases. For the words, we lowercase and tokenize the source English sentences via the Moses toolkit.3 The soft alignment is a similarity matrix filled with the cosine similarity between source words and visual concepts. To avoid unknown words, we convert the words and concepts into subword units using the byte pair encoding (BPE) model (Sennrich et al., 2016). Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens). The source subword embeddings can be generated directly, whereas the generation of visual concept embeddings should take an average of the embeddings of all constituent subwords because they are phrases. As shown in Figure 1, source subwords are represented by W = {w1 , w2 , w3 , · · · , wn }, and the visual concepts are represented by C = {c1 , c2 , c3 , · · · , c36 }. These embeddings provide a mapping function from a subword to a 300-d"
2021.wat-1.20,W18-6439,0,0.118929,"e the |R |and |H |represent the length of source words and the numbers of image regions: n and 36; the CONCAT is a concatenation operator. 175 H Atxt h1 Bi-directional RNN man in red shirt watches dog on an agility course . polo (yt-1) GRU (3) GRU (2) hn GRU (1) RoI r3 en zt hn-1 rouge (yt) Aimg Halign agilité ct CONCAT r2 homme Text-attention a3txt h3 R r1 un h2 . Image-attention a3img &lt;eos&gt; r35 r36 Figure 2: The TMEKU system. 2.3 (1) img T eimg ) tanh(U img st t,j = (V Decoder To generate target word yt at time step t, a hidden (1) state proposal st is computed in the first cell of deepGRU (Delbrouck and Dupont, 2018) (GRU (1)) by function fgru1 (yt−1 , st−1 ). The function considers the previously emitted target word yt−1 and generated hidden state st−1 as follows. (1) st = (1 − ξˆt ) s˙ t + ξˆt st−1 ˆ γt = σ(Wγ EY [yt−1 ] + Uγ st−1 ) ξˆt = σ(Wξ EY [yt−1 ] + Uξ st−1 ) where Wξ , Uξ , Wγ , Uγ , W , and U are training parameters, and EY is the target word embedding. 2.3.1 Text-Attention At time step t, the text-attention focuses on every textual annotation atxt in Atxt and assigns an ati tention weight. The textual context vector zt is generated as follows. (1) img αt,j = softmax(eimg t,j ) ct = img img αt,"
2021.wat-1.20,W17-4718,0,0.0171891,"ween the textual and visual modalities is the key to performance enhancement in our TMEKU system, which leads to better visual information use. 1 Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has achieved state-of-the-art translation performance. However, there remain numerous situations where textual context alone is insufficient for correct translation, such as in the presence of ambiguous words and grammatical gender. Therefore, researchers in this field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the"
2021.wat-1.20,2020.lrec-1.518,0,0.035343,"K, K 0 ∈ Rn×m and b, b0 ∈ Rn are the training parameters. 5 https://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 176 To ensure that both representations have their own projections to compute the candidate probabilities, a textual GRU block and visual GRU block (Delbrouck and Dupont, 2018) obtained as below. bvt = fght (Wbv st ) (2) btt = fght (Wbt st ) t v yt ∼ pt = softmax(Wproj btt + Wproj bvt ), t ,Wv where Wbv , Wbt , Wproj proj are training parameters. 3 Experiments 3.1 Dataset Firstly, we conducted experiments for the En→Ja task using the official Flickr30kEnt-JP dataset (Nakayama et al., 2020), which was extended from the Flickr30k (Young et al., 2014) and Flickr30k Entities (Plummer et al., 2017) datasets, where manual Japanese translations were newly added. For training and validation, we used the Flickr30kEnt-JP dataset6 for Japanese sentences, the Flickr30k Entities dataset7 for English sentences, and the Flickr30k dataset8 for images. They were sharing the same splits of training and validation data made in Flickr30k Entities. For test data, we used the officially provided data of the Flickr30kEnt-JP task, and their corresponding images were in the Flickr30k dataset. Note that"
2021.wat-1.20,P02-1040,0,0.109413,"m; word embedding to 200dim; batch size to 32; beam size to 12; text dropout to 0.3; image region dropout to 0.5; dropout of source RNN hidden states to 0.5; and blocks btt and bvt to 0.5. Specifically, the textual annotation Atxt was 800dim, which was consistent with H. Further, the visual annotation Aimg was 4,096-dim by a concatenation of R and Halign , where R was 2,048-dim and Halign was 2,048-dim by a linear transformation from 800-dim. We trained the model using stochastic gradient descent with ADAM (Kingma and Ba, 2015) and a learning rate of 0.0004. We stopped training when the BLEU (Papineni et al., 2002) score did not improve for 20 evaluations on the validation set, 11 177 https://taku910.github.io/mecab/ Model Baseline NMT Baseline MNMT TMEKU System v.s. baseline NMT v.s. baseline MNMT Ensemble (top 10 models) Test NMT baseline by BLEU scores of 0.86 and outperformed the MNMT baseline by BLEU scores of 0.69 on the official test set. Our TMEKU system achieved significant improvement over both the NMT and MNMT baselines. Moreover, the result of ensembling the top 10 models has achieved the first place in the ranking of this task. We also participated in the Ambiguous MSCOCO task on the En→Ja"
2021.wat-1.20,P16-1162,0,0.0113637,"concept consisting of an attribute class followed by an object class instead of only the object class. We take these visual concepts to represent the image regions. We set each image labeled with 36 visual concepts of image regions, which are space-separated phrases. For the words, we lowercase and tokenize the source English sentences via the Moses toolkit.3 The soft alignment is a similarity matrix filled with the cosine similarity between source words and visual concepts. To avoid unknown words, we convert the words and concepts into subword units using the byte pair encoding (BPE) model (Sennrich et al., 2016). Subsequently, we utilize fastText (Bojanowski et al., 2017) to learn subword embeddings. We use a pre-trained model4 containing two million word vectors trained with subword information on Common Crawl (600B tokens). The source subword embeddings can be generated directly, whereas the generation of visual concept embeddings should take an average of the embeddings of all constituent subwords because they are phrases. As shown in Figure 1, source subwords are represented by W = {w1 , w2 , w3 , · · · , wn }, and the visual concepts are represented by C = {c1 , c2 , c3 , · · · , c36 }. These em"
2021.wat-1.20,W16-2346,0,0.0443301,"Missing"
2021.wat-1.20,2020.wat-1.7,1,0.745616,"field have established multimodal neural machine translation (MNMT) tasks (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018), which translates sentences paired with images into a target language. Due to the lack of multimodal datasets, multimodal tasks on the English→Japanese (En→Ja) language pair have not been paid attention to. Since the year 2020, as the multimodal dataset on the En→Ja language pair has been made publicly available, the multimodal machine translation (MMT) tasks on the En→Ja were held at the WAT 2020 (Nakazawa et al., 2020) for the first time. Some studies (Tamura et al., 2020) have started to focus on incorporating multimodal contents, particularly images, to improve the translation performance on the En→Ja task. In this study, we apply our system (Zhao et al., 2021) for the MMT task on the En→Ja language pair, which is called TMEKU system. This system is designed to translate a source word into a target word, focusing on a relevant image region. To guide the model to translate certain words based on certain image regions, explicit alignment over source words and image regions is needed. We propose to generate soft alignment of word-region based on cosine similarit"
2021.wat-1.20,Q14-1006,0,0.0188477,"ps://github.com/nyu-dl/ dl4mt-tutorial/blob/master/docs/cgru.pdf 176 To ensure that both representations have their own projections to compute the candidate probabilities, a textual GRU block and visual GRU block (Delbrouck and Dupont, 2018) obtained as below. bvt = fght (Wbv st ) (2) btt = fght (Wbt st ) t v yt ∼ pt = softmax(Wproj btt + Wproj bvt ), t ,Wv where Wbv , Wbt , Wproj proj are training parameters. 3 Experiments 3.1 Dataset Firstly, we conducted experiments for the En→Ja task using the official Flickr30kEnt-JP dataset (Nakayama et al., 2020), which was extended from the Flickr30k (Young et al., 2014) and Flickr30k Entities (Plummer et al., 2017) datasets, where manual Japanese translations were newly added. For training and validation, we used the Flickr30kEnt-JP dataset6 for Japanese sentences, the Flickr30k Entities dataset7 for English sentences, and the Flickr30k dataset8 for images. They were sharing the same splits of training and validation data made in Flickr30k Entities. For test data, we used the officially provided data of the Flickr30kEnt-JP task, and their corresponding images were in the Flickr30k dataset. Note that the Japanese training data size is originally 148,915 sente"
2021.wat-1.20,2020.eamt-1.12,1,0.81952,"abulary sizes of En→Ja were 9,578→22,274 tokens. For image regions, we used Faster-RCNN (Ren et al., 2015) in Anderson et al. (2018) to detect up to 36 salient visual objects per image and extracted their corresponding 2,048-dim image region features and attribute-object combined concepts. 3.3 Settings (i) NMT: the baseline NMT system (Bahdanau et al., 2015) is the architecture comprised a 2-layer bidirectional GRU encoder and a 2-layer cGRU decoder with attention mechanism, which only encodes the source sentence as the input. (ii) MNMT: the baseline MNMT system without word-region alignment (Zhao et al., 2020). This architecture comprised a 2-layer bidirectional GRU encoder and a 2-layer cGRU decoder with double attentions to integrate visual and textual features. (iii) TMEKU system: our proposed MNMT system with word-region alignment. We conducted all experiments on Nmtpy toolkit (Caglayan et al., 2017). 3.3.1 Parameters We ensured that the parameters were consistent in all the settings. We set the encoder and decoder hidden state to 400-dim; word embedding to 200dim; batch size to 32; beam size to 12; text dropout to 0.3; image region dropout to 0.5; dropout of source RNN hidden states to 0.5; an"
2021.wat-1.5,D10-1092,0,0.0534875,"2020)5 , where dmodel = 512, dhidden = 512, nlayer = 6, and nhead = 8. However, we did not share the source and target vocabularies. Moreover, we changed the number of segments from the original paper (i.e., 10) because some examples had more than 10 (up to 14) RTVs in the test data. We also expanded the length of a segment to be able to insert all the tokens of the RTV if the RTV has more tokens than allowed by default. We examined four RecoverSAT models with different numbers of segments: 10 is the default value in 4 4.1 Results Official Evaluation Table 1 presents the official BLEU, RIBES (Isozaki et al., 2010), and AMFM (Banchs et al., 2015) scores, calculated in the evaluation server, for the model in which the number of segments is 14. As shown in Table 1, the BLEU, RIBES, and AMFM scores were 25.29, 0.653597, and 0.612290 points, respectively. 4.2 Our Evaluation Table 2 presents the scores obtained in our evaluation. Moreover, Figure 1 shows the BLEU score and consistency scores for different numbers of segments {10, 14, 21, 29}. 5 We used the implementation at https://github. com/ranqiu92/RecoverSAT and minimally modified it for inserting RTVs. 70 BLEU score. Figure 1 shows that the translation"
2021.wat-1.5,P07-2045,0,0.0184162,"order of RTVs using GIZA++, we used MeCab4 with IPADIC to tokenize Japanese sentences before computing the alignment. 3 4 69 Note that both Pi and i start from 0. https://taku910.github.io/mecab/ 3.2 Evaluation We evaluated system outputs using the following two distinct metrics. RecoverSAT RIBES AMFM 25.29 0.653597 0.612290 Table 1: Results of the official score using RecoverSAT with 14 segments and forced translation with sorted order. BLEU score. The BLEU score is a metric evaluated by the n-gram matching rate with the reference. We calculated it using multi-bleu.perl in the Moses toolkit (Koehn et al., 2007). Consistency score. The consistency score is the ratio of translations that satisfy the exact match of all the given constraints over the entire test corpus. The exact match is determined as follows. We simply lowercased hypotheses and constraints and then judged character-level sequence matching (including whitespaces) for each constraint. For the final score, we calculated the BLEU score using only the translations that exactly matched their RTVs. In other words, first, we calculated the exact match, and then, we replaced the translations that did not satisfy the constraint with an empty st"
2021.wat-1.5,W18-6318,0,0.0129755,"ore independently as the number of segments increases. As the number of segments increases, the length of each segment becomes shorter, and the model becomes closer to the non-autoregressive model. Table 2 shows that sorting the RTVs using GIZA++ improves the BLEU score. However, there is still a significant gap in the scores compared with those obtained using the oracle order. This is because the word order between Japanese and English is different. sistency score but also the BLEU score. 5 Related Work Previously, some NMT with terminology constraints have been studied (Hasler et al., 2018; Alkhouli et al., 2018; Dinu et al., 2019; Chen et al., 2020; Song et al., 2020). For example, Song et al. (2020) proposed a dedicated head in a multi-head Transformer architecture to learn explicit word alignment and use it to guide the constrained decoding process. When the source-aligned word matches a dictionary, the model outputs the corresponding target word. However, these models are not available for the “restricted translation” task because we can only access the target-side vocabularies. In this study, we used the semi-autoregressive model RecoverSAT (Ran et al., 2020). Originally, this model was not inte"
2021.wat-1.5,D18-2012,0,0.0147066,"model predicts the remainder of the segment in a semi-autoregressive manner. In contrast to the original “forced translation,” which only takes one constrained word (or phrase), we are required to place multiple RTVs in a transla3 Experimental Setup 3.1 Dataset We used the ASPEC (Nakazawa et al., 2016) dataset for Japanese-to-English translation. This dataset contains 3M sentences as training data, 1,790 sentences as validation data, and 1,812 sentences as test data. As explained in Section 2.1, we refined the latter half of the training data using forward-translation. We used SentencePiece (Kudo and Richardson, 2018) to tokenize the training data for both the source and target sentences, where the vocabulary size was set to 4K. Note that we used SentencePiece models obtained from the first 1.5M training data through all the experiments. When determining the insertion order of RTVs using GIZA++, we used MeCab4 with IPADIC to tokenize Japanese sentences before computing the alignment. 3 4 69 Note that both Pi and i start from 0. https://taku910.github.io/mecab/ 3.2 Evaluation We evaluated system outputs using the following two distinct metrics. RecoverSAT RIBES AMFM 25.29 0.653597 0.612290 Table 1: Results"
2021.wat-1.5,D19-5211,0,0.0111969,"” This task requires the output sentence to contain all the pre-specified restricted target vocabularies (RTVs)1 . In other words, we are given a source sentence and a set of RTVs, and we are supposed to generate an output sentence that contains all the RTVs in the set2 . Since the emergence of neural machine translation (NMT) models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), several 2 System Overview 1 Each RTV is either a word or a phrase. 2 For details of the task description, https://sites.google.com/view/ restricted-translation-task/. 2.1 see Corpus Refinement Morishita et al. (2019) reported that the synthetic 68 Proceedings of the 8th Workshop on Asian Translation, pages 68–73 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics tion. To compensate for this gap, we place the i-th RTV at the Pi -th segment as follows3 : data generated by back-translation (Sennrich et al., 2016) degraded the performance in the Japaneseto-English translation setting. The reason for this phenomenon was that the ASPEC (Nakazawa et al., 2016) training sentences are ordered by sentence alignment scores, and so the sentences with lower scores are conside"
2021.wat-1.5,P19-1294,0,0.0197907,"e number of segments increases. As the number of segments increases, the length of each segment becomes shorter, and the model becomes closer to the non-autoregressive model. Table 2 shows that sorting the RTVs using GIZA++ improves the BLEU score. However, there is still a significant gap in the scores compared with those obtained using the oracle order. This is because the word order between Japanese and English is different. sistency score but also the BLEU score. 5 Related Work Previously, some NMT with terminology constraints have been studied (Hasler et al., 2018; Alkhouli et al., 2018; Dinu et al., 2019; Chen et al., 2020; Song et al., 2020). For example, Song et al. (2020) proposed a dedicated head in a multi-head Transformer architecture to learn explicit word alignment and use it to guide the constrained decoding process. When the source-aligned word matches a dictionary, the model outputs the corresponding target word. However, these models are not available for the “restricted translation” task because we can only access the target-side vocabularies. In this study, we used the semi-autoregressive model RecoverSAT (Ran et al., 2020). Originally, this model was not intended to output forc"
2021.wat-1.5,J03-1002,0,0.0372958,"Missing"
2021.wat-1.5,P02-1040,0,0.109149,"Missing"
2021.wat-1.5,2020.acl-main.277,0,0.502627,"ems capable of decoding translations under terminological constraints (Hasler et al., 2018; Dinu et al., 2019; Chen et al., 2020; Song et al., 2020). However, these previous studies were conducted under the condition that a bilingual dictionary is given. Moreover, these challenges are limited to autoregressive NMT systems, and scant research has been conducted on non-autoregressive or semiautoregressive NMT systems, which have received more attention recently. To accomplish restricted translation, where only target terminologies are given, we used a semiautoregressive model called RecoverSAT (Ran et al., 2020), which generates a sentence as a sequence of segments. In this model, the segments are generated simultaneously, and each segment is predicted token-by-token. Ran et al. (2020) also attempted to force the model to generate a certain token at the beginning of a segment and showed that the model could generate valid sentences under the constraint. Then, we considered whether this model could be applied to generate sentences containing RTVs. When tackling this task using this model, the insertion order of RTVs is a critical issue. To address this issue, we used GIZA++ (Och and Ney, 2003) to obta"
2021.wat-1.5,P16-1009,0,0.025113,"ver et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), several 2 System Overview 1 Each RTV is either a word or a phrase. 2 For details of the task description, https://sites.google.com/view/ restricted-translation-task/. 2.1 see Corpus Refinement Morishita et al. (2019) reported that the synthetic 68 Proceedings of the 8th Workshop on Asian Translation, pages 68–73 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics tion. To compensate for this gap, we place the i-th RTV at the Pi -th segment as follows3 : data generated by back-translation (Sennrich et al., 2016) degraded the performance in the Japaneseto-English translation setting. The reason for this phenomenon was that the ASPEC (Nakazawa et al., 2016) training sentences are ordered by sentence alignment scores, and so the sentences with lower scores are considered relatively noisy data. Therefore, Morishita et al. (2019) attempted to generate synthetic data using forward-translation instead of standard back-translation and confirmed that forward-translation improved the performance of the Japanese-to-English translation setting. Following Morishita et al. (2019), we used forward-translation to re"
C12-1144,P12-2073,0,0.0231917,"ract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which depend on edit distance between words before and after"
C12-1144,P11-1091,0,0.0590753,"their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typographical errors, these spelling errors are difficult to detect because the words to be corrected are possible words in English. Previous studies in spelling correction for ESL writing depend mainly on edit distance between the words before and after correction. Some previous works for correcting misspelled words in native speaker misspellings focus on homophone, confusion, split, and merge errors (Golding and Roth, 1999; Bao et al., 2011), but no research has been done on inflection and derivation errors. One of the biggest problems in grammatical error detection and correction studies is that ESL writing contains spelling errors, and they are often obstacles to POS tagging and syntactic parsing. For example, POS tagging fails for the following sentence1 : Input: ... it is *verey/very *convent/convenient for the group. without spelling error correction: ... it/PRP, is/VBZ, verey/PRP, convent/NN ... with spelling error correction: ... it/PRP, is/VBZ, very/RB, convenient/JJ ... Conversely, spelling correction requires POS inform"
C12-1144,P00-1037,0,0.0271202,", and joint analysis (Section 2), and then describe our proposed method in detail (Section 3). The experimental setting and the results are presented in Section 4, and error analysis is given in Section 5. Finally, we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical erro"
C12-1144,P06-1032,0,0.0327827,"s preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which depend on edit distance between words before and after correction. Then, candidate words are often re-ranked or filtered using a language model. In fact, in the Helping Our Own (HOO) 2012 (Dale et al., 2012), which is a shared task on preposition and determiner error correction, highlyranked teams employ the strategy of spelling correction as preprocessing based on edit distance. Some recent studies deal with spelling correction at the same time as whole grammatical error correction. For example, (Brockett et al., 2006) presents a method to correct whole sentences containing various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty of error-annotated resources and cannot correct misspelled words that have never appeared in a corpus. Similarly, (Park and Levy, 2011) propose a noisy channel model to correct errors, although they depend on a bigram language model and do not use syntactic information. A discriminative approach for whole grammatica"
C12-1144,D07-1019,0,0.0210056,", we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing sp"
C12-1144,P11-1092,0,0.0206235,"novel types of misspelling in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typograph"
C12-1144,D12-1052,0,0.151857,"g various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty of error-annotated resources and cannot correct misspelled words that have never appeared in a corpus. Similarly, (Park and Levy, 2011) propose a noisy channel model to correct errors, although they depend on a bigram language model and do not use syntactic information. A discriminative approach for whole grammatical error correction is also proposed in a recent study (Dahlmeier and Ng, 2012) where spelling errors are corrected simultaneously. In terms of spelling error types, however, typographical errors using GNU Aspell are dealt with, but not other misspelling types such as split and merge errors. Our proposed model uses POS features in order to correct spelling. As result, a wider range of spelling errors such as inflection and derivation errors can be corrected. Inflection and derivation errors are usually regarded as grammatical errors, not spelling errors. However, we include inflection and derivation error correction in our task, given the difficulty of determining whethe"
C12-1144,W12-2006,0,0.0136988,"e learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which depend on edit distance between words before and after correction. Then, candidate words are often re-ranked or filtered using a language model. In fact, in the Helping Our Own (HOO) 2012 (Dale et al., 2012), which is a shared task on preposition and determiner error correction, highlyranked teams employ the strategy of spelling correction as preprocessing based on edit distance. Some recent studies deal with spelling correction at the same time as whole grammatical error correction. For example, (Brockett et al., 2006) presents a method to correct whole sentences containing various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty"
C12-1144,C08-1022,0,0.0300052,"ne analysis. We also show that the joint model can deal with novel types of misspelling in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/ple"
C12-1144,C10-1041,0,0.0540719,"ction 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers suc"
C12-1144,P08-1043,0,0.0312985,"regarded as grammatical errors, not spelling errors. However, we include inflection and derivation error correction in our task, given the difficulty of determining whether they are grammatical or spelling errors, as will be explained in Section 4.1. Joint learning and joint analysis have received much attention in recent studies for linguistic analysis. For example, the CoNLL-2008 Shared Task (Surdeanu et al., 2008) shows promising results in joint syntactic and semantic dependency parsing. There are also models that deal with joint morphological segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008), joint word segmentation and POS tagging in Chinese (Zhang and Clark, 2010), and joint word segmentation, POS tagging and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of"
C12-1144,P12-1110,0,0.0225835,"grammatical or spelling errors, as will be explained in Section 4.1. Joint learning and joint analysis have received much attention in recent studies for linguistic analysis. For example, the CoNLL-2008 Shared Task (Surdeanu et al., 2008) shows promising results in joint syntactic and semantic dependency parsing. There are also models that deal with joint morphological segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008), joint word segmentation and POS tagging in Chinese (Zhang and Clark, 2010), and joint word segmentation, POS tagging and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of spelling error correction and POS tagging for ESL writing. Our method is based on Japanese morphological analysis (Kudo et al., 2004), which disambiguates word boundaries and"
C12-1144,D09-1129,0,0.0264771,"ection 2), and then describe our proposed method in detail (Section 3). The experimental setting and the results are presented in Section 4, and error analysis is given in Section 5. Finally, we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correcti"
C12-1144,W04-3230,1,0.574724,"ing and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of spelling error correction and POS tagging for ESL writing. Our method is based on Japanese morphological analysis (Kudo et al., 2004), which disambiguates word boundaries and assigns POS tags using re-defined Conditional Random Fields (CRFs) (Lafferty et al., 1999), while the original CRFs deal with sequential labeling for sentences with word boundaries fixed. We use the re-defined CRFs rather than the original CRFs because disambiguating word boundaries is necessary for split and merge error correction. In terms of decoding, our model has a similar approach to the decoder proposed by (Dahlmeier and Ng, 2012), though the decoder by Dahlmeier and Ng uses beam search. In (Kudo et al., 2004),  they define CRFs as the conditio"
C12-1144,P08-1021,0,0.0164348,"that the joint model can deal with novel types of misspelling in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation"
C12-1144,P11-1121,0,0.213437,"Missing"
C12-1144,P11-1094,0,0.0123656,"pelling correction as preprocessing based on edit distance. Some recent studies deal with spelling correction at the same time as whole grammatical error correction. For example, (Brockett et al., 2006) presents a method to correct whole sentences containing various errors, applying a statistical machine translation (SMT) technique where input sentences are translated into correct English. Although this approach can deal with any type of spelling errors, it suffers from a poverty of error-annotated resources and cannot correct misspelled words that have never appeared in a corpus. Similarly, (Park and Levy, 2011) propose a noisy channel model to correct errors, although they depend on a bigram language model and do not use syntactic information. A discriminative approach for whole grammatical error correction is also proposed in a recent study (Dahlmeier and Ng, 2012) where spelling errors are corrected simultaneously. In terms of spelling error types, however, typographical errors using GNU Aspell are dealt with, but not other misspelling types such as split and merge errors. Our proposed model uses POS features in order to correct spelling. As result, a wider range of spelling errors such as inflect"
C12-1144,P11-1093,0,0.0159978,"ing in ESL writing. Keywords: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typographical errors, these spelling"
C12-1144,P10-1028,0,0.013743,"ror correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is performed before such linguistic analysis as POS tagging and syntactic parsing. Spelling correction as preprocessing generally uses existing spelling checkers such as GNU Aspell2 and Jazzy3 , which dep"
C12-1144,P12-2039,1,0.830212,"s: Part-of-Speech Tagging, Spelling Error Correction. Proceedings of COLING 2012: Technical Papers, pages 2357–2374, COLING 2012, Mumbai, December 2012. 2357 1 Introduction Automated grammatical error detection and correction have been focused on natural language processing (NLP) over the past dozen years or so. Researchers have mainly studied English grammatical error detection and correction of areas such as determiners, prepositions and verbs (Izumi et al., 2003; Han et al., 2006; Felice and Pulman, 2008; Lee and Seneff, 2008; Gamon, 2010; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011; Tajiri et al., 2012). In previous work on grammatical error detection and correction, spelling errors are usually corrected in a preprocessing step in a pipeline. These studies generally deal with typographical errors (e.g. *begginning/beginning). In ESL writing, however, there exist many other types of spelling errors, which often occur in combination with, for example, homophone (*there/their), confusion (*form/from), split (*Now a day/Nowadays), merge (*swimingpool/swimming pool), inflection (*please/pleased), and derivation (*badly/bad) errors. Unlike typographical errors, these spelling errors are difficult"
C12-1144,P02-1019,0,0.0253919,"method in detail (Section 3). The experimental setting and the results are presented in Section 4, and error analysis is given in Section 5. Finally, we conclude in Section 6. 2 Related works In spelling error correction, the main concern is how to extract confusion pairs that consist of words before and after correction. A number of studies depend on such edit distance between written and corrected words as Levenshtein Distance (LD), Longest Common Subsequence (LCS) string matching, and pronunciation similarities (Kukich, 1992; Brill and Moore, 2000; Islam and Inkpen, 2009; Bao et al., 2011; Toutanova and Moore, 2002). In order to cover more misspelled words, many spelling errors were collected from web search queries and their results (Chen et al., 2007; Gao et al., 2010), click through logs (Sun et al., 2010), and users’ keystroke logs (Baba and Suzuki, 2012). Note that previous studies for spelling correction described above focus on errors made by native speakers rather than second language learners, who show a wider range of misspellings with, for example, split, merge, inflection and derivation errors. 2359 In most grammatical error detection and correction research, spelling error correction is perf"
C12-1144,P11-1019,0,0.0256083,"Missing"
C12-1144,D10-1082,0,0.030964,"n and derivation error correction in our task, given the difficulty of determining whether they are grammatical or spelling errors, as will be explained in Section 4.1. Joint learning and joint analysis have received much attention in recent studies for linguistic analysis. For example, the CoNLL-2008 Shared Task (Surdeanu et al., 2008) shows promising results in joint syntactic and semantic dependency parsing. There are also models that deal with joint morphological segmentation and syntactic parsing in Hebrew (Goldberg and Tsarfaty, 2008), joint word segmentation and POS tagging in Chinese (Zhang and Clark, 2010), and joint word segmentation, POS tagging and dependency parsing in Chinese (Hatori et al., 2012). These studies demonstrate that joint models outperform conventional pipelined systems. Our work applies for the first time a joint analysis to spelling correction and POS tagging for ESL writing in which input sentences contains multiple errors, whereas previous joint models deal only with canonical texts. 3 Joint analysis of POS tagging and spelling correction In this section, we describe our proposed joint analysis of spelling error correction and POS tagging for ESL writing. Our method is bas"
C12-1144,W08-2121,0,\N,Missing
C12-1144,P03-2026,0,\N,Missing
C12-2084,J96-1002,0,0.0357184,"rtion-limit for Moses, therefore we chose the edit distance to be smaller than the distortion-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at"
C12-2084,P06-1032,0,0.289136,"in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = ar"
C12-2084,P11-1092,0,0.0143922,"ount for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statisti"
C12-2084,D12-1052,0,0.085074,"rror-tagged corpus annotated by the wisdom of crowds. In addition, they targeted only spelling, article, preposition and word form errors, while we do not restrict error types. Third, Han et al. (2010) developed a preposition correction system using a large scale error-tagged corpus of learner English. They built a maximum entropy-based model for preposition errors trained on learner and native corpora. We also take advantage of a large scale error-tagged corpus of learner English, but use phrase-based SMT to deal with various kinds of errors and to fully exploit the learner corpus. Recently, Dahlmeier and Ng (2012) presented a beam-search decoder for correcting spelling, article, preposition, punctuation and noun number errors. They reported that their discriminative model achieves considerably better results than an SMT baseline trained on a few hundreds of sentences. As we will see later, we observed a similar tendency in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phras"
C12-2084,W12-2006,0,0.0189159,"r baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluation criteria. To be precise, we use recall, precision and F-measure. Recall and precision for each type of errors are calculated from true positive, false positive and false negative based on error tags in KJ Corpus. The word which does not have any tag in KJ Corpus does not affect precision for each type of errors 8 . For example, let us cons"
C12-2084,C08-1022,0,0.0206406,"Missing"
C12-2084,han-etal-2010-using,0,0.0387109,"to emphasize that our work is the ﬁrst attempt to use a real world large learner corpus with phrase-based SMT technique. We will show that phrase-based SMT especially suffers from data sparseness. Second, Park and Levy (2011) attempted to correct various kinds of errors with a noisy channel model using a large scale unannotated corpus of learner English. Ours differs from their work in that we use a large scale error-tagged corpus annotated by the wisdom of crowds. In addition, they targeted only spelling, article, preposition and word form errors, while we do not restrict error types. Third, Han et al. (2010) developed a preposition correction system using a large scale error-tagged corpus of learner English. They built a maximum entropy-based model for preposition errors trained on learner and native corpora. We also take advantage of a large scale error-tagged corpus of learner English, but use phrase-based SMT to deal with various kinds of errors and to fully exploit the learner corpus. Recently, Dahlmeier and Ng (2012) presented a beam-search decoder for correcting spelling, article, preposition, punctuation and noun number errors. They reported that their discriminative model achieves conside"
C12-2084,N03-1017,0,0.023552,"eir discriminative model achieves considerably better results than an SMT baseline trained on a few hundreds of sentences. As we will see later, we observed a similar tendency in preposition error correction when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correc"
C12-2084,P08-1021,0,0.0105552,"It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like pr"
C12-2084,I11-1017,1,0.927515,"orrection when we trained a phrase-based SMT system on a small learner corpus. However, in this work, we exploit a large scale error-annotated corpus extracted from the web to overcome the data sparseness problem. 3 3.1 Using a large scale learner corpus with phrase-based SMT for grammatical error correction Error correction with phrase-based SMT We use phrase-based statistical machine translation (Koehn et al., 2003) to conduct unrestricted error correction. There are several studies about grammatical error correction using phrase-based statistical machine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = arg max P(e |f ) = arg ma"
C12-2084,P02-1038,0,0.0170949,"chine translation (Brockett et al., 2006; Mizumoto et al., 2011; Ehsan and Faili, 2012). Although Brockett et al. (2006) corrected English learners’ error using phrase-based statistical machine translation, they only targeted mass noun errors. Mizumoto et al. (2011) dealt with un865 restricted types of learners’ errors, but their target is not English but Japanese. Ehsan and Faili (2012) applied an SMT framework to English and Persian grammatical error correction, but used artiﬁcially created learner corpora. The well-known statistical machine translation formulation using a log-linear model (Och and Ney, 2002) is deﬁned by: eˆ = arg max P(e |f ) = arg max e e M ∑ λm hm (e, f ) (1) m=1 where e represents target sentences (corrected sentences) and f represents source sentences (sentences written by learners). hm (e, f ) is a feature function and λm is a model parameter for each feature function. This formulation ﬁnds a target sentence e that maximizes a weighted linear combination of feature functions for source sentence f . A translation model and a language model can be used as feature functions. The translation model is commonly represented as conditional probability P( f |e) factored into the tra"
C12-2084,J03-1002,0,0.0251172,"the effect of error correction methods, we also experimented on the preposition error correction task using a maximum entropy model as a discriminative baseline and SMT-based models as our proposal for all error correction. 3 http://lang-8.com/ 4 We use 6 as a distortion-limit for Moses, therefore we chose the edit distance to be smaller than the distortion-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008"
C12-2084,P11-1094,0,0.0394033,"er corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statistical model trained on native corpora and/or learner corpora. It was not until recently that large sca"
C12-2084,P11-1093,0,0.0268429,"s the distribution of errors found in KJ Corpus2 . The most frequent error type is article errors, followed by noun number and preposition errors. It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not of"
C12-2084,W12-2033,1,0.810438,"on-limit. 866 4.1 Tools and experimental data We used Moses 2010-08-13 5 with default parameters as a decoder and GIZA++ 1.0.5 6 as an alignment tool to implement an error correction system with phrase-based SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consi"
C12-2084,N12-1037,0,0.0216011,"udies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be corrected by simple rules using heuristics, while others like preposition errors are difﬁcult to correct without statistical model trained on native corpora and/or learner corpora. It was not until recently that large scale learner corpora became widely availa"
C12-2084,P12-2039,1,0.760617,"rticle errors, followed by noun number and preposition errors. It is not surprising that frequent types of errors account for the most errors, but it should be noted that there are many different types of errors in learner corpus. Thus far, a lot of studies have been made on automated error correction in regard to errors ESL learners make. However, most previous studies of second language learning deal with one or a few restricted types of learners’ errors. For example, there are studies on preposition errors (Rozovskaya and Roth, 2011), verb selection errors (Liu et al., 2011), tense errors (Tajiri et al., 2012), verb form errors (agreement and tense) (Lee and Seneff, 2008), preposition and article errors (Dahlmeier and Ng, 2011) and spelling, article, preposition and word form (agreement and tense) errors (Park and Levy, 2011). Recently, Swanson and Yamangil (2012) presented a detailed analysis on correcting all types of errors in the Cambridge Learner Corpus, but their task is different from the others in that their goal is to detect errors and select error types given both the original and corrected text, which is not often available in practice. Some types of errors like agreement errors can be c"
C12-2084,P10-2065,0,0.0129726,"d SMT. We applied growdiag-ﬁnal-and (Och and Ney, 2003) heuristics for phrase extraction. The number of extracted phrases are 1,050,070 (245 MB) using all data of Lang-8 Corpus. We used 3-gram as a language model trained on the corrected text of Lang-8 Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluati"
C12-2084,P11-1019,0,0.0348522,"Corpus. Next, we built the maximum entropy model (Berger et al., 1996) as a multi-class classiﬁer baseline for preposition error correction (Sakaguchi et al., 2012). We used the implementation of Maximum Entropy Modeling Toolkit 7 with its default parameters. We incorporated surface, POS, WordNet, parse and language model features described in (Tetreault et al., 2010) and (De Felice and Pulman, 2008). POS and parse features were extracted using the Stanford Parser 2.0.2. This system achieves recall of 18.44, precision of 34.88 and F-measure of 24.12 trained and tested on the CLC FCE dataset (Yannakoudakis et al., 2011), which ranked the 4th out of 13 systems at the HOO 2012 Shared Task (Dale et al., 2012). We use KJ Corpus as a test data. KJ Corpus consist of 170 essays, containing 2,411 sentences. When we experiment on a system using KJ Corpus, we perform 5-fold cross validation. 4.2 Evaluation metrics For the evaluation metrics, we use automatic evaluation criteria. To be precise, we use recall, precision and F-measure. Recall and precision for each type of errors are calculated from true positive, false positive and false negative based on error tags in KJ Corpus. The word which does not have any tag in"
C16-1109,S12-1051,0,0.0216001,"tistical machine translation and showed that sentence pairs with a moderate level of similarity are effective for training text simplification models. Therefore, we use the sentence similarity method to accurately measure the moderate level of 3 https://www.ukp.tu-darmstadt.de/data/sentence-simplification/ simple-complex-sentence-pairs/ 4 http://www.cs.pomona.edu/˜dkauchak/simplification/ 1149 similarity. To address the challenge of computing the similarity between sentences containing different words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automa"
C16-1109,S13-1004,0,0.0173112,"slation and showed that sentence pairs with a moderate level of similarity are effective for training text simplification models. Therefore, we use the sentence similarity method to accurately measure the moderate level of 3 https://www.ukp.tu-darmstadt.de/data/sentence-simplification/ simple-complex-sentence-pairs/ 4 http://www.cs.pomona.edu/˜dkauchak/simplification/ 1149 similarity. To address the challenge of computing the similarity between sentences containing different words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automatic construction of a"
C16-1109,S15-2045,0,0.158019,"15). These unsupervised sentence similarity metrics can be applied to the automatic construction of a monolingual parallel corpus for text simplification, without requiring the data to be labeled. 3 Sentence Similarity based on Alignment between Word Embeddings We propose four types of sentence similarity measures for building a monolingual parallel corpus for text simplification, based on alignments between word embeddings that have achieved outstanding performance on different NLP tasks. The methods discussed in Sections 3.1-3.3 are the sentence similarity measures proposed by Song and Roth (2015) for a short text similarity task. The Word Mover’s Distance (Kusner et al., 2015) discussed in Section 3.4 is another sentence similarity measure based on alignment between word embeddings that is known to achieve good performance on a document classification task. 3.1 Average Alignment The sentence similarity STSave (x, y) between sentence x and sentence y is computed by averaging the similarities between all pairs of words taken from the two sentences, as follows: |x ||y| 1 XX STSave (x, y) = φ(xi , yj ) |x||y| (1) i=1 j=1 Here, xi denotes the i-th word in the sentence x (x = (x1 , x2 , . ."
C16-1109,W11-1603,0,0.0188976,"015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex and simple texts. This was done in two steps. First, we compute the similarity for all"
C16-1109,W15-1604,0,0.0612777,"Missing"
C16-1109,W11-1601,0,0.304794,"tistical machine translation trained using the corpus built by the proposed method to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Ka"
C16-1109,P11-2117,0,0.220372,"tistical machine translation trained using the corpus built by the proposed method to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Ka"
C16-1109,2015.mtsummit-papers.2,0,0.569469,"oduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011),"
C16-1109,W11-2123,0,0.0106434,"implification corpora (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015). The results were compared to evaluate the effectiveness of our text simplification corpus. We treated text simplification as a translation problem from the normal sentence to the simple one and modeled it using a phrase-based SMT trained as a log linear model. In each corpus, we randomly sampled 500 sentence pairs for tuning with MERT (Och, 2003) and used the remainder for training. Moses was used as the phrase-based SMT tool. We employed GIZA++ (Och and Ney, 2003) to obtain the word alignment, and KenLM (Heafield, 2011) to build the 5-gram language model from the entire Simple English Wikipedia 7 . As test data, we used 277 sentence pairs labeled G and 281 sentence pairs labeled G + GP from the Hwang et al. (2015) dataset and evaluated the accuracy using BLEU. Table 3 shows the number of sentences, range of vocabulary, average number of words per sentence, and BLEU scores of the text simplification models trained on each corpus. The text simplification model trained on our corpus achieved the best BLEU score. To compare the learning curves of our corpus with that from Hwang et al. (2015), we recorded the BLE"
C16-1109,N15-1022,0,0.144244,"ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex"
C16-1109,W13-2902,0,0.0486956,"ation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex and simple texts. This was done in two steps. First, we compute the similarity for all combinations of complex and simple sentences using the alignment"
C16-1109,klerke-sogaard-2012-dsim,0,0.0310404,", unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et al., 2009), Spanish (Bott and Saggion, 2011), Danish (Klerke and Søgaard, 2012), German (Klaper et al., 2013), Italian (Brunato et al., 2015), and Japanese (Goto et al., 2015). In addition, only the English corpora are open to the public. We therefore propose an unsupervised method 1 that automatically builds monolingual parallel corpora for text simplification without using any external resources for computing sentence similarity. In this study, a monolingual parallel corpus for text simplification is built from a comparable corpus comprising complex and simple texts. This was done in two steps. First, we compute the similarity for all combinations of complex and simple"
C16-1109,P07-2045,0,0.0109755,"l corpus. • The proposed method can build a monolingual parallel corpus for text simplification at low cost because it does not require any external resources such as labeled data or dictionaries when computing sentence similarity. 2 Related Work The statistical machine translation framework has become widely used in text simplification. In English, text simplification using a monolingual parallel corpus extracted from the English Wikipedia and Simple English Wikipedia has been actively studied. Coster and Kauchak (2011b) simplified sentences using the standard phrase-based SMT toolkit Moses (Koehn et al., 2007) and evaluated it using the standard automatic MT evaluation metric BLEU (Papineni et al., 2002). In addition to generic SMT translation models, specialized translation models such as targeting phrasal deletion have been proposed (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012). These studies reported that models specialized 2 http://ssli.ee.washington.edu/tial/projects/simplification/ 1148 Figure 2: Readability score distribution of English Wikipedia and Simple English Wikipedia. A higher score in Flesch Reading Ease indicates simpler sentences. in text simplification improv"
C16-1109,J03-1002,0,0.00740768,"t simplification models using our corpus and existing text simplification corpora (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015). The results were compared to evaluate the effectiveness of our text simplification corpus. We treated text simplification as a translation problem from the normal sentence to the simple one and modeled it using a phrase-based SMT trained as a log linear model. In each corpus, we randomly sampled 500 sentence pairs for tuning with MERT (Och, 2003) and used the remainder for training. Moses was used as the phrase-based SMT tool. We employed GIZA++ (Och and Ney, 2003) to obtain the word alignment, and KenLM (Heafield, 2011) to build the 5-gram language model from the entire Simple English Wikipedia 7 . As test data, we used 277 sentence pairs labeled G and 281 sentence pairs labeled G + GP from the Hwang et al. (2015) dataset and evaluated the accuracy using BLEU. Table 3 shows the number of sentences, range of vocabulary, average number of words per sentence, and BLEU scores of the text simplification models trained on each corpus. The text simplification model trained on our corpus achieved the best BLEU score. To compare the learning curves of our corpu"
C16-1109,P03-1021,0,0.0105773,"rom our text simplification corpus ranked by similarity. 4.3 English Text Simplification We trained SMT-based text simplification models using our corpus and existing text simplification corpora (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015). The results were compared to evaluate the effectiveness of our text simplification corpus. We treated text simplification as a translation problem from the normal sentence to the simple one and modeled it using a phrase-based SMT trained as a log linear model. In each corpus, we randomly sampled 500 sentence pairs for tuning with MERT (Och, 2003) and used the remainder for training. Moses was used as the phrase-based SMT tool. We employed GIZA++ (Och and Ney, 2003) to obtain the word alignment, and KenLM (Heafield, 2011) to build the 5-gram language model from the entire Simple English Wikipedia 7 . As test data, we used 277 sentence pairs labeled G and 281 sentence pairs labeled G + GP from the Hwang et al. (2015) dataset and evaluated the accuracy using BLEU. Table 3 shows the number of sentences, range of vocabulary, average number of words per sentence, and BLEU scores of the text simplification models trained on each corpus. The"
C16-1109,P02-1040,0,0.0958609,"on at low cost because it does not require any external resources such as labeled data or dictionaries when computing sentence similarity. 2 Related Work The statistical machine translation framework has become widely used in text simplification. In English, text simplification using a monolingual parallel corpus extracted from the English Wikipedia and Simple English Wikipedia has been actively studied. Coster and Kauchak (2011b) simplified sentences using the standard phrase-based SMT toolkit Moses (Koehn et al., 2007) and evaluated it using the standard automatic MT evaluation metric BLEU (Papineni et al., 2002). In addition to generic SMT translation models, specialized translation models such as targeting phrasal deletion have been proposed (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012). These studies reported that models specialized 2 http://ssli.ee.washington.edu/tial/projects/simplification/ 1148 Figure 2: Readability score distribution of English Wikipedia and Simple English Wikipedia. A higher score in Flesch Reading Ease indicates simpler sentences. in text simplification improved readability and the BLEU score. In languages other than English, text ˇ simplification using"
C16-1109,N15-1138,0,0.078013,"words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automatic construction of a monolingual parallel corpus for text simplification, without requiring the data to be labeled. 3 Sentence Similarity based on Alignment between Word Embeddings We propose four types of sentence similarity measures for building a monolingual parallel corpus for text simplification, based on alignments between word embeddings that have achieved outstanding performance on different NLP tasks. The methods discussed in Sections 3.1-3.3 are the sentence similarity measures propose"
C16-1109,S15-2027,0,0.0260286,"/simplification/ 1149 similarity. To address the challenge of computing the similarity between sentences containing different words with similar meanings, many methods have been proposed. In semantic textual similarity task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015), sentence similarity is computed on the basis of word similarity following the success of word embeddings such as word2vec (Mikolov et al., 2013a). For example, a supervised approach using word embeddings when obtaining a word alignment achieved the best performance in SemEval-2015 Task 2 (Sultan et al., 2015). Word embeddings have also been used in unsupervised sentence similarity metrics (Mikolov et al., 2013b; Song and Roth, 2015; Kusner et al., 2015). These unsupervised sentence similarity metrics can be applied to the automatic construction of a monolingual parallel corpus for text simplification, without requiring the data to be labeled. 3 Sentence Similarity based on Alignment between Word Embeddings We propose four types of sentence similarity measures for building a monolingual parallel corpus for text simplification, based on alignments between word embeddings that have achieved outstandi"
C16-1109,P15-2135,0,0.0605519,"hod to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et a"
C16-1109,R15-1080,0,0.0623022,"hod to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu et al., 2010; Coster and Kauchak, 2011b; Hwang et al., 2015; Xu et al., 2015), Portuguese (Caseli et a"
C16-1109,P12-1107,0,0.144611,"Missing"
C16-1109,C10-1152,0,0.605214,"e framework of statistical machine translation trained using the corpus built by the proposed method to that using the existing corpora. 1 Introduction Text simplification is the process of rewriting a complex text into a simpler form while preserving its meaning. The purpose of text simplification is to assist the comprehension of readers, especially language learners and children. Recent studies have treated text simplification as a monolingual machine translation problem in which a simple synonymous sentence is generated using the framework of statistical machine translation (Specia, 2010; Zhu et al., 2010; Coster and Kauchak, 2011a; Coster and ˇ ˇ Kauchak, 2011b; Wubben et al., 2012; Stajner et al., 2015a; Stajner et al., 2015b; Goto et al., 2015). However, unlike statistical machine translation, which uses bilingual parallel corpora, text simplification requires a monolingual parallel corpus for training. While bilingual parallel data are available in large quantities, monolingual parallel data are hard to obtain because simplification of a complex text is not a by-product of other tasks. Monolingual parallel corpora for text simplification are available in only seven languages—English (Zhu e"
D08-1106,J04-3004,0,0.0966542,". It incorporates heuristics not present in Simplified Espresso to reduce semantic drift, but these heuristics have limited effect as we demonstrate in Section 3.3. In Section 4, we propose two graph-based algorithms to reduce semantic drift. These algorithms are used in link analysis community to reduce the effect of topic drift. In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. sented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using m"
D08-1106,W06-1669,0,0.0306795,"Missing"
D08-1106,P01-1005,0,0.00906266,"izes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of principled method for selecting optimal parameter values (Ng and Cardie, 2003; Banko and Brill, 2001). Also, there is an issue of generic patterns which deteriorates the quality of acquired instances. Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift. We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail. 2.2 The Espresso Algorithm 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting words in hyponym (is-a) r"
D08-1106,W99-0613,0,0.282231,"s have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Sp"
D08-1106,C92-2082,0,0.0511404,"s and are easy to calibrate. 1 Introduction In recent years machine learning techniques become widely used in natural language processing (NLP). These techniques offer various ways to exploit large corpora and are known to perform well in many tasks. However, these techniques often require tagged corpora, which are not readily available to many languages. So far, reducing the cost of human annotation is one of the important problems for building NLP systems. To mitigate the problem of hand-tagging resources, semi(or minimally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extr"
D08-1106,I08-1047,1,0.901317,"which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straightforward approach to avoid semantic drift is to terminate iterations before hitting generic patterns, but the optimal number of iterations is task dependent and is hard to come by. The recently proposed Espresso (Pantel and Pennacchiotti, 2006) algorithm incorporates sophisticated scoring functions to cope with generic patterns, but as Komachi and Suzuki (2008) pointed out, Espresso still shows semantic drift unless iterations are terminated appropriately. Another deficiency in bootstrapping is its sensitivity to many parameters such as the number of 1011 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011–1020, c Honolulu, October 2008. 2008 Association for Computational Linguistics seed instances, the stopping criterion of iteration, the number of instances and patterns selected on each iteration, and so forth. These parameters also need to be calibrated for each task. In this paper, we present a grap"
D08-1106,J04-1001,0,0.0130383,"ased algorithms to reduce semantic drift. These algorithms are used in link analysis community to reduce the effect of topic drift. In Section 5 we apply them to the task of word sense disambiguation on Senseval-3 Lexical Sample Task and verify that they indeed reduce semantic drift. Finally, we conclude our work in Section 6. sented an unsupervised WSD system which rivals supervised techniques. Abney (2004) presented a thorough discussion on the Yarowsky algorithm. He extended the original Yarowsky algorithm to a new family of bootstrapping algorithms that are mathematically well understood. Li and Li (2004) proposed a method called Bilingual Bootstrapping. It makes use of a translation dictionary and a comparable corpus to help disambiguate word senses in the source language, by exploiting the asymmetric many-to-many sense mapping relationship between words in two languages. Curran et al. (2007) presented an algorithm called Mutual Exclusion Bootstrapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. S"
D08-1106,N03-1023,0,0.0565315,"trapping, which minimizes semantic drift using mutual exclusion between semantic classes of learned instances. They prepared a list of so-called stop classes similar to a stop word list used in information retrieval to help bound the semantic classes. Stop classes are sets of terms known to cause semantic drift in particular semantic classes. However, stop classes vary from task to task and domain to domain, and human intervention is essential to create an effective list of stop classes. A major drawback of bootstrapping is the lack of principled method for selecting optimal parameter values (Ng and Cardie, 2003; Banko and Brill, 2001). Also, there is an issue of generic patterns which deteriorates the quality of acquired instances. Previously proposed bootstrapping algorithms differ in how they deal with the problem of semantic drift. We will take recently proposed Espresso algorithm as the example to explain common configuration for bootstrapping in detail. 2.2 The Espresso Algorithm 2 Related Work 2.1 Overview of Bootstrapping Bootstrapping (or self-training) is a general framework for reducing the requirement of manual annotation. Hearst (1992) described a bootstrapping procedure for extracting w"
D08-1106,P06-1015,0,0.581935,"method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-occur with these generic patterns, such as “Britney Spears.” This phenomenon is called semantic drift (Curran et al., 2007). A straight"
D08-1106,J98-1004,0,0.273044,"Missing"
D08-1106,P95-1026,0,0.752126,"ally)-supervised and unsupervised techniques have been actively studied. Hearst (1992) first presented a bootstrapping method which requires only a small amount of instances Masashi Shimbo NAIST, Japan shimbo@is.naist.jp Yuji Matsumoto NAIST, Japan matsu@is.naist.jp (seed instances) to start with, but can easily multiply the number of tagged instances with minimal human annotation cost, by iteratively applying the following phases: pattern induction, pattern ranking/selection, and instance extraction. Bootstrapping has been widely adopted in NLP applications such as word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999) and relation extraction (Riloff and Jones, 1999; Pantel and Pennacchiotti, 2006). However, it is known that bootstrapping often acquires instances not related to seed instances. For example, consider the task of collecting the names of common tourist sites from web corpora. Given words like “Geneva” and “Bali” as seed instances, bootstrapping would eventually learn generic patterns such as “pictures” and “photos,” which also co-occur with many other unrelated instances. The subsequent iterations would likely acquire frequent words that co-o"
D19-5221,W18-6325,0,0.0344346,"Missing"
D19-5221,P07-2045,0,0.00659451,"st train train development train train development #sent. 12,356 486 600 47,082 200,000 589 82,072 279,307 313 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 5.84M / 5.11M 21k / 16k 1.61M / 1.83M 7.00M / 7.41M 7.8k / 8.4k #types 22k / 42k 2.9k / 4.3k 3.5k / 5.6k 48k / 55k 45k / 78k 3.5k / 3.8k 144k / 74k 214k / 89k 3.2k / 2.3k Table 1: Statistics on our in-domain parallel data. Commentary5 data for Ja↔En and Ru↔En, respectively. Table 1 summarizes the size of train/development/test splits used in our experiments. We tokenized English and Russian sentences using tokenizer.perl of Moses (Koehn et al., 2007).6 To tokenize Japanese sentences, we used MeCab7 with the IPA dictionary. After tokenization, we eliminated duplicated sentence pairs and sentences with more than 100 tokens for all the languages. and outperform bi-directional and uni-directional translation approaches (Imankulova et al., 2019). Similarly, we exploit MultiNMT approach with Transformer architecture. Our work is heavily based on Imankulova et al. (2019). They proposed a multi-stage fine-tuning approach that combines multilingual modeling and domain adaptation. They utilize out-of-domain pivot parallel corpora to perform domain"
D19-5221,W17-3204,0,0.0257109,"ering: (a) extremely low resource setting, the size of parallel data is only 12k parallel sentences; (b) how distant given language pair is, in terms of different writing system, phonology, morphology, grammar, and syntax; (c) difficulty of translating news from various topics which leads to large presence of unknown tokens in such extremely low-resource scenario. Usually, neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) enables endto-end training of a translation system requiring a large amount of training parallel data (Koehn and Knowles, 2017). Therefore, there are different techniques of involving other pivot languages to increase the accuracy of low-resource MT such as pivot-based SMT (Utiyama and Isahara, 2007), 2 Related Work The existing state-of-the-art NMT model known as the Transformer (Vaswani et al., 2017) works well on different scenarios (Lakew et al., 2018; Imankulova et al., 2019). MultiNMT using the artificial token approach (Johnson et al., 2017) is known to help the language pairs with relatively lesser data (Lakew et al., 2018; Rikters et al., 2018) 165 Proceedings of the 6th Workshop on Asian Translation, pages 1"
D19-5221,C18-1054,0,0.0204397,"emely low-resource scenario. Usually, neural machine translation (NMT) (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) enables endto-end training of a translation system requiring a large amount of training parallel data (Koehn and Knowles, 2017). Therefore, there are different techniques of involving other pivot languages to increase the accuracy of low-resource MT such as pivot-based SMT (Utiyama and Isahara, 2007), 2 Related Work The existing state-of-the-art NMT model known as the Transformer (Vaswani et al., 2017) works well on different scenarios (Lakew et al., 2018; Imankulova et al., 2019). MultiNMT using the artificial token approach (Johnson et al., 2017) is known to help the language pairs with relatively lesser data (Lakew et al., 2018; Rikters et al., 2018) 165 Proceedings of the 6th Workshop on Asian Translation, pages 165–170 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Lang.pair Ja↔Ru Ja↔En Ru↔En Source Global Voices News Commentary News Commentary Global Voices Jiji News Commentary Global Voices News Commentary News Commentary Partition train development test train train development train train developme"
D19-5221,D14-1179,0,0.0269192,"Missing"
D19-5221,N16-1101,0,0.0304833,"Missing"
D19-5221,L18-1595,0,0.0222379,"ion system requiring a large amount of training parallel data (Koehn and Knowles, 2017). Therefore, there are different techniques of involving other pivot languages to increase the accuracy of low-resource MT such as pivot-based SMT (Utiyama and Isahara, 2007), 2 Related Work The existing state-of-the-art NMT model known as the Transformer (Vaswani et al., 2017) works well on different scenarios (Lakew et al., 2018; Imankulova et al., 2019). MultiNMT using the artificial token approach (Johnson et al., 2017) is known to help the language pairs with relatively lesser data (Lakew et al., 2018; Rikters et al., 2018) 165 Proceedings of the 6th Workshop on Asian Translation, pages 165–170 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Lang.pair Ja↔Ru Ja↔En Ru↔En Source Global Voices News Commentary News Commentary Global Voices Jiji News Commentary Global Voices News Commentary News Commentary Partition train development test train train development train train development #sent. 12,356 486 600 47,082 200,000 589 82,072 279,307 313 #tokens 341k / 229k 16k / 11k 22k / 15k 1.27M / 1.01M 5.84M / 5.11M 21k / 16k 1.61M / 1.83M 7.00M / 7.41M 7.8k / 8.4k #types 22k / 42k 2.9k"
D19-5221,W19-6613,1,0.824163,"Aizhan Imankulova Masahiro Kaneko Mamoru Komachi Tokyo Metropolitan University 6-6 Asahigaoka, Hino, Tokyo 191-0065, Japan {imankulova-aizhan, kaneko-masahiro}@ed.tmu.ac.jp komachi@tmu.ac.jp Abstract transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and multilingual modeling (Firat et al., 2016). Recently, a simple multilingual modeling (MultiNMT) was proposed by Johnson et al. (2017) which translates between multiple languages using a single model and an artificial token indicating a target language, taking advantage of multilingual data to improve NMT for all languages involved. Imankulova et al. (2019) showed that incorporating MultiNMT (Johnson et al., 2017) provided better BLEU scores than unidirectional and pivot-based PBSMT approaches and that domain mismatch had a negative effect on low-resource NMT. Therefore, we use MultiNMT modeling for an extremely low-resource Ja↔Ru translation involving English (En) as the pivoting third language (Utiyama and Isahara, 2007). Considering the importance of domain matching, we focus on only news domain of additional Ja↔En and Ru↔En auxiliary parallel corpora, which we will refer as pivot parallel corpora. And we investigate how translation results a"
D19-5221,N16-1005,0,0.0204459,"h that combines multilingual modeling and domain adaptation. They utilize out-of-domain pivot parallel corpora to perform domain adaptation on in-domain pivot parallel corpora and then perform multilingual transfer for a language pair of interest. However, instead of utilizing out-ofdomain pivot parallel corpora, we investigate the impact of other in-domain pivot parallel corpora. Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called back-translation can substantially improve the quality of NMT (Sennrich et al., 2016). However, this approach requires base MT systems that can generate somewhat accurate translations (Imankulova et al., 2017). Therefore, instead of creating noisy pseudo-parallel corpora, we take advantage of other in-domain pivot parallel corpora. 3 3.2 Systems This section describes our system TMU and our baseline which based on the same MultiNMT architecture (Johnson et al., 2017) but trained on different training corpora (Table 1). Here, MultiNMT translates from multiple source languages into different target languages within a single model. To realize such translation, an artificial token"
D19-5221,W17-5704,1,0.746739,"domain adaptation on in-domain pivot parallel corpora and then perform multilingual transfer for a language pair of interest. However, instead of utilizing out-ofdomain pivot parallel corpora, we investigate the impact of other in-domain pivot parallel corpora. Pseudo-parallel data can be used to augment existing parallel corpora for training, and previous work has reported that such data generated by so-called back-translation can substantially improve the quality of NMT (Sennrich et al., 2016). However, this approach requires base MT systems that can generate somewhat accurate translations (Imankulova et al., 2017). Therefore, instead of creating noisy pseudo-parallel corpora, we take advantage of other in-domain pivot parallel corpora. 3 3.2 Systems This section describes our system TMU and our baseline which based on the same MultiNMT architecture (Johnson et al., 2017) but trained on different training corpora (Table 1). Here, MultiNMT translates from multiple source languages into different target languages within a single model. To realize such translation, an artificial token is introduced at the beginning of the input sentence to indicate the target language the model should translate to. Since w"
D19-5221,Q17-1024,0,0.241776,"olitan University 6-6 Asahigaoka, Hino, Tokyo 191-0065, Japan {imankulova-aizhan, kaneko-masahiro}@ed.tmu.ac.jp komachi@tmu.ac.jp Abstract transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and multilingual modeling (Firat et al., 2016). Recently, a simple multilingual modeling (MultiNMT) was proposed by Johnson et al. (2017) which translates between multiple languages using a single model and an artificial token indicating a target language, taking advantage of multilingual data to improve NMT for all languages involved. Imankulova et al. (2019) showed that incorporating MultiNMT (Johnson et al., 2017) provided better BLEU scores than unidirectional and pivot-based PBSMT approaches and that domain mismatch had a negative effect on low-resource NMT. Therefore, we use MultiNMT modeling for an extremely low-resource Ja↔Ru translation involving English (En) as the pivoting third language (Utiyama and Isahara, 2007). Considering the importance of domain matching, we focus on only news domain of additional Ja↔En and Ru↔En auxiliary parallel corpora, which we will refer as pivot parallel corpora. And we investigate how translation results are improved by using in-domain pivot parallel corpora (Ja↔"
D19-5221,D16-1163,0,0.0471997,"Missing"
I08-1047,W99-0613,0,0.4512,"Missing"
I08-1047,C92-2082,0,0.14766,"es higher precision than the previously proposed methods. We also show that the proposed method offers an additional advantage for knowledge acquisition in an Asian language for which word segmentation is an issue, as the method utilizes no prior knowledge of word segmentation, and is able to harvest new terms with correct word segmentation. 1 Introduction Extraction of lexical knowledge from a large collection of text data with minimal supervision has become an active area of research in recent years. Automatic extraction of relations by exploiting recurring patterns in text was pioneered by Hearst (1992), who describes a bootstrapping procedure for extracting words in the hyponym (is-a) relation, starting with three manually given lexico-syntactic patterns. This idea of learning with a minimally supervised bootstrapping method using surface text patterns was subsequently adopted for many tasks, including relation extraction (e.g., Brin, 1998; Ri Web search queries capture the interest of search users directly, while the distribution of the Web documents do not necessarily reflect the distribution of what people search (Silverstein et al., 1998). The word categories acquired from query logs a"
I08-1047,P06-1015,0,0.684044,"imally Supervised Learning of Semantic Knowledge from Query Logs Mamoru Komachi Hisami Suzuki Nara Institute of Science and Technology 8916-5 Takayama Ikoma, Nara 630-0192, Japan Microsoft Research One Microsoft Way Redmond, WA 98052 USA mamoru-k@is.naist.jp hisamis@microsoft.com loff and Jones, 1999; Pantel and Pennacchiotti, 2006) and named entity recognition (e.g., Collins and Singer, 1999; Etzioni et al., 2005). In this paper, we describe a method of learning semantic categories of words using a large collection of Japanese search query logs. Our method is based on the Espresso algorithm (Pantel and Pennacchiotti, 2006) for extracting binary lexical relations, adapting it to work well on learning unary relations from query logs. The use of query data as a source of knowledge extraction offers some unique advantages over using regular text. Abstract We propose a method for learning semantic categories of words with minimal supervision from web search query logs. Our method is based on the Espresso algorithm (Pantel and Pennacchiotti, 2006) for extracting binary lexical relations, but makes important modifications to handle query log data for the task of acquiring semantic categories. We present experimental r"
I08-1047,W02-1028,0,0.506795,"nary, and can therefore use several hundred seed terms, they simply used the top-k highestscoring contexts and extracted new named entities once and for all, without iteration. Generic patterns receive low scores, and are therefore ignored by this algorithm. 1 # indicates where the instance occurs in the query string, and + indicates a white space in the original Japanese query. The underscore symbol (_) means there was originally no white space; it is used merely to make the translation in English more readable. 2 The manual classification assigns only one category 2.3 The Basilisk Algorithm Thelen and Riloff (2002) present a framework called Basilisk, which extracts semantic lexicons 359 for multiple categories. It starts with a small set of seed words and finds all patterns that match these seed words in the corpus. The bootstrapping process begins by selecting a subset of the patterns by the RlogF metric (Riloff, 1996): F R log F ( patterni )  i  log( Fi ) Ni where Fi is the number of category members extracted by patterni and Ni is the total number of instances extracted by patterni. It then identifies instances by these patterns and scores each instance by the following formula: Pi AvgLog(wordi )"
I08-1047,N04-1041,0,0.292438,"teration. 2.4 each input instance i in the set of instances I, weighted by the reliability of each instance i: r ( p)  I where rι(i) is the reliability of the instance i and maxpmi is the maximum PMI between all patterns and all instances. The PMI between instance i = {x,y} and pattern p is estimated by: x, p, y pmi(i, p)  log x,*, y *, p,* where x, p, y is the frequency of pattern p instantiated with terms x and y (recall that Espresso is targeted at extracting binary relations) and where the asterisk represents a wildcard. They multiplied pmi(i,p) with the discounting factor suggested in Pantel and Ravichandran (2004) to alleviate a bias towards infrequent events. The reliability of an instance is defined similarly: a reliable instance is one that associates with as many reliable patterns as possible. r (i)  The Espresso Algorithm We will discuss the Espresso framework (Pantel and Pennacchiotti, 2006) in some detail because our method is based on it. It is a general-purpose, minimally supervised bootstrapping algorithm that takes as input a few seed instances and iteratively learns surface patterns to extract more instances. The key to Espresso lies in its use of generic patterns: Pantel and Pennacchiott"
I11-1017,P06-1132,0,0.118427,"learned from a monolingual corpus of the language to be learned. Once we obtain a manuallycorrected corpus of language learners, it is possible to translate erroneous sentences into correct sentences using SMT. The use of SMT for spelling and grammar correction has the following three advantages. (1) It does not require expert knowledge. (2) It is straightforward to apply SMT tools to this task. (3) Error correction using SMT can benefit from the improvement of SMT method. Related work on error correction using phrasebased SMT includes research on English and Japanese (Brockett et al., 2006; Suzuki and Toutanova, 2006). Brockett et al. (2006) proposed to correct mass noun errors using SMT and used 45,000 sentences as training sets randomly extracted from automatically created 346,000 sentences. Our work differs from them in that we (1) do not restrict ourselves to a specific error type such as mass noun; and (2) exploit a large-scale real world data set. Suzuki and Toutanova (2006) proposed a machine learning-based method to preError Correction Using SMT eˆ = arg max P(e |f ) = arg max P(e)P( f |e) 銭湯に行った。 いつ行ったかがある方がいい (1) where e represents target sentences and f represents source sentences. P(e) is the p"
I11-1017,P01-1008,0,0.0146404,"Missing"
I11-1017,C10-2157,0,0.049317,"apanese language learners around the world has increased more than 30-fold in the past three decades. The Japan Foundation reports that more than 3.65 million people in 133 countries and regions are studying Japanese in 2009 1 . However, there are only 50,000 Japanese language teachers overseas, and thus it is in high demand to find good instructors for writers of Japanese as a Second Language (JSL). Recently, natural language processing research has begun to pay attention to second language learning (Rozovskaya and Roth, 2011; Park and Levy, 2011; Liu et al., 2011; Oyama and Matsumoto, 2010; Xue and Hwa, 2010). However, most previous research for second language learning deals with restricted types of learners’ errors. For example, research for JSL learners’ 1 http://www.jpf.go.jp/e/japanese/ survey/result/index.html 147 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 147–155, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP feedbacks from other users of the native language. However, they are not able to write about arbitrary topics. Third, Lang-8 is a “Multi-lingual language learning and language exchange Social Networking Service” 4 , which has"
I11-1017,P06-1032,0,0.745415,"at the character-wise model outperforms the word-wise model. 1 Yuji Matsumoto NAIST, Japan 何で日本語はこんなに難しい な の？ (Why does Japanese are so difficult?) which has a grammatical error of inserting ‘な’ due to literal translation from Chinese. Park and Levy (2011) proposed an EM-based unsupervised approach to perform whole sentence grammar correction, but the types of errors must be predetermined to learn the parameters for their noisy channel model. It requires expert knowledge of L2 teaching, which is often hard to obtain. One promising approach for correcting unrestricted errors of JSL learners is Brockett et al. (2006)’s automated error correction method using statistical machine translation (SMT). The advantage of their method is that it does not require expert knowledge. Instead, it learns a correction model from sentence-aligned corrected learners’ corpora. However, it is not easy to acquire largescale learners’ corpora. In fact, Brockett et al. (2006) used regular expressions to automatically create erroneous corpora from native corpora. To solve the knowledge acquisition bottleneck, we propose a method of mining revision logs to create a large-scale learners’ corpus. The corpus is compiled from error r"
I11-1017,J93-2003,0,0.0463045,"is that annotators may not correct all the errors in a sentence. Table 5 shows an example of JSL learner’s sentence for confusing case markers of “が” (NOM) and “は” (TOP). In this example, “は” and “が” should be corrected to “が” and “は”, respectively. However, the annotator left the second case markers “は” unchanged. Because the number of these cases seems low, we regard it as safe to ignore this issue for creating the corpus. 3 In this study, we attempt to solve the problem of JSL learners’ error correction using the SMT technique. The well-known SMT formulation using the noisy channel model (Brown et al., 1993) is: e e (I went to a public bath. It is better to say when you went.) from sentence-aligned parallel corpus while LM is learned from target language corpus. To adapt SMT to error correction, f can be regarded as the sentences written by Japanese learners, whereas e represents the manually-corrected Japanese sentences. TM can be learned from the sentence-aligned learners’ corpus. LM can be learned from a monolingual corpus of the language to be learned. Once we obtain a manuallycorrected corpus of language learners, it is possible to translate erroneous sentences into correct sentences using S"
I11-1017,P03-1021,0,0.0408272,"the effect of corpus size; (3) the difference of L1 model. We used Moses 2.1 14 as a decoder and GIZA++ 1.0.5 15 as an alignment tool. We used Japanese morphological analyzer MeCab 0.97 with UniDic 1.3.12 16 for word segmentation. We created a word-wise model as baseline. Hereafter, we refer to this as W and also constructed model with entries from UniDic for better alignment, denoted as W+Dic. We used word trigram as LM for W and W+Dic. We built two character-wise models: character 3-gram and 5-gram represented as C3 and C5, respectively. We also conducted minimum error rate training (MERT) (Och, 2003) in all experiments 17 . でもじょずじゃりません The correct counterpart would be: でもじょう ずじゃあ りません (But I am not good at it.) The corrected sentence has “う” and “あ” inserted 12 . These sentences written by a learner and corrected by a native speaker are tokenized as follows by MeCab 13 , which is one of the most popular Japanese Morphological Analyzer: でも じ ょずじゃりません ( but (fragment) (garbled word) ) でも じょうず じゃ あり ません ( but good at be 4.1 Experimental Data not ) All the data was created from 849,894 Japanese sentences extracted from revision logs of Lang8 crawled in December 2010. To see the difference of"
I11-1017,P02-1040,0,0.0983684,"granularity of tokenization Table 6 illustrates the performance with different 18 The W+Dic 0.9083 0.9210 0.9146 0.8101 methods (Training Corpus: L1 = ALL; Test Corpus: L1 = English; TM: 0.3M sentences; LM: 1M sentence). The character-wise models outperform the word-wise model in both recall and precision. C5 achieved the best precision, F and BLEU, while C3 obtained the best recall. As evaluation metrics, we use automatic evaluation criteria. To be precise, we used recall (R) and precision (P) based on longest common subsequence (LCS) (Mori et al., 1999; Aho, 1990) and character-based BLEU (Papineni et al., 2002). Park and Levy (2011) adopted character-based BLEU for automatic assessment of ESL errors. We followed their use of BLEU in the error correction task of JSL learners. Since we perform minimum error rate training using BLEU we can directly compare each model’s performance. Recall and precision based on LCS are defined as follows: Recall = W 0.9043 0.9175 0.9109 0.8072 19 Note that LM was trained from the whole training corpus. We did not change L1 for LM. pronunciation of “わ” is the same as “は”. 153 Table 7: Comparison of the performance (recall, precision, F, BLEU) of error correction trained"
I11-1017,P11-1094,0,0.0783935,"ructors. We also demonstrate that the extracted learners’ corpus of Japanese as a second language can be used as training data for learners’ error correction using an SMT approach. We evaluate different granularities of tokenization to alleviate the problem of word segmentation errors caused by erroneous input from language learners. Experimental results show that the character-wise model outperforms the word-wise model. 1 Yuji Matsumoto NAIST, Japan 何で日本語はこんなに難しい な の？ (Why does Japanese are so difficult?) which has a grammatical error of inserting ‘な’ due to literal translation from Chinese. Park and Levy (2011) proposed an EM-based unsupervised approach to perform whole sentence grammar correction, but the types of errors must be predetermined to learn the parameters for their noisy channel model. It requires expert knowledge of L2 teaching, which is often hard to obtain. One promising approach for correcting unrestricted errors of JSL learners is Brockett et al. (2006)’s automated error correction method using statistical machine translation (SMT). The advantage of their method is that it does not require expert knowledge. Instead, it learns a correction model from sentence-aligned corrected learne"
I11-1017,P11-1093,0,\N,Missing
I11-1023,P08-1090,0,0.0179781,"ffect of these features explicitly in their systems. Therefore we also investigate these in this paper. Suppose we want to identify the argument of 自首 した (surrendered) in Example (1). The argument is an antecedent of zero-pronoun φ of the predicate. police arrested I had surrendered that heard 私は (φ が) 自首した と 聞いた． I heard that φ had surrendered. 2.3 Similarity between an Argument Position and a co-Reference Chain In the study of implicit arguments1 for English nominal predicates, Gerber and Chai (2010) used similarity features between an argument position and a co-reference chain, inspired by Chambers and Jurafsky (2008), who proposed unsupervised learning of narrative event chains using pointwise mutual information (PMI) between syntactic positions. This method stands on the assumption 1 wa-particle hanako wo-particle (1) 警察 は 花子 を 逮捕した． Police arrested Hanako. With Salient Reference List for “自首する (surrendered)”, the rank of “警察 (police)” is higher than that of “花子 (Hanako)” and it is noisy information for analysis. We also cannot distinguish them with argument frequency information, because frequencies of both “花子 (Hanako)” and “ 警察 (police)” are 1. Though it is reasonable to use the similarity between an"
I11-1023,kawahara-kurohashi-2006-case,0,0.0274238,"Missing"
I11-1023,N09-1018,0,0.0516117,"Missing"
I11-1023,P10-1160,0,0.160346,"t shows if each candidate is ever used as an argument of predicates or not. However, they did not investigate the effect of these features explicitly in their systems. Therefore we also investigate these in this paper. Suppose we want to identify the argument of 自首 した (surrendered) in Example (1). The argument is an antecedent of zero-pronoun φ of the predicate. police arrested I had surrendered that heard 私は (φ が) 自首した と 聞いた． I heard that φ had surrendered. 2.3 Similarity between an Argument Position and a co-Reference Chain In the study of implicit arguments1 for English nominal predicates, Gerber and Chai (2010) used similarity features between an argument position and a co-reference chain, inspired by Chambers and Jurafsky (2008), who proposed unsupervised learning of narrative event chains using pointwise mutual information (PMI) between syntactic positions. This method stands on the assumption 1 wa-particle hanako wo-particle (1) 警察 は 花子 を 逮捕した． Police arrested Hanako. With Salient Reference List for “自首する (surrendered)”, the rank of “警察 (police)” is higher than that of “花子 (Hanako)” and it is noisy information for analysis. We also cannot distinguish them with argument frequency information, beca"
I11-1023,2002.tmi-papers.15,0,0.151366,"resolver. They compute PMI as follows. Suppose the resulting data has N co-referential pairs of argument positions and M of these pairs comprising Ea = hPa , Aa i, Eb = hPb , Ab i, and Ec = hPc , Ac i. Pa , Pb , and Pc are predicates, and Aa , Ab , and Ac are labels such as ARG0 or ARG1 . G(Ea , Eb ) pmi(Ea , Eb ) = log G(Ea , ∗)G(Eb , ∗) M G(Ea , Eb ) = N With this similarity between argument positions, they defined scores between an argument position and a co-reference chain. Related Work Capturing Discourse Context 2.1 Salient Reference List Iida et al. (2003) used Salient Reference List (Nariyama, 2002) based on Centering Theory (Grosz et al., 1995), which explains the structure of discourse and the transition of topics in order to capture discourse context. The list has the following four ordered slots. TOPIC (marked by wa-particle) &gt; SUBJECT (ga) &gt; INDIRECT OBJECT (ni) &gt; DIRECT OBJECT (wo), We check whether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidates in the list as a feature. 3 Predicate Argu"
I11-1023,J95-2003,0,0.426459,"pose the resulting data has N co-referential pairs of argument positions and M of these pairs comprising Ea = hPa , Aa i, Eb = hPb , Ab i, and Ec = hPc , Ac i. Pa , Pb , and Pc are predicates, and Aa , Ab , and Ac are labels such as ARG0 or ARG1 . G(Ea , Eb ) pmi(Ea , Eb ) = log G(Ea , ∗)G(Eb , ∗) M G(Ea , Eb ) = N With this similarity between argument positions, they defined scores between an argument position and a co-reference chain. Related Work Capturing Discourse Context 2.1 Salient Reference List Iida et al. (2003) used Salient Reference List (Nariyama, 2002) based on Centering Theory (Grosz et al., 1995), which explains the structure of discourse and the transition of topics in order to capture discourse context. The list has the following four ordered slots. TOPIC (marked by wa-particle) &gt; SUBJECT (ga) &gt; INDIRECT OBJECT (ni) &gt; DIRECT OBJECT (wo), We check whether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidates in the list as a feature. 3 Predicate Argument Structure Analysis Exploiting Argument Pos"
I11-1023,N06-1007,0,0.0198555,"s of each argument position (Sorted by frequency) chain, the similarity measure described in Section 2.3 has two problems. One is the strong dependency on the accuracy of co-reference resolver system. In fact, the accuracy of Japanese co-reference resolvers is not accurate enough to create co-reference chains in good quality.2 The other problem is the problem that it needs a lot of documents, because the method does not use any non co-referring nouns. To avoid using an unreliable co-reference resolver, we can suppose the same noun lemmas without pronouns in the same document are coreferences. Pekar (2006) called the noun lemmas anchors and they supposed the similarity measure between syntactic positions. For example, there are two anchors: “Mary” and “house” in the sentences “Mary bought a house. The house belongs to Mary.” They extract two groups: { buy(obj:X), belong(subj:X) } and {buy(subj:X), belong(to:X). } Nevertheless, this method also requires many documents because noun lemmas without anchors are not used for the calculation. In this paper, we propose a more robust similarity measure between argument positions which does not depend on unreliable co-reference annotations by the resolve"
I11-1023,W03-2604,1,0.920871,"references between arguments using a coreference resolver. They compute PMI as follows. Suppose the resulting data has N co-referential pairs of argument positions and M of these pairs comprising Ea = hPa , Aa i, Eb = hPb , Ab i, and Ec = hPc , Ac i. Pa , Pb , and Pc are predicates, and Aa , Ab , and Ac are labels such as ARG0 or ARG1 . G(Ea , Eb ) pmi(Ea , Eb ) = log G(Ea , ∗)G(Eb , ∗) M G(Ea , Eb ) = N With this similarity between argument positions, they defined scores between an argument position and a co-reference chain. Related Work Capturing Discourse Context 2.1 Salient Reference List Iida et al. (2003) used Salient Reference List (Nariyama, 2002) based on Centering Theory (Grosz et al., 1995), which explains the structure of discourse and the transition of topics in order to capture discourse context. The list has the following four ordered slots. TOPIC (marked by wa-particle) &gt; SUBJECT (ga) &gt; INDIRECT OBJECT (ni) &gt; DIRECT OBJECT (wo), We check whether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidat"
I11-1023,D08-1055,0,0.145069,"tion’ step in this model selects the most likely argument from all noun phrases preceding the predicate. 4.5 Targets for Comparison of Predicate Argument Analysis Model We evaluate our selection-and-classification approach by comparing our baseline model with two previous approaches TA and IM. 5 Discussion Table 7 presents the result of the experiments. According to the bottom row in Table 7, we achieved the state-of-the-art of Japanese predicate argument structure analysis by combining all discourse context features (+A+B+C+D+E). We investigate our result from five different standpoints. TA: Taira et al. (2008) used decision lists where features were sorted by their weights learned from Support Vector Machine. They simultaneously solved the argument of event nouns in the same lists. IM: Imamura et al. (2009) used discriminative models based on maximum entropy. They added the special noun phrase NULL, which expresses that the predicate does not have any argument. 5.1 Effect of the Selection-and-Classification Approach Because previous work use different features and machine learning methods and experiment on different setting from ours, we also compare with a baseline model BL in order to analyze the"
I11-1023,W07-1522,1,0.315603,"Kawahara and Kurohashi (2006) collected from the web. They are part-of-speech tagged with JUMAN7 and dependency structure parsed with KNP8 . We extracted 1,101,472,855 pairs of a predicate and an argument.9 Table 5: Discourse context features used in the experiment we generate two training examples: One is an example of (b) with the label INTER, “花子”, and the most likely argument selected by (a) at ‘Classification’ step. The other one is an example of (c) with the label HAVE-ARG and “花子”. 4 wo with the case maker が, を and に. 4.2 Training and Evaluation Dataset We used NAIST Text Corpus 1.4β (Iida et al., 2007) for training and evaluation. It is based on Kyoto Text Corpus 3.010 and annotated with predicate-argument structure, event noun structure, and co-reference of nouns about 40,000 sentences of Japanese newspaper text. We excluded 11 articles due to annotation error. We conducted five-fold cross-validation. In the experiments, base phrases and dependency relations are acquired from the Kyoto Text Corpus 3.0 in the same way of related work. Evaluation Setting of Predicate Argument Structure Analysis Exploiting Argument Position and Type We evaluate our proposed selection-andclassification approac"
I11-1023,P09-2022,0,0.659184,"hether each candidate corresponds to any slots from the beginning of a document. If the candidate corresponds to a slot, we (over)write the slot with the candidate. We repeat this until we reach the predicate to analyze. We use the ranks of candidates in the list as a feature. 3 Predicate Argument Structure Analysis Exploiting Argument Position and Type 2.2 Argument Frequency 3.1 Similarity between Argument Positions using Distribution Similarity Iida et al. (2003) used a feature (CHAIN LENGTH) that stands for how often each candidate is used as an argument of predicates in preceding context. Imamura et al. (2009) used a similar binary feature (USED) that shows if each candidate is ever used as an argument of predicates or not. However, they did not investigate the effect of these features explicitly in their systems. Therefore we also investigate these in this paper. Suppose we want to identify the argument of 自首 した (surrendered) in Example (1). The argument is an antecedent of zero-pronoun φ of the predicate. police arrested I had surrendered that heard 私は (φ が) 自首した と 聞いた． I heard that φ had surrendered. 2.3 Similarity between an Argument Position and a co-Reference Chain In the study of implicit ar"
I11-1033,P93-1024,0,0.0472564,"e for classification of unmarked characters. Therefore, we carry out clustering on Kanji characters and add character class n-gramin feature sets. For example, if “深” and “寒” (cold) belong to the same class X, and “寒” appears in training corpus as in a phrase “寒ければ” (if it is cold), then features corresponding to a phrase “X けれは” (if it is X) will be learned from “寒ければ.” As a result, we will be able to exploit “深” as evidence of detecting “は” in “深けれは” as unmarked character. Clustering was performed on Kanji characters with the subsequent and the previous two characters individually based on (Pereira et al. 1993). A Kanji character that appears left of the target character is replaced with the class of the formerclusters and that appears right is replaced with the class of the latter-clusters.  3/C 3/0  First, we built a naive generative model as baseline for labeling voiced consonant mark. This method labels voiced consonant marks that maximize the likelihood of a sentence by using a character 3-gram model. One deficiency of the baseline method is that it requires a fully annotated corpus with the marks. Second, for the dictionary-based approach, we created a dictionary and corpus from the same tra"
I11-1033,neubig-mori-2010-word,0,0.0317903,"ed (+1) or not (-1). Since proposed method does not require a corpus annotated with word boundaries or part-of-speech tags for learning, we take advantage of a large modern a Japanese corpus, Taiyo-Corpus,4 which is based on Japanese magazines from the Meiji Era. This corpus is not annotated with neither word boundaries nor partof-speech tags but is manually annotated with unmarked characters. We employed pointwise prediction which makes a single independent decision at each point: ambiguous Hiragana character or Kunoji-ten5 .6 Therefore, our method can learn from partially annotated corpora (Neubig and Mori, 2010) including raw corpora of modern Japanese literary text, and thus it is easy to obtain training data. Neubig et al. (2011) extend the word segmentation method proposed by Sassano (2002) to Japanese morphological analysis using pointwise prediction. In our method, we adopt the binary features from (Sassano, 2002) to this task. Unlike Sassano and Neubig et al. who use an SVM, we use an online Passive-Aggressive algorithm for 3.1.2 Character type n-grams These features are similar to previously mentioned character n-grams with only the modification of replacing the character itself with the chara"
I11-1033,P11-2093,0,0.02105,"or learning, we take advantage of a large modern a Japanese corpus, Taiyo-Corpus,4 which is based on Japanese magazines from the Meiji Era. This corpus is not annotated with neither word boundaries nor partof-speech tags but is manually annotated with unmarked characters. We employed pointwise prediction which makes a single independent decision at each point: ambiguous Hiragana character or Kunoji-ten5 .6 Therefore, our method can learn from partially annotated corpora (Neubig and Mori, 2010) including raw corpora of modern Japanese literary text, and thus it is easy to obtain training data. Neubig et al. (2011) extend the word segmentation method proposed by Sassano (2002) to Japanese morphological analysis using pointwise prediction. In our method, we adopt the binary features from (Sassano, 2002) to this task. Unlike Sassano and Neubig et al. who use an SVM, we use an online Passive-Aggressive algorithm for 3.1.2 Character type n-grams These features are similar to previously mentioned character n-grams with only the modification of replacing the character itself with the character type. We deal with eleven character types, Hiragana/H, Katakana/K, Kanji/C, Odoriji/O, Latin/L, Digit/D, dash/d, stop"
I11-1033,P02-1064,0,0.0250315,"aiyo-Corpus,4 which is based on Japanese magazines from the Meiji Era. This corpus is not annotated with neither word boundaries nor partof-speech tags but is manually annotated with unmarked characters. We employed pointwise prediction which makes a single independent decision at each point: ambiguous Hiragana character or Kunoji-ten5 .6 Therefore, our method can learn from partially annotated corpora (Neubig and Mori, 2010) including raw corpora of modern Japanese literary text, and thus it is easy to obtain training data. Neubig et al. (2011) extend the word segmentation method proposed by Sassano (2002) to Japanese morphological analysis using pointwise prediction. In our method, we adopt the binary features from (Sassano, 2002) to this task. Unlike Sassano and Neubig et al. who use an SVM, we use an online Passive-Aggressive algorithm for 3.1.2 Character type n-grams These features are similar to previously mentioned character n-grams with only the modification of replacing the character itself with the character type. We deal with eleven character types, Hiragana/H, Katakana/K, Kanji/C, Odoriji/O, Latin/L, Digit/D, dash/d, stop and comma/S, BOS (⟨s⟩)/B, EOS (⟨/s⟩)/E and others/o as the cha"
I11-1033,P98-2152,0,0.0633088,"rked characters is much simpler than error correction of all the Hiragana. Our method differs from Shinnou’s method in that we focus on automatic labeling of voiced consonant marks and emIf we assume an unmarked character as substitution error of one voiced consonant to one voiceless consonant, the task of detecting an unmarked character can be considered as a kind of error correction. In English, we can perform error correction for the one character’s error by word-based approach. However, in Japanese, we cannot simply apply word-based approach because sentences are not segmented into words. Nagata (1998) proposed a statistical method using dynamic programming for selecting the most likely word sequences from candidate word lattice estimated from observed characters in Japanese sentence. In this method, the product of the transition probability of words is used as a word segmentation model. However, most of the historical materials that we deal with are raw text, and there exist little, if any, annotated texts with words 294 ploy a discriminative character n-gram model using a classification-based method. Although Shinnou’s generative model is not capable of using overlapping features, our cla"
I11-1033,C10-1140,0,0.0526366,"Missing"
I11-1046,C10-1058,0,0.125449,"rough logs have been explored in the field of lexical acquisition. A web clickthrough is the process of clicking a URL and going to the page it refers. This ensures that the landing page is appropriate since the web user follows the hyperlink after checking the information displayed, such as ‘title’, ‘URL’, and ‘summary’ of their search. Two distinct queries landing on the same ‘URL’ are possibly input for the same purpose, meaning that they are likely to be related. In the NLP literature, clickthrough logs have been used to learn semantic categories (Komachi et al., 2009) and named entities (Jain and Pennacchiotti, 2010). The main contribution of this work is two fold: A novel reranking method has been developed to refine web search queries. A label propagation algorithm was applied on a clickthrough graph, and the candidates were reranked using a query language model. Our method first enumerates query candidates with common landing pages with regard to the given query to create a clickthrough graph. Second, it calculates the likelihood of the candidates, using a language model generated from web search query logs. Finally, the candidates are sorted by the score calculated from the likelihood and label propag"
I11-1046,C04-1066,0,0.0244885,"gation method based on Komachi et al. (2009) can be used as a relatedness measure that returns a similarity score relative to the seed instance, and thus is suitable for a query correction task. 413 4.2 Normalized PMI In the web search, neologims appear continuously, which make it hard to compute the likelihood of queries by a word n-gram language model. Moreover, characters themselves carry essential semantic information in Chinese and Japanese. Therefore, we build a character language model for the search query logs following observations of the usefulness of character n-grams for Japanese (Asahara and Matsumoto, 2004) and Chinese (Huang and Zhao, 2006). Asahara and Matsumoto used a window of two characters to the right and to the left of the focus character, which results in using character 5-grams. We also used 5-grams for a query language model from the preliminary experiment. Komachi et al. (2009) suggested that the normalized frequency causes semantic drift (Jurafsky and Martin, 2009), and we confirmed this phenomenon in our preliminary experiment. They suggested using relative frequency such as pointwise mutual information (PMI) and log-likelihood ratio as countermeasure against semantic drift. Theref"
I11-1046,D07-1086,0,0.0265732,"odel we use. In Section 6, we evaluate our method in an abbreviation expansion task and show its efficiency. Section 7 offers conclusions and directions for future work. 2 Related Work Query expansion for a web-search query has to handle neologisms and slang on the web. Thus, it is labor-intensive to maintain a list of correctly spelled words for search queries. Additionally, Japanese query expansion includes several tasks, such as word segmentation, word stemming, and acronym expansion. Much of the previous work has focused on each individual task (Ahmad and Kondrak, 2005; Chen et al., 2007; Bergsma and Wang, 2007; Li et al., 2006; Peng et al., 2007; Risvik et al., 2003). Cucerzan and Brill (2004) clarified problems of spelling correction for search queries, addressing them using a noisy channel model with a language model created from query logs. Gao et al. (2010) and Sun et al. (2010) applied a reranking method applying neural net to the search-query spelling correction candidates obtained from the 1 Note that our method can be applied to query expansion as well. 411 URL using the web search logs. We calculate the relatedness between the queries on this graph to select a set of candidates C. Since th"
I11-1046,D08-1106,1,0.849819,"} and a label set L = {1 , ..., c}, the first l instances xi (i &lt; l) are labeled as yi ∈ L. The goal is to predict the labels of the unlabeled instances xu (l + 1 ≤ u ≤ n). However, the seed instance like that in Figure 2 possibly causes a result to be worse in a task of lexical acquisition, due to an ambiguous instance “abc”, which belongs to more than one domain, e.g. “mass media” and “dance”. It is expected that the label propagates to unrelated instances if we have highly frequent ambiguous nodes. This problem is called “semantic drift” and has received a lot of attention in NLP research (Komachi et al., 2008). Let F denote the set of n × c matrices with nonnegative entries. A matrix F = [F1 , . . . , Fn ]T ∈ F corresponds to a classification on the dataset X by labeling each instance xi as a label yi = argmaxj≤c Fij . Define F0 as the initial F with Fij = 1 if xi is labeled as a label yi = j and Fij = 0 otherwise. The (i, j)-th element of the final matrix F represents a similarity to the labeled instances. We use these similarities as P (q|c) in Equation 2, where q is a seed instance, c is a labeled instance by label propagation. Komachi et al. (2008) have reported that bootstrapping algorithms li"
I11-1046,P09-2048,1,0.819789,"egories. More recently, web search clickthrough logs have been explored in the field of lexical acquisition. A web clickthrough is the process of clicking a URL and going to the page it refers. This ensures that the landing page is appropriate since the web user follows the hyperlink after checking the information displayed, such as ‘title’, ‘URL’, and ‘summary’ of their search. Two distinct queries landing on the same ‘URL’ are possibly input for the same purpose, meaning that they are likely to be related. In the NLP literature, clickthrough logs have been used to learn semantic categories (Komachi et al., 2009) and named entities (Jain and Pennacchiotti, 2010). The main contribution of this work is two fold: A novel reranking method has been developed to refine web search queries. A label propagation algorithm was applied on a clickthrough graph, and the candidates were reranked using a query language model. Our method first enumerates query candidates with common landing pages with regard to the given query to create a clickthrough graph. Second, it calculates the likelihood of the candidates, using a language model generated from web search query logs. Finally, the candidates are sorted by the sco"
I11-1046,D07-1019,0,0.0204741,"he query language model we use. In Section 6, we evaluate our method in an abbreviation expansion task and show its efficiency. Section 7 offers conclusions and directions for future work. 2 Related Work Query expansion for a web-search query has to handle neologisms and slang on the web. Thus, it is labor-intensive to maintain a list of correctly spelled words for search queries. Additionally, Japanese query expansion includes several tasks, such as word segmentation, word stemming, and acronym expansion. Much of the previous work has focused on each individual task (Ahmad and Kondrak, 2005; Chen et al., 2007; Bergsma and Wang, 2007; Li et al., 2006; Peng et al., 2007; Risvik et al., 2003). Cucerzan and Brill (2004) clarified problems of spelling correction for search queries, addressing them using a noisy channel model with a language model created from query logs. Gao et al. (2010) and Sun et al. (2010) applied a reranking method applying neural net to the search-query spelling correction candidates obtained from the 1 Note that our method can be applied to query expansion as well. 411 URL using the web search logs. We calculate the relatedness between the queries on this graph to select a set o"
I11-1046,W04-3238,0,0.0269144,"k and show its efficiency. Section 7 offers conclusions and directions for future work. 2 Related Work Query expansion for a web-search query has to handle neologisms and slang on the web. Thus, it is labor-intensive to maintain a list of correctly spelled words for search queries. Additionally, Japanese query expansion includes several tasks, such as word segmentation, word stemming, and acronym expansion. Much of the previous work has focused on each individual task (Ahmad and Kondrak, 2005; Chen et al., 2007; Bergsma and Wang, 2007; Li et al., 2006; Peng et al., 2007; Risvik et al., 2003). Cucerzan and Brill (2004) clarified problems of spelling correction for search queries, addressing them using a noisy channel model with a language model created from query logs. Gao et al. (2010) and Sun et al. (2010) applied a reranking method applying neural net to the search-query spelling correction candidates obtained from the 1 Note that our method can be applied to query expansion as well. 411 URL using the web search logs. We calculate the relatedness between the queries on this graph to select a set of candidates C. Since the label propagation is mathematically identical to the random walk with restart, prob"
I11-1046,P06-1129,0,0.0287333,"6, we evaluate our method in an abbreviation expansion task and show its efficiency. Section 7 offers conclusions and directions for future work. 2 Related Work Query expansion for a web-search query has to handle neologisms and slang on the web. Thus, it is labor-intensive to maintain a list of correctly spelled words for search queries. Additionally, Japanese query expansion includes several tasks, such as word segmentation, word stemming, and acronym expansion. Much of the previous work has focused on each individual task (Ahmad and Kondrak, 2005; Chen et al., 2007; Bergsma and Wang, 2007; Li et al., 2006; Peng et al., 2007; Risvik et al., 2003). Cucerzan and Brill (2004) clarified problems of spelling correction for search queries, addressing them using a noisy channel model with a language model created from query logs. Gao et al. (2010) and Sun et al. (2010) applied a reranking method applying neural net to the search-query spelling correction candidates obtained from the 1 Note that our method can be applied to query expansion as well. 411 URL using the web search logs. We calculate the relatedness between the queries on this graph to select a set of candidates C. Since the label propagati"
I11-1046,C10-1041,0,0.017612,"the web. Thus, it is labor-intensive to maintain a list of correctly spelled words for search queries. Additionally, Japanese query expansion includes several tasks, such as word segmentation, word stemming, and acronym expansion. Much of the previous work has focused on each individual task (Ahmad and Kondrak, 2005; Chen et al., 2007; Bergsma and Wang, 2007; Li et al., 2006; Peng et al., 2007; Risvik et al., 2003). Cucerzan and Brill (2004) clarified problems of spelling correction for search queries, addressing them using a noisy channel model with a language model created from query logs. Gao et al. (2010) and Sun et al. (2010) applied a reranking method applying neural net to the search-query spelling correction candidates obtained from the 1 Note that our method can be applied to query expansion as well. 411 URL using the web search logs. We calculate the relatedness between the queries on this graph to select a set of candidates C. Since the label propagation is mathematically identical to the random walk with restart, probability of the label propagation can be regarded as the conditional probability P (q|c). If we assume that the relatedness score represents the conditional probability of"
I11-1046,N09-1022,0,0.0177766,"china,tmaezawa,toshsato}@yahoo-corp.jp ykobayas@google.com ‡Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara 630-0192, Japan komachi@is.naist.jp Abstract To alleviate this problem, web-search engines often exploit web search query logs to automatically generate a thesaurus. A web search query is a query that a web user types into a web search engine to find information. It is noisy and sometimes ambiguous to detect query intent, but it is a great way to create a fresh web dictionary at low cost. Hence, the web search queries are widely used in the NLP field. For instance, Hagiwara and Suzuki (2009) used them for a query alteration task, and Sekine and Suzuki (2007) leveraged them for acquiring semantic categories. More recently, web search clickthrough logs have been explored in the field of lexical acquisition. A web clickthrough is the process of clicking a URL and going to the page it refers. This ensures that the landing page is appropriate since the web user follows the hyperlink after checking the information displayed, such as ‘title’, ‘URL’, and ‘summary’ of their search. Two distinct queries landing on the same ‘URL’ are possibly input for the same purpose, meaning that they ar"
I11-1046,Y06-1001,0,0.0244391,"9) can be used as a relatedness measure that returns a similarity score relative to the seed instance, and thus is suitable for a query correction task. 413 4.2 Normalized PMI In the web search, neologims appear continuously, which make it hard to compute the likelihood of queries by a word n-gram language model. Moreover, characters themselves carry essential semantic information in Chinese and Japanese. Therefore, we build a character language model for the search query logs following observations of the usefulness of character n-grams for Japanese (Asahara and Matsumoto, 2004) and Chinese (Huang and Zhao, 2006). Asahara and Matsumoto used a window of two characters to the right and to the left of the focus character, which results in using character 5-grams. We also used 5-grams for a query language model from the preliminary experiment. Komachi et al. (2009) suggested that the normalized frequency causes semantic drift (Jurafsky and Martin, 2009), and we confirmed this phenomenon in our preliminary experiment. They suggested using relative frequency such as pointwise mutual information (PMI) and log-likelihood ratio as countermeasure against semantic drift. Therefore, we used pointwise mutual infor"
I11-1046,I08-2127,0,0.030212,"od, our approach takes advantage of an enormous amount of clickthrough logs for learning the query abbreviation model. Query suggestion is another task that uses search logs (Mei et al., 2008; Cao et al., 2008). Query suggestion differs from our task in that it allows queries to be suggested that are different from the one that the search user types. Furthermore, some previous works have addressed acquiring a Japanese abbreviation task. Murayama and Okumura (2008) formulated the process of generating Japanese abbreviations by noisy channel model but they did not handle abbreviation expansion. Okazaki et al. (2008) dealt with recognizing Japanese abbreviation tasks as a binary classification problem. They extracted However, it is difficult to obtain them beforehand, because we have to check query logs to find incorrect queries and make necessary changes to define their corrections. Therefore, in this paper, we focus on query abbreviation and evaluate our proposed approach in an abbreviation expansion task. Abbreviation expansion itself is difficult for many query expansion methods based on edit distance, because the input and output have only a few, if any, characters in common. Our clickthroughlog-base"
I11-1046,P06-1015,0,0.0372106,"e the set of n × c matrices with nonnegative entries. A matrix F = [F1 , . . . , Fn ]T ∈ F corresponds to a classification on the dataset X by labeling each instance xi as a label yi = argmaxj≤c Fij . Define F0 as the initial F with Fij = 1 if xi is labeled as a label yi = j and Fij = 0 otherwise. The (i, j)-th element of the final matrix F represents a similarity to the labeled instances. We use these similarities as P (q|c) in Equation 2, where q is a seed instance, c is a labeled instance by label propagation. Komachi et al. (2008) have reported that bootstrapping algorithms like Espresso (Pantel and Pennacchiotti, 2006) can be viewed as Kleinberg’s HITS algorithm (Kleinberg, 1999) and the “semantic drift” problem on the graph is the same phenomenon as “topic drift” in HITS, which converges to the eigenvector of the instance-instance similarity graph created from instance-pattern cooccurrence graph as described in the next subsection. The instance-instance similarity matrix A in Figure 3 is defined as A = W T W where W is an instance-pattern matrix. The (i, j)-th element of Wij contains the relative frequency of occurrence of instance xi and pattern pj . D is a diagonal degree matrix of A where ∑ the (i, j)-t"
I11-1046,P10-1028,0,0.0148617,"labor-intensive to maintain a list of correctly spelled words for search queries. Additionally, Japanese query expansion includes several tasks, such as word segmentation, word stemming, and acronym expansion. Much of the previous work has focused on each individual task (Ahmad and Kondrak, 2005; Chen et al., 2007; Bergsma and Wang, 2007; Li et al., 2006; Peng et al., 2007; Risvik et al., 2003). Cucerzan and Brill (2004) clarified problems of spelling correction for search queries, addressing them using a noisy channel model with a language model created from query logs. Gao et al. (2010) and Sun et al. (2010) applied a reranking method applying neural net to the search-query spelling correction candidates obtained from the 1 Note that our method can be applied to query expansion as well. 411 URL using the web search logs. We calculate the relatedness between the queries on this graph to select a set of candidates C. Since the label propagation is mathematically identical to the random walk with restart, probability of the label propagation can be regarded as the conditional probability P (q|c). If we assume that the relatedness score represents the conditional probability of the typed query q give"
I11-1046,D09-1055,0,0.0306195,"the range of Wij can be normalized to [0,1]. Additionally, this prevents sparse matrix W from being dense and reduces the noise in the data. 5 2. Alphanumeric characters in a query are unified to one-byte lower-case characters Query Language Model 3. A sequence of white space in a query is unified to single one-byte white space character In this paper, we use a character n-gram language model to obtain the likelihood of the candidates for query expansion in Equation 2. P (c) = = N −1 ∏ i=0 N −1 ∏ i=0 4. All the URLs included in clickthrough logs are unique, i.e., we did not generalize URLs as Tseng et al. (2009) did. P (xi |xi−N +1 , . . . , xi−1 ) f req(xi−N +1 , . . . , xi ) f req(xi−N +1 , . . . , xi−1 ) The Japanese clickthrough logs were collected from October 22 to November 9, 2009 2 and from January 1 to 16 in Yahoo Japan web search logs. (6) 2 A storage device in our experimental environment became full when tallying clickthrough logs. As a result, we were not able to use clickthrough logs of some periods. where consider c is a contiguous sequences of N characters c = {x0 , x1 , . . . , xn−1 } . 414 Links clicked less than 10 times were removed for efficiency reasons. Finally, we obtained 4,4"
I11-1046,J00-4006,0,\N,Missing
I17-1005,D10-1104,0,0.0200659,"2 LSTM, whose embedding layer is initialized with word2vec. We also address unrestricted grammatical error detection; however, we focus on learning word embeddings that consider a learner’s error pattern and grammaticality of the target word. In this paper, subsequently, our word embeddings give statistically significant improvements over their method using exactly the same training data. Several studies considering grammatical error patterns in language learning have been performed. For example, Sawai et al. (2013) suggest correction candidates for verbs using the learner error pattern, and Liu et al. (2010) automatically correct verb selection errors in English essays written by Chinese students learning English, based on the error patterns created from a synonym dictionary and an English-Chinese bilingual dictionary. The main difference between these previous studies and ours is that the previous studies focused only on verb selection errors. As an example of research on learning word embeddings that consider grammaticality, Alikaniotis et al. (2016) proposed a model for constructing word embeddings by considering the importance of each word in predicting a quality score for an English learner’"
I17-1005,I11-1017,1,0.871268,"82 English learner corpus. The results demonstrated that representation learning is crucial for exploiting a noisy learner corpus for grammatical error detection. The main contributions of this study are summarized as follows: • We achieve the state-of-the-art accuracy in grammatical error detection on the First Certificate in English dataset (FCE-public) using a Bi-LSTM model initialized using our word embeddings that consider grammaticality and error patterns extracted from the FCE-public corpora. • We demonstrate that updating word embeddings using error patterns extracted from the Lang-8 (Mizumoto et al., 2011) in addition to FCE-public corpora greatly improves grammatical error detection. • The proposed word embeddings can distinguish between correct and incorrect phrase pairs. • We have released our code and learned word embeddings3 . The rest of this paper is organized as follows: in Section 2, we first give a brief overview of English grammatical error detection; Section 3 describes our grammatical error detection model using error- and grammaticality-specific word embeddings; Section 4 evaluates this model on the FCE-public dataset, and Section 5 presents an analysis of the grammatical error de"
I17-1005,W14-1701,0,0.0299752,"tokens of the replacement candidates is 272,561, and the number of types is 61,950. Our experiments on FCE+EWE-L8 and FCE+E&GWE-L8 were conducted by combining error patterns from all of Lang-8 corpus and the training part of FCE-public corpus to train word embeddings. However, since the number of error patterns of Lang-8 is larger than that of FCE-public, we normalized each frequency so that the ratio was 1:1. We use F0.5 as the main evaluation measure, following a previous study This mea(Rei and Yannakoudakis, 2016). sure was also adopted in the CoNLL-14 shared task on error correction task (Ng et al., 2014). It combines both precision and recall, while assigning twice as much weight to precision because accurate feedback is often more important than coverage in error detection applications (Nagata and Nakatani, 2010). Nagata and Nakatani (2010) presented a precision-oriented error detection system for articles and numbers that demonstrated precision of 0.72 and a recall of 0.25 and achieved a learning effect that is comparable to that of a human tutor. with the initial learning rate of 0.001. GWE is initialized randomly and EWE is initialized using pre-trained word2vec. 4.3 Classifier We use EWE"
I17-1005,C10-2103,0,0.100252,"the training part of FCE-public corpus to train word embeddings. However, since the number of error patterns of Lang-8 is larger than that of FCE-public, we normalized each frequency so that the ratio was 1:1. We use F0.5 as the main evaluation measure, following a previous study This mea(Rei and Yannakoudakis, 2016). sure was also adopted in the CoNLL-14 shared task on error correction task (Ng et al., 2014). It combines both precision and recall, while assigning twice as much weight to precision because accurate feedback is often more important than coverage in error detection applications (Nagata and Nakatani, 2010). Nagata and Nakatani (2010) presented a precision-oriented error detection system for articles and numbers that demonstrated precision of 0.72 and a recall of 0.25 and achieved a learning effect that is comparable to that of a human tutor. with the initial learning rate of 0.001. GWE is initialized randomly and EWE is initialized using pre-trained word2vec. 4.3 Classifier We use EWE, GWE, and E&GWE word embeddings to initialize the Bi-LSTM neural network, and predict the correctness of the target word in the input sentence. We update initialized weights of embedding layer while training class"
I17-1005,P16-1068,0,0.141442,"ns in language learning have been performed. For example, Sawai et al. (2013) suggest correction candidates for verbs using the learner error pattern, and Liu et al. (2010) automatically correct verb selection errors in English essays written by Chinese students learning English, based on the error patterns created from a synonym dictionary and an English-Chinese bilingual dictionary. The main difference between these previous studies and ours is that the previous studies focused only on verb selection errors. As an example of research on learning word embeddings that consider grammaticality, Alikaniotis et al. (2016) proposed a model for constructing word embeddings by considering the importance of each word in predicting a quality score for an English learner’s essay. Their approach learns word embedding from a document-level score using the mean square error whereas we learn word embeddings from a word-level binary error information using the hinge loss. The use of a large-scale learner corpus on grammatical error correction is described in works such as Xie et al. (2016) and Chollampatt et al. (2016a,b). These studies used the Lang-8 corpus as training data for phrase-based machine translation (Xie et"
I17-1005,P16-1112,0,0.11066,"ed by our word embeddings achieved the state-of-the-art accuracy by a large margin in an English grammatical error detection task on the First Certificate in English dataset. 1 W2V 0.84 0.84 0.40 0.85 C&W 0.75 0.77 0.46 0.71 EWE 0.64 0.90 0.36 0.82 GWE 0.58 0.80 0.25 0.76 E&GWE 0.54 0.88 0.30 0.80 Table 1: Cosine similarity of phrase pairs for each word embedding method. (Bi-LSTM) neural network. Their approach uses word embeddings learned from a large-scale native corpus to address the data sparseness problem of learner corpora. However, most of the word embeddings, including the one used by Rei and Yannakoudakis (2016), model only the context of the words from a raw corpus written by native speakers, and do not consider specific grammatical errors of language learners. This leads to the problem wherein the word embeddings of correct and incorrect expressions tend to be similar (Table 1, columns W2V and C&W) so that the classifier must decide grammaticality of a word from contextual information with a similar input vector. To address this problem, we introduce two methods: 1) error-specific word embeddings (EWE), which employ grammatical error patterns, that is to say the word pairs that learners tend to eas"
I17-1005,P13-2124,1,0.858001,"rammatical error detection model and learned word embeddings; and Section 6 concludes this paper. 2 LSTM, whose embedding layer is initialized with word2vec. We also address unrestricted grammatical error detection; however, we focus on learning word embeddings that consider a learner’s error pattern and grammaticality of the target word. In this paper, subsequently, our word embeddings give statistically significant improvements over their method using exactly the same training data. Several studies considering grammatical error patterns in language learning have been performed. For example, Sawai et al. (2013) suggest correction candidates for verbs using the learner error pattern, and Liu et al. (2010) automatically correct verb selection errors in English essays written by Chinese students learning English, based on the error patterns created from a synonym dictionary and an English-Chinese bilingual dictionary. The main difference between these previous studies and ours is that the previous studies focused only on verb selection errors. As an example of research on learning word embeddings that consider grammaticality, Alikaniotis et al. (2016) proposed a model for constructing word embeddings b"
I17-1005,D16-1195,0,0.0291219,"Missing"
I17-1005,C08-1109,0,0.10998,"se studies used the Lang-8 corpus as training data for phrase-based machine translation (Xie et al., 2016) and neural network joint models (Chollampatt et al., 2016a,b). In our study, Lang-8 was used to extract error patterns that were then utilized to learn word embeddings. Our experiments show that Lang-8 cannot be used as a reliable annotation for LSTM-based classifiers. Instead, we need to extract useful information as error patterns to improve the performance of error detection. Related Works Many studies on grammatical error detection try to address specific types of grammatical errors (Tetreault and Chodorow, 2008; Han et al., 2006; Kochmar and Briscoe, 2014). In contrast, Rei and Yannakoudakis (2016) target all errors using a Bi3 Grammatical Error Detection Using Error- and Grammaticality-Specific Word Embeddings 1 The similarity of the phrase pairs was calculated based on the similarity of the mean vector of the word vectors. 2 http://lang-8.com/ 3 https://github.com/kanekomasahiro/grammatical-errordetection In this section, we describe the details of the proposed word embeddings: EWE, GWE, and E&GWE. These models extend an existing word 41 Figure 1: Architecture of our learning methods for word embe"
I17-1005,P11-1019,0,0.130972,"ngs to distinguish between correct words and incorrect words by including grammaticality in distributed representations (Figure 1b). For that purpose, we add an additional output layer to predict grammaticality of word sequences, and extend Equation (3) to calculate following two error funcit = σ(Wie et + Wih ht−1 + Wic ct−1 + bi ) ft = σ(Wf e et + Wf h ht−1 + Wf c ct−1 + bf ) 43 (9) (10) 4 Experiments 4.1 Settings We used the FCE-public dataset and the Lang8 English learner corpus to train classifiers and word embeddings. For this evaluation, we used the test set from the FCE-public dataset (Yannakoudakis et al., 2011) for all experiments. FCE-public dataset. First, we compared the proposed methods (EWE, GWE, and E&GWE) to previous methods (W2V and C&W) relative to training word embeddings (see Table 2a). For this purpose, we trained our word embeddings and a classifier, which were initialized using pre-trained word embeddings, with the training set from the FCE-public dataset. This dataset is one of the most famous English learner corpus in grammatical error correction. It contains essays written by English learners. It is annotated with grammatical errors along with error classification. We followed the o"
I17-1005,C14-1164,0,0.0173209,"ta for phrase-based machine translation (Xie et al., 2016) and neural network joint models (Chollampatt et al., 2016a,b). In our study, Lang-8 was used to extract error patterns that were then utilized to learn word embeddings. Our experiments show that Lang-8 cannot be used as a reliable annotation for LSTM-based classifiers. Instead, we need to extract useful information as error patterns to improve the performance of error detection. Related Works Many studies on grammatical error detection try to address specific types of grammatical errors (Tetreault and Chodorow, 2008; Han et al., 2006; Kochmar and Briscoe, 2014). In contrast, Rei and Yannakoudakis (2016) target all errors using a Bi3 Grammatical Error Detection Using Error- and Grammaticality-Specific Word Embeddings 1 The similarity of the phrase pairs was calculated based on the similarity of the mean vector of the word vectors. 2 http://lang-8.com/ 3 https://github.com/kanekomasahiro/grammatical-errordetection In this section, we describe the details of the proposed word embeddings: EWE, GWE, and E&GWE. These models extend an existing word 41 Figure 1: Architecture of our learning methods for word embeddings (a) EWE and (b) GWE. Both models concat"
I17-1009,ganitkevitch-callison-burch-2014-multilingual,0,0.0552687,". (2011) reranked paraphrase pairs acquired via bilingual pivoting using distributional similarity. The main idea of reranking paraphrase pairs using information from a monolingual corpus is similar to ours, but Chan et al.’s method failed to acquire semantically similar paraphrases. We succeeded in acquiring semantically similar paraphrases because we effectively combined information from a bilingual corpus and a monolingual corpus by using weighted PMI. In addition to English, paraphrase databases are constructed in many languages using bilingual pivoting (Bannard and Callison-Burch, 2005). Ganitkevitch and Callison-Burch (2014) constructed paraphrase databases8 in 23 languages, including European languages and Chinese. 7 Conclusion We proposed a new approach for formalizing lexical paraphrasability based on weighted PMI and acquired paraphrase pairs using information from both a bilingual corpus and a monolingual corpus. Our proposed method, MIPA, uses bilingual pivoting weighted by distributional similarity to acquire paraphrase pairs robustly, as each of the methods complements the other. Experimental results using manually annotated datasets for lexical paraphrase showed that the proposed method outperformed bili"
I17-1009,N13-1092,0,0.0730301,"Missing"
I17-1009,P15-2011,0,0.0689292,"Missing"
I17-1009,W11-2123,0,0.0228054,"Missing"
I17-1009,2005.mtsummit-papers.11,0,0.0823385,"Missing"
I17-1009,D15-1163,0,0.019982,"roach in terms of MRR (Figure 9) and MAP (Figure 10). Furthermore, Mizukami et al. (2014) constructed the Japanese version9 . In this study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivot"
I17-1009,P05-1074,0,0.558871,"man’s correlation. The contributions of our study are as follows. Introduction Paraphrases are useful for flexible language understanding in many NLP applications. For example, the usefulness of the paraphrase database PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015), a publicly available largescale resource for lexical paraphrasing, has been reported for tasks such as learning word embeddings (Yu and Dredze, 2014) and semantic textual similarity (Sultan et al., 2015). In PPDB, paraphrase pairs are acquired via word alignment on a bilingual corpus by a process called bilingual pivoting (Bannard and Callison-Burch, 2005). Figure 1 shows an example of English language paraphrase acquisition using the German language as a pivot. Although bilingual pivoting is widely used for paraphrase acquisition, it always includes noise 80 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 80–89, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP without loss of generality, we set2 λ1 = λ2 = −1. • Bilingual pivoting-based lexical paraphrase acquisition is generalized using PMI. sbp (e1 , e2 ) = log p(e2 |e1 ) + log p(e1 |e2 ) • Lexical paraphrases are acquired robustly us"
I17-1009,W11-2504,0,0.0218909,"Missing"
I17-1009,N16-3013,0,0.0146782,"igure 10). Furthermore, Mizukami et al. (2014) constructed the Japanese version9 . In this study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivoting (Bannard and Callison-Burch, 2005; Ganitk"
I17-1009,J03-1002,0,0.0142727,"Missing"
I17-1009,P15-2070,0,0.0343085,"Missing"
I17-1009,Q14-1018,0,0.0202178,"ownstream applications. The semantic textual similarity task deals with calculating the semantic similarity between two sentences. In this study, we conducted the evaluation by applying Pearson’s correlation coefficient with a five-step manual evaluation using five datasets constructed by SemEval (Agirre et al., 2012, 2013, 2014, 2015, 2016). We applied the acquired paraphrase pairs to the unsupervised method of DLC@CU (Sultan et al., 2015), which achieved excellent results using PPDB in the semantic textual similarity task of SemEval2015 (Agirre et al., 2015). DLS@CU performs word alignment (Sultan et al., 2014) using PPDB, and calculates sentence similarity according to the ratio of aligned words: sts(s1 , s2 ) = na (s1 ) + na (s2 ) n(s1 ) + n(s2 ) 5.3 Reranking PPDB 2.0 Finally, we reranked paraphrase pairs from a publicly available state-of-the-art paraphrase database.8 PPDB 2.0 (Pavlick et al., 2015) scores paraphrase pairs using supervised learning with (12) 8 86 http://paraphrase.org/ Figure 9: Reranking PPDB 2.0 in MRR. Figure 10: Reranking PPDB 2.0 in MAP. 26,455 labeled data and 209 features. We sorted the paraphrase pairs from PPDB 2.0 using the MIPA instead of the PPDB 2.0 score and used t"
I17-1009,S15-2027,0,0.186232,"distributional similarity themselves in terms of metrics such as mean reciprocal rank (MRR), mean average precision (MAP), coverage, and Spearman’s correlation. The contributions of our study are as follows. Introduction Paraphrases are useful for flexible language understanding in many NLP applications. For example, the usefulness of the paraphrase database PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015), a publicly available largescale resource for lexical paraphrasing, has been reported for tasks such as learning word embeddings (Yu and Dredze, 2014) and semantic textual similarity (Sultan et al., 2015). In PPDB, paraphrase pairs are acquired via word alignment on a bilingual corpus by a process called bilingual pivoting (Bannard and Callison-Burch, 2005). Figure 1 shows an example of English language paraphrase acquisition using the German language as a pivot. Although bilingual pivoting is widely used for paraphrase acquisition, it always includes noise 80 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 80–89, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP without loss of generality, we set2 λ1 = λ2 = −1. • Bilingual pivoting-bas"
I17-1009,Q16-1009,0,0.0115207,"14) constructed the Japanese version9 . In this study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivoting (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), as a (weighted) PMI."
I17-1009,Q16-1029,0,0.019607,"study, we improved bilingual pivoting using a monolingual corpus. Since large-scale monolingual corpora are easily available for many languages, our proposed method may improve paraphrase databases in each of these languages. PPDB (Ganitkevitch et al., 2013) constructed by bilingual pivoting is used in many NLP applications, such as learning word embeddings (Yu and Dredze, 2014), semantic textual similarity (Sultan et al., 2015), machine translation (Mehdizadeh Seraj et al., 2015), sentence compression (Napoles et al., 2016), question answering (Sultan et al., 2016), and text simplification (Xu et al., 2016). Our proposed method may improve the performance of many of these NLP applications supported by PPDB. 6 Related Work Levy and Goldberg (2014) explained a wellknown representation learning method for word embeddings, the skip-gram with negativesampling (SGNS) (Mikolov et al., 2013a,b), as a matrix factorization of a word-context co-occurrence matrix with shifted positive PMI. In this paper, we explained a well-known method for paraphrase acquisition, bilingual pivoting (Bannard and Callison-Burch, 2005; Ganitkevitch et al., 2013), as a (weighted) PMI. Chan et al. (2011) reranked paraphrase pai"
I17-1009,P14-2089,0,0.150119,"lts show that MIPA outperforms bilingual pivoting and distributional similarity themselves in terms of metrics such as mean reciprocal rank (MRR), mean average precision (MAP), coverage, and Spearman’s correlation. The contributions of our study are as follows. Introduction Paraphrases are useful for flexible language understanding in many NLP applications. For example, the usefulness of the paraphrase database PPDB (Ganitkevitch et al., 2013; Pavlick et al., 2015), a publicly available largescale resource for lexical paraphrasing, has been reported for tasks such as learning word embeddings (Yu and Dredze, 2014) and semantic textual similarity (Sultan et al., 2015). In PPDB, paraphrase pairs are acquired via word alignment on a bilingual corpus by a process called bilingual pivoting (Bannard and Callison-Burch, 2005). Figure 1 shows an example of English language paraphrase acquisition using the German language as a pivot. Although bilingual pivoting is widely used for paraphrase acquisition, it always includes noise 80 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 80–89, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP without loss of gene"
I17-1009,S13-1004,0,\N,Missing
I17-1009,S12-1051,0,\N,Missing
I17-2047,P02-1040,0,0.114391,"Missing"
I17-2047,N16-1005,0,0.033785,"e voice of the correct translation in advance. Thus, this study presents a voice prediction method for generated sentences for neural machine translation. While evaluating on Japanese-to-English translation, we obtain a 0.70-improvement in the BLEU using the predicted voice. 1 Introduction Recently, recurrent neural networks such as encoder-decoder models have gained increasing attention in machine translation owing their ability to generate fluent sentences. Controlling the output of the encoder-decoder model is difficult; however, several control mechanisms have been developed. For example, Sennrich et al. (2016) attempted to control honorifics in EnglishGerman neural machine translation (NMT). They trained an attentional encoder-decoder model (Bahdanau et al., 2015) using source data wherein the honorific information of a target sentence was represented by an additional word. They obtained a 3.2-point improvement in the BLEU score when the sentence was controlled to the same honorifics as the reference. Similar to the research of Sennrich et al. (2016), 2 Voice Prediction Our previous study (Yamagishi et al., 2016) did not build a voice classifier for voice control; we used the majority of voice for"
I17-2047,W16-4620,1,0.85836,"del is difficult; however, several control mechanisms have been developed. For example, Sennrich et al. (2016) attempted to control honorifics in EnglishGerman neural machine translation (NMT). They trained an attentional encoder-decoder model (Bahdanau et al., 2015) using source data wherein the honorific information of a target sentence was represented by an additional word. They obtained a 3.2-point improvement in the BLEU score when the sentence was controlled to the same honorifics as the reference. Similar to the research of Sennrich et al. (2016), 2 Voice Prediction Our previous study (Yamagishi et al., 2016) did not build a voice classifier for voice control; we used the majority of voice for each verb in the training corpus. We reported that the majority vote did not consistently improve the BLEU score. In 277 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 277–282, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP sentences2 . Thus, this feature shows whether the sentence has an auxiliary verb of representing passive voice. contrast, this study develops a voice classifier using the following seven features. In this way, we can consider t"
L16-1060,C14-1117,0,0.0602296,"Missing"
L16-1060,C90-2036,0,\N,Missing
L16-1060,P12-2073,0,\N,Missing
L16-1060,rodrigues-rytting-2012-typing,0,\N,Missing
L16-1060,P14-1122,0,\N,Missing
L18-1152,P15-2025,0,0.0229296,"ord is represented as a sparse vector indicating the word itself (one-hot vector) or the context of the word (distributional vector). However, both the one-hot notation and distributional notation suffer from data sparseness since dimensions of the word vector do not interact with each other. Distributed word representation addresses the data sparseness problem by constructing a dense vector of a fixed length, wherein contexts are shared (or distributed) across dimensions. Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al., 2015) to name a few. The task to learn a distributed representation is called representation learning. However, evaluating the quality of learned distributed word representation itself is not straightforward. In language modeling, perplexity or cross-entropy is widely accepted as a de facto standard for intrinsic evaluation. In contrast, distributed word representations include the additive (or compositional) property of the vectors, which cannot be assessed by perplexity. Moreover, perplexity makes little use of infrequent words; thus, it is not appropriat"
L18-1152,isahara-etal-2008-development,0,0.0205144,"4,033 adjective pairs, 1,528 noun pairs and 902 adverb pairs. To balance the numbers of verb and adjective pairs with other parts of speech, we extracted samples at random for verbs and adjectives. Finally, we obtained 1,464 verb pairs and 960 adjective pairs. We observed that the similarity of the pairs extracted from the dataset of Kodaira et al. (2016) was low without providing contexts; thus, we did not augment the dataset by inserting pseudo-negative instances from WordNet’s synsets, as was done in the RW corpus. Another reason why we did not employ the synset from the Japanese WordNet (Isahara et al., 2008) was because its quality was not as good as the English WordNet except for concrete nouns2 . 3. Construction of a Japanese Word Similarity Dataset What makes a pair of words similar? Most of the previous datasets do not concretely define the similarity of word pairs. The difference in the similarity of word pairs originates from each annotator’s mind, resulting in different scales of a word. Thus, we propose to use an examplebased approach (Table 2) to control the variance of the similarity ratings. We remove the context of word when we extracted the word. So, we consider that an ambiguous wor"
L18-1152,W13-3512,0,0.714756,"tive, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with conventional models. However, low-frequency words and unknown words are often comprise high-frequency morphemes (e.g., unkingly → un + king + ly). Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 948 I don’t think it is likely to not include these people, or [exclude] まさかこういった方々を対象としない、[排除する]わけではないと思いますが ignore ostracize avoid exclude remove 無視する 排斥する 敬遠する 排除する 除外する Sentence Paraphrase Figure 1: An example of the dataset from a previous study (Kodaira et al., 2016). Frequency 1- 101- 1001- 10001- Verb Adjective Noun Adverb 239 183 15 23 539 322 63 75 710 523 172 80 598 350 258 81 word 1 word 2 EN JA EN JA close erase mopey investigate fly 瞑る 拭き取る 塞ぎ込んだ 手探る とばせる close wipe sick go control つぶる 拭う 病んだ 行く 制御できる sim. 10 8 5 2 0 Table 1: The number of parts of speech class"
L18-1152,N16-1018,0,0.0621131,"Missing"
L18-1152,N15-1186,0,0.0428367,"Missing"
L18-1152,P15-1150,0,0.0227087,"ting the word itself (one-hot vector) or the context of the word (distributional vector). However, both the one-hot notation and distributional notation suffer from data sparseness since dimensions of the word vector do not interact with each other. Distributed word representation addresses the data sparseness problem by constructing a dense vector of a fixed length, wherein contexts are shared (or distributed) across dimensions. Distributed word representation is known to improve the performance of many NLP applications such as machine translation (Chen and Guo, 2015) and sentiment analysis (Tai et al., 2015) to name a few. The task to learn a distributed representation is called representation learning. However, evaluating the quality of learned distributed word representation itself is not straightforward. In language modeling, perplexity or cross-entropy is widely accepted as a de facto standard for intrinsic evaluation. In contrast, distributed word representations include the additive (or compositional) property of the vectors, which cannot be assessed by perplexity. Moreover, perplexity makes little use of infrequent words; thus, it is not appropriate for evaluating distributed presentations"
L18-1152,D16-1235,0,0.0493268,"des rare words in addition to common words. 2. Related Work In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) have been used to evaluate word similarities in English. Moreover, Baker et al. (2014) built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of verbs in the word-similarity task. Recently, SimVerb-3500 was introduced to evaluate human understanding of verb meaning (Gerz et al., 2016). It provides human ratings for the similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 (Hill et al., 2015) have been translated and rescored in other languages: German, Italian and Russian (Leviant and Reichart, 2015). SimLex-999 has also been translated and rescored in Hebrew and Croatian (Mrksic et al., 2017). SimLex-999 explicitly targets at similarity rather than r"
L18-1152,J15-4004,0,0.0811388,"milarities in English. Moreover, Baker et al. (2014) built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of verbs in the word-similarity task. Recently, SimVerb-3500 was introduced to evaluate human understanding of verb meaning (Gerz et al., 2016). It provides human ratings for the similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from English, WordSim353 and SimLex-999 (Hill et al., 2015) have been translated and rescored in other languages: German, Italian and Russian (Leviant and Reichart, 2015). SimLex-999 has also been translated and rescored in Hebrew and Croatian (Mrksic et al., 2017). SimLex-999 explicitly targets at similarity rather than relatedness and includes adjective, noun and verb pairs. However, this dataset contains only frequent words. In addition, the distributed representation of words is generally learned using only word-level information. Consequently, the distributed representation for low-frequency words and unknown words cannot be learned well with con"
L18-1152,P12-1092,0,0.0599328,"we propose to build a Japanese dataset for the word similarity task. Currently at JustSystems Corporation. The main contributions of our work are as follows: • To the best of our knowledge, it is the first work that constructs a Japanese word similarity dataset. • The dataset contains various parts of speech and includes rare words in addition to common words. 2. Related Work In general, distributed word representations are evaluated using a word similarity task. For instance, WordSim353 (Finkelstein et al., 2002), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), and SCWS (Huang et al., 2012) have been used to evaluate word similarities in English. Moreover, Baker et al. (2014) built a verb similarity dataset (VSD) based on WordSim353 because there was no dataset of verbs in the word-similarity task. Recently, SimVerb-3500 was introduced to evaluate human understanding of verb meaning (Gerz et al., 2016). It provides human ratings for the similarity of 3,500 verb pairs so that it enables robust evaluation of distributed representation for verbs. However, most of these datasets include English words only. There has been no Japanese dataset for the word-similarity task. Apart from E"
L18-1152,P16-3001,1,0.830172,"ned well with conventional models. However, low-frequency words and unknown words are often comprise high-frequency morphemes (e.g., unkingly → un + king + ly). Some previous studies take advantage of the morphological information to provide a suitable representation for low-frequency words and unknown words (Luong et al., 2013; Soricut and Och, 948 I don’t think it is likely to not include these people, or [exclude] まさかこういった方々を対象としない、[排除する]わけではないと思いますが ignore ostracize avoid exclude remove 無視する 排斥する 敬遠する 排除する 除外する Sentence Paraphrase Figure 1: An example of the dataset from a previous study (Kodaira et al., 2016). Frequency 1- 101- 1001- 10001- Verb Adjective Noun Adverb 239 183 15 23 539 322 63 75 710 523 172 80 598 350 258 81 word 1 word 2 EN JA EN JA close erase mopey investigate fly 瞑る 拭き取る 塞ぎ込んだ 手探る とばせる close wipe sick go control つぶる 拭う 病んだ 行く 制御できる sim. 10 8 5 2 0 Table 1: The number of parts of speech classified into each frequency. Table 2: Example of the degree of similarity when we requested annotation at Lancers. 2015). Morphological information is particularly important for Japanese since Japanese is an agglutinative language. Pair construction: Because extracted words are annotated with"
N18-4014,N16-1078,0,0.0310038,"Missing"
N18-4014,D15-1166,0,0.0547107,"present the predicates which we were observed a few times on the training corpus by combining lemmas and conjugation forms found in the training corpus. In this research, MeCab3 is used as a Japanese morphological analyzer, and the morpheme information adopts the standard of IPADic. Specifically, “surface form”, “POS (coarse-grained)”, “POS (fine-grained)”, “conjugation type”, “conju• We found that conjugation features are best exploited as tokens rather than embeddings and suggested the connection between the position of the token and linguistic properties. 2 Related work Backoff dictionary. Luong et al. (2015b) proposed a method of rewriting an unknown word token in the output into an appropriate word using a dictionary. This method determines a corresponding word using alignment information between an output sentence and an input sentence and rewrites the unknown word token in the output using the dictionary. Therefore, it does not allow NMT to consider the meaning of OOVs. However, this method can be used together with the proposed method, which results in the further reduction of unknown words. Byte pair encoding. Sennrich et al. (2016) proposed a method to construct vocabulary by splitting all"
N18-4014,P15-1002,0,0.0720804,"Missing"
N18-4014,P02-1040,0,0.110909,"ted to the source side, only Japanese-to-English translation was performed. To restrict the embed size to 512, the size of each feature was set to POS (coarse-grained): 4, POS (fine-grained): 8, conjugation form: 8, lemma: 492. POS token. We increased the output limit by 2.5 times in English-to-Japanese translation because of additional POS tokens attached to all words. We used the same restoration rules as for Conjugation token to treat special tokens. We evaluated POS features in only ASPEC and Tanaka owing to time constraints. 6 Discussion 6.1 Translation quality The results of BLEU score (Papineni et al., 2002) are shown in Table 3. Compared to the baseline without BPE, Conjugation token improved in BLEU score on all corpora and in both translation directions. In addition, Conjugation token outperformed the baselines with BPE with an exception on NTCIR in Japanese-to-English direction. When the POS token was introduced, BLEU scores improved by 1.82 points on average from the baseline in Japanese-to-English translation. (ASPEC : 0.91, Tanaka : 2.73) Furthermore, we compared proposed methods with the baseline that adopted BPE to the Japanese side only8 . Table 3 shows the results of baseline Byte pair"
N18-4014,W16-2209,0,0.0231105,"posed a method to construct vocabulary by splitting all the words into characters and re-combining them based on their frequencies to make subword unit. Because all words can be split into known words based on characters, this method has an advantage in that OOV words disappear. However, because coupling of subwords depends on frequency, grammatical and semantic information is not taken into consideration. Incidentally, Japanese has many characters especially kanji; therefore, there might exist unknown characters that do not exist in the training corpus even after applying BPE. Input feature. Sennrich and Haddow (2016) proposed a method to add POS information and dependency structure as embeddings with the aim of explicitly learning syntax information in NMT. However, it can only be applied to the input side. 2 Derivational grammar (Ogawa et al., 1998) to unify multiple conjugation forms, but it cannot distinguish between plain and attributive forms and imperfective and continuative forms if they have the same surface. 3 https://github.com/taku910/mecab 101 Corpus NTCIR ASPEC Tanaka gation form”, and “lemma” are used. Hereafter, predicates represent verbs, adjectives, and auxiliary verbs. 4 Introducing Japa"
N18-4014,P16-1162,0,0.0520633,"and linguistic properties. 2 Related work Backoff dictionary. Luong et al. (2015b) proposed a method of rewriting an unknown word token in the output into an appropriate word using a dictionary. This method determines a corresponding word using alignment information between an output sentence and an input sentence and rewrites the unknown word token in the output using the dictionary. Therefore, it does not allow NMT to consider the meaning of OOVs. However, this method can be used together with the proposed method, which results in the further reduction of unknown words. Byte pair encoding. Sennrich et al. (2016) proposed a method to construct vocabulary by splitting all the words into characters and re-combining them based on their frequencies to make subword unit. Because all words can be split into known words based on characters, this method has an advantage in that OOV words disappear. However, because coupling of subwords depends on frequency, grammatical and semantic information is not taken into consideration. Incidentally, Japanese has many characters especially kanji; therefore, there might exist unknown characters that do not exist in the training corpus even after applying BPE. Input featu"
N18-4015,D15-1075,0,0.185995,"ons trained using three consecutive sentences, such as si−1 , si , and si+1 . It is an encoderdecoder model that encodes sentence si and predicts previous and next sentences si−1 and si+1 from its sentence representation si (Figure 1). As a result of training, this encoder can produce sentence representations. Skip-Thought demonstrates high performance, especially when applied to document classification tasks. Second, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence representations trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, entailment, contradiction and neutral; thus, InferSent can train sentence representations that are sensitive to differences in meaning. This model encodes sentence pairs u and v and generates features by sentence representations u and v with a bi-directional LSTM architecture with max pooling (Figure 2). InferSent demonstrates high performance across various document classification and semantic textual similarity tasks. ing three matching methods to extract relations between t and r (Figure 3)"
N18-4015,D18-2029,0,0.0534779,"Missing"
N18-4015,D17-1070,0,0.0302421,"arity, and we call them universal sentence representations. First, Skip-Thought5 (Kiros et al., 2015) builds an unsupervised model of universal sentence representations trained using three consecutive sentences, such as si−1 , si , and si+1 . It is an encoderdecoder model that encodes sentence si and predicts previous and next sentences si−1 and si+1 from its sentence representation si (Figure 1). As a result of training, this encoder can produce sentence representations. Skip-Thought demonstrates high performance, especially when applied to document classification tasks. Second, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence representations trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, entailment, contradiction and neutral; thus, InferSent can train sentence representations that are sensitive to differences in meaning. This model encodes sentence pairs u and v and generates features by sentence representations u and v with a bi-directional LSTM architecture with max pooling (Figure 2). InferSent demonstrates high"
N18-4015,D15-1124,0,0.236777,"Missing"
N18-4015,W15-3047,0,0.339521,"Missing"
N18-4015,W17-4755,0,0.153277,"n and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017), which achieved excellent results in the WMT-2017 Metrics task (Bojar et al., 2017). Therefore, they can exploit only limited information for segment-level MTE. In other words, MTE metrics based on character N-grams or word N-grams cannot make full use of sentence representations; they only check for word matches. 2 Related Work DPMFcomb (Yu et al., 2015a) achieved the best performance in the WMT-2016 Metrics task (Bojar et al., 2016). It incorporates 55 default metrics provided by the Asiya MT evaluation toolkit1 (Gim´enez and M`arquez, 2010), as well as three other metrics, namely, DPMF (Yu et al., 2015b), REDp (Yu et al., 2015a), and ENTFp (Yu et al., 2015a), using rankin"
N18-4015,W17-4768,0,0.36281,"c machine translation evaluation (MTE). MTE metrics having a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference translation based on character N-grams or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017), which achieved excellent results in the WMT-2017 Metrics task (Bojar et al., 2017). Therefore, they can exploit only limited information for segment-level MTE. In other words, MTE metrics based on character N-grams or word N-grams cannot make full use of sentence representations; they only check for word matches. 2 Related Work DPMFcomb (Yu et al., 2015a) achieved the best performance in the WMT-2016 Metrics task (Bojar et al., 2016). It incorporates 55 default metrics provided by the Asiya MT evaluation toolkit1 (Gim´enez and M`arquez, 2010"
N18-4015,W17-4770,0,0.0416477,"Missing"
N18-4015,W15-3031,0,0.108361,"Missing"
N18-4015,W15-3050,0,0.0950926,"Missing"
N18-4015,W16-2342,0,0.0447906,"trained using largescale data obtained in other tasks. Therefore, the proposed approach avoids the problem of using a small dataset for training sentence representations. After the success of DPMFcomb , Blend2 (Ma et al., 2017) achieved the best performance in the WMT-2017 Metrics task (Bojar et al., 2017). Similar to DPMFcomb , Blend is essentially an SVR (RBF kernel) model that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely, BEER (Stanojevi´c and Sima’an, 2015), CharacTER (Wang et al., 2016), DPMF and ENTFp. BEER (Stanojevi´c and Sima’an, 2015) is a linear model based on character N-grams and replacement trees. CharacTER (Wang et al., 2016) evaluates an MT hypothesis based on character-level edit distance. DPMFcomb is trained through relative ranking of human evaluation data in terms of relative ranking (RR). The quality of five MT hypotheses of the same source segment are ranked from 1 to 5 via comparison with the reference translation. In contrast, Blend is trained through direct assessment (DA) of human evaluation data. DA provides the absolute quality scores of hypotheses, by"
N18-4015,P02-1040,0,\N,Missing
N18-4015,C04-1072,0,\N,Missing
N18-4015,W16-2302,0,\N,Missing
N19-1344,P10-1160,0,0.0359718,"ral end-to-end models using neural networks showed high performance in English SRL (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Strubell et al. (2018) proposed a multi-task learning model that jointly learned dependency parsing, part-ofspeech tagging, predicate detection, and SRL based on multi-head self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we sho"
N19-1344,D17-1206,0,0.171507,"ngle (b) Multi-input (d) Multi-output (c) Multi-RNN (e) Multi-ALL Figure 3: Proposed models: (a) Single, (b) Multi-input, (c) Multi-RNN, (d) Multi-output, (e) Multi-ALL. over four labels, [NOM, ACC, DAT, ELSE]. ELSE denotes that the candidate argument does not have a case label. In testing, the maximum probability label is selected as the output label. We train the model using the cross-entropy loss function. 4 Multi-task Model Multi-task learning has been successfully applied to various natural language processing tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Luong et al., 2016; Hashimoto et al., 2017; Liu et al., 2017; Stoyanov et al., 2018; Marasovic and Frank, 2018; Strubell et al., 2018). One of the advantages of multi-task learning is that it learns better representation, which is robust against task-dependent noise by increasing training data. In this paper, we introduce multitask learning to PASA and ENASA for the first time. We propose three methods to extend the end-to-end single model to the multi-task learning model in the input layer, RNN layer, and output layer. Figure 3 shows the proposed models. Our final model combines all three methods (Figure 3e). 4.1 Multi Input Layer Ev"
N19-1344,I11-1023,1,0.803152,"is the first work to employ neural networks to effectively incorporate PASA. 2 Related Work 2.1 Japanese PASA and ENASA Approaches Many machine learning-based methods have been studied in Japanese PASA. Traditional models take pointwise approaches that construct independent models for each core case role (NOM, ACC, DAT). Taira et al. (2008) proposed a supervised model that learns features of each case using decision lists and support vector machines. Imamura et al. (2009) proposed a model that combines a maximum entropy model with a language model trained from large-scale newspaper articles. Hayashibe et al. (2011) designed three models exploiting argument position and type and determined the maximum likelihood output using pairwise comparison. However, the joint approach that optimizes the scores of all predicate-argument pairs in a sentence simultaneously showed better results than the pointwise approach. Yoshikawa et al. (2011) proposed a model that considers dependency between multiple predicate-argument relations using Markov logic networks. Ouchi et al. (2015) jointly optimized the combinations among multiple predicates and arguments in a sentence using a bipartite graph. Except for (Taira et al.,"
N19-1344,P17-1044,0,0.0329127,"ction of multiple predicates simultaneously using a Grid RNN. Matsubayashi and Inui (2018) combined self-attention with Ouchi et al. (2017)’s model to directly capture interaction among multiple predicate-arguments. In particular, the model improved the performance of arguments that have no syntactic dependency with predicates and achieved a state-of-the-art result on Japanese PASA. 3405 2.3 Semantic Role Labeling Semantic role labeling (SRL) is a similar task to Japanese PASA. Recently, several end-to-end models using neural networks showed high performance in English SRL (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Strubell et al. (2018) proposed a multi-task learning model that jointly learned dependency parsing, part-ofspeech tagging, predicate detection, and SRL based on multi-head self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and"
N19-1344,P06-1079,0,0.256758,"al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (event-noun) p = p1 , p2 , · · · , pq are given as input. Iida et al. (2006), Imamura et al. (2009), and Sasano and Kurohashi (2011) also analyze Inter-zero, which is a difficult task because the whole document must be searched. Following existing research (Ouchi et al., 2015, 2017; Matsubayashi and Inui, 2017, 2018; Taira et al., 2008), we only focus on three categories where arguments and their predicate (event-noun) are in the same sentence. In addition, we exclude the Bunsetsu category from the PASA evaluation following Ouchi et al. (2017) and Matsubayashi and Inui (2018). 3.2 End-to-end Single Model Our single model is based on an end-to-end approach (Zhou and Xu"
N19-1344,P17-1146,0,0.513509,"een task-shared and task-specific features using multi-task learning. 2.2 PASA using neural networks Some neural models have achieved higher performance than traditional machine learning models in Japanese PASA. Shibata et al. (2016) replaced Ouchi et al. (2015)’s scoring function with feed forward neural networks. Matsubayashi and Inui (2017) represented a dependency path between a predicate and its argument with path embeddings and showed that even the local model without multiple predicates can outperform a global model. Moreover, some end-to-end models have been proposed in Japanese PASA. Ouchi et al. (2017) proposed an end-to-end model based on the model using eight-layer bi-directional long shortterm memory (LSTM) proposed by Zhou and Xu (2015) and considered the interaction of multiple predicates simultaneously using a Grid RNN. Matsubayashi and Inui (2018) combined self-attention with Ouchi et al. (2017)’s model to directly capture interaction among multiple predicate-arguments. In particular, the model improved the performance of arguments that have no syntactic dependency with predicates and achieved a state-of-the-art result on Japanese PASA. 3405 2.3 Semantic Role Labeling Semantic role l"
N19-1344,W07-1522,1,0.839766,"or natural language processing that requires deep analysis of complicated sentences such as machine translation and recognizing textual entailment. PASA is a task targeted at predicates such as verbs and adjectives. However, there are also many nouns that have eventrelated arguments in a sentence. We call these nouns that refer to events event-nouns, for example, a verbal noun (sahen nouns) such as houkoku “report” or a deverbal noun (nominalized forms of verbs) such as sukui “rescue.” Figure 1 shows examples of PASA and eventnoun argument structure analysis (ENASA). In the NAIST Text Corpus (Iida et al., 2007), both predicates and event-nouns have one of three core Figure 1: Examples of PASA and ENASA. The edges denote dependency paths. case roles, nominative (NOM), accusative (ACC), and dative (DAT) as an argument. According to Iida et al. (2007), predicates have almost no argument in the same bunsetsu1 phrase. However, in the case of event-nouns, approximately half of the accusative and dative arguments appear in the same bunsetsu phrase. Accordingly, although PASA and ENASA are semantically highly related, they are syntactically different tasks. However, most previous studies focused on predicat"
N19-1344,D18-1191,0,0.0158646,"ular, the model improved the performance of arguments that have no syntactic dependency with predicates and achieved a state-of-the-art result on Japanese PASA. 3405 2.3 Semantic Role Labeling Semantic role labeling (SRL) is a similar task to Japanese PASA. Recently, several end-to-end models using neural networks showed high performance in English SRL (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Strubell et al. (2018) proposed a multi-task learning model that jointly learned dependency parsing, part-ofspeech tagging, predicate detection, and SRL based on multi-head self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al.,"
N19-1344,P09-2022,0,0.395955,"es a state-of-theart result on the NAIST Text Corpus (NTC) in PASA by combining syntactic information as one of the features. 3. For ENASA, this is the first work to employ neural networks to effectively incorporate PASA. 2 Related Work 2.1 Japanese PASA and ENASA Approaches Many machine learning-based methods have been studied in Japanese PASA. Traditional models take pointwise approaches that construct independent models for each core case role (NOM, ACC, DAT). Taira et al. (2008) proposed a supervised model that learns features of each case using decision lists and support vector machines. Imamura et al. (2009) proposed a model that combines a maximum entropy model with a language model trained from large-scale newspaper articles. Hayashibe et al. (2011) designed three models exploiting argument position and type and determined the maximum likelihood output using pairwise comparison. However, the joint approach that optimizes the scores of all predicate-argument pairs in a sentence simultaneously showed better results than the pointwise approach. Yoshikawa et al. (2011) proposed a model that considers dependency between multiple predicate-argument relations using Markov logic networks. Ouchi et al."
N19-1344,D09-1133,0,0.0424233,"irectional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (event-noun) p = p1 , p2 , · · · , pq are given as input. Iida et al. (2006), Imamura et al. (2009), an"
N19-1344,P17-1001,0,0.0296666,"Multi-output (c) Multi-RNN (e) Multi-ALL Figure 3: Proposed models: (a) Single, (b) Multi-input, (c) Multi-RNN, (d) Multi-output, (e) Multi-ALL. over four labels, [NOM, ACC, DAT, ELSE]. ELSE denotes that the candidate argument does not have a case label. In testing, the maximum probability label is selected as the output label. We train the model using the cross-entropy loss function. 4 Multi-task Model Multi-task learning has been successfully applied to various natural language processing tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Luong et al., 2016; Hashimoto et al., 2017; Liu et al., 2017; Stoyanov et al., 2018; Marasovic and Frank, 2018; Strubell et al., 2018). One of the advantages of multi-task learning is that it learns better representation, which is robust against task-dependent noise by increasing training data. In this paper, we introduce multitask learning to PASA and ENASA for the first time. We propose three methods to extend the end-to-end single model to the multi-task learning model in the input layer, RNN layer, and output layer. Figure 3 shows the proposed models. Our final model combines all three methods (Figure 3e). 4.1 Multi Input Layer Even if the surface"
N19-1344,N18-1054,0,0.020055,"Figure 3: Proposed models: (a) Single, (b) Multi-input, (c) Multi-RNN, (d) Multi-output, (e) Multi-ALL. over four labels, [NOM, ACC, DAT, ELSE]. ELSE denotes that the candidate argument does not have a case label. In testing, the maximum probability label is selected as the output label. We train the model using the cross-entropy loss function. 4 Multi-task Model Multi-task learning has been successfully applied to various natural language processing tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Luong et al., 2016; Hashimoto et al., 2017; Liu et al., 2017; Stoyanov et al., 2018; Marasovic and Frank, 2018; Strubell et al., 2018). One of the advantages of multi-task learning is that it learns better representation, which is robust against task-dependent noise by increasing training data. In this paper, we introduce multitask learning to PASA and ENASA for the first time. We propose three methods to extend the end-to-end single model to the multi-task learning model in the input layer, RNN layer, and output layer. Figure 3 shows the proposed models. Our final model combines all three methods (Figure 3e). 4.1 Multi Input Layer Even if the surface form is the same, the contexts are different for p"
N19-1344,K17-1041,0,0.0131375,". Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (event-noun) p = p1 , p2 , · · · , pq are given"
N19-1344,I17-2022,0,0.684885,"a report”). Since the previous ENASA models adopted the pointwise approach with a single model, they did not explore the effective features in each task. In contrast, our models simultaneously optimize three core case roles. Moreover, the proposed models allow us to distinguish between task-shared and task-specific features using multi-task learning. 2.2 PASA using neural networks Some neural models have achieved higher performance than traditional machine learning models in Japanese PASA. Shibata et al. (2016) replaced Ouchi et al. (2015)’s scoring function with feed forward neural networks. Matsubayashi and Inui (2017) represented a dependency path between a predicate and its argument with path embeddings and showed that even the local model without multiple predicates can outperform a global model. Moreover, some end-to-end models have been proposed in Japanese PASA. Ouchi et al. (2017) proposed an end-to-end model based on the model using eight-layer bi-directional long shortterm memory (LSTM) proposed by Zhou and Xu (2015) and considered the interaction of multiple predicates simultaneously using a Grid RNN. Matsubayashi and Inui (2018) combined self-attention with Ouchi et al. (2017)’s model to directly"
N19-1344,C18-1009,0,0.392242,"et al. (2015)’s scoring function with feed forward neural networks. Matsubayashi and Inui (2017) represented a dependency path between a predicate and its argument with path embeddings and showed that even the local model without multiple predicates can outperform a global model. Moreover, some end-to-end models have been proposed in Japanese PASA. Ouchi et al. (2017) proposed an end-to-end model based on the model using eight-layer bi-directional long shortterm memory (LSTM) proposed by Zhou and Xu (2015) and considered the interaction of multiple predicates simultaneously using a Grid RNN. Matsubayashi and Inui (2018) combined self-attention with Ouchi et al. (2017)’s model to directly capture interaction among multiple predicate-arguments. In particular, the model improved the performance of arguments that have no syntactic dependency with predicates and achieved a state-of-the-art result on Japanese PASA. 3405 2.3 Semantic Role Labeling Semantic role labeling (SRL) is a similar task to Japanese PASA. Recently, several end-to-end models using neural networks showed high performance in English SRL (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Strubell et al. (2018) proposed a multi-task learning"
N19-1344,P15-1093,0,0.204075,"et al. (2009) proposed a model that combines a maximum entropy model with a language model trained from large-scale newspaper articles. Hayashibe et al. (2011) designed three models exploiting argument position and type and determined the maximum likelihood output using pairwise comparison. However, the joint approach that optimizes the scores of all predicate-argument pairs in a sentence simultaneously showed better results than the pointwise approach. Yoshikawa et al. (2011) proposed a model that considers dependency between multiple predicate-argument relations using Markov logic networks. Ouchi et al. (2015) jointly optimized the combinations among multiple predicates and arguments in a sentence using a bipartite graph. Except for (Taira et al., 2008), these studies focused on the analysis of predicates while there are few studies that focus on event-nouns. Komachi et al. (2007) decomposed ENASA into two tasks: event-hood determination and argument identification; they proposed a supervised method using lexico-syntactic patterns. Eventhood determination is the most important characteristic that semantically differentiates ENASA from PASA. It is a task to determine whether a noun refers to an even"
N19-1344,W17-4305,0,0.0143263,"sed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (event-noun) p = p1 , p2 , · · · , pq are given as input. Iida et al"
N19-1344,P16-1113,0,0.0149957,"nd SRL based on multi-head self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (ev"
N19-1344,I11-1085,0,0.136792,"howed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (event-noun) p = p1 , p2 , · · · , pq are given as input. Iida et al. (2006), Imamura et al. (2009), and Sasano and Kurohashi (2011) also analyze Inter-zero, which is a difficult task because the whole document must be searched. Following existing research (Ouchi et al., 2015, 2017; Matsubayashi and Inui, 2017, 2018; Taira et al., 2008), we only focus on three categories where arguments and their predicate (event-noun) are in the same sentence. In addition, we exclude the Bunsetsu category from the PASA evaluation following Ouchi et al. (2017) and Matsubayashi and Inui (2018). 3.2 End-to-end Single Model Our single model is based on an end-to-end approach (Zhou and Xu, 2015; Ouchi et al., 2017; Matsubayashi and Inui, 2018)"
N19-1344,D16-1212,0,0.0233334,"ead self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · , wT and a predicate (event-noun) p = p1 ,"
N19-1344,P16-1117,0,0.0167857,"n refers to an event (e.g., houkoku can refer to either “to report” or the outcome of reporting action, “a report”). Since the previous ENASA models adopted the pointwise approach with a single model, they did not explore the effective features in each task. In contrast, our models simultaneously optimize three core case roles. Moreover, the proposed models allow us to distinguish between task-shared and task-specific features using multi-task learning. 2.2 PASA using neural networks Some neural models have achieved higher performance than traditional machine learning models in Japanese PASA. Shibata et al. (2016) replaced Ouchi et al. (2015)’s scoring function with feed forward neural networks. Matsubayashi and Inui (2017) represented a dependency path between a predicate and its argument with path embeddings and showed that even the local model without multiple predicates can outperform a global model. Moreover, some end-to-end models have been proposed in Japanese PASA. Ouchi et al. (2017) proposed an end-to-end model based on the model using eight-layer bi-directional long shortterm memory (LSTM) proposed by Zhou and Xu (2015) and considered the interaction of multiple predicates simultaneously usi"
N19-1344,P16-2038,0,0.160943,"bability for each argument candidate 3407 (a) Single (b) Multi-input (d) Multi-output (c) Multi-RNN (e) Multi-ALL Figure 3: Proposed models: (a) Single, (b) Multi-input, (c) Multi-RNN, (d) Multi-output, (e) Multi-ALL. over four labels, [NOM, ACC, DAT, ELSE]. ELSE denotes that the candidate argument does not have a case label. In testing, the maximum probability label is selected as the output label. We train the model using the cross-entropy loss function. 4 Multi-task Model Multi-task learning has been successfully applied to various natural language processing tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Luong et al., 2016; Hashimoto et al., 2017; Liu et al., 2017; Stoyanov et al., 2018; Marasovic and Frank, 2018; Strubell et al., 2018). One of the advantages of multi-task learning is that it learns better representation, which is robust against task-dependent noise by increasing training data. In this paper, we introduce multitask learning to PASA and ENASA for the first time. We propose three methods to extend the end-to-end single model to the multi-task learning model in the input layer, RNN layer, and output layer. Figure 3 shows the proposed models. Our final model combines all three m"
N19-1344,P18-1074,0,0.0212886,"Multi-RNN (e) Multi-ALL Figure 3: Proposed models: (a) Single, (b) Multi-input, (c) Multi-RNN, (d) Multi-output, (e) Multi-ALL. over four labels, [NOM, ACC, DAT, ELSE]. ELSE denotes that the candidate argument does not have a case label. In testing, the maximum probability label is selected as the output label. We train the model using the cross-entropy loss function. 4 Multi-task Model Multi-task learning has been successfully applied to various natural language processing tasks (Collobert et al., 2011; Søgaard and Goldberg, 2016; Luong et al., 2016; Hashimoto et al., 2017; Liu et al., 2017; Stoyanov et al., 2018; Marasovic and Frank, 2018; Strubell et al., 2018). One of the advantages of multi-task learning is that it learns better representation, which is robust against task-dependent noise by increasing training data. In this paper, we introduce multitask learning to PASA and ENASA for the first time. We propose three methods to extend the end-to-end single model to the multi-task learning model in the input layer, RNN layer, and output layer. Figure 3 shows the proposed models. Our final model combines all three methods (Figure 3e). 4.1 Multi Input Layer Even if the surface form is the same, the c"
N19-1344,D18-1548,0,0.0687338,"neously using a Grid RNN. Matsubayashi and Inui (2018) combined self-attention with Ouchi et al. (2017)’s model to directly capture interaction among multiple predicate-arguments. In particular, the model improved the performance of arguments that have no syntactic dependency with predicates and achieved a state-of-the-art result on Japanese PASA. 3405 2.3 Semantic Role Labeling Semantic role labeling (SRL) is a similar task to Japanese PASA. Recently, several end-to-end models using neural networks showed high performance in English SRL (Zhou and Xu, 2015; He et al., 2017; Tan et al., 2018). Strubell et al. (2018) proposed a multi-task learning model that jointly learned dependency parsing, part-ofspeech tagging, predicate detection, and SRL based on multi-head self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic"
N19-1344,D08-1055,0,0.398961,"ghly related, they are syntactically different tasks. However, most previous studies focused on predicates only; hence, there are few studies that focus 1 Functional chunk in Japanese. It consists of one or more content words (noun, verb, adjective, etc.) followed by zero or more function words (postposition, auxiliary verb, etc.). A verb phrase in Japanese thus cannot bear noun arguments in the same bunsetsu. 3404 Proceedings of NAACL-HLT 2019, pages 3404–3414 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics on event-nouns (Komachi et al., 2007; Taira et al., 2008). To identify the semantic units of a sentence and to correctly understand syntactic relations, it is not sufficient to target only PASA. Thus, we propose a multi-task learning model that effectively leverages ENASA and improves PASA. Our proposed model is based on an end-toend multilayer bi-directional recurrent neural network (RNN) used in recent works, and the model has networks that distinguish task-independent information and task-specific information. In summary, the main contributions of this work are the following: 1. This is the first attempt to design a multi-task learning framework"
N19-1344,D15-1186,0,0.0210985,"dicate detection, and SRL based on multi-head self-attention. Ouchi et al. (2018) proposed a span-based SRL model using bi-directional LSTMs and achieved state-of-theart results. The authors scored all possible spans for each label and selected correct spans satisfying constraints when decoding. In terms of the event-noun research, Gerber and Chai (2010) used pointwise mutual information (PMI) as a feature for 10 event-nouns with high frequency and identified semantic roles using a logistic regression model. There were several LSTM models that also achieved high accuracy gains in Chinese SRL (Wang et al., 2015; Roth and Lapata, 2016; Sha et al., 2016; Marcheggiani et al., 2017; Qian et al., 2017). For event-nouns, Li et al. (2009) showed that combining effective features in verbal SRL with nominal SRL can improve results. Although the authors did not demonstrate that verbal SRL also improves performance in combination with nominal SRL, we show that our model improves performance in both PASA and ENASA. 3 Inter-zero Zero anaphoric arguments and their predicate (event-noun) are not in the same sentence. Bunsetsu Arguments and their event-noun are in the same bunsetsu. A sentence w = w1 , w2 , · · · ,"
N19-1344,I11-1126,0,0.0509756,"Missing"
N19-1344,P15-1109,0,0.358907,"erformance than traditional machine learning models in Japanese PASA. Shibata et al. (2016) replaced Ouchi et al. (2015)’s scoring function with feed forward neural networks. Matsubayashi and Inui (2017) represented a dependency path between a predicate and its argument with path embeddings and showed that even the local model without multiple predicates can outperform a global model. Moreover, some end-to-end models have been proposed in Japanese PASA. Ouchi et al. (2017) proposed an end-to-end model based on the model using eight-layer bi-directional long shortterm memory (LSTM) proposed by Zhou and Xu (2015) and considered the interaction of multiple predicates simultaneously using a Grid RNN. Matsubayashi and Inui (2018) combined self-attention with Ouchi et al. (2017)’s model to directly capture interaction among multiple predicate-arguments. In particular, the model improved the performance of arguments that have no syntactic dependency with predicates and achieved a state-of-the-art result on Japanese PASA. 3405 2.3 Semantic Role Labeling Semantic role labeling (SRL) is a similar task to Japanese PASA. Recently, several end-to-end models using neural networks showed high performance in Englis"
N19-3012,W17-4746,0,0.112625,"Missing"
N19-3012,P17-1175,0,0.152748,"Missing"
N19-3012,W14-3348,0,0.0169461,"K´ad´ar, 2017) model. “+ pretrained” models are initialized with pretrained embeddings. 3.2 Model The model is implemented using nmtpytorch toolkit v3.0.04 (Caglayan et al., 2017b). The shared encoder has 256 hidden dimensions, and therefore the bidirectional GRU has 512 dimensions. The decoder in NMT model has 256 hidden dimension. The input word embedding size and output vector size is 300 each. The latent space vector size is 2,048. We used the Adam optimizer with learning rate of 0.0004. The gradient norm is clipped to 1.0. The dropout rate is 0.3. BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) are used as performance metrics. We also evaluated the models using the F-score of each word; this shows how accurately each word is translated into target sentences, as was proposed in Kumar and Tsvetkov (2019). The F-score is calculated as the harmonic mean of the precision (fraction of produced sentences with a word that is in the references sentences) and the recall (fraction of reference sentences with a word that is in model outputs). We ran the experiment three times with different random seeds and obtained the mean and variance for each model. To clarify the effect of pretrained embed"
N19-3012,W16-3210,0,0.238002,"Missing"
N19-3012,I17-1014,0,0.171316,"Missing"
N19-3012,W18-6439,0,0.183757,"Missing"
N19-3012,W18-6441,0,0.171639,"Missing"
N19-3012,P15-1027,0,0.0236279,"(ˆ ej , e(w))} (3) v ′ =v (7) where v is the latent vector of the paired image; v ′ , the image vector for other examples; and α, the margin that adjusts the sparseness of each vector in the latent space3 . w∈V 3 Experiment where sj , e ˆj , and yˆj are the hidden state of the decoder, predicted embedding, and system output, respectively, for each timestep j in the decoding process. e(w) is the pretrained word embedding for a target word w. d is a distance function that is used to calculate the word similarity. Wo and bo are parameters of the output layer. We adopt margin-based ranking loss (Lazaridou et al., 2015) as the loss function 3.1 Dataset We train, validate, and test our model with the Multi30k (Elliott et al., 2016) dataset published in the WMT17 Shared Task. We choose French as the source language and English as the target one. The vocabulary size of both the source and the target languages is 10,000. 2 1 (6) i 3 We use λ = 0.01 in the experiment. 87 We use γ = 0.5 in the experiment. We use α = 0.1 in our experiment. Following Kumar and Tsvetkov (2019), byte pair encoding (Sennrich et al., 2016) is not applied. The source and target sentences are preprocessed with lower-casing, tokenizing and"
N19-3012,P02-1040,0,0.105506,"on of the IMAGINATION (Elliott and K´ad´ar, 2017) model. “+ pretrained” models are initialized with pretrained embeddings. 3.2 Model The model is implemented using nmtpytorch toolkit v3.0.04 (Caglayan et al., 2017b). The shared encoder has 256 hidden dimensions, and therefore the bidirectional GRU has 512 dimensions. The decoder in NMT model has 256 hidden dimension. The input word embedding size and output vector size is 300 each. The latent space vector size is 2,048. We used the Adam optimizer with learning rate of 0.0004. The gradient norm is clipped to 1.0. The dropout rate is 0.3. BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) are used as performance metrics. We also evaluated the models using the F-score of each word; this shows how accurately each word is translated into target sentences, as was proposed in Kumar and Tsvetkov (2019). The F-score is calculated as the harmonic mean of the precision (fraction of produced sentences with a word that is in the references sentences) and the recall (fraction of reference sentences with a word that is in model outputs). We ran the experiment three times with different random seeds and obtained the mean and variance for each model. To"
N19-3012,N18-2084,0,0.124287,"Yamagishi and Yukio Matsumura and Mamoru Komachi Tokyo Metropolitan University {tosho.hirasawa@, yamagishi-hayahide@ed., matsumura-yukio@ed., komachi@}tmu.ac.jp Abstract models. Data augmentation aims to deal with the fact that the size of available datasets for multimodal translation is quite small. To alleviate this problem, parallel corpora without a visual source (Elliott and K´ad´ar, 2017; Gr¨onroos et al., 2018) and pseudo-parallel corpora obtained using back-translation (Helcl et al., 2018) are used as additional learning resources. Due to the availability of parallel corpora for NMT, Qi et al. (2018) suggested that initializing the encoder with pretrained word embedding improves the translation performance in low-resource language pairs. Recently, Kumar and Tsvetkov (2019) proposed an NMT model that predicts the embedding of output words and searches for the output word instead of calculating the probability using the softmax function. This model performed as well as conventional NMT, and it significantly improved the translation accuracy for rare words. In this study, we introduce an NMT model with embedding prediction for multimodal machine translation that fully uses pretrained embeddi"
N19-3012,P16-1162,0,0.0913763,"ord similarity. Wo and bo are parameters of the output layer. We adopt margin-based ranking loss (Lazaridou et al., 2015) as the loss function 3.1 Dataset We train, validate, and test our model with the Multi30k (Elliott et al., 2016) dataset published in the WMT17 Shared Task. We choose French as the source language and English as the target one. The vocabulary size of both the source and the target languages is 10,000. 2 1 (6) i 3 We use λ = 0.01 in the experiment. 87 We use γ = 0.5 in the experiment. We use α = 0.1 in our experiment. Following Kumar and Tsvetkov (2019), byte pair encoding (Sennrich et al., 2016) is not applied. The source and target sentences are preprocessed with lower-casing, tokenizing and normalizing the punctuation. Visual features are extracted using pretrained ResNet (He et al., 2016). Specifically, we encode all images in Multi30k with ResNet-50 and pick out the hidden state in the pool5 layer as a 2,048dimension visual feature. We calculate the centroid of visual features in the training dataset as the bias vector and subtract the bias vector from all visual features in the training, validation and test datasets. val BLEU BLEU + pretrained 50.83 52.05 51.03 52.40 51.00±.37 5"
N19-3012,Q17-1010,0,\N,Missing
N19-3012,L18-1550,0,\N,Missing
ogiso-etal-2012-unidic,W04-3230,1,\N,Missing
ogiso-etal-2012-unidic,den-etal-2008-proper,1,\N,Missing
ogiso-etal-2012-unidic,maekawa-etal-2010-design,1,\N,Missing
P09-2048,P08-1003,0,0.0516046,"Missing"
P09-2048,D08-1061,0,0.0761492,"Missing"
P09-2048,I08-1047,1,\N,Missing
P09-2048,N04-1041,0,\N,Missing
P11-2006,I08-1065,1,0.855014,"AT i Scale p so that the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: return v 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed vector) is an ndime"
P11-2006,J04-3004,0,0.0254462,"s and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. 1 Introduction Bootstrapping (Yarowsky, 1995; Abney, 2004) is a technique frequently used in natural language processing to expand limited resources with minimal supervision. Given a small amount of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrappi"
P11-2006,P94-1020,0,0.0126978,"Missing"
P11-2006,W03-0407,0,0.0770922,"acterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrele"
P11-2006,W99-0613,0,0.0704383,"h often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photo"
P11-2006,C92-2082,0,0.0559469,"of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instance"
P11-2006,P10-1135,0,0.0168595,"at the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: return v 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed vector) is an ndimensional binary vector w"
P11-2006,I08-1047,1,0.820481,"r t = 1, 2, ..., τ do p ← AT i Scale p so that the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: return v 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: clustering to find competing semantic classes (negative categories). 2.3 Espresso Espresso (Pantel and Pennacchiotti, 2006) is one of the state-of-the-art bootstrapping algorithms used in many natural language tasks (Komachi and Suzuki, 2008; Abe et al., 2008; Ittoo and Bouma, 2010; Yoshida et al., 2010). Espresso takes advantage of pointwise mutual information (pmi) (Manning and Sch¨utze, 1999) between instances and patterns to evaluate their reliability. Let n be the number of all instances in the corpus, and p the number of all possible patterns. We denote all pmi values as an n × p instance-pattern matrix A, with the (i, j) element of A holding the value of pmi between the ith instance and the jth pattern. Let AT denote the matrix transpose of A. Algorithm 1 shows the pseudocode of Espresso. The input vector i0 (called seed v"
P11-2006,D08-1106,1,0.958751,"be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espresso has the same root as topic drift (Bharat and Henzinger, 1998) observed with HITS, noting the algorithmic similarity between them. While Komachi et al. proposed to use algorithms different from Espresso to Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30–36, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics avoid semantic drift, in this paper we take advantage of this similarity to make better use of Espresso. We demonstrate the effectiveness of our approa"
P11-2006,H93-1051,0,0.201279,"d we can expect the most frequent sense in the corpus to form the highest HITS ranking instances. This allows us to completely automate our experiments, without the need to manually check the HITS ranking in Step 2 of Section 3.2. That is, for the most frequent sense (majority sense), we take Step 3a and use the highest ranked instances as seeds; for the rest of the senses (minority senses), we take Step 3b and use them as the stop list. 4.1 Datasets We used the seven most frequent polysemous nouns (arm, bank, degree, difference, paper, party and shelter) in the S ENSEVAL-3 dataset, and line (Leacock et al., 1993) and interest (Bruce and Wiebe, Task Method MAP AUC R-Precision P@30 P@50 P@100 arm Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS Random HITS 84.3 ±4.1 85.9 74.8 ±6.5 84.8 69.4 ±3.0 62.4 48.3 ±3.8 50.2 75.2 ±4.1 75.2 79.1 ±5.0 85.2 74.9 ±2.3 77.0 44.5 ±15.1 72.2 64.9 ±8.3 75.3 59.6 ±8.1 59.7 61.6 ±9.6 77.6 54.3 ±4.2 49.3 54.5 ±5.0 60.1 56.4 ±7.1 61.0 57.0 ±9.7 68.2 51.5 ±3.3 54.6 36.3 ±16.9 68.6 64.9 ±12.0 83.0 80.9 ±2.2 79.3 72.6 ±4.5 78.0 66.7 ±2.3 63.2 47.0 ±4.4 51.1 71.6 ±3.3 75.2 76.6 ±3.1 78.5 73.2 ±1.3 72.0 40.1 ±14.6 68.5 63.7 ±10.2 80."
P11-2006,N06-1020,0,0.0271799,"g data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire freque"
P11-2006,P09-1045,0,0.0938664,"2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration would likely acquire frequent words co-occurring with these generic patterns, such as Michael Jackson. Previous work has tried to reduce the effect of semantic drift by making the stop list of instances that must not be extracted (Curran et al., 2007; McIntosh and Curran, 2009). Drift can also be reduced with carefully selected seeds. However, both of these approaches require expert knowledge. In this paper, we propose a graph-based approach to seed selection and stop list creation for the stateof-the-art bootstrapping algorithm Espresso (Pantel and Pennacchiotti, 2006). An advantage of this approach is that it requires zero or minimal supervision. The idea is to use the hubness score of instances and patterns computed from the pointwise mutual information matrix with the HITS algorithm (Kleinberg, 1999). Komachi et al. (2008) pointed out that semantic drift in Espr"
P11-2006,D10-1035,0,0.0171502,"e stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To apply the idea of multi-class bootstrapping to singleclass bootstrapping, one has to first find appropriate competing semantic classes and good seeds for them, which is in itself a difficult problem. Along this line of research, McIntosh (2010) recently used 31 Algorithm 1 Espresso algorithm Input: Seed vector i0 Instance-pattern co-occurrence matrix A Instance cutoff parameter k Pattern cutoff parameter m Number of iterations τ Output: Instance score vector i Pattern score vector p function E SPRESSO(i0 , A, k, m, τ) i ← i0 for t = 1, 2, ..., τ do p ← AT i Scale p so that the components sum to one. p ← S ELECT KB EST(p, k) i ← Ap Scale i so that the components sum to one. i ← S ELECT KB EST(i, m) return i and p function S ELECT KB EST(v, k) Retain only the k largest components of v, resetting the remaining components to 0. 20: retu"
P11-2006,P06-1015,0,0.808699,"of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of th"
P11-2006,N03-1031,0,0.0317941,"pply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrapping might learn, at one point of the iteration, patterns like “pictures of X” and “photos of X,” which also co-occur with many irrelevant instances. In this case, a later iteration wou"
P11-2006,W02-1028,0,0.0429194,"particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Many bootstrapping algorithms have been proposed for a variety of tasks: word sense disambiguation (Yarowsky, 1995; Abney, 2004), information extraction (Hearst, 1992; Riloff and Jones, 1999; Thelen and Riloff, 2002; Pantel and Pennacchiotti, 2006), named entity recognition (Collins and Singer, 1999), part-of-speech tagging (Clark et al., 2003), 30 and statistical parsing (Steedman et al., 2003; McClosky et al., 2006). Bootstrapping algorithms, however, are known to suffer from the problem called semantic drift: as the iteration proceeds, the algorithms tend to select instances increasingly irrelevant to the seed instances (Curran et al., 2007). For example, suppose we want to collect the names of common tourist sites from a web corpus. Given seed instances {New York City, Maldives Islands}, bootstrappin"
P11-2006,C02-1154,0,0.0262596,"as et al., 2009). Vyas et al. (2009) studied the impact of the composition of the seed sets on the expansion performance, confirming that seed set composition has a significant impact on the quality of expansions. They also found that the seeds chosen by non-expert editors are often worse than randomly chosen ones. A similar observation was made by McIntosh and Curran (2009), who reported that randomly chosen seeds from the gold-standard set often outperformed seeds chosen by domain experts. These results suggest that even for humans, selecting good seeds is a non-trivial task. 2.2 Stop Lists Yangarber et al. (2002) proposed to run multiple bootstrapping sessions in parallel, with each session trying to extract one of several mutually exclusive semantic classes. Thus, the instances harvested in one bootstrapping session can be used as the stop list of the other sessions. Curran et al. (2007) pursued a similar idea in their Mutual Exclusion Bootstrapping, which uses multiple semantic classes in addition to hand-crafted stop lists. While multi-class bootstrapping is a clever way to reduce human supervision in stop list construction, it is not generally applicable to bootstrapping for a single class. To app"
P11-2006,P95-1026,0,0.271556,"ecting good seeds and creating stop lists are two effective ways to reduce semantic drift, but these methods generally need human supervision. In this paper, we propose a graphbased approach to helping editors choose effective seeds and stop list instances, applicable to Pantel and Pennacchiotti’s Espresso bootstrapping algorithm. The idea is to select seeds and create a stop list using the rankings of instances and patterns computed by Kleinberg’s HITS algorithm. Experimental results on a variation of the lexical sample task show the effectiveness of our method. 1 Introduction Bootstrapping (Yarowsky, 1995; Abney, 2004) is a technique frequently used in natural language processing to expand limited resources with minimal supervision. Given a small amount of sample data (seeds) representing a particular semantic class of interest, bootstrapping first trains a classifier (which often is a weighted list of surface patterns characterizing the seeds) using the seeds, and then apply it on the remaining data to select instances most likely to be of the same class as the seeds. These selected instances are added to the seed set, and the process is iterated until sufficient labeled data are acquired. Ma"
P12-2039,P06-1032,0,0.461086,"*go to KAIYUKAN 1 with my friends. Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this case, we can exploit global context to determine which correction is appropriate: the first sentence describes a past event, and the second sentence refers the first sentence. Thus, the verb should be changed to past tense. This deduction is easy for humans, but is difficult for machines. One way to incorporate such global context into tens"
P12-2039,W07-1604,0,0.0262177,"n Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this case, we can exploit global context to determine which correction is appropriate: the first sentence describes a past event, and the second sentence refers the first sentence. Thus, the verb should be changed to past tense. This deduction is easy for humans, but is difficult for machines. One way to incorporate such global context into tense/aspect error correction is to use a machine"
P12-2039,P08-1021,0,0.0537025,"Missing"
P12-2039,I11-1017,1,0.695233,"(Lafferty, 2001), which provides state-of-the-art performance in sequence labeling while allowing flexible feature design for combining local and global feature sets. 3.1 Local Features Table 1 shows the local features used to train the error correction model. 2 Konan-JIEM Learner Corpus Second Edition (http:// gsk.or.jp/catalog/GSK2011-B/catalog.html) contains 170 essays, and Cambridge English First Certificate in English (http://www.cambridgeesol.org/exams/ fce/index.html) contains 1244 essays. 3 http://lang-8.com/ 4 As of January, 2012. More details about the Lang-8 corpus can be found in (Mizumoto et al., 2011). 5 Note that not all the 750,000 verb phrases were corrected due to the misuse of tense/aspect. 199 name t-learn bare L R nsubj dobj aux pobj p-tmod norm-p-tmod advmod conj main-clause sub-clause description tense/aspect written by the learner (surface tense/aspect) the verb lemma the word to the left the word to the right nominal subject direct object auxiliary verb object of a preposition temporal adverb normalized temporal adverb other adverb subordinating conjunction true if the target VP is in main clause true if the target VP is in subordinate clause We use dependency relations such as"
P12-2039,C10-2103,0,0.0218078,"n order to take global information into account, we regard the task as sequence labeling: each verb phrase in a document is labeled with tense/aspect depending on surrounding labels. Our experiments show that the global context makes a moderate contribution to tense/aspect error correction. 1 (1) I had a good time this Summer Vacation. First, I *go to KAIYUKAN 1 with my friends. Introduction Because of the growing number of learners of English, there is an increasing demand to help learners of English. It is highly effective for learners to receive feedback on their essays from a human tutor (Nagata and Nakatani, 2010). However, manual feedback needs a lot of work and time, and it also requires much grammatical knowledge. Thus, a variety of automatic methods for helping English learning and education have been proposed. The mainstream of English error detection and correction has focused on article errors (Knight and Chander, 1994; Brockett et al., 2006) and preposition errors (Chodorow et al., 2007; Rozovskaya and In this example, go in the second sentence should be written as went. It is difficult to correct this type of error because there are two choices for correction, namely went and will go. In this"
P12-2039,P11-1093,0,0.165612,"Missing"
P13-2043,2012.eamt-1.60,0,0.00669511,"Missing"
P13-2043,D11-1010,0,0.061191,"ill-in-the-blank quiz, where (a) blaming is the answer and (b) accusing is a distractor. There are previous studies on distractor generation for automatic fill-in-the-blank quiz generation (Mitkov et al., 2006). Hoshino and Nakagawa (2005) randomly selected distractors from words in the same document. Sumita et al. (2005) used an English thesaurus to generate distractors. Liu et al. (2005) collected distractor candidates that are close to the answer in terms of word-frequency, and ranked them by an association/collocation measure between the candidate and surrounding words in a given context. Dahlmeier and Ng (2011) generated candidates for collocation error correction for English as a Second Language (ESL) writing using paraphrasing with native language (L1) pivoting technique. This method takes an sentence containing a collocation error as input and translates it into L1, and then translate it back to English to generate correction candidates. Although the purpose is different, the technique is also applicable for distractor generation. To our best knowledge, there have not been studies that fully employed actual errors made by ESL learners for distractor generation. In this paper, we propose automated"
P13-2043,W05-0203,0,0.01914,"Missing"
P13-2043,W05-0201,0,0.508229,"Missing"
P13-2043,W05-0210,0,0.153225,"nly word B is correct, (c) both are correct. The source sentences to generate a quiz are collected from VOA, which are not included in the training dataset of the DiscSimESL. We generate 50 quizzes using different sentences per each method to avoid showing the same sentence multiple times to participants. We randomly ordered the quizzes generated by different methods for fair comparison. We compare the proposed methods to two baselines implementing previous studies: Thesaurusbased Method (THM) and Roundtrip Translation Method (RTM). Table 2 shows a summary of each method. The THM is based on (Sumita et al., 2005) and extract distractor candidates from synonyms of the target extracted from WordNet10 . The RTM is based on (Dahlmeier and Ng, 2011) and extracts distractor candidates from roundtrip (pivoting) translation lexicon constructed from the WIT3 corpus (Cettolo et al., 2012)11 , which covers a wide variety of topics. We build EnglishJapanese and Japanese-English word-based translation tables using GIZA++ (IBM Model4). In this dictionary, the target word is translated into Japanese words and they are translated back to English as distractor candidates. To consider (local) context, the candidates ge"
P13-2124,P05-1074,0,0.0745339,"Missing"
P13-2124,C12-1038,0,0.0502083,"Missing"
P13-2124,P07-1033,0,0.164947,"Missing"
P13-2124,D10-1104,0,0.0295498,"ith a variety of resources used for creating candidate sets. • WordNet Candidates are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3.0. • LearnerSmall Candidates are retrieved from following learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10 . • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based candidate sets, similar to (Liu et al., 2010) and (Liu et al., 2011). • Lang-8 The proposed candidate sets obtained from a large scale learner corpus. • Lang-8+DA Lang-8 candidate sets with domain adaptaExperiments Performance of verb suggestion is evaluated on two error-tagged learner corpora: CLC-FCE and KJ. In the experiments, we assume that the target verb and its context for suggestion are already given. For the experiment on the CLC-FCE dataset, the targets are all words tagged with “RV” (re10 Our roundtrip translation lexicons are built using a subset of the WIT3 corpus (Cettolo et al., 2012), which is available 9 https://github.c"
P13-2124,I11-1017,1,0.872798,"th the CLC-FCE and the KJ corpora used for testing. The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Suggestion Model Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83 . An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released the dataset for research purpose4 . First, we performed POS tagging for the dataset using the treebank POS tagger in the NLTK toolkit 2.10. Second, we extracted the correction pairs which have “VB*” tag. The set of correction pairs given an incorrect verb is considered as a candidate set for the verb. We then performed the following preprocessing for the dataset because we focus on lexical selection of verbs: • Lemmatize verbs to reduce da"
P13-2124,P11-1093,0,0.0215626,"nk task, the candidate sets and domain adaptation can be applied to this task to take the original word into account. The model is trained on a huge native corpus, namely the ukWaC corpus, because the data-size of learner corpora is limited compared to native corpora. It is then adapted to the target domain, i.e., learner writing. In our experiment, the Lang8 corpus is used as the target domain corpus, since we assume that it shares the same characteristics with the CLC-FCE and the KJ corpora used for testing. The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Suggestion Model Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83 . An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the s"
P13-2124,C10-1133,0,0.0640981,"Missing"
P13-2124,P10-2021,0,0.117977,"and domain adaptation can be applied to this task to take the original word into account. The model is trained on a huge native corpus, namely the ukWaC corpus, because the data-size of learner corpora is limited compared to native corpora. It is then adapted to the target domain, i.e., learner writing. In our experiment, the Lang8 corpus is used as the target domain corpus, since we assume that it shares the same characteristics with the CLC-FCE and the KJ corpora used for testing. The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Suggestion Model Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83 . An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released"
P13-2124,J92-4003,0,\N,Missing
P13-2124,P12-2076,0,\N,Missing
P13-2124,2012.eamt-1.60,0,\N,Missing
P15-1160,D11-1145,1,0.808815,"suggest that our task is independent of the type of disease/symptom. 1 Introduction Social media services, including Twitter and Facebook, provide opportunities for individuals to share their experiences, thoughts, and opinions. The wide use of social media services has led to the emergence of new approaches for surveying the population and addressing social issues. One popular application of social media data is flu surveillance, i.e., predicting the outbreak of influenza epidemics by detecting mentions of flu infections on social media platforms (Culotta, 2010; Lampos and Cristianini, 2010; Aramaki et al., 2011; Paul and Dredze, 2011; Signorini et al., 2011; Collier, 2012; Dredze et al., 2013; Gesualdo et al., 2013; Stoové and Pedrana, 2014). Previous studies mainly relied on shallow textual clues in Twitter posts in order to predict the number of flu infections, e.g., the number of occurrences of specific keywords (such as “flu” or “influenza”) on Twitter. However, such a simple approach can lead to incorrect predictions. Broniatowski et al. (2013) argued that media attention increases chatter, i.e., the number of tweets that mention the flu without the poster being actually infected. Examples incl"
P15-1160,N13-1121,0,0.066141,"Missing"
P15-1160,J96-2004,0,0.0431357,"Missing"
P15-1160,P14-2111,0,0.0261289,"symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and"
P15-1160,P10-1160,0,0.0932612,"the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the verb “beat” is the first person “I” in the sent"
P15-1160,P11-2008,0,0.131837,"Missing"
P15-1160,P11-1038,0,0.037841,"we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), even"
P15-1160,P13-4002,0,0.0262922,"cted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of"
P15-1160,N13-1097,0,0.258011,"Missing"
P15-1160,D14-1214,0,0.0348142,"nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro"
P15-1160,P14-1016,0,0.0322227,"nner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to Pro"
P15-1160,E12-1062,0,0.0297451,", first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun"
P15-1160,W04-2705,0,0.134012,"challenges in this study. 2. The experimental results show that the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example,"
P15-1160,J05-1004,0,0.0123815,"r profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our wo"
P15-1160,C14-1168,0,0.0255805,"ably. This is because the subjects for these symptoms are mostly F IRST P ERSON, as we seldom mention the symptoms of another person in such cases. In other words, the episode classifier can predict a positive label for these symptoms without knowing the subjects of these symptoms. 4 Related Work 4.1 Twitter and NLP NLP researchers have addressed two major directions for Twitter: adapting existing NLP technologies to noisy texts and extracting useful knowledge from Twitter. The former includes improving the accuracy of part-of-speech tagging (Gimpel et al., 2011) and named entity recognition (Plank et al., 2014), as well as normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock ma"
P15-1160,C04-1174,0,0.0455393,"tudy. 2. The experimental results show that the task of identifying subjects is independent of the type of diseases/symptom. We verify the possibility of transferring supervision data to different targets of diseases and symptoms. In other words, we verify that it is possible to utilize the supervision data for a particular disease/symptom to improve the accuracy of predicting subjects of another disease/symptom. 1. Novel task setting. The task of identifying the subject of a disease/symptom is similar to predicate-argument structure (PAS) analysis for nominal predicates (Meyers et al., 2004; Sasano et al., 2004; Komachi et al., 2007; Gerber and Chai, 2010). However, these studies do not treat diseases (e.g., “influenza”) and symptoms (e.g., “headache”) as nominal predicates. To the best of our knowledge, this task has not been explored in natural language processing (NLP) thus far. 2. Identifying whether the subject has a disease/symptom. Besides the work on PAS analysis for nominal predicates, the most relevant work is PAS analysis for verb predicates. However, our task is not as simple as predicting the subject of the verb governing a disease/symptom-related noun. For example, the subject of the v"
P15-1160,C08-1097,0,0.0123551,"opBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1 (typically, objects, patients, themes) is “issue” for the nominal predicate “complaints” in the sentence “There have been no customer complaints about that issue.” Gerber and Chai (2010) improved the coverage of NomBank by handling implicit arguments. Some studies have addressed the task of identifying implicit and omitted arguments for nominal predicates in Japanese (Komachi et al., 2007; Sasano et al., 2008). Our work shares a similar goal with the abovementioned studies, i.e., identifying an implicit ARG 0 for a disease and symptom. However, these studies do not regard a disease/symptom as a nominal predicate because they consider verb nominalizations as nominal predicates. In addition, 1667 they use a corpus that consists of newswire text, the writing style and word usage of which differ considerably from those of tweets. For these reasons, we proposed a novel task setting for identifying subjects of diseases and symptoms, and we built an annotated corpus for developing the subject classifier a"
P15-1160,N13-1135,0,0.0604966,"Missing"
P15-1160,P13-2005,0,0.0142294,"normalizing ill-formed words into canonical forms (Han and Baldwin, 2011; Chrupała, 2014). Even though we did not incor7 For the “predicted” setting, first, we predicted the subject labels in a similar manner to five-fold cross validation, and we used the predicted labels as features for the episode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be"
P15-1160,P12-2044,0,0.0303353,". The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates and their arguments are identified: for example, ARG 0 (typically, subject or agent) is “customer” and ARG 1"
P15-1160,P14-2114,0,0.0245499,"ode classifier. porate the findings of these studies, they could be beneficial to our work in the future. The latter has led to the development of several interesting applications besides health surveillance. These include prediction of future revenue (Asur and Huberman, 2010) and stock market trends (Si et al., 2013), mining of public opinion (O’Connor et al., 2010), event extraction and summarization (Sakaki et al., 2010; Thelwall et al., 2011; Marchetti-Bowick and Chambers, 2012; Shen et al., 2013; Li et al., 2014a), user profiling (Bergsma et al., 2013; Han et al., 2013; Li et al., 2014b; Zhou et al., 2014), disaster management (Varga et al., 2013), and extraction of common-sense knowledge (Williams and Katz, 2012). Our work can directly contribute to these applications, e.g., sentiment analysis, user profiling, event extraction, and disaster management. 4.2 Semantic analysis for nouns Our work can be considered as a semantic analysis that identifies an argument (subject) for a disease/symptom-related noun. NomBank (Meyers et al., 2004) provides annotations of noun arguments in a similar manner to PropBank (Palmer et al., 2005), which provides annotations of verbs. In NomBank, nominal predicates"
P15-1160,P15-3005,1,\N,Missing
P15-1160,P13-1159,0,\N,Missing
P15-3005,D11-1145,1,0.801339,"nalysis: shallow modality analysis based on a surface string match and deep modality analysis based on predicate-argument structure analysis. The main contribution of this paper is two-fold: Consequently, this study proposes the use of modality features to improve disease event detection from Twitter messages, or “tweets”. Experimental results demonstrate that the combination of a modality dictionary and a modality analyzer improves the F1-score by 3.5 points. 1 • We annotate a new dataset extracted from Twitter for flu detection and prediction task, and extend the naïve bag-of-words model of Aramaki et al. (2011) and propose several Twitter-specific features for disease event detection tasks. Introduction The rapidly increasing popularity of Social Networking Services (SNSs) such as Twitter and Facebook has greatly eased the dissemination of information. Such data can serve as a valuable information resource for various applications. For instance, Huberman et al. (2009) investigated actual linked structures of human networks, Boyd et al. (2010) mapped out retweeting as a conversational practice, and Sakaki et al. (2010) detected earthquakes using SNSs. An important and widespread application of SNS mi"
P15-3005,P15-1160,1,0.513046,"Missing"
P15-3005,D14-1214,0,0.0613251,"a from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) reported that half of the tweets containing the word “cold (disease)” simply mention some information about a disease, but do not refer to the actual eventuality of having the disease. To address that problem, a classifier was produced to ascertain the factuality of the disease event. This paper follows that approach, using modality analysis, which provides a strong clue for factuality analysis (Saurí and Pustejovsky, 2012). Modality has been used and discussed in various places. Li et al. (2014) employ such modality features, although they do not describe the effect of using modality features in web application tasks. Furthermore, several workshops have been organized around the use of specific modalities, such as Negation and Speculation (e.g. NeSPNLP1 ). In this study, we use generic modality features to improve factuality analysis. 1 3.2 Shallow modality feature In Japanese, multiple words can serve as a function word as a whole (Matsuyoshi et al., 2007). We designate them as “functional expressions.” Even though functional expressions often carry modality information, previous wo"
P15-3005,matsuyoshi-etal-2010-annotating,0,0.0757664,"ies (Aramaki et al., 2011) 2 . If a tweet writer (or anybody near the writer) is infected with influenza, then the label is positive. Otherwise, the label is negative. Additionally, we save the time stamp when the tweet was posted online. Table 1 presents some examples. For this study, we use 10,443 Japanese tweet messages including the word “flu.” In this dataset, the number of positive examples is 1,319; the number of negative examples is 9,124. Because language heavily relies on modality to judge the factuality of sentences, modality analysis is a necessary process for factuality analysis (Matsuyoshi et al., 2010b). In line with this observation, we propose two ways to incorporate modality analysis for factuality analysis. Twitter is the SNS that is most frequently used for influenza detection (Achrekar et al., 2012; Aramaki et al., 2011; Ji et al., 2012; Sadilek et al., 2012; Lamb, 2013). Previous research on the subject has revealed a high correlation ratio between the number of influenza patients and actual tweets related to influenza. It is possible to obtain large amounts of data from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) repor"
P15-3005,J12-2002,0,0.0273849,"and actual tweets related to influenza. It is possible to obtain large amounts of data from Twitter texts, but the main challenge is to filter noise from this data. For example, Aramaki et al. (2011) reported that half of the tweets containing the word “cold (disease)” simply mention some information about a disease, but do not refer to the actual eventuality of having the disease. To address that problem, a classifier was produced to ascertain the factuality of the disease event. This paper follows that approach, using modality analysis, which provides a strong clue for factuality analysis (Saurí and Pustejovsky, 2012). Modality has been used and discussed in various places. Li et al. (2014) employ such modality features, although they do not describe the effect of using modality features in web application tasks. Furthermore, several workshops have been organized around the use of specific modalities, such as Negation and Speculation (e.g. NeSPNLP1 ). In this study, we use generic modality features to improve factuality analysis. 1 3.2 Shallow modality feature In Japanese, multiple words can serve as a function word as a whole (Matsuyoshi et al., 2007). We designate them as “functional expressions.” Even t"
P16-3001,I13-1110,0,0.0154535,"d which size is 21,700 ((2010 + 2330) × 5) rankings. Then, we calculate correlation between the accuracies of annotated data and either those of Kajiwara and Yamamoto (2015) or those of our dataset. 5.2.1 Lexical simplification systems We used several metrics for these experiments: Frequency Because it is said that a high frequent word is simple, most frequent word is selected as a simplification candidate from substitutes using uni-gram frequency of Japanese Web N-gram (Kudo and Kazawa, 2007). This uni-gram frequency is counted from two billion sentences in Japanese Web text. Number of Users Aramaki et al. (2013) claimed that a word used by many people is simple, so we pick the word used by the most of users. Number of Users were estimated from the Twitter corpus created by Aramaki et al. (2013). The corpus contains 250 million tweets from 100,000 users. 5.2 Extrinsic evaluation In this section, we evaluate our dataset using five simple lexical simplification methods. We calcu5 References Familiarity Assuming that a word which is known by many people is simple, replace a target word with substitutes according to the familiarity score using familiarity data constructed by Amano and Kondo (2000). The fa"
P16-3001,P14-2075,0,0.102896,"Missing"
P16-3001,P15-3006,1,0.653284,"Missing"
P16-3001,S07-1009,0,0.0312672,"ion for Computational Linguistics – Student Research Workshop, pages 1–7, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics sentence paraphrase list 「技を出し合い、気分が高揚するのがたまらない」とはいえ、技量で相手を上回りたい気持ちも強い。 Although using their techniques makes you feel exalted, I strongly feel I want to outrank my competitors in terms of skill. 盛り上がる 高まる 高ぶる 上がる 高揚する 興奮する 熱を帯びる 活性化する come alive raised, excited up exalted excited heated revitalized Figure 1: A part of the dataset of Kajiwara and Yamamoto (2015). notated on top of the evaluation dataset for English lexical substitution (McCarthy and Navigli, 2007). They asked university students to rerank substitutes according to simplification ranking. Sentences in their dataset do not always contain complex words, and it is not appropriate to evaluate simplification systems if a test sentence does not include any complex words. In addition, De Belder and Moens (2012) built an evaluation dataset for English lexical simplification based on that developed by McCarthy and Navigli (2007). They used Amazon’s Mechanical Turk to rank substitutes and employed the reliability of annotators to remove outlier annotators and/or downweight unreliable annotators. T"
P16-3001,S12-1046,0,0.19961,"vious datasets for evaluating systems with respect to correlation with human judgment. Introduction Lexical simplification is the task to find and substitute a complex word or phrase in a sentence with its simpler synonymous expression. We define complex word as a word that has lexical and subjective difficulty in a sentence. It can help in reading comprehension for children and language learners (De Belder and Moens, 2010). This task is a rather easier task which prepare a pair of complex and simple representations than a challenging task which changes the substitute pair in a given context (Specia et al., 2012; Kajiwara and Yamamoto, 2015). Construction of a benchmark dataset is important to ensure the reliability and reproducibility of evaluation. However, few resources are available for the automatic evaluation of lexical simplification. Specia et al. (2012) and De Belder and Moens (2010) created benchmark datasets for evaluating English lexical simplifica• The consistency of simplification ranking is greatly improved by allowing candidates to have ties and by considering the reliability of annotators. Our dataset is available at GitHub2 . 2 Related work The evaluation dataset for the English Lex"
P17-3007,D15-1075,0,0.0151488,"hod that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which only replaces content"
P17-3007,N15-1053,0,0.015866,"rase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which only replaces content words. For this reason, annotators classify paraphrase candidate pairs in our study similar to the method used in the TPC and previous studies on RTE. • Generated paraphrases using multiple machine tra"
P17-3007,P05-1074,0,0.357992,"bilingual parallel corpus are similar to our method. In fact, our method is an extension of previous studies that acquire paraphrases using manual translations of the same documents (Barzilay and McKeown, 2001; Pang et al., 2003). However, it is expensive to manually translate sentences to create large numbers of translation pairs. Thus, we propose a method that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that ann"
P17-3007,P01-1008,0,0.0991419,". To actively acquire negative instances, we use Wikipedia to randomly extract sentences. In general, it is rare for sentences to become paraphrase when sentence pairs are collected randomly, so it is effective to acquire negative instances in this regard. Our contributions are summarized as follows: candidates from any sentences, and this allows us to choose any domain required by an application. Methods using a bilingual parallel corpus are similar to our method. In fact, our method is an extension of previous studies that acquire paraphrases using manual translations of the same documents (Barzilay and McKeown, 2001; Pang et al., 2003). However, it is expensive to manually translate sentences to create large numbers of translation pairs. Thus, we propose a method that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many stud"
P17-3007,I05-5002,0,0.448344,"Missing"
P17-3007,D16-1025,0,0.0138668,"judge whether the candidate pairs are paraphrases. In this paper, we focus on the Japanese PI task and build a monolingual parallel corpus for its evaluation as there is no Japanese sentential paraphrase corpus available. As Figure 1 shows, we use phrase-based machine translation (PBMT) and neural machine translation (NMT) to generate two different Japanese sentences from one English sentence. We expect the two systems provide widely different translations with regard to surface form such as lexical variation and word order difference because they are known to have different characteristics (Bentivogli et al., 2016); for instance, PBMT produces more literal translations, whereas NMT produces more fluent translations. We believe that when the translation succeeds, the two Japanese sentences have the same meaning but different expressions, which is a positive instance. On the other hand, translated candidates can be negative instances when they include fluent mistranslations. This occurs since adequacy is not checked during an annotation phase. Thus, we can also acquire some negative instances in this manner. To actively acquire negative instances, we use Wikipedia to randomly extract sentences. In general"
P17-3007,P13-1158,0,0.0397279,"andidate sentence pairs using multiple machine translation systems. In the random extraction part, we extract candidate sentence pairs from a monolingual corpus. To collect both trivial and non-trivial instances, we sample candidate pairs Introduction When two sentences share the same meaning but are written using different expressions, they are deemed to be a sentential paraphrase pair. Paraphrase Identification (PI) is a task that recognizes whether a pair of sentences is a paraphrase. PI is useful in many applications such as information retrieval (Wang et al., 2013) or question answering (Fader et al., 2013). Despite this usefulness, there are only a few corpora that can be used to develop and evaluate PI systems. Moreover, such corpora are unavailable in many languages other than English. This is because manual paraphrase generation tends to cost a lot. Furthermore, unlike a bilingual parallel corpus for machine translation, a monolingual parallel corpus for PI cannot be spontaneously built. Even though some paraphrase corpora are available, there are some limitations on them. For example, the Microsoft Research Paraphrase Corpus 1 Non-trivial positive instances are difficult to identify as sema"
P17-3007,N13-1092,0,0.158714,"Missing"
P17-3007,P15-2070,0,0.0502663,"Missing"
P17-3007,P11-1109,0,0.0619616,"Missing"
P17-3007,rus-etal-2014-paraphrase,0,0.0305828,"use multiple machine translation systems to generate positive candidates and a monolingual corpus to extract negative candidates. To collect nontrivial instances, the candidates are uniformly sampled by word overlap rate. Finally, annotators judge whether the candidates are either positive or negative. Using this method, we built and released the first evaluation corpus for Japanese paraphrase identification, which comprises 655 sentence pairs. 1 Figure 1: Overview of candidate pair generation. (MSRP) (Dolan and Brockett, 2005) is a standardized corpus in English for the PI task. However, as Rus et al. (2014) pointed out, MSRP collects candidate pairs using short edit distance, but this approach is limited to collecting positive instances with a low word overlap rate (WOR) (non-trivial positive instances, hereafter)1 . In contrast, the Twitter Paraphrase Corpus (TPC) (Xu et al., 2014) comprises short noisy user-generated texts; hence, it is difficult to acquire negative instances with a high WOR (non-trivial negative instances, hereafter)2 . To develop a more robust PI model, it is important to collect both “non-trivial” positive and negative instances for the evaluation corpus. To create a useful"
P17-3007,W11-2123,0,0.0112219,"anding of sentences and that there are some ambiguous instances without context (e.g., good child and good kid), the score is considered to be sufficiently high. There were 89 disagreements, and the final label was decided by discussion. As a result, we acquired 363 positive and 102 negative machinetranslated pairs. We built the first evaluation corpus for Japanese PI using our method. We used Google Translate PBMT4 and NMT5 (Wu et al., 2016) to translate English sentences extracted from English Wikipedia 6 into Japanese sentences7 . We calculated the language model probabilities using KenLM (Heafield, 2011), and built a 5-gram language model from the English Gigaword Fifth Edition (LDC2011T07). Then we translated the top 500,000 sentences and sampled 200 pairs in the descending order of machine translation output quality for each range, except for the exact match pairs (Table 1). Although the machine translation part of our method successfully collected non-trivial positive instances, it acquired only a few non-trivial negative instances as we expected. To fill the gap between positive and negative in higher WOR, we randomly collected sentence pairs from Japanese Wikipedia8 and added 190 non-tri"
P17-3007,P15-3006,1,0.762812,"Missing"
P17-3007,P13-2008,0,0.0198718,"he machine translation part, we generate candidate sentence pairs using multiple machine translation systems. In the random extraction part, we extract candidate sentence pairs from a monolingual corpus. To collect both trivial and non-trivial instances, we sample candidate pairs Introduction When two sentences share the same meaning but are written using different expressions, they are deemed to be a sentential paraphrase pair. Paraphrase Identification (PI) is a task that recognizes whether a pair of sentences is a paraphrase. PI is useful in many applications such as information retrieval (Wang et al., 2013) or question answering (Fader et al., 2013). Despite this usefulness, there are only a few corpora that can be used to develop and evaluate PI systems. Moreover, such corpora are unavailable in many languages other than English. This is because manual paraphrase generation tends to cost a lot. Furthermore, unlike a bilingual parallel corpus for machine translation, a monolingual parallel corpus for PI cannot be spontaneously built. Even though some paraphrase corpora are available, there are some limitations on them. For example, the Microsoft Research Paraphrase Corpus 1 Non-trivial positive"
P17-3007,P16-3001,1,0.811346,"tent words. For this reason, annotators classify paraphrase candidate pairs in our study similar to the method used in the TPC and previous studies on RTE. • Generated paraphrases using multiple machine translation systems for the first time • Adjusted for a balance from two viewpoints: positive/negative and trivial/non-trivial • Released3 the first evaluation corpus for the Japanese PI task 2 Related Work As for Japanese, there exists a paraphrase database (Mizukami et al., 2014) and an evaluation dataset that includes some paraphrases for lexical simplification (Kajiwara and Yamamoto, 2015; Kodaira et al., 2016). They provide either lexical or phrase-level paraphrases, but we focus on collecting sentence-level paraphrases for PI evaluation. There is also an evaluation dataset for RTE (Watanabe et al., 2013) containing 70 sentential paraphrase pairs; however, as there is a limitation in terms of size, we aim to build a larger corpus. Paraphrase acquisition has been actively studied. For instance, paraphrases have been acquired from monolingual comparable corpora such as news articles regarding the same event (Shinyama et al., 2002) and multiple definitions of the same concept (Hashimoto et al., 2011)."
P17-3007,W16-2385,0,0.0156499,"on Systems 3.2 Non-Paraphrase Extraction from a Monolingual Corpus We use different types of machine translation systems (PBMT and NMT) to translate source sentences extracted from a monolingual corpus into a target language. This means that each source sentence has two versions in the target language, and we use the sentences as a pair. To avoid collecting ungrammatical sentences as much as possible, we use Quality Estimation and eliminate inappropriate sentences for paraphrase candidate pairs. At WMT2016 (Bojar et al., 2016) in the Shared Task on Quality Estimation, the winning system YSDA (Kozlova et al., 2016) shows that it is effective for Quality Estimation to employ language model probabilities of source and target sentences, and BLEU scores between the source sentence and back-translation. Therefore, we calculate the language model probabilities of source sentences and translate them in the order of their probabilities. To further obtain better translations, we select sentence pairs in the descending order of machine translation output quality, which is defined as follows: QEi = SBLEU(ei , BTPBMT (ei )) × SBLEU(ei , BTNMT (ei )) This extraction part of our method is for acquiring non-trivial ne"
P17-3007,marelli-etal-2014-sick,0,0.0321766,"Thus, we propose a method that inexpensively generates translations using machine translation and Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which"
P17-3007,Q16-1029,0,0.0254615,"Quality Estimation. Ganitkevitch et al. (2013) and Pavlick et al. (2015) also use a bilingual parallel corpora to build a paraphrase database using bilingual pivoting (Bannard and Callison-Burch, 2005). Their methods differ from ours in that they aim to acquire phrase level paraphrase rules and carry out word alignment instead of machine translation. There are also many studies on building a large scale corpora utilizing crowdsourcing in related tasks such as Recognizing Textual Entailment (RTE) (Marelli et al., 2014; Bowman et al., 2015) and Lexical Simplification (De Belder and Moens, 2012; Xu et al., 2016). Moreover, there are studies collecting paraphrases from captions to videos (Chen and Dolan, 2011) and images (Chen et al., 2015). One advantage of leveraging crowdsourcing is that annotation is done inexpensively, but it requires careful task design to gather valid data from non-expert annotators. In our study, we collect sentential paraphrase pairs, but we presume that it is difficult for nonexpert annotators to provide well-balanced sentential paraphrase pairs, unlike lexical simplification, which only replaces content words. For this reason, annotators classify paraphrase candidate pairs"
P17-3007,C12-1121,0,0.0238424,"ans the negative instances are not distinguishable, so this does not affect the balance of the corpus. 3.3 Balanced Sampling using Word Overlap Rate To collect both trivial and non-trivial instances, we carefully sample candidate pairs. We classify the pairs into eleven ranges depending on the WOR and sample pairs uniformly for each range, except for the exact match pairs. The WOR is calculated as follows: (1) Here, ei denotes the i-th source sentence, BTPBMT denotes the back-translation using PBMT, BTNMT denotes the back-translation using NMT, and SBLEU denotes the sentence-level BLEU score (Nakov et al., 2012). When this score is high, it indicates that the difference in sentence 38 Jaccard(TPBMT (ei ), TNMT (ei )) TPBMT (ei ) ∩ TNMT (ei ) (2) = TPBMT (ei ) ∪ TNMT (ei ) Label Positive Negative Unnatural Other Example Input: PBMT: NMT: Input: PBMT: NMT: Input: PBMT: NMT: Input: PBMT: NMT: My father was a very strong man. 私の父は非常に強い男でした。 父はとても強い男だった。 It is available as a generic medication. これは、一般的な薬として利用可能です。 ジェネリック医薬品として入手できます。 I want to wake up in the morning 私は午前中に目を覚ますしたいです* 私は朝起きたい Academy of Country Music Awards : アカデミーオブカントリーミュージックアワード： アカデミー・オブ・カントリー・ミュージック賞： My father was a very strong man."
P17-3007,Q14-1034,0,0.0214984,"positive or negative. Using this method, we built and released the first evaluation corpus for Japanese paraphrase identification, which comprises 655 sentence pairs. 1 Figure 1: Overview of candidate pair generation. (MSRP) (Dolan and Brockett, 2005) is a standardized corpus in English for the PI task. However, as Rus et al. (2014) pointed out, MSRP collects candidate pairs using short edit distance, but this approach is limited to collecting positive instances with a low word overlap rate (WOR) (non-trivial positive instances, hereafter)1 . In contrast, the Twitter Paraphrase Corpus (TPC) (Xu et al., 2014) comprises short noisy user-generated texts; hence, it is difficult to acquire negative instances with a high WOR (non-trivial negative instances, hereafter)2 . To develop a more robust PI model, it is important to collect both “non-trivial” positive and negative instances for the evaluation corpus. To create a useful evaluation corpus, we propose a novel paraphrase acquisition method that has two viewpoints of balancing the corpus: positive/negative and trivial/non-trivial. To balance between positive and negative, our method has a machine translation part collecting mainly positive instances"
P17-3007,N03-1024,0,0.561922,"Missing"
P17-3007,P11-1020,0,\N,Missing
P18-3016,I11-1017,1,0.82025,"f 150K. The mini-batch size is 100. Table 5: F0.5 of COMMON and DIFF outputs. 5 Genetic refers the chance of inheriting a disorder or disease . Genetic refers the chance of inheriting a disorder or disease . Genetic refers to the chance of inheriting a disorder or disease . Genetic risk refers to the chance of inheriting a disorder or disease . Grammatical Error Correction 5.1 Experimental setting The second experiment addresses GEC. We combine the FCE public dataset (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and English learner corpus from the Lang-8 learner corpus (Mizumoto et al., 2011) and remove sentences longer than 100 words to create a training corpus. From the Lang-8 learner corpus, we use only the pairs of erroneous and corrected sentences. We use 1,452,584 sentences as a training set (502,908 types on the encoder side and 639,574 types on the decoder side). We evaluate the models’ performances on the standard sets from the CoNLL-14 shared task (Ng et al., 2014) using CoNLL-13 data as a development set (1,381 sentences) and CoNLL-14 data as a test set (1,312 sentences)4 . We employ F0.5 as an evaluation measure for the CoNLL-14 shared task. We use the same model as in"
P18-3016,D14-1179,0,0.0148875,"Missing"
P18-3016,W13-1703,0,0.026349,"ess otherwise noted, we conduct an analysis of the model using the vocabulary size of 150K. The mini-batch size is 100. Table 5: F0.5 of COMMON and DIFF outputs. 5 Genetic refers the chance of inheriting a disorder or disease . Genetic refers the chance of inheriting a disorder or disease . Genetic refers to the chance of inheriting a disorder or disease . Genetic risk refers to the chance of inheriting a disorder or disease . Grammatical Error Correction 5.1 Experimental setting The second experiment addresses GEC. We combine the FCE public dataset (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and English learner corpus from the Lang-8 learner corpus (Mizumoto et al., 2011) and remove sentences longer than 100 words to create a training corpus. From the Lang-8 learner corpus, we use only the pairs of erroneous and corrected sentences. We use 1,452,584 sentences as a training set (502,908 types on the encoder side and 639,574 types on the decoder side). We evaluate the models’ performances on the standard sets from the CoNLL-14 shared task (Ng et al., 2014) using CoNLL-13 data as a development set (1,381 sentences) and CoNLL-14 data as a test set (1,312 sentences)4 . We employ F0.5"
P18-3016,W14-1701,0,0.0132081,"ting The second experiment addresses GEC. We combine the FCE public dataset (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and English learner corpus from the Lang-8 learner corpus (Mizumoto et al., 2011) and remove sentences longer than 100 words to create a training corpus. From the Lang-8 learner corpus, we use only the pairs of erroneous and corrected sentences. We use 1,452,584 sentences as a training set (502,908 types on the encoder side and 639,574 types on the decoder side). We evaluate the models’ performances on the standard sets from the CoNLL-14 shared task (Ng et al., 2014) using CoNLL-13 data as a development set (1,381 sentences) and CoNLL-14 data as a test set (1,312 sentences)4 . We employ F0.5 as an evaluation measure for the CoNLL-14 shared task. We use the same model as in Section 4.1 as a neural model for GEC. The models’ parameter settings are similar to the MT experiment, except for the vocabulary and batch sizes. In this experiment, we set the vocabulary size on the encoder and decoder sides to 150K and 50K, respectively. Ad5.2 Result Table 4 shows the performance of the baseline and proposed method. The PPMI model improves precision and recall; it ac"
P18-3016,N13-1073,0,0.055003,"Missing"
P18-3016,P02-1040,0,0.100771,"ut is applied with a probability of 0.2. For this experiment, a bilingual dictionary is prepared for postprocessing unknown words (Jean et al., 2015). When the model outputs an unknown word token, the word with the highest attention score is used as a query to replace the unknown token with the corresponding word from the dictionary. If not in the dictionary, we replace the unknown word token with the source word (unk rep). This dictionary is created based on word alignment obtained using fast align (Dyer et al., 2013) on the training corpus. We evaluate translation results using BLEU scores (Papineni et al., 2002). 4.2 Results Table 1 shows the translation accuracy (BLEU scores) and p-value of a significance test (p &lt; 0.05) by bootstrap resampling (Koehn, 2004). The PPMI model improves translation accuracy by 0.56 points in Japanese-to-English translation, which is a significant improvement. Next, we examine differences in vocabulary by comparing each model with the baseline. Compared to the vocabulary of the baseline in 100K setting, Freq and PPMI replace 16,107 and 17,166 3 BLEU score for postprocessing (unk rep) improves by 0.46, 0.44, and 0.46 points in the baseline, Freq, and PPMI, respectively. 1"
P18-3016,W03-1028,0,0.0181337,"sufficiently large value. 3.2 Vocabulary selection using HITS Algorithms that rank words using cooccurrence are employed in many natural language processing tasks. For example, TextRank (Mihalcea and Tarau, 2004) uses PageRank (Brin and Page, 1998) for keyword extraction. TextRank constructs a word graph in which nodes represent words, and edges represent co-occurrences between words within a fixed window; TextRank then executes the PageRank algorithm to extract keywords. Although this is an unsupervised method, it achieves nearly the same precision as one state-of-the-art supervised method (Hulth, 2003). Kiso et al. (2011) used HITS (Kleinberg, 1999) to select seeds and create a stop list for bootstrapping in natural language processing. They reported significant improvements over a baseline method using unigram frequency. Their graph-based algorithm was effective at extracting the relevance between words, which cannot be grasped with a simple unigram frequency. In this study, we use HITS In this study, we create an adjacency matrix from a training corpus by considering a word as a node and the co-occurrence between words as an edge. Unlike in web pages, co-occurrence between words is nonbin"
P18-3016,J98-1004,0,0.789828,"Missing"
P18-3016,W15-3014,0,0.0965209,"models, Sennrich et al. (2016) applied byte pair encoding to words and created a partial character string set that could express all the words in the training data. They increased the number of words included in the vocabulary to enable the encoder-decoder model to robustly learn contextual information. In contrast, we aim to improve neural models by using vocabulary that is appropriate for a training corpus—not to improve neural models by increasing their vocabulary. to retrieve co-occurring words from a training corpus to reduce noise in the source text. 3 Graph-based Filtering of OOV Words Jean et al. (2015) proposed a method of replacing and copying an unknown word token with a bilingual dictionary in neural MT. They automatically constructed a translation dictionary from a training corpus using a word-alignment model (GIZA++), which finds a corresponding source word for each unknown target word token. They replaced the unknown word token with the corresponding word into which the source word was translated by the bilingual dictionary. Yuan and Briscoe (2016) used a similar method for neural GEC. Because our proposed method is performed as preprocessing, it can be used simultaneously with this r"
P18-3016,P16-1162,0,0.0210668,"ociation for Computational Linguistics Algorithm 1 HITS et al., 2016) and GEC (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017); hence, this study focuses on improving the simple attentional encoder-decoder models that are applied to these tasks. Require: hubness vector i0 Require: adjacency matrix A Require: iteration number τ Ensure: hubness vector i Ensure: authority vector p 1: function HITS(i0 , A, τ ) 2: i ← i0 3: for t = 1, 2, ..., τ do 4: p ← AT i 5: i ← Ap 6: normalize i and p 7: return i and p 8: end function In the investigation of vocabulary restriction in neural models, Sennrich et al. (2016) applied byte pair encoding to words and created a partial character string set that could express all the words in the training data. They increased the number of words included in the vocabulary to enable the encoder-decoder model to robustly learn contextual information. In contrast, we aim to improve neural models by using vocabulary that is appropriate for a training corpus—not to improve neural models by increasing their vocabulary. to retrieve co-occurring words from a training corpus to reduce noise in the source text. 3 Graph-based Filtering of OOV Words Jean et al. (2015) proposed a"
P18-3016,P17-1070,0,0.0118643,"selection improves encoder-decoder model performance. 2. This study is the first to address noise reduction in the source text of encoder-decoder models. 2 Related Work There is currently a growing interest in applying neural models to MT (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Wu ∗ Both authors equally contributed to the paper. 112 Proceedings of ACL 2018, Student Research Workshop, pages 112–119 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Algorithm 1 HITS et al., 2016) and GEC (Yuan and Briscoe, 2016; Xie et al., 2016; Ji et al., 2017); hence, this study focuses on improving the simple attentional encoder-decoder models that are applied to these tasks. Require: hubness vector i0 Require: adjacency matrix A Require: iteration number τ Ensure: hubness vector i Ensure: authority vector p 1: function HITS(i0 , A, τ ) 2: i ← i0 3: for t = 1, 2, ..., τ do 4: p ← AT i 5: i ← Ap 6: normalize i and p 7: return i and p 8: end function In the investigation of vocabulary restriction in neural models, Sennrich et al. (2016) applied byte pair encoding to words and created a partial character string set that could express all the words in"
P18-3016,1983.tc-1.13,0,0.274653,"Missing"
P18-3016,P11-2006,1,0.803322,"large value. 3.2 Vocabulary selection using HITS Algorithms that rank words using cooccurrence are employed in many natural language processing tasks. For example, TextRank (Mihalcea and Tarau, 2004) uses PageRank (Brin and Page, 1998) for keyword extraction. TextRank constructs a word graph in which nodes represent words, and edges represent co-occurrences between words within a fixed window; TextRank then executes the PageRank algorithm to extract keywords. Although this is an unsupervised method, it achieves nearly the same precision as one state-of-the-art supervised method (Hulth, 2003). Kiso et al. (2011) used HITS (Kleinberg, 1999) to select seeds and create a stop list for bootstrapping in natural language processing. They reported significant improvements over a baseline method using unigram frequency. Their graph-based algorithm was effective at extracting the relevance between words, which cannot be grasped with a simple unigram frequency. In this study, we use HITS In this study, we create an adjacency matrix from a training corpus by considering a word as a node and the co-occurrence between words as an edge. Unlike in web pages, co-occurrence between words is nonbinary; therefore, seve"
P18-3016,W04-3250,0,0.0456829,"model outputs an unknown word token, the word with the highest attention score is used as a query to replace the unknown token with the corresponding word from the dictionary. If not in the dictionary, we replace the unknown word token with the source word (unk rep). This dictionary is created based on word alignment obtained using fast align (Dyer et al., 2013) on the training corpus. We evaluate translation results using BLEU scores (Papineni et al., 2002). 4.2 Results Table 1 shows the translation accuracy (BLEU scores) and p-value of a significance test (p &lt; 0.05) by bootstrap resampling (Koehn, 2004). The PPMI model improves translation accuracy by 0.56 points in Japanese-to-English translation, which is a significant improvement. Next, we examine differences in vocabulary by comparing each model with the baseline. Compared to the vocabulary of the baseline in 100K setting, Freq and PPMI replace 16,107 and 17,166 3 BLEU score for postprocessing (unk rep) improves by 0.46, 0.44, and 0.46 points in the baseline, Freq, and PPMI, respectively. 115 src baseline Freq PPMI ref 有用 物質 の 分離 ・ 抽出 , 反応 性 向上 , 新 材料 創製 , 廃棄 物 処理 , 分析 等 の 分野 が ある 。 there are fields such as separation , extraction , extr"
P18-3016,P11-1019,0,0.025752,"gate the effect of the vocabulary size. Unless otherwise noted, we conduct an analysis of the model using the vocabulary size of 150K. The mini-batch size is 100. Table 5: F0.5 of COMMON and DIFF outputs. 5 Genetic refers the chance of inheriting a disorder or disease . Genetic refers the chance of inheriting a disorder or disease . Genetic refers to the chance of inheriting a disorder or disease . Genetic risk refers to the chance of inheriting a disorder or disease . Grammatical Error Correction 5.1 Experimental setting The second experiment addresses GEC. We combine the FCE public dataset (Yannakoudakis et al., 2011), NUCLE corpus (Dahlmeier et al., 2013), and English learner corpus from the Lang-8 learner corpus (Mizumoto et al., 2011) and remove sentences longer than 100 words to create a training corpus. From the Lang-8 learner corpus, we use only the pairs of erroneous and corrected sentences. We use 1,452,584 sentences as a training set (502,908 types on the encoder side and 639,574 types on the decoder side). We evaluate the models’ performances on the standard sets from the CoNLL-14 shared task (Ng et al., 2014) using CoNLL-13 data as a development set (1,381 sentences) and CoNLL-14 data as a test"
P18-3016,D15-1166,0,0.104517,"Missing"
P18-3016,N16-1042,0,0.076317,"rmation, which we capture using the HITS algorithm. We apply our proposed method to two tasks: machine translation and grammatical error correction. For Japanese-to-English translation, this method achieves a BLEU score that is 0.56 points more than that of a baseline. Furthermore, it outperforms the baseline method for English grammatical error correction, with an F0.5 -measure that is 1.48 points higher. 1 Introduction Encoder-decoder models (Sutskever et al., 2014) are effective in tasks such as machine translation (Cho et al., 2014; Bahdanau et al., 2015) and grammatical error correction (Yuan and Briscoe, 2016). Vocabulary in encoder-decoder models is generally selected from the training corpus in descending order of frequency, and low-frequency words are replaced with an unknown word token &lt;unk&gt;. The so-called out-of-vocabulary (OOV) words are replaced with &lt;unk&gt; to not increase the decoder’s complexity and to reduce noise. However, naive frequency-based OOV replacement may lead to loss of information that is necessary for modeling context in the encoder. 1. The simple but effective preprocessing method we propose for vocabulary selection improves encoder-decoder model performance. 2. This study is"
P18-3016,W04-3252,0,\N,Missing
P19-2020,E17-2037,0,0.198712,"R tokens to create new training data. Then, we train the neural network model using the new training data annotated with the degree of correction. At the time of inference, this model can control the degree of correction by adding a WER token to the input. In addition, we propose a method to select and estimate the degree of correction required for each input sequence. Introduction The number and types of corrections in a sentence containing grammatical errors written by an English learner vary from annotator to annotator (Bryant and Ng, 2015). For example, it is known that the JFLEG dataset (Napoles et al., 2017) has a higher degree of correction in terms of the amount of corrections per sentence than that in the CoNLL-2014 dataset (Ng et al., 2014). This is because CoNLL-2014 contains only minimal edits, whereas JFLEG contains corrections with fluency edits (Napoles et al., 2017). Similarly, the degree of correction depends on the learners because it should be personalized to the level of learners. In this study, we used word edit rate (WER) as an index of the degree of correction. As WER is an index that shows the number of rewritten words in sentences, the WER between an erroneous sentence and a co"
P19-2020,W14-1701,0,0.0466632,"n. At the time of inference, this model can control the degree of correction by adding a WER token to the input. In addition, we propose a method to select and estimate the degree of correction required for each input sequence. Introduction The number and types of corrections in a sentence containing grammatical errors written by an English learner vary from annotator to annotator (Bryant and Ng, 2015). For example, it is known that the JFLEG dataset (Napoles et al., 2017) has a higher degree of correction in terms of the amount of corrections per sentence than that in the CoNLL-2014 dataset (Ng et al., 2014). This is because CoNLL-2014 contains only minimal edits, whereas JFLEG contains corrections with fluency edits (Napoles et al., 2017). Similarly, the degree of correction depends on the learners because it should be personalized to the level of learners. In this study, we used word edit rate (WER) as an index of the degree of correction. As WER is an index that shows the number of rewritten words in sentences, the WER between an erroneous sentence and a corrected sentence can represent the degree of correction of the sentence. Figure 1 shows that the WER of the JFLEG test set is higher than t"
P19-2020,W13-3601,0,0.0207913,"best candidates. The oracle WER shows the scores when selecting a corrected sentence for each erroneous sentence that maximizes the F0.5 on CoNLL2014 test set and GLEU on JFLEG test set. The gold WER shows the scores when using the WER token calculated from the reference in evaluation datasets. data by introducing synthetic grammatical errors to the 2007 New York Times Annotated Corpus (LDC2008T19)3 to the original training data in the same manner as the random noising method done by Xie et al. (2018). We used the CoNLL-2014 test set and JFLEG test set as the test sets and CoNLL-2013 dataset (Ng et al., 2013) and JFLEG dev set as the development sets, respectively. 3.3 Controlling experiment Table 3 shows the experimental result of controlling the degree of correction using WER. The “WER Token” models are all the same model except for each WER token added to the beginning of the all of input sentences at the time of inference. The WER in Table 3 show that the average WER is proportional to the WER tokens added to the input sentences. Hence, the WER of the GEC model can be controlled by the WER tokens defined by WER. The precision is the highest for the WER token ⟨1⟩ and the recall is low. In contr"
P19-2020,D17-1298,0,0.0158913,"f the degree of correction. Kikuchi et al. (2016) proposed to control the output length by hinting about the output length to the encoder-decoder model in the text summarization task. Sennrich et al. (2016) controlled the politeness of output sentences by adding politeness information to the training data as WER tokens in machine translation. In this research, similar to Sennrich et al. (2016), we added WER indicating the degree of correction as WER tokens to the training data to control the degree of correction for the input sentences. Similar to our method, Junczys-Dowmunt et al. (2018) and Schmaltz et al. (2017) trained a GEC model with corrective edits information to control the tendency of generating corrections. gree of correction of model. Therefore, we determined the gold WER tokens for each sentence from the WERs calculated from erroneous and corrected sentences in the CoNLL-2014 test set and JFLEG test set, as shown in Table 2. Then, we calculated the average of the M2 score, GLEU, and the controlling accuracy because the CoNLL2014 test set and JFLEG test set have multiple references. The controlling accuracy is the concordance rate of the gold and system WER tokens determined from system outp"
P19-2020,P17-1074,0,0.111975,"Missing"
P19-2020,P15-1068,0,0.130205,"information about the degree of correction to erroneous sentences as WER tokens to create new training data. Then, we train the neural network model using the new training data annotated with the degree of correction. At the time of inference, this model can control the degree of correction by adding a WER token to the input. In addition, we propose a method to select and estimate the degree of correction required for each input sequence. Introduction The number and types of corrections in a sentence containing grammatical errors written by an English learner vary from annotator to annotator (Bryant and Ng, 2015). For example, it is known that the JFLEG dataset (Napoles et al., 2017) has a higher degree of correction in terms of the amount of corrections per sentence than that in the CoNLL-2014 dataset (Ng et al., 2014). This is because CoNLL-2014 contains only minimal edits, whereas JFLEG contains corrections with fluency edits (Napoles et al., 2017). Similarly, the degree of correction depends on the learners because it should be personalized to the level of learners. In this study, we used word edit rate (WER) as an index of the degree of correction. As WER is an index that shows the number of rewr"
P19-2020,N16-1005,0,0.024372,"a neural network were proposed to ensure fluent corrections, considering the context and meaning between words. Among them, the method by Chollampatt and Ng (2018) uses a multilayer convolutional encoder-decoder neural network (Gehring et al., 2017). This model is one of the state-of-the-art models in GEC, and its implementation is currently being published5 . However, these models cannot be controlled in terms of the degree of correction. Kikuchi et al. (2016) proposed to control the output length by hinting about the output length to the encoder-decoder model in the text summarization task. Sennrich et al. (2016) controlled the politeness of output sentences by adding politeness information to the training data as WER tokens in machine translation. In this research, similar to Sennrich et al. (2016), we added WER indicating the degree of correction as WER tokens to the training data to control the degree of correction for the input sentences. Similar to our method, Junczys-Dowmunt et al. (2018) and Schmaltz et al. (2017) trained a GEC model with corrective edits information to control the tendency of generating corrections. gree of correction of model. Therefore, we determined the gold WER tokens for"
P19-2020,N18-1057,0,0.148198,"equal-sized subsets. Different WER tokens are defined for each subset and added to the beginning of the source sentences. Finally, the following parallel corpus is obtained: error-containing sentences annotated with the WER token representing the correction degree |y | 1 ∑ ˆ = arg max y log P (yi |yi−(n−1) , · · · , yi−1 ) y ∈Y |y| i=1 3 Experiments 3.1 Datasets Table 1 summarizes the training data. We used Lang-8 (Mizumoto et al., 2012) and NUCLE (Dahlmeier et al., 2013) as the training data. The accuracy of the GEC task is known to be improved by increasing the amount of the training data (Xie et al., 2018). Therefore, we added more 2 WER may exceed the one in which the Levenshtein distance is larger than the number of words in the target sentence. 1 Only sentences with corrections are used, and the sentence length limit is 80 words. 150 Model CoNLL-2013 (Dev) CoNLL-2014 (Test) JFLEG (Dev) JFLEG (Test) P R F0.5 P R F0.5 Baseline 42.19 15.28 31.20 53.20 25.18 43.52 47.92 51.77 0.10 WER Token ⟨1⟩ ⟨2⟩ ⟨3⟩ ⟨4⟩ ⟨5⟩ 52.45 47.55 43.38 40.91 29.48 13.60 17.94 20.05 21.32 13.98 33.39 35.75 35.19 34.56 24.13 60.07 54.64 50.48 47.43 33.77 23.52 28.41 31.45 32.68 22.95 *45.83 *46.12 *45.03 43.50 *30.86 44.8"
P19-2020,N12-1067,0,0.402479,"Missing"
P19-2020,W13-1703,0,0.146575,"culated by normalizing this distance with respect to the source length. Second, appropriate cutoffs are selected to divide the sentences into five equal-sized subsets. Different WER tokens are defined for each subset and added to the beginning of the source sentences. Finally, the following parallel corpus is obtained: error-containing sentences annotated with the WER token representing the correction degree |y | 1 ∑ ˆ = arg max y log P (yi |yi−(n−1) , · · · , yi−1 ) y ∈Y |y| i=1 3 Experiments 3.1 Datasets Table 1 summarizes the training data. We used Lang-8 (Mizumoto et al., 2012) and NUCLE (Dahlmeier et al., 2013) as the training data. The accuracy of the GEC task is known to be improved by increasing the amount of the training data (Xie et al., 2018). Therefore, we added more 2 WER may exceed the one in which the Levenshtein distance is larger than the number of words in the target sentence. 1 Only sentences with corrections are used, and the sentence length limit is 80 words. 150 Model CoNLL-2013 (Dev) CoNLL-2014 (Test) JFLEG (Dev) JFLEG (Test) P R F0.5 P R F0.5 Baseline 42.19 15.28 31.20 53.20 25.18 43.52 47.92 51.77 0.10 WER Token ⟨1⟩ ⟨2⟩ ⟨3⟩ ⟨4⟩ ⟨5⟩ 52.45 47.55 43.38 40.91 29.48 13.60 17.94 20.05"
P19-2020,W11-2123,0,0.0470971,"Missing"
P19-2020,D16-1161,0,0.13488,"d sentences is obtained from a single erroneous sentence. Moreover, compared to a GEC model that does not use information on the degree of correction, the proposed method improves correction accuracy. 1 Figure 1: Histogram of the WER in one sentence. However, existing GEC models consider only the single degree of correction suited for training corpus. Recently, neural network-based models have been actively studied for use in grammatical error correction (GEC) tasks (Chollampatt and Ng, 2018). These models outperform conventional models using phrasebased statistical machine translation (SMT) (Junczys-Dowmunt and Grundkiewicz, 2016). Nonetheless, controlling the amount of correction required to obtain an error-free sentence is not possible. Therefore, we propose a method for neural GEC that can control the degree of correction. In the training data, in which grammatical errors are corrected, we add information about the degree of correction to erroneous sentences as WER tokens to create new training data. Then, we train the neural network model using the new training data annotated with the degree of correction. At the time of inference, this model can control the degree of correction by adding a WER token to the input."
P19-2020,N18-1055,0,0.0402295,"ls cannot be controlled in terms of the degree of correction. Kikuchi et al. (2016) proposed to control the output length by hinting about the output length to the encoder-decoder model in the text summarization task. Sennrich et al. (2016) controlled the politeness of output sentences by adding politeness information to the training data as WER tokens in machine translation. In this research, similar to Sennrich et al. (2016), we added WER indicating the degree of correction as WER tokens to the training data to control the degree of correction for the input sentences. Similar to our method, Junczys-Dowmunt et al. (2018) and Schmaltz et al. (2017) trained a GEC model with corrective edits information to control the tendency of generating corrections. gree of correction of model. Therefore, we determined the gold WER tokens for each sentence from the WERs calculated from erroneous and corrected sentences in the CoNLL-2014 test set and JFLEG test set, as shown in Table 2. Then, we calculated the average of the M2 score, GLEU, and the controlling accuracy because the CoNLL2014 test set and JFLEG test set have multiple references. The controlling accuracy is the concordance rate of the gold and system WER tokens"
P19-2020,D16-1140,0,0.019181,". However, the SMT model can only correct few words or phrases based on a local context, resulting in unnatural sentences. Therefore, several methods using a neural network were proposed to ensure fluent corrections, considering the context and meaning between words. Among them, the method by Chollampatt and Ng (2018) uses a multilayer convolutional encoder-decoder neural network (Gehring et al., 2017). This model is one of the state-of-the-art models in GEC, and its implementation is currently being published5 . However, these models cannot be controlled in terms of the degree of correction. Kikuchi et al. (2016) proposed to control the output length by hinting about the output length to the encoder-decoder model in the text summarization task. Sennrich et al. (2016) controlled the politeness of output sentences by adding politeness information to the training data as WER tokens in machine translation. In this research, similar to Sennrich et al. (2016), we added WER indicating the degree of correction as WER tokens to the training data to control the degree of correction for the input sentences. Similar to our method, Junczys-Dowmunt et al. (2018) and Schmaltz et al. (2017) trained a GEC model with c"
P19-2020,C12-2084,1,0.563142,"he training data. Then, WER is calculated by normalizing this distance with respect to the source length. Second, appropriate cutoffs are selected to divide the sentences into five equal-sized subsets. Different WER tokens are defined for each subset and added to the beginning of the source sentences. Finally, the following parallel corpus is obtained: error-containing sentences annotated with the WER token representing the correction degree |y | 1 ∑ ˆ = arg max y log P (yi |yi−(n−1) , · · · , yi−1 ) y ∈Y |y| i=1 3 Experiments 3.1 Datasets Table 1 summarizes the training data. We used Lang-8 (Mizumoto et al., 2012) and NUCLE (Dahlmeier et al., 2013) as the training data. The accuracy of the GEC task is known to be improved by increasing the amount of the training data (Xie et al., 2018). Therefore, we added more 2 WER may exceed the one in which the Levenshtein distance is larger than the number of words in the target sentence. 1 Only sentences with corrections are used, and the sentence length limit is 80 words. 150 Model CoNLL-2013 (Dev) CoNLL-2014 (Test) JFLEG (Dev) JFLEG (Test) P R F0.5 P R F0.5 Baseline 42.19 15.28 31.20 53.20 25.18 43.52 47.92 51.77 0.10 WER Token ⟨1⟩ ⟨2⟩ ⟨3⟩ ⟨4⟩ ⟨5⟩ 52.45 47.55 4"
P19-3001,P12-3027,0,0.0188911,"x. For example, when the user writes “We discussed,” the system displays the patterns for the use of the word “discussed.” In our system, we also employ collocation patterns; however, we use a large-scale learner corpus to search for dependency structures. Sketch Engine (Kilgarriff et al., 2004) displays grammar constructs associated with words along with thesaurus information. As previously mentioned, our system presents incorrect examples by using a learner corpus apart from the correct examples extracted from a raw corpus. 4.1 Example Retrieval System for English as a Second Language FLOW (Chen et al., 2012) is a system that shows some candidates for English words when learners of English as a Second Language (ESL) write a sentence in their native language by using paraphrase candidates with bilingual pivoting. In contrast, our system suggests incorrect examples and their counterparts based on corrections from the learner corpus. Another system, called StringNet (Wible and Tsao, 2010), displays the patterns in which a query is used, together with their frequency. The noun and the preposition are substituted by their parts of speech, instead of the words themselves, to eliminate data sparseness. O"
P19-3001,I11-1017,1,0.764621,"o extract triples of a noun phrase, particle, and verb to be used as a co-occurrence. Sakura: Large-scale Incorrect Example Retrieval System for JSL This section describes the dataset and user interface of our proposed system, Sakura. Our system uses the data explained in Section 2.1 as the database for example retrieval. The user interface illustrated in Section 2.2 allows learners to search for incorrect examples. 2.2 User Interface Figure 1 shows the user interface of Sakura. Its components are explained below. 2.1 Lang-8 Dataset In this study, we used the Lang-8 Learner Corpora created by Mizumoto et al. (2011). The developers of the dataset used it for Japanese grammatical error correction, whereas we used it as an example retrieval database for JSL. A. Query Input the word to be searched for. The input query is assumed as a word or a phrase (se3 4 2 https://github.com/taku910/mecab https://github.com/taku910/cabocha quence of words). each system (KYC and Sakura) and counted the number of matches for each system that led to correct expressions to ensure accuracy. We randomly extracted 55 incorrect phrases from the learner’s erroneous sentences with correct phrases from the Lang-8 dataset, which wer"
P19-3001,Y13-1014,1,0.827037,".jp, kodaira.tomonori@gmail.com, komachi@tmu.ac.jp Abstract cles. Particles in Japanese indicate grammatical relations between verbs and nouns. For example, the sentence, “日本語を勉強する。”, which means “I study Japanese.” includes an accusative case marker “を”, which introduces the direct object of the verb. However, in this case, Japanese learners often make mistakes, such as “日本語が勉強する。”, which means “Japanese language studies.” Thus, the appropriate use of particles is not obvious for non-native speakers of Japanese. Particle errors and word choice are the most common Japanese grammatical errors (Oyama et al., 2013), both of which require a sufficient number of correct and incorrect examples to understand the usage in context. Word n-gram search provides only few or no examples for a phrase because Japanese is a relatively free language in terms of word order, in which a syntactically dependent word may appear in a distant position. Considering this, our study develops an incorrect example retrieval system, called Sakura1 , that uses the large-scale Lang-82 dataset for learners of Japanese as a second language (JSL) by focusing on the usability of incorrect example retrieval. The main contributions of th"
P19-3001,W16-6509,0,0.0306407,"ples. If a retrieval system has a wide coverage of incorrect examples along with their correct counterparts, learners can revise their composition themselves. Considering the usability of retrieving incorrect examples, our proposed system uses a large-scale corpus to expand the coverage of incorrect examples and present correct as well as incorrect expressions. Intrinsic and extrinsic evaluations indicate that our system is more useful than existing systems. 1 Introduction A standard method that supports second language learning effort is the use of examples. Example retrieval systems such as Rakhilina et al. (2016) and Kilgarriff et al. (2004) particularly check for the appropriate use of words in the context in which they are written. However, in such a system, if the query word is incorrect, finding appropriate examples is impossible using ordinary search engines such as Google. Even if learners have access to an incorrect example retrieval system, such as Kamata and Yamauchi (1999) and Nishina et al. (2014), they are often unable to rewrite a composition without correct versions of the incorrect examples. Furthermore, existing example retrieval systems only provide a small number of examples; hence,"
P19-3001,P12-2039,1,0.874976,"Missing"
P19-3001,W10-0804,0,0.0160442,"As previously mentioned, our system presents incorrect examples by using a learner corpus apart from the correct examples extracted from a raw corpus. 4.1 Example Retrieval System for English as a Second Language FLOW (Chen et al., 2012) is a system that shows some candidates for English words when learners of English as a Second Language (ESL) write a sentence in their native language by using paraphrase candidates with bilingual pivoting. In contrast, our system suggests incorrect examples and their counterparts based on corrections from the learner corpus. Another system, called StringNet (Wible and Tsao, 2010), displays the patterns in which a query is used, together with their frequency. The noun and the preposition are substituted by their parts of speech, instead of the words themselves, to eliminate data sparseness. Our system shows collocation patterns for each query by using parts of speech information and dependency parsing; however, our system does not explicitly present the parts of speech because our dataset is sufficiently large and need not replace and display the part-of-speech tag. The ESCORT (Matsubara et al., 2008) system shows example sentences to learners based on the grammatical"
P19-3001,P15-4024,0,0.0260396,"nding to the incorrect sentence; and “Co-occurrence” denotes whether the system can provide co-occurrence examples. are not intended to retrieve examples for language learners; therefore, the search engines show neither example sentences nor the correct version of a given incorrect sentence to aid learners. Language learners can use several example retrieval systems. The following subsections present information on some of those systems for learners of English and Japanese as a second language. Furthermore, ESL learners can check examples while writing an English sentence by using WriteAhead (Yen et al., 2015). This system provides pattern suggestions based on collocation and syntax. For example, when the user writes “We discussed,” the system displays the patterns for the use of the word “discussed.” In our system, we also employ collocation patterns; however, we use a large-scale learner corpus to search for dependency structures. Sketch Engine (Kilgarriff et al., 2004) displays grammar constructs associated with words along with thesaurus information. As previously mentioned, our system presents incorrect examples by using a learner corpus apart from the correct examples extracted from a raw cor"
W07-1522,P06-1079,1,0.472904,"rpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several anno"
W07-1522,kawahara-etal-2002-construction,0,0.390972,"Japanese Text Corpus with Predicate-Argument and Coreference Relations Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto Graduate School of Information Science, Nara Institute of Science and Technology 8916-5 Takayama, Ikoma, Nara, 630-0192, Japan {ryu-i,mamoru-k,inui,matsu}@is.naist.jp Abstract In this paper, we discuss how to annotate coreference and predicate-argument relations in Japanese written text. There have been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations as are done in the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDATagged Corpus (Hasida, 2005). However, there is still much room for reﬁning their speciﬁcations. For this reason, we discuss issues in annotating these two types of relations, and propose a new speciﬁcation for each. In accordance with the speciﬁcation, we built a large-scaled annotated corpus, and examined its reliability. As a result of our current work, we have released an annotated corpus named the NAIST Text Corpus1 , which is used as the evaluation data set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coref"
W07-1522,P02-1014,0,0.136204,"ference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On t"
W07-1522,J05-1004,0,0.129176,"notated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research activities for building Japanese text corpora annotated with coreference and predicate-argument relations such as the Kyoto Text Corpus version 4.0 (Kawahara et al., 2002) and the GDA3 -Tagged Corpus (Hasida, 2005). However, as we discuss in this paper, there is still much room for arguing and reﬁning the speciﬁcation of such sorts of semantic annotation. In fact, for neither of the above two corpora, the adequacy and reliability of the annotation scheme has been deeply examined. In this paper, we discuss how to annotate coreferen"
W07-1522,P04-1019,0,0.0132674,"uch as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed for several years. On the other hand, the speciﬁcation of predicate-argument structure analysis has mainly been discussed in the context of the CoNLL shared task2 on the basis of the PropBank (Palmer et al., 2005). In parallel with these efforts, there have also been research a"
W07-1522,J01-4004,0,0.294742,"ata set in the coreference and zero-anaphora resolution tasks in Iida et al. (2005) and Iida et al. (2006). 1 Introduction Coreference resolution and predicate-argument structure analysis has recently been a growing ﬁeld of research due to the demands from NLP application such as information extraction and machine translation. With the research focus placed on these tasks, the speciﬁcation of annotating corpora and the 1 The NAIST Text Corpus is downloadable from http://cl.naist.jp/nldata/corpus/, and it has already been downloaded by 102 unique users. data sets used in supervised techniques (Soon et al., 2001; Ng and Cardie, 2002, etc.) have also grown in sophistication. For English, several annotation schemes have already been proposed for both coreference relation and argument structure, and annotated corpora have been developed accordingly (Hirschman, 1997; Poesio et al., 2004; Doddington et al., 2004). For instance, in the Coreference task on Message Understanding Conference (MUC) and the Entity Detection and Tracking (EDT) task in the Automatic Content Extraction (ACE) program, which is the successor of MUC, the details of speciﬁcation of annotating coreference relation have been discussed fo"
W07-1522,P06-2105,0,0.00720716,"ikely to be more cost-efﬁcient than semantic roles 134 because they are often explicitly marked by case markers. This fact also allows us to avoid the difﬁculties in deﬁning a label set. • In Japanese, the mapping from syntactic cases to semantic roles tends to be reasonably straightforward if a semantically rich lexicon of verbs like the VerbNet (Kipper et al., 2000) is available. • Furthermore, we have not yet found many NLP applications for which the utility of semantic roles is actually demonstrated. One may think of using semantic roles in textual inference as exempliﬁed by, for example, Tatu and Moldovan (2006). However, similar sort of inference may well be realized with syntactic cases as demonstrated in the information extraction and question answering literature. Taking these respects into account, we choose to label predicate-argument relations in terms of syntactic cases, which follows the annotation scheme adopted in the Kyoto Corpus. 3.2 Syntactic case alternation Once the level of syntactic cases is chosen for our annotation, another issue immediately arises, alteration of syntactic cases by syntactic transformations such as passivization and causativization. For example, sentence (5) is an"
W07-1522,W99-0213,0,0.0702454,"Missing"
W07-1522,M95-1005,0,0.107471,"Missing"
W07-1522,W04-2705,0,\N,Missing
W07-1522,M98-1029,0,\N,Missing
W11-0318,N07-1026,0,0.019423,"nstruction, despite their lower computational complexity. 1 Introduction Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification (Zhou et al., 2004; Zhu et al., 2003; Callut et al., 2008; Wang et al., 2008) are recognized as a promising approach. Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (Alexandrescu and Kirchhoff, 2007; Niu et al., 2005), sentiment analysis (Goldberg and Zhu, 2006), and statistical machine translation (Alexandrescu and Kirchhoff, 2009), to name but a few. However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains “more of an art than a science” (Zhu, 2005). Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results."
W11-0318,P94-1020,0,0.04979,"ion is violated; hence, smaller the φ-edge ratio, the better. The plot for b-matching graph is missing for the 20 newsgroups dataset, because its construction did not finish after one week for this dataset. dataset examples features labels interest line Reuters 20 newsgroups 2,368 4,146 4,028 19,928 3,689 8,009 17,143 62,061 6 6 4 20 Table 2: Datasets used in experiments. portion of the Wall Street Journal Corpus. Each instance of the polysemous word “interest” has been tagged with one of the six senses in Longman Dictionary of Contemporary English. The details of the dataset are described in Bruce and Wiebe (1994). The “line” data is originally used in numerous comparative studies of word sense disambiguation. Each instance of the word “line” has been tagged with one of the six senses on the WordNet thesaurus. Further details can be found in the Leacock et al. (1993). Following Niu et al. (2005), we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation. Details of these context features can be found in Lee and Ng (2002). The Reuters dataset is extracted from RCV1v2/LYRL2004, a text"
W11-0318,D09-1052,0,0.0165412,"d sense disambiguation. Each instance of the word “line” has been tagged with one of the six senses on the WordNet thesaurus. Further details can be found in the Leacock et al. (1993). Following Niu et al. (2005), we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation. Details of these context features can be found in Lee and Ng (2002). The Reuters dataset is extracted from RCV1v2/LYRL2004, a text categorization test collection (Lewis et al., 2004). In the same manner as Crammer et al. (2009), we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random. The features described in Lewis et al. (2004) are used with this dataset. 159 The 20 newsgroups dataset is a popular dataset frequently used for document classification and clustering. The dataset consists of approximately 20,000 messages on newsgroups and is originally distributed by Lang (1995). Each message is assigned one of the 20 possible labels indicating which newsgroup it has been posted to, and represented as binary bag-of-"
W11-0318,W06-3808,0,0.0228219,"Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification (Zhou et al., 2004; Zhu et al., 2003; Callut et al., 2008; Wang et al., 2008) are recognized as a promising approach. Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (Alexandrescu and Kirchhoff, 2007; Niu et al., 2005), sentiment analysis (Goldberg and Zhu, 2006), and statistical machine translation (Alexandrescu and Kirchhoff, 2009), to name but a few. However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains “more of an art than a science” (Zhu, 2005). Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results. Both for semi-supervised classification and for clustering, the"
W11-0318,H93-1051,0,0.0486517,"rix, i.e., Pij = Pji for all i and j, while Pˆ may 1 See also the experimental results of Section 6.3.2 in which the full similarity matrix W 0 is used as the baseline. 156 3.2 Effect of Hubs on Classification In this section, we demonstrate that hubs in k-NN graphs are indeed harmful to semi-supervised classification as we claimed earlier. To this end, we eliminate such high degree vertices from the graph, and compare the classification accuracy of other vertices before and after the elimination. For this preliminary experiment, we used the “line” dataset of a word sense disambiguation task (Leacock et al., 1993). For details of the dataset and the task, see Section 6. In this experiment, we randomly selected 10 percent of examples as labeled examples. The remaining 90 percent makes the set of unlabeled examples, and the goal is to predict the label (word sense) of these unlabeled examples. We first built a k-NN graph (with k = 3) from the dataset, and ran Gaussian Random Fields (GRF) (Zhu et al., 2003), one of the most widelyused graph-based semi-supervised classification algorithms. Then we removed vertices with degree greater than or equal to 30 from the k-NN graph, and ran GRF again on this “hub-r"
W11-0318,W02-1006,0,0.00958422,"orary English. The details of the dataset are described in Bruce and Wiebe (1994). The “line” data is originally used in numerous comparative studies of word sense disambiguation. Each instance of the word “line” has been tagged with one of the six senses on the WordNet thesaurus. Further details can be found in the Leacock et al. (1993). Following Niu et al. (2005), we used the following context features in the word sense disambiguation tasks: part-of-speech of neighboring words, single words in the surrounding context, and local collocation. Details of these context features can be found in Lee and Ng (2002). The Reuters dataset is extracted from RCV1v2/LYRL2004, a text categorization test collection (Lewis et al., 2004). In the same manner as Crammer et al. (2009), we produced the classification dataset by selecting approximately 4,000 documents from 4 general topics (corporate, economic, government and markets) at random. The features described in Lewis et al. (2004) are used with this dataset. 159 The 20 newsgroups dataset is a popular dataset frequently used for document classification and clustering. The dataset consists of approximately 20,000 messages on newsgroups and is originally distri"
W11-0318,P05-1049,0,0.0709981,"mputational complexity. 1 Introduction Semi-supervised classification try to take advantage of a large amount of unlabeled data in addition to a small amount of labeled data, in order to achieve good classification accuracy while reducing the cost of manually annotating data. In particular, graph-based techniques for semi-supervised classification (Zhou et al., 2004; Zhu et al., 2003; Callut et al., 2008; Wang et al., 2008) are recognized as a promising approach. Some of these techniques have been successfully applied for NLP tasks: word sense disambiguation (Alexandrescu and Kirchhoff, 2007; Niu et al., 2005), sentiment analysis (Goldberg and Zhu, 2006), and statistical machine translation (Alexandrescu and Kirchhoff, 2009), to name but a few. However, the focus of these studies is how to assign accurate labels to vertices in a given graph. By contrast, there has not been much work on how such a graph should be built, and graph construction remains “more of an art than a science” (Zhu, 2005). Yet, it is an essential step for graph-based semisupervised classification and (unsupervised) clustering, and the input graph affects the quality of final classification/clustering results. Both for semi-supe"
W11-0318,N09-1014,0,\N,Missing
W11-1913,P09-1068,0,0.335718,"stract system based on the cluster-ranking model proposed by Rahman and Ng (2009). We then experimented with adding novel semantic features derived from co-referring predicate-argument chains. These narrative schema were developed by Chambers and Jurafsky (2009). They are described in more detail in a later section. In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system’s performance. 1 Yuji Matsumoto Nara Institute of Science and Technology Nara Prefecture, Japan matsu@is.naist.jp 2 Introduction Coreference resolution is a problem for automated document understanding. We say two segments of a natural-language document corefer when they refer to the same real-world entity. The segments of a document which refer to an entity are called mentions. In coreference resol"
W11-1913,chambers-jurafsky-2010-database,0,0.0443508,"tted two results to the CoNLL-2011 Shared Task. In the “closed” track we submitted the results of our baseline system without the schema features, trained on all documents in both the training and development portions of the OntoNotes corpus. We also submitted a result in the “open” track: a version of our system with the schema features added. Due to issues with the implementation of this second version, however, we were only able to submit results from a model trained on just the WSJ portion of the training dataset. For the schema features, we used a database of narrative schema released by Chambers and Jurafsky (2010) – specifically the list of schemas of size 12. 4 The official system scores for our system are listed in Table 3. We can attribute some of the low performance of our system to features which are too noisy, and to having not enough features compared to the large size of the dataset. It is likely that these two factors adversely impact the ability of the SVM to learn effectively. In fact, the features which we introduced partially to provide more features to learn with, the NE features, had the worst impact on performance according to later analysis. Because of a problem with our implementation"
W11-1913,H05-1013,0,0.0607459,"Missing"
W11-1913,N10-1061,0,0.0231043,"rube, 2006b). Surprisingly, Rahman and Ng (2011) demonstrated that a system using almost exclusively lexical features could outperform 86 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86–92, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics systems which used more traditional sets of features. Although string features have a large effect on performance, it is recognized that the use of semantic information is important for further improvement (Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto and Strube, 2006b; Haghighi and Klein, 2010). The use of predicate-argument structure has been explored by Ponzetto and Strube (2006b; 2006a). 3 Narrative Schema for Coreference Narrative schema are extracted from large-scale corpora using coreference information to identify predicates whose arguments often corefer. Similarity measures are used to build up schema consisting of one or more event chains – chains of typicallycoreferring predicate arguments (Chambers and Jurafsky, 2009). Each chain corresponds to a role in the schema. A role defines a class of participants in the schema. Conceptually, if a schema is present in a document, t"
W11-1913,P10-1142,0,0.126334,"th several layers of syntactic and semantic information, making it a rich resource for investigating coreference resolution (Pradhan et al., 2007). We participated in both the “open” and “closed” tracks. The “closed” track requires systems to only use the provided data, while the “open” track allows use of external data. We created a baseline Related Work Supervised machine-learning approaches to coreference resolution have been researched for almost two decades. Recently, the state of the art seems to be moving away from the early mention-pair classification model toward entity-based models. Ng (2010) provides an excellent overview of the history and recent developments within the field. Both entity-mention and mention-pair models are formulated as binary classification problems; however, ranking may be a more natural approach to coreference resolution (Ng, 2010; Rahman and Ng, 2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In another approach, Daum´e and Marcu (2005) apply their Learning as Search Optimization framework to coreference resolution, and show good results. Feature selection is important for good performance in corefe"
W11-1913,N06-1025,0,0.398465,"lems; however, ranking may be a more natural approach to coreference resolution (Ng, 2010; Rahman and Ng, 2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In another approach, Daum´e and Marcu (2005) apply their Learning as Search Optimization framework to coreference resolution, and show good results. Feature selection is important for good performance in coreference resolution. Ng (2010) discusses commonly used features, and analyses of the contribution of various features can be found in (Daum´e and Marcu, 2005; Rahman and Ng, 2011; Ponzetto and Strube, 2006b). Surprisingly, Rahman and Ng (2011) demonstrated that a system using almost exclusively lexical features could outperform 86 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86–92, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics systems which used more traditional sets of features. Although string features have a large effect on performance, it is recognized that the use of semantic information is important for further improvement (Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto and Strube, 2006b; Haghighi a"
W11-1913,E06-2015,0,0.133754,"lems; however, ranking may be a more natural approach to coreference resolution (Ng, 2010; Rahman and Ng, 2009). Rahman and Ng (2009) in particular propose the cluster-ranking model which we used in our baseline. In another approach, Daum´e and Marcu (2005) apply their Learning as Search Optimization framework to coreference resolution, and show good results. Feature selection is important for good performance in coreference resolution. Ng (2010) discusses commonly used features, and analyses of the contribution of various features can be found in (Daum´e and Marcu, 2005; Rahman and Ng, 2011; Ponzetto and Strube, 2006b). Surprisingly, Rahman and Ng (2011) demonstrated that a system using almost exclusively lexical features could outperform 86 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86–92, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics systems which used more traditional sets of features. Although string features have a large effect on performance, it is recognized that the use of semantic information is important for further improvement (Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto and Strube, 2006b; Haghighi a"
W11-1913,W11-1901,0,0.0436156,"t varied between the metrics used. We also explore the impact of various features on our system’s performance. 1 Yuji Matsumoto Nara Institute of Science and Technology Nara Prefecture, Japan matsu@is.naist.jp 2 Introduction Coreference resolution is a problem for automated document understanding. We say two segments of a natural-language document corefer when they refer to the same real-world entity. The segments of a document which refer to an entity are called mentions. In coreference resolution tasks, mentions are usually restricted to noun phrases. The goal of the CoNLL-2011 Shared Task (Pradhan et al., 2011) is to model unrestricted coreference using the OntoNotes corpus. The OntoNotes corpus is annotated with several layers of syntactic and semantic information, making it a rich resource for investigating coreference resolution (Pradhan et al., 2007). We participated in both the “open” and “closed” tracks. The “closed” track requires systems to only use the provided data, while the “open” track allows use of external data. We created a baseline Related Work Supervised machine-learning approaches to coreference resolution have been researched for almost two decades. Recently, the state of the art"
W11-1913,D09-1101,0,0.644344,"p Mamoru Komachi Nara Institute of Science and Technology Nara Prefecture, Japan komachi@is.naist.jp Abstract system based on the cluster-ranking model proposed by Rahman and Ng (2009). We then experimented with adding novel semantic features derived from co-referring predicate-argument chains. These narrative schema were developed by Chambers and Jurafsky (2009). They are described in more detail in a later section. In this paper we describe the system with which we participated in the CoNLL-2011 Shared Task on modelling coreference. Our system is based on a cluster-ranking model proposed by Rahman and Ng (2009), with novel semantic features based on recent research on narrative event schema (Chambers and Jurafsky, 2009). We demonstrate some improvements over the baseline when using schema information, although the effect varied between the metrics used. We also explore the impact of various features on our system’s performance. 1 Yuji Matsumoto Nara Institute of Science and Technology Nara Prefecture, Japan matsu@is.naist.jp 2 Introduction Coreference resolution is a problem for automated document understanding. We say two segments of a natural-language document corefer when they refer to the same r"
W11-1913,D08-1067,0,\N,Missing
W11-3506,P00-1031,0,0.04063,"editor by converting erroneous text written in Roman characters into correct text written in kana while leaving foreign words unchanged. Our method consists of three steps: iden2 Related Work Our interest is mainly focused on how to deal with erroneous inputs. Error detection and correction on sentences written in kana with kana character N-gram was proposed in (Shinnou, 1999). Our approach is similar to this, but our target is sentences in Roman characters and has the additional difficulty of language identification. Errortolerant Chinese input methods were introduced in (Zheng et al., 2011; Chen and Lee, 2000). Though Roman-to-kana conversion is similar to pinyin-toChinese conversion, our target differs from them because our motivation is to help Japanese language teachers. Japanese commercial IMs such as Microsoft Office IME1 , ATOK2 , and Google IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on A"
W11-3506,I08-1058,0,0.0138414,"proposed in (Shinnou, 1999). Our approach is similar to this, but our target is sentences in Roman characters and has the additional difficulty of language identification. Errortolerant Chinese input methods were introduced in (Zheng et al., 2011; Chen and Lee, 2000). Though Roman-to-kana conversion is similar to pinyin-toChinese conversion, our target differs from them because our motivation is to help Japanese language teachers. Japanese commercial IMs such as Microsoft Office IME1 , ATOK2 , and Google IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 38–42, Chiang Mai, Thailand, November 13, 2011. 5.1 Language Identification error correction in addition to language identification. Correcting Japanese learners’ error is also proposed in (Mizumoto et al., 2011). They try to correct sentences written in kana and kanji mixed, whereas we aim at texts in Roman cha"
W11-3506,I11-1017,1,0.785266,"gle IME3 have a module of spelling correction, but their target is native Japanese speakers. (Ehara and Tanaka-Ishii, 2008) presented a high accuracy language detection system for text input. We perform 1 http://www.microsoft.com/japan/ office/2010/ime/default.mspx 2 http://www.atok.com/ 3 http://www.google.com/intl/ja/ime/ 38 Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011), pages 38–42, Chiang Mai, Thailand, November 13, 2011. 5.1 Language Identification error correction in addition to language identification. Correcting Japanese learners’ error is also proposed in (Mizumoto et al., 2011). They try to correct sentences written in kana and kanji mixed, whereas we aim at texts in Roman characters. 3 Language identification is done by exact matching input sequences in English with a romanized5 Japanese dictionary. Learners sometimes directly write words in their native language without adapting to Japanese romaji style. Since we are not focusing on implementing full transliteration (Knight and Graehl, 1998), we would like to convert only Japanese words into kana. To achieve this, we use an English word dictionary because most foreign words found in learners’ sentences are English"
W11-3506,C10-1096,0,0.0204255,"mber of candidates using approximate word matching with cosine distance before calculating edit distance (Kukich, 1992). Cosine distance is calculated using character n-gram features. We set n = 1 because it covers most candidates in dictionary and reduces the number of candidates appropriately. For example, when we retrieved the approximate words for packu in our dictionary with cosine distance, the number of candidates is reduced to 163, and examples of retrieved words are kau, pakku, chikau, pachikuri, etc. Approximate word matching with cosine similarity can be performed very efficiently (Okazaki and Tsujii, 2010)9 to get candidates from a large scale word dictionary. kana よろしく おねがい します。 Muscle musical を みたい。 あなた は えいご が わかります か。 Table 2: Examples of successfully corrected word misspelled shuutmatsu do-yoobi packu kana しゅう t まつ どよおび ぱcく correct shuumatsu doyoubi pakku kana しゅうまつ どようび ぱっく some pairs have several possibilities. One of them is a pair of n and following characters. For example, we can read Japanese word kinyuu as “きん ゆう/kin-yuu: finance” and “きにゅう/kinyuu: entry.” The reason why it occurs is that n can be a syllable alone. Solving this kind of ambiguity is out of scope of this paper; and we"
W11-3506,H01-1044,0,0.0967435,"Missing"
W11-3506,J98-4003,0,\N,Missing
W12-2033,P98-1013,0,0.0553111,"s Run0. nize “keep” takes an object and a complement; in Example (5) “love” is the object and “secret” is the complement of “keep” while the former is leftextraposed. A rule-based approach may be better suited for these cases than a machine learning approach. Third, most deletion errors involve discrimination between transitive and intransitive. For instance, “NONE” in Example (6) must be changed to “for”, because “wait” is intransitive. I’ll wait NONEf or your next letter. (6) To deal with these errors, we may use rich knowledge about verbs such as VerbNet (Kipper et al., 2000) and FrameNet (Baker et al., 1998) in order to judge whether a verb is transitive or intransitive. 5.2 Error Analysis of Determiner Correction We conducted additional experiments for determiner errors and report the results here because the submitted system contained a bug. In the submitted system, while the test data were parsed by the “mixed” model, the training data and the test data were parsed by the default grammar provided with Berkeley Parser. Moreover, though there were about 5.5 million sentences in the BNC corpus, only about 286 2.7 million of them had been extracted. Though these errors seem to have improved the pe"
W12-2033,J96-1002,0,0.104285,"Missing"
W12-2033,W11-2841,0,0.041053,"Missing"
W12-2033,W10-4236,0,0.0462786,"Missing"
W12-2033,P07-1033,0,0.126783,"Missing"
W12-2033,C08-1022,0,0.374974,"Missing"
W12-2033,W07-0712,0,0.0636995,"Missing"
W12-2033,N10-1019,0,0.120128,"Missing"
W12-2033,J06-4003,0,0.0368252,"of i (3 to 5) window size phrase including the preposition prep, the value of log100 (fi + 1) The proportion pprep,i (i is 3 to 5). f pprep,i = ∑ prep,i , given the set of target prepositions T . fk,i k∈T Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic categories for all words used as surface features. As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. Table 1: Baseline features for English preposition error correction. original CLC-FCE plain sentences. We performed sentence splitting using the implementation of Kiss and Strunk (2006) in NLTK 2.0.1rc2. We conducted dependency parsing by Stanford parser 1.6.9.8 We used the features described in (Tetreault et al., 2010) as shown in Table 1 with Maximum Entropy (ME) modeling (Berger et al., 1996) as a multi-class classifier. We used the implementation of Maximum Entropy Modeling Toolkit9 with its default parameters. For web n-gram calculation, we used Google N-gram with a search system for giga-scale n-gram corpus, called SSGNC 0.4.6.10 4 System Architecture for Determiner Error Correction We focused on article error correction in the determiner error correction subtask, beca"
W12-2033,P11-1093,0,0.105332,"Missing"
W12-2033,P10-2065,0,0.183624,"pprep,i = ∑ prep,i , given the set of target prepositions T . fk,i k∈T Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic categories for all words used as surface features. As De Felice and Pulman (2008) did not perform word sense disambiguation, neither did we. Table 1: Baseline features for English preposition error correction. original CLC-FCE plain sentences. We performed sentence splitting using the implementation of Kiss and Strunk (2006) in NLTK 2.0.1rc2. We conducted dependency parsing by Stanford parser 1.6.9.8 We used the features described in (Tetreault et al., 2010) as shown in Table 1 with Maximum Entropy (ME) modeling (Berger et al., 1996) as a multi-class classifier. We used the implementation of Maximum Entropy Modeling Toolkit9 with its default parameters. For web n-gram calculation, we used Google N-gram with a search system for giga-scale n-gram corpus, called SSGNC 0.4.6.10 4 System Architecture for Determiner Error Correction We focused on article error correction in the determiner error correction subtask, because the errors related to articles significantly outnumber the errors unrelated to them. Though more than twenty types of determiners ar"
W12-2033,P07-1031,0,0.01999,"augmenting the training data for the parser model with sentences lacking articles, the span of NPs that lack an article might have better chance of being correctly recognized. In addition, dependency information was extracted from the parse using the Stanford parser 1.6.9. For each NP in the parse, we extracted a feature vector representation. We used the feature templates shown in Table 2, which are inspired by (De Felice, 2008) and adapted to the CFG representation. For the parser models, we trained the “normal” model on the WSJ part of Penn Treebank sections 02-21 with the NP annotation by Vadas and Curran (2007). The “mixed” model was trained on the concatenation of the WSJ part and its modified version. For the classification model, we used the written part of the British National Corpus (BNC) in addition to the CLC FCE Dataset, because the amount of indomain data was limited. In examples taken from the CLC FCE Dataset, the true labels after the correction were used. In examples taken from the BNC, the article of each NP was used as the label. We trained a linear classifier using opal12 with the PA-I algorithm. We also used the feature augmentation 12 Run 0 1 2 3 4 5 6 7 Table 3: Distinct configurat"
W12-2033,C98-1013,0,\N,Missing
W13-1717,C12-1025,0,0.167283,"entification (NLI) system in the NLI 2013 Shared Task. We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the"
W13-1717,C12-1027,0,0.0528286,"m in the NLI 2013 Shared Task. We apply feature selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balance"
W13-1717,P09-2012,0,0.0311602,"=3 Tree substitution grammer RB difficult NN (i, nsubj) (think, i) (nsubj, i, think) (PRP UNK-INITCKNOWNLC) (VB think) (NP RB DT ADJP NN) (JJ UNK-LC) Table 1: All features for native language identification. using the Stanford Parser 2.0.2 3 . We use tree substitution grammars as features. TSGs are generalized context-free grammars (CFGs) that allow nonterminals to re-write to tree fragments. The fragments reflect both syntactic and surface structures of a given sentence more efficiently than using several CFG rules. In practice, efficient Bayesian approaches have been proposed in prior work (Post and Gildea, 2009). In terms of the application of TSG to NLI task, (Swanson and Charniak, 2012) have shown a promising result. Post (2011) also uses TSG to judge grammaticality of a sentence written by language learners. With these previous findings in mind, we also extract TSG rules. We use the training settings and public software from Post (2011)4 , obtaining 21,020 unique TSG fragments from the training dataset of the TOEFL-11 corpus. 3 Closed Track In this section, we describe our system for the closed track. We use the tools and features described in Section 2. In our system, feature selection is perform"
W13-1717,P11-2038,0,0.0225251,"P NN) (JJ UNK-LC) Table 1: All features for native language identification. using the Stanford Parser 2.0.2 3 . We use tree substitution grammars as features. TSGs are generalized context-free grammars (CFGs) that allow nonterminals to re-write to tree fragments. The fragments reflect both syntactic and surface structures of a given sentence more efficiently than using several CFG rules. In practice, efficient Bayesian approaches have been proposed in prior work (Post and Gildea, 2009). In terms of the application of TSG to NLI task, (Swanson and Charniak, 2012) have shown a promising result. Post (2011) also uses TSG to judge grammaticality of a sentence written by language learners. With these previous findings in mind, we also extract TSG rules. We use the training settings and public software from Post (2011)4 , obtaining 21,020 unique TSG fragments from the training dataset of the TOEFL-11 corpus. 3 Closed Track In this section, we describe our system for the closed track. We use the tools and features described in Section 2. In our system, feature selection is performed using a measure based on frequency. Although Tsur 3 4 http://nlp.stanford.edu/software/lex-parser.shtml https://github"
W13-1717,P12-2038,0,0.0139454,", i, think) (PRP UNK-INITCKNOWNLC) (VB think) (NP RB DT ADJP NN) (JJ UNK-LC) Table 1: All features for native language identification. using the Stanford Parser 2.0.2 3 . We use tree substitution grammars as features. TSGs are generalized context-free grammars (CFGs) that allow nonterminals to re-write to tree fragments. The fragments reflect both syntactic and surface structures of a given sentence more efficiently than using several CFG rules. In practice, efficient Bayesian approaches have been proposed in prior work (Post and Gildea, 2009). In terms of the application of TSG to NLI task, (Swanson and Charniak, 2012) have shown a promising result. Post (2011) also uses TSG to judge grammaticality of a sentence written by language learners. With these previous findings in mind, we also extract TSG rules. We use the training settings and public software from Post (2011)4 , obtaining 21,020 unique TSG fragments from the training dataset of the TOEFL-11 corpus. 3 Closed Track In this section, we describe our system for the closed track. We use the tools and features described in Section 2. In our system, feature selection is performed using a measure based on frequency. Although Tsur 3 4 http://nlp.stanford.e"
W13-1717,C12-1158,0,0.302929,"econd language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the closed track, we tackle feature selection for increasing accuracy. We use a feature selection method based on the frequency of each feature (e.g., document frequency, TF-IDF). In the open tracks, to address the problem of imbalanced data, we tried two approaches: Capping and Sampling data in ord"
W13-1717,W13-1706,0,0.195726,"k, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the closed track, we tackle feature selection for increasing accuracy. We use a feature selection method based on the frequency of each feature (e.g., document frequency, TF-IDF). In"
W13-1717,W07-0602,0,0.392158,"Missing"
W13-1717,D11-1148,0,0.0243131,"selection using a measure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the clos"
W13-1717,U11-1015,0,0.0122657,"sure based on frequency for the closed track and try Capping and Sampling data methods for the open tracks. Our system ranked ninth in the closed track, third in open track 1 and fourth in open track 2. 1 Introduction There have been many studies using English as a second language (ESL) learner corpora. For example, automatic grammatical error detection and correction is one of the most active research areas in this field. More recently, attention has been paid to native language identification (NLI) (Brooke and Hirst, 2012; Bykh and Meurers, 2012; Brooke and Hirst, 2011; Wong and Dras, 2011; Wong et al., 2011). Native language identification is the task of identifying the ESL learner’s L1 given a learner’s essay. The NLI Shared Task 2013 (Tetreault et al., 2013) is the first shared task on NLI using the common dataset “TOEFL-11” (Blanchard et al., 2013; Tetreault et al., 2012). TOEFL-11 consists of essays written by learners of 11 native languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Koran, Spanish, Telugu, Turkish), and it contains 1,100 essays for each native language. In addition, the essay topics are balanced, and the number of topics is 8. In the closed track, we tackle"
W13-3604,N03-1017,0,0.0099261,"rs. To focus on preposition and determiner errors, we retain proposed edits that include 48 prepositions and 25 determiners listed in Table 3. best thresholds for singular and plural forms, respectively. For proper and uncountable nouns, we do not change number because of the nature of those nouns. In order to determine whether to change number or not, we create a list which consists of words frequently used as singular forms in the native corpus. 3.3 Prepositions and Determiners For preposition and determiner errors, we construct a system using a phrase-based statistical machine translation (Koehn et al., 2003) framework. The SMT-based approach functions well in corrections of conventional usage of determiners and prepositions such as “the young” and “take care of ”. The characteristic of the SMT-based approach is its ability to capture tendencies in learners’ errors. This approach translates erroneous phrases that learners often make to correct phrases. Hence, it can handle errors in conventional expressions without over-generalization. The phrase-based SMT framework which we used is based on the log-linear model (Och and Ney, 2002), where the decision rule is expressed as follow: argmax P (e|f ) ="
W13-3604,P08-1021,0,0.201996,"nd Technology (NAIST) error correction system in the CoNLL 2013 Shared Task. We constructed three systems: a system based on the Treelet Language Model for verb form and subjectverb agreement errors; a classifier trained on both learner and native corpora for noun number errors; a statistical machine translation (SMT)-based model for preposition and determiner errors. As for subject-verb agreement errors, we show that the Treelet Language Model-based approach can correct errors in which the target verb is distant from its subject. Our system ranked fourth on the official run. 1 2 Related Work Lee and Seneff (2008) tried correcting English verb errors including SVA and Vform. They proposed correction candidates with template matching on parse trees and filtered candidates by utilizing n-gram counts. Our system suggests candidates based on the Part-Of-Speech (POS) tag of a target word and filters them by using a syntactic language model. For the noun number errors, we improved the system proposed by Izumi et al. (2003). In Izumi et al. (2003), a noun number error detection method is a part of an automatic error detection system for transcribed spoken English by Japanese learners. They used a maximum entr"
W13-3604,C12-2084,1,0.823085,"Missing"
W13-3604,P02-1038,0,0.126271,"ct a system using a phrase-based statistical machine translation (Koehn et al., 2003) framework. The SMT-based approach functions well in corrections of conventional usage of determiners and prepositions such as “the young” and “take care of ”. The characteristic of the SMT-based approach is its ability to capture tendencies in learners’ errors. This approach translates erroneous phrases that learners often make to correct phrases. Hence, it can handle errors in conventional expressions without over-generalization. The phrase-based SMT framework which we used is based on the log-linear model (Och and Ney, 2002), where the decision rule is expressed as follow: argmax P (e|f ) = argmax e e M ∑ 4 Experiments 4.1 Experimental setting 4.1.1 Subject-Verb Agreement and Verb Form We describe here the training data and tools used to train our model. Our model was trained with the Berkeley LM9 version 1.1.3. We constructed the training data by concatenating the WSJ sections of the Penn Treebank and the AFP sections of the English Gigaword Corpus version 5.10 Our training data consists of about 27 million sentences. Although human-annotated parses for the WSJ are available, there is no gold standard for the AF"
W13-3604,P12-1101,0,0.0611644,"nd evaluated its performance for 18 error types, including preposition and determiner errors in the Konan-JIEM Learner Corpus. On preposition error correction, they showed that their SMT system outperformed a system using a maximum entropy model. The main difference with this work is that our new corpus collection here is about three times larger. They want *go to Nara this summer. Verbs can be a complement of another verb and preposition. The “go” in the above sentence is incorrect. It should be in the infinitive form, “to go”. 3.1.2 Treelet Language Model We used the Treelet Language Model (Pauls and Klein, 2012) for SVA and Vform error correction. Our model assigns probability to a production rule of the form r = P → C1 · · · Cd in a constituent tree T , conditioned on a context h consisting of previously generated treelets,3 where P is the parent symbol of a rule r and C1d = C1 · · · Cd are its children. 3 System Architecture 3.1 Subject-Verb Agreement and Verb Form For SVA and Vform errors, we used the Treelet Language Model (Pauls and Klein, 2012) to capture syntactic information and lexical information simultaneously. We will first show examples of SVA and Vform errors and then describe our model"
W13-3604,N04-1030,0,0.0509398,"nary classifier indicates “singular” or “plural” for all nouns except proper and uncountable nouns. First, if a noun is found in the training corpus, we extract an instance with features created by the feature template in Table 2.8 Second, we train a classifier with extracted instances and labels from the training corpus. We use unigram, bigram, and trigram features around the target word and the path features between the target word and all the other nodes in the NPs that dominate the target word as the rightmost constituent. The path feature is commonly used in semantic role labeling tasks (Pradhan et al., 2004). For the path features, we do not use the right subtree of the NP as the path features because we assume that right subtrees do not affect the number of the target word. We limit the maximum depth of the subtree containing the NP to be 3 because nodes over this limit may be noisy. To encode the relationship between the target word and another node in the NP, we append a symbol which reflects the direction of tree traversal to the label: ‘p’ for going up (parent) and ‘c’ for going down (child). For example, we show extracted features in Table 2 for the phrase “some interesting and recent topic"
W13-3604,P05-1034,0,0.079463,"Missing"
W13-3604,P06-1032,0,0.309534,"NP, pc JJ, pc recent, pp NP, ppc CC, ppc and, ppc NP, ppcc DT, ppcc some, ppcc JJ, ppcc interesting Table 2: Features used for the detection of noun number errors and example features for the phrase “some interesting and recent topics about politics and economics”. model and the language model. The translation model suggests translation hypotheses and the language model filters out ill-formed hypotheses. For an error correction system based on SMT, the translation model is constructed from pairs of original sentences and corrected sentences, and the language model is built on a native corpus (Brockett et al., 2006). Brockett et al. (2006) trained the translation model on a corpus where the errors are restricted to mass noun errors. In our case, we trained our model on a corpus with no restriction on error types. Consequently, the system corrects all types of errors. To focus on preposition and determiner errors, we retain proposed edits that include 48 prepositions and 25 determiners listed in Table 3. best thresholds for singular and plural forms, respectively. For proper and uncountable nouns, we do not change number because of the nature of those nouns. In order to determine whether to change number"
W13-3604,N12-1067,0,0.0683393,"inal revised Precision Recall F-score Precision Recall F-score ALL 0.2707 0.1832 0.2185 0.3392 0.2405 0.2814 submitted system Verb Nn Prep 0.1378 0.4452 0.2649 0.2520 0.1641 0.1286 0.1782 0.2399 0.1732 0.1814 0.5578 0.3245 0.2867 0.1708 0.1494 0.2222 0.2616 0.2046 ArtOrDet 0.3118 0.2029 0.2458 0.4027 0.2497 0.3082 additional experiments Verb Nn 0.2154 0.3687 0.0569 0.2020 0.0900 0.2610 0.3846 0.4747 0.0880 0.2137 0.1433 0.2947 Table 4: Results of the submitted system for each type of error and results of additional experiments with the SMT-based system. The score is evaluated on the m2scorer (Dahlmeier and Ng, 2012). ALL is the official result of formal run, and each of the others shows the result of the corresponding error type. Since our system did not distinguish SVA and Vform, we report the combined result for them in the column Verb. gold: The rising life expectancy is like a two side sword to the modern world. of” are features with strong weights for the plural class as expected. However, n-gram features sometimes work to the contrary of our expectations. Since the subject of “are” is “expectancies”, the sentence looks correct at first. However, this example includes not only an SVA error but also"
W13-3604,W13-1703,0,0.43257,"e trained a classifier on the mixed corpus of the leaner corpus and the native corpus. We employ a treepath feature in our system. Our SMT system for correcting preposition and Introduction Grammatical error correction is the task of automatically detecting and correcting grammatical errors in text, especially text written by second language learners. Its purpose is to assist learners in writing and helps them learn languages. Last year, HOO 2012 (Dale et al., 2012) was held as a shared task on grammatical error correction, focusing on prepositions and determiners. The CoNLL-2013 shared task (Dahlmeier et al., 2013) includes these areas and also noun number, verb form, and subject-verb agreement errors. We divide the above 5 error types into three groups: (1) subject-verb agreement (SVA) and verb form (Vform) errors, (2) noun number (Nn) errors, and (3) preposition (Prep) and determiner (ArtOrDet) errors. For the subject-verb agreement and verb form errors, we used a syntactic language model, the Treelet Language Model, because syntactic information is important for verb error correction. For the noun number errors, we used a binary classifier trained on both learner and native 26 Proceedings of the Seve"
W13-3604,W12-2006,0,0.146672,"rained a classifier on only a learner corpus. The main difference between theirs and ours is a domain of the training corpus and features we used. We trained a classifier on the mixed corpus of the leaner corpus and the native corpus. We employ a treepath feature in our system. Our SMT system for correcting preposition and Introduction Grammatical error correction is the task of automatically detecting and correcting grammatical errors in text, especially text written by second language learners. Its purpose is to assist learners in writing and helps them learn languages. Last year, HOO 2012 (Dale et al., 2012) was held as a shared task on grammatical error correction, focusing on prepositions and determiners. The CoNLL-2013 shared task (Dahlmeier et al., 2013) includes these areas and also noun number, verb form, and subject-verb agreement errors. We divide the above 5 error types into three groups: (1) subject-verb agreement (SVA) and verb form (Vform) errors, (2) noun number (Nn) errors, and (3) preposition (Prep) and determiner (ArtOrDet) errors. For the subject-verb agreement and verb form errors, we used a syntactic language model, the Treelet Language Model, because syntactic information is i"
W13-3604,N10-1140,0,\N,Missing
W13-3604,J03-4003,0,\N,Missing
W13-3604,P03-2026,0,\N,Missing
W14-7006,I13-1147,0,0.14157,". Translating Japanese to English is difficult because they belong to different language families. Na¨ıve phrase-based statistical machine translation (SMT) often fails to address syntactic difference between Japanese and English. Preordering methods are one of the simple but effective approaches that can model reordering in a long distance, which is crucial in translating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in (Hoshino et al., 2013), and extends their rules to handle abbreviation and passivization frequently found in scientific papers. Experimental results show that our proposed method improves performance of both (Hoshino et al., 2013)’s system and our phrase-based SMT baseline without preordering. 1 However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; Hoshino et al., 2013) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academ"
W14-7006,P11-1081,0,0.0668605,"Missing"
W14-7006,D10-1092,0,0.255986,"Missing"
W14-7006,2006.iwslt-evaluation.11,1,0.630392,"ating Japanese and English. Thus, we apply a predicate-argument structure-based preordering method to the Japanese-English statistical machine translation task of scientific papers. Our method is based on the method described in (Hoshino et al., 2013), and extends their rules to handle abbreviation and passivization frequently found in scientific papers. Experimental results show that our proposed method improves performance of both (Hoshino et al., 2013)’s system and our phrase-based SMT baseline without preordering. 1 However, these preordering methods are tested on limited domains: travel (Komachi et al., 2006) and patent (Katz-Brown and Collins, 2008; Hoshino et al., 2013) corpora. Translating Japanese to English in a different domain such as scientific papers is still a big challenge for preordering-based approach. For example, academic writing in English traditionally relies on passive voice to give an objective impression, but one can use either passive construction or a zeropronoun in the Japanese translation of passive construction on the English side. It is not clear whether existing preordering rules are applicable to scientific domain due to such stylistic difference. Predicate-argument str"
W14-7006,W14-7001,0,0.0460537,"Missing"
W14-7006,P03-1021,0,0.0820535,"Missing"
W14-7006,P02-1040,0,0.0886379,"Missing"
W15-4417,P06-1032,0,0.0873722,"Error Correction using Corpus Augmentation and Hierarchical Phrase-based Statistical Machine Translation Yinchen Zhao Mamoru Komachi Hiroshi Ishikawa Graduate School of System Design, Tokyo Metropolitan University, Japan chou.innchenn@gmail.com komachi@tmu.ac.jp ishikawa-hiroshi@tmu.ac.jp Abstract In this study, we describe our system submitted to the 2nd Workshop on Natural Language Processing Techniques for Educational Applications (NLP-TEA-2) shared task on Chinese grammatical error diagnosis (CGED). We use a statistical machine translation method already applied to several similar tasks (Brockett et al., 2006; Chiu et al., 2013; Zhao et al., 2014). In this research, we examine corpus-augmentation and explore alternative translation models including syntaxbased and hierarchical phrase-based models. Finally, we show variations using different combinations of these factors. 1 Introduction The concept of “translating” an error sentence into a correct one was first researched by Brockett et al. (2006). They proposed a statistical machine translation (SMT) system with noisy channel model to correct automatically erroneous sentences for learners of English as a Second Language (ESL). It seems that a stat"
W15-4417,C14-1028,0,0.206934,"ays are collected from HSK Chinese tests taken by foreign Chinese language learners, and error sentences are annotated with special marks. For example: 这就{CQ 要}由有关部门和政策管理制度来控制。 1 http://nlp.blcu.edu.cn/online-systems/hsk-language-libindexing-system.html 112 where {CQ 要} refers to a redundant word and is Score =α*Accuracy+β* F0.5+γ*(1-FP_rate) where α+β+γ = 1.0. revised with the word that follows it. 可是这两个问题同时{CJX}要解决非常不容易。 where {CJX} refers to a reordering error. We conducted a series of preliminary experiments to discover the most effective set of parameters. We followed Kunchukuttan et al. (2014) and Wang et al. (2014) in using F0.5 instead of F1. In other words, we expected our system to have high accuracy because, as Ng et al. say in CoNLL-2014, “it is important for a grammar checker that its proposed corrections are highly accurate in order to gain user acceptance.” However, we discovered that even when we used a parameter set of α=0.0, β=1.0, andγ=0.0, we still failed to reach a satisfactory correction rate. Finally, we use α=0.5, β=0.0, andγ=0.5 as a final parameter set for phrase-based and hierarchical phrase-based systems because it produces the greatest number of corrections a"
W15-4417,P05-1033,0,0.0783028,"Missing"
W15-4417,W13-4408,0,0.0565649,"Missing"
W15-4417,W14-1702,0,0.200259,"ays are collected from HSK Chinese tests taken by foreign Chinese language learners, and error sentences are annotated with special marks. For example: 这就{CQ 要}由有关部门和政策管理制度来控制。 1 http://nlp.blcu.edu.cn/online-systems/hsk-language-libindexing-system.html 112 where {CQ 要} refers to a redundant word and is Score =α*Accuracy+β* F0.5+γ*(1-FP_rate) where α+β+γ = 1.0. revised with the word that follows it. 可是这两个问题同时{CJX}要解决非常不容易。 where {CJX} refers to a reordering error. We conducted a series of preliminary experiments to discover the most effective set of parameters. We followed Kunchukuttan et al. (2014) and Wang et al. (2014) in using F0.5 instead of F1. In other words, we expected our system to have high accuracy because, as Ng et al. say in CoNLL-2014, “it is important for a grammar checker that its proposed corrections are highly accurate in order to gain user acceptance.” However, we discovered that even when we used a parameter set of α=0.0, β=1.0, andγ=0.0, we still failed to reach a satisfactory correction rate. Finally, we use α=0.5, β=0.0, andγ=0.5 as a final parameter set for phrase-based and hierarchical phrase-based systems because it produces the greatest number of corrections a"
W15-4417,W14-1703,0,0.175489,"ays are collected from HSK Chinese tests taken by foreign Chinese language learners, and error sentences are annotated with special marks. For example: 这就{CQ 要}由有关部门和政策管理制度来控制。 1 http://nlp.blcu.edu.cn/online-systems/hsk-language-libindexing-system.html 112 where {CQ 要} refers to a redundant word and is Score =α*Accuracy+β* F0.5+γ*(1-FP_rate) where α+β+γ = 1.0. revised with the word that follows it. 可是这两个问题同时{CJX}要解决非常不容易。 where {CJX} refers to a reordering error. We conducted a series of preliminary experiments to discover the most effective set of parameters. We followed Kunchukuttan et al. (2014) and Wang et al. (2014) in using F0.5 instead of F1. In other words, we expected our system to have high accuracy because, as Ng et al. say in CoNLL-2014, “it is important for a grammar checker that its proposed corrections are highly accurate in order to gain user acceptance.” However, we discovered that even when we used a parameter set of α=0.0, β=1.0, andγ=0.0, we still failed to reach a satisfactory correction rate. Finally, we use α=0.5, β=0.0, andγ=0.5 as a final parameter set for phrase-based and hierarchical phrase-based systems because it produces the greatest number of corrections a"
W15-4417,I11-1017,1,0.881671,"好消息 X4, 我 有 一个 好消息 X4) → (我 有 一 好消息 告诉 你, 我 有 一个 好消息 告诉 你) (I have a piece of good news to tell you) To determine a weight of a derivation, this model utilizes features such as generation probability, lexical weights, and phrase penalty. In addition, to avoid too many distinct yet similar translations, rules are constrained by certain filters that, for example, limit the length of the initial phrase the number of non-terminals per rule. 3 3.1 Chinese Learner Corpora Lang-8 Learner Corpus The Lang-8 Chinese Learner Corpus was built by extracting error-correct sentence pairs from the Internet (Mizumoto et al., 2011; Zhao et al., 2014). We use it as a training corpus for our SMT-based grammatical error diagnosis system in NLP-TEA-1. However, after we analyzed edit distance (ED) between error-correct sentence pairs based on word level, we determined it may not be suitable for training our translation model. As Figugre 1 shows, NLP-TEA-2 training data has ED mostly from 1 to 3 whereas Lang-8 Chinese Corpus has many ED longer than 4. 0 2 4 6 8 10 12 14 16 18 Edit Distance NLP-TEA2015 Training Data Lang-8 Chinese Corpus Figure 1: Distribution of ED in different data sets. The distribution of ED in the Lang-8"
W15-4417,P03-1021,0,0.0642015,"end our previous system (Zhao et al., 2014) to the NLP-TEA-2 shared task on Chinese grammatical error diagnosis, which is based on SMT. The main contribution of this study is as follows:  We investigate the hierarchical phrasebased model (Chiang et al., 2005) and determine that it yields higher recall and thus F score than does the phrase-based model, but is less accurate.  We increase our Chinese learner corpus by web scraping (Yu et al., 2012; Cheng et al., 2014) and show that the greater the size of the learner corpus, the better the performance.  We perform minimum error-rate training (Och, 2003) using several evaluation metrics and demonstrate that tuning improves the final F score. 2 Hierarchical phrase-based model A hierarchical phase-based model for SMT first suggested by Chiang et al. (2005). The tem first achieves proper word alignment, instead of extracting phrase alignment, the was sysand sys111 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 111–116, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing tem extracts rules in the form of s"
W15-4417,W14-1711,0,0.0472097,"d a statistical machine translation (SMT) system with noisy channel model to correct automatically erroneous sentences for learners of English as a Second Language (ESL). It seems that a statistical machine translation toolkit has become increasingly popular for grammatical error correction. In the CoNLL2014 shared task on English grammatical error correction (Ng et al., 2014), four teams of 13 participants each used a phrase-based SMT system. Grammatical error correction using a phrasebased SMT system can be improved by tuning using evaluation metrics such as F0.5 (Kunchukuttan et al., 2014; Wang et al., 2014) or even a combination of different tuning algorithms (Junczys-Dowmunt and Grundkiewicz, 2014). In addition, SMT can be merged with other methods. For example, the language modelbased and rule-based methods can be integrated into a single sophisticated but effective system (Felice et al., 2014). For Chinese, SMT has also been used to correct spelling errors (Chiu et al., 2013). Furthermore, as is shown in NLP-TEA-1, an SMT system can be applied to Chinese grammatical error correction if we can employ a large-scale learner corpus (Zhao et al., 2014). In this study, we extend our previous system"
W15-4417,C12-1184,0,0.149594,"Missing"
W15-4417,W13-3607,0,0.0851919,"Missing"
W15-4417,W14-1701,0,\N,Missing
W15-4417,O13-1005,0,\N,Missing
W15-4417,W14-1708,0,\N,Missing
W15-5013,W03-0318,0,0.119351,"Missing"
W15-5013,I13-1147,0,0.0163402,"segmentation model, which is then reused by the decoder. This process is performed iteratively, improving the phrase segmentation model. Our approach also involves pre-ordering, one of the means of coping with the reordering problem. It reorders the word order of a source language sentence in the pre-processing phase to bring the sequence of words closer to the word order of the target language. Previous work addressing pre-ordering in SOV/SVO language pairs such as Japanese and English includes Isozaki et al. (2010), Komachi et al. (2006), Katz-Brown and Collins (2008), Xu et al. (2009), and Hoshino et al. (2013). These methods use some source language information to reorder the words of source language words with manual rules: morphological analysis (Katz-Brown and Collins, 2008), dependency analysis (Katz-Brown and Collins, 2008), and predicate argument structure analysis (Komachi et al., 2006; Hoshino et al., 2013). Our method also uses dependency analysis for preprocessing, but reordering is not performed. We use a dependency parser only to extract the basic frame and dependent phrases. a non-terminal as a placeholder, they reduced the problem of reordering a complex sentence to a simple problem o"
W15-5013,W10-1736,0,0.0247745,"annotates the source language phrase boundaries. The annotated data are used to train a new phrase segmentation model, which is then reused by the decoder. This process is performed iteratively, improving the phrase segmentation model. Our approach also involves pre-ordering, one of the means of coping with the reordering problem. It reorders the word order of a source language sentence in the pre-processing phase to bring the sequence of words closer to the word order of the target language. Previous work addressing pre-ordering in SOV/SVO language pairs such as Japanese and English includes Isozaki et al. (2010), Komachi et al. (2006), Katz-Brown and Collins (2008), Xu et al. (2009), and Hoshino et al. (2013). These methods use some source language information to reorder the words of source language words with manual rules: morphological analysis (Katz-Brown and Collins, 2008), dependency analysis (Katz-Brown and Collins, 2008), and predicate argument structure analysis (Komachi et al., 2006; Hoshino et al., 2013). Our method also uses dependency analysis for preprocessing, but reordering is not performed. We use a dependency parser only to extract the basic frame and dependent phrases. a non-termina"
W15-5013,2006.iwslt-evaluation.11,1,0.496276,"nguage phrase boundaries. The annotated data are used to train a new phrase segmentation model, which is then reused by the decoder. This process is performed iteratively, improving the phrase segmentation model. Our approach also involves pre-ordering, one of the means of coping with the reordering problem. It reorders the word order of a source language sentence in the pre-processing phase to bring the sequence of words closer to the word order of the target language. Previous work addressing pre-ordering in SOV/SVO language pairs such as Japanese and English includes Isozaki et al. (2010), Komachi et al. (2006), Katz-Brown and Collins (2008), Xu et al. (2009), and Hoshino et al. (2013). These methods use some source language information to reorder the words of source language words with manual rules: morphological analysis (Katz-Brown and Collins, 2008), dependency analysis (Katz-Brown and Collins, 2008), and predicate argument structure analysis (Komachi et al., 2006; Hoshino et al., 2013). Our method also uses dependency analysis for preprocessing, but reordering is not performed. We use a dependency parser only to extract the basic frame and dependent phrases. a non-terminal as a placeholder, the"
W15-5013,C12-2060,0,0.0303827,"Missing"
W15-5013,W15-5001,0,0.0345878,"uage (Japanese) and then correctly produce the SVO of the target language (English). Concretely, we devise a dependencybased method that extracts a sentence’s frame (hareafter “basic frame”), consisting of the predicate and its direct children (hereafter “anchor words”), and its dependent phrases consisting of the anchor words and their all descendants. After extracting these words and phrases, our method translates them separately and then combines their translation. We conducted an experiment with the proposed method on a Japanese-to-English task at the Second Workshop on Asian Translation (Nakazawa et al., 2015). Although the results of our method are not positive, we discuss potential improvements. The rest of this paper is organized as follows. In the next section, we discuss related work. In Section 3, the details of our method are explained. Then, we describe our experiments and analyze the results. There are various approaches to statistical machine translation (SMT). In particular, phrase-based SMT (PBSMT) is used as a de facto standard for many language pairs because it works robustly across languages and it is easy to implement. However, the results of PBSMT can include ungrammatical sentence"
W15-5013,W10-1762,0,0.162931,"ed SMT therefore it cannot handle long-distance reordering that frequently occurs in these language pairs. To incorporate syntactic information into the PBSMT framework, we attempt to identify the 2 Related Work A substantial, systematic difference in word orders creates difficulty for SMT, especially, PBSMT, which is not based on syntactic phrases. Good translation can be achieved in such situations by segmenting the input sentences into portion for simpler and adequate scale inputs. For translating a long and complex sentence composed of several clauses in English into Japanese translation, Sudoh et al. (2010) proposed segmenting the sentence into clauses that include non-terminals as placeholders corresponding to embedded clauses using an HPSG parser, translating the clauses, and then replacing the nonterminals with the corresponding clause’s translations. By representing an embedded clause with 99 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 99‒104, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s). Figure 1 : Illustrative example of our method. rated into the log-linear model of PBSMT, and the phrase segmenter based on the decoder annotates the sou"
W15-5013,N09-1028,0,0.0349101,"to train a new phrase segmentation model, which is then reused by the decoder. This process is performed iteratively, improving the phrase segmentation model. Our approach also involves pre-ordering, one of the means of coping with the reordering problem. It reorders the word order of a source language sentence in the pre-processing phase to bring the sequence of words closer to the word order of the target language. Previous work addressing pre-ordering in SOV/SVO language pairs such as Japanese and English includes Isozaki et al. (2010), Komachi et al. (2006), Katz-Brown and Collins (2008), Xu et al. (2009), and Hoshino et al. (2013). These methods use some source language information to reorder the words of source language words with manual rules: morphological analysis (Katz-Brown and Collins, 2008), dependency analysis (Katz-Brown and Collins, 2008), and predicate argument structure analysis (Komachi et al., 2006; Hoshino et al., 2013). Our method also uses dependency analysis for preprocessing, but reordering is not performed. We use a dependency parser only to extract the basic frame and dependent phrases. a non-terminal as a placeholder, they reduced the problem of reordering a complex sen"
W15-5013,W14-7001,0,\N,Missing
W16-3906,I11-1108,0,0.0311414,"m that can capture the exact location of the disaster in real time. The main contributions of this study are as follows: • We proposed a new task of flood disaster analysis and created a flood event dataset. • We developed a supervised classifier to classify whether a disaster occurs to predict time when a disaster occurs and to predict distance between a reporter and disaster. 2 Related Work With the development of the information society, studies of disaster information analysis from social media have attracted attention in the NLP literature. In the case of the Great East Japan Earthquake, Neubig et al. (2011) constructed an information extraction system that visualized the security of the people in the disaster-stricken area using Twitter. They made a corpus by annotating safety information to tweets and trained a classifier to categorize the security of the population. We also perform disaster information analysis; however, we focus on event classification rather than classification of personal information. In addition, Varga et al. (2013) extracted distress and support information concerning disaster from tweets. Their distress recognizer used geographical information as a feature; however, they"
W16-3906,J12-2002,0,0.0367983,"Missing"
W16-4603,D16-1162,0,0.0126436,"hat the solution in order for PBSMT to handle these errors requires a translation model with a syntactic rule such as a constituent structure or dependency structure. On the other hand, NMT has many more errors in terms of substitution than with PBSMT. In substitution, there were errors in which the meanings of the source word and the target word were not similar at all. For example, ‘sweet potato’ was output as the translated word for ‘キャベツ’ (cabbage). To solve this problem, the use of lexicon probability obtained from a phrase table or a dictionary is considered promising for the NMT model (Arthur et al., 2016). There were many omission errors and addition errors in both PBSMT and NMT. In particular, omission errors account for a large percentage in both methods. The following example shows that omission errors or addition errors occur in either, or both methods. (14) ホーム ベーカリー の 生地 作り コース で 生地 を 作る ． bread maker of dough setting with dough ACC make. PBSMT: Make the dough in the bread maker to make the dough. NMT: Make the dough using the dough setting. Reference: Use the bread dough function on the bread maker to make the bread dough. In terms of omission or addition errors, PBSMT and NMT output er"
W16-4603,D16-1025,0,0.0401019,"al because some of step sentences have an order relationship in which reference is made to words that have previously appeared, especially ingredients with zero pronouns. In other words, better translation performance can be obtained if ingredients in the flow of the recipe are correctly detected. Mori et al. (2014) annotated a role label for each ingredient in a monolingual recipe corpus to model the recipe flow. If the information is appropriately adapted to the machine translation process well, some problems encountered by the machine translation systems in the recipe domain can be solved. Bentivogli et al. (2016) conducted error analysis of PBSMT and NMT with the English-German language pair. THe authors were the first to work on error analysis of NMT and also with PBSMT and tree-based statistical machine translation in which they analyzed errors in several ways. The automatic evaluation metrics used in their study were BLEU and two types of modified translation error rate (TER) (Snover et al., 2006): Human-targeted TER and Multi-reference TER. For analysis of linguistic errors, three error categories were used: morphology errors, lexical errors and word order errors. In terms of word order errors, th"
W16-4603,L16-1389,1,0.815654,"del. The model had 512-dimensional word embeddings and 512-dimensional hidden units with one layer LSTM. We set the vocabulary size of the model to 30, 000, and we did not perform any unknown word processing during training. Adagrad (Duchi et al., 2011) was used with the initial learning rate of 0.01 as an optimization method. The initial values for word embeddings on both sides were obtained by training word2vec2 with default setting because better results were shown in our preliminary experiments. The initial word embeddings on the source side were learned with a raw Japanese recipe corpus (Harashima et al., 2016) consisting of approximately 13 million step sentences. Conversely, initial word embeddings on the target side were learned with approximately 120, 000 English step sentences included in the parallel corpus. Title sentences were not used for learning because they were often written with free expression largely different depending on each recipe. Ingredient sentences were also not used because most of them consisted of a few words. The batch size was set to 64 and the number of epochs was set to 10. We selected the model that gave the highest BLEU score in the development set for testing. Beam"
W16-4603,W11-2123,0,0.0337655,"quent words. The method also has the disadvantage that it often generates target words that do not correspond to any words in the source sentences (Tu et al., 2016). The setting for each method in this study was as follows. We used the parallel corpus described in Section 2 as our corpus, Moses (ver.2.1.1) (Koehn et al., 2007) as the PBSMT method, and conducted Japanese word segmentation using MeCab (Kudo et al., 2004) with IPADIC (ver.2.7.0) as the dictionary. Word alignment was obtained by running Giza++. The language model was learned with the English side of the recipe corpus using KenLM (Heafield, 2011) with 5-gram. Other resources in English were not used for training the language model because the style of recipe texts is different from general corpus in that it contains many noun phrases in title and ingredient, and many imperatives in step. The size of the phrase table was approximately 3 million pairs, and we used the development set to tune the weights for all features by minimum error rate training (MERT) (Och, 2003). We used the default parameter 6 for the distortion limit. We reimplemented the NMT model in accordance with (Bahdanau et al., 2015). Note that we used long short-term me"
W16-4603,D10-1092,0,0.177101,"itten with free expression largely different depending on each recipe. Ingredient sentences were also not used because most of them consisted of a few words. The batch size was set to 64 and the number of epochs was set to 10. We selected the model that gave the highest BLEU score in the development set for testing. Beam search for decoding in NMT was not carried out. When testing, the output length was set up to 40 words. Each output was evaluated via two metrics: bilingual evaluation understudy (BLEU) (Papineni et al., 2002) score and rank-based intuitive bilingual evaluation score (RIBES) (Isozaki et al., 2010). BLEU is more sensitive to word agreement than RIBES, whereas RIBES is more sensitive to word order evaluation. We set two hyper-parameters for RIBES: α was 0.25 and β was 0.10. 4 Error Classification of Recipe Translation We conducted blackbox analysis on the outputs of PBSMT and NMT. Blackbox analysis is a type of analysis that does not take into account how translation output is obtained. The error classification used for this analysis is based on the MQM ANNOTATION DECISION TREE (Burchardt and Lommel, 2014) because it makes the classification of each error more consistent. The method clas"
W16-4603,N03-1017,0,0.00924812,"). The former was used to tune our translation models, while the latter was used to analyze translation errors and to evaluate the translation models. 3 Machine Translation Methods We used two methods in our experiments: PBSMT and NMT. The former has been widely accepted as one of the bases of machine translation systems that we generally use, whereas the latter has been gaining great attention in research community because of its fluency and simplicity. PBSMT obtains a language model and a translation model (phrase table) from a parallel corpus and translates sentences based on these models (Koehn et al., 2003). The method achieves good performance on any language pair consisting of languages whose word orders are similar to each other, as in the case of English and French. Conversely, it performs poorly when the word orders of the languages differ, as in the case of English and Japanese. In addition, PBSMT often generates ungrammatical sentences because it does not consider syntactic information. 59 NMT embeds each source word into a d-dimensional vector and generates a target sentence from the vectors (Sutskever et al., 2014). Even though the method does not use any syntactic information, it can g"
W16-4603,P07-2045,0,0.0150336,"erate grammatical sentences. However, due to the execution time it requires, NMT generally limits the size of the vocabulary for a target language. Therefore, compared with PBSMT, which can handle many phrases in the target language, NMT has a disadvantage in that it cannot generate low frequent words. The method also has the disadvantage that it often generates target words that do not correspond to any words in the source sentences (Tu et al., 2016). The setting for each method in this study was as follows. We used the parallel corpus described in Section 2 as our corpus, Moses (ver.2.1.1) (Koehn et al., 2007) as the PBSMT method, and conducted Japanese word segmentation using MeCab (Kudo et al., 2004) with IPADIC (ver.2.7.0) as the dictionary. Word alignment was obtained by running Giza++. The language model was learned with the English side of the recipe corpus using KenLM (Heafield, 2011) with 5-gram. Other resources in English were not used for training the language model because the style of recipe texts is different from general corpus in that it contains many noun phrases in title and ingredient, and many imperatives in step. The size of the phrase table was approximately 3 million pairs, an"
W16-4603,W04-3230,0,0.0168149,"ts the size of the vocabulary for a target language. Therefore, compared with PBSMT, which can handle many phrases in the target language, NMT has a disadvantage in that it cannot generate low frequent words. The method also has the disadvantage that it often generates target words that do not correspond to any words in the source sentences (Tu et al., 2016). The setting for each method in this study was as follows. We used the parallel corpus described in Section 2 as our corpus, Moses (ver.2.1.1) (Koehn et al., 2007) as the PBSMT method, and conducted Japanese word segmentation using MeCab (Kudo et al., 2004) with IPADIC (ver.2.7.0) as the dictionary. Word alignment was obtained by running Giza++. The language model was learned with the English side of the recipe corpus using KenLM (Heafield, 2011) with 5-gram. Other resources in English were not used for training the language model because the style of recipe texts is different from general corpus in that it contains many noun phrases in title and ingredient, and many imperatives in step. The size of the phrase table was approximately 3 million pairs, and we used the development set to tune the weights for all features by minimum error rate train"
W16-4603,W15-2206,0,0.023235,"ted recipes, and discuss available room for improvements. 1 Introduction In recent years, an increasing amount of recipe data has become available on the web. For example, as of September 2016, more than 2.45 million recipes are available on cookpad, 1 million on Yummly, and 0.3 million on Allrecipes, to name a few. These recipes are from all over the world, and are written in various languages, including English and Japanese. However, language barriers may prevent the users from discovering recipes of local specialities. Many researchers have focused on various tasks such as recipe analysis (Maeta et al., 2015), information retrieval (Yasukawa et al., 2014), summarization (Yamakata et al., 2013), and recommendation (Forbes and Zhu, 2011). However, to date, little work has been done on machine translation of recipe texts. In particular, Japanese foods are gaining popularity because they are considered healthy. We believe that many people would be able to use cooking recipes currently available only in Japanese if those Japanese recipes were translated into other languages. In this study, we translated recipes via machine translation and investigated the advantages and disadvantages of machine transla"
W16-4603,mori-etal-2014-flow,0,0.0321161,"ar general. Repetition of the same word and phrase were commonly seen in NMT but never in PBSMT. (18) 6 NMT: leave to steam for about 2 hours , and open the pot , and open the pot . Related Work In machine translation in the recipe domain, solving zero-anaphora analysis problems appears to be essential because some of step sentences have an order relationship in which reference is made to words that have previously appeared, especially ingredients with zero pronouns. In other words, better translation performance can be obtained if ingredients in the flow of the recipe are correctly detected. Mori et al. (2014) annotated a role label for each ingredient in a monolingual recipe corpus to model the recipe flow. If the information is appropriately adapted to the machine translation process well, some problems encountered by the machine translation systems in the recipe domain can be solved. Bentivogli et al. (2016) conducted error analysis of PBSMT and NMT with the English-German language pair. THe authors were the first to work on error analysis of NMT and also with PBSMT and tree-based statistical machine translation in which they analyzed errors in several ways. The automatic evaluation metrics used"
W16-4603,P03-1021,0,0.0251802,"(ver.2.7.0) as the dictionary. Word alignment was obtained by running Giza++. The language model was learned with the English side of the recipe corpus using KenLM (Heafield, 2011) with 5-gram. Other resources in English were not used for training the language model because the style of recipe texts is different from general corpus in that it contains many noun phrases in title and ingredient, and many imperatives in step. The size of the phrase table was approximately 3 million pairs, and we used the development set to tune the weights for all features by minimum error rate training (MERT) (Och, 2003). We used the default parameter 6 for the distortion limit. We reimplemented the NMT model in accordance with (Bahdanau et al., 2015). Note that we used long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) instead of gated recurrent unit (Cho et al., 2014) for each recurrent neural network (RNN) unit of the model. The model had 512-dimensional word embeddings and 512-dimensional hidden units with one layer LSTM. We set the vocabulary size of the model to 30, 000, and we did not perform any unknown word processing during training. Adagrad (Duchi et al., 2011) was used with the initi"
W16-4603,P02-1040,0,0.094803,"the parallel corpus. Title sentences were not used for learning because they were often written with free expression largely different depending on each recipe. Ingredient sentences were also not used because most of them consisted of a few words. The batch size was set to 64 and the number of epochs was set to 10. We selected the model that gave the highest BLEU score in the development set for testing. Beam search for decoding in NMT was not carried out. When testing, the output length was set up to 40 words. Each output was evaluated via two metrics: bilingual evaluation understudy (BLEU) (Papineni et al., 2002) score and rank-based intuitive bilingual evaluation score (RIBES) (Isozaki et al., 2010). BLEU is more sensitive to word agreement than RIBES, whereas RIBES is more sensitive to word order evaluation. We set two hyper-parameters for RIBES: α was 0.25 and β was 0.10. 4 Error Classification of Recipe Translation We conducted blackbox analysis on the outputs of PBSMT and NMT. Blackbox analysis is a type of analysis that does not take into account how translation output is obtained. The error classification used for this analysis is based on the MQM ANNOTATION DECISION TREE (Burchardt and Lommel,"
W16-4603,2006.amta-papers.25,0,0.0499483,"l the recipe flow. If the information is appropriately adapted to the machine translation process well, some problems encountered by the machine translation systems in the recipe domain can be solved. Bentivogli et al. (2016) conducted error analysis of PBSMT and NMT with the English-German language pair. THe authors were the first to work on error analysis of NMT and also with PBSMT and tree-based statistical machine translation in which they analyzed errors in several ways. The automatic evaluation metrics used in their study were BLEU and two types of modified translation error rate (TER) (Snover et al., 2006): Human-targeted TER and Multi-reference TER. For analysis of linguistic errors, three error categories were used: morphology errors, lexical errors and word order errors. In terms of word order errors, they also conducted fine-grained word order error analysis in which they took part-of-speech tagging and dependency parsing into account. Ishiwatari et al. (2016) used the same recipe corpus as we used for domain adaptation of SMT without a sentence-aligned parallel corpus. In their research, the MT system was trained only with an out-domain corpus that consisted of words related to Japanese hi"
W16-4603,P16-1008,0,0.033767,"ional vector and generates a target sentence from the vectors (Sutskever et al., 2014). Even though the method does not use any syntactic information, it can generate grammatical sentences. However, due to the execution time it requires, NMT generally limits the size of the vocabulary for a target language. Therefore, compared with PBSMT, which can handle many phrases in the target language, NMT has a disadvantage in that it cannot generate low frequent words. The method also has the disadvantage that it often generates target words that do not correspond to any words in the source sentences (Tu et al., 2016). The setting for each method in this study was as follows. We used the parallel corpus described in Section 2 as our corpus, Moses (ver.2.1.1) (Koehn et al., 2007) as the PBSMT method, and conducted Japanese word segmentation using MeCab (Kudo et al., 2004) with IPADIC (ver.2.7.0) as the dictionary. Word alignment was obtained by running Giza++. The language model was learned with the English side of the recipe corpus using KenLM (Heafield, 2011) with 5-gram. Other resources in English were not used for training the language model because the style of recipe texts is different from general co"
W16-4607,P05-1066,0,0.0776155,"fluency and adequacy. As a result, the proposed method improved fluency (NRM:NRM+PT+WA = 17.5:20) but not adequacy (NRM:NRM+PT+WA = 19:14.5). Although the outputs of two methods are similar, the proposed method favored fluent translation and resulted in slight improvements in BLEU and RIBES. 6 Related Work There are several studies on phrase reordering of statistical machine translation. They are divided into three groups: in-ordering such as distance-based reordering (Koehn et al., 2003) and lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), pre-ordering (Collins et al., 2005; Isozaki et al., 2010b; Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). In-ordering is performed during decoding, pre-ordering is performed as pre-processing before decoding and post-ordering is executed as post-processing after decoding. In this section, we explain other reordering methods other than lexicalized reordering. In early studies on PBSMT, a simple distance-based reordering penalty was used (Koehn et al., 2003). 101 It worked fairly well for some language pairs with similar word order such as English-French but is not appropriate for"
W16-4607,D08-1089,0,0.48787,"ults show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reorder"
W16-4607,P12-2061,0,0.0194978,"2010b) parse source sentences and reorder the words using hand-crafted rules. (2) Discriminative pre-ordering models (Tromble and Eisner, 2009; Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015) learn whether children of each node should be reordered using (automatically) aligned parallel corpus. However, pre-ordering models cannot use the target language information in decoding. Therefore, optimizing phrase ordering using target-side features like phrase translation probability and word alignment is not possible, as done in our proposed method. Post-ordering methods (Sudoh et al., 2011; Goto et al., 2012) are sometimes used in Japanese-toEnglish translation. They first translate Japanese input into head final English texts, then reorder head final English texts into English word orders. Post-ordering methods have the advantage of being able to use syntactic features at low computational cost, but need an accurate parser on the target side. 7 Conclusion In this study, we improved a neural reordering model in PBSMT using phrase translation and word alignment. We proposed phrase translation and word alignment features to construct phrase vectors. The experimental results demonstrate that our prop"
W16-4607,P15-2023,1,0.884694,"d method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014) addressed the problem"
W16-4607,D10-1092,1,0.878159,"Missing"
W16-4607,W10-1736,1,0.91174,"points higher than that of all the test data) for instances including the NULL alignment. 5.3 MT Evaluation We investigate whether our reordering system improves translation accuracy. We use our reordering model for N-best re-ranking and optimize BLEU (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003). We output a 1,000-best candidate list of translations that Moses generated for development data and replace the lexical reordering score of Moses with the score of the proposed method. Then, we re-tune the weights of the Moses features using MERT again. BLEU-4, RIBES (Isozaki et al., 2010a) and WER are used as measures for evaluation. Table 5 shows the BLEU, RIBES and WER scores of the basic system and our proposed system. Bold scores represent the highest accuracies. When we compare the plain NRM and the proposed method with LRM, we confirm significant differences in BLEU, RIBES and WER scores on Japanese-to-English and English-to-Japanese translations using bootstrap resampling. Unfortunately, the proposed method is not able to identify significant differences in comparison with NRM. The reordering accuracy does not necessarily relate to the translation accuracy because we m"
W16-4607,N03-1017,0,0.272666,"ambiguity, data sparseness and noises in a phrase table. Previous neural reordering model is successful to solve the first and second problems but fails to address the third one. Therefore, we propose new features using phrase translation and word alignment to construct phrase vectors to handle inherently noisy phrase translation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the w"
W16-4607,P07-2045,0,0.123911,"The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some"
W16-4607,D13-1054,0,0.0917624,"a distant language pairs like Japanese and English often contain the NULL alignment and mis-aligned words. On the 1 We experimented in the Kyoto Free Translation Task. 95 Figure 1: Four orientations, namely Monotone, Swap, Discontinuous-right and Discontinuous-left, are shown. Monotone means that the source phrases f ai , f ai−1 are adjoining and monotonic with respect to the target phrases ei , ei−1 . Swap means f ai , f ai−1 are adjoining and swapping. Discontinuous-right means f ai , f ai−1 are separated and monotonic, and Discontinuous-left means f ai , f ai−1 are separated and swapping. Li et al. (2013) proposed an NRM, which uses a deep neural network to address the problems of high ambiguity and data sparsity. We describe the NRM in the next section and propose our model to improve the NRM to address the problem of noisy phrases in Section 4. 3 Neural Reordering Model Li et al. (2013) tackled the ambiguity and sparseness problem by distributed representation of phrases. The distributed representation maps sparse phrases into a dense vector space where elements with similar roles are expected to be located close to each other. 3.1 Distributed Representation of Phrases Socher et al. (2011) p"
W16-4607,P06-1090,0,0.0230635,"rce phrases f = f a1 , . . . , f ai , . . . , f aI , we translate and reorder the phrases to generate a sequence of target phrases e = e1 , . . . , ei , . . . , eI . Here a = a1 , . . . , aI expresses the alignment between the source phrase f ai and the target phrase ei . The alignment a can be used to represent the phrase orientation o. Three orientations with respect to previous phrase (Monotone, Swap, Discontinuous) are typically used in lexicalized reordering models (Galley and Manning, 2008). However, because global phrase reordering appears frequently in Japanese-to-English translation, Nagata et al. (2006) proposed four orientations instead of three by dividing the Discontinous label. In Figure 1, we show four orientations, called Monotone (Mono), Swap, Discontinuous-right (Dright ) and Discontinuous-left (Dleft ). Using alignments ai and ai−1 , orientation oi respected to the target phrases ei , ei−1 follows:   Mono (ai − ai−1 = 1)    Swap (a − a i i−1 = −1) (1) oi = Dright (ai − ai−1 > 1)    D (ai − ai−1 &lt; −1) left If the reordering probability of every phrase is expressed as P (oi |f ai , ei ), that of the sentence can be approximated as I ∏ I P (a1 |e) = P (oi |f ai , ei ). (2) i"
W16-4607,P15-1021,0,0.079694,"th phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014) addressed the problem of reordering amb"
W16-4607,J03-1002,0,0.0172476,"sion corresponding to the NULL word. Table 1 explains each dimension of W A. For example, the fourth dimension of W A of the word “日本 (Japan)” in Figure 2 is 1 because the aligned word “Japan” is located at the center of the phrase. 5 Experiment We conduct two kinds of experiments: intrinsic evaluation of reordering accuracy and extrinsic evaluation of MT quality. 5.1 Setting We use the Kyoto Free Translation Task2 (KFTT) for our experiment. It is a task for Japanese-to-English translation that focuses on Wikipedia articles. We use KyTea3 (ver.0.4.7) for Japanese word segmentation and GIZA++ (Och and Ney, 2003) with grow-diag-final-and for word alignment. We extract 70M phrase bigram pairs and automatically annotate the correct reordering orientation using Moses (Koehn et al., 2007). We filter out phrases that appear only once. We randomly divide the parallel corpus into training, development, and test. We retain 10K instances for development and test and use 1M instances for training. We experimented 15, 25, 50, and 100-dimensional word vectors; 25-dimensional word vectors are used in all experiments involving our model. Thus, we set the vector size of the recursive auto-encoder to 31, to include t"
W16-4607,P03-1021,0,0.0265921,"at the instances of Mono are not affected much by the NULL alignment, because they contain less NULL alignment (See the top row in Table 4). Overall, as compared with the NRM, our proposed method using phrase translation and word alignment improves the accuracy by 3.17 points (1.5 points higher than that of all the test data) for instances including the NULL alignment. 5.3 MT Evaluation We investigate whether our reordering system improves translation accuracy. We use our reordering model for N-best re-ranking and optimize BLEU (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003). We output a 1,000-best candidate list of translations that Moses generated for development data and replace the lexical reordering score of Moses with the score of the proposed method. Then, we re-tune the weights of the Moses features using MERT again. BLEU-4, RIBES (Isozaki et al., 2010a) and WER are used as measures for evaluation. Table 5 shows the BLEU, RIBES and WER scores of the basic system and our proposed system. Bold scores represent the highest accuracies. When we compare the plain NRM and the proposed method with LRM, we confirm significant differences in BLEU, RIBES and WER sco"
W16-4607,P02-1040,0,0.0949791,"t , whereas that of Mono is not improved. This result suggests that the instances of Mono are not affected much by the NULL alignment, because they contain less NULL alignment (See the top row in Table 4). Overall, as compared with the NRM, our proposed method using phrase translation and word alignment improves the accuracy by 3.17 points (1.5 points higher than that of all the test data) for instances including the NULL alignment. 5.3 MT Evaluation We investigate whether our reordering system improves translation accuracy. We use our reordering model for N-best re-ranking and optimize BLEU (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003). We output a 1,000-best candidate list of translations that Moses generated for development data and replace the lexical reordering score of Moses with the score of the proposed method. Then, we re-tune the weights of the Moses features using MERT again. BLEU-4, RIBES (Isozaki et al., 2010a) and WER are used as measures for evaluation. Table 5 shows the BLEU, RIBES and WER scores of the basic system and our proposed system. Bold scores represent the highest accuracies. When we compare the plain NRM and the proposed method with LRM, we confi"
W16-4607,D11-1014,0,0.0181379,"ping. Li et al. (2013) proposed an NRM, which uses a deep neural network to address the problems of high ambiguity and data sparsity. We describe the NRM in the next section and propose our model to improve the NRM to address the problem of noisy phrases in Section 4. 3 Neural Reordering Model Li et al. (2013) tackled the ambiguity and sparseness problem by distributed representation of phrases. The distributed representation maps sparse phrases into a dense vector space where elements with similar roles are expected to be located close to each other. 3.1 Distributed Representation of Phrases Socher et al. (2011) proposed the recursive autoencoder, which recursively compresses a word vector and generates a phrase vector with the same dimension as the word vector. We define a word vector of u dimension x ∈ Ru , an encoding weight matrix We ∈ Ru×2u , and a bias term be . A phrase vector p1:2 is constructed as follows: p1:2 = f (We [x1 ; x2 ] + be ) (3) f is an activation function such as tanh, which is used in our experiments. When a phrase consists of more than two words, we compute a phrase vector p1:n recursively from the phrase vector p1:n−1 and the word vector xn . p1:n = f (We [p1:n−1 ; xn ] + be"
W16-4607,2011.mtsummit-papers.36,1,0.922434,"ments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014) addressed the problem of reordering ambiguity and data sparsity using a neural"
W16-4607,N04-4026,0,0.353693,"nslation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. How"
W16-4607,D09-1105,0,0.0217846,"for some language pairs with similar word order such as English-French but is not appropriate for distant language pairs including Japanese-English. Lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008) introduced lexical constraints of the phrase reordering and not just penalizing long-distance reordering. Pre-ordering methods can be divided into two types: (1) Rule-based preprocessing methods (Collins et al., 2005; Isozaki et al., 2010b) parse source sentences and reorder the words using hand-crafted rules. (2) Discriminative pre-ordering models (Tromble and Eisner, 2009; Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015) learn whether children of each node should be reordered using (automatically) aligned parallel corpus. However, pre-ordering models cannot use the target language information in decoding. Therefore, optimizing phrase ordering using target-side features like phrase translation probability and word alignment is not possible, as done in our proposed method. Post-ordering methods (Sudoh et al., 2011; Goto et al., 2012) are sometimes used in Japanese-toEnglish translation. They first translate Japanese input into head final English texts, the"
W16-4607,I11-1004,1,0.876014,"that the proposed method works well with phrase pairs including NULL alignments. 1 Introduction Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) has been widely used in the last decade. One major problem with PBSMT is word reordering. Since PBSMT models the translation process using a phrase table, it is not easy to incorporate global information during translation. There are many methods to address this problem, such as lexicalized reordering (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008), distance-based reordering (Koehn et al., 2003), pre-ordering (Wu et al., 2011; Hoshino et al., 2015; Nakagawa, 2015), and post-ordering (Sudoh et al., 2011). However, word reordering still faces serious errors, especially when the word order greatly differs in two languages, such as the case between English and Japanese. In this paper, we focus on the lexicalized reordering model (LRM), which directly constrains reordering of phrases in PBSMT. LRM addresses the problem of a simple distance-based reordering approach in distant language pairs. However, there are some disadvantages: (1) reordering ambiguity, (2) data sparsity and (3) noisy phrases pairs. Li et al. (2014)"
W16-4620,D10-1092,0,0.05174,"entence to become active. However, the encoder-decoder model did not know what to generate as a subject if the training data had only a few examples of an active sentence for that verb. On the other hand, when we forced the voice of the generated sentence to become passive, we failed to find any tendencies of this type of the failure. We would like to do some additional investigation on the tendency of this result. 4.2 Experiments with Predicated Voice: TMU at WAT 2016 Table 4 shows the results of two methods submitted for the shared task at WAT 2016 (Nakazawa et al., 2016a). The BLEU, RIBES (Isozaki et al., 2010), and AMFM (Banchs et al., 2015) were calculated 6 We were not able to submit REFERENCE for the human evaluation because we were not allowed to look at the references in WAT 2016. 207 Example 1 Example 2 Example 3 Source Reference Controlling to Active Controlling to Passive Source Reference Controlling to Active Controlling to Passive Source Reference Controlling to Active Controlling to Passive Example 4 Example 5 Source Reference Controlling to Active Controlling to Passive Source Reference Controlling to Active Controlling to Passive 熱戻り反応の機構を議論した。 This paper discusses the mechanism of the"
W16-4620,D16-1140,0,0.159696,"s ...” and, sometimes, “... are introduced.” Therefore, it is possible that the translation model failed to learn the correspondence between Japanese and English. Recently, recurrent neural networks (RNNs) such as encoder-decoder models have gained considerable attention in machine translation because of their ability to generate fluent sentences. However, compared to traditional statistical machine translation, it is not straightforward to interpret and control the output of the encoder-decoder models. Several attempts have been made to control the output of the encoderdecoder models. First, Kikuchi et al. (2016) proposed a new Long Short-Term Memory (LSTM) network to control the length of the sentence generated by an encoder-decoder model in a text summarization task. In their experiment, they controlled the sentence length while maintaining the performance compared to the results of previous works. Second, Sennrich et al. (2016) attempted to control the honorific in English-German neural machine translation (NMT). They trained an attentional encoder-decoder model using English (source) data to which the honorific information of a German (target) sentence was added. They restricted the honorific on t"
W16-4620,W02-2016,0,0.157719,"Missing"
W16-4620,D15-1166,0,0.0189482,"d our system in the configuration of PREDICT for pairwise crowdsourcing evaluation. It improved by 1.40 points in the BLEU score compared to the NMT baseline. Since we did not perform an ensemble learning for PREDICT, we expected a similar improvement in the BLEU score if we combined multiple models of PREDICT using an ensemble technique. 5 Related Work An NMT framework consists of two recurrent neural networks (RNNs), called the RNN encoder-decoder, proposed by Cho et al. (2014) and Sutskever et al. (2014). The accuracy of NMT improves by using the attention structure (Bahdanau et al., 2015; Luong et al., 2015). However, the optimization of an RNN using log-likelihood does not always yield a satisfactory performance depending on the tasks at hand. For example, one may prefer a polite expression for generating conversation in a dialog system. Thus, several methods have been proposed several methods to control the output of encoder-decoder models. First, Kikuchi et al. (2016) tried to control the length of the sentence generated by an encoder-decoder model in a text summarization task. They proposed four methods for restricting the length in the text summarization task and compared them. In their resu"
W16-4620,W16-4601,0,0.0207209,"Missing"
W16-4620,L16-1350,0,0.0222062,"Missing"
W16-4620,P02-1040,0,0.0952295,"Missing"
W16-4620,N16-1005,0,0.0801939,"iffer because the use of the verbs between the source and the target languages is different. In particular, English uses the passive voice to change the word order of a sentence for object topicalization to encode the information structure. Thus, it is beneficial to control the syntactic structure of the English sentences for discourse-aware machine translation. Moreover, if the voice of the generated sentence fluctuates at each sentence, it is difficult to train a translation model consistently. In this paper, we attempt to add a ability of voice control to an encoder-decoder model, based on Sennrich et al. (2016), which controls the honorifics in English-German neural machine translation. They restricted the honorifics of the generated sentence by adding the honorific information to the source side. Instead of the honorific information, we extracted the voice information of the target sentence as a gold standard label to annotate the source sentence. At the test phase, we specified the voice of the generated 204 Figure 2: Flow of the voice prediction for testing an NMT. sentence, and instructed the model to translate along with it. In the following experiment, we used the attentional encoder-decoder m"
W17-5703,W15-3014,0,0.019616,"re have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sentence using a translation dictionary. This method needs to align training sentence pairs before training to learn correspondences between OOV words and their translations. In the method described in this paper, we need no word alignment, and we retain the meaning of the original word by paraphrasing the target side of the training corpus. Jean et al. (2015) proposed another post-processing method that translates each OOV word with the word that has the largest attention weight in the source sentence using a translation dictionary. Their method does not need word alignment, but it still does not necessarily consider the meaning in the target language, unlike our paraphrasing approach. Sennrich et al. (2016) applied byte pair encoding (BPE) to source and target corpora to split OOV words into units of frequent substrings to reduce the OOV rate. Their method splits words greedily without considering their meaning. Since we use lexical paraphrasing"
W17-5703,P02-1040,0,0.0989452,"on (SMT). However, NMT has a problem of high computational cost because it addresses the output generation task by solving a classification problem in vocabulary dimension. Typically, NMT has to restrict the size of the vocabulary to reduce the computational cost. Therefore, the target language vocabulary includes only high-frequency words • We propose a paraphrasing-based preprocessing method for Japanese-to-English NMT to improve translation accuracy with regard to OOV words. Our method can be combined with any NMT system. • We show that our method achieved a statistically significant BLEU (Kishore et al., 2002) score improvement of 0.58 and a METEOR (Lavie and Agarwal, 2007) score improvement of 0.52 over the previous method (Li et al., 2016) and reduced the OOV rate in output sentences by approximately 0.20. 64 Proceedings of the 4th Workshop on Asian Translation, pages 64–69, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work There have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sente"
W17-5703,P17-4012,0,0.0410569,"to build a 2gram language model trained with all sentences from ASPEC. We utilized the XXXL-size PPDB 2.0 (Pavlick et al., 2015) as the English paraphrase dictionary and PPDB:Japanese (Mizukami et al., 2014) as the Japanese paraphrase dictionary. Neither of these dictionaries contains the ASPEC corpus. We paraphrased either the target side of the training corpus only or both the source and target sides of the training corpus to conduct a fair comparison. We experimented with λ = 0.0, 0.25, 0.50, 0.75, and 1.0. We used OpenNMT-py5 as the NMT system, which is a Python implementation of OpenNMT (Klein et al., 2017). We built a model with settings as described below. We used bi-recurrentneural-network, batch size 64, epoch 20, embedding size 500, vocabulary size of source and target 30,000, dropout rate 0.3, optimizer SGD with learning rate 1.0, and number of RNN layers 2 with an RNN size of 500. Our baseline was trained with these settings without any paraphrasing. We re-implemented previous methods described in this paper (Luong et al., 2015; Li et al., 2016; Sennrich et al., 2016) using the underlying NMT with the abovementioned settings. We Experiment 4.1 Settings In this study, we used the Japanese–"
W17-5703,W07-0734,0,0.371531,"t because it addresses the output generation task by solving a classification problem in vocabulary dimension. Typically, NMT has to restrict the size of the vocabulary to reduce the computational cost. Therefore, the target language vocabulary includes only high-frequency words • We propose a paraphrasing-based preprocessing method for Japanese-to-English NMT to improve translation accuracy with regard to OOV words. Our method can be combined with any NMT system. • We show that our method achieved a statistically significant BLEU (Kishore et al., 2002) score improvement of 0.58 and a METEOR (Lavie and Agarwal, 2007) score improvement of 0.52 over the previous method (Li et al., 2016) and reduced the OOV rate in output sentences by approximately 0.20. 64 Proceedings of the 4th Workshop on Asian Translation, pages 64–69, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work There have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sentence using a translation dictionary. This method needs to align tr"
W17-5703,P15-1002,0,0.114721,"n be combined with any NMT system. • We show that our method achieved a statistically significant BLEU (Kishore et al., 2002) score improvement of 0.58 and a METEOR (Lavie and Agarwal, 2007) score improvement of 0.52 over the previous method (Li et al., 2016) and reduced the OOV rate in output sentences by approximately 0.20. 64 Proceedings of the 4th Workshop on Asian Translation, pages 64–69, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work There have been studies on improving translation accuracy by reducing the OOV rate using pre- and post-processing for machine translation. Luong et al. (2015) proposed a post-processing method that translates OOV words with a corresponding word in the source sentence using a translation dictionary. This method needs to align training sentence pairs before training to learn correspondences between OOV words and their translations. In the method described in this paper, we need no word alignment, and we retain the meaning of the original word by paraphrasing the target side of the training corpus. Jean et al. (2015) proposed another post-processing method that translates each OOV word with the word that has the largest attention weight in the source"
W17-5703,P15-2070,0,0.0413359,"Missing"
W17-5703,W16-3411,0,0.023253,"source and target corpora to split OOV words into units of frequent substrings to reduce the OOV rate. Their method splits words greedily without considering their meaning. Since we use lexical paraphrasing in the training data, we hope to reduce the OOV rate in the translation output while retaining the meaning. Additionally, since ours is a preprocessing method, it can be combined with a postprocessing method. On the other hand, there are methods similar to ours that paraphrase corpora as a preprocessing step of machine translation to reduce the complexity of source and/or target sentences. Sanja and Maja (2016) paraphrased source sentence vocabulary with a simple grammar as a preprocessing step for machine translation. We attempt to improve translation quality by reducing the OOV rate in the target language using paraphrasing without simplifying the source input sentences. Li et al. (2016) substituted OOV words in training corpora with a similar in-vocabulary word as pre- and post-processing steps. They replaced OOV words with frequent words using cosine similarity and a language model. They obtained word alignment between an OOV word and its counterpart in training corpora. In addition, they delete"
W17-5703,P16-1162,0,0.275476,"re training to learn correspondences between OOV words and their translations. In the method described in this paper, we need no word alignment, and we retain the meaning of the original word by paraphrasing the target side of the training corpus. Jean et al. (2015) proposed another post-processing method that translates each OOV word with the word that has the largest attention weight in the source sentence using a translation dictionary. Their method does not need word alignment, but it still does not necessarily consider the meaning in the target language, unlike our paraphrasing approach. Sennrich et al. (2016) applied byte pair encoding (BPE) to source and target corpora to split OOV words into units of frequent substrings to reduce the OOV rate. Their method splits words greedily without considering their meaning. Since we use lexical paraphrasing in the training data, we hope to reduce the OOV rate in the translation output while retaining the meaning. Additionally, since ours is a preprocessing method, it can be combined with a postprocessing method. On the other hand, there are methods similar to ours that paraphrase corpora as a preprocessing step of machine translation to reduce the complexit"
W17-5704,P10-2041,0,0.0767819,"ated from a monolingual in-domain corpus. Hsieh et al. (2013) create a pseudo-parallel corpus from patterns learned from source and monolingual target in-domain corpora for cross-domain adaptation. They manually conducted filtration of “relatively more accurate” translated sentences and used them to revise the language model. Similarly, we use a pseudoparallel corpus created by translating a monolingual corpus from the target language rather than the source language; however we apply automatic filtering to the obtained pseudo-parallel corpus. Data filtering is often used in domain adaptation (Moore and Lewis, 2010; Axelrod et al., 2011) and phrase-based SMT systems. Sentences are extracted from large corpora to optimize the language model and the translation model (Wang et al., 2014; Yıldız et al., 2014). The work most closely related to our work is Yıldız et al. (2014), who build a quality estimator to obtain high-quality parallel sentence pairs and achieve better translation performance and reduce timecomplexity with a small high-quality corpus. This method filters data by calculating similarity between source and target sentences. In our work, we calculate similarity between monolingual and syntheti"
W17-5704,P13-4016,0,0.0280735,"h concatenation, maximum batch size was 32, and the optimization method was Adadelta. For the German→English experiments, OpenNMT default settings were used. The vocabulary size in all experiments was 50,000. We tokenized and truecased French, English, German, and Russian sentences using Moses’ scripts. For Japanese sentences, we used MeCab 0.996 with the IPAdic dictionary3 for word segmentation. We eliminated duplicated sentences and sentences with more than 50 words for all languages. We report BLEU scores (Papineni et al., 2002) to compare translation results. We used the Travatar toolkit (Neubig, 2013) to calculate the significance of differences between systems using bootstrap resampling (p &lt; 0.05). 3.2 Bootstrapping Bootstrapping involves the following steps: 1. “Bootstrap 1”: we use a pseudo-parallel corpus created using the “Parallel” model as additional data to train the seed NMT systems. 2. “Bootstrap 2”: we select the best model on the development set from “Bootstrap 1” and train its target→source model. Here, we use target sentences from the pseudo-parallel cor2 3 72 http://opennmt.net/OpenNMT/ http://taku910.github.io/mecab Threshold Parallel Ja-Ru Parallel Ru-Ja Unfiltered sent-LM"
W17-5704,P02-1040,0,0.103257,"rameters: the number of recurrent layers of the encoder and decoder was 1, BiLSTM with concatenation, maximum batch size was 32, and the optimization method was Adadelta. For the German→English experiments, OpenNMT default settings were used. The vocabulary size in all experiments was 50,000. We tokenized and truecased French, English, German, and Russian sentences using Moses’ scripts. For Japanese sentences, we used MeCab 0.996 with the IPAdic dictionary3 for word segmentation. We eliminated duplicated sentences and sentences with more than 50 words for all languages. We report BLEU scores (Papineni et al., 2002) to compare translation results. We used the Travatar toolkit (Neubig, 2013) to calculate the significance of differences between systems using bootstrap resampling (p &lt; 0.05). 3.2 Bootstrapping Bootstrapping involves the following steps: 1. “Bootstrap 1”: we use a pseudo-parallel corpus created using the “Parallel” model as additional data to train the seed NMT systems. 2. “Bootstrap 2”: we select the best model on the development set from “Bootstrap 1” and train its target→source model. Here, we use target sentences from the pseudo-parallel cor2 3 72 http://opennmt.net/OpenNMT/ http://taku91"
W17-5704,P16-1009,0,0.535107,"ed on back-translation. As a result of experiments with three language pairs using small, medium, and large parallel corpora, language pairs with fewer training data filtered out more sentence pairs and improved BLEU scores more significantly. 1 Figure 1: Creating and filtering a pseudo-parallel corpus using back-translation. Various approaches have been proposed to create a pseudo-parallel corpus from a monolingual corpus. For example, Zhang et al. (2016) proposed a method to generate a pseudo-parallel corpus based on a monolingual corpus of the source language and its automatic translation. Sennrich et al. (2016) obtained substantial improvements by automatically translating a monolingual corpus of the target language into the source language, which they refer to as synthetic, and treating the obtained pseudo-parallel corpus as additional training data. They used monolingual data of the target language to learn the language model more effectively. However, they experimented on language pairs where relatively large-scale parallel corpora are available. Thus, they did not need to fully exploit the training corpus nor care about the quality of the pseudo-parallel corpus. Introduction A large-scale parall"
W17-5704,D11-1033,0,0.0650747,"in-domain corpus. Hsieh et al. (2013) create a pseudo-parallel corpus from patterns learned from source and monolingual target in-domain corpora for cross-domain adaptation. They manually conducted filtration of “relatively more accurate” translated sentences and used them to revise the language model. Similarly, we use a pseudoparallel corpus created by translating a monolingual corpus from the target language rather than the source language; however we apply automatic filtering to the obtained pseudo-parallel corpus. Data filtering is often used in domain adaptation (Moore and Lewis, 2010; Axelrod et al., 2011) and phrase-based SMT systems. Sentences are extracted from large corpora to optimize the language model and the translation model (Wang et al., 2014; Yıldız et al., 2014). The work most closely related to our work is Yıldız et al. (2014), who build a quality estimator to obtain high-quality parallel sentence pairs and achieve better translation performance and reduce timecomplexity with a small high-quality corpus. This method filters data by calculating similarity between source and target sentences. In our work, we calculate similarity between monolingual and synthetic target sentences. Rec"
W17-5704,N15-1138,0,0.0253738,"Russian. As a result, “Bootstrap 3” used an incorrect translation of the original sentence. The experimental results show that bootstrapping over several iterations improves the NMT without significant difference and eventually stops improving over the previous step. We hypothesize that the reason for this is that the “Parallel” system used to create a new pseudo-parallel corpus becomes weaker in each iteration. We used sent-BLEU to calculate the similarity of the synthetic and monolingual target sentences. However, word embedding-based sentence similarity measures, such as those employed by Song and Roth (2015), can be used to further improve the corpus filtering because sentence-level BLEU is sensitive to surface mismatch. 7 Conclusion The models trained using the filtered pseudoparallel corpus as additional data showed better translation performance than the baselines for lowresource language pairs. We have also shown that we can further improve translation performance by bootstrapping, although bootstrapping has its limitations. These results suggest that translation ac77 curacy depends on both data size and quality. Further experimental investigations are required to estimate the limitations of"
W17-5704,W09-0432,0,0.0348284,"ormative feedback signals to train the translation models. While the dual learning approach is shown to alleviate the issue of noisy data by increasing coverage, we are attempting to remove the noisy data. In addition, they assume a high-recourse language pair to cold start the reinforcement learning process, while we target low-resource language pairs wherein highquality seed NMT models are difficult to obtain. Related Work To address the data sparsity problem, there are many methods that use source language monolingual data to improve translation quality (Ueffing et al., 2007; Shwenk, 2008; Bertoldi and Federico, 2009; Hsieh et al., 2013; Zhang et al., 2016). Specifically, Bertoldi and Federico 1 https://github.com/aizhanti/filteredpseudo-parallel-corpora 71 3 Corpus Parallel Dev Test Mono target Improving Low-resource Neural Machine Translation (NMT) with Filtered Pseudo-parallel Corpus In this paper, we propose a method of filtering a pseudo-parallel corpus used as additional training data by back-translating a monolingual corpus for low-resource language pairs. Then, we attempt to bootstrap an NMT model by iterating the filtering process until convergence. Ru↔Ja 10,231 500 500 75k↔167k Fr→Mg 106,406 1,0"
W17-5704,D17-1147,0,0.0595202,"Missing"
W17-5704,W13-2817,0,0.147764,"tional training data. The remainder of this paper is organized as follows: Section 2 discusses previous studies related to improving low-resource machine translation systems; Section 3 outlines the proposed method for filtering a pseudo-parallel corpus and bootstrapping NMT; Sections 4 and 5 evaluate the proposed model; and Section 6 discusses the results. Conclusions and suggestions for future work are presented in Section 7. 2 (2009) addressed the problem of domain adaptation by training a translation model from a generated pseudo-parallel corpus created from a monolingual in-domain corpus. Hsieh et al. (2013) create a pseudo-parallel corpus from patterns learned from source and monolingual target in-domain corpora for cross-domain adaptation. They manually conducted filtration of “relatively more accurate” translated sentences and used them to revise the language model. Similarly, we use a pseudoparallel corpus created by translating a monolingual corpus from the target language rather than the source language; however we apply automatic filtering to the obtained pseudo-parallel corpus. Data filtering is often used in domain adaptation (Moore and Lewis, 2010; Axelrod et al., 2011) and phrase-based"
W17-5704,P17-4012,0,0.0404969,"Sort the monolingual target sentences and the corresponding synthetic source sentences by a descending order of sentence-level similarity metric scores and filter out sentences with low scores. The threshold is determined by the translation quality on the development set. 5. Use the filtered synthetic source sentences as the source side and the monolingual target sentences as the target side of the pseudoparallel corpus; this is referred to as a Filtered pseudo-parallel corpus as additional data. 4 Experiments Using a Filtered Pseudo-parallel Corpus 4.1 Settings We used the OpenNMT toolkit2 (Klein et al., 2017) to train all translation models. For the Russian↔Japanese and French→Malagasy experiments, we used the following parameters: the number of recurrent layers of the encoder and decoder was 1, BiLSTM with concatenation, maximum batch size was 32, and the optimization method was Adadelta. For the German→English experiments, OpenNMT default settings were used. The vocabulary size in all experiments was 50,000. We tokenized and truecased French, English, German, and Russian sentences using Moses’ scripts. For Japanese sentences, we used MeCab 0.996 with the IPAdic dictionary3 for word segmentation."
W17-5704,D16-1160,0,0.04829,"Missing"
W17-5704,P04-1077,0,0.0617487,", November 27, 2017. 2017 AFNLP parallel sentences will degrade NMT performance more than SMT. Our motivation is to filter out lowquality synthetic sentences that might be included in such a pseudo-parallel corpus to obtain a highquality pseudo-parallel corpus for low-resource language pairs. To the best of our knowledge, this is the first attempt to (1) filter a pseudo-parallel corpus using back-translation and (2) bootstrap NMT. The main contributions of our research are as follows: • We filter a pseudo-parallel corpus using sentence-level similarity metric, in our case sentence-level BLEU (Lin and Och, 2004a,b), and obtain a trainable high-quality pseudo-parallel corpus. • We show that the proposed filtering method is useful for low-resource language pairs, although bootstrapping does not outperform the proposed filtering method significantly. • We will release the obtained filtered pseudoparallel corpora1 . In this study, we used Japanese↔Russian as low-resource language pairs, French→Malagasy as medium-resource language pairs and German→English as high-resource language pairs. We show that a previous state-of-the-art method (Sennrich et al., 2016) is effective for high-resource language pairs;"
W17-5704,L16-1561,0,0.0191049,"glish model to filter this pseudo-parallel corpus. We used newtest2013 (3,000 sentence pairs) as a development set and newtest2014 (3,003 sentence pairs) as a test set. Table 1 shows the data statistics. ments, we compared the sent-BLEU scores, which require back-translation of the target monolingual data for the proposed filtration method, with a language model (sent-LM) that performs filtration by scoring only synthetic source sentences. We used the KenLM Language Model Toolkit9 to build a 5gram language model from 23,239,280 sentences from the Russian side of the Russian-English UN corpus (Ziemski et al., 2016).10 . We also applied Kneser-Ney smoothing. To extract the scores, we normalized the language model log probability of the sentence to be between [0, 1] as in sent-BLEU using a feature scaling method. Translation performance increases as the number of parallel sentences increases (Koehn, 2002). For a pseudo-parallel corpus, however, translation performance does not necessarily increase with the number of sentences. To determine the effects of the quantity and quality of the pseudoparallel corpus in machine translation, we set thresholds with increment steps of 0.1. Thus, pseudo-parallel senten"
W17-5704,C04-1072,0,0.0526052,", November 27, 2017. 2017 AFNLP parallel sentences will degrade NMT performance more than SMT. Our motivation is to filter out lowquality synthetic sentences that might be included in such a pseudo-parallel corpus to obtain a highquality pseudo-parallel corpus for low-resource language pairs. To the best of our knowledge, this is the first attempt to (1) filter a pseudo-parallel corpus using back-translation and (2) bootstrap NMT. The main contributions of our research are as follows: • We filter a pseudo-parallel corpus using sentence-level similarity metric, in our case sentence-level BLEU (Lin and Och, 2004a,b), and obtain a trainable high-quality pseudo-parallel corpus. • We show that the proposed filtering method is useful for low-resource language pairs, although bootstrapping does not outperform the proposed filtering method significantly. • We will release the obtained filtered pseudoparallel corpora1 . In this study, we used Japanese↔Russian as low-resource language pairs, French→Malagasy as medium-resource language pairs and German→English as high-resource language pairs. We show that a previous state-of-the-art method (Sennrich et al., 2016) is effective for high-resource language pairs;"
W17-5716,D10-1092,0,0.0535477,"ults show that beam search and ensemble decoding improve the translation accuracy by 3.55 points in Japanese-English translation and 3.28 points in English-Japanese translation in BLEU scores. As regards Japanese-English translation, our NMT system improved the translation accuracy by 6.10 points compared with our previous NMT system. From a BLEU score standpoint, with increasing beam size, the translation accuracy is enhanced. However, it does not always improve translation accuracy in other metrics. Results Tables 2 and 3 show the translation accuracy in BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), AMFM (Banchs and Li, 2011) and HUMAN evaluation scores. In the “Model” column, “beam n” indicates the model with the beam size of n, “n ensemble” indicates the model ensembled by n trained models on testing. “Previous system” in Table 2 indicates our previous NMT system for WAT 2016 (Yamagishi et al., 2016). This system is based on the attention-based NMT (Bahdanau et al., 2015) and did not implement dropout, beam search, and ensemble decoding. Table 4 shows examples of outputs of JapaneseEnglish translations. In Example 1, the output is significantly poor when the beam size is 1. However, b"
W17-5716,D15-1166,0,0.254262,"ranslation (WAT 2017) (Nakazawa et al., 2017). We implemented beam search and ensemble decoding in our NMT system. We applied our NMT system to Japanese-English, EnglishJapanese, and Japanese-Chinese scientific paper translation subtasks. The experimental results show that beam search and ensemble decoding improve the translation accuracy by 3.55 points in Japanese-English translation and 3.28 points in English-Japanese translation in terms of BLEU (Papineni et al., 2002) scores. In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT (Luong et al., 2015) and uses long shortterm memories (LSTM) as RNN. We implemented beam search and ensemble decoding in the NMT system. The system was tested on the 4th Workshop on Asian Translation (WAT 2017) (Nakazawa et al., 2017) shared tasks. In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks. The experimental results showed that implementation of beam search and ensemble decoding can effectively improve the translation quality. 1 2 Neural Machine Translation Introduction Herein, we describe the archit"
W17-5716,P02-1040,0,0.101063,"s the variance of output during decoding. In this paper, we describe the NMT system that was tested on the shared tasks at 4th Workshop on Asian Translation (WAT 2017) (Nakazawa et al., 2017). We implemented beam search and ensemble decoding in our NMT system. We applied our NMT system to Japanese-English, EnglishJapanese, and Japanese-Chinese scientific paper translation subtasks. The experimental results show that beam search and ensemble decoding improve the translation accuracy by 3.55 points in Japanese-English translation and 3.28 points in English-Japanese translation in terms of BLEU (Papineni et al., 2002) scores. In this paper, we describe our neural machine translation (NMT) system, which is based on the attention-based NMT (Luong et al., 2015) and uses long shortterm memories (LSTM) as RNN. We implemented beam search and ensemble decoding in the NMT system. The system was tested on the 4th Workshop on Asian Translation (WAT 2017) (Nakazawa et al., 2017) shared tasks. In our experiments, we participated in the scientific paper subtasks and attempted Japanese-English, English-Japanese, and Japanese-Chinese translation tasks. The experimental results showed that implementation of beam search an"
W17-5716,P11-2027,0,0.0147678,"d ensemble decoding improve the translation accuracy by 3.55 points in Japanese-English translation and 3.28 points in English-Japanese translation in BLEU scores. As regards Japanese-English translation, our NMT system improved the translation accuracy by 6.10 points compared with our previous NMT system. From a BLEU score standpoint, with increasing beam size, the translation accuracy is enhanced. However, it does not always improve translation accuracy in other metrics. Results Tables 2 and 3 show the translation accuracy in BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), AMFM (Banchs and Li, 2011) and HUMAN evaluation scores. In the “Model” column, “beam n” indicates the model with the beam size of n, “n ensemble” indicates the model ensembled by n trained models on testing. “Previous system” in Table 2 indicates our previous NMT system for WAT 2016 (Yamagishi et al., 2016). This system is based on the attention-based NMT (Bahdanau et al., 2015) and did not implement dropout, beam search, and ensemble decoding. Table 4 shows examples of outputs of JapaneseEnglish translations. In Example 1, the output is significantly poor when the beam size is 1. However, by increasing the beam size,"
W17-5716,P16-1162,0,0.108945,"Missing"
W17-5716,D14-1179,0,0.0439736,"Missing"
W17-5716,W16-4620,1,0.869779,"atch size: 128 In ensemble decoding, the conditional probability of the output word yˆj is the average of each model’s score. It is computed by • Optimizer: Adagrad • Initial learning rate: 0.01 M 1 ∑ (m) p (yˆj |Y<j , X) M m=1 (12) where M is the number of models. Adopting this approach reduces the risk of predicting a wrong word at each time step. • Dropout rate: {0.1, 0.2, 0.3, 0.4, 0.5} p(yˆj |Y<j , X) = • Beam size: {1, 2, 5, 10, 20} 1 https://github.com/taku910/mecab http://www.statmt.org/moses/ 3 https://github.com/google/sentencepiece 2 162 Japanese-English Model BLEU Previous system (Yamagishi et al., 2016) 18.45 beam 1 21.00 beam 2 22.21 beam 5 22.85 beam 10 22.99 beam 20 23.03 5 ensemble + beam 1 22.78 5 ensemble + beam 2 24.02 5 ensemble + beam 5 24.46 5 ensemble + beam 10 24.55 RIBES 0.711542 0.725284 0.733571 0.737631 0.739629 0.741175 0.738325 0.743581 0.744955 0.744928 AMFM 0.546880 0.585710 0.591740 0.595180 0.595030 0.595260 0.587630 0.596840 0.597760 0.596360 HUMAN +56.750 +61.000 - Table 2: Japanese-English translation results. Model beam 1 beam 2 beam 5 beam 10 beam 20 5 ensemble + beam 1 5 ensemble + beam 2 5 ensemble + beam 5 5 ensemble + beam 10 English-Japanese BLEU RIBES 33.72 0"
W17-5716,N18-1122,0,0.0757794,"Missing"
W17-5911,P12-3027,0,0.012713,"ty. 4 Related Work In recent years, many writing assistance systems have been developed. One of them is ESCORT, which is an English search system (Matsubara et al., 2008) for writing scholarly papers and survey reports; it aims to demonstrate examples of word usage. The input to this system is a sentence that will be parsed, and then the system will output sentences with the same syntactic structure. However, it assumes that there is a syntactic structure between keywords, which is not a valid assumption in our task. Further, latent intent of query is not modeled in their system. In contrast, Chen et al. (2012) propose an English writing assistance system for ESL learners. The system, called FLOW, supplements the English vocabulary of non-English native speakers. If ESL learners cannot write in English owing to a lack of vocabulary, they can continue to write words in their first language within the sentence. Figure 2: p@k of RBF kernel. 3.5 Discussion We show the error analysis on the 19-gram RBF kernel, which exhibited the best score in the experimental results. Table 2 presents examples of one of the top-ten results of sentence retrieval. The topmost results of the proposed method comprise senten"
W17-5911,W12-4805,0,0.0563459,"Missing"
W17-5911,N15-1138,0,0.0307852,"k( qi , sj ) |q||s| 1 (3) https://code.google.com/archive/p/word2vec/ http://nlp.stanford.edu/software/stanford-corenlp-full2015-12-09.zip 2 i=1 j=1 65 aged the word vectors of the words in the sentence. We compared these vectors using cosine similarity and RBF kernel. Table 1: Example of pairs of query. education innovative, identify research, provide advice, plan annual, recipient award, goal ensure, partnership support, field industry, improve success, lead experience Alignment-based similarity. As another baseline, we used one of the unsupervised sentence similarity measures proposed by Song and Roth (2015). These methods achieved state-of-theart performance for a short text similarity (STS) task. We used their method to calculate the intersentence similarity (maximum alignment) based on the alignment in the distributed representation expressed by the following equation. Preliminary experiments were performed using the hyperparameter γ of the RBF kernel within the range of γ ∈ {10−1 , 100 , 101 , 102 }. We set the hyperparameter γ as γ = 101 based on the results. 3.2 Data |q| 1 ∑ simmax (q, s) = max k(qi , sj ) j |q| In this study, we experimented on a domain of academic press release articles."
W18-0521,S16-1151,0,0.0638048,"Missing"
W18-0521,S16-1157,0,0.132156,"Missing"
W18-0521,S16-1162,0,0.0995403,"Missing"
W18-0521,P13-3015,0,0.146511,"s. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Substitution ranking • Spanish monolingual CWI In this study, we work on complex word identification (CWI) (Shardlow, 2013), a subtask of lexical simplification. Previous studies (Specia et al., 2012; Paetzold and Specia, 2016a) concluded that the most effective way to estimate word difficulty is to count the word frequency in a corpus. However, they counted the word frequency in corpora written by native speakers, such as Wikipedia. Language learners tend to use simple words as compared to native speakers. Therefore, we expect the word frequency in the learner corpus to be a useful feature for CWI. • German monolingual CWI • Multilingual CWI with a French test set The English dataset contained a mixture of profes"
W18-0521,S16-1154,0,0.0697223,"Missing"
W18-0521,S12-1046,0,0.190121,"classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Substitution ranking • Spanish monolingual CWI In this study, we work on complex word identification (CWI) (Shardlow, 2013), a subtask of lexical simplification. Previous studies (Specia et al., 2012; Paetzold and Specia, 2016a) concluded that the most effective way to estimate word difficulty is to count the word frequency in a corpus. However, they counted the word frequency in corpora written by native speakers, such as Wikipedia. Language learners tend to use simple words as compared to native speakers. Therefore, we expect the word frequency in the learner corpus to be a useful feature for CWI. • German monolingual CWI • Multilingual CWI with a French test set The English dataset contained a mixture of professionally written news, non-professionally written news (WikiNews), and Wikip"
W18-0521,S16-1153,0,0.194208,"Missing"
W18-0521,S16-1146,0,0.0382937,"Missing"
W18-0521,I11-1017,1,0.85983,"ses tend to be less frequent, we used the number of words as the second feature. Others features (3-8) are based on the frequency of targets in a corpus. We counted frequencies from texts written by native speakers and language learners. Language learners are more likely to use simple words than native speakers. Therefore, we expected word frequency in the learner corpus to be a useful feature for CWI. As a text written by native speakers, we counted the frequency from Wikipedia and WikiNews. By contrast, as a text written by language learners, we counted the frequency from the Lang-8 corpus (Mizumoto et al., 2011). The Lang-8 corpus contains texts before and after corrections written by learners and native speakers, respectively. We use the former. 2.2 Probabilistic Classification Task Labels in the probabilistic classification task were assigned as the proportion of annotators identifying the target as complex. Systems were evaluated using the MAE (mean absolute error). TMU Systems According to previous studies (Specia et al., 2012; Paetzold and Specia, 2016a), we estimated the word difficulty by counting word frequency. 3.1 Classifiers We used random forest classifiers and random forest regressors fo"
W18-0521,W18-0507,0,0.245705,". 1 Mamoru Komachi‡ ‡ Graduate School of Systems Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Sub"
W18-0521,S16-1152,0,0.3754,"Missing"
W18-0521,I17-2068,0,0.120278,"Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for text simplification (Shardlow, 2014), which facilitates children and language learners ’ reading comprehension. Lexical simplification comprises the following steps: 1. Complex word identification 2. Substitution generation 3. Substitution selection • English monolingual CWI 4. Substitution ranking • Spanish monolingual CWI In t"
W18-0521,yimam-etal-2017-multilingual,0,0.0757314,"Missing"
W18-0521,S16-1149,0,0.453088,"task 2018. TMU systems use random forest classifiers and regressors whose features are the number of characters and words and the frequency of target words in various corpora. Our simple systems performed best on 5 of the 12 tracks. Ablation analysis confirmed the usefulness of a learner corpus for a CWI task. 1 Mamoru Komachi‡ ‡ Graduate School of Systems Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one"
W18-0521,W17-5910,0,0.463178,"random forest classifiers and regressors whose features are the number of characters and words and the frequency of target words in various corpora. Our simple systems performed best on 5 of the 12 tracks. Ablation analysis confirmed the usefulness of a learner corpus for a CWI task. 1 Mamoru Komachi‡ ‡ Graduate School of Systems Design Tokyo Metropolitan University Tokyo, Japan komachi@tmu.ac.jp 2 CWI Shared Task 2018 In CWI shared tasks, systems predict whether words in a given context are complex or noncomplex for a non-native speaker. The first CWI shared task (Paetzold and Specia, 2016a; Zampieri et al., 2017) contained only English data designed for non-native English speakers. Totally, 20 annotators were assigned to each instance in the training set. However, in the test set, only one annotator was assigned to each instance. By contrast, the CWI shared task 2018 (Yimam et al., 2018) used a multilingual dataset (Yimam et al., 2017a,b) having all instances annotated by multiple annotators. This shared task was divided into two tasks (binary and probabilistic classification) and the following four tracks: Introduction Lexical simplification (Paetzold and Specia, 2017) is one of the approaches for te"
W18-0521,S16-1158,0,0.135728,"Missing"
W18-0521,S16-1155,0,0.259473,"Missing"
W18-0521,S16-1161,0,0.118343,"Missing"
W18-0544,W18-0521,1,0.737654,"lexrich 0.843 zz 0.839 TMU 0.834 Cam 0.822 btomosch 0.815 LambdaLearning 0.813 Grotoco 0.811 nihalnayak 0.808 jilljenn 0.808 ymatusevich 0.806 caseykennington 0.795 renhk 0.770 SLAM baseline Table 4: SLAM official evaluation results. Systems are ranked by AUROC. Model W/ History Model W/O History Model AUROC 0.834 0.648 In this work, we have not used any languagespecific information. As future work, we plan to exploit additional data for each language, such as pre-trained word representations, ngrams, and character-based features. Additionally, we hope to incorporate word difficulty features (Kajiwara and Komachi, 2018). In particular, the more complex a word is, the more difficult it likely is to be learned. Table 5: The history model has an effect to improve AUROC on English subtask. does not consider history (W/O History Model) on the dev set for English. The W/O History Model used only the Prediction Bi-LSTM component which does not use the history feature. For experiments using this model, we used a single model trained only on the English corpus. The default split of training set and dev set was 824,012 exercises and 115,770 exercises, respectively. Both aforementioned models used the same parameters a"
W18-0544,W18-0506,0,0.0370715,"Missing"
W18-6303,I17-2064,0,0.0191181,"ruct ideographs; ideographs are used to construct characters, which are the basic units for meaningful words. Words can then further compose sentences. In alphabetic languages, sub-word units are easy to identify, whereas in logographic languages, a similar effect can be achieved only if sub-character level information is taken into consideration.1 Having noticed this significant difference between these two writing systems, Shi et al. (2015), Liu et al. (2017), Peng et al. (2017), and Cao et al. (2017) used stroke-level information for logographic languages when constructing word embeddings; Toyama et al. (2017) used visual information for strokes and Japanese Kanji Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units. However, important differences between languages with logographic and alphabetic writing systems have long been overlooked. This study focuses on these differences and uses a simple approach to improve the performance of NMT systems utilizing decomposed sub-character level information for logographic languages. Our results indicate that our approach not only improves the translation capabilitie"
W18-6303,W04-3250,0,0.100623,"scores) of NMT systems for Japanese/English and Chinese/English language pairs. All the scores are statistically significant at p = 0.0001 (marked by ∗). ing steps, we set the learning rate to be four times smaller until the end of training. Additionally, we set the drop-out rate to 0.2 during training. BLEU was used as the evaluation metric in our experiments. For Chinese and Japanese data, a KyTea tokenization was applied before we applied BLEU, following the WAT (Workshop on Asian Translation) leaderboard standard. To validate the significance of our results, we ran bootstrap re-sampling (Koehn, 2004) for all results using Travatar (Neubig, 2013) at a significance level of p = 0.0001. 4.3 NMT of Logographic Language Pairs Results 4.3.1 NMT of Logographic and Alphabetic Language Pairs Table 4 shows the experimental results for the Japanese/English and Chinese/English language pairs in both translation directions. Generally, for each of the experiment settings, the models using ideograph and stroke data outperformed the baseline systems, regardless of the language pair or translation direction. However, for the Japanese/English language pair, the stroke sequence models performed better. For"
W18-6303,1983.tc-1.13,0,0.437445,"Missing"
W18-6303,P18-1007,0,0.0113902,"s, it is possible to represent a large number of words with a small vocabulary. Originally, sub-word units were only applied to unknown words (Sennrich et al., 2016). However, in the recent GNMT (Wu et al., 2016) and transformer systems (Vaswani et al., 2017), all words are broken up into subword units to better represent the shared information. For alphabetic languages, researchers have indicated that sub-word units are useful for solving OOV problems, and that shared information can further improve translation quality. The Sentencepiece project5 compared several combinations of word-pieces (Kudo, 2018) and BPE sub-word 1. We create a sub-character database of Chinese character-based languages, and conduct MT experiments using various types of subcharacter NMT models. 2 To be more precise, there is another so-called syllabic writing system, which uses individual symbols to represent symbols rather than phonemes. Japanese hiragana and katakana are actually syllabic symbols rather than ideographs. In this paper, we focus only on the logographic part. 3 An official Romanization system for standard Chinese in mainland China. Pinyin includes both letters and diacritics, which represent phonemic a"
W18-6303,P17-1188,0,0.0147161,"her major writing systems—namely, logographic (or character-based) languages such as Chinese, Japanese, and traditional Korean—strokes are used to construct ideographs; ideographs are used to construct characters, which are the basic units for meaningful words. Words can then further compose sentences. In alphabetic languages, sub-word units are easy to identify, whereas in logographic languages, a similar effect can be achieved only if sub-character level information is taken into consideration.1 Having noticed this significant difference between these two writing systems, Shi et al. (2015), Liu et al. (2017), Peng et al. (2017), and Cao et al. (2017) used stroke-level information for logographic languages when constructing word embeddings; Toyama et al. (2017) used visual information for strokes and Japanese Kanji Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units. However, important differences between languages with logographic and alphabetic writing systems have long been overlooked. This study focuses on these differences and uses a simple approach to improve the performance of NMT systems utilizing"
W18-6303,D15-1166,0,0.0672197,"posed sub-character level information for logographic languages. Our results indicate that our approach not only improves the translation capabilities of NMT systems between Chinese and English, but also further improves NMT systems between Chinese and Japanese, because it utilizes the shared information brought by similar sub-character units. 1 Introduction Neural machine translation (Cho et al., 2014) (NMT) systems based on sequence-to-sequence models (Sutskever et al., 2014) have recently become the de facto standard architecture. The models use attention mechanisms (Bahdanau et al., 2015; Luong et al., 2015) to keep records of all encoding results, and can focus on particular parts of these results during decoding, so that the model can produce longer and more accurate translations. Sub-word units are another technique first introduced by Sennrich’s (2016) application of the byte pair encoding (BPE) algorithm, and are used to break up words in both source and target sentences into sequences of smaller units, learned without supervision. This alleviates the risk of producing <unk> symbols when the model encounters infrequent “unknown” words, also known as the out-of-vocabulary (OOV) problem. Moreo"
W18-6303,P13-4016,0,0.01471,"and Chinese/English language pairs. All the scores are statistically significant at p = 0.0001 (marked by ∗). ing steps, we set the learning rate to be four times smaller until the end of training. Additionally, we set the drop-out rate to 0.2 during training. BLEU was used as the evaluation metric in our experiments. For Chinese and Japanese data, a KyTea tokenization was applied before we applied BLEU, following the WAT (Workshop on Asian Translation) leaderboard standard. To validate the significance of our results, we ran bootstrap re-sampling (Koehn, 2004) for all results using Travatar (Neubig, 2013) at a significance level of p = 0.0001. 4.3 NMT of Logographic Language Pairs Results 4.3.1 NMT of Logographic and Alphabetic Language Pairs Table 4 shows the experimental results for the Japanese/English and Chinese/English language pairs in both translation directions. Generally, for each of the experiment settings, the models using ideograph and stroke data outperformed the baseline systems, regardless of the language pair or translation direction. However, for the Japanese/English language pair, the stroke sequence models performed better. For the Chinese/English language pairs, the ideogr"
W18-6303,P16-1162,0,0.116865,"ny word that is not in the vocabulary, which will harm the translation quality. This is called the out-ofvocabulary (OOV) problem. Sub-word unit algorithms (such as BPE algorithms) first break up a sentence into the smallest possible units. Then, two adjacent units at a time are merged according to some standard (e.g., the co-occurrence frequency). Finally, after n steps, the algorithm collects the merged units as “subword” units. By using sub-word units, it is possible to represent a large number of words with a small vocabulary. Originally, sub-word units were only applied to unknown words (Sennrich et al., 2016). However, in the recent GNMT (Wu et al., 2016) and transformer systems (Vaswani et al., 2017), all words are broken up into subword units to better represent the shared information. For alphabetic languages, researchers have indicated that sub-word units are useful for solving OOV problems, and that shared information can further improve translation quality. The Sentencepiece project5 compared several combinations of word-pieces (Kudo, 2018) and BPE sub-word 1. We create a sub-character database of Chinese character-based languages, and conduct MT experiments using various types of subcharact"
W18-6303,P15-2098,0,0.02801,"rds. However, in other major writing systems—namely, logographic (or character-based) languages such as Chinese, Japanese, and traditional Korean—strokes are used to construct ideographs; ideographs are used to construct characters, which are the basic units for meaningful words. Words can then further compose sentences. In alphabetic languages, sub-word units are easy to identify, whereas in logographic languages, a similar effect can be achieved only if sub-character level information is taken into consideration.1 Having noticed this significant difference between these two writing systems, Shi et al. (2015), Liu et al. (2017), Peng et al. (2017), and Cao et al. (2017) used stroke-level information for logographic languages when constructing word embeddings; Toyama et al. (2017) used visual information for strokes and Japanese Kanji Recent neural machine translation (NMT) systems have been greatly improved by encoder-decoder models with attention mechanisms and sub-word units. However, important differences between languages with logographic and alphabetic writing systems have long been overlooked. This study focuses on these differences and uses a simple approach to improve the performance of NM"
W18-6456,W17-4767,0,0.100146,"lation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex features. https://github.com/Shi-ma/RUSE 751 Proce"
W18-6456,W17-4755,0,0.270568,"as well as three other metrics, namely DPMF (Yu et al., 2015b), REDp (Yu et al., 2015a), and ENTFp (Yu et al., 2015a), using ranking SVM to train parameters of each metric score. DPMF evaluates the syntactic similarity between an MT hypothesis and a reference translation. REDp evaluates an MT hypothesis based on the dependency tree of the reference translation that comprises both lexical and syntactic information. ENTFp (Yu et al., 2015a) evaluates the fluency of an MT hypothesis. After the success of DPMFcomb , Blend3 (Ma et al., 2017) achieved the best performance in the WMT17 metrics task (Bojar et al., 2017). Similar to DPMFcomb , Blend is essentially an SVR model with RBF kernel that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely BEER (Stanojevi´c and Sima’an, 2015), CharacTER (Wang et al., 2016), DPMF, and ENTFp. BEER is a linear model based on character Ngrams and replacement trees. CharacTER evaluates an MT hypothesis based on character-level edit distance. DPMFcomb is trained through relative ranking (RR) of human evaluation data in terms of relative 2 3 4 http://asiya.lsi.u"
W18-6456,W16-2302,0,0.0681552,"Missing"
W18-6456,W17-4768,0,0.633165,"c for automatic machine translation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex features. https://github.com"
W18-6456,D15-1075,0,0.0998304,"tion 3.2. 3.1 Universal Sentence Embeddings Several approaches have been proposed to learn sentence embeddings. These sentence embeddings are learned through large-scale data such that they constitute potentially useful features for MTE. These have been proven effective in various NLP tasks, such as document classification and measurement of semantic textual similarity, and we call them universal sentence embeddings. First, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence embeddings trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, namely entailment, contradiction, and neutral; thus, InferSent can train sentence embeddings that are sensitive to differences in meaning. This model encodes a sentence pair u and v and generates features by sentence embeddings u and v with a bi-directional 6 7 8 https://github.com/lajanugen/S2V https://www.tensorflow.org/hub/modules/google/universalsentence-encoder-large/2 10 en: English, cs: Czech, de: German, fi: Finnish, ro: Romanian, ru: Russian, tr: Turkish, lv: Latvian, zh: Chinese 9 ht"
W18-6456,P02-1040,0,0.106657,"describes a segment-level metric for automatic machine translation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 datasets for to-English language pairs without using any complex fea"
W18-6456,D18-2029,0,0.0548051,"Missing"
W18-6456,W17-4770,0,0.071977,"Missing"
W18-6456,D17-1070,0,0.0340089,"ree types of sentence embeddings used in the proposed metric in Section 3.1. We then explain the proposed regression model and feature extraction for MTE in Section 3.2. 3.1 Universal Sentence Embeddings Several approaches have been proposed to learn sentence embeddings. These sentence embeddings are learned through large-scale data such that they constitute potentially useful features for MTE. These have been proven effective in various NLP tasks, such as document classification and measurement of semantic textual similarity, and we call them universal sentence embeddings. First, InferSent6 (Conneau et al., 2017) constructs a supervised model computing universal sentence embeddings trained using Stanford Natural Language Inference (SNLI) datasets7 (Bowman et al., 2015). The Natural Language Inference task is a classification task of sentence pairs with three labels, namely entailment, contradiction, and neutral; thus, InferSent can train sentence embeddings that are sensitive to differences in meaning. This model encodes a sentence pair u and v and generates features by sentence embeddings u and v with a bi-directional 6 7 8 https://github.com/lajanugen/S2V https://www.tensorflow.org/hub/modules/goo"
W18-6456,N18-4015,1,0.11916,"raining sentence embeddings using small-scale translation datasets with manual evaluation is difficult, sentence embeddings trained from large-scale data in other tasks can improve the automatic evaluation of machine translation. We use a multi-layer perceptron regressor based on three types of sentence embeddings. The experimental results of the WMT16 and WMT17 datasets show that the RUSE metric achieves a state-of-the-art performance in both segment- and system-level metrics tasks with embedding features only. 1 Figure 1: Outline of the RUSE metric. Introduction We extend our previous work (Shimanaka et al., 2018) and propose a segment-level MTE metric using universal sentence embeddings capable of capturing global information that cannot be captured by local features based on character or word N-grams. The experimental results in both segment- and system-level metrics tasks conducted using the datasets for to-English language pairs on WMT16 and WMT17 indicated that the proposed regression model using sentence embeddings, RUSE, achieves the best performance. The main contributions of the study are summarized below: This study describes a segment-level metric for automatic machine translation evaluation"
W18-6456,W15-3031,0,0.0584875,"Missing"
W18-6456,D15-1124,0,0.245194,"Missing"
W18-6456,W15-3050,0,0.175092,"Missing"
W18-6456,W15-3047,0,0.0401563,"Missing"
W18-6456,P15-1150,0,0.11408,"Missing"
W18-6456,S13-1005,0,0.0312385,"yers. Features. Publicly available pre-trained sentence embeddings, such as InferSent6 , QuickThought8 , and Universal Sentence Encoder9 , were used as the features mentioned in Section 3. InferSent is a collection of 4096-dimensional sentence embeddings trained on both 560,000 sentences of the SNLI dataset (Bowman et al., 2015) and 433,000 sentences of the MultiNLI dataset (Williams et al., 2018). Quick-Thought is a collection of 4800-dimensional sentence embeddings trained on both 45 million sentences of the BookCorpus dataset (Zhu et al., 2015) and 129 million sentences of the UMBC corpus (Han et al., 2013). Universal Sentence Encoder is a collection of 512-dimensional sentence embeddings trained on many sentences from a variety of web Sources, such as Wikipedia, web news, web question-answer pages, and discussion forums. • Number of layers ∈ {1, 2, 3} • Batch size ∈ {64, 128, 256, 512, 1024} • Dropout rate ∈ {0.1, 0.3, 0.5} • Optimizer ∈ {Adam} • C ∈ {0.1, 1.0, 10} • ϵ ∈ {0.01, 0.1, 1.0} • γ ∈ {0.001, 0.01, 0.1} Baseline Metrics. We compared the proposed metric with the four baseline metrics for each dataset. One is BLEU, which is the de facto standard metric for machine translation evaluation."
W18-6456,W16-2342,0,0.0779174,"the dependency tree of the reference translation that comprises both lexical and syntactic information. ENTFp (Yu et al., 2015a) evaluates the fluency of an MT hypothesis. After the success of DPMFcomb , Blend3 (Ma et al., 2017) achieved the best performance in the WMT17 metrics task (Bojar et al., 2017). Similar to DPMFcomb , Blend is essentially an SVR model with RBF kernel that uses the scores of various metrics as features. It incorporates 25 lexical metrics provided by the Asiya MT evaluation toolkit, as well as four other metrics, namely BEER (Stanojevi´c and Sima’an, 2015), CharacTER (Wang et al., 2016), DPMF, and ENTFp. BEER is a linear model based on character Ngrams and replacement trees. CharacTER evaluates an MT hypothesis based on character-level edit distance. DPMFcomb is trained through relative ranking (RR) of human evaluation data in terms of relative 2 3 4 http://asiya.lsi.upc.edu/ http://github.com/qingsongma/blend 5 752 https://github.com/rohitguptacs/ReVal http://clic.cimec.unitn.it/composes/sick.html WMT15 WMT16 WMT17 cs-en de-en fi-en lv-en ro-en ru-en tr-en zh-en 500 560 560 500 560 560 500 560 560 560 560 - 500 560 560 560 560 560 Table 1: Number of segment-level DA human e"
W18-6456,C04-1072,0,0.0850149,"tributions of the study are summarized below: This study describes a segment-level metric for automatic machine translation evaluation (MTE). The MTE metrics with a high correlation with human evaluation enable the continuous integration and deployment of a machine translation (MT) system. Various MTE metrics have been proposed in the metrics task of the Workshops on Statistical Machine Translation (WMT) that was started in 2008. However, most MTE metrics are obtained by computing the similarity between an MT hypothesis and a reference based on the character or word N-grams, such as SentBLEU (Lin and Och, 2004), which is a smoothed version of BLEU (Papineni et al., 2002), Blend (Ma et al., 2017), MEANT 2.0 (Lo, 2017), and chrF++ (Popovi´c, 2017). Therefore, they can exploit only limited information for the segment-level MTE. In other words, the MTE metrics based on character or word N-grams cannot make full use of sentence embeddings. They only check for word matches. 1 • We propose a novel supervised regression model for the segment-level MTE based on universal sentence embeddings. • We achieved a state-of-the-art performance in segment- and system-level metrics tasks on the WNT16 and WMT17 dataset"
W18-6456,N18-1101,0,0.0181493,"uation scores for to-English language pairs in WMT17. IS: InferSent; QT: Quick-Thought; and USE: Universal Sentence Encoder. man scores. rameters using the development data. We used ReLU as an activation function in all layers. Features. Publicly available pre-trained sentence embeddings, such as InferSent6 , QuickThought8 , and Universal Sentence Encoder9 , were used as the features mentioned in Section 3. InferSent is a collection of 4096-dimensional sentence embeddings trained on both 560,000 sentences of the SNLI dataset (Bowman et al., 2015) and 433,000 sentences of the MultiNLI dataset (Williams et al., 2018). Quick-Thought is a collection of 4800-dimensional sentence embeddings trained on both 45 million sentences of the BookCorpus dataset (Zhu et al., 2015) and 129 million sentences of the UMBC corpus (Han et al., 2013). Universal Sentence Encoder is a collection of 512-dimensional sentence embeddings trained on many sentences from a variety of web Sources, such as Wikipedia, web news, web question-answer pages, and discussion forums. • Number of layers ∈ {1, 2, 3} • Batch size ∈ {64, 128, 256, 512, 1024} • Dropout rate ∈ {0.1, 0.3, 0.5} • Optimizer ∈ {Adam} • C ∈ {0.1, 1.0, 10} • ϵ ∈ {0.01, 0.1"
W19-4413,W11-2123,0,0.030868,"were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013)8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT (Och, 2003) was used with the tuning data for Mˆ2 Scorer (Dahlmeier and Ng, 2012). Synthetic sentence pairs with a [3, 80] sentence length were used at the refinement step. The number of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the dev data and submitted the output of the best iteration model. We used pyspellchecker9 as a spell checker. This tool uses Levenshtein distance to obtain permutations within an edit dist"
W19-4413,W14-1703,0,0.0165634,". The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data. Ge et al. (2018), who presented state-of-the-art results in GEC, proposed a supervised NMT method trained on corpora of a total 5.4 M sentence pairs. We mainly use the monolingual corpus because the low resource track does not permit the use of the learner corpora. Despite the success of NMT, many studies on GEC traditionally use SMT (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). These studies apply an offthe-shelf SMT toolkit, Moses, to GEC. JunczysDowmunt and Grundkiewicz (2014) claimed that the SMT system optimized for BLEU learns to not change the source sentence. Instead of BLEU, they proposed tuning an SMT system using the M2 score with annotated development data. In this study, we also tune the weights with an F0.5 score measured by the M2 scorer because the official score is an F0.5 score. 6 Conclusion In this paper, we described our GEC system for the low resource track of the shared task at BEA2019. We introduced an unsupervised approach based on SMT for GE"
W19-4413,P18-1073,0,0.163266,"The experimental results show that our system achieved an F0.5 score of 28.31 points. We introduce unsupervised techniques based on phrase-based statistical machine translation for grammatical error correction (GEC) trained on a pseudo learner corpus created by Google Translation. We verified our GEC system through experiments on a low resource track of the shared task at Building Educational Applications 2019 (BEA2019). As a result, we achieved an F0.5 score of 28.31 points with the test data. 2 Unsupervised GEC Algorithm 1 shows the pseudocode for unsupervised GEC. This code is derived from Artetxe et al. (2018b). First, the cross-lingual phrase embeddings are acquired. Second, a phrase table is created based on these cross-lingual embeddings. Third, the phrase table is combined with a language model trained by monolingual data to initialize a phrase-based SMT system. Finally, the SMT system is updated through iterative forwardtranslation. 1 Introduction Research on grammatical error correction (GEC) has gained considerable attention recently. Many studies treat GEC as a task that involves translation from a grammatically erroneous sentence (sourceside) into a correct sentence (target-side) and thus"
W19-4413,P07-2045,0,0.00645643,"The implementation proposed by Artetxe et al. (2018b)7 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013)8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT (Och, 2003) was used with the tuning data for Mˆ2 Scorer (Dahlmeier and Ng, 2012). Synthetic sentence pairs with a [3, 80] sentence length were used at the refinement step. The number of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the dev data and submitted the output of the best iteration model. We"
W19-4413,D18-1399,0,0.306095,"Missing"
W19-4413,Q17-1010,0,0.0229184,"d from processed English News Crawl; the number of operations is 50K. The implementation proposed by Artetxe et al. (2018b)7 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013)8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT (Och, 2003) was used with the tuning data for Mˆ2 Scorer (Dahlmeier and Ng, 2012). Synthetic sentence pairs with a [3, 80] sentence length were used at the refinement step. The number of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the d"
W19-4413,D18-1549,0,0.215911,"atical error correction (GEC) has gained considerable attention recently. Many studies treat GEC as a task that involves translation from a grammatically erroneous sentence (sourceside) into a correct sentence (target-side) and thus, leverage methods based on machine translation (MT) for GEC. For instance, some GEC systems use large parallel corpora and synthetic data (Ge et al., 2018; Xie et al., 2018). We introduce an unsupervised method based on MT for GEC that does not use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018), Artetxe et al. (2018b), and Lample et al. (2018). These methods are based on phrase-based statistical machine translation (SMT) and phrase table refinements. Forward refinement used by Marie and Fujita (2018) simply augments a learner corpus with automatic corrections. We also use forward refinement for improvement of phrase table. Unsupervised MT techniques do not require a parallel but a comparable corpus as training data. Therefore, we use comparable translated texts using Google Translation as the source-side data. Specifically, we use News Crawl written in English as target-side data and News Crawl written in another language translate"
W19-4413,P17-1074,0,0.0838497,"of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the dev data and submitted the output of the best iteration model. We used pyspellchecker9 as a spell checker. This tool uses Levenshtein distance to obtain permutations within an edit distance of 2 over the words included in a word list. We made the word list from One Billion Word Benchmark and included words that occur more than five times. We report precision, recall, and F0.5 score based on the dev data and official test data. The output of dev data was evaluated using ERRANT scorer (Bryant et al., 2017) similarly to official test data. 3.2 Results Table 2 shows the results of the GEC experiments with test data. The F0.5 score for our system (TMU) is 28.31; this score is eighth among the nine teams. In particular, the number of false positives of our system is 4,314; this is the worst result of all. 4 Discussion Table 3 shows the results of the dev data listed in Table 1. On the dev data, the system of iteration 1 is the best among all. According to the improvement of iteration from 0 to 1, it is confirmed that the refinement method works well. However, it is observed that the system is not i"
W19-4413,P03-1021,0,0.0933824,"8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT (Och, 2003) was used with the tuning data for Mˆ2 Scorer (Dahlmeier and Ng, 2012). Synthetic sentence pairs with a [3, 80] sentence length were used at the refinement step. The number of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the dev data and submitted the output of the best iteration model. We used pyspellchecker9 as a spell checker. This tool uses Levenshtein distance to obtain permutations within an edit distance of 2 over the words included in a word list. We made the word list from One Billion Word Benchmark and included words that occu"
W19-4413,P16-1162,0,0.0128099,"ences from English News Crawl 2017 and excluded the sentences with more than 150 words for either source- and target-side data. Finally, the synthetic comparable corpus comprises processed News Crawl data listed in Table 1. The low resource track permitted to use W&I+LOCNESS (Bryant et al., 2019; Granger, 1998) development set, so we split it in half; tune data and dev data5 . These data are tokenized by spaCy v1.9.06 and the en_core_web_sm-1.2.0 model. We used moses truecaser for the training data; this truecaser model is learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016) learned from processed English News Crawl; the number of operations is 50K. The implementation proposed by Artetxe et al. (2018b)7 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013)8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fas"
W19-4413,N12-1067,0,0.0354852,"iewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT (Och, 2003) was used with the tuning data for Mˆ2 Scorer (Dahlmeier and Ng, 2012). Synthetic sentence pairs with a [3, 80] sentence length were used at the refinement step. The number of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the dev data and submitted the output of the best iteration model. We used pyspellchecker9 as a spell checker. This tool uses Levenshtein distance to obtain permutations within an edit distance of 2 over the words included in a word list. We made the word list from One Billion Word Benchmark and included words that occur more than five times. We report precision, recall, and F0.5 score ba"
W19-4413,D14-1102,0,0.0190605,"d Fujita (2018) to GEC. The UNMT method (Lample et al., 2018) was ineffective under the GEC setting in our preliminary experiments. GEC with NMT/SMT Several studies that introduce sequence-to-sequence models in GEC heavily rely on large amounts of training data. Ge et al. (2018), who presented state-of-the-art results in GEC, proposed a supervised NMT method trained on corpora of a total 5.4 M sentence pairs. We mainly use the monolingual corpus because the low resource track does not permit the use of the learner corpora. Despite the success of NMT, many studies on GEC traditionally use SMT (Susanto et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014). These studies apply an offthe-shelf SMT toolkit, Moses, to GEC. JunczysDowmunt and Grundkiewicz (2014) claimed that the SMT system optimized for BLEU learns to not change the source sentence. Instead of BLEU, they proposed tuning an SMT system using the M2 score with annotated development data. In this study, we also tune the weights with an F0.5 score measured by the M2 scorer because the official score is an F0.5 score. 6 Conclusion In this paper, we described our GEC system for the low resource track of the shared task at BEA2019. We introduced an"
W19-4413,P13-2071,0,0.0262268,"lit it in half; tune data and dev data5 . These data are tokenized by spaCy v1.9.06 and the en_core_web_sm-1.2.0 model. We used moses truecaser for the training data; this truecaser model is learned from processed English News Crawl. We used byte-pair-encoding (Sennrich et al., 2016) learned from processed English News Crawl; the number of operations is 50K. The implementation proposed by Artetxe et al. (2018b)7 was modified to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013)8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT ("
W19-4413,N18-1057,0,0.0512513,"le is combined with a language model trained by monolingual data to initialize a phrase-based SMT system. Finally, the SMT system is updated through iterative forwardtranslation. 1 Introduction Research on grammatical error correction (GEC) has gained considerable attention recently. Many studies treat GEC as a task that involves translation from a grammatically erroneous sentence (sourceside) into a correct sentence (target-side) and thus, leverage methods based on machine translation (MT) for GEC. For instance, some GEC systems use large parallel corpora and synthetic data (Ge et al., 2018; Xie et al., 2018). We introduce an unsupervised method based on MT for GEC that does not use parallel learner data. In particular, we use methods proposed by Marie and Fujita (2018), Artetxe et al. (2018b), and Lample et al. (2018). These methods are based on phrase-based statistical machine translation (SMT) and phrase table refinements. Forward refinement used by Marie and Fujita (2018) simply augments a learner corpus with automatic corrections. We also use forward refinement for improvement of phrase table. Unsupervised MT techniques do not require a parallel but a comparable corpus as training data. There"
W19-4413,N13-1073,0,0.0318297,"ed to conduct the experiments. Specifically, some features were added; word-level Levenshtein distance, word-, and character-level edit operation, operation sequence model, (Durrani et al., 2013)8 and 9-gram word class language model, similar to Grundkiewicz and Junczys-Dowmunt (2018) without sparse features. Word class language model was trained with One Billion Word Benchmark data; the number of classes is 200, and the word class was estimated with fastText (Bojanowski et al., 2017). The distortion feature was not used. Moses (Koehn et al., 2007) was used to train the SMT system. FastAlign (Dyer et al., 2013) was used for word alignment and KenLM (Heafield, 2011) was used to train the 5-gram language model over each processed English News Crawl and One Billion Word Benchmark. MERT (Och, 2003) was used with the tuning data for Mˆ2 Scorer (Dahlmeier and Ng, 2012). Synthetic sentence pairs with a [3, 80] sentence length were used at the refinement step. The number of iterations N was set to 5, and the embedding dimension was set to 300. We decided best iteration using the dev data and submitted the output of the best iteration model. We used pyspellchecker9 as a spell checker. This tool uses Levensht"
W19-4422,W13-1703,0,0.126707,"al. (2019). We used the same learner corpora with incorrect and correct sentences used for training our GEC model to finetune BERT. The 5-gram language model for re-ranking was trained on a subset of the Common Crawl corpus (Chollampatt and Ng, 2018a).5 We used a Python spell checker tool6 on the GEC model hypothesis sentences. Table 2: Hyperparameter values of our transformer GEC model. # 1 2 7 14 P 37.79 38.75 37.85 36.46 F0.5 69.47 69.00 64.73 53.45 Table 3: Results of GEC systems with the highest P, R and F0.5 overall vs TMU on restricted track on official W&I test data. English (NUCLE) (Dahlmeier et al., 2013) and Write & Improve (W&I)+LOCNESS corpus (Yannakoudakis et al., 2018; Granger, 1998) were used for this shared task. W&I+LOCNESS corpus was a new corpus released for this shared task and the shared task systems were evaluated on a gold test set of the overall W&I+LOCNESS dataset. 3.3 Evaluation The systems submitted to the shared task were evaluated using the ERRANT7 scorer (Felice et al., 2016; Bryant et al., 2017). This metric is an improved version of the MaxMatch scorer (Dahlmeier and Ng, 2012) originally used in the We used FCE (official split of train, dev, and test set), Lang-8, NUCLE,"
W19-4422,W12-2006,0,0.0214392,"ose to finetune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W&I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance. 1 Introduction Grammatical error correction (GEC) systems may be used for language learning to detect and correct grammatical errors in text written by language learners. GEC has grown in importance over the past few years due to the increasing need for people to learn new languages. GEC has been addressed in the Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and Conference on Natural Language Learning (CoNLL) (Ng et al., 2013, 2014) shared tasks between 2011 and 2014. Recent research has demonstrated the effectiveness of the neural machine translation model for 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ 207 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207–212 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Corpus FCE Lang-8 NUCLE W&I+LOCNESS the experimental results demonstrate that BERT, which considers both the representations trained on la"
W19-4422,W11-2838,0,0.0188641,"errors. Therefore, we propose to finetune BERT on learner corpora with grammatical errors for re-ranking. The experimental results of the W&I+LOCNESS development dataset demonstrate that re-ranking using BERT can effectively improve the correction performance. 1 Introduction Grammatical error correction (GEC) systems may be used for language learning to detect and correct grammatical errors in text written by language learners. GEC has grown in importance over the past few years due to the increasing need for people to learn new languages. GEC has been addressed in the Helping Our Own (HOO) (Dale and Kilgarriff, 2011; Dale et al., 2012) and Conference on Natural Language Learning (CoNLL) (Ng et al., 2013, 2014) shared tasks between 2011 and 2014. Recent research has demonstrated the effectiveness of the neural machine translation model for 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ 207 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207–212 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Corpus FCE Lang-8 NUCLE W&I+LOCNESS the experimental results demonstrate that BERT, which considers both the represent"
W19-4422,N19-1423,0,0.464275,"s to the improved hypotheses of the GEC model (Chollampatt and Ng, 2018a). Typically, a language model is trained by maximizing the log-likelihood of a sentence. Hence, such models observe only the positive examples of a raw corpus. However, these models may not be sufficient to take into account the grammatical errors written by language learners. Therefore, we fine-tune these models trained from large-scale raw data on learner corpora to explicitly take into account grammatical errors to re-rank the hypotheses for the GEC tasks. Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019) can consider information of large-scale raw corpora and task specific information by fine-tuning on the target task corpora. Moreover, BERT is known to be effective in the distinction of grammatical sentences from ungrammatical sentences (Kaneko and Komachi, 2019). They proposed a grammatical error detection (GED) model based on BERT that achieved state-of-the-art results in word-level GED tasks. Therefore, we use BERT, pre-trained with large-scale raw corpora, and fine-tune it with learner corpora for re-ranking the hypotheses of our GEC model to utilize not only the large-scale raw corpora"
W19-4422,C16-1079,0,0.610689,"P 37.79 38.75 37.85 36.46 F0.5 69.47 69.00 64.73 53.45 Table 3: Results of GEC systems with the highest P, R and F0.5 overall vs TMU on restricted track on official W&I test data. English (NUCLE) (Dahlmeier et al., 2013) and Write & Improve (W&I)+LOCNESS corpus (Yannakoudakis et al., 2018; Granger, 1998) were used for this shared task. W&I+LOCNESS corpus was a new corpus released for this shared task and the shared task systems were evaluated on a gold test set of the overall W&I+LOCNESS dataset. 3.3 Evaluation The systems submitted to the shared task were evaluated using the ERRANT7 scorer (Felice et al., 2016; Bryant et al., 2017). This metric is an improved version of the MaxMatch scorer (Dahlmeier and Ng, 2012) originally used in the We used FCE (official split of train, dev, and test set), Lang-8, NUCLE, and W&I+LOCNESS training set as training data and we split the W&I+LOCNESS development set into development and test data by random selection from each Common European Framework of Reference for Languages (CEFR) levels (beginner, intermediate, advanced, native) for the transformer and BERT. The development and test data sizes were 2,191 and 2,193, respectively. 2 https://github.com/pytorch/fair"
W19-4422,P18-1097,0,0.288801,"Missing"
W19-4422,Q17-1010,0,0.014534,"re-ranking Team Name UEDIN-MS Kakao&Brain ML@IITB TMU P 72.28 75.19 65.70 53.91 R 60.12 51.91 61.12 51.65 R 28.08 23.76 26.41 22.91 F0.5 35.35 34.41 34.83 32.60 Table 4: Effectiveness of re-ranking without different features. 3.2 Setup We implemented the transformer model based on the Fairseq tool2 . The hyperparameters used in our transformer GEC model are listed in Table 2. The parameters of the ensemble models were initialized with different values. We initialized the embedding layers of the encoder and decoder with the embeddings pre-trained on the English Wikipedia using fastText tool3 (Bojanowski et al., 2017). We used a publicly available pre-trained BERT model4 , namely the BERTBASE uncased model, which was pre-trained on large-scale BooksCorpus and English Wikipedia corpora. This model had 12 layers, 768 hidden sizes, and 16 heads of self-attention. Our model’s hyperparameters for re-ranking were similar to the default ones described by Devlin et al. (2019). We used the same learner corpora with incorrect and correct sentences used for training our GEC model to finetune BERT. The 5-gram language model for re-ranking was trained on a subset of the Common Crawl corpus (Chollampatt and Ng, 2018a).5"
W19-4422,N18-2046,0,0.0337807,"Missing"
W19-4422,P17-1074,0,0.0732655,"36.46 F0.5 69.47 69.00 64.73 53.45 Table 3: Results of GEC systems with the highest P, R and F0.5 overall vs TMU on restricted track on official W&I test data. English (NUCLE) (Dahlmeier et al., 2013) and Write & Improve (W&I)+LOCNESS corpus (Yannakoudakis et al., 2018; Granger, 1998) were used for this shared task. W&I+LOCNESS corpus was a new corpus released for this shared task and the shared task systems were evaluated on a gold test set of the overall W&I+LOCNESS dataset. 3.3 Evaluation The systems submitted to the shared task were evaluated using the ERRANT7 scorer (Felice et al., 2016; Bryant et al., 2017). This metric is an improved version of the MaxMatch scorer (Dahlmeier and Ng, 2012) originally used in the We used FCE (official split of train, dev, and test set), Lang-8, NUCLE, and W&I+LOCNESS training set as training data and we split the W&I+LOCNESS development set into development and test data by random selection from each Common European Framework of Reference for Languages (CEFR) levels (beginner, intermediate, advanced, native) for the transformer and BERT. The development and test data sizes were 2,191 and 2,193, respectively. 2 https://github.com/pytorch/fairseq https://github.com"
W19-4422,D16-1161,0,0.124669,"Missing"
W19-4422,D18-1274,0,0.25855,"komachi@}.tmu.ac.jp Abstract GEC. There are three main types of neural network models for GEC, namely, recurrent neural networks (Ge et al., 2018), a multi-layer convolutional model based on convolutional neural networks (Chollampatt and Ng, 2018a) and a transformer model based on self-attention (JunczysDowmunt et al., 2018). We follow the best practices to develop our system based on the transformer model, which has achieved better performance for GEC (Zhao et al., 2019). Re-ranking using a language model trained on large-scale corpora contributes to the improved hypotheses of the GEC model (Chollampatt and Ng, 2018a). Typically, a language model is trained by maximizing the log-likelihood of a sentence. Hence, such models observe only the positive examples of a raw corpus. However, these models may not be sufficient to take into account the grammatical errors written by language learners. Therefore, we fine-tune these models trained from large-scale raw data on learner corpora to explicitly take into account grammatical errors to re-rank the hypotheses for the GEC tasks. Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019) can consider information of large-scale raw corpor"
W19-4422,N18-1055,0,0.0860589,"ght to the precision. In this study, we report on precision, recall, and F0.5 based on the ERRANT score. 3.4 Results Table 3 presents the results of our system (TMU) and others on precision (P), recall (R) and F0.5 on W&I+LOCNESS test data for the BEA 2019 GEC shared task on the restricted track. Our system was ranked 14 out of 21 teams. 4 5 Discussions Related Work Re-ranking using a language model trained on large-scale raw data significantly improved the results in numerous GEC studies (JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2018a; Grundkiewicz and JunczysDowmunt, 2018; Junczys-Dowmunt et al., 2018; Zhao et al., 2019). However, their models do not explicitly consider grammatical errors of language learners. Yannakoudakis et al. (2017) utilized the score from a GED model as a feature to consider grammatical errors for re-ranking. Chollampatt and Ng (2018b) proposed a neural quality estimator for GEC. Their models predict the quality score when given a source sentence and its corresponding hypothesis. They consider representations of grammatical errors of learners for re-ranking. However, their models did not use large-scale raw corpora. Rei and Søgaard (2018) used a sentence-level GED mo"
W19-4422,N12-1067,0,0.139243,"st P, R and F0.5 overall vs TMU on restricted track on official W&I test data. English (NUCLE) (Dahlmeier et al., 2013) and Write & Improve (W&I)+LOCNESS corpus (Yannakoudakis et al., 2018; Granger, 1998) were used for this shared task. W&I+LOCNESS corpus was a new corpus released for this shared task and the shared task systems were evaluated on a gold test set of the overall W&I+LOCNESS dataset. 3.3 Evaluation The systems submitted to the shared task were evaluated using the ERRANT7 scorer (Felice et al., 2016; Bryant et al., 2017). This metric is an improved version of the MaxMatch scorer (Dahlmeier and Ng, 2012) originally used in the We used FCE (official split of train, dev, and test set), Lang-8, NUCLE, and W&I+LOCNESS training set as training data and we split the W&I+LOCNESS development set into development and test data by random selection from each Common European Framework of Reference for Languages (CEFR) levels (beginner, intermediate, advanced, native) for the transformer and BERT. The development and test data sizes were 2,191 and 2,193, respectively. 2 https://github.com/pytorch/fairseq https://github.com/facebookresearch/ fastText 4 https://github.com/google-research/ bert 5 https://git"
W19-4422,I11-1017,1,0.893993,". We annotated sentences from parallel learner corpora having incorrect and correct sentences with 0 (incorrect) and 1 (correct) labels. Hence, using the above, we can take advantage of • Hypothesis sentence length: The number of words in the hypothesis sentence to penalize short hypothesis sentences. Feature weights are optimized by minimum error rate training (MERT) (Och, 2003) on the development set. 3 3.1 Experiments Dataset In the restricted track, we only used the corpora listed in Table 1. The First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011), Lang8 learner corpus (Mizumoto et al., 2011), National University of Singapore Corpus of Learner 208 Parameter Word embedding size Multi-head number Layer size Hidden size Optimizer Adam β1 Adam β2 Learning rate Learning rate scheduler Warmup steps Minimum learning rate Dropout Weight decay Label smoothing Max token size Ensemble size Value 500 10 6 2,048 Adam 0.9 0.98 0.0005 inverse square root 4,000 1e-09 0.3 0.0001 0.1 4,096 3 Model TMU system w/o BERT w/o language model w/o re-ranking Team Name UEDIN-MS Kakao&Brain ML@IITB TMU P 72.28 75.19 65.70 53.91 R 60.12 51.91 61.12 51.65 R 28.08 23.76 26.41 22.91 F0.5 35.35 34.41 34.83 32.60"
W19-4422,P03-1021,0,0.00813532,"of the next sentence. We fine-tuned the pre-trained BERT on learner corpora to judge the grammatical quality of the input sentence, i.e., to distinguish between a sentence with and without grammatical errors on a sentence-level. We annotated sentences from parallel learner corpora having incorrect and correct sentences with 0 (incorrect) and 1 (correct) labels. Hence, using the above, we can take advantage of • Hypothesis sentence length: The number of words in the hypothesis sentence to penalize short hypothesis sentences. Feature weights are optimized by minimum error rate training (MERT) (Och, 2003) on the development set. 3 3.1 Experiments Dataset In the restricted track, we only used the corpora listed in Table 1. The First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011), Lang8 learner corpus (Mizumoto et al., 2011), National University of Singapore Corpus of Learner 208 Parameter Word embedding size Multi-head number Layer size Hidden size Optimizer Adam β1 Adam β2 Learning rate Learning rate scheduler Warmup steps Minimum learning rate Dropout Weight decay Label smoothing Max token size Ensemble size Value 500 10 6 2,048 Adam 0.9 0.98 0.0005 inverse square root 4,000"
W19-4422,N18-1027,0,0.0213483,"JunczysDowmunt, 2018; Junczys-Dowmunt et al., 2018; Zhao et al., 2019). However, their models do not explicitly consider grammatical errors of language learners. Yannakoudakis et al. (2017) utilized the score from a GED model as a feature to consider grammatical errors for re-ranking. Chollampatt and Ng (2018b) proposed a neural quality estimator for GEC. Their models predict the quality score when given a source sentence and its corresponding hypothesis. They consider representations of grammatical errors of learners for re-ranking. However, their models did not use large-scale raw corpora. Rei and Søgaard (2018) used a sentence-level GED model based on bidirectional long short-term memory (LSTM). The goal of their study was to predict the token-level labels on a sentence-level using the attention mechanism for zero-shot sequence labeling. We investigated whether using BERT as a feature for re-ranking can improve the corrected results. Table 4 presents the experimental results of removing the following re-ranking features: BERT (w/o BERT); language model (w/o language model); and all features (w/o re-ranking). The recall and F0.5 of the complete model (TMU system) is higher than those of w/o BERT, ind"
W19-4422,P11-1019,0,0.130423,"and without grammatical errors on a sentence-level. We annotated sentences from parallel learner corpora having incorrect and correct sentences with 0 (incorrect) and 1 (correct) labels. Hence, using the above, we can take advantage of • Hypothesis sentence length: The number of words in the hypothesis sentence to penalize short hypothesis sentences. Feature weights are optimized by minimum error rate training (MERT) (Och, 2003) on the development set. 3 3.1 Experiments Dataset In the restricted track, we only used the corpora listed in Table 1. The First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011), Lang8 learner corpus (Mizumoto et al., 2011), National University of Singapore Corpus of Learner 208 Parameter Word embedding size Multi-head number Layer size Hidden size Optimizer Adam β1 Adam β2 Learning rate Learning rate scheduler Warmup steps Minimum learning rate Dropout Weight decay Label smoothing Max token size Ensemble size Value 500 10 6 2,048 Adam 0.9 0.98 0.0005 inverse square root 4,000 1e-09 0.3 0.0001 0.1 4,096 3 Model TMU system w/o BERT w/o language model w/o re-ranking Team Name UEDIN-MS Kakao&Brain ML@IITB TMU P 72.28 75.19 65.70 53.91 R 60.12 51.91 61.12 51.65 R 28.08 2"
W19-4422,D17-1297,0,0.0868062,"sults of our system (TMU) and others on precision (P), recall (R) and F0.5 on W&I+LOCNESS test data for the BEA 2019 GEC shared task on the restricted track. Our system was ranked 14 out of 21 teams. 4 5 Discussions Related Work Re-ranking using a language model trained on large-scale raw data significantly improved the results in numerous GEC studies (JunczysDowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2018a; Grundkiewicz and JunczysDowmunt, 2018; Junczys-Dowmunt et al., 2018; Zhao et al., 2019). However, their models do not explicitly consider grammatical errors of language learners. Yannakoudakis et al. (2017) utilized the score from a GED model as a feature to consider grammatical errors for re-ranking. Chollampatt and Ng (2018b) proposed a neural quality estimator for GEC. Their models predict the quality score when given a source sentence and its corresponding hypothesis. They consider representations of grammatical errors of learners for re-ranking. However, their models did not use large-scale raw corpora. Rei and Søgaard (2018) used a sentence-level GED model based on bidirectional long short-term memory (LSTM). The goal of their study was to predict the token-level labels on a sentence-level"
W19-4422,N19-1014,0,0.579051,"Kengo Hotate Satoru Katsumata Mamoru Komachi Tokyo Metropolitan University, Japan {kaneko-masahiro@ed, hotate-kengo@ed, katsumata-satoru@ed, komachi@}.tmu.ac.jp Abstract GEC. There are three main types of neural network models for GEC, namely, recurrent neural networks (Ge et al., 2018), a multi-layer convolutional model based on convolutional neural networks (Chollampatt and Ng, 2018a) and a transformer model based on self-attention (JunczysDowmunt et al., 2018). We follow the best practices to develop our system based on the transformer model, which has achieved better performance for GEC (Zhao et al., 2019). Re-ranking using a language model trained on large-scale corpora contributes to the improved hypotheses of the GEC model (Chollampatt and Ng, 2018a). Typically, a language model is trained by maximizing the log-likelihood of a sentence. Hence, such models observe only the positive examples of a raw corpus. However, these models may not be sufficient to take into account the grammatical errors written by language learners. Therefore, we fine-tune these models trained from large-scale raw data on learner corpora to explicitly take into account grammatical errors to re-rank the hypotheses for t"
W19-4431,I17-1005,1,0.859286,"rrect examples is small; therefore, it is difficult for language learners to use the limited set of examples as a reference. • Our intrinsic evaluation shows that our system is good at correcting lexical choice and misformation errors in a learner’s writing. Our extrinsic evaluation also shows that our example sentence retrieval system improves the quality of a learner’s writing. 2 Related Works 2.1 Grammatical Error Detection In the grammatical error detection task in English, neural methods such as Bi-LSTM in particular have been actively used (Rei et al., 2016; Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Kasewa et al., 2018). Most studies on grammatical error detection/correction in Japanese limit the target learner’s error types, mainly to particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012; Oyama et al., 2013). Among others, there are studies in Japanese grammatical error correction using statistical machine translation which do not limit the type of errors from the learner (Mizumoto et al., 2011). On the other hand, in Japanese, there are few studies on grammatical error detection and correction using neural networks. In this study, we constructed an error det"
W19-4431,D18-1541,0,0.0180649,"ll; therefore, it is difficult for language learners to use the limited set of examples as a reference. • Our intrinsic evaluation shows that our system is good at correcting lexical choice and misformation errors in a learner’s writing. Our extrinsic evaluation also shows that our example sentence retrieval system improves the quality of a learner’s writing. 2 Related Works 2.1 Grammatical Error Detection In the grammatical error detection task in English, neural methods such as Bi-LSTM in particular have been actively used (Rei et al., 2016; Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Kasewa et al., 2018). Most studies on grammatical error detection/correction in Japanese limit the target learner’s error types, mainly to particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012; Oyama et al., 2013). Among others, there are studies in Japanese grammatical error correction using statistical machine translation which do not limit the type of errors from the learner (Mizumoto et al., 2011). On the other hand, in Japanese, there are few studies on grammatical error detection and correction using neural networks. In this study, we constructed an error detection system using a"
W19-4431,C10-2103,0,0.0291529,"al system (BC) and the baseline incorrect example retrieval system (BI) are used as the baseline systems. We searched for these phrases in each system (BC, BI, and ours) and counted the number of hits for each system that led to the top-1 correct expressions to measure relevance. The proposed system searches either the correct or incorrect sentences including the target phrase depending on whether the query contains errors while it searches for the phrase corResult The results are shown in Table 3. It can be seen that all of the precision, recall, and F-values are better than the baseline. As Nagata and Nakatani (2010) suggested, a high precision error detection system can be used to help learners write essays. We will verify this hypothesis in the next subsection. Table 4 lists the number of true positives and false negatives by error type. Particle choice, pronunciation, and misformation are easy to detect. Lexical choice and alternating form are hard to detect. The number of false negatives for particle choice is large because it forms the majority of all the errors. 301 true positive incorrect correct meaning おねがい 、 しあわせ に なる ！ おねがい 、 しあわせ に なり たい ！ Please, I hope to be happy! false negative incorrect c"
W19-4431,P12-3027,0,0.0161952,"al System for English as a Second Language Web-based search engines are the most common search systems that can be used to search for example sentences. However, these search engines are not intended to retrieve examples for language learners; therefore, the search engines neither show example sentences nor the correct version of a given incorrect sentence to aid learners. Language learners can use several example retrieval systems. All of them provide special features for writing assistance, but none of them offers grammatical error detection and incorrect examples to support learners. FLOW (Chen et al., 2012) is a system that displays some candidates for English words when ESL learners write a sentence in their native language using candidate paraphrases with bilingual pivoting. By contrast, our system suggests incorrect examples and their counterparts based on corrections from the learner corpus. Another system, called StringNet (Wible and Tsao, 2010), displays the patterns in which a query is used, along with their frequency. The noun and the preposition are substituted by their parts of speech, in place of the words themselves, to eliminate data sparseness. The ESCORT (Matsubara et al., 2008) s"
W19-4431,P03-1021,0,0.0203842,"ror tags as the gold label for grammatical error detection. Setting For hyper parameter settings, the dimension of the word embedding and the word-level LSTM are 300, and the dimension of the character embedding and the character-level LSTM are 100. The Bi-LSTM models are optimized using Adadelta with a learning rate of 1.0 and a batch size of 64 sentences. These word and character embeddings are updated during training. We reimplemented the word-wise phrase-based statistical machine translation system of Mizumoto et al. (2011) as a baseline system. We used minimum error rate training (MERT) (Och, 2003) for the model. 4.3 Incorrect Example Retrieval System Intrinsic Evaluation We randomly extracted 55 incorrect phrases and 55 correct phrases from the learner’s sentences in the Lang-8 dataset, which are not included in the corpus of the retrieval system. We classified each incorrect example into seven types: alternating form (A), lexical choice (L), omission (O), misformation (M), redundant (R), pronunciation (P), and others (X). Table 6 lists the examples of the test phrases. Table 7 shows the frequency of each error type and the relevance of each system per error type. An example is judged"
W19-4431,Y13-1014,1,0.864758,"arner’s writing. Our extrinsic evaluation also shows that our example sentence retrieval system improves the quality of a learner’s writing. 2 Related Works 2.1 Grammatical Error Detection In the grammatical error detection task in English, neural methods such as Bi-LSTM in particular have been actively used (Rei et al., 2016; Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Kasewa et al., 2018). Most studies on grammatical error detection/correction in Japanese limit the target learner’s error types, mainly to particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012; Oyama et al., 2013). Among others, there are studies in Japanese grammatical error correction using statistical machine translation which do not limit the type of errors from the learner (Mizumoto et al., 2011). On the other hand, in Japanese, there are few studies on grammatical error detection and correction using neural networks. In this study, we constructed an error detection system using a neural network without limiting the target error type. Although phrase-based statistical machine translation cannot consider long-distance relationships because it is n-gram based, neural networks using BiLSTM can consid"
W19-4431,W16-6509,0,0.0706222,"Missing"
W19-4431,P12-2076,0,0.680858,"e learner does not need to be aware of how to search for examples. Intrinsic and extrinsic evaluations indicate that our method improves the accuracy of example sentence retrieval and the quality of a learner’s writing. 1 Introduction Grammatical error detection for learners of English as a second language (ESL) is widely studied. However, there are few studies on grammatical error detection for learners of Japanese as a second language (JSL). Most studies on grammatical error detection in Japanese focus on a learner’s particular error types, mainly with particles (Suzuki and Toutanova, 2006; Imamura et al., 2012). Among others, there are studies using phrase-based statistical machine translation (PBSMT), which does not limit the types of grammatical errors made by a learner (Mizumoto et al., 2011). However, PBSMT-based grammatical error detection cannot consider long-distance relationships because it relies on either character or word n-grams. A standard method that supports the effort of learning a second language is the use of examples. Example retrieval systems such as Rakhilina et al. (2016) and Kilgarriff et al. (2004) in particular check for the appropriate use of words based on • This is the fi"
W19-4431,C16-1030,0,0.0766856,"highfrequency words, because the number of incorrect examples is small; therefore, it is difficult for language learners to use the limited set of examples as a reference. • Our intrinsic evaluation shows that our system is good at correcting lexical choice and misformation errors in a learner’s writing. Our extrinsic evaluation also shows that our example sentence retrieval system improves the quality of a learner’s writing. 2 Related Works 2.1 Grammatical Error Detection In the grammatical error detection task in English, neural methods such as Bi-LSTM in particular have been actively used (Rei et al., 2016; Rei and Yannakoudakis, 2016; Kaneko et al., 2017; Kasewa et al., 2018). Most studies on grammatical error detection/correction in Japanese limit the target learner’s error types, mainly to particles (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Imamura et al., 2012; Oyama et al., 2013). Among others, there are studies in Japanese grammatical error correction using statistical machine translation which do not limit the type of errors from the learner (Mizumoto et al., 2011). On the other hand, in Japanese, there are few studies on grammatical error detection and correction using neural ne"
W19-4431,P06-1132,0,0.803669,"detection system so that the learner does not need to be aware of how to search for examples. Intrinsic and extrinsic evaluations indicate that our method improves the accuracy of example sentence retrieval and the quality of a learner’s writing. 1 Introduction Grammatical error detection for learners of English as a second language (ESL) is widely studied. However, there are few studies on grammatical error detection for learners of Japanese as a second language (JSL). Most studies on grammatical error detection in Japanese focus on a learner’s particular error types, mainly with particles (Suzuki and Toutanova, 2006; Imamura et al., 2012). Among others, there are studies using phrase-based statistical machine translation (PBSMT), which does not limit the types of grammatical errors made by a learner (Mizumoto et al., 2011). However, PBSMT-based grammatical error detection cannot consider long-distance relationships because it relies on either character or word n-grams. A standard method that supports the effort of learning a second language is the use of examples. Example retrieval systems such as Rakhilina et al. (2016) and Kilgarriff et al. (2004) in particular check for the appropriate use of words ba"
W19-4431,P12-2039,1,0.912422,"Missing"
W19-4431,W10-0804,0,0.0139189,"ct sentence to aid learners. Language learners can use several example retrieval systems. All of them provide special features for writing assistance, but none of them offers grammatical error detection and incorrect examples to support learners. FLOW (Chen et al., 2012) is a system that displays some candidates for English words when ESL learners write a sentence in their native language using candidate paraphrases with bilingual pivoting. By contrast, our system suggests incorrect examples and their counterparts based on corrections from the learner corpus. Another system, called StringNet (Wible and Tsao, 2010), displays the patterns in which a query is used, along with their frequency. The noun and the preposition are substituted by their parts of speech, in place of the words themselves, to eliminate data sparseness. The ESCORT (Matsubara et al., 2008) system shows example sentences to learners based on the grammatical relations of queries. The syntactic structures of the English sentences are stored in 3 Incorrect Example Retrieval System using Grammatical Error Detection for JSL This section describes our incorrect example retrieval system with grammatical error detection. It combines grammatica"
W19-4431,P15-4024,0,0.0287783,"incorrect and correct labels. The c indicates that the target word is correct. The i indicates that the target word is incorrect. The meaning of this sentence is “I am very busy at school now.” the database of a raw corpus. ESCORT analyzes the dependency relations of the input queries and only displays appropriate examples that match the relations. Our system displays the examples in descending order of the cosine similarity of the input vector and vectors of the examples to avoid data sparseness. Furthermore, ESL learners can check examples while writing an English sentence using WriteAhead (Yen et al., 2015). This system shows pattern suggestions based on collocation and syntax. For example, when the user writes “We discussed,” the system displays the patterns for the use of the word “discussed.” Sketch Engine (Kilgarriff et al., 2004) displays the grammar constructs associated with words along with the thesaurus information. As previously mentioned, our system presents incorrect examples using a learner corpus apart from the correct examples extracted from a raw corpus. 2.3 Example Retrieval System for English as a Second Language Web-based search engines are the most common search systems that"
W19-5360,D14-1162,0,0.0817908,"Missing"
W19-5360,N18-1202,0,0.0144012,"al., 2019) to filter pseudo-references by checking the paraphrasability with a gold reference. BERT is a new approach to pre-train language representations, and it obtains state-of-the-art results on a wide variety of natural language processing (NLP) tasks, including question answering (QA), semantic textual similarity (STS), natural language inference (NLI). The key to pre-training BERT is the prediction of masked words and of the next sentence. Masking words allows bidirectional learning, which improves joint training of language context relative to Embeddings from Language Models (ELMo) (Peters et al., 2018), which combines forward and backward training. Prediction of the next sentence leads to capturing the relationship between two sentences. Figure 2 shows the BERT model architec4. Calculate the sentence evaluation score with multiple references obtained by adding filtered pseudo-references to the single gold references. 3.2 Automatic pseudo-reference generation Any MT system can be used as a pseudo-reference generation system except for the translation system to be evaluated. 1 There are no restrictions on the type of MT systems, such as neural machine translation (NMT) or statistical machine"
W19-5360,W08-0330,0,0.344973,"tic evaluation methods allow the use of multiple references that potentially cover various surfaces; in particular, Finch et al. (2004) reported that correlation between automatic evaluation results and human evaluation increases when multiple references are used for evaluation. However, owing to the time and costs involved in manually creating references, many datasets only include one reference per source sentence, which leads to improper translation evaluation, especially in the case of diverse machine translation systems. In order to obtain cheap references without any human intervention, Albrecht and Hwa (2008) used the outputs of off-the-shelf MT systems as pseudo-references; They showed that using mulAlbrecht and Hwa (2008) showed that using the outputs of off-the-shelf MT systems as pseudoreferences in n-gram based metrics such as BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) may yield higher correlation with human evaluation than using a gold reference. They use the outputs of off-theshelf MT systems as they are, whereas we filter them by paraphrasing the gold reference. Kauchak and Barzilay (2006) proposed a method to obtain a paraphrase of a gold reference that is closer"
W19-5360,N15-1138,0,0.0160223,"paraphrase score. 4.4 Evaluation We calculated the SentBLEU score with system output and multiple references which consisted of a single gold reference and pseudo-references. The SentBLEU is computed using the sentencebleu.cpp 5 , a part of the Moses toolkit. It is a smoothed version of BLEU (Lin and Och, 2004). We followed the tokenization method for each year’s dataset. We measured Pearson correlation identically to WMT 2016 and WMT 2017 between the automatic and human evaluation scores. In order to compare with our method, we also performed filtering by Maximum Alignment Similarity (MAS) (Song and Roth, 2015), which is one of the unsupervised sentence similarity measures based on alignments between word embeddings 6 https://nlp.stanford.edu/projects/glove/ Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors) 5 https://github.com/moses-smt/ mosesdecoder/blob/master/mert/sentence-bleu.cpp 524 7 Conclusions David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (NAACL2006). We proposed a method to filter pseudo-references in terms of paraphrasability with a gold referenc"
W19-5360,W17-4755,0,0.0504782,"and human evaluation scores. 4.2 Off-the-shelf MT systems We used Google Translation 2 and Bing Microsoft Translator 3 as MT systems to generate pseudoreferences. We chose these two MT systems because they are widely used, easy to use, and well known to have good performance. We automatically translated source files using each translation API. Experiments 4.1 Data 4.3 Fine-tuning BERT with MRPC We used the segment-level evaluation datasets of Czech-English (cs-en), German-English (deen), Finnish-English (fi-en), Russian-English (ruen) language pair from WMT 2016 (Bojar et al., 2016) and 2017 (Bojar et al., 2017). The datasets consist of 560 pairs of sources and references, We use the pre-trained BERT-Base Uncased model 4 , which has 12 layer, 768 hidden, 12 heads 2 https://translate.google.com/ https://www.bing.com/translator 4 https://github.com/google-research/bert 3 523 system output gold reference gymnastics and freestyle exercises - where bayles defends the title of world champion - lie in the veil . balance beam and floor exercise - where biles is the defending world champion - lay in wait . pseudo-reference (Google) gymnastics log and floor exercises - where biles defends the world champion ti"
W19-5360,W16-2302,0,0.0687876,"Missing"
W19-5360,W11-2107,0,0.0461522,"ng to the time and costs involved in manually creating references, many datasets only include one reference per source sentence, which leads to improper translation evaluation, especially in the case of diverse machine translation systems. In order to obtain cheap references without any human intervention, Albrecht and Hwa (2008) used the outputs of off-the-shelf MT systems as pseudo-references; They showed that using mulAlbrecht and Hwa (2008) showed that using the outputs of off-the-shelf MT systems as pseudoreferences in n-gram based metrics such as BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) may yield higher correlation with human evaluation than using a gold reference. They use the outputs of off-theshelf MT systems as they are, whereas we filter them by paraphrasing the gold reference. Kauchak and Barzilay (2006) proposed a method to obtain a paraphrase of a gold reference that is closer in wording to the system output than the gold reference for MT evaluation. They evaluated an MT system using only the generated references, whereas we evaluated MT systems using multiple references, including those obtained by adding generated references to the gold reference. They generate a p"
W19-5360,N19-1423,0,0.0137703,"eudo-References 3.1 Overview Figure 1 shows the overview of our proposed method. The procedure of our proposed method is as follows. 1. Prepare off-the-shelf MT systems for generating pseudo-references. Figure 2: BERT model architecture for sentence pair classification. 2. Translate the source sentence in the evaluation data using the abovementioned MT systems. 3.3 Filtering by paraphrasing 3. Filter the outputs of off-the-shelf MT systems by checking the paraphrasability of being a paraphrase to the single gold reference. We use Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) to filter pseudo-references by checking the paraphrasability with a gold reference. BERT is a new approach to pre-train language representations, and it obtains state-of-the-art results on a wide variety of natural language processing (NLP) tasks, including question answering (QA), semantic textual similarity (STS), natural language inference (NLI). The key to pre-training BERT is the prediction of masked words and of the next sentence. Masking words allows bidirectional learning, which improves joint training of language context relative to Embeddings from Language Models (ELMo) (Peters et a"
W19-5360,I05-5002,0,0.094809,"corpus MRPC train 3,669 dev 408 test 1726 Accuracy 0.845 Table 3: Numbers of sentences in each split of MRPC and accuracy of BERT. ture for sentence pair classification. In classification tasks where labels are attached to sentence pairs, BERT encodes sentence pairs together with a [CLS] token for classification and a [SEP] token for sentence boundaries; The output of the [CLS] token is used for the input of classifier of a feedforward neural network with softmax. BERT achieves state-of-the-art performance in a paraphrase identification task on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) with this architecture. For that reason, we use BERT to estimate the paraphrasability between pseudo-references and the gold reference. We fine-tune BERT with MRPC. The output of the classifier is the probability of the paraphrase from 0 to 1. We use pseudoreferences whose paraphrase probability is greater than 0.5. 4 Figure 3: Histograms of paraphrase score of pseudoreferences in the fi-en language pairs of WMT 2016. along with the outputs of each system and human evaluation scores. 4.2 Off-the-shelf MT systems We used Google Translation 2 and Bing Microsoft Translator 3 as MT systems to gen"
W19-5360,finch-etal-2004-automatic,0,0.089871,"perimental results show that our method achieves higher correlation with human evaluation than the previous work. Introduction 2 Related Work In general, automatic evaluation of MT is based on n-gram agreement between the system output and a manually translated reference of the source sentence. Therefore, automatic evaluation fails to evaluate a semantically correct sentence if the surface of the system output differs from that in the reference. To solve this problem, many automatic evaluation methods allow the use of multiple references that potentially cover various surfaces; in particular, Finch et al. (2004) reported that correlation between automatic evaluation results and human evaluation increases when multiple references are used for evaluation. However, owing to the time and costs involved in manually creating references, many datasets only include one reference per source sentence, which leads to improper translation evaluation, especially in the case of diverse machine translation systems. In order to obtain cheap references without any human intervention, Albrecht and Hwa (2008) used the outputs of off-the-shelf MT systems as pseudo-references; They showed that using mulAlbrecht and Hwa ("
W19-6604,W16-2506,0,0.164562,"eddings have proved useful in low-resource domains (Qi et al., 2018), in which FastText (Bojanowski et al., 2017) embeddings are used to initialize the encoder and decoder of the NMT model. They provided substantial overall performance improvement for lowresource language pairs. Similarly, Hirasawa et al. (2019) introduced a multimodal NMT model with embedding prediction that provided substantial performance improvement. However, when word embeddings are used in the k-nearest neighbor (kNN) problem, certain words appear frequently in the k-nearest neighbors for other words (Dinu et al., 2015; Faruqui et al., 2016); this is called the hubness problem in the general machine learning domain (Radovanovi´c et al., 2010). This phenomenon harms the utility of pretrained word embeddings. In the context of NMT, Rios Gonzales et al. (2017) reported that NMT models produce less-accurate translations for less-frequent words, but they are not aware of the hubness problem in word embeddings. Instead, they proposed annotating sense labels or lexical labels to address this problem. However, it is known to be effective to debias word embeddings based on their local bias (Hara et al., 2015) or global bias (Mu and Viswan"
W19-6604,W18-6439,0,0.0472068,"Missing"
W19-6604,Q17-1010,0,0.264441,"tion, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Mamoru Komachi Tokyo Metropolitan University komachi@tmu.ac.jp of the attentive encoder-decoder model, in which machine translation is treated as a sequence-tosequence learning problem and is trained to pay attention to the source sentence while decoding (Bahdanau et al., 2015). Pretrained word embeddings are considered an important part of neural network models in many natural language processing (NLP) tasks. In the context of NMT, pretrained word embeddings have proved useful in low-resource domains (Qi et al., 2018), in which FastText (Bojanowski et al., 2017) embeddings are used to initialize the encoder and decoder of the NMT model. They provided substantial overall performance improvement for lowresource language pairs. Similarly, Hirasawa et al. (2019) introduced a multimodal NMT model with embedding prediction that provided substantial performance improvement. However, when word embeddings are used in the k-nearest neighbor (kNN) problem, certain words appear frequently in the k-nearest neighbors for other words (Dinu et al., 2015; Faruqui et al., 2016); this is called the hubness problem in the general machine learning domain (Radovanovi´c et"
W19-6604,W18-6438,0,0.033451,"Missing"
W19-6604,P17-1175,0,0.0487614,"Missing"
W19-6604,W14-4012,0,0.127486,"Missing"
W19-6604,W14-3348,0,0.0144356,"VAGNET, respectively. 5.3 Model All models are implemented using nmtpytorch toolkit v3.0.013 (Caglayan et al., 2017). The encoder for each model has one layer with 256 hidden dimensions, and therefore the bidirectional GRU has 512 dimensions. We set the latent space vector size for IMAGINATION to 2048 and the dimension of the shared visual-text space for VAG-NET to 512. The input word embedding size and output vector size are 300 each. We use the Adam optimizer with learning rate of 0.0004. The gradient norm is clipped to 1.0. The dropout rate is 0.3. BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) are used as performance metrics. As in (Qi et al., 2018), we also evaluated the models using the F-score of each word. The F-score is calculated as the harmonic mean of the precision (the fraction of produced sentences containing a word that is in the references sentences) and the recall (the fraction of reference sentences containing a word that is in the model outputs). We ran the experiment three times with different random seeds and obtained the mean for each model. Dublin, Aug. 19-23, 2019 |p. 37 English → German Model debiasing embedding BLEU None METEOR Localized Centering BLEU METEOR"
W19-6604,W18-6441,0,0.0397951,"Missing"
W19-6604,N19-3012,1,0.59485,"sequence-tosequence learning problem and is trained to pay attention to the source sentence while decoding (Bahdanau et al., 2015). Pretrained word embeddings are considered an important part of neural network models in many natural language processing (NLP) tasks. In the context of NMT, pretrained word embeddings have proved useful in low-resource domains (Qi et al., 2018), in which FastText (Bojanowski et al., 2017) embeddings are used to initialize the encoder and decoder of the NMT model. They provided substantial overall performance improvement for lowresource language pairs. Similarly, Hirasawa et al. (2019) introduced a multimodal NMT model with embedding prediction that provided substantial performance improvement. However, when word embeddings are used in the k-nearest neighbor (kNN) problem, certain words appear frequently in the k-nearest neighbors for other words (Dinu et al., 2015; Faruqui et al., 2016); this is called the hubness problem in the general machine learning domain (Radovanovi´c et al., 2010). This phenomenon harms the utility of pretrained word embeddings. In the context of NMT, Rios Gonzales et al. (2017) reported that NMT models produce less-accurate translations for less-fr"
W19-6604,D17-1308,0,0.0390899,"Missing"
W19-6604,P02-1040,0,0.104357,"layer of 2048D for IMAGINATION and VAGNET, respectively. 5.3 Model All models are implemented using nmtpytorch toolkit v3.0.013 (Caglayan et al., 2017). The encoder for each model has one layer with 256 hidden dimensions, and therefore the bidirectional GRU has 512 dimensions. We set the latent space vector size for IMAGINATION to 2048 and the dimension of the shared visual-text space for VAG-NET to 512. The input word embedding size and output vector size are 300 each. We use the Adam optimizer with learning rate of 0.0004. The gradient norm is clipped to 1.0. The dropout rate is 0.3. BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) are used as performance metrics. As in (Qi et al., 2018), we also evaluated the models using the F-score of each word. The F-score is calculated as the harmonic mean of the precision (the fraction of produced sentences containing a word that is in the references sentences) and the recall (the fraction of reference sentences containing a word that is in the model outputs). We ran the experiment three times with different random seeds and obtained the mean for each model. Dublin, Aug. 19-23, 2019 |p. 37 English → German Model debiasing embedding BLEU None"
W19-6604,D14-1162,0,0.0878412,"t vectors of a source sentence and the paired image closer. ∑ JV (θ, ϕV ) = max{0, α − d(ˆ v , v) + d(ˆ v , v ′ )} N ∑ (29) 3 We use ρ = 0.5 in our experiment. Dublin, Aug. 19-23, 2019 |p. 35 where d is a cosine similarity function; k and p is the index for sentences and images, respectively; tk=p , the negative samples for which all examples in the same batch with the target example are selected; and γ, the margin that adjusts the sparseness of each item in the latent space4 . 4 Word Embedding In this study, we compare three different word embeddings: word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017). Section 5.1 describes the configurations to build each embedding. When we use word embeddings of high dimension in the kNN problem in which the similarity of two words is computed using a distance function, certain words frequently appear in the k-nearest neighbors of other words (Dinu et al., 2015; Faruqui et al., 2016); this is called the hubness problem in the general machine learning domain (Radovanovi´c et al., 2010). This phenomenon harms the utility of pretrained word embeddings. In the context of NMT, Rios Gonzales et al. (2017) report that les"
W19-6604,I17-1014,0,0.037309,"Missing"
W19-6604,N18-2084,0,0.0946926,"licence, no derivative works, attribution, CCBY-ND. Proceedings of MT Summit XVII, volume 1 Mamoru Komachi Tokyo Metropolitan University komachi@tmu.ac.jp of the attentive encoder-decoder model, in which machine translation is treated as a sequence-tosequence learning problem and is trained to pay attention to the source sentence while decoding (Bahdanau et al., 2015). Pretrained word embeddings are considered an important part of neural network models in many natural language processing (NLP) tasks. In the context of NMT, pretrained word embeddings have proved useful in low-resource domains (Qi et al., 2018), in which FastText (Bojanowski et al., 2017) embeddings are used to initialize the encoder and decoder of the NMT model. They provided substantial overall performance improvement for lowresource language pairs. Similarly, Hirasawa et al. (2019) introduced a multimodal NMT model with embedding prediction that provided substantial performance improvement. However, when word embeddings are used in the k-nearest neighbor (kNN) problem, certain words appear frequently in the k-nearest neighbors for other words (Dinu et al., 2015; Faruqui et al., 2016); this is called the hubness problem in the gen"
W19-6604,W16-3210,0,0.0710096,"Missing"
W19-6604,P16-1162,0,0.0362493,"ings for both types of OOV words are calculated as the average embedding over words that are a part of pretrained word embeddings but are not included in the vocabularies, and they are updated individually during training. 5.2 Dataset We train, validate, and test all multimodal NMT models using the Multi30K (Elliott et al., 2016) dataset. English is selected as the source language, and German/French are selected as target languages. All sentences in all languages are preprocessed by lower-casing, tokenizing, and normalizing the punctuation. We run experiments without byte pair encoding (BPE) (Sennrich et al., 2016) for all models as BPE breaks a word into subwords, resulting in an increase in OOV words for word2vec and GloVe embeddings. In addition, we also run experiments using BPE with 10k merge operations to show the utility of pretrained word embeddings. The BPE subwords are shared for source and target languages and learnt from training dataset12 . Table 2 shows the statistics of vocabularies in the Multi30K training data. Visual features are extracted using pretrained ResNet-50 (He et al., 2016). We encode all images in the Multi30K dataset using ResNet-50 and pick out the hidden state in the res4"
W19-6604,D18-1400,0,0.222325,"lated Works With the recent development of multimodal parallel corpora such as Multi30K (Elliott et al., 2016), many multimodal NMT models have been proposed. Most of these models are divided into two categories: visual feature integration and multitask learning. In both categories, visual features are extracted using image processing techniques. Visual feature adaptation Visual features are extracted using image processing techniques and then integrated into a machine translation model in many ways. These studies include incorporation with visual features in NMT models (Calixto et al., 2017; Zhou et al., 2018) and multitask learning models (Elliott and K´ad´ar, 2017; Zhou et al., 2018), as discussed later in Section 3. Data augmentation Owing to the lack of the available datasets, data augmentation is widely studied in multimodal NMT. Compared to a parallel corpus without images (Gr¨onroos et al., 2018) and a pseudo-parallel corpus (Helcl et al., 2018), few studies have used monolingual data. Hirasawa et al. (2019) proposed a multimodal NMT model with embedding prediction to fully use pretrained word embeddings. However, the use of word embeddings has not been studied among various multimodal NMT m"
W19-6604,W17-4702,0,\N,Missing
Y13-1014,P06-1032,0,0.0692453,"d Yuji Matsumoto 27th Pacific Asia Conference on Language, Information, and Computation pages 163－172 PACLIC-27 2012)4 (540 files). Our work aims to add error type annotation on learner corpora. Unlike previous research which depend on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto e"
Y13-1014,C08-1022,0,0.102905,"Missing"
Y13-1014,I08-1059,0,0.0188571,"iles). Our work aims to add error type annotation on learner corpora. Unlike previous research which depend on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error t"
Y13-1014,P12-2076,0,0.19364,"untable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays compiled by the National Institute of Japanese Language 2 consisting of 1,565 essays writt"
Y13-1014,I11-1017,1,0.945977,"al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays compiled by the National Institute of Japanese Language 2 consisting of 1,565 essays written by learners from 15 different"
Y13-1014,P07-1011,0,0.0120641,"on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learner"
Y13-1014,P06-1132,0,0.229623,"grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays c"
Y13-1014,N12-1037,0,0.0291488,"(Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As for automatic error type classification, Swanson and Yamangil (2012) deal with 15 error type classification in English learners’ essays in the Cambridge Learner Corpus（CLC 1 ). However, they did not report an inter-corpus evaluation. Japanese Language Learners’ Corpora: Japanese language learner corpora include Taiyaku DB, which is a multilingual database of Japanese learners’ essays compiled by the National Institute of Japanese Language 2 consisting of 1,565 essays written by learners from 15 different countries. The KY corpus (Kamata and Yamauchi, 1999) has spoken data of Japanese language learners at different proficiency levels. There are several Japanese"
Y13-1014,C08-1109,0,0.0266031,"63－172 PACLIC-27 2012)4 (540 files). Our work aims to add error type annotation on learner corpora. Unlike previous research which depend on entirely manual annotation, we focus on semi-automatic annotation method to reduce human cost and to improve consistency in annotation. in Section 5. 2 Previous Work Automatic Error Detection Systems: In the writing of learners of English, automatic grammatical error detection is used for spelling error (Mays et al., 1991), countable or uncountable noun errors (Brockett et al., 2006; Nagata et al., 2006), prepositional errors (De Felice and Pulman, 2008; Tetreault and Chodorow, 2008; Gamon et al., 2008) and article errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2008). Sun et al. (2007) focus on discriminating between erroneous and correct sentences without considering error types. As for texts by Japanese learners, most of the research focuses on correcting errors with particles (postpositions) (Imaeda et al., 2003; Suzuki and Toutanova, 2006; Nampo et al., 2007; Oyama and Matsumoto, 2010; Ohki et al., 2011; Imamura et al., 2012). Besides, Mizumoto et al. (2011) consider error correction in the language learners’ writing handling any error types. As"
Y15-1018,D08-1083,0,0.103985,"Missing"
Y15-1018,C14-1008,0,0.0736357,"Missing"
Y15-1018,I08-1039,0,0.027461,"uracy in sentiment classification tasks. • We achieve state-of-the-art performance in Japanese sentiment classification tasks without designing complex features and models. 2 Related Works In this section, we discuss related works from two areas: sentiment classification and deep learning (distributed word representation and multi-layer neural networks). 2.1 Sentiment classification Sentiment classification has been researched extensively in the past decade. Most of the previous approaches in this area rely on either timeconsuming hand-tagged dictionaries or knowledgeintensive complex models. Ikeda et al. (2008) proposed a method that classifies polarities by learning them within a window around a word. Their proposed method works well with words registered in a dictionary. However, building a polarity dictionary is expensive and their approach is not able to cope with unknown words. In contrast, our proposed approach does not use a polarity dictionary and works robustly even when there are infrequent words in the test data. In a similar manner, Choi et al. (2008) proposed a method in which rules are manually built up and polarities are classified considering dependency structures. However, the rules"
Y15-1018,D14-1080,0,0.0826555,"Missing"
Y15-1018,D14-1181,0,0.0262189,"arched sentiment classification of microblogs such as Twitter using the distributed representation learned by the methods of Collobert et al. (2008) and Mikolov et al. (2013b; 2013a). Those two tasks are the same task as ours, but the former generats sentence vectors using string-based convolution networks while the latter utilizes a model that treats the distributed word representation itself as polarities. Our proposed approach makes sentence vectors by simply averaging the distributed word representation, yet achieves state-of-the-art performance in Japanese sentiment classification tasks. Kim (2014) classified the polarities of sentences using convolutional neural networks. He built a simple CNN with one layer of convolution, whereas our model uses multiple hidden layers. Socher et al. (2011; 2013) placed common autoencoders recursively (recursive neural networks) and concatenated input vectors to take syntactic information such as the order of words into account. In addition, they arranged auto-encoders (AEs) to syntactic trees to represent the polarities of each phrase. Recursive neural networks construct sentence vectors differently from our approach. Compared to their model, our dist"
Y15-1018,N10-1120,0,0.653138,"lassification is the bag-of-words feature (Wang and Manning, 2012; Pang et al., 2002). In machine learningbased frameworks, the weights of words are automatically learned from a training corpus instead of being manually assigned. However, the bag-of-words feature cannot take syntactic structures into account. This leads to mistakes such as “a great design but inconvenient” and “inconvenient but a great design” being deemed to have the same meaning, even though their nuances are different; the former is somewhat negative whereas the latter is slightly positive. To solve this syntactic problem, Nakagawa et al. (2010) proposed a sentiment analysis model that used dependency trees with polarities assigned to their subtrees. However, their proposed model requires specialized knowledge to design complicated feature templates. In this study, we propose an approach that uses distributed word representation to overcome the first problem and deep neural networks to alleviate the second problem. The former is an unsupervised method capable of representing a word ’s meaning without using hand-tagged resources such as a polarity dictionary. In addition, it is robust to the data sparseness problem. The latter is a hi"
Y15-1018,W02-1011,0,0.0208478,"pproaches are fairly accurate, they depend on languages that may require significant amounts of manual labor. Further, dictionary-based methods have difficulty dealing with new or unknown words. Mamoru Komachi Graduate School of System Design Tokyo Metropolitan University komachi@tmu.ac.jp Machine learning-based methods are widely adopted in sentiment classification in order to mitigate the problems associated with the making of dictionaries and/or rules. One of the most basic features used in machine learning-based sentiment classification is the bag-of-words feature (Wang and Manning, 2012; Pang et al., 2002). In machine learningbased frameworks, the weights of words are automatically learned from a training corpus instead of being manually assigned. However, the bag-of-words feature cannot take syntactic structures into account. This leads to mistakes such as “a great design but inconvenient” and “inconvenient but a great design” being deemed to have the same meaning, even though their nuances are different; the former is somewhat negative whereas the latter is slightly positive. To solve this syntactic problem, Nakagawa et al. (2010) proposed a sentiment analysis model that used dependency trees"
Y15-1018,D14-1162,0,0.0829,"tors are not sparse but dense vectors that can express the meaning of words. Sentiment classification tasks are significantly influenced by the data sparseness problem. As a result, distributed word representation is more suitable than traditional 1-of-K representation, which only treats words as symbols. In our proposed method, to learn the word embeddings, we employ a state-of-the-art word embedding technique called word2vec (Mikolov et al., 2013b; Mikolov et al., 2013a), which we discuss in Section 3.1. Although several word embedding techniques currently exist (Collobert and Weston, 2008; Pennington et al., 2014), word2vec is one of the most computationally efficient and is considered to be state-of-the-art. Collobert et al. (2008) presented a model that learns word embedding by jointly performing multi-task learning using a deep convolutional architecture. Their method is considered to be state-of-the-art as well, but it is not readily applicable to Japanese. Multi-layer neural networks A stacked denoising auto-encoder (SdA) is a deep neural network that extends a stacked auto-encoder (Bengio et al., 2007) with denoising auto-encoders (dA). Stacking multiple layers and introducing noise to the input"
Y15-1018,D11-1014,0,0.259316,"Missing"
Y15-1018,D13-1170,0,0.0170799,"t to replay the tragedy),” which ends with “批判した (criticize).” The annotations of the above two examples were divided into both positive and negative7 . At the bottom, the proposed method did not successfully identify the polarity flipping with the phrase “圧力に屈せず (not yield to the pressure).” Because the model with negation handling 7 As explained in Section 4.2, we arbitrarily determined the polarity of a sentence when the annotations were split. 157 answered it correctly, there remains much room for improvement on how to deal with interactions between syntax and semantics (Tai et al., 2015; Socher et al., 2013). 6 Conclusion In this study, we presented a high performance Japanese sentiment classification method that uses distributed word representation learned from a largescale corpus with word2vec and a stacked denoising auto-encoder. The proposed method requires no dictionaries, complex models, or the engineering of numerous features. Consequently, it can easily be adapted to other tasks and domains without the need for advanced knowledge from experts. In addition, due to the nature of learning with vectors, our system does not depend on languages. As our future works, we will try to create the di"
Y15-1018,P15-1150,0,0.0232269,"劇の再演を防止 する (prevent to replay the tragedy),” which ends with “批判した (criticize).” The annotations of the above two examples were divided into both positive and negative7 . At the bottom, the proposed method did not successfully identify the polarity flipping with the phrase “圧力に屈せず (not yield to the pressure).” Because the model with negation handling 7 As explained in Section 4.2, we arbitrarily determined the polarity of a sentence when the annotations were split. 157 answered it correctly, there remains much room for improvement on how to deal with interactions between syntax and semantics (Tai et al., 2015; Socher et al., 2013). 6 Conclusion In this study, we presented a high performance Japanese sentiment classification method that uses distributed word representation learned from a largescale corpus with word2vec and a stacked denoising auto-encoder. The proposed method requires no dictionaries, complex models, or the engineering of numerous features. Consequently, it can easily be adapted to other tasks and domains without the need for advanced knowledge from experts. In addition, due to the nature of learning with vectors, our system does not depend on languages. As our future works, we wil"
Y15-1018,P14-1146,0,0.0288414,"tion ability to auto-encoders. This method is used in speech recognition (Dahl et al., 2011), image processing (Xie et al., 2012) and domain adaptation (Chen et al., 2012); further, it exhibits high representation ability. Glorot et al. (2011) used SdAs to perform domain adaptation in sentiment analysis. After learning sentiment classification in four domains of the reviews of products on Amazon, they tested each model with different domains. Although the task and method are similar to those of our proposed approach, they only use the most frequent verbs as input. Dos Santos et al. (2014) and Tang et al. (2014) researched sentiment classification of microblogs such as Twitter using the distributed representation learned by the methods of Collobert et al. (2008) and Mikolov et al. (2013b; 2013a). Those two tasks are the same task as ours, but the former generats sentence vectors using string-based convolution networks while the latter utilizes a model that treats the distributed word representation itself as polarities. Our proposed approach makes sentence vectors by simply averaging the distributed word representation, yet achieves state-of-the-art performance in Japanese sentiment classification ta"
Y15-1018,P12-2018,0,0.0314644,"rules. Although these approaches are fairly accurate, they depend on languages that may require significant amounts of manual labor. Further, dictionary-based methods have difficulty dealing with new or unknown words. Mamoru Komachi Graduate School of System Design Tokyo Metropolitan University komachi@tmu.ac.jp Machine learning-based methods are widely adopted in sentiment classification in order to mitigate the problems associated with the making of dictionaries and/or rules. One of the most basic features used in machine learning-based sentiment classification is the bag-of-words feature (Wang and Manning, 2012; Pang et al., 2002). In machine learningbased frameworks, the weights of words are automatically learned from a training corpus instead of being manually assigned. However, the bag-of-words feature cannot take syntactic structures into account. This leads to mistakes such as “a great design but inconvenient” and “inconvenient but a great design” being deemed to have the same meaning, even though their nuances are different; the former is somewhat negative whereas the latter is slightly positive. To solve this syntactic problem, Nakagawa et al. (2010) proposed a sentiment analysis model that u"
Y18-1033,P16-1039,0,0.041128,"ield accurate results by considering the sequence of words, whereas it is not robust if training data differ from test data (Neubig et al., 2011). Another popular approach employs pointwise prediction by using a local window (Neubig et al., 2011; Neubig and Mori, 2010). However, both approaches do not consider the global context because they use feature templates of a fixed length. Additionally, they both suffer from feature sparseness. Recently, deep neural network architectures have been widely used for CWS tasks (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014; Zhang et al., 2016; Cai and Zhao, 2016). These approaches are mainly divided into two types: structured prediction model (Zhang et al., 2016; Cai and Zhao, 2016) and pointwise prediction model (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014). However, a deep neural network approach requires high compuIn this paper, we presented an LSTM neural network approach to JWS. We proposed learning Japanesespecific features, such as character-type and character n-gram, as embeddings, and dictionary features as a sparse vector. The proposed method was shown to achieve comparable accuracy to state-of-the-art systems on various domains"
Y18-1033,P15-1168,0,0.135274,"Missing"
Y18-1033,D15-1141,0,0.056848,"Missing"
Y18-1033,W16-3918,0,0.0121731,", it yields a correct segmentation result. There is still room for improvement by using a dictionary to address the problem of spurious words. The upper bound of the proposed method is shown in Table 3. tational costs compared to previous approaches. In JWS, Morita et al. (2015) proposed integrating an RNN language model into JWS by interpolating it with traditional JWS. As opposed to using recurrent neural architecture as side information, word segmentation in Japanese is directly learned by using LSTM. Furthermore, a neural network approach for normalization was explored (Kann et al., 2016; Ikeda et al., 2016). Kann et al. (2016) proposed a characterbased encoder-decoder model and achieved stateof-the-art accuracy for the task of canonical morphological segmentation. Because their method was based on unsupervised learning, it could be learned at a low cost. However, it was necessary to adjust word segmentation criteria to human annotation. Ikeda et al. (2016) also presented an encoder-decoder model for Japanese text normalization. However, their model was only as good as conventional CRF, although it was trained with a largescale artificially created corpus. 7 6 Conclusion Related Works In JWS, a s"
Y18-1033,I13-1018,0,0.0177854,"corpus to classify the existence of word boundaries around a target character. In word segmentation, each character is assigned to several labels, such as {B, I, E, S}, {B, I, E}, and {B, I} to indicate the segmentation, where {B}, {I}, {E}, and {S} represents Begin, Inside, End, and Single, in that order. In JWS, the most prevalent label set corresponds to {B, I, E, S}, and the label sets do not significantly affect the accuracy of our preliminary experiments. Classification of these labels is performed by running the Viterbi algorithm over a word lattice (Kudo et al., 2004; Nakagawa, 2004; Kaji and Kitsuregawa, 2013) or by independently performing predictions (Neubig et al., 2011). However, previous approaches used feature templates to expand windowbased local features. Thus, they suffered data sparseness and a lack of global information in a sentence. An RNN, such as LSTM, addresses the problem of the lack of history by using recurrent hidden units, in which the output at each time depends on that of the previous time. This method has been successfully demonstrated with respect to several NLP tasks, such as language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011). Thus, we pr"
Y18-1033,D14-1011,0,0.0385826,"Missing"
Y18-1033,D16-1097,0,0.0624967,"Missing"
Y18-1033,W04-3230,0,0.879492,"uild a classifier from an annotated corpus to classify the existence of word boundaries around a target character. In word segmentation, each character is assigned to several labels, such as {B, I, E, S}, {B, I, E}, and {B, I} to indicate the segmentation, where {B}, {I}, {E}, and {S} represents Begin, Inside, End, and Single, in that order. In JWS, the most prevalent label set corresponds to {B, I, E, S}, and the label sets do not significantly affect the accuracy of our preliminary experiments. Classification of these labels is performed by running the Viterbi algorithm over a word lattice (Kudo et al., 2004; Nakagawa, 2004; Kaji and Kitsuregawa, 2013) or by independently performing predictions (Neubig et al., 2011). However, previous approaches used feature templates to expand windowbased local features. Thus, they suffered data sparseness and a lack of global information in a sentence. An RNN, such as LSTM, addresses the problem of the lack of history by using recurrent hidden units, in which the output at each time depends on that of the previous time. This method has been successfully demonstrated with respect to several NLP tasks, such as language modeling (Mikolov et al., 2010) and text gen"
Y18-1033,N16-1030,0,0.0708547,"se the dictionary is created from the training corpus, (dictsys ). Additional experiments using the gold dictionary created from the test corpus, (dictgold ), support this hypothesis6 . Our findings are similar to Zhang et al. (2018), who employed n-gram based feature templates and dictionaries for CWS. n-gram embeddings. A comparison of LSTM + ctype with LSTM + ctype + n-gram indicates ngram embeddings significantly improve the performance of the model by a large margin. There are several attempts to incorporate neural representations into a conditional random field (CRF) (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2017), all of which use bidirectional LSTM as encoders for sequence labeling tasks. In contrast, we apply simple n-gram embeddings, which can be easily obtained using a raw corpus. Our findings are in line with the rich pretraining method for neural CWS (Yang et al., 2017). 5 Error Analysis 5.1 Effect of Domain To determine the characteristics of the proposed method, we conducted an error analysis by comparing the proposed method with KyTea, with respect 6 The singletons of the combined corpus are removed while creating the gold dictionary. Thus the test corpus may still conta"
Y18-1033,D15-1176,0,0.0709648,"Missing"
Y18-1033,D15-1280,0,0.0141278,"and Chinese), which lack a trivial word segmentation process, can cause problems for downstream NLP applications. Thus, it is crucial to perform accurate word segmentation for the Japanese language. To achieve high accuracy, modern Japanese word segmentation (JWS) methods utilize discriminative models relying on extensive feature engineering. However, machine-learning-based methods tend to require hand-crafted feature templates. Thus, they suffer from data sparseness. Neural network models have, therefore, been investigated for various NLP tasks to address the problem of feature engineering (Liu et al., 2015; Sutskever et al., 2014; ∗ Now at Yahoo Japan Corporation. Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013). Neural network models enable the use of dense feature vectors (i.e., embeddings) that are learned via representation learning. Another important problem in JWS corresponds to context modeling. Traditional JWS methods employ feature templates to expand local features in a fixed window. However, global information beyond the window is not considered. Conversely, recurrent neural network (RNN) models grasp long distance information owing to the use of long short– term memor"
Y18-1033,P16-1101,0,0.0551454,"onary feature because the dictionary is created from the training corpus, (dictsys ). Additional experiments using the gold dictionary created from the test corpus, (dictgold ), support this hypothesis6 . Our findings are similar to Zhang et al. (2018), who employed n-gram based feature templates and dictionaries for CWS. n-gram embeddings. A comparison of LSTM + ctype with LSTM + ctype + n-gram indicates ngram embeddings significantly improve the performance of the model by a large margin. There are several attempts to incorporate neural representations into a conditional random field (CRF) (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2017), all of which use bidirectional LSTM as encoders for sequence labeling tasks. In contrast, we apply simple n-gram embeddings, which can be easily obtained using a raw corpus. Our findings are in line with the rich pretraining method for neural CWS (Yang et al., 2017). 5 Error Analysis 5.1 Effect of Domain To determine the characteristics of the proposed method, we conducted an error analysis by comparing the proposed method with KyTea, with respect 6 The singletons of the combined corpus are removed while creating the gold dictionary. Thus the test c"
Y18-1033,D15-1276,0,0.371285,"hus, window size 5 is selected. Dimension of character and character-type embeddings. The dimension of character embeddings is fixed by following (Chen et al., 2015b). In contrast, we search six configurations of charactertype embeddings: 1, 3, 5, 10, 20, and 50. We set the hidden units of character-type embeddings to 10 because of the preliminary experiments. Label set. In CWS, the label set {B, I, E, S} is often used. In contrast, various label sets are adopted in JWS. We explore three label sets and show that {B, I, E, S} is slightly better than the others. 3 http://www.phontron.com/kytea/ Morita et al. (2015) used a different segmentation standard than ours, thus it is not directly applicable to our dataset. 5 http://chainer.org 4 283 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 Table 2: The hyperparameter set of the study. hyperparameter value window size character embeddings character type embeddings hidden layer size label set learning rate coefficient of L2 regularization 5 100 10 150 {B, I, E, S} 0.1 0.0001 Learning rate. In this task, the learning rate largely affects accuracy. A small learning rate"
Y18-1033,P99-1036,0,0.275284,"December 2018 Copyright 2018 by the authors PACLIC 32 Figure 1: An overview of the proposed LSTM for JWS. 2.1.2 Character-Type Embeddings Character embeddings are extremely effective in identifying prefixes and postfixes. However, they can be too sparse when crossing a word boundary. To address this problem, it is helpful to exploit character types, such as hiragana, katakana, and kanji (e.g., ひらがな，カタカナ，漢字), for JWS (Neubig et al., 2011). For example, katakana sequences tends to correspond to a loan word. A transition from a character type to another will likely correspond to a word boundary (Nagata, 1999). 2.1.3 Character-Based n-gram Embeddings In addition to character type, the n-gram is effective in JWS (Neubig et al., 2011). Thus, the character-type sequence information is incorporated as embeddings. Each character is converted to a onehot vector corresponding to its character type. A one-hot vector comprises either hiragana, katakana, kanji, alphabet, number, symbol, start symbol, or terminal symbol. The advantages of a deep neural network include dealing with sparse vectors by converting them to dense vectors. This enables the utilization of a sparse feature, such as character trigram. A"
Y18-1033,C04-1067,0,0.0571349,"rom an annotated corpus to classify the existence of word boundaries around a target character. In word segmentation, each character is assigned to several labels, such as {B, I, E, S}, {B, I, E}, and {B, I} to indicate the segmentation, where {B}, {I}, {E}, and {S} represents Begin, Inside, End, and Single, in that order. In JWS, the most prevalent label set corresponds to {B, I, E, S}, and the label sets do not significantly affect the accuracy of our preliminary experiments. Classification of these labels is performed by running the Viterbi algorithm over a word lattice (Kudo et al., 2004; Nakagawa, 2004; Kaji and Kitsuregawa, 2013) or by independently performing predictions (Neubig et al., 2011). However, previous approaches used feature templates to expand windowbased local features. Thus, they suffered data sparseness and a lack of global information in a sentence. An RNN, such as LSTM, addresses the problem of the lack of history by using recurrent hidden units, in which the output at each time depends on that of the previous time. This method has been successfully demonstrated with respect to several NLP tasks, such as language modeling (Mikolov et al., 2010) and text generation (Sutskev"
Y18-1033,neubig-mori-2010-word,0,0.0321211,"as good as conventional CRF, although it was trained with a largescale artificially created corpus. 7 6 Conclusion Related Works In JWS, a supervised learning approach is widely used. A popular method in JWS involves creating a word lattice by using a dictionary and using Viterbi decoding (Kudo et al., 2004; Sassano, 2002). This approach is known to yield accurate results by considering the sequence of words, whereas it is not robust if training data differ from test data (Neubig et al., 2011). Another popular approach employs pointwise prediction by using a local window (Neubig et al., 2011; Neubig and Mori, 2010). However, both approaches do not consider the global context because they use feature templates of a fixed length. Additionally, they both suffer from feature sparseness. Recently, deep neural network architectures have been widely used for CWS tasks (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014; Zhang et al., 2016; Cai and Zhao, 2016). These approaches are mainly divided into two types: structured prediction model (Zhang et al., 2016; Cai and Zhao, 2016) and pointwise prediction model (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014). However, a deep neural network appro"
Y18-1033,P11-2093,0,0.0794014,"racter. In word segmentation, each character is assigned to several labels, such as {B, I, E, S}, {B, I, E}, and {B, I} to indicate the segmentation, where {B}, {I}, {E}, and {S} represents Begin, Inside, End, and Single, in that order. In JWS, the most prevalent label set corresponds to {B, I, E, S}, and the label sets do not significantly affect the accuracy of our preliminary experiments. Classification of these labels is performed by running the Viterbi algorithm over a word lattice (Kudo et al., 2004; Nakagawa, 2004; Kaji and Kitsuregawa, 2013) or by independently performing predictions (Neubig et al., 2011). However, previous approaches used feature templates to expand windowbased local features. Thus, they suffered data sparseness and a lack of global information in a sentence. An RNN, such as LSTM, addresses the problem of the lack of history by using recurrent hidden units, in which the output at each time depends on that of the previous time. This method has been successfully demonstrated with respect to several NLP tasks, such as language modeling (Mikolov et al., 2010) and text generation (Sutskever et al., 2011). Thus, we propose character-based embeddings and an LSTM network for JWS. Fig"
Y18-1033,P14-1028,0,0.049379,"no, 2002). This approach is known to yield accurate results by considering the sequence of words, whereas it is not robust if training data differ from test data (Neubig et al., 2011). Another popular approach employs pointwise prediction by using a local window (Neubig et al., 2011; Neubig and Mori, 2010). However, both approaches do not consider the global context because they use feature templates of a fixed length. Additionally, they both suffer from feature sparseness. Recently, deep neural network architectures have been widely used for CWS tasks (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014; Zhang et al., 2016; Cai and Zhao, 2016). These approaches are mainly divided into two types: structured prediction model (Zhang et al., 2016; Cai and Zhao, 2016) and pointwise prediction model (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014). However, a deep neural network approach requires high compuIn this paper, we presented an LSTM neural network approach to JWS. We proposed learning Japanesespecific features, such as character-type and character n-gram, as embeddings, and dictionary features as a sparse vector. The proposed method was shown to achieve comparable accuracy to st"
Y18-1033,P17-1161,0,0.0149511,"created from the training corpus, (dictsys ). Additional experiments using the gold dictionary created from the test corpus, (dictgold ), support this hypothesis6 . Our findings are similar to Zhang et al. (2018), who employed n-gram based feature templates and dictionaries for CWS. n-gram embeddings. A comparison of LSTM + ctype with LSTM + ctype + n-gram indicates ngram embeddings significantly improve the performance of the model by a large margin. There are several attempts to incorporate neural representations into a conditional random field (CRF) (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2017), all of which use bidirectional LSTM as encoders for sequence labeling tasks. In contrast, we apply simple n-gram embeddings, which can be easily obtained using a raw corpus. Our findings are in line with the rich pretraining method for neural CWS (Yang et al., 2017). 5 Error Analysis 5.1 Effect of Domain To determine the characteristics of the proposed method, we conducted an error analysis by comparing the proposed method with KyTea, with respect 6 The singletons of the combined corpus are removed while creating the gold dictionary. Thus the test corpus may still contain words that are not"
Y18-1033,C14-1167,0,0.0180724,"ise prediction model (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014). However, a deep neural network approach requires high compuIn this paper, we presented an LSTM neural network approach to JWS. We proposed learning Japanesespecific features, such as character-type and character n-gram, as embeddings, and dictionary features as a sparse vector. The proposed method was shown to achieve comparable accuracy to state-of-the-art systems on various domains. In JWS, it is important to deal with colloquial expressions that are frequently found in dialoguebased conversations and web text (Saito et al., 2014; Sasano et al., 2013; Kaji and Kitsuregawa, 2014). It is expected that deep neural architectures, such as convolutional neural networks, may be effective for this scenario because of their ability to learn robust representations of characters and words (Ling et al., 2015). Acknowledgments This work was partly supported by the Microsoft Research Collaborative Research (CORE) Projects. We thank anonymous reviewers for suggestions and comments, which helped in improving the paper. 286 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 201"
Y18-1033,I13-1019,0,0.0193497,"(Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014). However, a deep neural network approach requires high compuIn this paper, we presented an LSTM neural network approach to JWS. We proposed learning Japanesespecific features, such as character-type and character n-gram, as embeddings, and dictionary features as a sparse vector. The proposed method was shown to achieve comparable accuracy to state-of-the-art systems on various domains. In JWS, it is important to deal with colloquial expressions that are frequently found in dialoguebased conversations and web text (Saito et al., 2014; Sasano et al., 2013; Kaji and Kitsuregawa, 2014). It is expected that deep neural architectures, such as convolutional neural networks, may be effective for this scenario because of their ability to learn robust representations of characters and words (Ling et al., 2015). Acknowledgments This work was partly supported by the Microsoft Research Collaborative Research (CORE) Projects. We thank anonymous reviewers for suggestions and comments, which helped in improving the paper. 286 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACL"
Y18-1033,P02-1064,0,0.0310602,"heir method was based on unsupervised learning, it could be learned at a low cost. However, it was necessary to adjust word segmentation criteria to human annotation. Ikeda et al. (2016) also presented an encoder-decoder model for Japanese text normalization. However, their model was only as good as conventional CRF, although it was trained with a largescale artificially created corpus. 7 6 Conclusion Related Works In JWS, a supervised learning approach is widely used. A popular method in JWS involves creating a word lattice by using a dictionary and using Viterbi decoding (Kudo et al., 2004; Sassano, 2002). This approach is known to yield accurate results by considering the sequence of words, whereas it is not robust if training data differ from test data (Neubig et al., 2011). Another popular approach employs pointwise prediction by using a local window (Neubig et al., 2011; Neubig and Mori, 2010). However, both approaches do not consider the global context because they use feature templates of a fixed length. Additionally, they both suffer from feature sparseness. Recently, deep neural network architectures have been widely used for CWS tasks (Chen et al., 2015b; Chen et al., 2015a; Pei et al"
Y18-1033,Y18-1000,0,0.0912817,"Missing"
Y18-1033,D14-1101,0,0.0237415,"n determining the character sequences constituting a word. Thus, a Japanese morphological analyzer typically uses a dictionary. It is essential for a JWS using a word lattice during decoding to use word-level information such as a unigram and a bigram. However, this is not necessary for characterbased JWS approaches. 281 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 Figure 2: Example of a dictionary vector. Notably, it is not trivial to encode dictionary information into a neural network architecture. Tsuboi (2014) suggests that it is ineffective to learn both dense continuous and sparse discrete vector representations in the same layer. Thus, we follow the same practice to create a sparse dictionary vector. Whereas, instead of learning embeddings, this is used for the input to the final output layer, as shown in Figure 1. Figure 2 illustrates the creation of a dictionary vector, comprising three parts, as follows: left-side feature L, right-side feature R, and inside-feature I. For example, L2 is activated if a word with a length corresponding to 2 exists in the dictionary on the left side of the predi"
Y18-1033,P10-1040,0,0.0680555,"NLP applications. Thus, it is crucial to perform accurate word segmentation for the Japanese language. To achieve high accuracy, modern Japanese word segmentation (JWS) methods utilize discriminative models relying on extensive feature engineering. However, machine-learning-based methods tend to require hand-crafted feature templates. Thus, they suffer from data sparseness. Neural network models have, therefore, been investigated for various NLP tasks to address the problem of feature engineering (Liu et al., 2015; Sutskever et al., 2014; ∗ Now at Yahoo Japan Corporation. Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013). Neural network models enable the use of dense feature vectors (i.e., embeddings) that are learned via representation learning. Another important problem in JWS corresponds to context modeling. Traditional JWS methods employ feature templates to expand local features in a fixed window. However, global information beyond the window is not considered. Conversely, recurrent neural network (RNN) models grasp long distance information owing to the use of long short– term memory (LSTM), achieving state-of-the-art accuracy in Chinese word segmentation (Chen et al., 2015b). How"
Y18-1033,W01-0512,0,0.152139,"t to two different datasets: a popular Japanese corpus, the Balanced Corpus of Contemporary Written Japanese (BCCWJ) version 1.1 (Maekawa et al., 2014); and another widely used Japanese corpus, the Kyoto University Corpus (KC), version 4.0. The BCCWJ is composed of various domains, whereas KC only includes the newswire domain. The details of the corpora are shown in Table 1. The train and test split of BCCWJ follow, per the Project Next NLP2 . We used a short unit word as the segmentation standard, and we adopted the same train and test split of KC used in previous studies (Kudo et al., 2004; Uchimoto et al., 2001). With respect to word-level features, Neubig et al. (2011) do not use any external dictionary, except the dictionary created from the training corpus. Hence, the same scenario is adopted, and all the words in the training corpus are added. However, words appearing only once in a corpus are omitted to prevent overfitting of the training data, as described in (Neubig et al., 2011). To analyze the effect of the dictionary feature, we recreate a larger dictionary created from both training and test sets. This is termed as “gold dict” in Table 3. 2 https://goo.gl/QCxxwB Tools. In the experiments,"
Y18-1033,D16-1157,0,0.0216696,"JWS (Neubig et al., 2011). Thus, the character-type sequence information is incorporated as embeddings. Each character is converted to a onehot vector corresponding to its character type. A one-hot vector comprises either hiragana, katakana, kanji, alphabet, number, symbol, start symbol, or terminal symbol. The advantages of a deep neural network include dealing with sparse vectors by converting them to dense vectors. This enables the utilization of a sparse feature, such as character trigram. Additionally, a character-based n-gram is effective for sentence similarity, part-of-speech tagging (Wieting et al., 2016), and for Japanese morphological analysis (Neubig et al., 2011). Therefore, n-gram is used for character and character-type embeddings. More precisely, a one-hot vector is created for each unigram, bigram, and trigram. Each embedding is selected by a lookup table as well as unigram embeddings. The embedding vectors lt and et are defined as follows: lt = l[t−2:t] ⊕ l[t−1:t] ⊕ l[t] (4) et = e[t−2:t] ⊕ e[t−1:t] ⊕ e[t] (5) where l[a:b] denotes the embedding for the strings from a to b. The same holds for et . 2.2 Incorporating Word Dictionary Character embeddings, character-type embeddings, and th"
Y18-1033,P17-1078,0,0.103856,"ries for CWS. n-gram embeddings. A comparison of LSTM + ctype with LSTM + ctype + n-gram indicates ngram embeddings significantly improve the performance of the model by a large margin. There are several attempts to incorporate neural representations into a conditional random field (CRF) (Ma and Hovy, 2016; Lample et al., 2016; Peters et al., 2017), all of which use bidirectional LSTM as encoders for sequence labeling tasks. In contrast, we apply simple n-gram embeddings, which can be easily obtained using a raw corpus. Our findings are in line with the rich pretraining method for neural CWS (Yang et al., 2017). 5 Error Analysis 5.1 Effect of Domain To determine the characteristics of the proposed method, we conducted an error analysis by comparing the proposed method with KyTea, with respect 6 The singletons of the combined corpus are removed while creating the gold dictionary. Thus the test corpus may still contain words that are not in the gold dictionary. 284 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 Table 4: Token-level and sentence-level performance on various domains in the BCCWJ dataset. Domain F"
Y18-1033,P16-1040,0,0.03953,"proach is known to yield accurate results by considering the sequence of words, whereas it is not robust if training data differ from test data (Neubig et al., 2011). Another popular approach employs pointwise prediction by using a local window (Neubig et al., 2011; Neubig and Mori, 2010). However, both approaches do not consider the global context because they use feature templates of a fixed length. Additionally, they both suffer from feature sparseness. Recently, deep neural network architectures have been widely used for CWS tasks (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014; Zhang et al., 2016; Cai and Zhao, 2016). These approaches are mainly divided into two types: structured prediction model (Zhang et al., 2016; Cai and Zhao, 2016) and pointwise prediction model (Chen et al., 2015b; Chen et al., 2015a; Pei et al., 2014). However, a deep neural network approach requires high compuIn this paper, we presented an LSTM neural network approach to JWS. We proposed learning Japanesespecific features, such as character-type and character n-gram, as embeddings, and dictionary features as a sparse vector. The proposed method was shown to achieve comparable accuracy to state-of-the-art syste"
Y18-1035,Y18-1000,0,0.246487,"Missing"
Y18-1035,P15-1001,0,0.0215862,"active sentence summarization, Rush et al. (2015) proposed a new summarization method to generate an abstractive summary using a sequence-to-sequence model; in particular, they achieved state-of-the-art performance on the DUC2004 and Gigaword corpora. In contrast, for abstractive text summarization using the CNN / Daily Mail datasets, the objective is to output a summary of an article consisting of multisentences. Nallapati et al. (2016) proposed an improved summarization model for this task, which is essentially an attention encoder-decoder model, including a trick to use a large vocabulary (Jean et al., 2015), switching pointer-generator mechanism, and hierarchical networks. They proposed a new dataset for multi-sentence summarization and established a benchmark using this dataset. However, these works did not address the problem of information structure in a generated summary. Therefore, in our work, we attempt to consider the information structure of a summary to further improve the summarization model. 2.2 Attention encoder decoder. Let the input sequences be tokens of the article wi and the output sequences be tokens of the summary yi . A bidirectional long short-term memory (LSTM) network is"
Y18-1035,P13-1099,0,0.0173355,"es. Our experimental results show that the information structure of a summary can be controlled, thus improving the performance of the overall summarization. 1 Introduction Summarization can be achieved using two approaches, namely the extractive and abstractive approaches. The extractive approach involves selecting some part of a document (i.e., sentence, phrase, or word) to construct a summary; in contrast, the abstractive approach involves generating a document summary with words that are not necessarily present in the document itself. Thus, the extractive approach (Nallapati et al., 2017; Li et al., 2013) is considered to yield a Mamoru Komachi komachi@tmu.ac.jp Graduate School of System Design Tokyo Metropolitan University more grammatical summary than the abstractive approach, because the former involves directly extracting output expressions from the source text. However, as is obvious, because words that are not present in the source text cannot be selected in the case of the extractive approach, abstractive approaches are becoming increasingly popular for automatic summarization tasks. In previous work, Rush et al. (2015) proposed an abstractive sentence summarization method that involves"
Y18-1035,W04-1013,0,0.0269109,"Missing"
Y18-1035,K16-1028,0,0.363427,"y extracting output expressions from the source text. However, as is obvious, because words that are not present in the source text cannot be selected in the case of the extractive approach, abstractive approaches are becoming increasingly popular for automatic summarization tasks. In previous work, Rush et al. (2015) proposed an abstractive sentence summarization method that involves generating novel words in a summary based on the sequence-to-sequence model proposed by Sutskever et al. (2014). Furthermore, recently, improvements to the abstractive text summarization method were proposed by (Nallapati et al., 2016; See et al., 2017). Although their proposed model generates fluent summaries owing to the use of a large-scale dataset, it cannot produce a structured summarization, because it is trained using the CNN / Daily Mail datasets, which are not annotated with any structural information. Therefore, in this work, we focus on generating a structured summary for a document, in particular, a summary in three sentences. Because the CNN / Daily Mail datasets include summaries with a varying number of sentences, they cannot be annotated with information structures directly. Considering this, we employ a Ja"
Y18-1035,D15-1044,0,0.290166,"cument itself. Thus, the extractive approach (Nallapati et al., 2017; Li et al., 2013) is considered to yield a Mamoru Komachi komachi@tmu.ac.jp Graduate School of System Design Tokyo Metropolitan University more grammatical summary than the abstractive approach, because the former involves directly extracting output expressions from the source text. However, as is obvious, because words that are not present in the source text cannot be selected in the case of the extractive approach, abstractive approaches are becoming increasingly popular for automatic summarization tasks. In previous work, Rush et al. (2015) proposed an abstractive sentence summarization method that involves generating novel words in a summary based on the sequence-to-sequence model proposed by Sutskever et al. (2014). Furthermore, recently, improvements to the abstractive text summarization method were proposed by (Nallapati et al., 2016; See et al., 2017). Although their proposed model generates fluent summaries owing to the use of a large-scale dataset, it cannot produce a structured summarization, because it is trained using the CNN / Daily Mail datasets, which are not annotated with any structural information. Therefore, in"
Y18-1035,P17-1099,0,0.399764,"essions from the source text. However, as is obvious, because words that are not present in the source text cannot be selected in the case of the extractive approach, abstractive approaches are becoming increasingly popular for automatic summarization tasks. In previous work, Rush et al. (2015) proposed an abstractive sentence summarization method that involves generating novel words in a summary based on the sequence-to-sequence model proposed by Sutskever et al. (2014). Furthermore, recently, improvements to the abstractive text summarization method were proposed by (Nallapati et al., 2016; See et al., 2017). Although their proposed model generates fluent summaries owing to the use of a large-scale dataset, it cannot produce a structured summarization, because it is trained using the CNN / Daily Mail datasets, which are not annotated with any structural information. Therefore, in this work, we focus on generating a structured summary for a document, in particular, a summary in three sentences. Because the CNN / Daily Mail datasets include summaries with a varying number of sentences, they cannot be annotated with information structures directly. Considering this, we employ a Japanese summarizatio"
Y18-1035,P16-1008,0,0.0181728,"ect a word from the vocabulary distribution Pvocab or a word from the attention distribution at . For each document, the authors developed an extended vocabulary, which is the union of the vocabulary and all words in the source document. The probability of the extended vocabulary is calculated as follows: X P (w) = pgen Pvocab (w) + (1 − pgen ) ati (8) i:wi =w If w is an out-of-vocabulary (OOV) word, Pvocab (w) is 0; inP addition, if w does not exist in the source words, i:wi =w ati is 0. Coverage mechanism. Aside from the abovementioned changes, See et al. (2017) improved the coverage model (Tu et al., 2016) to address the repetition problem. In their model, a coverage vector ct , which is the sum of attention distributions at all previous decoder timesteps, is saved: t c = t−1 X where vector wc is a learnable parameter. This prevents the attention mechanism from repeatedly visiting the same location in the document, and thus avoids generating repetitive text. In addition, they constructed a new loss function to incorporate the coverage loss to penalize repeated attention to the same location. X losst = − log P (wt∗ ) + λ min(ati , cti ) (11) i 3 Annotation of the Summary Structure 3.1 We crawled"
Y18-1054,P16-1078,0,0.0187063,"a RvNN, as mentioned in Section 3.1. Thanks to the application of LSTM, this model can learn increased numbers of parameters appropriately, unlike the other models. 2.3 Attentional Models Based on psychological studies, the human ability of intuition of attention (Rensink, 2000) has been introduced into many computer science fields. The main function of this ability is deciding which part of an input needs to be focused on. In natural language processing (NLP), the attention mechanism is utilized for many tasks, including neural machine translation (Bahdanau et al., 2015; Luong et al., 2015; Eriguchi et al., 2016), neural summarization (Hermann et al., 2015; Rush et al., 2015; Vinyals et al., 2015), representation learning (Ling et al., 2015), and image captioning (Xu et al., 2015). These approaches incorporate consideration as to how each source word or region contributes to the generation of a target word. Unlike most of the above-mentioned NLP tasks for generating word sequences, our model derives attention information for sentiment classification. Eriguchi et al. (2016) proposed an attentional model focusing on phrase structure; our model omits the word-level recurrent LSTM layer from their model."
Y18-1054,P14-1062,0,0.0153466,"es representing the polarities of the respective subtrees. In this manner, the model can handle polarity-shifting operations such as negation. However, this method suffers from feature sparseness because almost all features are combination features. ∗ Now at Yahoo Japan Corporation. Mamoru Komachi Graduate School of System Design Tokyo Metropolitan University komachi@tmu.ac.jp To overcome the data sparseness problem, deepneural-network-based methods have attracted much attention because of their ability to use dense feature representations (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhang and Komachi, 2015). In particular, tree-structured approaches called recursive neural networks (RvNNs) have been shown to perform well in sentiment classification tasks (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Tai et al., 2015). Whereas Tree-CRF employs sparse and binary feature representations, RvNNs avoid feature sparseness by learning dense and continuous feature representations. However, annotation for each phrase is crucial for learning RvNN models, but there is no phrase-level annotated corpus in any language other than English. We therefore propose"
Y18-1054,D14-1181,0,0.312494,"den variables representing the polarities of the respective subtrees. In this manner, the model can handle polarity-shifting operations such as negation. However, this method suffers from feature sparseness because almost all features are combination features. ∗ Now at Yahoo Japan Corporation. Mamoru Komachi Graduate School of System Design Tokyo Metropolitan University komachi@tmu.ac.jp To overcome the data sparseness problem, deepneural-network-based methods have attracted much attention because of their ability to use dense feature representations (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhang and Komachi, 2015). In particular, tree-structured approaches called recursive neural networks (RvNNs) have been shown to perform well in sentiment classification tasks (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Tai et al., 2015). Whereas Tree-CRF employs sparse and binary feature representations, RvNNs avoid feature sparseness by learning dense and continuous feature representations. However, annotation for each phrase is crucial for learning RvNN models, but there is no phrase-level annotated corpus in any language other than En"
Y18-1054,E17-2093,0,0.376735,"sentiment classification tasks (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Tai et al., 2015). Whereas Tree-CRF employs sparse and binary feature representations, RvNNs avoid feature sparseness by learning dense and continuous feature representations. However, annotation for each phrase is crucial for learning RvNN models, but there is no phrase-level annotated corpus in any language other than English. We therefore propose an RvNN model with an attention mechanism and augment the training example with polar dictionaries to compensate for the lack of phrase-level annotation. Although Kokkinos and Potamianos (2017) also provided an attention mechanism for phrase-level annotated corpus, our model performs well on a sentence-level annotated corpus through the introduction of polar dictionaries. The main contributions of this work are as follows. • We show that RvNN models can be learned from a sentence-level polarity-tagged Japanese corpus using an attention mechanism and polar dictionaries. • We achieve the state-of-the-art performance in a Japanese sentiment classification task. 466 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the a"
Y18-1054,D15-1161,0,0.0293819,"tely, unlike the other models. 2.3 Attentional Models Based on psychological studies, the human ability of intuition of attention (Rensink, 2000) has been introduced into many computer science fields. The main function of this ability is deciding which part of an input needs to be focused on. In natural language processing (NLP), the attention mechanism is utilized for many tasks, including neural machine translation (Bahdanau et al., 2015; Luong et al., 2015; Eriguchi et al., 2016), neural summarization (Hermann et al., 2015; Rush et al., 2015; Vinyals et al., 2015), representation learning (Ling et al., 2015), and image captioning (Xu et al., 2015). These approaches incorporate consideration as to how each source word or region contributes to the generation of a target word. Unlike most of the above-mentioned NLP tasks for generating word sequences, our model derives attention information for sentiment classification. Eriguchi et al. (2016) proposed an attentional model focusing on phrase structure; our model omits the word-level recurrent LSTM layer from their model. Yang et al. (2016) presented a hierarchical attention network for document classification, incorporating sentenceand word-level att"
Y18-1054,D15-1166,0,0.128633,"Missing"
Y18-1054,N13-1090,0,0.00523543,"n an English corpus where phrase-level annotation is available, but without using phrase-level annotation, to see its effect. 2 If the dataset contains only sentence-level annotation, m is equal to the size of the dataset. 469 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 (a) RvNN (b) RvNN with Attention Figure 1: Sentiment classification by Tree-LSTM with attention. 4.1 Data Word embeddings. For Japanese experiments, we obtained pre-trained word representations from word2vec3 using a skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). We learned word representations on Japanese Wikipedia’s dump data (2014.11) segmented by KyTea (version-0.4.7) (Neubig et al., 2011). We used pre-trained GloVe word representations4 for the English experiments. We fine-tuned both word representations in our experiments. Parse trees. For Japanese constituency parsing, we used Ckylark (Oda et al., 2015) as of 2016.075 with KyTea for word segmentation. For English, we used the automatic syntactic annotation of the Stanford Sentiment Treebank. Dictionaries. We followed Nakagawa et al. (2010) to cre"
Y18-1054,P09-1113,0,0.0434979,"l-polarity-tagged corpus is difficult to obtain in many languages, polar dictionaries are easy to compile (semi-)automatically. Therefore, we opt for the use of polar dictionaries as an alternative source of sentiment information. We utilize the same polar dictionaries for short phrases and words as used in Nakagawa et al. (2010). The phrase in the training sets that matches an entry in the polar dictionaries is annotated with the corresponding polarity. The key difference from Nakagawa et al. (2010) is that we use polar dictionaries as a hard label in a manner similar to distant supervision (Mintz et al., 2009). In contrast, the previous work used polar dictionaries as a soft label # ! &quot; for an initial hidden variable in Tree-CRF. Teng et aj (a) (s0 ) +b , (12) al. (2016) also incorporated sentiment lexicons into pˆθ (y|hj ) = softmax W hj an recurrent neural network model. Their method X aj = aji hi , (13) predicts weights for each sentiment score of subjeci tive words to predict a sentence label. Our method g(hi , hj ) aji = P , (14) uses polar dictionaries only during the training step, 0 g(h , h ) whereas the method by Teng et al. (2016) needs poj i i0 &quot; #!! lar dictionaries for both training an"
Y18-1054,N10-1120,0,0.295264,"ism that pays attention to each subtree of a parse tree. Experimental results indicate that our model achieves state-ofthe-art performance for a Japanese sentiment classification task. 1 Introduction Traditional approaches for sentiment classification rely on simple lexical features, such as a bag-ofwords, that are ineffective for many sentiment classification tasks (Pang et al., 2002). For example, the sentence “Insecticides kill pests.” contains both kill and pests, indicating negative polarity. But, the overall expression is still deemed positive. To address this problem of polarity shift, Nakagawa et al. (2010) presented a dependency-treebased approach for the sentiment classification of a sentence. Their method assigns sentiment polarity to each subtree as a hidden variable that is not observable in the training data. The polarity of the overall sentence is then classified by a tree-conditional random field (Tree-CRF), marginalizing over the hidden variables representing the polarities of the respective subtrees. In this manner, the model can handle polarity-shifting operations such as negation. However, this method suffers from feature sparseness because almost all features are combination feature"
Y18-1054,P11-2093,0,0.0365565,", m is equal to the size of the dataset. 469 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 (a) RvNN (b) RvNN with Attention Figure 1: Sentiment classification by Tree-LSTM with attention. 4.1 Data Word embeddings. For Japanese experiments, we obtained pre-trained word representations from word2vec3 using a skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). We learned word representations on Japanese Wikipedia’s dump data (2014.11) segmented by KyTea (version-0.4.7) (Neubig et al., 2011). We used pre-trained GloVe word representations4 for the English experiments. We fine-tuned both word representations in our experiments. Parse trees. For Japanese constituency parsing, we used Ckylark (Oda et al., 2015) as of 2016.075 with KyTea for word segmentation. For English, we used the automatic syntactic annotation of the Stanford Sentiment Treebank. Dictionaries. We followed Nakagawa et al. (2010) to create polar dictionaries. We employed a Japanese polar dictionary composed by Kobayashi et al. (2005) and Higashiyama et al. (2008)6 that contains 5,447 positive and 8,117 negative exp"
Y18-1054,N15-3009,0,0.0147179,"1: Sentiment classification by Tree-LSTM with attention. 4.1 Data Word embeddings. For Japanese experiments, we obtained pre-trained word representations from word2vec3 using a skip-gram model (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c). We learned word representations on Japanese Wikipedia’s dump data (2014.11) segmented by KyTea (version-0.4.7) (Neubig et al., 2011). We used pre-trained GloVe word representations4 for the English experiments. We fine-tuned both word representations in our experiments. Parse trees. For Japanese constituency parsing, we used Ckylark (Oda et al., 2015) as of 2016.075 with KyTea for word segmentation. For English, we used the automatic syntactic annotation of the Stanford Sentiment Treebank. Dictionaries. We followed Nakagawa et al. (2010) to create polar dictionaries. We employed a Japanese polar dictionary composed by Kobayashi et al. (2005) and Higashiyama et al. (2008)6 that contains 5,447 positive and 8,117 negative expressions.7 We created an English polar lexicon from 3 See https://code.google.com/archive/p/ word2vec/ 4 See http://nlp.stanford.edu/data/glove. 6B.zip 5 See https://github.com/odashi/ckylark 6 See http://www.cl.ecei.toho"
Y18-1054,W02-1011,0,0.0371799,"timent classification models required phrase-level annotated corpora, which are not readily available in many languages other than English. Thus, we propose the use of treestructured Long Short-Term Memory with an attention mechanism that pays attention to each subtree of a parse tree. Experimental results indicate that our model achieves state-ofthe-art performance for a Japanese sentiment classification task. 1 Introduction Traditional approaches for sentiment classification rely on simple lexical features, such as a bag-ofwords, that are ineffective for many sentiment classification tasks (Pang et al., 2002). For example, the sentence “Insecticides kill pests.” contains both kill and pests, indicating negative polarity. But, the overall expression is still deemed positive. To address this problem of polarity shift, Nakagawa et al. (2010) presented a dependency-treebased approach for the sentiment classification of a sentence. Their method assigns sentiment polarity to each subtree as a hidden variable that is not observable in the training data. The polarity of the overall sentence is then classified by a tree-conditional random field (Tree-CRF), marginalizing over the hidden variables representi"
Y18-1054,P15-1132,0,0.0685234,"d attempts to learn polarity shifting via RvNNs. Zhang and Komachi (2015) adopted a stacked denoising auto-encoder. Their model treats an input sentence as an average vector of their word vectors, which is then fed into a stacked denoising autoencoder. Although this model omits syntactic information, it achieves high performance through its generalization ability. However, it is not straightforward to employ polar dictionaries in their model. 2.2 Recursive Neural Networks There are various RvNN models for sentiment classification (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Qian et al., 2015; Tai et al., 2015; Zhu and Sobhani, 2015). All of these models attempt to capture sentence representation in a bottom-up fashion in accordance with a parse tree. 1 See https://github.com/tmu-nlp/ AttnTreeLSTM4SentimentClassification In this way, sentence representation can be calculated by learning compositional functions for each phrase. Several studies have focused on using a compositional function to improve compositionality (Socher et al., 2012; Socher et al., 2013; Qian et al., 2015; Tai et al., 2015). Socher et al. (2012) parameterized each word as a matrix–vector combination to denote"
Y18-1054,D15-1044,0,0.044515,"STM, this model can learn increased numbers of parameters appropriately, unlike the other models. 2.3 Attentional Models Based on psychological studies, the human ability of intuition of attention (Rensink, 2000) has been introduced into many computer science fields. The main function of this ability is deciding which part of an input needs to be focused on. In natural language processing (NLP), the attention mechanism is utilized for many tasks, including neural machine translation (Bahdanau et al., 2015; Luong et al., 2015; Eriguchi et al., 2016), neural summarization (Hermann et al., 2015; Rush et al., 2015; Vinyals et al., 2015), representation learning (Ling et al., 2015), and image captioning (Xu et al., 2015). These approaches incorporate consideration as to how each source word or region contributes to the generation of a target word. Unlike most of the above-mentioned NLP tasks for generating word sequences, our model derives attention information for sentiment classification. Eriguchi et al. (2016) proposed an attentional model focusing on phrase structure; our model omits the word-level recurrent LSTM layer from their model. Yang et al. (2016) presented a hierarchical attention network f"
Y18-1054,D11-1014,0,0.189827,"Missing"
Y18-1054,D12-1110,0,0.411024,"Our model uses only a polar dictionary and attempts to learn polarity shifting via RvNNs. Zhang and Komachi (2015) adopted a stacked denoising auto-encoder. Their model treats an input sentence as an average vector of their word vectors, which is then fed into a stacked denoising autoencoder. Although this model omits syntactic information, it achieves high performance through its generalization ability. However, it is not straightforward to employ polar dictionaries in their model. 2.2 Recursive Neural Networks There are various RvNN models for sentiment classification (Socher et al., 2011; Socher et al., 2012; Socher et al., 2013; Qian et al., 2015; Tai et al., 2015; Zhu and Sobhani, 2015). All of these models attempt to capture sentence representation in a bottom-up fashion in accordance with a parse tree. 1 See https://github.com/tmu-nlp/ AttnTreeLSTM4SentimentClassification In this way, sentence representation can be calculated by learning compositional functions for each phrase. Several studies have focused on using a compositional function to improve compositionality (Socher et al., 2012; Socher et al., 2013; Qian et al., 2015; Tai et al., 2015). Socher et al. (2012) parameterized each word a"
Y18-1054,P15-1150,0,0.165138,"ies of the respective subtrees. In this manner, the model can handle polarity-shifting operations such as negation. However, this method suffers from feature sparseness because almost all features are combination features. ∗ Now at Yahoo Japan Corporation. Mamoru Komachi Graduate School of System Design Tokyo Metropolitan University komachi@tmu.ac.jp To overcome the data sparseness problem, deepneural-network-based methods have attracted much attention because of their ability to use dense feature representations (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhang and Komachi, 2015). In particular, tree-structured approaches called recursive neural networks (RvNNs) have been shown to perform well in sentiment classification tasks (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Tai et al., 2015). Whereas Tree-CRF employs sparse and binary feature representations, RvNNs avoid feature sparseness by learning dense and continuous feature representations. However, annotation for each phrase is crucial for learning RvNN models, but there is no phrase-level annotated corpus in any language other than English. We therefore propose an RvNN model wit"
Y18-1054,D16-1169,0,0.0249529,"Missing"
Y18-1054,H05-1044,0,0.310097,") to create polar dictionaries. We employed a Japanese polar dictionary composed by Kobayashi et al. (2005) and Higashiyama et al. (2008)6 that contains 5,447 positive and 8,117 negative expressions.7 We created an English polar lexicon from 3 See https://code.google.com/archive/p/ word2vec/ 4 See http://nlp.stanford.edu/data/glove. 6B.zip 5 See https://github.com/odashi/ckylark 6 See http://www.cl.ecei.tohoku. ac.jp/index.php?OpenResources/ JapaneseSentimentPolarityDictionary 7 Note that these figures are slightly different from Nakagawa et al. (2010). We suspect that the reason why they can Wilson et al. (2005) in the same way as Nakagawa et al. (2010). The dictionary contains 2,289 positive and 4,143 negative expressions. Corpora. We used the NTCIR Japanese opinion corpus (NTCIR-J), which includes 997 positive and 2,400 negative sentences (Seki et al., 2007; Seki et al., 2008). We removed neutral sentences following previous studies. The corpus comprised two NTCIR Japanese opinion corpora, the NTCIR-6 corpus and the NTCIR-7 corpus, as in (Nakagawa et al., 2010). We performed 10-fold cross-validation by randomly splitting each corpus into 10 parts (one for testing, one for development, and the remai"
Y18-1054,N16-1174,0,0.0535812,", neural summarization (Hermann et al., 2015; Rush et al., 2015; Vinyals et al., 2015), representation learning (Ling et al., 2015), and image captioning (Xu et al., 2015). These approaches incorporate consideration as to how each source word or region contributes to the generation of a target word. Unlike most of the above-mentioned NLP tasks for generating word sequences, our model derives attention information for sentiment classification. Eriguchi et al. (2016) proposed an attentional model focusing on phrase structure; our model omits the word-level recurrent LSTM layer from their model. Yang et al. (2016) presented a hierarchical attention network for document classification, incorporating sentenceand word-level attention mechanisms. Our task ad467 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 dresses sentence-level sentiment classification, so that our model employs a phrase-level (constituent) attention network rather than sentence-level information in a document. Probably the most related work to our study is (Kokkinos and Potamianos, 2017) and (Zou et al., 2018). Kokkinos and Potamianos (2017) buil"
Y18-1054,Y15-1018,1,0.913801,"ive subtrees. In this manner, the model can handle polarity-shifting operations such as negation. However, this method suffers from feature sparseness because almost all features are combination features. ∗ Now at Yahoo Japan Corporation. Mamoru Komachi Graduate School of System Design Tokyo Metropolitan University komachi@tmu.ac.jp To overcome the data sparseness problem, deepneural-network-based methods have attracted much attention because of their ability to use dense feature representations (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhang and Komachi, 2015). In particular, tree-structured approaches called recursive neural networks (RvNNs) have been shown to perform well in sentiment classification tasks (Socher et al., 2011; Socher et al., 2013; Kim, 2014; Tai et al., 2015). Whereas Tree-CRF employs sparse and binary feature representations, RvNNs avoid feature sparseness by learning dense and continuous feature representations. However, annotation for each phrase is crucial for learning RvNN models, but there is no phrase-level annotated corpus in any language other than English. We therefore propose an RvNN model with an attention mechanism a"
Y18-1054,C18-1074,0,0.0253679,"urrent LSTM layer from their model. Yang et al. (2016) presented a hierarchical attention network for document classification, incorporating sentenceand word-level attention mechanisms. Our task ad467 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 dresses sentence-level sentiment classification, so that our model employs a phrase-level (constituent) attention network rather than sentence-level information in a document. Probably the most related work to our study is (Kokkinos and Potamianos, 2017) and (Zou et al., 2018). Kokkinos and Potamianos (2017) built a similar model to ours, but they did not use the hidden state of a RvNN in their final softmax, and they also did not emphasize the use of polar dictionaries. Zou et al. (2018) proposed a lexicon-based supervised attention model to take advantage of a sentiment lexicon. They injected type-level lexical information into an additional attention network, whereas we injected token-level lexical information into a single RvNN model as a phrase-level annotation. 3 3.1 Tai et al. (2015) addressed this problem by introducing LSTM (Hochreiter and Schmidhuber, 199"
Y18-3008,D18-1549,0,0.0435497,"Missing"
Y18-3008,Q17-1010,0,0.233572,"gnificant amounts of monolingual data. Lample et al. (2018) have proposed an unsupervised NMT model that is effective on similar language pairs, such as English–French and English–German. In this work, Chinese–Japanese language pair is used because they also share a lot of characters which can be used to replace the need for bilingual dictionaries. New sub-character datasets were also created to enhance the shared information. The byte-pair encodings (BPE) (Sennrich et al., 2016c) vocabularies were shared between the two related languages by jointly trained both monolingual corpora. FastText (Bojanowski et al., 2017) was then used to generate cross-lingual embeddings. Following this, two encoder–decoder language models were trained on noisy data on either monolingual corpora, respectively. For the translation models, back-translation (Sennrich et al., 2016b) was used to handle both direc981 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 tions in tandem, from source to target and from target to source (the former generates data to train the later and vice versa). The"
Y18-3008,D17-1263,0,0.0125475,"uction Neural machine translation (NMT) (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) systems have achieved great success in recent years and outperform traditional statistical machine translation (SMT) (Sennrich et al., 1 Our team ID for the submission to this shared task (Nakazawa et al., 2018) is TMU. 2016a; Wu et al., 2016; Zhou et al., 2016) systems. Nevertheless, one of its major challenges has been that it is necessary for NMT models to be trained using large parallel data, meaning that they can fail when the training data is not big enough (Koehn and Knowles, 2017; Isabelle et al., 2017). Unfortunately, the lack of large parallel corpora is a practical problem for the vast majority of language pairs, and these are often non-existent for low-resource languages. On the other hand, monolingual data is much easier to find; many languages with limited parallel data still possess significant amounts of monolingual data. Lample et al. (2018) have proposed an unsupervised NMT model that is effective on similar language pairs, such as English–French and English–German. In this work, Chinese–Japanese language pair is used because they also share a lot of characters which can be used to"
Y18-3008,W17-3204,0,0.0194331,"stroke data). 1 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) systems have achieved great success in recent years and outperform traditional statistical machine translation (SMT) (Sennrich et al., 1 Our team ID for the submission to this shared task (Nakazawa et al., 2018) is TMU. 2016a; Wu et al., 2016; Zhou et al., 2016) systems. Nevertheless, one of its major challenges has been that it is necessary for NMT models to be trained using large parallel data, meaning that they can fail when the training data is not big enough (Koehn and Knowles, 2017; Isabelle et al., 2017). Unfortunately, the lack of large parallel corpora is a practical problem for the vast majority of language pairs, and these are often non-existent for low-resource languages. On the other hand, monolingual data is much easier to find; many languages with limited parallel data still possess significant amounts of monolingual data. Lample et al. (2018) have proposed an unsupervised NMT model that is effective on similar language pairs, such as English–French and English–German. In this work, Chinese–Japanese language pair is used because they also share a lot of charact"
Y18-3008,J82-2005,0,0.735985,"Missing"
Y18-3008,P02-1040,0,0.107413,"6 . Transformer (Vaswani et al., 2017) cells were used as the basic units in the encoders and decoders through the PyTorch 0.4.0 toolkit, and the numbers of both the encoder and decoder layers were set to 4. The dimension of the token embeddings and the hidden layers was set to 512. The Adam optimizer (Kingma and Ba, 2015) was used, with a learning rate of 0.0001 and a batch size of 32. A maximum length of 175 tokens per sentence for each type of dataset and a dropout rate of 0.1 was set. It is worth mentioning that the random blank-out rate was set to 0.1 in the last experiment. BLEU scores (Papineni et al., 2002) of the translation in both directions were evaluated at every epoch, and training was stopped when the scores from the last ten epochs did not improve. 5 Results BLEU scores of 7.01 for translation from ZH-JA and 7.73 for JA-ZH were recorded, respectively, at the time of result submission. However, after bug-fixing and fine-tuning, the best scores increased to 31.99 and 25.87 respectively (both using stroke data). The results of the baseline systems and the two experiments on sub-character level data are recorded in Table 3. The two sub-character level models outperform the character level ba"
Y18-3008,W16-2323,0,0.0782235,"resource languages. On the other hand, monolingual data is much easier to find; many languages with limited parallel data still possess significant amounts of monolingual data. Lample et al. (2018) have proposed an unsupervised NMT model that is effective on similar language pairs, such as English–French and English–German. In this work, Chinese–Japanese language pair is used because they also share a lot of characters which can be used to replace the need for bilingual dictionaries. New sub-character datasets were also created to enhance the shared information. The byte-pair encodings (BPE) (Sennrich et al., 2016c) vocabularies were shared between the two related languages by jointly trained both monolingual corpora. FastText (Bojanowski et al., 2017) was then used to generate cross-lingual embeddings. Following this, two encoder–decoder language models were trained on noisy data on either monolingual corpora, respectively. For the translation models, back-translation (Sennrich et al., 2016b) was used to handle both direc981 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 -"
Y18-3008,P16-1009,0,0.215631,"resource languages. On the other hand, monolingual data is much easier to find; many languages with limited parallel data still possess significant amounts of monolingual data. Lample et al. (2018) have proposed an unsupervised NMT model that is effective on similar language pairs, such as English–French and English–German. In this work, Chinese–Japanese language pair is used because they also share a lot of characters which can be used to replace the need for bilingual dictionaries. New sub-character datasets were also created to enhance the shared information. The byte-pair encodings (BPE) (Sennrich et al., 2016c) vocabularies were shared between the two related languages by jointly trained both monolingual corpora. FastText (Bojanowski et al., 2017) was then used to generate cross-lingual embeddings. Following this, two encoder–decoder language models were trained on noisy data on either monolingual corpora, respectively. For the translation models, back-translation (Sennrich et al., 2016b) was used to handle both direc981 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 -"
Y18-3008,P16-1162,0,0.27729,"resource languages. On the other hand, monolingual data is much easier to find; many languages with limited parallel data still possess significant amounts of monolingual data. Lample et al. (2018) have proposed an unsupervised NMT model that is effective on similar language pairs, such as English–French and English–German. In this work, Chinese–Japanese language pair is used because they also share a lot of characters which can be used to replace the need for bilingual dictionaries. New sub-character datasets were also created to enhance the shared information. The byte-pair encodings (BPE) (Sennrich et al., 2016c) vocabularies were shared between the two related languages by jointly trained both monolingual corpora. FastText (Bojanowski et al., 2017) was then used to generate cross-lingual embeddings. Following this, two encoder–decoder language models were trained on noisy data on either monolingual corpora, respectively. For the translation models, back-translation (Sennrich et al., 2016b) was used to handle both direc981 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 -"
Y18-3008,W18-6303,1,0.838604,"daries, MeCab (Kudo et al., 2004) was used to pre-tokenize Japanese with the IPADic dictionary, and Jieba to pre-tokenize Chinese with its default dictionary. Then, a BPE sub-word model was trained on concatenated Chinese and Japanese monolingual data with a vocabulary size of 30,000 using fastBPE 3 , in order to reduce the vocabulary size and eliminate the presence of unknown words (OOV). Further, unsupervised NMT models rely heavily on shared information between the source and target data. Therefore, to enhance this information, new ideographs and stroke datasets were created. As opposed to Zhang and Komachi (2018), who utilized three corpora for different language pairs, namely, ASPEC–JC (Japanese Chinese), ASPEC–JE (Japanese English) and Casia2015 4 (Chinese English) to create decomposed datasets, only ASPEC–JC was chosen in this work in order to focus on the shared information between Chinese and Japanese characters. Another difference is that CHISE was used instead of CNS11643 charset 3 4 https://github.com/glample/fastBPE http://nlp.nju.edu.cn/cwmt-wmt/ 982 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyri"
Y18-3008,Q16-1027,0,0.0301123,"d embeddings further, a decomposed ideograph and stroke dataset for ASPEC Chinese–Japanese Language pairs was also created. BLEU scores of 32.99 for ZHJA and 26.39 for JA-ZH translation were recorded, respectively (both using stroke data). 1 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2014; Cho et al., 2014; Sutskever et al., 2014) systems have achieved great success in recent years and outperform traditional statistical machine translation (SMT) (Sennrich et al., 1 Our team ID for the submission to this shared task (Nakazawa et al., 2018) is TMU. 2016a; Wu et al., 2016; Zhou et al., 2016) systems. Nevertheless, one of its major challenges has been that it is necessary for NMT models to be trained using large parallel data, meaning that they can fail when the training data is not big enough (Koehn and Knowles, 2017; Isabelle et al., 2017). Unfortunately, the lack of large parallel corpora is a practical problem for the vast majority of language pairs, and these are often non-existent for low-resource languages. On the other hand, monolingual data is much easier to find; many languages with limited parallel data still possess significant amounts of monolingual data. Lample et al"
Y18-3014,P11-2027,0,0.0190703,"nces in the parallel corpus. 5.2 Network Settings We conducted the experiment using the following configuration: • Number of layers: 3 • Number of hidden units: 512 • Word embedding dimensionality: 512 • Source vocabulary size: 100,000 • Target vocabulary size: 30,000 • Minibatch size: 128 • Optimizer: Adam, SGD • Initial learning rate: 0.01 • Dropout rate: 0.2 • Beam size: 20 Regarding the optimizer, after we train our model using Adam for 20 epochs, we switch to SGD. 5.3 Results Tables 2 and 3 show the translation accuracy in BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), AMFM (Banchs and Li, 2011), and HUMAN evaluation scores, which are the result of pairwise crowdsourcing evaluation by five different workers at the WAT 2018. In the “Model” column, “Ensemble of two models each” indicates the ensemble of two baselines, two GAN NMTs, and two reconstructor NMTs (ensemble of six models in total). Regarding Japanese–English translation, the results show that GAN NMT and reconstructor NMT slightly improved BLEU score compared with the baseline. However, in English–Japanese translation, BLEU score of the baseline is higher than the GAN and reconstructor NMTs. In terms of AMFM score, both meth"
Y18-3014,D14-1179,0,0.0277254,"Missing"
Y18-3014,D10-1092,0,0.0127307,"e 1 shows the number of sentences in the parallel corpus. 5.2 Network Settings We conducted the experiment using the following configuration: • Number of layers: 3 • Number of hidden units: 512 • Word embedding dimensionality: 512 • Source vocabulary size: 100,000 • Target vocabulary size: 30,000 • Minibatch size: 128 • Optimizer: Adam, SGD • Initial learning rate: 0.01 • Dropout rate: 0.2 • Beam size: 20 Regarding the optimizer, after we train our model using Adam for 20 epochs, we switch to SGD. 5.3 Results Tables 2 and 3 show the translation accuracy in BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), AMFM (Banchs and Li, 2011), and HUMAN evaluation scores, which are the result of pairwise crowdsourcing evaluation by five different workers at the WAT 2018. In the “Model” column, “Ensemble of two models each” indicates the ensemble of two baselines, two GAN NMTs, and two reconstructor NMTs (ensemble of six models in total). Regarding Japanese–English translation, the results show that GAN NMT and reconstructor NMT slightly improved BLEU score compared with the baseline. However, in English–Japanese translation, BLEU score of the baseline is higher than the GAN and reconstructor NMTs. In te"
Y18-3014,D15-1166,0,0.175238,"nd 36.14 points in Japanese–English and English–Japanese translations, respectively, in terms of BLEU scores. Furthermore, we found that GAN NMT can translate fluently. 1 Introduction In recent years, neural machine translation (NMT) has been researched all over the world. Once the encoder–decoder NMT (Sutskever et al., 2014; Cho et al., 2014), which combines two recurrent neural networks (RNNs), was proposed, NMT gained huge popularity in the machine translation community. However, the conventional encoder–decoder NMT works poorly on long sequences. Attentionbased NMT (Bahdanau et al., 2015; Luong et al., 2015) can provide better prediction of output words by using the weights of each hidden state of the encoder as the context vector. It contributed to improvement of translation quality, especially in long sentences. Transformer (Vaswani et al., 2017) is an extension of attention-based NMT; however, it is different from previous NMTs. They proposed a selfattention network and positional encoding. Thereby, NMT achieved high-quality translation without using RNN and convolutional neural network (CNN). Nevertheless, NMT has several problems such as over-translation, wherein some words are translated re"
Y18-3014,P02-1040,0,0.103786,"nsiders both: forward and back-translations. This approach can reduce over- and under-translation in forward translation because back-translation fails if there is a lack of information. The effect of this approach in English–Japanese translation is reported in (Matsumura et al., 2017). We experimented with our NMT system for Japanese–English and English–Japanese scientific paper translation subtasks. The experimental results demonstrate that the ensemble of baseline systems achieved 25.85 and 36.14 points in Japanese– English and English–Japanese translations, respectively, in terms of BLEU (Papineni et al., 2002) scores. Furthermore, we found that GAN NMT can translate fluently in English-Japanese pairwise evaluation. 2 Attention-based NMT In this section, we describe our baseline NMT system1 . This system is based on the attention-based NMT (Luong et al., 2015). We adopted a bidirectional long short-term memory (LSTM) as the encoder and a unidirectional LSTM as the decoder. 2.1 Encoder by and → −−→ − →(l) − hi = LSTM(hi (l−1) , hi−1 (l) ) (3) ← − ← − ←−− hi (l) = LSTM(hi (l−1) , hi+1 (l) ) (4) − →(0) where l is the layer number. Note that hi and ← − hi (0) are regarded as esi . 2.2 Decoder Similar to"
Y18-3014,D14-1162,0,0.0813317,"Copyright 2018 by the authors PACLIC 32 - WAT 2018 Source Reference or Output Generator (NMT) True or Generate Discriminator Figure 1: Overview of GAN NMT. The conditional probability of the output word yˆj is computed as: ˜ j + bp ) p(yˆj |Y<j , X) = softmax(Wp h (11) where Wp ∈ Rvt ×r is a weight matrix and bp ∈ Rvt is a bias vector. 2.3 Training The objective function of this system is D |Y | 1 ∑∑ (d) (d) L(θ) = log p(yj |Y<j , X (d) , θ) D d=1 j=1 (12) where D denotes the number of data and θ denotes the model parameters. The model parameters in word embedding are pretrained using GloVe (Pennington et al., 2014). All other model parameters are randomly initialized. 2.4 Testing To achieve better predictions, we adopted beam search and ensemble decoding. In beam search, the system retains hypotheses of beam size n at each time step. During the subsequent time step, for each hypothesis, it computes n hypotheses; further, it retains n hypotheses out of the total n2 hypotheses. In ensemble decoding, the conditional probability of the output word yˆj is the average of each model’s score p(m) . It is computed by p(yˆj |Y<j , X) = M 1 ∑ (m) p (yˆj |Y<j , X) (13) M m=1 where M denotes the number of models. Th"
Y18-3014,N18-1122,0,0.0501576,"Missing"
