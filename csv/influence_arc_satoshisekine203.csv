1995.iwpt-1.26,H91-1060,0,0.0257967,"Missing"
1995.iwpt-1.26,H92-1026,0,0.0660896,"Missing"
1995.iwpt-1.26,E93-1006,0,0.0676845,"coverage grammars . Although it is inevitable that a stru ctured corpus will contains errors , statistical methods and t he size of the corpus may be able to ameliorate the effect of i ndividual errors. Aiso , because a large corpus will i nclude examples of many rare constructs, we have the potenitial of obtain ing broader coverage than we might with a hand-constructed grammar . Furthermore, experi ments over the past few years have shown the benefits of usi ng probabilistic i nformation in pars i ng, and the large corpus allows us to t rain the probabilities of a grammar (8) {7] [ l l] :(2) [4] ,(1 2] . A number of recent parsing experiments have also indicated that grammars whose production prob,:i bilities are dependent on the context can be more effective than context-free grammars in selecti ng a correct parse. This context sensitivity can be acqu i red ea5ily using a large corpus, whereas human abi l i ty to com pute such information is obviously limited. . There have been 216 several attempts to build context-dependent grammars based on large corpora. ( 1 4] ( 1 1] ( 1 3] (2] (4] ( 1 2] . As i s evident from the two lists of citations, there has been considerable research invo"
1995.iwpt-1.26,P93-1035,0,0.0953284,"Missing"
1995.iwpt-1.26,J93-1002,0,0.0706276,"Missing"
1995.iwpt-1.26,H90-1053,1,0.900788,"Missing"
1995.iwpt-1.26,E85-1024,0,0.0599362,"Missing"
1995.iwpt-1.26,C94-2119,1,0.869156,"Missing"
2020.lrec-1.150,Y09-1009,0,0.0384769,"tured knowledge-base, Sekine et al. (2018b) try to structure Wikipedia. Their final goal is to have, for each Wikipedia article, known entities and sets of attributes, with each attribute linking to other entities wherever possible. The initial step towards this goal would be to classify the entities into predefined categories and verify the results using human annotators1 . Throughout the past years, many have tried classifying Wikipedia articles into different category sets mostly containing between 3 to 15 class types (Toral and Munoz, 2006; Watanabe et al., 2007; Dakka and Cucerzan, 2008; Chang et al., 2009; Tardif et al., * The author was an intern at AIP Center for Advanced Intelligence, during this project. 1 Please note that the verification process plays an important role in the knowledge-base construction process since it leads to what is represented to our models as world facts. 2009). Such categorization type sets are not much helpful when the classified articles are being used as the training data for question answering systems, since the extracted knowledge-base does not provide detailed enough information to the model. On the other hand, much larger categorization type sets such as Cy"
2020.lrec-1.150,I08-1071,0,0.0296937,"ssue and make a more structured knowledge-base, Sekine et al. (2018b) try to structure Wikipedia. Their final goal is to have, for each Wikipedia article, known entities and sets of attributes, with each attribute linking to other entities wherever possible. The initial step towards this goal would be to classify the entities into predefined categories and verify the results using human annotators1 . Throughout the past years, many have tried classifying Wikipedia articles into different category sets mostly containing between 3 to 15 class types (Toral and Munoz, 2006; Watanabe et al., 2007; Dakka and Cucerzan, 2008; Chang et al., 2009; Tardif et al., * The author was an intern at AIP Center for Advanced Intelligence, during this project. 1 Please note that the verification process plays an important role in the knowledge-base construction process since it leads to what is represented to our models as world facts. 2009). Such categorization type sets are not much helpful when the classified articles are being used as the training data for question answering systems, since the extracted knowledge-base does not provide detailed enough information to the model. On the other hand, much larger categorization"
2020.lrec-1.150,C12-1071,0,0.236448,"7), or Wikipedia’s own taxonomy of categories (Sch¨onhofen, 2009) are not suitable for classifying Wikipedia articles since the tags are not verifiable for annotators2 . In addition, taxonomies are not designed in a tree format, so some categories might have multiple super-categories and this would make the verification process much harder for articles discussing multiple topics. Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy (Sekine et al., 2002), containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set. Higashinaka et al. (2012) were the first to use this extended tag set as output labels when categorizing Wikipedia pages. Their model was trained using a hand-extracted feature set that converted the pages into model compatible input vectors. Following their work, Suzuki et al. (2016) augmented the extracted input features with trained vectors modelling the links between different Wikipedia pages. They proposed a more complex model for learning the mapping be2 They need to keep 200K+ classes in mind to find the most suitable ones for the article at hand or verify the classifier category prediction for it. 1197 languag"
2020.lrec-1.150,sekine-etal-2002-extended,1,0.547215,"the other hand, much larger categorization type sets such as Cyc-Taxonomy (Lenat, 1995), YagoTaxonomy (Suchanek et al., 2007), or Wikipedia’s own taxonomy of categories (Sch¨onhofen, 2009) are not suitable for classifying Wikipedia articles since the tags are not verifiable for annotators2 . In addition, taxonomies are not designed in a tree format, so some categories might have multiple super-categories and this would make the verification process much harder for articles discussing multiple topics. Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy (Sekine et al., 2002), containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set. Higashinaka et al. (2012) were the first to use this extended tag set as output labels when categorizing Wikipedia pages. Their model was trained using a hand-extracted feature set that converted the pages into model compatible input vectors. Following their work, Suzuki et al. (2016) augmented the extracted input features with trained vectors modelling the links between different Wikipedia pages. They proposed a more complex model for learning the mapping be2 They need to keep 200K+ classes"
2020.lrec-1.150,Y16-3027,1,0.470349,"le super-categories and this would make the verification process much harder for articles discussing multiple topics. Considering the mentioned problem requirements, we believe Extended Named Entities Hierarchy (Sekine et al., 2002), containing 200 fine-grained categories tailored for Wikipedia articles, is the best fitting tag set. Higashinaka et al. (2012) were the first to use this extended tag set as output labels when categorizing Wikipedia pages. Their model was trained using a hand-extracted feature set that converted the pages into model compatible input vectors. Following their work, Suzuki et al. (2016) augmented the extracted input features with trained vectors modelling the links between different Wikipedia pages. They proposed a more complex model for learning the mapping be2 They need to keep 200K+ classes in mind to find the most suitable ones for the article at hand or verify the classifier category prediction for it. 1197 language ja en fr de fa average size in folds total average count max train dev test total classes article/class annot./article annotations 96,321.8 12,004.9 12,006.3 120,333 141 853.426 1.0359 5 42,652.8 5,301.1 5,301.1 53,228 127 419.331 1.0359 5 27,750.5 3,425.7 3"
2020.lrec-1.150,U09-1015,0,0.0606456,"Missing"
2020.lrec-1.150,W06-2809,0,0.0510601,"to be useful for the models. To address this issue and make a more structured knowledge-base, Sekine et al. (2018b) try to structure Wikipedia. Their final goal is to have, for each Wikipedia article, known entities and sets of attributes, with each attribute linking to other entities wherever possible. The initial step towards this goal would be to classify the entities into predefined categories and verify the results using human annotators1 . Throughout the past years, many have tried classifying Wikipedia articles into different category sets mostly containing between 3 to 15 class types (Toral and Munoz, 2006; Watanabe et al., 2007; Dakka and Cucerzan, 2008; Chang et al., 2009; Tardif et al., * The author was an intern at AIP Center for Advanced Intelligence, during this project. 1 Please note that the verification process plays an important role in the knowledge-base construction process since it leads to what is represented to our models as world facts. 2009). Such categorization type sets are not much helpful when the classified articles are being used as the training data for question answering systems, since the extracted knowledge-base does not provide detailed enough information to the mode"
2020.lrec-1.150,P12-2018,0,0.0498598,"have suggested classifying Wikipedia articles using ENEs. We also decided to study the usefulness of the hierarchy in the process of training the classifiers using ENEs. Hence, we also selected the models suggested by Wehrmann et al. (2018) as our third set of models. The following sections describe our feature selection procedure and briefly explain each of the models. 3.1. Feature Selection A fair comparison between the models on the dataset is not possible unless we can guarantee the same input to each of them. With that in mind, we went through the feature selection methods suggested in (Wang and Manning, 2012), (Higashinaka et al., 2012) and (Suzuki et al., 2016) and created a union of their suggestions. However, we had to remove some of the features such as ‘Last one/two/three characters in the headings or titles” or “Last character type (Hiragana/Katakana/Kanji/Other)” from the union due to the multi-lingual nature of our task. Figure 1 summarizes the final unified schema for categorization of the Wikipedia articles in SHINRA5LDS. 3.2. Binary Logistic Regression Higashinaka et al. (2012) suggested learning a set of separate Binary Logistic Regression Classifier Models to learn the contribution of"
2020.lrec-1.150,D07-1068,0,0.0620426,"dels. To address this issue and make a more structured knowledge-base, Sekine et al. (2018b) try to structure Wikipedia. Their final goal is to have, for each Wikipedia article, known entities and sets of attributes, with each attribute linking to other entities wherever possible. The initial step towards this goal would be to classify the entities into predefined categories and verify the results using human annotators1 . Throughout the past years, many have tried classifying Wikipedia articles into different category sets mostly containing between 3 to 15 class types (Toral and Munoz, 2006; Watanabe et al., 2007; Dakka and Cucerzan, 2008; Chang et al., 2009; Tardif et al., * The author was an intern at AIP Center for Advanced Intelligence, during this project. 1 Please note that the verification process plays an important role in the knowledge-base construction process since it leads to what is represented to our models as world facts. 2009). Such categorization type sets are not much helpful when the classified articles are being used as the training data for question answering systems, since the extracted knowledge-base does not provide detailed enough information to the model. On the other hand, m"
2021.findings-emnlp.383,2021.wanlp-1.28,0,0.061468,"Missing"
2021.findings-emnlp.383,2020.sdp-1.24,0,0.0555761,"Missing"
2021.findings-emnlp.383,P17-1171,0,0.0153477,"pants can access the manually annotated training data and articles in each category. For the SHINRA shared task evaluation, a portion of the data is hidden, and all the participants have to annotate all the data so that the organizer can create unified data for all Wikipedia entries. A total of nine teams participated in the SHINRA2019-JP task. Some participants submit results for a subset of the categories, and six to nine systems submit results for every category. Various methods are used, including rule-based methods, the ML method using CRF and SVM, a deep learning-based method, and DrQA (Chen et al., 2017). This task follows the Resource by Collaborative Contribution (RbCC) scheme. Therefore, the task organizers release all submission results as a resource. In this task, participants do not necessarily need to submit the prediction probabilities assigned to the system outputs; thus, the organizers do not share these values. The organizer also distributed the development data for the City and Lake categories, which are not used to evaluate. In our study, we use those labels for our detailed analysis. The task organizers have not released the evaluation set used in the task. Therefore, we sent ou"
2021.findings-emnlp.383,N19-1423,0,0.0144675,"ages 1,615 51,035 35,356 5,819 308,610 354 269 1,304 2,054 2,269 395 2,292 772 2,525 3,718 12,008 2,764 291 1,080 386 1,133 1,904 3,053 949 3,368 5,046 3,867 1,177 1,543 10,290 790 841 4,828 Num. Num. Train Attributes 599 24 1,000 25 995 34 598 15 999 21 200 28 147 15 158 22 200 13 200 19 200 18 173 34 200 32 200 18 200 32 198 26 200 17 200 28 190 21 200 28 200 13 200 18 200 20 191 20 200 22 200 23 183 13 200 12 199 23 196 22 200 23 189 24 199 29 Table 1: Distribution of SHINRA2019-JP data. Furthermore, we define the student model used in this experiment, as shown in Fig. 3. We use BERT-base (Devlin et al., 2019) for θsh and a linear layer for θ1 , ..., θS , θout , respectively, and apply Dropout (Srivastava et al., 2014) and an activation function GeLU (Hendrycks and Gimpel, 2020) to the output of BERT. BERT is pretrained using the same scheme as RoBERTa (Liu et al., 2019) utilizing Japanese Wikipedia. Class Balanced Focal Loss (Cui et al., 2019), which is a combination of Class Balanced Loss (Cui et al., 2019) and Focal Loss (Lin et al., 2017), is used for the loss functions ˆ t and L ˆ g to deal with the class imbalance IOB2 laL bels. We also determine α, which balances the loss between the ground"
2021.findings-emnlp.383,2020.fnp-1.1,0,0.0607426,"Missing"
2021.findings-emnlp.383,S10-1006,0,0.0181162,"Correlation between the best and second-best system difference and the improvement from the best system with Co-Teaching. Micro-average F1 73.0 72.5 72.0 71.5 71.0 final output : p mean private layers : pmean output layer : pout Best System 2000 3000 4000 5000 6000 7000 8000 9000 10000 Number of instances used for student learning (including development data) Figure 5: Changes in student model scores on data for analysis in City category when the number of articles used to train the student model is increased. tasks such as the Sentiment Analysis and Relation Classification tasks in SemEval (Hendrickx et al., 2010), Dialect Classification task in NADI (AbdulMageed et al., 2021), and word classification task in CoNLL (Tjong Kim Sang and De Meulder, 2003). Also, the conditions are satisfied for translation tasks such as those in WMT (Barrault et al., 2020) and WAT (Nakazawa et al., 2020) as well as generation tasks such as those in SDP (Chandrasekaran et al., 2020) and FNS (El-Haj et al., 2020). The similarity between these tasks is that the formats of the training data and the task submissions are essentially the same. That is, a student model can be designed with few modifications to the model designed"
2021.findings-emnlp.383,H93-1012,0,0.19298,"Missing"
2021.findings-emnlp.383,sekine-2008-extended,1,0.505261,"re these values. The organizer also distributed the development data for the City and Lake categories, which are not used to evaluate. In our study, we use those labels for our detailed analysis. The task organizers have not released the evaluation set used in the task. Therefore, we sent our results to the organizers and received the evaluation results. SHINRA2019-JP is a shared task to extract the attribute values in Japanese Wikipedia articles. These articles are preclassified into Extended Named Entity (ENE) categories by Suzuki et al. (2018). ENE is a set of Named Entity types defined by Satoshi (2008) and includes about 200 categories. The attributes are predefined for each category by ENE, and the participants build attribute value extraction systems using the distributed training portion of Wikipedia and are instructed to make their predictions for all remaining Wikipedia articles. The 4.2 Co-Teaching on Shared Task task requires specifying where the mention of the value occurs and not just extracting the surface In order to demonstrate the effectiveness of the protext. The SHINRA2019-JP targets 35 categories; posed scheme, we use the submission results shared five categories called JP-5"
2021.findings-emnlp.383,M95-1002,0,0.514804,"istics of each system. We call this scheme “CoTeaching.” This scheme creates a unified system that performs better than the task’s single best system. It only requires the system outputs, and slightly extra effort is needed for the participants and organizers. We apply this scheme to the “SHINRA2019-JP” shared task, which has nine participants with various output accuracies, confirming that the unified system outperforms the best system. Moreover, the code used in our experiments has been released.1 1 Introduction Shared tasks have a long history and have become the highlight of NLP research (Sundheim, 1995; Tjong Kim Sang and Buchholz, 2000; Ounis et al., 2008; Dang and Owczarzak, 2009). These tasks have contributed to natural language processing technology development by attracting researchers interested in being the best task player. The systems are evaluated using the output submitted to the task, and they usually have no obligation to submit the system. It limits the participant’s contribution once the task is over because the system 1 https://github.com/k141303/co_ teaching_scheme is a future asset. We believe all participating systems have values as a resource, even if they are not the be"
2021.findings-emnlp.383,W00-0726,0,0.112443,"call this scheme “CoTeaching.” This scheme creates a unified system that performs better than the task’s single best system. It only requires the system outputs, and slightly extra effort is needed for the participants and organizers. We apply this scheme to the “SHINRA2019-JP” shared task, which has nine participants with various output accuracies, confirming that the unified system outperforms the best system. Moreover, the code used in our experiments has been released.1 1 Introduction Shared tasks have a long history and have become the highlight of NLP research (Sundheim, 1995; Tjong Kim Sang and Buchholz, 2000; Ounis et al., 2008; Dang and Owczarzak, 2009). These tasks have contributed to natural language processing technology development by attracting researchers interested in being the best task player. The systems are evaluated using the output submitted to the task, and they usually have no obligation to submit the system. It limits the participant’s contribution once the task is over because the system 1 https://github.com/k141303/co_ teaching_scheme is a future asset. We believe all participating systems have values as a resource, even if they are not the best. It may be desirable to share it"
2021.findings-emnlp.383,E99-1023,0,0.0404809,"; posed scheme, we use the submission results shared five categories called JP-5 subclass are those pre- by SHINRA2019-JP to train a student model. Alviously targeted in SHINRA2018-JP in addition to though this is an attribute value extraction task, 30 new categories. Of the 30 categories, 14 belong it can be solved as a sequence labeling task beto the Location subclass, and the rest belong to cause each attribute value contains the offset of its the Organization subclass. Examples of attributes occurrence in the text. We use the IOB2 (Tjong and values in the Lake category of the Location Kim Sang and Veenstra, 1999) scheme to solve subclass are shown in Fig. 2. Note that in this the sequence labeling task. That is, we classify 4528 the first word of an attribute value as Beginning (B), the following words as Inside (I), and words outside the attribute value as Outside (O). In this task, a word may have multiple attribute labels. Therefore we use I, O, and B tags for each attribute. More specifically, we classify the word g s {ti,j ∈ xi }j∈[K] into yi,j and yi,j ,∀s∈[S] , where g s |yi,j |= C, |yi,j ,∀s∈[S] |= C, K is the sentence length, and C is the number of attributes to be extracted. We use MeCab2 to"
2021.findings-emnlp.383,P95-1026,0,0.198815,"annot access the teacher model. Therefore we transfer responsebased knowledge to a student through offline distillation. Response-based knowledge refers more explicitly to information propagated through the teacher’s output probability. Suppose the output probabilities are not included in the submission results of the shared task, as in this paper. In that case, the teacher’s knowledge can also be extracted from their predictions for additional unlabeled data. 2.2 Semi-Supervised Learning The learning method used in our scheme can be classified as a semisupervised method such as SelfTraining (Yarowsky, 1995) and Co-Training (Blum and Mitchell, 1998) in that it uses unlabeled data predictions for learning. Self-Training is a method for building a more robust machine learning model • We enumerated the shared tasks that have by adding labels with high confidence to the trainbeen conducted recently in the field of nat- ing data from trained model predictions and retrainural language processing and discussed our ing the model. Co-Training is an extension of the scheme’s applicability. Self-Training method, where the instances added to the training data are determined by the label confidence obtained u"
2021.socialnlp-1.3,Q17-1010,0,0.0091802,"Missing"
2021.socialnlp-1.3,D18-1449,1,0.874029,"Missing"
2021.socialnlp-1.3,P19-1250,1,0.612232,"i) user feedback will be biased by where comments appear in a comment thread (also known as “position bias” (Craswell et al., 2008)). A typical example for (i) can be seen in political comments, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Spec"
2021.socialnlp-1.3,W17-3002,0,0.337997,"n be seen in political comments, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Specifically, in collaboration with Yahoo! JAPAN News, we report a case study of deploying a model that ranks constructive comments in a commercial service. The Introduction"
2021.socialnlp-1.3,W17-4218,0,0.324695,"n be seen in political comments, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Specifically, in collaboration with Yahoo! JAPAN News, we report a case study of deploying a model that ranks constructive comments in a commercial service. The Introduction"
2021.socialnlp-1.3,E17-2068,0,0.0087148,"etween an article and its comment, where this setting is a kind of stacking ensemble. Model: The model was an L2-regularized L2loss linear rankSVM model that was implemented as an instance of the well-known SVM tool LIBLINEAR (ver. 2.1.1) (Lin, 2020). The cost parameter C was determined from {2−13 , . . . , 21 } on the basis of the performance on the validation set. Features: The features consisted of two factors. The first was the expected C-score, which was determined by first computing the probabilities of C-scores (considered as classes) using the opensource library fastText (ver. 0.2.0) (Joulin et al., 2017; Facebook, 2020)2 with word embeddings trained on news articles and then calculating their expected value. The second feature was the Euclidean distance between the comment and title vectors, each of which consisted of the frequencies of words. • Model-14: The model with the highest NDCGL. It is a gradient boosting model (pointwise learning) with features based on maximal substrings and words. Model: The model was based on LightGBM (ver. 2.2.1) (Microsoft, 2020; Ke et al., 2017), a tree-based gradient boosting framework. The parameters were hand-tuned with a tuning guide (LightGBM Doc., 2020)"
2021.socialnlp-1.3,D18-2012,0,0.0201552,"Missing"
2021.socialnlp-1.3,W04-3230,0,0.0603111,"Missing"
2021.socialnlp-1.3,P16-3007,0,0.0252811,"rmation retrieval task (Kato and Liu, 2019). Their purpose to find good models for a specific task is almost the same as ours, and the main difference (ignoring the task) is that the competition in our work was conducted within a company. As this kind of work towards a commercial service is rarely released in the form of an academic paper, we expect that our findings will become valuable knowledge for practitioners in this field. Related Work Constructiveness: Analyzing the comments on online news services or discussion forums has been extensively studied (Wanas et al., 2008; Ma et al., 2012; Llewellyn et al., 2016; Shi and Lam, 2018). In this line of research, many studies have focused on ranking comments (Hsu et al., 2009; Das Sarma et al., 2010; Brand and Van Der Merwe, 2014; Wei et al., 2016). However, the prior approaches have been based on user feedback, which is completely different from constructiveness. Constructiveness has been introduced in argument analysis frameworks (Napoles et al., 2017a,b; Kolhatkar and Taboada, 2017a,b; Kolhatkar et al., 2020). The purpose of these studies was to classify constructive comments, whereas Fujita et al. (2019) recently expanded their tasks to a ranking one."
2021.socialnlp-1.3,P16-2032,0,0.128498,"iences, and simplified explanations of the article. There is a limit, however, on the number of comments that can be displayed on a page, and as users typically do not have the time or inclination to read through all the comments, ideally they should be ranked in some way. Prioritizing the comments for display is directly linked to user satisfaction, so improving this ranking is an important issue for such services. There have already been multiple studies on comment ranking in online news services and discussion forums (Hsu et al., 2009; Das Sarma et al., 2010; Brand and Van Der Merwe, 2014; Wei et al., 2016). All of these studies have utilized user feedback (e.g., “Like”-button clicks in Figure 1) as their ranking metrics. Although such user feedback is ∗ 1 Equal contribution. https://news.yahoo.co.jp/ 24 Proceedings of the Ninth International Workshop on Natural Language Processing for Social Media, pages 24–35 Online Workshop, June 10, 2021. ©2021 Association for Computational Linguistics Precondition • Related to article and not libelous Main conditions • Intended to stimulate discussions • Objective and supported by fact • New idea, solution, or insight • User’s unique experience characterist"
2021.socialnlp-1.3,W17-0802,0,0.121146,"s, where the “goodness” of the comment tends to be decided on the basis of the political views of the majority of the users rather than on its quality. A typical example of (ii) can be illustrated by a case where earlier comments tend to receive more feedback since they are displayed at the top of the page, which implies later comments will be ignored irrespective of their quality. To resolve this issue, Fujita et al. (2019) introduced a metric representing a comment’s constructiveness (see Section 2 for details), which has also been studied in argument analysis (Kolhatkar and Taboada, 2017a; Napoles et al., 2017a). Interestingly, they found empirical evidence that the constructiveness has no correlation with the user feedback, which has been commonly used for ranking comments. This implies that we need to consider the constructiveness rather than the user feedback to avoid unfavorable situations (i) and (ii) in real services. In this paper, we take their study one step further towards practical application. Specifically, in collaboration with Yahoo! JAPAN News, we report a case study of deploying a model that ranks constructive comments in a commercial service. The Introduction In online news service"
A92-1014,J90-1003,0,0.0889935,"Missing"
A92-1014,C90-1005,0,0.185946,"Missing"
A92-1014,C90-3010,0,0.0926168,"Missing"
A92-1014,J90-2002,0,0.0621388,"Missing"
A92-1014,E85-1013,0,0.0522061,"Missing"
A92-1014,J93-1005,0,\N,Missing
A92-1014,E85-1024,0,\N,Missing
A92-1014,C92-2085,1,\N,Missing
A97-1015,J93-2004,0,\N,Missing
A97-1015,C94-2174,0,\N,Missing
A97-1015,J93-2001,0,\N,Missing
ando-etal-2004-automatic,W03-0415,0,\N,Missing
ando-etal-2004-automatic,C92-2085,1,\N,Missing
ando-etal-2004-automatic,C92-2082,0,\N,Missing
C00-2109,W98-1512,0,0.0643188,"Missing"
C00-2109,W98-1510,0,\N,Missing
C00-2109,E99-1026,1,\N,Missing
C00-2109,P97-1003,0,\N,Missing
C00-2109,P98-1083,0,\N,Missing
C00-2109,C98-1080,0,\N,Missing
C00-2109,P95-1037,0,\N,Missing
C00-2109,W98-1511,0,\N,Missing
C00-2110,W98-1511,0,0.222665,"Missing"
C00-2110,E99-1026,1,\N,Missing
C00-2110,C00-1060,0,\N,Missing
C00-2110,C94-1071,0,\N,Missing
C00-2110,P98-1083,0,\N,Missing
C00-2110,C98-1080,0,\N,Missing
C00-2110,C00-2109,1,\N,Missing
C00-2126,J96-1002,0,\N,Missing
C00-2126,P99-1018,0,\N,Missing
C00-2167,sekine-isahara-2000-irex,1,\N,Missing
C02-1064,C00-1007,0,0.0708941,"ndidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words"
C02-1064,J96-1002,0,0.00733679,"n bunsetsus. 4. posterior dependency bigram model We assume that ki depends only on the headword, ws , and the word on its right, ws+1 , in the bunsetsu that is modiﬁed by the bunsetsu including ki (see Fig. 3). Text-Generation Model We next describe the model represented by Eq. (4); that is, a keyword-production model, a morpheme model that estimates how likely a string is to be a morpheme, and a dependency model. The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences. We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998). 4.1 Keyword-Production Models This section describes ﬁve keyword-production models which are represented by P (K|M, D, T ) in Eq. (4). In these models, we deﬁne the set of headwords whose frequency in the corpus is over a certain threshold as a set of keywords, KS, and we restrict the bunsetsus to those generated by the generation rules represented in form (5). We assume that all keywords are independent and that ki corresponds to word wj (1 ≤ j ≤ m) when text is given as a series of words w1 . . . wm . 1. trigram model We assume that ki depends only on the two a"
C02-1064,J90-2002,0,0.0214748,"uction Text generation is an important technique used for applications like machine translation, summarization, and human/computer dialogue. In recent years, many corpora have become available, and have been used to generate natural surface sentences. For example, corpora have been used to generate sentences for language model estimation in statistical machine translation. In such translation, given a source language text, S, the translated text, T , in the target language that maximizes the probability P (T |S) is selected as the most appropriate translation, Tbest , which is represented as (Brown et al., 1990) Tbest = argmaxT P (T |S) = argmaxT (P (S|T ) × P (T )) . (1) In this equation, P (S|T ) represents the model used to replace words or phrases in a source language with those in the target language. It is called a translation model. P (T ) represents a language model that is used to reorder translated words or phrases into a natural order in Hitoshi Isahara† ‡ New York University 715 Broadway, 7th floor New York, NY 10003, USA sekine@cs.nyu.edu the target language. The input of the language model is a “bag of words,” and the goal of the model is basically to reorder the words. At this point, t"
C02-1064,W01-0812,0,0.0111773,"erate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words are replaced with semantic attributes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed for sentence-structure analysis, we also use those developed for morphological analysis and found that these models contribute to the generation of appropriate text. Berger and Laﬀerty proposed a language model for information retrieval (Berger and Lafferty, 1999). Their concept is similar to that of our model, which can be regarded as a model that translates keywords into text, while their model can be regarded as one that translates query words into documents. However, the purpose of their model is diﬀerent: their goal is to retrieve text that already exists whi"
C02-1064,P95-1034,0,0.051106,"ed. In this section, we describe the diﬀerences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as “ga” and “wo”, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, “the” and “a,” to distinguish definite and indeﬁnite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a “knowledge gap” (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkil"
C02-1064,P98-1116,0,0.053813,"e diﬀerences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as “ga” and “wo”, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, “the” and “a,” to distinguish definite and indeﬁnite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a “knowledge gap” (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the can"
C02-1064,W98-1426,0,0.0486915,"e diﬀerences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as “ga” and “wo”, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, “the” and “a,” to distinguish definite and indeﬁnite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a “knowledge gap” (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the can"
C02-1064,A00-2023,0,0.031131,"u, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semant"
C02-1064,A00-2026,0,0.0407408,"y the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words are replaced with semantic attributes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed fo"
C02-1064,E99-1026,1,0.849331,"appropriate. We used headwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The"
C02-1064,C00-2126,1,0.918068,"eadwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol + indicates a co"
C02-1064,2000.iwpt-1.43,1,0.881498,"eadwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol + indicates a co"
C02-1064,W01-0512,1,0.834673,"priate, it is judged as appropriate. We used headwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency mod"
C02-1064,C98-1112,0,\N,Missing
C02-2019,maekawa-etal-2000-spontaneous,1,0.820306,"ethod to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string. The model estimates how likely a string is to be a morpheme. This model is independent of the domain of corpora; in this paper we demonstrate that this is true by applying our model to the spontaneous speech corpus, Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000). We also show that a dictionary developed for a corpus on a certain domain is helpful for improving accuracy in analyzing a corpus on another domain. 2 A Morpheme Model This section describes a model which estimates how likely a string is to be a morpheme. We implemented this model within an M.E. framework. Given a tokenized test corpus, the problem of Japanese morphological analysis can be reduced to the problem of assigning one of two tags to each string in a sentence. A string is tagged with a 1 or a 0 to indicate whether or not it is a morpheme. When a string is a morpheme, a grammatical"
C02-2019,C96-2202,0,0.317545,"Japanese sentence analysis. A morpheme is a minimal grammatical unit, such as a word or a suﬃx, and morphological analysis is the process of segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as part-of-speech (POS) and inﬂection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to ﬁnd unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (M.E.) model (Uchimoto et al., 2001). We used their method to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what typ"
C02-2019,P99-1036,0,0.261165,"ess of segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as part-of-speech (POS) and inﬂection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to ﬁnd unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (M.E.) model (Uchimoto et al., 2001). We used their method to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string. The model estimates how likely a string is to be a morpheme. This model is independent"
C02-2019,W01-0512,1,0.81887,"and inﬂection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to ﬁnd unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (M.E.) model (Uchimoto et al., 2001). We used their method to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string. The model estimates how likely a string is to be a morpheme. This model is independent of the domain of corpora; in this paper we demonstrate that this is true by applying our model to the spontaneous speech corpus, Corpus of Spontaneous"
C02-2019,J96-1002,0,\N,Missing
C04-1122,P01-1008,0,0.0107293,"ernative expression of the same content. Now, two document sets where each document of one set is associated with one in the other set, is called a “comparable” corpus. A comparable corpus is less restricted than a parallel corpus and usually more available. Several different newspapers published on the same day report lots of the same events, therefore contain a number of comparable documents. One can also take another view of a comparable corpus, which is a set of paraphrased documents. By exploiting this feature, one can extract paraphrastic expressions automatically from parallel corpora (Barzilay and McKeown, 2001) or comparable corpora (Shinyama and Sekine, 2003). Named Entities in comparable documents have one notable characteristic: they tend to be preserved across comparable documents because it is generally difficult to paraphrase names. We think that it is also hard to paraphrase product names or disease names, so they will also be preserved. Therefore, if one Named Entity appears in one document, it should also appear in the comparable document. 30 istic. LATWP NYT REUTE 25 3 Frequency 20 15 10 5 0 0 50 100 150 200 Date 250 300 350 400 The occurrence of the word “yigal” 140 LATWP NYT REUTE 120 Fr"
C04-1122,W99-0613,0,0.444154,"Missing"
C04-1122,sekine-etal-2002-extended,1,0.523802,"ere the number of occurrences of a certain name is rather small. 2 1 Introduction Recently, Named Entity (NE) recognition has been getting more attention as a basic building block for practical natural language applications. A Named Entity tagger identifies proper expressions such as names, locations and dates in sentences. We are trying to extend this to an Extended Named Entity tagger, which additionally identifies some common nouns such as disease names or products. We believe that identifying these names is useful for many applications such as information extraction or question answering (Sekine et al., 2002). Normally a Named Entity tagger uses lexical or contextual knowledge to spot names which appear in documents. One of the major problem of this task is its data sparseness. Names appear very frequently in regularly updated documents such as news articles or web pages. They are, however, much more varied than common nouns, and changing continuously. Since it is hard to construct a set of predefined names by hand, usually some corpus based approaches are used for building such taggers. However, as Zipf’s law indicates, most of the names which occupy a large portion of vocabulary are rarely used."
C04-1122,W03-1609,1,0.803268,"ocument sets where each document of one set is associated with one in the other set, is called a “comparable” corpus. A comparable corpus is less restricted than a parallel corpus and usually more available. Several different newspapers published on the same day report lots of the same events, therefore contain a number of comparable documents. One can also take another view of a comparable corpus, which is a set of paraphrased documents. By exploiting this feature, one can extract paraphrastic expressions automatically from parallel corpora (Barzilay and McKeown, 2001) or comparable corpora (Shinyama and Sekine, 2003). Named Entities in comparable documents have one notable characteristic: they tend to be preserved across comparable documents because it is generally difficult to paraphrase names. We think that it is also hard to paraphrase product names or disease names, so they will also be preserved. Therefore, if one Named Entity appears in one document, it should also appear in the comparable document. 30 istic. LATWP NYT REUTE 25 3 Frequency 20 15 10 5 0 0 50 100 150 200 Date 250 300 350 400 The occurrence of the word “yigal” 140 LATWP NYT REUTE 120 Frequency 100 80 60 40 20 0 0 50 100 150 200 Date 25"
C04-1122,C96-2157,0,0.0478919,"Missing"
C04-1122,C02-1154,0,0.0186194,"Missing"
C04-1122,sekine-isahara-2000-irex,1,\N,Missing
C04-1127,N03-4013,1,\N,Missing
C04-1127,P03-1029,1,\N,Missing
C04-1127,J03-1002,0,\N,Missing
C04-1127,P03-1044,0,\N,Missing
C04-1127,C02-1070,0,\N,Missing
C08-3010,W04-3205,0,0.0617513,"Missing"
C08-3010,C92-2082,0,0.153411,"Missing"
C08-3010,P04-1053,1,0.870049,"Missing"
C08-3010,W99-0613,0,\N,Missing
C08-3010,I08-1025,0,\N,Missing
C14-2009,W02-1001,0,0.0596611,"g sequence y ∗ for an input string x is inferred based on the features ϕ(y) and the weight vector w as y ∗ = arg maxy∈Y (x) w · ϕ(y), where Y (x) denotes all the possible tag sequences for x, via standard Viterbi decoding. Table 1 shows the feature template sets. For training, we used soft conﬁdence weighted (SCW) (Wang et al., 2012). SCW is an online learning scheme based on Conﬁdence Weighted (CW), which maintains “conﬁdence” of each parameter as variance Σ in order to better control the updates. Since SCW itself is a general classiﬁcation model, we employed the structured prediction model (Collins, 2002) for WS. The code snippet in Figure 2 shows typical usage of Rakuten MA in an interactive way. Lines starting with “//” and “&gt;” are comments and user input, and the next lines are returned results. Notice that the analysis of バラクオバマ大統領 “President Barak Obama” get better as the model observes more instances. The analyzer can only segment it into individual characters when the model is empty ((1) in the code), whereas WS is partially correct after observing the ﬁrst 10 sentences of the corpus ((2) in the code). After directly providing the gold standard, the result (3) becomes perfect. We used a"
C14-2009,W04-3230,0,0.175962,"tation. We have achieved a compact model size (5MB) while maintaining the state-of-the-art performance, via techniques such as feature hashing, FOBOS, and feature quantization. 1 Introduction Word segmentation (WS) and part-of-speech (PoS) tagging, often jointly called morphological analysis (MA), are the essential component for processing Chinese and Japanese, where words are not explicitly separated by whitespaces. There have been many word segmentater and PoS taggers proposed in both Chinese and Japanese, such as Stanford Segmenter (Tseng et al., 2005), zpar (Zhang and Clark, 2011), MeCab (Kudo et al., 2004), JUMAN (Kurohashi and Nagao, 1994), to name a few. Most of them are intended for server-side use and provide limited capability to extend or re-train models. However, as mobile devices such as smartphones and tablets become popular, there is a growing need for client based, lightweight language analysis, and a growing number of applications are built upon lightweight languages such as HTML, CSS, and JavaScript. Techniques such as domain adaptation and model extension are also becoming more important than ever. In this paper, we present Rakuten MA, a morphological analyzer entirely written in"
C14-2009,P11-2093,0,0.114391,"Missing"
C14-2009,J11-1005,0,0.0151965,"de model update and domain adaptation. We have achieved a compact model size (5MB) while maintaining the state-of-the-art performance, via techniques such as feature hashing, FOBOS, and feature quantization. 1 Introduction Word segmentation (WS) and part-of-speech (PoS) tagging, often jointly called morphological analysis (MA), are the essential component for processing Chinese and Japanese, where words are not explicitly separated by whitespaces. There have been many word segmentater and PoS taggers proposed in both Chinese and Japanese, such as Stanford Segmenter (Tseng et al., 2005), zpar (Zhang and Clark, 2011), MeCab (Kudo et al., 2004), JUMAN (Kurohashi and Nagao, 1994), to name a few. Most of them are intended for server-side use and provide limited capability to extend or re-train models. However, as mobile devices such as smartphones and tablets become popular, there is a growing need for client based, lightweight language analysis, and a growing number of applications are built upon lightweight languages such as HTML, CSS, and JavaScript. Techniques such as domain adaptation and model extension are also becoming more important than ever. In this paper, we present Rakuten MA, a morphological an"
C14-2009,I05-3027,0,\N,Missing
C18-1060,M92-1003,0,0.740127,"Missing"
C18-1060,Q16-1026,0,0.039607,"ormance, as described in Section 3.4. 3.3 LSTM+CNN+CRF model for FG-NER We re-implemented the LSTM+CNN+CRF NER model described by Ma and Hovy (2016) and adjust the model to work with FG-NER. The LSTM+CNN+CRF model originally described by (Ma and Hovy, 2016) is for NER problem with few NE categories. It first uses Convolutional Neural Network (CNN) to learn character level embeddings in the training process. For NLP tasks, previous works have shown that CNN is likely to extract morphological features such as prefix and suffix effectively (Ma and Hovy, 715 2016; dos Santos and Guimar˜aes, 2015; Chiu and Nichols, 2016). The model then concatenates the character level embeddings with word embeddings to create a feature vector for each token in the input sentence. The input sentence is then fed to a BiLSTM network (Bi-directional Long-Short Term Memory network). Finally, CRF is used at the top layer of the BiLSTM to explore the correlations between outputs and jointly decode the best sequence of labels (i.e., NE categories). For both English and Japanese FG-NER task, we use pre-trained word embeddings as input for our models. Previous studies have shown that GloVe achieves the best performance for English NER"
C18-1060,W15-3904,0,0.0457865,"Missing"
C18-1060,N16-1030,0,0.0742374,"Missing"
C18-1060,P16-1101,0,0.488972,"lect the best model for different settings of training data size and target language. In FG-NER, because the number of NE categories is large, some categories might face with the data sparseness problem, whereas some other categories might have a large number of training samples in a dataset. Hence, it would be worth investigating the relation between dataset size and the performance of the system. The current state-of-the-art method for English NER (coarse-grained NER) is a neural network-based method, which uses convolutional neural network (CNN) to calculate the character level embeddings (Ma and Hovy, 2016). This leads to the question whether this method works well for languages with a large number of character types, such as Japanese. In this paper, we first investigate the relationship between the F-score of various FG-NER algorithms with the size of training datasets for both English and Japanese. Second, we suggest the direction to choose an appropriate FG-NER algorithm for appropriate target language and training data size. We show that the state-of-the-art method for English NER also performs well for English FG-NER. On the other hand, for Japanese FG-NER, the state-of-the-art method does"
C18-1060,W03-0430,0,0.0211231,"put for our models. Previous studies have shown that GloVe achieves the best performance for English NER task (Reimers and Gurevych, 2017). Consequently, we use the embeddings based on GloVe for English5 . For Japanese, we use pretrained word2vec6 embeddings. The vector dimension is 300 for English and 200 for Japanese. We use the default hyperparameters by Ma and Hovy (2016) in our model: learning rate = 0.01, batch size = 10 and decay rate = 0.09. 3.4 Incorporating dictionary information Dictionary information (gazetteer feature) has been proved to be efficient in many NER and FG-NER tasks (McCallum and Li, 2003; Sekine and Nobata, 2004; Yosef et al., 2012). While there are previous studies that use dictionary for CRF (McCallum and Li, 2003) or SVM (Yosef et al., 2012) in the NER/FGNER tasks, we believe that dictionary information would be useful in both sequence labelling and entity category disambiguation phase in the CRF+SVM method. Furthermore, the dictionary information can also be used in LSTM+CNN+CRF method. Consequently, we propose a method that efficiently utilizes dictionary information in the method LSTM+CNN+CRF and in both sequence labelling (CRF) and entity category disambiguation (SVM)"
C18-1060,W17-4114,0,0.0298835,"Missing"
C18-1060,D17-1035,0,0.0147056,"odel then concatenates the character level embeddings with word embeddings to create a feature vector for each token in the input sentence. The input sentence is then fed to a BiLSTM network (Bi-directional Long-Short Term Memory network). Finally, CRF is used at the top layer of the BiLSTM to explore the correlations between outputs and jointly decode the best sequence of labels (i.e., NE categories). For both English and Japanese FG-NER task, we use pre-trained word embeddings as input for our models. Previous studies have shown that GloVe achieves the best performance for English NER task (Reimers and Gurevych, 2017). Consequently, we use the embeddings based on GloVe for English5 . For Japanese, we use pretrained word2vec6 embeddings. The vector dimension is 300 for English and 200 for Japanese. We use the default hyperparameters by Ma and Hovy (2016) in our model: learning rate = 0.01, batch size = 10 and decay rate = 0.09. 3.4 Incorporating dictionary information Dictionary information (gazetteer feature) has been proved to be efficient in many NER and FG-NER tasks (McCallum and Li, 2003; Sekine and Nobata, 2004; Yosef et al., 2012). While there are previous studies that use dictionary for CRF (McCallu"
C18-1060,D11-1141,0,0.061483,"ed classification to recognize a music band name to answer the question “Which band was Paul in”, from the information shown in Figure 1. A fine-grained named entity recognition (FG-NER) model refers to a NER model that can recognize and classify a large number of entity categories (e.g., hundreds of NE categories). In classical coarse-grained named entity (NE) definition, often less than ten named entity categories are defined. For example, in the CoNLL-2003 Named Entity Recognition task, there are four NE categories: Person, Location, Organization and Miscellaneous (Sang and Meulder, 2003). Ritter et al. (2011) proposed a NER algorithm to recognize ten categories of entities from Twitter text. On the other hand, in FG-NER, there are hundreds of NE categories, which are fine-grained classification of coarse-grained categories. *) Equally contributed to the paper This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 711 Proceedings of the 27th International Conference on Computational Linguistics, pages 711–722 Santa Fe, New Mexico, USA, August 20-26, 2018. Paul, a former member of The Beatles, known for &quot;Let"
C18-1060,sekine-nobata-2004-definition,1,0.820793,"category Country and Organization Other. This is because the category Country is very easy to recognize, as there are only about 200 entities frequently used in this category, whereas, recognizing Organization Other or Car Stop is very difficult because of the ambiguity. This also indicates that the performance of an FG-NER system tends to depend on the categories and we can confirm this in the experimental results in the next sections. 3 Fine-Grained Named Entity Recognition Methods 3.1 Dictionary and Rule-based FG-NER The simplest method for FG-NER is using a dictionary and a set of rules. Sekine and Nobata (2004) presented a dictionary and rule-based Japanese FG-NER system that contains more than 1400 rules to recognize 140 entity categories. In this work, we added 200 rules to the existing 1400 rules by Sekine and Nobata to create a rule set of 1600 rules to classify 200 NE categories in the Sekine’s Extended Named Entity Hierarchy. We then built a rule-based Japanese FG-NER model to recognize 200 NE categories based on these 1600 rules. We use a Japanese FG-NER dictionary containing 1.6 million Wikipedia entities in this model. In the 1.6 million entities in the dictionary, only 70 thousand entities"
C18-1060,sekine-etal-2002-extended,1,0.613956,"egories from a knowledge base such as Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007), filtering out the categories with a small number of entities and merging the categories with similar semantic meaning into one FG-NER category (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). The second method is to manually build an entity hierarchy to cover important domains in the real world. Following the second method, Sekine et al. proposed an Extended Named Entity Hierarchy (ENEH), which contains 200 entity categories in a three-layer hierarchy, as shown in Figure 2 (Sekine et al., 2002; Sekine, 2008). In this paper, we use the entity hierarchy described by Sekine (2008), which contains 200 NE categories at the leaf-level, as our tag set3 . At the top level of the hierarchy, there are about twenty coarse-grained named entity categories, such as Person, Organization, Location, Facility, Product, Event, . . . Each top-level categories is further divided into several second-level categories as shown in Figure 2. Each second-level category is in turn divided into several leaf-level categories. We use this hierarchy because it is carefully designed by humans, it does not ignore i"
C18-1060,sekine-2008-extended,1,0.793193,"International Conference on Computational Linguistics, pages 711–722 Santa Fe, New Mexico, USA, August 20-26, 2018. Paul, a former member of The Beatles, known for &quot;Let It Be”, Paul, a former member of The Beatles, known for &quot;Let It Be”, Person Person Artifact Organization Org > Show_Org will be holding a concert at Carnegie Hall in New York. Location Location Product > Art > Music will be holding a concert at Carnegie Hall in New York. Facilty > GOE > Theatre (a) Named entity recognition result Location > GPE > City (b) Fine-grained NER result Figure 1: Example of NER and FG-NER For example, Sekine (2008) divided the coarse-grained category Organization into the fine-grained categories such as Political Party, Military, Sports Organization, Show Organization, as shown in Figure 2. Person International Org Family Government ... Location Organization ... Show Org Political Party Time Numx Political Org Cabinet Military Other Political Org Figure 2: Sekine’s Extended Named Entity (ENE) hierarchy FG-NER is still an open research domain, with little information concerning the state-of-the-art performance, the relation between training data size and performance, and how to select the best model for"
C18-1060,W02-2029,0,0.218657,"Missing"
C18-1060,P15-2048,0,0.023901,"eral SVM models (each for a top-level category) to classify the entities into the leaf-level categories. We use the following features for both SVM and CRF: bag-of-words, POS-tag, the number of digits in the word, the Brown cluster of the current word, the appearance of the word as a substring of a word in the Wikipedia ENE dictionary, the orthography features (the word is written in Kanji, Hiragana, Katakana or Romanji), is capital letter, and the last 2-3 characters. Those features are proved to be useful in previous work on named entity recognition (Ling and Weld, 2012; Yosef et al., 2012; Yogatama et al., 2015; Suzuki et al., 2016). Once we have the sequence labelling result, we have already known the surfaces and the top-level categories of the entities in the input sentence. We then use SVM to classify the entities into leaf-level categories. Because the number of leaf-level categories in each top-level categories is also not too large (e.g., less than 15), SVM can achieve a reasonable performance at this step. We also propose a method to incorporate dictionary information in both CRF and SVM step to improve the entire performance, as described in Section 3.4. 3.3 LSTM+CNN+CRF model for FG-NER We"
C18-1060,C12-2133,0,0.31149,"nd Datasets 2.1 FG-NER tag set The first challenge in FG-NER is defining a comprehensive tag set with a very large number of entity categories (Ling and Weld, 2012). There are two methods for defining a tag set (i.e., set of entity categories to recognize) in previous studies on FG-NER. The first method is to take the entity categories from a knowledge base such as Freebase (Bollacker et al., 2008) or YAGO (Suchanek et al., 2007), filtering out the categories with a small number of entities and merging the categories with similar semantic meaning into one FG-NER category (Ling and Weld, 2012; Yosef et al., 2012; Gillick et al., 2014). The second method is to manually build an entity hierarchy to cover important domains in the real world. Following the second method, Sekine et al. proposed an Extended Named Entity Hierarchy (ENEH), which contains 200 entity categories in a three-layer hierarchy, as shown in Figure 2 (Sekine et al., 2002; Sekine, 2008). In this paper, we use the entity hierarchy described by Sekine (2008), which contains 200 NE categories at the leaf-level, as our tag set3 . At the top level of the hierarchy, there are about twenty coarse-grained named entity categories, such as Perso"
C18-1060,P02-1060,0,0.404785,"Missing"
C92-2085,J86-3002,0,\N,Missing
C92-2085,A92-1014,1,\N,Missing
C92-2085,C90-1005,0,\N,Missing
C92-2085,A88-1019,0,\N,Missing
C92-2085,P91-1030,0,\N,Missing
C92-2085,P90-1032,0,\N,Missing
C96-2154,C88-1071,0,\N,Missing
C96-2154,H94-1011,0,\N,Missing
C96-2154,H92-1021,0,\N,Missing
C96-2154,H91-1057,0,\N,Missing
C96-2154,H89-1054,0,\N,Missing
D18-1453,D16-1203,0,0.087457,"Missing"
D18-1453,N18-1144,0,0.0185537,"atasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an"
D18-1453,P17-1168,0,0.139783,"whether this exposes any differences among the subsets. To investigate these two concerns, we manually annotate sample questions from each subset in terms of validity and required reasoning skills, such as word matching, knowledge inference, and multiple sentence reasoning. We examine 12 recently proposed MRC datasets (Table 1), which include answer extraction, description, and multiple-choice styles. We also observe differences based on these styles. For our baselines, we use two neural-based systems, namely, the Bidirectional Attention Flow (Seo et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017). In Section 5, we describe the advantages and disadvantages of different question styles with regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. A"
D18-1453,N18-2017,0,0.0586381,"Missing"
D18-1453,P16-1145,0,0.02858,"nswer extraction, description, and 1 All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingr"
D18-1453,P99-1042,0,0.140082,"ned sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC datasets. Consider the question shown in Figure 1, for example. Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context). I"
D18-1453,D17-1215,0,0.405835,"ng of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC datasets. Consider the question shown in Figure 1, for example. Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context). In other words, the question has only a single candidate answer. The system can solve it merely by recognizing the entity type required by when. In addition to this, even"
D18-1453,P17-1147,0,0.0383334,"h regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which ski"
D18-1453,N18-1023,0,0.0976911,"ults, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved"
D18-1453,Q18-1023,0,0.0646454,"Missing"
D18-1453,D17-1082,0,0.118876,"et into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets. • Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparis"
D18-1453,W04-1013,0,0.00928634,"It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets. Why we used different baseline systems: The multiple-choice style can be transformed to answer extraction, as mentioned in Clark et al. (2018). However, in some datasets, many questions have no textual overlap to determine the correct answer span in the context. Therefore, in order to avoid underestimating the baseline performance of those datasets, we used the GA system which is applicable to multiple choice questions. We scored the performance using exact match (EM)/F1 (Rajpurkar et al., 2016), Rouge-L (Lin, 2004), and accuracy for the answer extraction, description, and multiple-choice datasets, respectively (henceforth, we refer to these collectively as the score, for simplicity). For the description datasets, we determined in advance the answer span of the context that gives the highest Rouge-L score to the human-generated gold answer. We computed the Rouge-L score between 2 The ARC Easy and Challenge were collected using different methods; hence, we treated them as different datasets (see Clark et al. (2018) for further details). the predicted span and the gold answer.3 Reproduction of the baseline"
D18-1453,P11-2057,0,0.0719576,"Missing"
D18-1453,P18-2124,0,0.0286545,"ultiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets"
D18-1453,P18-1160,0,0.0649339,"Missing"
D18-1453,D16-1264,0,0.521837,"lls to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiplechoice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC. 1 Figure 1: Example from the SQuAD dataset (Rajpurkar et al., 2016). The baseline system can answer the token-limited question and, even if there are other candidate answers, it can easily attend to the answercontained sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI"
D18-1453,N16-1098,0,0.0584165,"Missing"
D18-1453,W17-0906,0,0.0162088,"ults suggest that one might overestimate recent advances in MRC. 1 Figure 1: Example from the SQuAD dataset (Rajpurkar et al., 2016). The baseline system can answer the token-limited question and, even if there are other candidate answers, it can easily attend to the answercontained sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC da"
D18-1453,D13-1020,0,0.317104,"ntences, only one sentence (i.e., s1 ) appears to be related to the question; thus, the system can easily determine the correct answer by attention, that is, by matching the words appearing both in the context and the ques4208 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208–4219 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tion. Therefore, this kind of question does not require a complex understanding of language—e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013). In Section 3, we define two heuristics, namely entity-type recognition and attention. We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence). Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; U"
D18-1453,P15-1024,0,0.0244362,"y et al., 2018; Khashabi et al., 2018). Nonetheless, the description style is difficult to evaluate because the Rouge-L and BLEU scores are insufficient for testing NLU. Whereas it is easy to evaluate the performance on multiple-choice questions, generating multiple reasonable options requires considerable effort. Interpretation of our heuristics: When we regard the MRC task as recognizing textual entailment (RTE) (Dagan et al., 2006), the task requires the reader to construct one or more premises from the context and form the most reasonable hypothesis from the question and candidate answer (Sachan et al., 2015). Thus, easier questions are those (i) where the reader needs to generate only one hypothesis, and (ii) where the premises directly describe the correct hypothesis. Our two heuristics can also be seen as the formalizations of these criteria. Therefore, to make questions more realistic, we need to create multiple hypotheses that require complex reasoning to be distinguished. Moreover, the integration of premises should be complemented by external knowledge to provide sufficient information to verify the correct hypothesis. 6 Related Work Our heuristics and annotation were motivated by unintende"
D18-1453,P18-1156,0,0.0157401,"s to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets. Why we used different baselin"
D18-1453,D16-1241,0,0.0358516,"along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a"
D18-1453,P10-1122,0,0.0650563,"Missing"
D18-1453,L18-1564,0,0.0175734,"d examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets. • Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparison with easy questions. • Compared to"
D18-1453,W17-0907,0,0.0253284,"Missing"
D18-1453,P16-1144,0,0.0228959,"nd 1 All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the"
D18-1453,P17-1075,1,0.886477,"Missing"
D18-1453,N18-1140,0,0.0171139,"github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the"
D18-1453,W17-2623,0,0.328136,"es and disadvantages of different question styles with regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and re"
D18-1453,K17-1028,0,0.0374338,"e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013). In Section 3, we define two heuristics, namely entity-type recognition and attention. We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence). Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; Using these simple heuristics, we split each dataset into easy and hard subsets for further investigation of the baseline performance. After conducting the experiments, we analyze the following two points in Section 4. First, we consider which questions are valid for testing, i.e., reasonably solvable. Second, we consider what reasoning skills are required and whether this exposes any differences among the subsets. To investigate these two concerns, we manually annotate sample questions from each subset in terms of val"
D18-1453,Q18-1021,0,0.0363293,"interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and"
D18-1453,W16-0103,0,0.049189,"Missing"
D19-1054,P18-1063,0,0.241541,"The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trained within the same objective, the generations are thus more faithful to the selected contents than Bottom-up methods. Our model is task-agnostic, end-to-end trainable and can be seamlessly inserted into any encoder-dec"
D19-1054,D18-1443,0,0.369633,"e supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the dec"
D19-1054,D16-1011,0,0.0681628,"Missing"
D19-1054,D18-1205,0,0.079091,"), then decode with pθ (Y |X, β). Source-text pairs are available for training, but the ground-truth content selection for each pair is unknown. 3.1 3.2 Soft-select falls back on a deterministic network to output the likelihood function’s first-order Taylor series approximation expanded at Eβ∼B(γ) β: Bottom-up log Eβ∼B(γ) pθ (Y |X, β) The most intuitive way is training the content selector to target some heuristically extracted contents. For example, we can train the selector to select overlapped words between the source and target (Gehrmann et al., 2018), sentences with higher tf-idf scores (Li et al., 2018) or identified image objects that appear in the caption (Wang et al., 2017). A standard encoder-decoder model is independently trained. In the testing stage, the prediction of the content selector is used to hard-mask the attention vector to guide the text generation in a bottom-up way. Though easy to train, Bottomup generation has the following two problems: (1) The heuristically extracted contents might be coarse and cannot reflect the variety of human languages and (2) The selector and decoder are independently trained towards different objectives thus might not adapt to each other well. β"
D19-1054,P18-2027,0,0.0154788,"and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardless of the low precision, so the controllability over generated text is weak. Fan et al. (2018) control the generation by manually conc"
D19-1054,P18-1013,0,0.0209586,"a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorith"
D19-1054,W17-4505,0,0.299395,"on weight will first be “soft-masked” by γ before being passed to the decoder. soft-select is fully differentiable and can be easily trained by gradient descent. However, this soft-approximation is normally inaccurate, especially when B(γ) has a high entropy, which is common in one-to-many text generation tasks. The gap between log Eβ∼B(γ) pθ (Y |X, β) and log pθ (Y |X, Eβ∼B(γ) ) will be large (Ma et al., 2017; Deng et al., 2018). In practice, this would lead to unrealistic generations when sampling β from the deterministically trained distribution. 3.3 Reinforce-Select Reinforce-select (RS) (Ling and Rush, 2017; Chen and Bansal, 2018) utilizes reinforcement learning to approximate the marginal likelihood. Specifically, it is trained to maximize a lower bound of the likelihood by applying the Jensen inequalily: log Eβ∼B(γ) pθ (Y |X, β) ≥ Eβ∼B(γ) log pθ (Y |X, β) The gradient to γ is approximated with MonteCarlo sampling by applying the REINFORCE algorithm (Williams, 1992; Glynn, 1990). To speed up convergence, we pre-train the selector by some distant supervision, which is a common practice in reinforcement learning. REINFORCE is unbiased but has a high variance. Many research have proposed sophistic"
D19-1054,D15-1166,0,0.0535494,"elector and generated text (Alemi et al., 2018; Zhao et al., 2018). A higher CMI leads to stronger controllability with a bit more risk of text disfluency. In summary, our contributions are (1) systematically studying the problem of controllable content selection for Enc-Dec text generation, (2) proposing a task-agnostic training framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is"
D19-1054,N16-1086,0,0.493898,"in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and g"
D19-1054,N19-1236,0,0.0404738,"ain the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the closest source word for each target word in the embedding space. Stop words and punctuations are prohibited from being selected. Choice of α/: As seen in Sec 3.5, we need to set the hyperparameter α for RS/SS and  for Most content selection models train the selector with heuristic rules (Hsu et al., 2018; Li et al., 2018; Yu et al., 2018; Gehrmann et al., 2018; Yao et al., 2019; Moryossef et al., 2019), which fail to fully capture the relation between selection and generation. Mei et al. (2016); Zhou et al. (2017); Lin et al. (2018); Li et al. (2018) “soft-select” word or sentence embeddings based on a gating function. The output score from the gate is a deterministic vector without any probabilistic variations, so controlling the selection to generate diverse text is impossible. Very few works explicitly define a bernoulli distribution for the selector, then train with the REINFORCE algorithm (Ling and Rush, 2017; Chen and Bansal, 2018), but the selection targets at a high recall regardles"
D19-1054,P17-1098,0,0.0234923,"ve concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e.g., data-to-text, summarization and image captioning, can be naturally divided into two steps: content selection and surface realization. The generations are supposed to have two levels of diversity: (1) content-level diversity reflecting multiple possibilities of content selection (what to say) and (2) surface-level diversity reflecting the linguistic variations of verbalizing the selected contents (how to say) (Reiter and Dale, 2000; Nema et al., 2017). Recent neural network models handle these tasks with the encoder-decoder (Enc-Dec) framework (Sutskever et al., 2014; Bahdanau et al., 2015), which simultaneously performs selecting and verbalizing in a 1. Bottom-up: Train a separate content selector to constrain the attention to source tokens (Gehrmann et al., 2018), but the separate training of selector/generator might lead to ∗ Work mostly done while at RIKEN AIP. Correspondence to xshen@mpi-inf.mpg.de 1 The source code is available on https://github. com/chin-gyou/controllable-selection 579 Proceedings of the 2019 Conference on Empirical"
D19-1054,P04-1011,0,0.221726,"Missing"
D19-1054,D14-1162,0,0.0812737,"Missing"
D19-1054,N19-1269,0,0.0384767,"Missing"
D19-1054,D18-1411,0,0.0685899,"elected : sri lankan, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many te"
D19-1054,D15-1044,0,0.0525735,"etup, then present the evaluation results. In practice, we can set  to adjust the degree of controllability we want. Later we will show it leads to a trade-off with performance. The final algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom"
D19-1054,P16-1162,0,0.0204755,"algorithm is detailed in Algorithm 1. To keep fairness, we trian RS and VRS with the same control variate and pre-training strategy.4 4 Experiments 5.1 Related Work Tasks and Setup We test content-selection models on the headline and data-to-text generation task. Both tasks share the same framework with the only difference of source-side encoders. Headline Generation: We use English Gigaword preprocessed by Rush et al. (2015), which pairs first sentences of news articles with their headlines. We keep most settings same as in Zhou et al. (2017), but use a vocabulary built by bytepair-encoding (Sennrich et al., 2016). We find it speeds up training with superior performance. Data-to-Text Generation: We use the Wikibio dataset (Lebret et al., 2016). The source is a Wikipedia infobox and the target is a one-sentence biography description. Most settings are the same as in Liu et al. (2018), but we use a bi-LSTM encoder for better performance. Heuristically extracted content: This is used to train the selector for bottom up models and pretrain the RS and VRS model. For wikibio, we simply extract overlapped words between the source and target. In Gigaword, as the headline is more abstractive, we select the clos"
D19-1054,D15-1199,0,0.0719588,"Missing"
D19-1054,D18-1356,0,0.0459964,"an, announced, closure, schools Text: sri lanka declares closure of schools. Table 1: Headline generation examples from our model. We can generate text describing various contents by sampling different content selections. The selected source word and its corresponding realizations in the text are highlighted with the same color. black-box way. Therefore, both levels of diversity are entangled within the generation. This entanglement, however, sacrifices the controllability and interpretability, making it diffifcult to specify the content to be conveyed in the generated text (Qin et al., 2018; Wiseman et al., 2018). With this in mind, this paper proposes decoupling content selection from the Enc-Dec framework to allow finer-grained control over the generation. Table 1 shows an example. We can easily modify the content selection to generate text with various focuses, or sample multiple paraphrases by fixing the content selection. Though there has been much work dealing with content selection for the Enc-Dec, none of them is able to address the above concerns properly. Current methods can be categorized into the following three classes and have different limits: Introduction Many text generation tasks, e."
D19-1054,C18-1091,0,0.133731,"raining framework achieving promising results and (3) introducing an effective way to achieve the trade-off between performance and controllability. 2 (1) dt is the hidden state of the decoder at time step t. f is a score function to compute the similarity between hi and dt (Luong et al., 2015). 3 Content Selection Our goal is to decouple the content selection from the decoder by introducing an extra content selector. We hope the content-level diversity can be fully captured by the content selector for a more interpretable and controllable generation process. Following Gehrmann et al. (2018); Yu et al. (2018), we define content selection as a sequence labeling task. Let β1 , β2 , . . . , βn denote a sequence of binary selection masks. βi = 1 if hi is selected and 0 otherwise. βi is assumed to be independent from each other and is sampled from a bernoulli distribution B(γi )2 . γi is the bernoulli parameter, which we estimate using a two-layer feedforward network on top of the source encoder. Text are generated by first sampling β from B(γ) to decide which content to cover, then decode with the conditional distribution pθ (Y |X, β). The text is expected to faithfully convey all selected contents an"
D19-1054,P17-1101,0,0.347238,"ge Processing and the 9th International Joint Conference on Natural Language Processing, pages 579–590, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2015; Xu et al., 2015). The encoder first encodes each xi into a vector hi . At each time step, the decoder pays attentions to some source embeddings and outputs the probability of the next token by p(yt |y1:t−1 , Ct ). Ct is a weighted average of source embeddings: X Ct = αt,i hi discrepancy when integrating them together. 2. Soft-select: Learn a soft mask to filter useless information (Mei et al., 2016; Zhou et al., 2017). However, the mask is deterministic without any probabilistic variations, making it hard to model the content-level diversity. 3. Reinforce-select: Train the selector with reinforcement learning (Chen and Bansal, 2018), which has high training variance and low diversity on content selection. i ef (hi ,dt ) αt,i = P f (h ,d ) j t je In this paper, we treat the content selection as latent variables and train with amortized variational inference (Kingma and Welling, 2014; Mnih and Gregor, 2014). This provides a lower training variance than Reinforce-select. The selector and generator are co-trai"
D19-1357,P81-1030,0,0.700947,"achine explanations of word meanings for human readers (Ni and Wang, 2017; Ishiwatari et al., 2019). The other is learning word embeddings from definitions to obtain semanticoriented word representations (Tissier et al., 2017; Bosc and Vincent, 2018). Although previous methods for encoding or decoding definitions using recurrent neural networks (RNNs) yielded promising results in definition modeling (Noraset et al., 2017; Bosc and Vincent, 2018), they did not explicitly utilize lexical semantic relations between defined words and defining words. Various lexical relations exist in definitions (Amsler, 1981), as displayed in Figure 1, where the defined word knife exhibits an Is-a relation with the defining words, tool and instrument, Has-a relation with edge, and Used-for relation with cutting. Utilizing structures of definitions about lexical semantic relations facilitates the understanding and generation of definitions. Based on this observation, we propose definition modeling methods that exploit lexical semantic relations between defined and defining words. However, lexical semantic relations in definitions are not explicit. To solve this problem, we use unsupervisedly learned word-pair embed"
D19-1357,D18-1181,0,0.42261,"h resources of semantic information for both humans and machines. Recent studies on definition modeling are primarily divided into two categories. One is definition generation, in which a definition is generated for a target word from its word representation. Definition generation involves analyzing word embeddings using generated definitions (Noraset et al., 2017) and machine explanations of word meanings for human readers (Ni and Wang, 2017; Ishiwatari et al., 2019). The other is learning word embeddings from definitions to obtain semanticoriented word representations (Tissier et al., 2017; Bosc and Vincent, 2018). Although previous methods for encoding or decoding definitions using recurrent neural networks (RNNs) yielded promising results in definition modeling (Noraset et al., 2017; Bosc and Vincent, 2018), they did not explicitly utilize lexical semantic relations between defined words and defining words. Various lexical relations exist in definitions (Amsler, 1981), as displayed in Figure 1, where the defined word knife exhibits an Is-a relation with the defining words, tool and instrument, Has-a relation with edge, and Used-for relation with cutting. Utilizing structures of definitions about lexi"
D19-1357,P18-2043,0,0.177165,"the aforementioned study, the LSTM conditional language model was used as a definition decoder to model this probability as follows: p (D|wtrg ) = T Y p (wt |wi<t , wtrg ) (3) t=1 p (wt |wi<t , wtrg ) = Sof tmax(W d h0t + bd ) (4) 0 that contributes to the definition generation at each step. As additional features, they used morphological information from a character-level convolutional neural network (CNN) to process a character sequence of the defined word and the embeddings of the hypernyms from the WebIsA database (Seitner et al., 2016). These features are called CH and HE, respectively. Gadetsky et al. (2018) introduced context-aware definition generation to disambiguate polysemous words with their context and generate the corresponding definitions. They extended Equation 3 to consider the context word sequence of the defined word C = {c1 , . . . , cm } as follows: where ht is a hidden state from the LSTM definition decoder. Sof tmax is the softmax function. They conditioned the decoder by providing the embedding of the defined word at the first step of the LSTM. They referred to this model as the Seed (S). Moreover, they extended this model to update the output of the recurrent unit with a gate f"
D19-1357,D16-1235,0,0.042866,"Missing"
D19-1357,J15-4004,0,0.388612,"cal semantic relations in definitions are not explicit. To solve this problem, we use unsupervisedly learned word-pair embeddings that represent semantic relations of word pairs based on co-occurring relational patterns in a corpus (Turney, 2005; Washio and Kato, 2018b). Experimental results show that our definition modeling methods improve previous models, with respect to both definition generation and the acquisition of word embeddings from definitions. 2 2.1 Background Definition Embedding Relationships between words captured in embeddings are studied in terms of similarity or relatedness (Hill et al., 2015). For example, coffee and cup have high relatedness because coffee is often contained in a cup; meanwhile, these words ex3521 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3521–3527, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics hibit low similarity as coffee is a beverage and cup is a container. Definition embeddings that are learned from word definitions are useful to capture similarity, while word embeddings based on distributiona"
D19-1357,P12-1092,0,0.0642151,"74.3 MT 65.0 67.1 60.2 67.6 WS353 47.8 63.5 66.8 65.4 Table 1: Spearman’s correlation coefficient ρ × 100 on the benchmarks. Context-agnostic (Noraset et al., 2017) Model PPL BLEU S+G+CH+HE 46.8 35.4 w/ Lrel 39.5 37.9 Context-aware (Gadetsky et al., 2018) Model PPL BLEU S+I-Attention 59.6 12.0 43.8 12.3 w/ Lrel Table 2: Perplexity and equally-weighted BLEU scores for up to 4-grams on the definition generation datasets SimLex333 (SL333) (Hill et al., 2015), SimVerb (SV) (Gerz et al., 2016), MEN (Bruni et al., 2014), RG (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2002), SCWS (Huang et al., 2012), and MTurk (Radinsky et al., 2011; Halawi et al., 2012). To evaluate the definition embeddings, we scored word pairs in the benchmarks using the cosine similarity between the corresponding definition embeddings and calculated Spearman’s correlation to the ground truth. The definitions in WordNet were used to train the definition encoder. The development sets of the SimVerb and MEN were used for the hyperparameter tuning. We implemented the CPAE (Section 2.1) as a baseline and compared it to CPAE with the wordpair embeddings (Section 3.1), which is our proposed method. The word embeddings in t"
D19-1357,N19-1350,0,0.314847,"rom WordNet (Fellbaum, 1998) and lexical semantic relations between the defined word and the defining words. Introduction Dictionary definitions are rich resources of semantic information for both humans and machines. Recent studies on definition modeling are primarily divided into two categories. One is definition generation, in which a definition is generated for a target word from its word representation. Definition generation involves analyzing word embeddings using generated definitions (Noraset et al., 2017) and machine explanations of word meanings for human readers (Ni and Wang, 2017; Ishiwatari et al., 2019). The other is learning word embeddings from definitions to obtain semanticoriented word representations (Tissier et al., 2017; Bosc and Vincent, 2018). Although previous methods for encoding or decoding definitions using recurrent neural networks (RNNs) yielded promising results in definition modeling (Noraset et al., 2017; Bosc and Vincent, 2018), they did not explicitly utilize lexical semantic relations between defined words and defining words. Various lexical relations exist in definitions (Amsler, 1981), as displayed in Figure 1, where the defined word knife exhibits an Is-a relation wit"
D19-1357,N19-1362,0,0.275778,"elevant dimensions of the embedding of the defined word. They referred to this model as Input Attention (I-Attention). 2.3 Word-Pair Embedding Word-pair embeddings represent relations of word pairs. Although representation of word pairs as the vector offsets of their pretrained word embeddings is a simple and powerful method (Mikolov et al., 2013), recent studies have shown that neural pattern-based word-pair embeddings are more effective than vector offsets in various tasks such as calculating relational similarity (Washio and Kato, 2018b), natural language inference, and question answering (Joshi et al., 2019). Neural pattern-based word-pair embedding models (Washio and Kato, 2018a,b; Joshi et al., 2019) unsupervisedly learn two neural networks: a word-pair encoder and pattern encoder, both of which encode the word-pair and lexico-syntactic pattern respectively into the same embedding space. These networks are trained by predicting co-occurrences between word-pairs and patterns in a corpus with the negative sampling objective. After the unsupervised learning, the wordpair encoder provides word-pair embeddings for any word pair given their word embeddings. 3 Method We propose methods that consider l"
D19-1357,I17-2070,0,0.195613,"finition of knife from WordNet (Fellbaum, 1998) and lexical semantic relations between the defined word and the defining words. Introduction Dictionary definitions are rich resources of semantic information for both humans and machines. Recent studies on definition modeling are primarily divided into two categories. One is definition generation, in which a definition is generated for a target word from its word representation. Definition generation involves analyzing word embeddings using generated definitions (Noraset et al., 2017) and machine explanations of word meanings for human readers (Ni and Wang, 2017; Ishiwatari et al., 2019). The other is learning word embeddings from definitions to obtain semanticoriented word representations (Tissier et al., 2017; Bosc and Vincent, 2018). Although previous methods for encoding or decoding definitions using recurrent neural networks (RNNs) yielded promising results in definition modeling (Noraset et al., 2017; Bosc and Vincent, 2018), they did not explicitly utilize lexical semantic relations between defined words and defining words. Various lexical relations exist in definitions (Amsler, 1981), as displayed in Figure 1, where the defined word knife exh"
D19-1357,P02-1040,0,0.103015,"Missing"
D19-1357,D14-1162,0,0.0867263,"set of stopwords. As in Section 3.1, we ignore the loss when wt is a stopword. This additional loss allows the definition decoder to learn the pattern of what semantic relations occur in definitions and how they occur. For example, if wtrg indicates a type of tools, a defining word that has the Is-a relation to wtrg tends to be followed by the Used-for word. 4 To obtain pattern-based word-pair embeddings, we extracted triples (w1 , w2 , p) ∈ T from the Wikipedia corpus, where (w1 , w2 ) is a word pair composed of nouns, verbs, or adjectives in the the 100K most frequent words of the GloVe 1 (Pennington et al., 2014), and p is the co-occurring shortest dependency path2 . We discarded the triples if p occurred less than five times and subsampled the triples based on word-pair probability with a threshold of 5 · 10−7 , following Joshi et al. (2019). For the word-pair encoder, we used the neural networks as follows: h(w1 ,w2 ) = M LP ([v w1 ; v w2 ; v w1 v w2 ]) (9) For Definition Decoder Lrel Obtaining Word-Pair Embedding (6) where ; denotes vector concatenation. To exclude meaningless relations between the defined word and functional words, we replace v (wtrg ,wt ) with the zero vector if wt is a stopword."
D19-1357,L16-1056,0,0.0318537,"obability of the defining word sequence D given the defined word wtrg . In the aforementioned study, the LSTM conditional language model was used as a definition decoder to model this probability as follows: p (D|wtrg ) = T Y p (wt |wi<t , wtrg ) (3) t=1 p (wt |wi<t , wtrg ) = Sof tmax(W d h0t + bd ) (4) 0 that contributes to the definition generation at each step. As additional features, they used morphological information from a character-level convolutional neural network (CNN) to process a character sequence of the defined word and the embeddings of the hypernyms from the WebIsA database (Seitner et al., 2016). These features are called CH and HE, respectively. Gadetsky et al. (2018) introduced context-aware definition generation to disambiguate polysemous words with their context and generate the corresponding definitions. They extended Equation 3 to consider the context word sequence of the defined word C = {c1 , . . . , cm } as follows: where ht is a hidden state from the LSTM definition decoder. Sof tmax is the softmax function. They conditioned the decoder by providing the embedding of the defined word at the first step of the LSTM. They referred to this model as the Seed (S). Moreover, they e"
D19-1357,D17-1024,0,0.0616152,"ry definitions are rich resources of semantic information for both humans and machines. Recent studies on definition modeling are primarily divided into two categories. One is definition generation, in which a definition is generated for a target word from its word representation. Definition generation involves analyzing word embeddings using generated definitions (Noraset et al., 2017) and machine explanations of word meanings for human readers (Ni and Wang, 2017; Ishiwatari et al., 2019). The other is learning word embeddings from definitions to obtain semanticoriented word representations (Tissier et al., 2017; Bosc and Vincent, 2018). Although previous methods for encoding or decoding definitions using recurrent neural networks (RNNs) yielded promising results in definition modeling (Noraset et al., 2017; Bosc and Vincent, 2018), they did not explicitly utilize lexical semantic relations between defined words and defining words. Various lexical relations exist in definitions (Amsler, 1981), as displayed in Figure 1, where the defined word knife exhibits an Is-a relation with the defining words, tool and instrument, Has-a relation with edge, and Used-for relation with cutting. Utilizing structures"
D19-1357,N18-1102,1,0.921935,"ment, Has-a relation with edge, and Used-for relation with cutting. Utilizing structures of definitions about lexical semantic relations facilitates the understanding and generation of definitions. Based on this observation, we propose definition modeling methods that exploit lexical semantic relations between defined and defining words. However, lexical semantic relations in definitions are not explicit. To solve this problem, we use unsupervisedly learned word-pair embeddings that represent semantic relations of word pairs based on co-occurring relational patterns in a corpus (Turney, 2005; Washio and Kato, 2018b). Experimental results show that our definition modeling methods improve previous models, with respect to both definition generation and the acquisition of word embeddings from definitions. 2 2.1 Background Definition Embedding Relationships between words captured in embeddings are studied in terms of similarity or relatedness (Hill et al., 2015). For example, coffee and cup have high relatedness because coffee is often contained in a cup; meanwhile, these words ex3521 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confe"
D19-1357,D18-1058,1,0.921371,"ment, Has-a relation with edge, and Used-for relation with cutting. Utilizing structures of definitions about lexical semantic relations facilitates the understanding and generation of definitions. Based on this observation, we propose definition modeling methods that exploit lexical semantic relations between defined and defining words. However, lexical semantic relations in definitions are not explicit. To solve this problem, we use unsupervisedly learned word-pair embeddings that represent semantic relations of word pairs based on co-occurring relational patterns in a corpus (Turney, 2005; Washio and Kato, 2018b). Experimental results show that our definition modeling methods improve previous models, with respect to both definition generation and the acquisition of word embeddings from definitions. 2 2.1 Background Definition Embedding Relationships between words captured in embeddings are studied in terms of similarity or relatedness (Hill et al., 2015). For example, coffee and cup have high relatedness because coffee is often contained in a cup; meanwhile, these words ex3521 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confe"
E99-1026,W98-1512,0,0.0661847,"Missing"
E99-1026,J96-1002,0,0.112055,". Because of the statistical property, we can incorporate a beam search, an effective way of limiting the search space in a backward analysis. 0 : otherwise. Here ""has(h,z)"" is a binary function which returns true if the history h has an attribute x. We focus on attributes on a bunsetsu itself and those between bunsetsus. Section 3 will mention these attributes. Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai. This allows us to compute the conditional probability as follows (Berger et al., 1996): P(flh) Y I iz~(h) a [ '(n'l) - ~,i I 2 The Probability Model Given a tokenization of a test corpus, the problem of dependency structure analysis in Japanese can be reduced to the problem of assigning one of two tags to each relationship which consists of two bunsetsus. A relationship could be tagged as ""0"" or ""1"" to indicate whether or not there is a dependency between the bunsetsus, respectively. The two tags form the space of ""futures"" for a maximum entropy formulation of our dependency problem between bunsetsus. A maximum entropy solution to this, or any other similar problem allows the c"
E99-1026,P96-1025,0,0.102655,"Missing"
E99-1026,W98-1511,0,0.439654,"consistent rules or assign consistent scores. • As syntactic characteristics differ across different domains, the rules have to be changed when the target domain changes. It is costly to create a new hand-made rule for each domain. At/other approach is a fully automatic corpusbased approach. This approach has the potential to overcome the problems of the rule-based approach. It automatically learns the likelihoods of dependencies from a tagged corpus and calculates the best dependencies for an input sentence. We take this approach. This approach is taken by some other systems (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et ah, 1998). The parser proposed by Ratnaparkhi (Ratnaparkhi, 1997) is considered to be one of the most accurate parsers in English. Its probability estimation is based on the maximum entropy models. We also use the maximum entropy model. This model learns the weights of given features from a training corpus. The weights are calculated based on the frequencies of the features in the training data. The set of features is defined by a human. In our model, we use features of bunsetsu, such as character strings, parts of speech, and inflection types of bunsetsu, as well as information be"
E99-1026,P98-1083,0,0.271188,"eriments and the results. Then we describe some interesting statistics that we found in our experiments. Finally, we compare our work with some related systems. 3.1 R e s u l t s o f E x p e r i m e n t s The features used in our experiments are listed in Tables 1 and 2. Each row in Table 1 contains a feature type, feature values, and an experimental result that will be explained later. Each feature consists of a type and a value. The features are basically some attributes of a bunsetsu itself or those between bunsetsus. We call them 'basic features.' The list is expanded from tIaruno's list (Haruno et al., 1998). The features in the list are classified into five categories that are related to the ""Head"" part of the anterior bunsetsu (category ""a""), the '~rype"" part of the anterior bunsetsu (category ""b""), the ""Head"" part of the posterior bunsetsu (category ""c""), the '~l~ype"" part of the posterior bunsetsu (category ""d""), and the features between bunsetsus (category ""e"") respectively. The term ""Head"" basically means a rightmost content word in a bunsetsu, and the term ""Type"" basically means a function word following a ""Head"" word or an inflection type of a ""Head"" word. The terms are defined in the fol"
E99-1026,W97-0301,0,0.0805237,"Missing"
E99-1026,C98-1080,0,\N,Missing
H01-1009,W98-1120,1,0.899463,"logical analyzer and NE-tagger. Then the system retrieves the relevant documents for the scenario as a relevant document set. The system, further, selects a set of relevant sentences as a relevant sentence set from those in the relevant document set. Finally, all the sentences in the relevant sentence set are parsed and the paths in the dependency tree are taken as patterns. 3.1 Document Preprocessing Morphological analysis and Named Entity (NE) tagging is performed on the training data at this stage. We used JUMAN [2] for the former and a NE-system which is based on a decision tree algorithm [5] for the latter. Also the part-of-speech information given by JUMAN is used in the later stages. 3.2 Document Retrieval The system first retrieves the documents that describe the events of the scenario of interest, called the relevant document set. A set of narrative sentences describing the scenario is selected to create a query for the retrieval. For this experiment, we set the size of the relevant document set to 300 and retrieved the documents using CRL’s stochastic-model-based IR system [3], which performed well in the IR task in IREX, Information Retrieval and Extraction evaluation proje"
H01-1009,A00-1039,1,0.509615,"ring and the common paths in the parse tree of relevant sentences are taken as extracted patterns. Keywords Information Extraction, Pattern Acquisition 1. INTRODUCTION Information Extraction (IE) systems today are commonly based on pattern matching. New patterns need to be written when we customize an IE system for a new scenario (extraction task); this is costly if done by hand. This has led to recent research on automated acquisition of patterns from text with minimal pre-annotation. Riloff [4] reported a successful result for her procedure that needs only a pre-classified corpus. Yangarber [6] developed a procedure for unannotated natural language texts. One of their common assumption is that the relevant documents include good patterns. Riloff implemented this idea by applying the pre-defined heuristic rules to pre-classified (relevant) documents and Yangarber advanced further so that the system can classify the documents by itself given seed patterns specific to a scenario and then find the best patterns from the relevant document set. Considering how they represent the patterns, we can see that, in general, Riloff and Yangarber relied on the sentence structure of English. Riloff"
I05-5011,W03-1609,1,\N,Missing
I05-5011,C92-2082,0,\N,Missing
I05-5011,P01-1008,0,\N,Missing
I05-5011,P04-1053,1,\N,Missing
I05-5011,P03-1029,1,\N,Missing
I05-5011,P02-1006,0,\N,Missing
I13-1190,S07-1012,1,0.834383,"Missing"
I13-1190,P08-1047,0,0.0343399,"Missing"
I13-1190,P12-1085,0,0.0212246,"Their method does not deal with attribute synonymy and problematic annotations in their training data. They evaluated the result of their extraction model only, while this paper reports on evaluation results of not only extraction models but also induced KBs and annotation methodology. In addition, their method employs LDA for generating word classes. This may involve the issue of scalability when running the method on large size real-world data. On the other hand, we employ a simple rule-based approach to induce the KBs. We can then straightforwardly apply the method on the large-scale data. Mauge et al. (2012) also proposed methods to extract product attribute-values using hand-crafted patterns, and to group synonymous attributes in a supervised manner. They, however, only evaluated a part of the extracted attribute names, and aggregated synonymous attribute names. They did not evaluate the extracted attribute values. Our work is also similar to Ghani et al. (2006), who construct an annotated corpus using a manually tailored KB and then train models using the corpus to extract attribute values. Probst et al. (2007) and Putthividhya and Hu (2011) also proposed a similar approach with the work of Gha"
I13-1190,P09-1113,0,0.0432248,"ls. In particular, product attributes and their values (PAVs) are crucial for many applications such as faceted navigation and recommendation. However, since structured information is not always provided by the merchants, it is important to build technologies to create this structured information (such as PAVs) from unstructured data (such as a product description). Because of the wide variety of product types, such technology should not rely on a manually annotated corpus. One of the promising methods for information extraction (IE) without a manually annotated corpus is distant supervision (Mintz et al., 2009), which leverages an existing knowledge base (KB) such as Wikipedia or Freebase to annotate texts using the KB instances. These popular KBs, however, are not very helpful for distant supervision in an e-commerce domain for the following reasons. (1) An infobox in a Wikipedia article is not always tailored towards e-commerce. For instance, as of May 2013, the infobox attributes of wine in English Wikipedia included energy, carbohydrates, fat, protein and alcohol1 . These are not particularly useful for users seeking their favorite wines through online shopping. Instead, the grape variety, produ"
I13-1190,D11-1144,0,0.205411,"Missing"
I13-1190,P99-1014,0,0.0573961,"Missing"
I13-1190,W98-1120,1,0.471392,"フルボディ[23] (full body) 153 pages. Values in the KB are simply annotated for the pages. Then, sentences possibly including incorrect annotations or sentences where annotation are missing are automatically ﬁltered out from the annotated pages to improve annotation quality. 4.2.1 Attribute Value Annotation All given product pages are split into sentences following block-type HTML tags, punctuation, and brackets. Each sentence is then tokenized by a morphological analyzer4 . The longest attribute value matching a sub-sequence of the token sequence is annotated. We employed the Start/End tag model (Sekine et al., 1998) as chunk tags for the matched sequence. If the matched value corresponds to more than one attribute, the entry with the largest MF is selected for annotation. Note that if other attribute values are contained in the matched sub-sequence, they are not annotated. 4.2.2 Incorrect Annotation Filtering Some attribute values with low MFs are likely to be incorrect. The quality of the corpus, and the performance of extraction models based on the corpus deteriorate if such values are frequently annotated. We detect incorrect value annotation in the corpus according to the assumption that attribute va"
I13-1190,N04-1010,1,0.833722,"Missing"
lin-etal-2010-new,N04-1043,0,\N,Missing
lin-etal-2010-new,sekine-dalwani-2010-ngram,1,\N,Missing
lin-etal-2010-new,C08-3010,1,\N,Missing
lin-etal-2010-new,1999.tc-1.8,0,\N,Missing
lin-etal-2010-new,J93-2004,0,\N,Missing
lin-etal-2010-new,S07-1044,0,\N,Missing
lin-etal-2010-new,N07-2005,1,\N,Missing
lin-etal-2010-new,J92-4003,0,\N,Missing
lin-etal-2010-new,J03-3005,0,\N,Missing
lin-etal-2010-new,P08-1068,0,\N,Missing
lin-etal-2010-new,P03-1059,0,\N,Missing
lin-etal-2010-new,A00-1031,0,\N,Missing
lin-etal-2010-new,P01-1005,0,\N,Missing
lin-etal-2010-new,W05-0603,0,\N,Missing
lin-etal-2010-new,P09-1116,1,\N,Missing
lin-etal-2010-new,Y09-1024,1,\N,Missing
lin-etal-2010-new,U08-1008,0,\N,Missing
lin-etal-2010-new,I05-2018,0,\N,Missing
lin-etal-2010-new,W04-3205,0,\N,Missing
lin-etal-2010-new,J03-3001,0,\N,Missing
lin-etal-2010-new,U07-1008,0,\N,Missing
M98-1019,W97-0312,0,0.28839,", current and following tokens. The three types of information are the part-of-speech, character type and special dictionary information described above. If we just use the deterministic decision created by the tree, it could cause a problem in the running phase. Because the decisions are made locally, the system could make an inconsistent sequence of decisions overall. For example, one token could be tagged as the opening of an organization, while the next token might be tagged as the closing of person name. We can think of several strategies to solve this problem (for example, the method by [2] will be described in a later section), but we used a probabilistic method. The instances in the training corpus corresponding to a leaf of the decision tree may not all have the same tag. At a leaf we don&apos;t just record the most probable tag; rather, we keep the probabilities of the all possible tags for that leaf. In this way we can salvage cases where a tag is part of the most probable globally-consistent tagging of the text, even though it is not the most probable tag for this token, and so would be discarded if we made a deterministic decision at each token. subsectionRunning Phase In the"
M98-1019,A97-1029,0,0.180277,"Missing"
M98-1019,C96-1072,0,0.154341,"Missing"
moreno-etal-2000-treebank,J93-2004,0,\N,Missing
moreno-etal-2000-treebank,A97-1014,0,\N,Missing
N03-4013,H01-1009,1,0.89314,"Missing"
N06-1039,A00-2018,0,0.0281631,"Missing"
N06-1039,P04-1053,1,0.544888,"among its columns. In this example, the obtained table is just what an IE system (whose task is to find a hurricane name and the affected place) would create. However, these articles might also include other things, which could represent different relations. For example, the governments might call for help or some casualties might have been reported. To obtain such relations, we need to choose different entities from the articles. Several existing works have tried to extract a certain type of relation by manually choosing different pairs of entities (Brin, 1998; Ravichandran and Hovy, 2002). Hasegawa et al. (2004) tried to extract multiple relations by choosing entity types. We assume that we can find such relations by trying all possible combinations from a set of entities we have chosen in advance; some combinations might represent a hurricane and government relation, and others might represent a place and its casualties. To ensure that an article can have several different relations, we let each article belong to several different clusters. In a real-world situation, only using basic patterns sometimes gives undesired results. For example, “(President) Bush flew to Texas” and “(Hurricane) Katrina fl"
N06-1039,W01-1511,0,0.012075,"rns is crucial to performance. We think a basic pattern should be somewhat specific, since each pattern should capture an entity with some relevant context. But at the same time a basic pattern should be general enough to reduce data sparseness. We choose a predicate-argument structure as a natural solution for this problem. Compared to traditional constituent trees, a predicate-argument structure is a higher-level representation of sentences that has gained wide acceptance from the natural language community recently. In this paper we used a logical feature structure called GLARF proposed by Meyers et al. (2001a). A GLARF converter takes a syntactic tree as an input and augments it with several ’s ... Newspapers SUFFIX Louisiana Web Crawling T-POS Katrina coast SBJ Basic Clustering OBJ ... hit Figure 3: GLARF structure of the sentence “Katrina hit Louisiana’s coast.” features. Figure 3 shows a sample GLARF structure obtained from the sentence “Katrina hit Louisiana’s coast.” We used GLARF for two reasons: first, unlike traditional constituent parsers, GLARF has an ability to regularize several linguistic phenomena such as participial constructions and coordination. This allows us to handle this synt"
N06-1039,P02-1006,0,0.0982712,"ble while retaining a relation among its columns. In this example, the obtained table is just what an IE system (whose task is to find a hurricane name and the affected place) would create. However, these articles might also include other things, which could represent different relations. For example, the governments might call for help or some casualties might have been reported. To obtain such relations, we need to choose different entities from the articles. Several existing works have tried to extract a certain type of relation by manually choosing different pairs of entities (Brin, 1998; Ravichandran and Hovy, 2002). Hasegawa et al. (2004) tried to extract multiple relations by choosing entity types. We assume that we can find such relations by trying all possible combinations from a set of entities we have chosen in advance; some combinations might represent a hurricane and government relation, and others might represent a place and its casualties. To ensure that an article can have several different relations, we let each article belong to several different clusters. In a real-world situation, only using basic patterns sometimes gives undesired results. For example, “(President) Bush flew to Texas” and"
N06-1039,P03-1029,1,0.788523,"this risk. An IE task can be defined as finding a relation among several entities involved in a certain type of event. For example, in the MUC-6 management succession scenario, one seeks a relation between COMPANY, PERSON and POST involved with hiring/firing events. For each row of an extracted table, you can always read it as “COMPANY hired (or fired) PERSON for POST.” The relation between these entities is retained throughout the table. There are many existing works on obtaining extraction patterns for pre-defined relations (Riloff, 1996; Yangarber et al., 2000; Agichtein and Gravano, 2000; Sudo et al., 2003). Unrestricted Relation Discovery is a technique to automatically discover such relations that repeatedly appear in a corpus and present them as a table, with absolutely no human intervention. Unlike most existing IE research, a user does not specify the type of articles or information wanted. Instead, a system tries to find all the kinds of relations that are reported multiple times and can be reported in tabular form. This technique will open up the possibility of trying new IE scenarios. Furthermore, the system itself can be used as an IE system, since an obtained relation is already presen"
N06-1039,A00-1039,0,0.00585083,"tion is easily obtained in advance would help reduce this risk. An IE task can be defined as finding a relation among several entities involved in a certain type of event. For example, in the MUC-6 management succession scenario, one seeks a relation between COMPANY, PERSON and POST involved with hiring/firing events. For each row of an extracted table, you can always read it as “COMPANY hired (or fired) PERSON for POST.” The relation between these entities is retained throughout the table. There are many existing works on obtaining extraction patterns for pre-defined relations (Riloff, 1996; Yangarber et al., 2000; Agichtein and Gravano, 2000; Sudo et al., 2003). Unrestricted Relation Discovery is a technique to automatically discover such relations that repeatedly appear in a corpus and present them as a table, with absolutely no human intervention. Unlike most existing IE research, a user does not specify the type of articles or information wanted. Instead, a system tries to find all the kinds of relations that are reported multiple times and can be reported in tabular form. This technique will open up the possibility of trying new IE scenarios. Furthermore, the system itself can be used as an IE sys"
N06-2034,P02-1047,0,0.591589,"Missing"
N06-2034,J05-2005,0,\N,Missing
N12-3004,D11-1024,0,0.0466894,"Missing"
N12-3004,N10-1012,0,\N,Missing
nobata-etal-2002-summarization,P98-1009,0,\N,Missing
nobata-etal-2002-summarization,C98-1009,0,\N,Missing
nobata-etal-2002-summarization,H01-1009,1,\N,Missing
nobata-etal-2002-summarization,A00-1039,1,\N,Missing
P00-1044,A97-1028,0,\N,Missing
P00-1044,O97-1012,0,\N,Missing
P03-1029,sekine-etal-2002-extended,1,\N,Missing
P03-1029,H01-1009,1,\N,Missing
P03-1029,A00-1039,1,\N,Missing
P03-1061,J96-1002,0,0.0122742,"a sentence. A string is tagged with a 1 or a 0 to indicate whether it is a morpheme. When a string is a morpheme, a grammatical attribute is assigned to it. A tag designated as a 1 is thus assigned one of a number, n, of grammatical attributes assigned to morphemes, and the problem becomes to assign an attribute (from 0 to n) to every string in a given sentence. We define a model that estimates the likelihood that a given string is a morpheme and has a grammatical attribute i(1 ≤ i ≤ n) as a morpheme model. We implemented this model within an ME modeling framework (Jaynes, 1957; Jaynes, 1979; Berger et al., 1996). The model is represented by Eq. (1): pλ (a|b) = exp  i,j λi,j gi,j (a, b) Zλ (b)  (1) Word 形態 (form) Short word Pronunciation POS Others ケータイ(keitai) Noun Word 形態素解析 (morphological 素 (element) ソ (so) Suffix 解析 (analysis) カイセキ(kaiseki) Noun に ニ (ni) PPP case marker について Verb KA-GYO, ADF, euphonic change PPP conjunctive Prefix Long word Pronunciation POS ケー タ イ ソ カ イ セ(keitaisokaiseki) Noun analysis) キ (about) ニツイテ (nitsuite) オハナシシタシ (ohanashiitasi) Verb つい (relate) ツイ (tsui) て お テ オ (te) (o) 話し (talk) いたし(do) ます ハナシ イタシ マス (hanashi) Verb SA-GYO, ADF (itashi) Verb SA-GYO, ADF (masu) AUX end"
P03-1061,maekawa-etal-2000-spontaneous,1,0.907195,"Missing"
P03-1061,C96-2202,0,0.498087,"tioned in Section 1, tagging the whole of the CSJ manually would be difficult. Therefore, we are taking a semi-automatic approach. This section describes major problems in tagging a large spontaneous speech corpus with high precision in a semiautomatic way, and our solutions to those problems. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to find unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and"
P03-1061,P99-1036,0,0.44756,"s section describes major problems in tagging a large spontaneous speech corpus with high precision in a semiautomatic way, and our solutions to those problems. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to find unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure (Uchimoto et al., 2002). That is much lower than the accuracy obtained by manual taggin"
P03-1061,W01-0512,1,0.862115,"ns to those problems. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to find unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure (Uchimoto et al., 2002). That is much lower than the accuracy obtained by manual tagging. Several problems led to this inaccuracy. In the following, we describe these problems and our solutions to them. • Fillers and disfluencies Fille"
P03-1061,C02-2019,1,0.560951,"that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure (Uchimoto et al., 2002). That is much lower than the accuracy obtained by manual tagging. Several problems led to this inaccuracy. In the following, we describe these problems and our solutions to them. • Fillers and disfluencies Fillers and disfluencies are characteristic expressions often used in spoken language, but they are randomly inserted into text, so detecting their segmentation is difficult. In the CSJ, they are tagged manually. Therefore, we first delete fillers and disfluencies and then put them back in their original place after analyzing a text. • Accuracy for unknown words The morpheme model that will"
P03-1061,W99-0606,0,\N,Missing
P03-1061,A00-2020,0,\N,Missing
P04-1053,sekine-etal-2002-extended,1,\N,Missing
P04-1053,P02-1006,0,\N,Missing
P04-1053,W02-1010,0,\N,Missing
P06-2094,P04-1053,1,0.217894,") but with some sacrifice of precision. Table 2 shows the correctness evaluation results. We randomly select 100 table rows among the topics which were judged “very useful” or “useful”, and determine the correctness of the information by reading the newspaper articles the information was extracted from. Out of 100 rows, 84 rows have correct information in all slots. 4 735 first proposed by (Aone and Ramos-Santacruz 00) and the ACE evaluations of event detection follow this line (ACE Home Page). An unsupervised learning method has been applied to a more restricted IE task, Relation Discovery. (Hasegawa et al. 2004) used large corpora and an Extended Named Entity tagger to find novel relations and their participants. However, the results are limited to a pair of participants and because of the nature of the procedure, the discovered relations are static relations like a country and its presidents rather than events. Topic-oriented summarization, currently pursued by the DUC evaluations (DUC Home Page), is also closely related. The systems are trying to create summaries based on the specified topic for a manually prepared set of documents. In this case, if the result is suitable to present in table format"
P06-2094,sekine-etal-2002-extended,1,0.475478,"Missing"
P06-2094,I05-5011,1,0.273111,"Missing"
P06-2094,C00-2136,0,\N,Missing
P06-2094,W03-0509,1,\N,Missing
P06-2094,P01-1008,0,\N,Missing
P06-2094,P03-1029,1,\N,Missing
P06-2094,A00-1011,0,\N,Missing
P06-2094,C96-1079,0,\N,Missing
P06-2094,A00-1039,0,\N,Missing
P07-2005,P06-2094,1,0.804643,"Missing"
P07-2005,N06-1039,1,0.888304,"Missing"
P11-1053,J92-4003,0,0.445595,"Missing"
P11-1053,H05-1091,0,0.917,"c parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simp"
P11-1053,C10-1018,0,0.862302,"We study the impact of the size of training data on cluster features and analyze the performance improvements through an extensive experimental study. The rest of this paper is organized as follows: Section 2 presents related work and Section 3 provides the background of the relation extraction task and the word clustering algorithm. Section 4 describes in detail a state-of-the-art supervised baseline system. Section 5 describes the clusterbased features and the cluster selection methods. We present experimental results in Section 6 and conclude in Section 7. Though Boschee et al. (2005) and Chan and Roth (2010) used word clusters in relation extraction, they shared the same limitation as the above approaches in choosing clusters. For example, Boschee et al. (2005) chose clusters of different granularities and Chan and Roth (2010) simply used a single threshold for cutting the word hierarchy. Moreover, Boschee et al. (2005) only augmented the predicate (typically a verb or a noun of the most importance in a relation in their definition) with word clusters while Chan and Roth (2010) performed this for any lexical feature consisting of a single word. In this paper, we systematically explore the effecti"
P11-1053,N07-1015,0,0.863911,"upervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical"
P11-1053,P04-3022,0,0.104269,"lation between the entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of"
P11-1053,P08-1068,0,0.0680594,"this experiment. The training documents for each size setup were split into a real training set and a development set in a ratio of 7:3 for selecting clusters. There are some clear trends in Table 8. Under each training size, PC4 consistently outperformed the baseline and the system Prefix10 for relation classification. For PC4, the gain for classification was more pronounced than detection. The mixed detection results of Prefix10 indicated that only using a single prefix may not be stable. We did not observe the same trend in the reduction of annotation need with cluster-based features as in Koo et al. (2008) for dependency parsing. PC4 with sizes 50, 125, 175 outperformed the baseline with sizes 75, 175, 225 respectively. But this was not the case when PC4 was tested with sizes 75 and 225. This might due to the complexity of the relation extraction task. 6.5 Analysis There were on average 69 cross-type errors in the baseline in Section 6.1 which were reduced to 56 528 by using PC4. Table 9 showed that most of the improvements involved EMP-ORG, GPE-AFF, DISC, ART and OTHER-AFF. The performance gain for PER-SOC was not as pronounced as the other five types. The five types of relations are ambiguous"
P11-1053,P09-1116,0,0.0257204,"Missing"
P11-1053,A00-2030,0,0.0270118,"ract an Employment relation between the entities US soldier and US in the phrase US soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. T"
P11-1053,N04-1043,0,0.146331,"clusters of various granularities for feature the previous work, this paper only focuses on decoding. Ratinov and Roth (2009) and Turian et relation extraction of major types. al. (2010) also explored this approach for name Given a relation instance x  ( s, mi , m j ) , where tagging. Though all of them used the same mi and m j are a pair of mentions and s is the hierarchical word clustering algorithm for the task sentence containing the pair, the goal is to learn a of name tagging and reported improvements, we function which maps the instance x to a type c, noticed that the clusters used by Miller et al. (2004) where c is one of the 7 defined relation types or the were quite different from that of Ratinov and Roth type Nil (no relation exists). There are two (2009) and Turian et al. (2010). To our knowledge, commonly used learning paradigms for relation there has not been work on selecting clusters in a extraction: Flat: This strategy performs relation detection principled way. We move a step further to explore and classification at the same time. One multi-class several methods in choosing effective clusters. A classifier is trained to discriminate among the 7 second difference between this work an"
P11-1053,C08-1088,0,0.27258,"nd semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting the relation. This mo"
P11-1053,W09-1119,0,0.0160139,"roper name), NOM al. (2004), who augmented name tagging training (nominal) or PRO (pronoun). A relation was data with hierarchical word clusters generated by defined over a pair of entity mentions within a the Brown clustering algorithm (Brown et al., 1992) single sentence. The 7 major relation types with from a large unlabeled corpus. They used different examples are shown in Table 1. ACE 2004 also thresholds to cut the word hierarchy to obtain defined 23 relation subtypes. Following most of clusters of various granularities for feature the previous work, this paper only focuses on decoding. Ratinov and Roth (2009) and Turian et relation extraction of major types. al. (2010) also explored this approach for name Given a relation instance x  ( s, mi , m j ) , where tagging. Though all of them used the same mi and m j are a pair of mentions and s is the hierarchical word clustering algorithm for the task sentence containing the pair, the goal is to learn a of name tagging and reported improvements, we function which maps the instance x to a type c, noticed that the clusters used by Miller et al. (2004) where c is one of the 7 defined relation types or the were quite different from that of Ratinov and Roth"
P11-1053,R09-2014,1,0.824651,"pes of patterns: 1) the sequence of the tokens between the two mentions as used in Boschee et al. (2005); 2) the sequence of the heads of the constituents between the two mentions as used by Grishman et al. (2005); 3) the shortest dependency path between the two mentions in a dependency tree as adopted by Bunescu and Mooney (2005a). These patterns can provide more structured information of how the two mentions are connected. Title list: This is tailored for the EMP-ORG type of relations as the head of one of the mentions is usually a title. The features are decoded in a way similar to that of Sun (2009). Position Before M1 Between Feature BM1F BM1L WM1 HM1 WBNULL WBFL WBF WBL WBO M2 M12 After WM2 HM2 HM12 AM2F AM2L Description first word before M1 second word before M1 bag-of-words in M1 head3 word of M1 when no word in between the only word in between when only one word in between first word in between when at least two words in between last word in between when at least two words in between other words in between except first and last words when at least three words in between bag-of-words in M2 head word of M2 combination of HM1 and HM2 first word after M2 second word after M2 Table 3: Le"
P11-1053,C10-2137,1,0.534869,"Missing"
P11-1053,P10-1040,0,0.116661,"010) also explored this approach for name Given a relation instance x  ( s, mi , m j ) , where tagging. Though all of them used the same mi and m j are a pair of mentions and s is the hierarchical word clustering algorithm for the task sentence containing the pair, the goal is to learn a of name tagging and reported improvements, we function which maps the instance x to a type c, noticed that the clusters used by Miller et al. (2004) where c is one of the 7 defined relation types or the were quite different from that of Ratinov and Roth type Nil (no relation exists). There are two (2009) and Turian et al. (2010). To our knowledge, commonly used learning paradigms for relation there has not been work on selecting clusters in a extraction: Flat: This strategy performs relation detection principled way. We move a step further to explore and classification at the same time. One multi-class several methods in choosing effective clusters. A classifier is trained to discriminate among the 7 second difference between this work and the above relation types plus the Nil type. ones is that we utilize word clusters in the task of Hierarchical: This one separates relation relation extraction which is very differe"
P11-1053,P06-1104,0,0.210243,"racts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are cruci"
P11-1053,P05-1052,1,0.946513,"sed method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the w"
P11-1053,P05-1053,0,0.911563,"soldier. Current supervised approaches for tackling this problem, in general, fall into two categories: feature based and kernel based. Given an entity pair and a sentence containing the pair, both approaches usually start with multiple level analyses of the sentence such as tokenization, partial or full syntactic parsing, and dependency parsing. Then the feature based method explicitly extracts a variety of lexical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by"
P11-1053,D07-1076,0,0.435276,"exical, syntactic and semantic features for statistical learning, either generative or discriminative (Miller et al., 2000; Kambhatla, 2004; Boschee et al., 2005; Grishman et al., 2005; Zhou et al., 2005; Jiang and Zhai, 2007). In contrast, the kernel based method does not explicitly extract features; it designs kernel functions over the structured sentence representations (sequence, dependency or parse tree) to capture the similarities between different relation instances (Zelenko et al., 2003; Bunescu and Mooney, 2005a; Bunescu and Mooney, 2005b; Zhao and Grishman, 2005; Zhang et al., 2006; Zhou et al., 2007; Qian et al., 2008). Both lines of work depend on effective features, either explicitly or implicitly. The performance of a supervised relation extraction system is usually degraded by the sparsity of lexical features. For example, unless the example US soldier has previously been seen in the training data, it would be difficult for both the feature based and the kernel based systems to detect whether there is an Employment relation or not. Because the syntactic feature of the phrase US soldier is simply a noun-noun compound which is quite general, the words in it are crucial for extracting t"
P11-1053,P08-1000,0,\N,Missing
P11-2010,H05-1120,0,0.0976296,"pages 53–57, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics α s: t: i flextime furekkusutaimu β i Figure 1: Minimum edit operation sequence in the alphabeta model (Underlined letters are match operations) using lists of Western names with mixed languages. The results showed that the proposed model achieves higher accuracy than conventional models without latent classes. Related researches include Llitjos and Black (2001), where it is shown that source language origins may improve the pronunciation of proper nouns in text-to-speech systems. Another one by Ahmad and Kondrak (2005) estimates character-based error probabilities from query logs via the EM algorithm. This model is less general than ours because it only deals with character-based error probability. 2 Alpha-Beta Model We adopted the alpha-beta model (Brill and Moore, 2000), which directly models the string substitution probabilities of transliterated pairs, as the base model in this paper. This model is an extension to the conventional edit distance, and gives probabilities to general string substitutions in the form of α → β (α, β are strings of any length). The whole probability of rewriting word s with t"
P11-2010,P00-1037,0,0.825599,"proposed model can achieve higher accuracy compared to the conventional models without latent classes. 1 Introduction Transliteration (e.g., “バラクオバマ baraku obama / Barak Obama”) is phonetic translation between languages with different writing systems. Words are often transliterated when imported into differet languages, which is a major cause of spelling variations of proper nouns in Japanese and many other languages. Accurate transliteration is also the key to robust machine translation systems. Phonetic-based rewriting models (Knight and Jonathan, 1998) and spelling-based supervised models (Brill and Moore, 2000) have been proposed for 53 recognizing word-to-word transliteration correspondence. These methods usually learn a single model given a training set. However, single models cannot deal with words from multiple language origins. For example, the “get” parts in “piaget / ピアジェ piaje” (French origin) and “target / ターゲット t¯agetto” (English origin) may differ in how they are transliterated depending on their origins. Li et al. (2007) tackled this issue by proposing a class transliteration model, which explicitly models and classifies origins such as language and genders, and switches corresponding tr"
P11-2010,N09-1022,1,0.86243,"When W = 2, for example, the first non-match operation ε →u is merged with one operation on the left and right, producing f→fu and l→ur. Finally, substitution probabilities are calculated as relative frequencies of all substitution operations created in this way. Note that the minimum edit operation sequence is not unique, so we take the averaged frequencies of all the possible minimum sequences. 3 Class Transliteration Model The alpha-beta model showed better performance in tasks such as spelling correction (Brill and Moore, 2000), transliteration (Brill et al., 2001), and query alteration (Hagiwara and Suzuki, 2009). However, the substitution probabilities learned by this model are simply the monolithic average of training set statistics, and cannot be switched depending on the source language origin of given pairs, as explained in Section 1. Li et al. (2007) pointed out that similar problems arise in Chinese. Transliteration of Indo-European names such as “亜歴山大 / Alexandra” can be addressed by Mandarin pronunciation (Pinyin) “Ya-LiShan-Da,” while Japanese names such as “山本 / Yamamoto” can only be addressed by considering the Japanese pronunciation, not the Chinese pronunciation “Shan-Ben.” Therefore, Li"
P11-2010,P07-1016,0,0.467673,"literation is also the key to robust machine translation systems. Phonetic-based rewriting models (Knight and Jonathan, 1998) and spelling-based supervised models (Brill and Moore, 2000) have been proposed for 53 recognizing word-to-word transliteration correspondence. These methods usually learn a single model given a training set. However, single models cannot deal with words from multiple language origins. For example, the “get” parts in “piaget / ピアジェ piaje” (French origin) and “target / ターゲット t¯agetto” (English origin) may differ in how they are transliterated depending on their origins. Li et al. (2007) tackled this issue by proposing a class transliteration model, which explicitly models and classifies origins such as language and genders, and switches corresponding transliteration model. This method requires training sets of transliterated word pairs with language origin. However, it is difficult to obtain such tagged data, especially for proper nouns, a rich source of transliterated words. In addition, the explicitly tagged language origins are not necessarily helpful for loanwords. For example, the word “spaghetti” (Italian origin) can also be found in an English dictionary, but applying"
P11-2010,J98-4003,0,\N,Missing
P13-2033,C04-1066,0,0.0268168,"h in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ID 1 2 3* 4* 5* 6* 7 8* 9* 10* 11* 12 depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF conﬁdence to detect new words. For oﬄine approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurre"
P13-2033,W10-3606,0,0.0288641,"m a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omission problem in Urdu compound w"
P13-2033,P04-1021,0,0.0452383,"t it appears frequently in the training corpus. To incorporate this intuition, we used log probability of n-gram as features, which are included in Table 1 S (w ) = log p(w ) and (ID 19 and 20): φLM i i 1 S (w φLM i−1 , wi ) = log p(wi−1 , wi ). Here the 2 empirical probability p(wi ) and p(wi−1 , wi ) are computed from the source language corpus. In Japanese, we applied this source language augmentation only to Katakana words. In Chinese, we did not limit the target. 5 Transliteration For transliterating Japanese/Chinese words back to English, we adopted the Joint Source Channel (JSC) Model (Li et al., 2004), a generative model widely used as a simple yet powerful baseline in previous research e.g., (Hagiwara and Sekine, 2012; Finch and Sumita, 2010).2 The JSC model, given an input of source word s and target word t, deﬁnes the transliteration probability based on transliteration units (TUs) ui = hsi , ti i as: ∏f PJSC (hs, ti) = i=1 P (ui |ui−n+1 , ..., ui−1 ), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative upd"
P13-2033,W02-1001,0,0.100151,"Missing"
P13-2033,W09-3501,0,0.0294855,"Missing"
P13-2033,W09-3502,0,0.0525233,"Missing"
P13-2033,2010.iwslt-papers.7,0,0.0273519,"cluded in Table 1 S (w ) = log p(w ) and (ID 19 and 20): φLM i i 1 S (w φLM i−1 , wi ) = log p(wi−1 , wi ). Here the 2 empirical probability p(wi ) and p(wi−1 , wi ) are computed from the source language corpus. In Japanese, we applied this source language augmentation only to Katakana words. In Chinese, we did not limit the target. 5 Transliteration For transliterating Japanese/Chinese words back to English, we adopted the Joint Source Channel (JSC) Model (Li et al., 2004), a generative model widely used as a simple yet powerful baseline in previous research e.g., (Hagiwara and Sekine, 2012; Finch and Sumita, 2010).2 The JSC model, given an input of source word s and target word t, deﬁnes the transliteration probability based on transliteration units (TUs) ui = hsi , ti i as: ∏f PJSC (hs, ti) = i=1 P (ui |ui−n+1 , ..., ui−1 ), where f is the number of TUs in a given source / target word pair. TUs are atomic pair units of source / target words, such as “la/ラ” and “ish/ッシュ”. The TU n-gram probabilities are learned from a training corpus by following iterative updates similar to the EM algorithm3 . In order to generate transliteration candidates, we used a stack decoder described in (Hagiwara and Sekine, 2"
P13-2033,mcenery-xiao-2004-lancaster,0,0.0698605,"Missing"
P13-2033,N07-1047,0,0.0733656,"Missing"
P13-2033,C96-2202,0,0.0438426,"ics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ID 1 2 3* 4* 5* 6* 7 8* 9* 10* 11* 12 depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF conﬁdence to detect new words. For oﬄine approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such"
P13-2033,P08-1103,0,0.0600314,"Missing"
P13-2033,P99-1036,0,0.0655417,"balanced corpus and an electronic commerce domain corpus, and a balanced Chinese corpus. The results show that we achieved a signiﬁcant improvement in WS accuracy in both languages. 2 Related Work In Japanese WS, unknown words are usually dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ID 1 2 3* 4* 5* 6* 7 8* 9* 10* 11* 12 depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF conﬁdence to detect new words. For oﬄine approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. Th"
P13-2033,D11-1089,0,0.503732,"Missing"
P13-2033,I05-1060,0,0.420959,"chimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF conﬁdence to detect new words. For oﬄine approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monoling"
P13-2033,E03-1076,0,0.276845,"unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurrence in an English corpus. Similar approaches are proposed for other languages, such as German (Koehn and Knight, 2003) and Urdu-Hindi (Lehal, 2010). Correct splitting of compound nouns has a positive effect on MT (Koehn and Knight, 2003) and IR (Braschler and Ripplinger, 2004). A similar problem can be seen in Korean, German etc. where compounds may not be explicitly split by whitespaces. Koehn and Knight (2003) tackled the splitting problem in German, by using word statistics in a monolingual corpus. They also used the information whether translations of compound parts appear in a German-English bilingual corpus. Lehal (2010) used Urdu-Devnagri transliteration and a Hindi corpus for handling the space omissi"
P13-2033,P11-2093,0,0.150008,"Missing"
P13-2033,W04-3230,0,0.897107,"odels on a Japanese balanced corpus and an electronic commerce domain corpus, and a balanced Chinese corpus. The results show that we achieved a signiﬁcant improvement in WS accuracy in both languages. 2 Related Work In Japanese WS, unknown words are usually dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ID 1 2 3* 4* 5* 6* 7 8* 9* 10* 11* 12 depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF conﬁdence to detect new words. For oﬄine approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the clos"
P13-2033,C04-1081,0,0.0293998,"ly dealt with in an online manner with the unknown word model, which uses heuristics 183 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183–189, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ID 1 2 3* 4* 5* 6* 7 8* 9* 10* 11* 12 depending on character types (Kudo et al., 2004). Nagata (1999) proposed a Japanese unknown word model which considers PoS (part of speech), word length model and orthography. Uchimoto et al. (2001) proposed a maximum entropy morphological analyzer robust to unknown words. In Chinese, Peng et al. (2004) used CRF conﬁdence to detect new words. For oﬄine approaches, Mori and Nagao (1996) extracted unknown word and estimated their PoS from a corpus through distributional analysis. Asahara and Matsumoto (2004) built a character-based chunking model using SVM for Japanese unknown word detection. Kaji and Kitsuregawa (2011)’s approach is the closest to ours. They built a model to split Katakana compounds using backtransliteration and paraphrasing mined from large corpora. Nakazawa et al. (2005) is a similar approach, using a Ja-En dictionary to translate compound components and check their occurre"
P13-2033,P08-1101,0,0.0253078,"MeCab (Kudo et al., 2004). In Chinese, we aggregated consecutive 1 to 4 characters add them as “n (common noun)”, “ns (place name)”, “nr (personal name)”, and “nz (other proper nouns),” since most of the unknown words in Chinese are proper nouns. Also, we aggregated up to 20 consecutive numerical characters, making them a single node, and assign “m” (number). For other character types, a single node with PoS “w (others)” is created. Word Segmentation Model Out baseline model is a semi-Markov structure prediction model which estimates WS and the PoS sequence simultaneously (Kudo et al., 2004; Zhang and Clark, 2008). This model ﬁnds the best output y ∗ from the input sentence string x as: y ∗ = arg maxy∈Y (x) w · φ(y). Here, Y (x) denotes all the possible sequences of words derived from x. The best analysis is determined by the feature function φ(y) the 1 The Japanese dictionary and the corpus we used have 6 levels of PoS tag hierarchy, while the Chinese ones have only one level, which is why some of the PoS features are not included in Chinese. As character type, Hiragana (JA), Katakana (JA), Latin alphabet, Number, Chinese characters, and Others, are distinguished. Word length is in Unicode. 184 corpus"
P13-2033,P11-2010,1,\N,Missing
P17-4007,P05-1045,0,0.0578589,"an utilize the start and length information to calculate the exact position of the entity in the input sentence. The ENE tag can then be used in various subsequent tasks such as Relation Extraction (RE), Question Answering (QA) or automatic dialogue generation. The AL+ ENER API is freely accessible online.2 Currently, the API supports Japanese only, but we are also developing an API for English ENER. Figure 3 shows an example input sentence and output ENE tags. 2 Extended Named Entity recognition algorithms Existing NER systems often use Conditinal Random Fields (CRFs) (McCallum and Li, 2003; Finkel et al., 2005), HMM (Zhou and Su, 2002) or SVM (Yamada et al., 2002; Takeuchi and Collier, 2002; Sasano and Kurohashi, 2008) to assign tags to the tokens in an input sentence. However, these methods are supposed to work with only small number of categories (e.g., 10 categories). In the ENER problem, the number of categories is 200, which is very large, compared with the number in traditional NER. Consequently, traditional approaches might not achieve good performance and even be infeasible. Actually, we have tried to use CRF for 200 classes, but the training process took too long time and did not finish. In"
P17-4007,W03-0430,0,0.111565,"who uses the ENER API can utilize the start and length information to calculate the exact position of the entity in the input sentence. The ENE tag can then be used in various subsequent tasks such as Relation Extraction (RE), Question Answering (QA) or automatic dialogue generation. The AL+ ENER API is freely accessible online.2 Currently, the API supports Japanese only, but we are also developing an API for English ENER. Figure 3 shows an example input sentence and output ENE tags. 2 Extended Named Entity recognition algorithms Existing NER systems often use Conditinal Random Fields (CRFs) (McCallum and Li, 2003; Finkel et al., 2005), HMM (Zhou and Su, 2002) or SVM (Yamada et al., 2002; Takeuchi and Collier, 2002; Sasano and Kurohashi, 2008) to assign tags to the tokens in an input sentence. However, these methods are supposed to work with only small number of categories (e.g., 10 categories). In the ENER problem, the number of categories is 200, which is very large, compared with the number in traditional NER. Consequently, traditional approaches might not achieve good performance and even be infeasible. Actually, we have tried to use CRF for 200 classes, but the training process took too long time"
P17-4007,I08-2080,1,0.735218,"sentence. The ENE tag can then be used in various subsequent tasks such as Relation Extraction (RE), Question Answering (QA) or automatic dialogue generation. The AL+ ENER API is freely accessible online.2 Currently, the API supports Japanese only, but we are also developing an API for English ENER. Figure 3 shows an example input sentence and output ENE tags. 2 Extended Named Entity recognition algorithms Existing NER systems often use Conditinal Random Fields (CRFs) (McCallum and Li, 2003; Finkel et al., 2005), HMM (Zhou and Su, 2002) or SVM (Yamada et al., 2002; Takeuchi and Collier, 2002; Sasano and Kurohashi, 2008) to assign tags to the tokens in an input sentence. However, these methods are supposed to work with only small number of categories (e.g., 10 categories). In the ENER problem, the number of categories is 200, which is very large, compared with the number in traditional NER. Consequently, traditional approaches might not achieve good performance and even be infeasible. Actually, we have tried to use CRF for 200 classes, but the training process took too long time and did not finish. In this system, we use a combination approach to recognize ENEs. We first implement four base algorithms, namely"
P17-4007,sekine-nobata-2004-definition,1,0.843586,"et user feedbacks from this service to improve the ENER system, and the statistics obtained from the user feedIntroduction Named entity recognition (NER) is one of the most fundamental tasks in Information Retrieval, Information Extraction and Question Answering (Bellot et al., 2002; Nadeau and Sekine, 2007). A high quality named entity recognition API (Application Programming Interface) is therefore important for higher level tasks such as entity retrieval, recommendation and automatic dialogue generation. To extend the ability of named entity recognition, Sekine et al. (Sekine et al., 2002; Sekine and Nobata, 2004) have proposed an Extended Named Entity (ENE) hierarchy, which refines the definition of named entity. The ENE hierarchy is a three-level hierarchy, which contains more than ten coarse-grained categories at the top level and 200 fine-grained categories at the leaf level. The top level of the hierarchy includes traditional named entity categories, such as Person, Location or Organization. The middle level and leaf level refine the top level categories to more fine37 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, pages 37–42 c Vanco"
P17-4007,sekine-etal-2002-extended,1,0.632536,"ners, the method to get user feedbacks from this service to improve the ENER system, and the statistics obtained from the user feedIntroduction Named entity recognition (NER) is one of the most fundamental tasks in Information Retrieval, Information Extraction and Question Answering (Bellot et al., 2002; Nadeau and Sekine, 2007). A high quality named entity recognition API (Application Programming Interface) is therefore important for higher level tasks such as entity retrieval, recommendation and automatic dialogue generation. To extend the ability of named entity recognition, Sekine et al. (Sekine et al., 2002; Sekine and Nobata, 2004) have proposed an Extended Named Entity (ENE) hierarchy, which refines the definition of named entity. The ENE hierarchy is a three-level hierarchy, which contains more than ten coarse-grained categories at the top level and 200 fine-grained categories at the leaf level. The top level of the hierarchy includes traditional named entity categories, such as Person, Location or Organization. The middle level and leaf level refine the top level categories to more fine37 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstra"
P17-4007,W02-2029,0,0.111615,"of the entity in the input sentence. The ENE tag can then be used in various subsequent tasks such as Relation Extraction (RE), Question Answering (QA) or automatic dialogue generation. The AL+ ENER API is freely accessible online.2 Currently, the API supports Japanese only, but we are also developing an API for English ENER. Figure 3 shows an example input sentence and output ENE tags. 2 Extended Named Entity recognition algorithms Existing NER systems often use Conditinal Random Fields (CRFs) (McCallum and Li, 2003; Finkel et al., 2005), HMM (Zhou and Su, 2002) or SVM (Yamada et al., 2002; Takeuchi and Collier, 2002; Sasano and Kurohashi, 2008) to assign tags to the tokens in an input sentence. However, these methods are supposed to work with only small number of categories (e.g., 10 categories). In the ENER problem, the number of categories is 200, which is very large, compared with the number in traditional NER. Consequently, traditional approaches might not achieve good performance and even be infeasible. Actually, we have tried to use CRF for 200 classes, but the training process took too long time and did not finish. In this system, we use a combination approach to recognize ENEs. We first implement"
P17-4007,P02-1060,0,\N,Missing
S01-1038,W01-1415,1,0.879344,"ources of information related to the input sentence and examples. Since we want to avoid making complicated rules, we use machine learning models to calculate the similarity. Instead of all examples in the TM, English headwords are used as classes in machine learning models. Therefore, examples having the same English headword are put into the same class and are considered to have the same similarity. 1 A description on how to use ""diff"" can be found in (Murata and Isahara, 2001). 2 Work on using machine learning methods for the tra!lslation of tenses, aspects, and modalities can be found'in {Murata et al., 2001a). 156 Classes identified by machine learning models are basically English headwords in TM, and they are detected manually. For example, English headwords of the examples in Figure 1 are ""feel constrained"", ""constraint"", and ""refrain"", respectively. When English headwords are verbs, they are represented by their basic forms. English words obtained when a Japanese headword is looked up in a Japanese-English dictionary are also used as classes. For the training data, we use not only examples in the TM but also other data collected from bilingual dictionaries or a parallel corpus. The collected"
S07-1012,P98-1012,0,0.963564,"number of “senses” (actual people) is unknown a priori, and it is in average much higher than in the WSD task (there are 90,000 different names shared by 100 million people according to the U.S. Census Bureau). There is also a strong relation of our proposed task with the Co-reference Resolution problem, focused on linking mentions (including pronouns) in a text. Our task can be seen as a co-reference resolution problem where the focus is on solving interdocument co-reference, disregarding the linking of all the mentions of an entity inside each document. An early work in name disambiguation (Bagga and Baldwin, 1998) uses the similarity between documents in a Vector Space using a “bag of words” representation. An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information. Fleischman and Hovy (2004) propose a Maximum Entropy model trained to give the probability that 64 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 64–69, c Prague, June 2007. 2007 Association for Computational Linguistics two names refer to the same individual 1 . The paper is organized as follows. Section 2 provides a des"
S07-1012,W04-0701,0,0.119357,"ed task with the Co-reference Resolution problem, focused on linking mentions (including pronouns) in a text. Our task can be seen as a co-reference resolution problem where the focus is on solving interdocument co-reference, disregarding the linking of all the mentions of an entity inside each document. An early work in name disambiguation (Bagga and Baldwin, 1998) uses the similarity between documents in a Vector Space using a “bag of words” representation. An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information. Fleischman and Hovy (2004) propose a Maximum Entropy model trained to give the probability that 64 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 64–69, c Prague, June 2007. 2007 Association for Computational Linguistics two names refer to the same individual 1 . The paper is organized as follows. Section 2 provides a description of the experimental methodology, the training and test data provided to the participants, the evaluation measures, baseline systems and the campaign design. Section 3 gives a description of the participant systems and provides the evaluation results"
S07-1012,W03-0405,0,0.74146,"red by 100 million people according to the U.S. Census Bureau). There is also a strong relation of our proposed task with the Co-reference Resolution problem, focused on linking mentions (including pronouns) in a text. Our task can be seen as a co-reference resolution problem where the focus is on solving interdocument co-reference, disregarding the linking of all the mentions of an entity inside each document. An early work in name disambiguation (Bagga and Baldwin, 1998) uses the similarity between documents in a Vector Space using a “bag of words” representation. An alternative approach by Mann and Yarowsky (2003) is based on a rich feature space of automatically extracted biographic information. Fleischman and Hovy (2004) propose a Maximum Entropy model trained to give the probability that 64 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 64–69, c Prague, June 2007. 2007 Association for Computational Linguistics two names refer to the same individual 1 . The paper is organized as follows. Section 2 provides a description of the experimental methodology, the training and test data provided to the participants, the evaluation measures, baseline systems and th"
S07-1012,C98-1012,0,\N,Missing
S19-1027,E17-2039,1,0.846664,"Missing"
S19-1027,P18-2103,0,0.357987,"inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels onl"
S19-1027,W17-6901,1,0.779854,"iers (dancing w happily dancing), or adding 3.1 Source corpus We use sentences from the Parallel Meaning Bank (PMB, Abzianidze et al., 2017) as a source while creating the inference dataset. The reason behind choosing the PMB is threefold. First, the finegrained annotations in the PMB facilitate our automatic monotonicity-driven construction of inference problems. In particular, semantic tokenization and WordNet (Fellbaum, 1998) senses make narrow and broad concept substitutions easy while the syntactic analyses in Combinatory Categorial Grammar (CCG, Steedman, 2000) format and semantic tags (Abzianidze and Bos, 2017) contribute to monotonicity and polarity detection. Second, the PMB contains lexically and syntactically diverse texts from a wide range of genres. Third, the gold (silver) documents are fully (partially) manually verified, which control noise in the automated generated dataset. To prevent easy inferences, we use the sentences with more than five tokens from 5K gold and 5K silver portions of the PMB. 1 Our dataset and its generation code will be made publicly available at https://github.com/verypluming/HELP. 251 Section Size Up 7784 Down 21192 All [NP kids↓] were [VP dancing on the floor↑] Non"
S19-1027,N18-2017,0,0.0624952,"Missing"
S19-1027,D15-1075,0,0.0783552,"sentence can become longer, yet the inference is valid. FraCaS (Cooper et al., 1994) contains such logically challenging problems as downward inferences. However, it is small in size (only 346 examples) for training neural models, and it covers only simple syntactic patterns with severely restricted vocabularies. The lack of such a dataset on a large scale is due to at least two factors: it is hard to instruct crowd workers without deep knowledge of natural language syntax and semantics, and it is also unfeasible to employ experts to obtain a large number of logically challenging inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et a"
S19-1027,2014.lilt-9.7,0,0.233344,"ators and syntactic structures. 3 Monotonicity Reasoning Data Creation We address three issues when creating the inference problems: (a) Detect the monotone operators and their arguments; (b) Based on the syntactic structure, induce the polarity of the argument positions; (c) Using lexical knowledge or logical connectives, narrow or broaden the arguments. Monotonicity reasoning is a sort of reasoning based on word replacement. Based on the monotonicity properties of words, it determines whether a certain word replacement results in a sentence entailed from the original one (van Benthem, 1983; Icard and Moss, 2014). A polarity is a characteristic of a word position imposed by monotone operators. Replacements with more general (or specific) phrases in ↑ (or ↓) polarity positions license entailment. Polarities are determined by a function which is always upward monotone (+) (i.e., an order preserving function that licenses entailment from specific to general phrases), always downward monotone (−) (i.e., an order reversing function) or neither, non-monotone. Determiners are modeled as binary operators, taking noun and verb phrases as the first and second arguments, respectively, and they entail sentences w"
S19-1027,W15-4002,0,0.172203,"sentence can become longer, yet the inference is valid. FraCaS (Cooper et al., 1994) contains such logically challenging problems as downward inferences. However, it is small in size (only 346 examples) for training neural models, and it covers only simple syntactic patterns with severely restricted vocabularies. The lack of such a dataset on a large scale is due to at least two factors: it is hard to instruct crowd workers without deep knowledge of natural language syntax and semantics, and it is also unfeasible to employ experts to obtain a large number of logically challenging inferences. Bowman et al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et a"
S19-1027,P17-1152,0,0.115701,"Missing"
S19-1027,marelli-etal-2014-sick,0,0.0337584,"ho don’t value my time). These problems contain disjunction or modifiers in downward environments where either (i) the premise P contains all words in the hypothesis H yet the inference is invalid or (ii) H contains more words than those in P yet the inference is valid.2 Although HELP contains 21K such problems, the models nevertheless misclassified them. This indicates that the difficulty in learning these non-lexical downward inferences might not come from the lack of training datasets. conjunction, and disjunction sections), (ii) FraCaS (the generalized quantifier section), (iii) the SICK (Marelli et al., 2014) test set, and (iv) MultiNLI matched/mismatched test set. We used the Matthews correlation coefficient (ranging [−1, 1]) as the evaluation metric for GLUE. Regarding other datasets, we used accuracy as the metric. We also check if our data augmentation does not decrease the performance on MultiNLI. 4.2 Results and discussion Table 3 shows that adding HELP to MultiNLI improved the accuracy of all models on GLUE, FraCaS, and SICK. Regarding MultiNLI, note that adding data for downward inference can be harmful for performing upward inference, because lexical replacements work in an opposite way i"
S19-1027,C18-1198,0,0.0606078,"al. (2015b) proposed an artificial dataset for logical reasoning, whose premise and hypothesis are automatically generated from a simple English-like grammar. Following this line of work, Geiger et al. (2018) presented a method to construct a complex dataset for multiple quantifiers (e.g., Every dwarf licks no rifle ⇒ No ugly dwarf licks some rifle). These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis s"
S19-1027,W18-5441,0,0.0640217,"Missing"
S19-1027,S18-2023,0,0.0660154,"Missing"
S19-1027,L18-1239,0,0.0559481,"proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). Moreover, these standard datasets come with the so-called 250 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 250–255 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Poliak et al., 2018a). In these works, however, little attention has been paid to downward inferences. The GLUE leaderboard (Wang et al., 2019) reported that neural models did not perform well on downward inferences, and this leaves us guessing whether the lack of large datasets for such kind of inferences that involve the interaction between lexical a"
S19-1027,N18-1101,0,0.0822646,"These datasets contain downward inferences, but they are designed not to require lexical knowledge. There are also NLI datasets which expand lexical knowledge by replacing words using lexical rules (Monz and de Rijke, 2001; Glockner et al., 2018; Naik et al., 2018; Introduction Natural language inference (NLI) has been proposed as a benchmark task for natural language understanding. This task is to determine whether a given statement (premise) semantically entails another statement (hypothesis) (Dagan et al., 2013). Large crowdsourced datasets such as SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been created from naturally-occurring texts for training and testing neural models on NLI. Recent reports showed that these crowdsourced datasets contain undesired biases that allow prediction of entailment labels only from hypothesis sentences (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). Moreover, these standard datasets come with the so-called 250 Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM), pages 250–255 c Minneapolis, June 6–7, 2019. 2019 Association for Computational Linguistics Poliak et al., 2018a). In these works, h"
S19-1027,D18-1007,0,\N,Missing
S19-1027,N19-1423,0,\N,Missing
S19-1027,W19-4810,0,\N,Missing
sadamitsu-etal-2008-sentiment,N04-1015,0,\N,Missing
sadamitsu-etal-2008-sentiment,P02-1053,0,\N,Missing
sadamitsu-etal-2008-sentiment,W02-1011,0,\N,Missing
sadamitsu-etal-2008-sentiment,I08-1039,0,\N,Missing
sadamitsu-etal-2008-sentiment,P06-1028,0,\N,Missing
sekine-2008-extended,C02-1150,0,\N,Missing
sekine-2008-extended,I05-7009,0,\N,Missing
sekine-2008-extended,W04-3221,0,\N,Missing
sekine-2008-extended,sekine-isahara-2000-irex,1,\N,Missing
sekine-2008-extended,C96-1079,0,\N,Missing
sekine-dalwani-2010-ngram,C92-2082,0,\N,Missing
sekine-dalwani-2010-ngram,P04-1053,1,\N,Missing
sekine-dalwani-2010-ngram,W99-0613,0,\N,Missing
sekine-etal-2002-extended,W98-1120,1,\N,Missing
sekine-nobata-2004-definition,sekine-etal-2002-extended,1,\N,Missing
sekine-nobata-2004-definition,sekine-isahara-2000-irex,1,\N,Missing
sekine-nobata-2004-definition,C96-1079,0,\N,Missing
W01-0512,J96-1002,0,0.030443,") : verb (1) ( ) = > & =1 : 0 : otherwise h; x g h; f ; 00 x ; f : Here has(h,x)&quot; is a binary function that returns true if the history h has feature x. In our experiments, we focused on such information as whether or not a string is found in a dictionary, the length of the string, what types of characters are used in the string, and the part-of-speech of the adjacent morpheme. Given a set of features and some training data, the M.E. estimation process produces a model in which every feature gi has an associated parameter i . This enables us to compute the conditional probability as follows (Berger et al., 1996): P (f jh) = Z (h) = Q g (h;f ) i i i Z (h) gi i (h;f ) : f i XY (2) (3) The M.E. estimation process guarantees that for every feature gi , the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words, X~ h;f P (h; f ) 1 gi (h; f ) = X~ h P (h) 1 X f PM:E: (f jh) 1 gi (h; f ): (4) Here P~ is an empirical probability and PM:E: is the probability assigned by the model. We de ne part-of-speech and bunsetsu boundaries as grammatical attributes. Here a bunsetsu is a phrasal unit consisting of one or more morphemes. When the"
W01-0512,J95-4004,0,0.149518,"Missing"
W01-0512,A92-1018,0,0.0221582,"Missing"
W01-0512,P97-1030,0,0.0242452,"Missing"
W01-0512,P97-1031,0,0.0336342,"Missing"
W01-0512,C96-2202,0,0.211023,"Missing"
W01-0512,C94-1032,0,0.0153311,"pheme and has the grammatical attribute i(1  i  n). We call it a morpheme model. This model is represented by Eq. (2), in which f can be one of (n + 1) tags from 0 to n. A given sentence is divided into morphemes, and a grammatical attribute is assigned to each morpheme so as to maximize the sentence probability estimated by our morpheme model. Sentence probability is de ned as the product of the probabilities estimated for a particular division of morphemes in a sentence. We use the Viterbi algorithm to nd the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the Nbest sets. 3 Experiments and Discussion 3.1 Experimental Conditions The part-of-speech categories that we used follow those of JUMAN (Kurohashi and Nagao, 1999). There are 53 categories covering all possible combinations of major and minor categories as de ned in JUMAN. The number of grammatical attributes is 106 if we include the detection of whether or not the left side of a morpheme is a bunsetsu boundary. We do not identify in ection types probabilistically since 1 Not only morphemes but also bunsetsus can be identi ed by considering the information related to their bun"
W01-0512,P99-1036,0,0.582214,"menting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as a partof-speech (POS) and an in ection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus, and there have been two statistical approaches to this problem. One is to acquire unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). We would like to be able to make good use of both approaches. If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem. Mori and Nagao proposed a statistical model that can consult a dictionary (Mori and Nagao, 1998). In their model the probability that a string of letters or characters is y Hitoshi Isahara sekine@cs.nyu.edu a morpheme is augmented when the string is found in a dictionary"
W01-0512,W96-0213,0,0.0502922,"Missing"
W01-0512,C94-1027,0,0.0244018,"Missing"
W01-0512,P94-1025,0,0.048906,"Missing"
W01-0512,E99-1026,1,0.87637,"Missing"
W01-0512,P98-1081,0,0.0207993,"Missing"
W03-0509,C00-2136,0,\N,Missing
W03-0509,H01-1009,1,\N,Missing
W03-0509,W01-0100,0,\N,Missing
W03-0509,C96-1079,0,\N,Missing
W03-1204,P98-1009,0,0.0289388,"eatures we use are worth analyzing. Sentence extraction is one of the main methods required for a summarization system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types"
W03-1204,C02-1053,0,0.0850511,"system to reduce the size of a document. Edmundson (1969) proposed a method of integrating several features, such as the positions of sentences and the frequencies of words in an article, in order to extract sentences. He manually assigned parameter values to integrate features for estimating the significance scores of sentences. On the other hand, machine learning methods can also be applied to integrate features. For sentence extraction from training data, Kupiec et al. (1995) and Aone et al. (1998) used Bayes’ rule, Lin (1999) and Nomoto and Matsumoto (1997) generated a decision tree, and Hirao et al. (2002) generated an SVM. In this paper, we not only show evaluation results for our sentence extraction system using combinations of features but also analyze the features for different types of corpora. The analysis gives us some indication about how to use these features and how to combine them. 2 Summarization data The summarization data we used for this research were prepared from Japanese newspaper articles, Japanese lectures, and English newspaper articles. By using these three types of data, we could compare two languages and also two different types of corpora, a written corpus and a speech"
W03-1204,maekawa-etal-2000-spontaneous,1,0.885642,"Missing"
W03-1204,H01-1009,1,0.780656,"Si ) = w∈H∩Si tf(w) DN log tf(w)+1 df(w) X tf(w) w∈H tf(w)+1 log Ratio System Lead DN df(w) We also evaluated another method based on this scoring function by using only named entities (NEs) instead of words for the TSC data and DUC data. Only the term frequency was used for NEs, because we judged that the document frequency for an entity was usually quite small, thereby making the differences between entities negligible. 3.1.5 Patterns For the DUC data, we used dependency patterns as a type of scoring function. These patterns were extracted by pattern discovery during information extraction (Sudo et al., 2001). The details of this approach are not explained here, because this feature is not among the features we analyze in Section 5. The definition of the function appears in (Nobata et al., 2002). 3.2 Optimal weight Our system set weights for each scoring function in order to calculate the total score of a sentence. The total score (Si ) is defined from the scoring functions (Scorej ()) and weights (αj ) as follows: TotalScore(Si ) = X αj Scorej (Si ) (1) j We estimated the optimal values of these weights from the training data. After the range of each weight was set manually, the system changed th"
W03-1204,C98-1009,0,\N,Missing
W03-1204,nobata-etal-2002-summarization,1,\N,Missing
W03-1609,P01-1008,0,0.417367,"in this attempt only limited forms of expressions could be obtained. Furthermore, the obtained paraphrases were limited to existing IE patterns only. We are interested in collecting various kinds of clues, including similar IE patterns themselves, to connect two patterns. In this paper, we tried to obtain more varied paraphrases. Although our current method is intended for use in Information Extraction, we think the same approach can be applied to obtain paraphrases for other purposes, such as machine translation or text summarization. There have been several attempts to obtain paraphrases. (Barzilay and McKeown, 2001) applied text alignment to parallel translations of a single text and used a part-of-speech tagger to obtain paraphrases. (Lin and Pantel, 2001) used mutual information of word distribution to calculate the similarity of expressions. (Pang et al., 2003) also used text alignment and obtained a finite state automaton which generates paraphrases. (Ravichandran and Hovy, 2002) used pairs of questions and answers to obtain varied patterns which give the same answer. Our approach is different from these works in that we used comparable news articles as a source of paraphrases and used Named Entity t"
W03-1609,N03-1024,0,0.0424145,"o patterns. In this paper, we tried to obtain more varied paraphrases. Although our current method is intended for use in Information Extraction, we think the same approach can be applied to obtain paraphrases for other purposes, such as machine translation or text summarization. There have been several attempts to obtain paraphrases. (Barzilay and McKeown, 2001) applied text alignment to parallel translations of a single text and used a part-of-speech tagger to obtain paraphrases. (Lin and Pantel, 2001) used mutual information of word distribution to calculate the similarity of expressions. (Pang et al., 2003) also used text alignment and obtained a finite state automaton which generates paraphrases. (Ravichandran and Hovy, 2002) used pairs of questions and answers to obtain varied patterns which give the same answer. Our approach is different from these works in that we used comparable news articles as a source of paraphrases and used Named Entity tagging and dependency analysis to extract corresponding expressions. 2 Overall Procedure of Paraphrase Acquisition Our main goal is to obtain pattern clusters for IE, which consist of sets of equivalent patterns capturing the same information. So we tri"
W03-1609,P02-1006,0,0.00803945,"or use in Information Extraction, we think the same approach can be applied to obtain paraphrases for other purposes, such as machine translation or text summarization. There have been several attempts to obtain paraphrases. (Barzilay and McKeown, 2001) applied text alignment to parallel translations of a single text and used a part-of-speech tagger to obtain paraphrases. (Lin and Pantel, 2001) used mutual information of word distribution to calculate the similarity of expressions. (Pang et al., 2003) also used text alignment and obtained a finite state automaton which generates paraphrases. (Ravichandran and Hovy, 2002) used pairs of questions and answers to obtain varied patterns which give the same answer. Our approach is different from these works in that we used comparable news articles as a source of paraphrases and used Named Entity tagging and dependency analysis to extract corresponding expressions. 2 Overall Procedure of Paraphrase Acquisition Our main goal is to obtain pattern clusters for IE, which consist of sets of equivalent patterns capturing the same information. So we tried to discover paraphrases contained in Japanese news articles for a specific domain. Our basic idea is to search news art"
W03-1609,sekine-etal-2002-extended,1,0.778722,"In the actual matching process we used a method described in (Papka et al., 1999) to find a set of comparable articles. Then we use a simple vector space model for sentence matching. 2.2 Identify Anchors Before extracting paraphrases, we find anchors in comparable sentences. We used Extended Named Entity tagging to identify anchors. A Named Entity tagger identifies proper expressions such as names, locations and dates in sentences. In addition to these expressions, an Extended Named Entity tagger identifies some common nouns such as disease names or numbers, that are also unlikely to change (Sekine et al., 2002). For each corresponding pair of sentences, we apply the tagger and identify the same noun phrases which appear in both sentences as anchors. 2.3 Extract Corresponding Sentence Portions Now we identify appropriate boundaries of expressions which share the anchors identified in the previous stage. To avoid extracting non-grammatical expressions, we operate on syntactically structured text rather than sequences of words. Dependency analysis is suitable for this purpose, since using dependency trees we can reconstruct grammatically correct expressions from a spanning subtree whose root is a predi"
W03-1609,H01-1009,1,0.828454,"e many combinations of sentence portions which don’t make sense as paraphrases. For example, from the expression “two more people have died in Hong Kong” and “Hong Kong reported two more deaths”, we could extract expressions “in Hong Kong” and “Hong Kong reported”. Although both of them share one anchor, this is not a correct paraphrase. To avoid this sort of error, we need to put some additional restrictions on the expressions. (Shinyama et al., 2002) used the frequency of expressions to filter these incorrect pairs of expressions. First the system obtained a set of IE patterns from corpora (Sudo and Sekine, 2001), and then calculated the score for each candidate paraphrase by counting how many times that expression appears as an IE pattern in the whole corpus. However, with this method, obtainable expressions are limited to existing IE patterns only. Since we wanted to obtain a broader range of expressions not limited to IE patterns themselves, we tried to use other restrictions which can be acquired independently of the IE system. We partly solve this problem by calculating the plausibility of each tree structure. In Japanese sentences, the case of each argument which modifies a predicate is represen"
W03-1609,P00-1042,0,0.0274767,"Missing"
W04-2208,C00-1007,0,0.0337696,"h expressions corresponding to a Japanese expression is 1.3 as shown in Table 2. Even when there are two or more possible English expressions, an appropriate English expression can be chosen by selecting a Japanese expression by referring to dependencies in extracted translation pairs. Therefore, in many cases, English sentences can be generated just by reordering the selected expressions. The English word order was estimated manually in this experiment. However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriat"
W04-2208,P01-1030,0,0.0114299,"o a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually"
W04-2208,2002.tmi-papers.9,0,0.015857,"and Japanese-English machine translation. We can directly compare various methods of machine translation by using this corpus. It can be summarized as follows in terms of the characteristics of the corpus. One-sentence to one-sentence translation can be simply used for the evaluation of various methods of machine translation. Morphological and syntactic information can be used for the evaluation of methods that actively use morphological and syntactic information, such as methods for examplebased machine translation (Nagao, 1981; Watanabe et al., 2003), or transfer-based machine translation (Imamura, 2002). Phrasal alignment is used for the evaluation of automatically acquired translation knowledge (Yamamoto and Matsumoto, 2003). An actual comparison and evaluation is our future work. 3.2 Analysis of Translation One-sentence to one-sentence translation reﬂects contextual information. Therefore, it is suitable to investigate the inﬂuence of the context on the translation. For example, we can investigate the diﬀerence in the use of demonstratives and pronouns between English and Japanese. We can also investigate the diﬀerence in the use of anaphora. Morphological and syntactic information and phr"
W04-2208,J93-2004,0,0.0235999,"ng a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many. On the other hand, high-quality treebanks such as the Penn Treebank (Marcus et al., 1993) and the Kyoto University text corpus (Kurohashi and Nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis. However, almost all of these highquality treebanks are based on monolingual corgual information. There are few high-quality bilingual or multilingual treebank corpora because parallel corpora have mainly been actively used for machine translation between related languages such as English and French, therefore their syntactic structures are not required so much for"
W04-2208,P00-1056,0,0.126608,"ntence is similar to a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-"
W04-2208,C02-1064,1,0.816552,"tence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriate keywords. We are investigating whether or not we can generate similar parallel translations to all of the Japanese sentences appearing on January 17, 1995. So far, we found that we can generate similar parallel translations to 691 out of 840 sentences (the average number of bunsetsus is about 10.3) including the 102 sentences described in Section 3.3. We found that we could not generate similar parallel translations to 149 out of 840 sentences. In the proposed framework of similar parallel translation generation, the language appearing in a corpus corresp"
W04-2208,P01-1067,0,0.102382,"erefore their syntactic structures are not required so much for aligning words or phrases. However, syntactic structures are necessary for machine translation between languages whose syntactic structures are diﬀerent from each other, such as in Japanese-English, Japanese-Chinese, and Chinese-English machine translations, because it is more diﬃcult to automatically align words or phrases between two unrelated languages than between two related languages. Actually, it has been reported that syntactic structures contribute to improving the accuracy of word alignment between Japanese and English (Yamada and Knight, 2001). Therefore, if we had a high-quality parallel treebank corpus, the accuracies of machine translation between languages whose syntactic structures are diﬀerent from each other would improve. Furthermore, if the parallel treebank corpus had word or phrase alignment, the accuracy of automatic word or phrase alignment would increase by using the parallel treebank corpus as training data. However, so far, there is no aligned parallel treebank corpus whose domain is not restricted. For example, the Japanese Electronics Industry Development Association’s (JEIDA’s) bilingual corpus (Isahara and Harun"
W04-2208,A00-2018,0,\N,Missing
W12-4404,H05-1120,0,0.0269999,"given an input word s (such as “piaget”), the system is asked to generate from scratch the most probable transliterated word t (e.g., “ピア ジェpiaje”). The transliteration recognition task, on the other hand, is to induce the most probable transliteration t∗ ∈ T such that t∗ = arg maxt∈T P (hs, ti) given the input word s and a pool of transliteration candidates T . We call P (hs, ti) transliteration model in this paper. This model can be regarded as the hybrid of an unsupervised alignment technique 31 for transliteration and class-based transliteration. Related researches for the former include (Ahmad and Kondrak, 2005), who estimate character-based error probabilities from query logs via the EM algorithm. For the latter, Llitjos and Black (2001) showed that source language origins may improve the pronunciation of proper nouns in text-to-speech systems. The structure of this paper is as follows: we introduce the alpha-beta model(Brill and Moore, 2000) in Section 2, which is the most basic spelling-based transliteration model on which other models are based. In the following Section 3, we introduce and relate the joint source channel (JSC) model (Li et al., 2004) to the alphabeta model. We describe the LCT mo"
W12-4404,P00-1037,0,0.859638,"based on maximum likelihood estimation. We propose a novel latent semantic transliteration model based on Dirichlet mixture, where a Dirichlet mixture prior is introduced to mitigate the overﬁtting problem. We have shown that the proposed method considerably outperform the conventional transliteration models. 1 Introduction Transliteration (e.g., バ ラ ク オ バ マ baraku obama “Barak Obama”) is phonetic translation between languages with diﬀerent writing systems, which is a major way of importing foreign words into diﬀerent languages. Supervised, spelling-based grapheme-to-grapheme models such as (Brill and Moore, 2000; Li et To address this issue, Li et al. (2007) have proposed class transliteration model, which explicitly models and classiﬁes classes of languages (such as Chinese Hanzi, Japanese Katakana, and so on) and genders, and switches corresponding transliteration models based on the input. This model requires training sets of transliterated word pairs tagged with language origin, which is diﬃcult to obtain. Hagiwara and Sekine proposed the latent class transliteration (LCT) model (Hagiwara and Sekine, 2011), which models source language origins as directly unobservable latent classes and applies a"
W12-4404,P11-2010,1,0.509881,"ゲッ ト tāgetto” diﬀer in pronunciation and spelling correspondence depending on their source languages, which are French and English in this case. Transliteration has been usually recognized by spelling-based supervised models. However, a single model cannot deal with mixture of words with diﬀerent origins, such as “get” in “piaget” and “target”. Li et al. (2007) propose a class transliteration method, which explicitly models the source language origins and switches them to address this issue. In contrast to their model which requires an explicitly tagged training corpus with language origins, Hagiwara and Sekine (2011) have proposed the latent class transliteration model, which models language origins as latent classes and train the transliteration table via the EM algorithm. However, this model, which can be formulated as unigram mixture, is prone to overﬁtting since it is based on maximum likelihood estimation. We propose a novel latent semantic transliteration model based on Dirichlet mixture, where a Dirichlet mixture prior is introduced to mitigate the overﬁtting problem. We have shown that the proposed method considerably outperform the conventional transliteration models. 1 Introduction Transliterati"
W12-4404,N09-1022,1,0.899791,"Missing"
W12-4404,P04-1021,0,0.551315,"ted researches for the former include (Ahmad and Kondrak, 2005), who estimate character-based error probabilities from query logs via the EM algorithm. For the latter, Llitjos and Black (2001) showed that source language origins may improve the pronunciation of proper nouns in text-to-speech systems. The structure of this paper is as follows: we introduce the alpha-beta model(Brill and Moore, 2000) in Section 2, which is the most basic spelling-based transliteration model on which other models are based. In the following Section 3, we introduce and relate the joint source channel (JSC) model (Li et al., 2004) to the alphabeta model. We describe the LCT model as an extension to the JSC model in Section 4. In Section 5, we propose the DM-LST model, and show the experimental results on transliteration generation in Section 6. 2 Alpha-Beta Model In this section, we describe the alpha-beta model, which is one of the simplest spellingbased transliteration models. Though simple, the model has been shown to achieve better performance in tasks such as spelling correction (Brill and Moore, 2000), transliteration (Brill et al., 2001), and query alteration (Hagiwara and Suzuki, 2009). The method directly mode"
W12-4404,P07-1016,0,0.0771102,"honetic-based methods such as (Knight and Jonathan, 1998). However, single, monolithic models fail to deal with sets of foreign words with multiple language origins mixed together. For example, the “get” part of “piaget / ピ ア ジェ piaje” and “target / ター ゲッ ト tāgetto” diﬀer in pronunciation and spelling correspondence depending on their source languages, which are French and English in this case. Transliteration has been usually recognized by spelling-based supervised models. However, a single model cannot deal with mixture of words with diﬀerent origins, such as “get” in “piaget” and “target”. Li et al. (2007) propose a class transliteration method, which explicitly models the source language origins and switches them to address this issue. In contrast to their model which requires an explicitly tagged training corpus with language origins, Hagiwara and Sekine (2011) have proposed the latent class transliteration model, which models language origins as latent classes and train the transliteration table via the EM algorithm. However, this model, which can be formulated as unigram mixture, is prone to overﬁtting since it is based on maximum likelihood estimation. We propose a novel latent semantic tr"
W12-4404,W09-3501,0,0.0187974,"th mixed origins such as “naïveness”. In fact, we conﬁrmed through a preliminary experiment that LDA does not improve the transliteration performance over the baseline. 6 Experiments 6.1 Evaluation In this section, we compare the following models: alpha-beta (AB), joint source channel (JSC), latent class transliteration (LCT), and latent semantic transliteration based on Dirichlet mixture (DM-LST). For the performance evaluation, we used three language pairs, namely, English-Japanese (EnJa), English-Chinese (En-Ch), and EnglishKorean (En-Ko), from the transliteration shared task at NEWS 2009 (Li et al., 2009a; Li et al., 2009b). The size of each training/test set is shown in the ﬁrst column of Table 1. In general, rn , a set of one or more reference transliterated words, is associated with the n-th input sn in the training/test corpus. Let cn,i , cn,2 , ... be the output of the transliteration system, i.e., the candidates with highest probabilities assigned by the transliteration model being evaluated. We used the following three performance measures: • ACC (averaged Top-1 accuracy): For ev35 ery hsn , rn i, let an be an = 1 if the candidate with the highest probability cn,1 is contained in the r"
W12-4404,W09-3502,0,0.0183492,"th mixed origins such as “naïveness”. In fact, we conﬁrmed through a preliminary experiment that LDA does not improve the transliteration performance over the baseline. 6 Experiments 6.1 Evaluation In this section, we compare the following models: alpha-beta (AB), joint source channel (JSC), latent class transliteration (LCT), and latent semantic transliteration based on Dirichlet mixture (DM-LST). For the performance evaluation, we used three language pairs, namely, English-Japanese (EnJa), English-Chinese (En-Ch), and EnglishKorean (En-Ko), from the transliteration shared task at NEWS 2009 (Li et al., 2009a; Li et al., 2009b). The size of each training/test set is shown in the ﬁrst column of Table 1. In general, rn , a set of one or more reference transliterated words, is associated with the n-th input sn in the training/test corpus. Let cn,i , cn,2 , ... be the output of the transliteration system, i.e., the candidates with highest probabilities assigned by the transliteration model being evaluated. We used the following three performance measures: • ACC (averaged Top-1 accuracy): For ev35 ery hsn , rn i, let an be an = 1 if the candidate with the highest probability cn,1 is contained in the r"
W12-4404,J98-4003,0,\N,Missing
W12-4805,D07-1090,0,0.0194673,"Missing"
W12-4805,P12-3027,0,0.197109,"Missing"
W12-4805,P98-1055,0,0.156203,"Missing"
W12-4805,P04-3001,0,0.0846644,"Missing"
W12-4805,E12-2004,0,0.0617901,"Missing"
W12-4805,kawahara-kurohashi-2006-case,0,0.0344078,"Missing"
W12-4805,Y08-1044,0,0.0668922,"Missing"
W12-4805,U08-1007,0,0.0689352,"Missing"
W12-4805,W09-2111,0,0.0715387,"Missing"
W12-4805,P00-1067,0,0.0787156,"Missing"
W12-4805,C94-2136,0,0.252583,"Missing"
W12-4805,D09-1151,0,0.0546942,"Missing"
W12-4805,2012.eamt-1.9,0,0.102185,"Missing"
W12-4805,W10-0804,0,0.0423242,"Missing"
W15-2907,W04-3230,0,0.0588216,"Missing"
W15-2907,N10-1122,0,0.0304088,"ining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of generating coherent aspect groups that can be easily interpreted by humans. On the other hand, approaches using prior knowledge sources or a small amount of annotation data are also studied to maintain high precision while lowering the manual annotation cost (Carenini et al., 2005; Zhai et al., 2010; Chen et al., 2013a; Chen et al., 2013b; Chen et al., 2013c). Particularly, the method proposed by Zhai et al. (2010) can easily incorporate aspect expressions into predefined aspect groups and requires only a small amount"
W15-2907,P14-1030,0,0.0121879,"lude the paper and discuss the future directions. 2 Relevant work Although the task we propose is new, there is a large body of work on sentiment analysis and aspect extraction that we can employ to build the components of our solution. In this section, we concentrate on research most directly relevant or applicable to our task. First, identifying product aspects as opinion targets has been extensively studied since it is an essential component of opinion mining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of generating coherent aspect groups that can be easily int"
W15-2907,D13-1172,0,0.0159346,"spect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of generating coherent aspect groups that can be easily interpreted by humans. On the other hand, approaches using prior knowledge sources or a small amount of annotation data are also studied to maintain high precision while lowering the manual annotation cost (Carenini et al., 2005; Zhai et al., 2010; Chen et al., 2013a; Chen et al., 2013b; Chen et al., 2013c). Particularly, the method proposed by Zhai et al. (2010) can easily incorporate aspect expressions into predefined aspect groups and requires only a small amount of manually annotated data as aspect seeds. Their work utilizes an extension of Naive Bayes classification defined by Nigam et al. (2000), which allows for a semi-supervised approach to assigning 3 Proposed method To determine which aspects can be included in the blurb of the product, we utilize the following review analysis technique: aspect grouping and aspect group ranking. We begin by ass"
W15-2907,H05-1043,0,0.0922772,"In Section 4, we evaluate our method with a travel domain data. In Section 5, we conclude the paper and discuss the future directions. 2 Relevant work Although the task we propose is new, there is a large body of work on sentiment analysis and aspect extraction that we can employ to build the components of our solution. In this section, we concentrate on research most directly relevant or applicable to our task. First, identifying product aspects as opinion targets has been extensively studied since it is an essential component of opinion mining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still f"
W15-2907,P14-1033,0,0.0246971,"Missing"
W15-2907,J11-1002,0,0.035285,"domain data. In Section 5, we conclude the paper and discuss the future directions. 2 Relevant work Although the task we propose is new, there is a large body of work on sentiment analysis and aspect extraction that we can employ to build the components of our solution. In this section, we concentrate on research most directly relevant or applicable to our task. First, identifying product aspects as opinion targets has been extensively studied since it is an essential component of opinion mining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of generating coherent"
W15-2907,P08-1036,0,0.0443644,"been extensively studied since it is an essential component of opinion mining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of generating coherent aspect groups that can be easily interpreted by humans. On the other hand, approaches using prior knowledge sources or a small amount of annotation data are also studied to maintain high precision while lowering the manual annotation cost (Carenini et al., 2005; Zhai et al., 2010; Chen et al., 2013a; Chen et al., 2013b; Chen et al., 2013c). Particularly, the method proposed by Zhai et al. (2010) can easily incorporate aspect exp"
W15-2907,J93-1003,0,0.0199814,"aspect expression e, collect all occurrences of e from all reviews. Next, for all occurrences of e, pick words from a context window (t left words, t right words, and the e itself) except for stop-words 1 . We used a window size of t = 3, 1 Aspect group ranking The next step is ranking aspect groups by their importance to display only those with a higher ranking. To rank aspect groups, we regard aspect groups that are distinguishing as important ones, and base our approach on an aspect ranking method proposed by Inui et al. (2013). Their ranking method is based on log-likelihood ratio (LLR) (Dunning, 1993), which compares the probabilities of observing the entire data under the hypothesis that a given product and aspect are dependent and a hypothesis that they are independent. In this way, the LLR score takes into account the entire review data including other products’ reviews. As it has a higher value for aspects that differentiate a product from the others, it is a great fit for our goal of finding aspects that distinguishes a product from its competitors. We extend the method proposed by Inui et al. (2013) because their goal differs from ours in two ways. First, as they are interested in ra"
W15-2907,P13-1173,0,0.0125,"ection 5, we conclude the paper and discuss the future directions. 2 Relevant work Although the task we propose is new, there is a large body of work on sentiment analysis and aspect extraction that we can employ to build the components of our solution. In this section, we concentrate on research most directly relevant or applicable to our task. First, identifying product aspects as opinion targets has been extensively studied since it is an essential component of opinion mining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of generating coherent aspect groups tha"
W15-2907,P11-1150,0,0.022495,"gestions, because enumerating aspects that have the same or similar concepts is re• We propose a novel task: finding characteristic aspects for blurb improvements. • To achieve this goal, we break the task into the two subtasks: aspect grouping and aspect group ranking. 42 words to appropriate aspect groups. Their method serves as a component that enables us to treat aspects at the concept level instead of just the word level. Lastly, another area applicable to our task is that of ranking aspects on the basis of various indicators, as exemplified by Zhang et al. (2010a), Zhang et al. (2010b), Yu et al. (2011), and Inui et al. (2013). While Zhang et al. (2010a), Zhang et al. (2010b), and Yu et al. (2011) propose aspect ranking methods based on aspect importance in a whole given domain, Inui et al. (2013) aim to find distinguishing aspect expressions of a product from other ones. They use a scoring method to rank aspect expressions and their variants, so theirs is the most appropriate technique for our task of discovering important aspects for users. To employ their approach for our task, we extend their method as described in the section 3.2. • To confirm our two-step framework, we adopt known and"
W15-2907,C10-1143,0,0.0372208,"Missing"
W15-2907,D07-1114,0,0.0248658,"our method with a travel domain data. In Section 5, we conclude the paper and discuss the future directions. 2 Relevant work Although the task we propose is new, there is a large body of work on sentiment analysis and aspect extraction that we can employ to build the components of our solution. In this section, we concentrate on research most directly relevant or applicable to our task. First, identifying product aspects as opinion targets has been extensively studied since it is an essential component of opinion mining and sentiment analysis work (Hu and Liu, 2004; Popescu and Etzioni, 2005; Kobayashi et al., 2007; Qiu et al., 2011; Xu et al., 2013; Liu et al., 2014). This direction of research has been changing from merely enumerating aspects to capturing a more structured organization such as aspect meaning, a task that is also attempted as part of this work. Existing research that focuses on structuring aspect groups is particularly relevant to our task. Although there exist fully unsupervised solutions based on topic modeling (Titov and McDonald, 2008a; Titov and McDonald, 2008b; Guo et al., 2009; Brody and Elhadad, 2010; Chen et al., 2014), the unsupervised approach still faces the challenge of ge"
W15-2907,C10-2167,0,0.0504667,"Missing"
W15-2907,H05-2017,0,\N,Missing
W16-4405,N13-1122,0,0.127349,"ine similarity and can be distinguished from similar and related question pairs. However, similar and related question pairs are not easily distinguishable. Our proposed algorithm will aim to distinguish between these question pairs. Figure 1: Cosine similarity and number of similar entities in question pair From this sample dataset, we noticed that the higher the number of common entities or entity variations in a question pair, the easier it is to use cosine similarity to distinguish the question pair categories i.e. potential answers, similar questions, related questions. entities in a KB (Guo et al., 2013), from the title and content sections of questions. The key contribution of this paper is to propose an entity-based algorithm to reduce the number of unanswered questions in entity rich question categories by recommending the best answer to past resolved questions with shared needs to a an unanswered question, if it exists, otherwise recommend past resolved questions with similar needs. 2 Cosine similarity and Entity-based approach Cosine similarity has been widely used to find similar questions and sentences (Salton and McGill , 1986). However, due to the lack of uniformity in CQA users writ"
W19-4433,Q13-1032,0,0.736654,"onse (a subsequence of words) that causes the response to be awarded points in the analytic score. In Figure 1, for example, the phrase Western culture is identified as a justification for criterion A, whereas the phrase Conflicts of interest is a justification for criterion B. Justification cues not only explain the model’s prediction but also help students learn how to improve their responses. One crucial issue in addressing such analytical assessment tasks is the lack of data. The datasets that are presently available for SAS research (Mohler et al., 2011; ASAP-SAS; Dzikovska et al., 2013; Basu et al., 2013, etc.) are all accompanied by annotations of holistic scores alone. In this study, we developed a new dataset with annotated analytic scores and justification cues as well as holistic scores. The dataset contains 2,100 sample student responses for each of six distinct reading comprehension test prompts, collected from commercial achievement tests for Japanese high school students. The dataset is publicly available for research purposes.1 SAS requires content-based, prompt-specific rubrics, which means that one needs to create a labeled dataset to train a model for each given prompt. This natu"
W19-4433,N12-1021,0,0.0239498,"rd issue is the comparison between the “NN base” model and the “+just.” model trained on both the analytic score and justification signals. We can observe that using justification signals as well as analytic score signals for training further boosts the performance at holistic score prediction, particularly when the training set is smaller. 6 Related Work Short answer scoring Previous research on SAS has solely focused on holistic score prediction. We believe that this is partly because, to date, the publicly available datasets for SAS have contained holistic scores only (Mohler et al., 2011; Dzikovska et al., 2012, 2013; ASAP-SAS) . To the best of our knowledge, our dataset is the first to provide both annotated analytic scores and their justification cues. Analytical assessment Analytical assessment has been studied in the context of automated essay scoring (Persing and Ng, 2016, 2015, etc.). The analytic criteria adopted in essay scoring tend to be more general, e.g., organization, clarity, and argument strength. In contrast, analytic criteria in SAS are typically prompt-specific as in our examples in Figure 1. Thus, the analytic criteria need to be learned by the model separately for each individual"
W19-4433,I17-1004,0,0.0527303,"Missing"
W19-4433,W15-0610,0,0.0184508,"rompt is a text which either elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics spelling). Assessing student responses by analyt"
W19-4433,I17-2002,0,0.0135897,"evaluation phase, the predicted scores are re-scaled back to their original range. Training with holistic scores. To train the whole network on holistic score annotations, we minimize the MSE calculated with gold and predicted holistic scores (Equation 1) as follows: N 1 X (n) (n) (shol − sˆhol )2 , N where N is the number of training instances, and (n) (n) shol and sˆhol are the predicted score and gold score, respectively. Supervised attention. We further train the attention mechanism for each criterion in a supervised manner, called supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017). In supervised attention, attention is learned from the difference between the span where the attention is focused and the given gold signal of a justification cue. Following a previous study by Liu et al. (2016), we add a soft constraint method to obtain the following objective function: att is defined as follows: An attention mechanism fm T X αm,t ht (4) t=1 An attention value αm,t denotes the importance weight, which represents relative importance of the t-th word for predicting analytic score sm . 4.4 Justification identification method N X 1 X (n) (n) { (si − sˆi )2 N The attention mecha"
W19-4433,N06-1023,0,0.0363905,"only have analytic scores annotated to a small set of responses. Thus we can train a model on these annotations for each task. We consider this scenario as our baseline scenario. We refer to the model for this scenario as “NN base.” SVR Baseline We also implemented another simpler baseline model based on the support vector regression model (SVR) following Sakaguchi et al. (2015) to provide sparse feature-based baseline results. We adopted the feature set proposed by Sakaguchi et al. (2015), which includes word 1gram, word 2-gram, and predicate-argument structure features2 . We used KNP 4.16 (Kawahara and Kurohashi, 2006) to extract Japanese predicateargument structure features. 5.2 SVR NN base +just. +hol. Q2 Q3 Q4 Q5 Q6 Analytic/Justification: 25 .60 .20 .54 .58 .45 .62 .19 .58 .64 .47 .73 .29 .64 .74 .53 .84 .48 .72 .86 .75 Analytic/Justification: 50 .73 .29 .64 .68 .56 .78 .29 .68 .72 .59 .85 .38 .71 .78 .64 .93 .59 .71 .87 .79 Analytic/Justification: 100 .80 .35 .72 .73 .66 .84 .40 .74 .79 .67 .88 .52 .76 .81 .72 .93 .67 .81 .87 .82 Analytic/Justification: 200 .87 .44 .77 .78 .71 .91 .57 .78 .83 .76 .92 .65 .80 .84 .78 .94 .72 .82 .88 .83 .94 .76 .84 .82 .90 Scenario (ii): (i) + justification signals In a"
W19-4433,N15-1111,0,0.022321,"nymous URL once the paper is accepted. We chose the same hyperparameters and training settings as in Riordan et al. (2017)’s holistic scoring model. Scenario (i): Basic setting (analytic score signals only) The first scenario assumes that we only have analytic scores annotated to a small set of responses. Thus we can train a model on these annotations for each task. We consider this scenario as our baseline scenario. We refer to the model for this scenario as “NN base.” SVR Baseline We also implemented another simpler baseline model based on the support vector regression model (SVR) following Sakaguchi et al. (2015) to provide sparse feature-based baseline results. We adopted the feature set proposed by Sakaguchi et al. (2015), which includes word 1gram, word 2-gram, and predicate-argument structure features2 . We used KNP 4.16 (Kawahara and Kurohashi, 2006) to extract Japanese predicateargument structure features. 5.2 SVR NN base +just. +hol. Q2 Q3 Q4 Q5 Q6 Analytic/Justification: 25 .60 .20 .54 .58 .45 .62 .19 .58 .64 .47 .73 .29 .64 .74 .53 .84 .48 .72 .86 .75 Analytic/Justification: 50 .73 .29 .64 .68 .56 .78 .29 .68 .72 .59 .85 .38 .71 .78 .64 .93 .59 .71 .87 .79 Analytic/Justification: 100 .80 .35"
W19-4433,W04-3250,0,0.0171153,", which was improved by the extra holistic score signals. A more in-depth analysis of this matter is needed, but our findings do raise the nontrivial question of which architecture is optimal to maximize the gain that results from including justification identification from holistic score signals. Summary These results suggest that our scenarios (ii) and (iii) are both worth considering in order to improve the performance of analytic score prediction. Note that the gains achieved by incorporating scenarios (ii) and (iii) are both statistically significant (p &lt; 0.01 by a paired bootstrap test (Koehn, 2004)). Specifically, the performance of the “+just.” model was significantly better than that of the “NN base” model for all the prompts. The performance of the “+hol.” model was also significantly better than that of the “+just.” model for all the prompts. Additional analysis Another interesting question deals with how well the accuracy of analytic score prediction correlates with the accuracy of justification identification. We observed that the neural baseline models showed strong performance for justification identification. These results raise the simple question of whether the sys3 Since our"
W19-4433,W04-3230,0,0.227915,"Missing"
W19-4433,N16-1123,0,0.0257892,"rompt. Typically, a prompt is a text which either elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics spelling). Assessing"
W19-4433,C16-1291,0,0.0253373,"the scale. In the evaluation phase, the predicted scores are re-scaled back to their original range. Training with holistic scores. To train the whole network on holistic score annotations, we minimize the MSE calculated with gold and predicted holistic scores (Equation 1) as follows: N 1 X (n) (n) (shol − sˆhol )2 , N where N is the number of training instances, and (n) (n) shol and sˆhol are the predicted score and gold score, respectively. Supervised attention. We further train the attention mechanism for each criterion in a supervised manner, called supervised attention (Mi et al., 2016; Liu et al., 2016; Kamigaito et al., 2017). In supervised attention, attention is learned from the difference between the span where the attention is focused and the given gold signal of a justification cue. Following a previous study by Liu et al. (2016), we add a soft constraint method to obtain the following objective function: att is defined as follows: An attention mechanism fm T X αm,t ht (4) t=1 An attention value αm,t denotes the importance weight, which represents relative importance of the t-th word for predicting analytic score sm . 4.4 Justification identification method N X 1 X (n) (n) { (si − sˆi"
W19-4433,D16-1249,0,0.0661177,"Missing"
W19-4433,P11-1076,0,0.333807,"justification cue, we refer to the segment of the response (a subsequence of words) that causes the response to be awarded points in the analytic score. In Figure 1, for example, the phrase Western culture is identified as a justification for criterion A, whereas the phrase Conflicts of interest is a justification for criterion B. Justification cues not only explain the model’s prediction but also help students learn how to improve their responses. One crucial issue in addressing such analytical assessment tasks is the lack of data. The datasets that are presently available for SAS research (Mohler et al., 2011; ASAP-SAS; Dzikovska et al., 2013; Basu et al., 2013, etc.) are all accompanied by annotations of holistic scores alone. In this study, we developed a new dataset with annotated analytic scores and justification cues as well as holistic scores. The dataset contains 2,100 sample student responses for each of six distinct reading comprehension test prompts, collected from commercial achievement tests for Japanese high school students. The dataset is publicly available for research purposes.1 SAS requires content-based, prompt-specific rubrics, which means that one needs to create a labeled data"
W19-4433,P15-1053,0,0.0747804,"Missing"
W19-4433,P16-1205,0,0.0157497,"stic score prediction, particularly when the training set is smaller. 6 Related Work Short answer scoring Previous research on SAS has solely focused on holistic score prediction. We believe that this is partly because, to date, the publicly available datasets for SAS have contained holistic scores only (Mohler et al., 2011; Dzikovska et al., 2012, 2013; ASAP-SAS) . To the best of our knowledge, our dataset is the first to provide both annotated analytic scores and their justification cues. Analytical assessment Analytical assessment has been studied in the context of automated essay scoring (Persing and Ng, 2016, 2015, etc.). The analytic criteria adopted in essay scoring tend to be more general, e.g., organization, clarity, and argument strength. In contrast, analytic criteria in SAS are typically prompt-specific as in our examples in Figure 1. Thus, the analytic criteria need to be learned by the model separately for each individual prompt. It is an interesting open question whether the insights gained from essay scoring research can be applicable to analytic SAS research. Interpretability of neural models In recent years, the interpretability of neural models has received widespread attention. Som"
W19-4433,W05-0202,0,0.0689774,"r elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics spelling). Assessing student responses by analytic scores as well as holisti"
W19-4433,W17-5017,0,0.366746,"free-text student responses to a given prompt. Typically, a prompt is a text which either elicits recall of information that was given in a reading passage, asks for a summary of a reading passage, or asks students to draw on knowledge they already have. The task is to assess the responses based on context and writing quality, in accordance with the criteria prespecified for each assessment by a scoring rubric. Automation of this process has the potential to significantly reduce the workload of human raters and has attracted a considerable amount of attention from both academia and industry (Riordan et al., 2017; Zhao et al., 2017; Sultan et al., 2016; Heilman and Madnani, 2015; Pulman and Sukkarieh, 2005; Leacock and Chodorow, 2003; Vigilante, 1999, etc.). It should be emphasized that, in admissions tests and other tests, such as writing proficiency tests, large groups of students receive and respond to the exact same set of problems, for which ∗ Current affiliation: Future moto.tomoya.mh7@is.naist.jp Corporation, mizu316 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 316–325 c Florence, Italy, August 2, 2019. 2019 Association for Computa"
W19-4804,E17-2039,1,0.838305,"Missing"
W19-4804,D15-1075,0,0.650059,"lity in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference"
W19-4804,W15-4002,0,0.598488,"lity in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference"
W19-4804,P18-1224,0,0.0386824,"Missing"
W19-4804,P17-1152,0,0.0677033,"Missing"
W19-4804,N19-1423,0,0.0446964,"any, ever, at all, anything, anyone, anymore, anyhow, anywhere) in the hypothesis 4 Results and Discussion 4.1 Baselines To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; Wang et al., 2017), ESIM (Enhanced Sequential Inference Model; Chen et al., 2017), Decomposable Attention Model (Parikh et al., 2016), KIM (Knowledge-based Inference Model; Chen et al., 2018), and BERT (Bidirectional Encoder Representations from Transformers model; Devlin et al., 2019). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI. For other models, we checked the performance trained with SNLI. In agreement with our dataset, we regarded the prediction label contradiction as non-entailment. Table 6 shows that the accuracies of all models were better on upward inferences, in accordance with the reported results of the GLUE leaderboard. The overall accuracy of each model was low. In particular, all models underperformed the majority baseline on downward inferences, despite"
W19-4804,P18-2103,0,0.0768747,"gmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined f"
W19-4804,N18-2017,0,0.0596308,"Missing"
W19-4804,W18-5446,0,0.0755493,"Missing"
W19-4804,N18-1101,0,0.578916,"xamples by crowdsourcing and well-designed ones from linguistics publications. To enable the evaluation of 1 The dataset will be made publicly available at https://github.com/verypluming/MED. skills required for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section 4). The results show that all models trained with SNLI (Bowman et al., 2015b) and MultiNLI (Williams et al., 2018) perform worse on downward inferences than on upward inferences. In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP (Yanaka et al., 2019). The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations be"
W19-4804,S18-2015,0,0.053669,"ght for both tasks. 3.1.1 Premise collection As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017). The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; Steedman, 2000) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg (Yoshikawa et al., 2017). Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono (Hu and Moss, 2018) and make the polarized part a blank field. We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2. As a result, we selected 1,485 examples involving expressions having arguments with upward monotonicity and 1,982 examples involving expressions having arguments with downward monotonicity. 3.1.2 Hy"
W19-4804,J16-4012,0,0.0701837,"oriented dataset We also collect monotonicity inference problems from previous manually curated datasets and linguistics publications. The motivation is that previous linguistics publications related to monotonicity reasoning are expected to contain welldesigned inference problems, which might be challenging problems for NLI models. We collected 1,184 examples from 11 linguistics publications (Barwise and Cooper, 1981; Hoeksema, 1986; Heim and Kratzer, 1998; Bonevac et al., 1999; Fyodorov et al., 2003; Geurts, 2003; Geurts and van der Slik, 2005; Zamansky et al., 2006; Szabolcsi et al., 2008; Winter, 2016; Denic et al., 2019). Regarding previous manually-curated datasets, we collected 93 examples for monotonicity reasoning from the GLUE diagnostic dataset, and 37 single-premise problems from FraCaS. Genre Crowd Paper Tags up up: cond up:rev: conj up:lex down:lex down:conj down:cond down up:rev up:disj up:lex: rev non down Premise There is a cat on the chair If you heard her speak English, you would take her for a native American Dogs and cats have all the good qualities of people without at the same time possessing their weaknesses He approached the boy reading a magazine Tom hardly ever liste"
W19-4804,2014.lilt-9.7,0,0.235188,"tual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A context is upward entailing (shown by [... ↑]) that allows an inference from (1a) to (1b), where French dinner is replaced by a more general concept dinner. On the other hand, a downward ent"
W19-4804,S19-1027,1,0.650772,"red for monotonicity reasoning, we annotate each example in our dataset with linguistic tags associated with monotonicity reasoning. We measure the performance of state-of-the-art NLI models on monotonicity reasoning and investigate their generalization ability in upward and downward reasoning (Section 4). The results show that all models trained with SNLI (Bowman et al., 2015b) and MultiNLI (Williams et al., 2018) perform worse on downward inferences than on upward inferences. In addition, we analyzed the performance of models trained with an automatically created monotonicity dataset, HELP (Yanaka et al., 2019). The analysis with monotonicity data augmentation shows that models tend to perform better in the same direction of monotonicity with the training set, while they perform worse in the opposite direction. This indicates that the accuracy on monotonicity reasoning depends solely on the majority direction in the training set, and models might lack the ability to capture the structural relations between monotonicity operators and their arguments. 2 Monotonicity As an example of a monotonicity inference, consider the example with the determiner every in (3); here the premise P entails the hypothes"
W19-4804,J88-2003,0,0.468338,"Missing"
W19-4804,C18-1198,0,0.039856,"d that these models might be limited in their generalization ability in upward and downward reasoning. 1 Introduction Natural language inference (NLI), also known as recognizing textual entailment (RTE), has been proposed as a benchmark task for natural language understanding. Given a premise P and a hypothesis H, the task is to determine whether the premise semantically entails the hypothesis (Dagan et al., 2013). A number of recent works attempt to test and analyze what type of inferences an NLI model may be performing, focusing on various types of lexical inferences (Glockner et al., 2018; Naik et al., 2018; Poliak et al., 2018) and logical inferences (Bowman et al., 2015b; Evans et al., 2018). Concerning logical inferences, monotonicity reasoning (van Benthem, 1983; Icard and Moss, 2014), which is a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures. Consider examples in (1) and (2). (1) a. All [ workers ↓] [joined for a French dinner ↑] b. All workers joined for a dinner c. All new workers joined for a French dinner (2) a. Not all [new workers ↑] joined for a dinner b. Not all workers joined for a dinner A conte"
W19-4804,D16-1244,0,0.0952364,"Missing"
W19-4804,S18-2023,0,0.0661357,"Missing"
W19-4804,L18-1239,0,0.100411,"Missing"
W19-4804,P17-1026,0,0.0754829,"rated in (i). Figure 1 summarizes the overview of our human-oriented dataset creation. We used the crowdsourcing platform Figure Eight for both tasks. 3.1.1 Premise collection As a resource, we use declarative sentences with more than five tokens from the Parallel Meaning Bank (PMB) (Abzianidze et al., 2017). The PMB contains syntactically correct sentences annotated with its syntactic category in Combinatory Categorial Grammar (CCG; Steedman, 2000) format, which is suitable for our purpose. To get a whole CCG derivation tree, we parse each sentence by the state-of-the-art CCG parser, depccg (Yoshikawa et al., 2017). Then, we add a polarity to every constituent of the CCG tree by the polarity computation system ccg2mono (Hu and Moss, 2018) and make the polarized part a blank field. We ran a trial rephrasing task on 500 examples and detected 17 expressions that were too general and thus difficult to rephrase them in a natural way (e.g., every one, no time). We removed examples involving such expressions. To collect more downward inference examples, we select examples involving determiners in Table 1 and downward operators in Table 2. As a result, we selected 1,485 examples involving expressions having arg"
Y16-3027,Y09-1009,0,0.387638,"tion (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set wh"
Y16-3027,D15-1103,0,0.0232597,"ally signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challen"
Y16-3027,I08-1071,0,0.74711,"small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall int"
Y16-3027,doddington-etal-2004-automatic,0,0.0532304,"context of automatic construction of an NE gazetteer 1http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/ jawiki_vector/ PACLIC 30 Proceedings from Wikipedia articles. Toral and Muñoz (2006) proposed a method to classify Wikipedia articles into three NE types (Location, Organization, Person) using words included in the body of the article. They used WordNet as an external knowledge base for collecting hypernym information. They also applied weighted voting heuristics to determine NE types of articles. Dakka and Cucerzan (2008) classiﬁed articles into four NE types (PER, ORG, LOC, MISC) deﬁned in ACE (Doddington et al., 2004) using supervised machine learning algorithms based on SVMs and naive Bayes. They used the bag-of-words in the target article as well as context words from the anchor text linking to the target article. Watanabe et al. (2007) focused on the HTML tree/link structure in Wikipedia articles. They formalized an NE categorization problem as assigning of NE labels to anchor texts in Wikipedia. They constructed graph-based representations of articles and estimated assignments of NE labels over the graphs using conditional random ﬁelds. In addition to these studies, there have been eﬀorts toward automa"
Y16-3027,P98-1068,0,0.0357389,"n in the ﬁrst sentence Headings of the article Direct categories deﬁned in Wikipedia Upper categories deﬁned in Wikipedia • An anchor text is not always identical to the article title to which the anchor refers. For this reason, we need to normalize an anchor text to the title of the article linked by the anchor. able to reproduce features T8, T12, T14, and M22 described in the original paper (Higashinaka et al., 2012) because those features require the authors’ internal resources to implement. For similar reasons, we used MeCab (Kudo et al., 2004) as a morphological analyzer instead of JTAG (Fuchi and Takagi, 1998), which was unavailable to us. For extracting text from Wikipedia dump, we used Wikipedia Extractor (http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor). We denote this baseline feature set as Fb . 4.2.2 Article Vectors To extend the aforementioned basic feature set, we hypothesize that the way how each article (i.e. named entity) is mentioned in other articles can also be a useful clue for classifying that article. To test this hypothesis, we introduce distributed representations of Wikipedia articles. Consider an article “Mount Everest”. This article is hyperlinked from other articles as"
Y16-3027,C12-1071,0,0.253416,"ined entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location i"
Y16-3027,P08-1047,0,0.0208216,"Missing"
Y16-3027,W04-3230,0,0.0226318,"haracters in the title Last character type in the title Last noun in the ﬁrst sentence Headings of the article Direct categories deﬁned in Wikipedia Upper categories deﬁned in Wikipedia • An anchor text is not always identical to the article title to which the anchor refers. For this reason, we need to normalize an anchor text to the title of the article linked by the anchor. able to reproduce features T8, T12, T14, and M22 described in the original paper (Higashinaka et al., 2012) because those features require the authors’ internal resources to implement. For similar reasons, we used MeCab (Kudo et al., 2004) as a morphological analyzer instead of JTAG (Fuchi and Takagi, 1998), which was unavailable to us. For extracting text from Wikipedia dump, we used Wikipedia Extractor (http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor). We denote this baseline feature set as Fb . 4.2.2 Article Vectors To extend the aforementioned basic feature set, we hypothesize that the way how each article (i.e. named entity) is mentioned in other articles can also be a useful clue for classifying that article. To test this hypothesis, we introduce distributed representations of Wikipedia articles. Consider an article"
Y16-3027,Q15-1023,0,0.033578,"s a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, a"
Y16-3027,W02-1111,0,0.0133169,"Missing"
Y16-3027,P13-1146,0,0.0145923,"Auer et al., 2007) have been devoted to provide Wikipedia articles with ontology class labels by applying simple heuristic or hand-crafted rules. However, these approaches heavily rely on metadata (e.g., infobox templates and category labels) and suﬀer from insuﬃcient coverage of rules due to the lack of metadata, as reported by Aprosio et al. (2013). Another trend of research which may seem relevant to our work can be found in eﬀorts for automatically annotating entity mentions in text with ﬁne-grained NE type labels deﬁned in an existing type hierarchy such as Freebase (Ling and Weld, 2012; Nakashole et al., 2013; Shimaoka et al., 2016). While these studies focus on the identiﬁcation and classiﬁcation of individual mentions, our work aims at the classiﬁcation of Wikipedia articles. The two tasks are related and may well beneﬁt from each other. However, they are not the same; techniques proposed for mention classiﬁcation cannot directly apply to our task nor can be compared with our methods. 537 The work closest to our study is done by Higashinaka et al. (2012), who proposed a supervised machine learning model for classifying Wikipedia articles into the 200 ﬁne-grained NE types deﬁned by Sekine et al."
Y16-3027,sekine-etal-2002-extended,1,0.691678,"hat both ideas gained their own statistically signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coa"
Y16-3027,W16-1313,1,0.800285,"been devoted to provide Wikipedia articles with ontology class labels by applying simple heuristic or hand-crafted rules. However, these approaches heavily rely on metadata (e.g., infobox templates and category labels) and suﬀer from insuﬃcient coverage of rules due to the lack of metadata, as reported by Aprosio et al. (2013). Another trend of research which may seem relevant to our work can be found in eﬀorts for automatically annotating entity mentions in text with ﬁne-grained NE type labels deﬁned in an existing type hierarchy such as Freebase (Ling and Weld, 2012; Nakashole et al., 2013; Shimaoka et al., 2016). While these studies focus on the identiﬁcation and classiﬁcation of individual mentions, our work aims at the classiﬁcation of Wikipedia articles. The two tasks are related and may well beneﬁt from each other. However, they are not the same; techniques proposed for mention classiﬁcation cannot directly apply to our task nor can be compared with our methods. 537 The work closest to our study is done by Higashinaka et al. (2012), who proposed a supervised machine learning model for classifying Wikipedia articles into the 200 ﬁne-grained NE types deﬁned by Sekine et al. (2002). They conducted e"
Y16-3027,U09-1015,0,0.778436,"Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-gra"
Y16-3027,W06-2809,0,0.899903,"nd explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given th"
Y16-3027,D07-1068,0,0.814403,"al models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁcation. One big challenge is how to alleviate the problem of data sparseness when applying supervised machine learning approaches. For example, articles such as “Japan”, “Mt. Fuji”, and “Tokyo dome”, may be classiﬁed as Country, Mountain, and Sports_Facility respectively in a ﬁne-grained type set whereas all of them fall into the same type Location in a common coarse-grained type set. Given the same number of labeled"
Y16-3027,C12-2133,0,0.0668656,"their own statistically signiﬁcant improvement separately in classiﬁcation accuracy. 1 Introduction Recognizing named entities (NEs) in text is a crucial component task of a broad range of NLP applications including information extraction and question answering. Early work on named entity recognition (NER) deﬁned a small number of coarse-grained entity types such as Person and Location and explored computational models for automatizing the task. One recent direction of extending this research ﬁeld is to consider a larger set of ﬁne-grained entity types (Lee et al., 2006; Sekine et al., 2002; Yosef et al., 2012; Corro et al., 2015). Recent studies report that ﬁne-grained NER makes improvements to such applications as entity linking (Ling et al., 2015) and The task of classifying Wikipedia articles into a predeﬁned set of semantic classes has already been addressed by many researchers (Chang et al., 2009; Dakka and Cucerzan, 2008; Higashinaka et al., 2012; Tardif et al., 2009; Toral and Muñoz, 2006; Watanabe et al., 2007). However, most of these studies assume a coarse-grained NE type set (3 to 15 types). Fine-grained classiﬁcation is naturally expected to be more diﬃcult than coarsegrained classiﬁca"
Y16-3027,C98-1065,0,\N,Missing
